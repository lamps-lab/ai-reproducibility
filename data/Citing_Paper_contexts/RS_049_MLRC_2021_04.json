{
    "offset": 0,
    "data": [
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Although the roll-out of learned weights from attention mechanisms has been used as a proxy to approximate in a post-hoc fashion which regions of the input space support the prediction [17], the extent to which this holds true is still a matter of debate [1, 5]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "596bd7ac0a2dcb4762df97dd99291bf952f0b088",
                "externalIds": {
                    "DOI": "10.1145/3577190.3614143",
                    "CorpusId": 263742969
                },
                "corpusId": 263742969,
                "publicationVenue": {
                    "id": "d11025b6-9660-45df-b13a-555e3ff4ceca",
                    "name": "International Conference on Multimodal Interaction",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Multimodal Interact",
                        "International Conference on Multimodal Interfaces",
                        "Int Conf Multimodal Interface",
                        "ICMI"
                    ],
                    "url": "https://en.wikipedia.org/wiki/ACM/IEEE_Virtual_Reality_International_Conference"
                },
                "url": "https://www.semanticscholar.org/paper/596bd7ac0a2dcb4762df97dd99291bf952f0b088",
                "title": "Interpreting Sign Language Recognition using Transformers and MediaPipe Landmarks",
                "abstract": "Sign Language Recognition (SLR) is a challenging task that aims to bridge the communication gap between the deaf and hearing communities. In recent years, deep learning-based approaches have shown promising results in SLR. However, the lack of interpretability remains a significant challenge. In this paper, we seek to understand which hand and pose MediaPipe Landmarks are deemed the most important for prediction as estimated by a Transformer model. We propose to embed a learnable array of parameters into the model that performs an element-wise multiplication of the inputs. This learned array highlights the most informative input features that contributed to solve the recognition task. Resulting in a human-interpretable vector that lets us interpret the model predictions. We evaluate our approach on public datasets called WLASL100 (SRL) and IPNHand (gesture recognition). We believe that the insights gained in this way could be exploited for the development of more efficient SLR pipelines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2132811142",
                        "name": "Cristina Luna-Jim\u00e9nez"
                    },
                    {
                        "authorId": "1409279087",
                        "name": "M. Gil-Mart\u00edn"
                    },
                    {
                        "authorId": "150278269",
                        "name": "Ricardo Kleinlein"
                    },
                    {
                        "authorId": "1403812163",
                        "name": "R. San-Segundo"
                    },
                    {
                        "authorId": "1400972762",
                        "name": "F. Fern\u00e1ndez-Mart\u00ednez"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Method Drop (\u2193) Inc (\u2191) Coher (\u2191) Compl (\u2193) ADCC (\u2191) Drop (\u2193) Inc (\u2191) Coher (\u2191) Compl (\u2193) ADCC (\u2191) Relevance[Chefer et al. [2021]] 55.",
                "Chefer et al. [2021] improved the visualization quality of attention-rollout and enabled generating class specific activation map with relevance information but it requires gradient information so it cannot be used for supporting explainability in inference time. And, all the existing techniques require the accessibility to the all attention layer\u2019s activation in a model and it requires deeper model dependency. To overcome the limitations of existing XAI techniques for ViT, we propose an accurate and efficient reciprocal information-based approach. Our method utilizes reciprocal relationship between new spatially masked feature inputs (positional token masking) and network\u2019s prediction results. By identifying these relations, we can generate visual explanations that provide insights into the model\u2019s decision-making process without using attention layers\u2019 information and assuming their internal relationship as previous researches. Our approach not only improves the interpretability of ViT models but also enables users to use XAI result in their inference system without trainable model. We evaluate our method on a range of ViT classification models and demonstrate its effectiveness in generating high-quality visual explanations that aid in understanding the models\u2019 behavior and its accuracy in measuring Average Drop-Coherence-Complexity (ADCC) score suggested by Poppi et al. [2021]. The main contributions of this paper include:",
                "Chefer et al. [2021] improved the visualization quality of attention-rollout and enabled generating class specific activation map with relevance information but it requires gradient information so it cannot be used for supporting explainability in inference time."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "2d9e604632ff7848aa92aae1a1f828b8f755ad2c",
                "externalIds": {
                    "ArXiv": "2310.02588",
                    "CorpusId": 263620450
                },
                "corpusId": 263620450,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2d9e604632ff7848aa92aae1a1f828b8f755ad2c",
                "title": "ViT-ReciproCAM: Gradient and Attention-Free Visual Explanations for Vision Transformer",
                "abstract": "This paper presents a novel approach to address the challenges of understanding the prediction process and debugging prediction errors in Vision Transformers (ViT), which have demonstrated superior performance in various computer vision tasks such as image classification and object detection. While several visual explainability techniques, such as CAM, Grad-CAM, Score-CAM, and Recipro-CAM, have been extensively researched for Convolutional Neural Networks (CNNs), limited research has been conducted on ViT. Current state-of-the-art solutions for ViT rely on class agnostic Attention-Rollout and Relevance techniques. In this work, we propose a new gradient-free visual explanation method for ViT, called ViT-ReciproCAM, which does not require attention matrix and gradient information. ViT-ReciproCAM utilizes token masking and generated new layer outputs from the target layer's input to exploit the correlation between activated tokens and network predictions for target classes. Our proposed method outperforms the state-of-the-art Relevance method in the Average Drop-Coherence-Complexity (ADCC) metric by $4.58\\%$ to $5.80\\%$ and generates more localized saliency maps. Our experiments demonstrate the effectiveness of ViT-ReciproCAM and showcase its potential for understanding and debugging ViT models. Our proposed method provides an efficient and easy-to-implement alternative for generating visual explanations, without requiring attention and gradient information, which can be beneficial for various applications in the field of computer vision.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2032811",
                        "name": "Seokhyun Byun"
                    },
                    {
                        "authorId": "2148961398",
                        "name": "Won-Jo Lee"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "58b817ca88d35656ac289f0011d4371cfda85382",
                "externalIds": {
                    "ArXiv": "2310.00795",
                    "CorpusId": 263605903
                },
                "corpusId": 263605903,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/58b817ca88d35656ac289f0011d4371cfda85382",
                "title": "A Comprehensive Review of Generative AI in Healthcare",
                "abstract": "The advancement of Artificial Intelligence (AI) has catalyzed revolutionary changes across various sectors, notably in healthcare. Among the significant developments in this field are the applications of generative AI models, specifically transformers and diffusion models. These models have played a crucial role in analyzing diverse forms of data, including medical imaging (encompassing image reconstruction, image-to-image translation, image generation, and image classification), protein structure prediction, clinical documentation, diagnostic assistance, radiology interpretation, clinical decision support, medical coding, and billing, as well as drug design and molecular representation. Such applications have enhanced clinical diagnosis, data reconstruction, and drug synthesis. This review paper aims to offer a thorough overview of the generative AI applications in healthcare, focusing on transformers and diffusion models. Additionally, we propose potential directions for future research to tackle the existing limitations and meet the evolving demands of the healthcare sector. Intended to serve as a comprehensive guide for researchers and practitioners interested in the healthcare applications of generative AI, this review provides valuable insights into the current state of the art, challenges faced, and prospective future directions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2155559607",
                        "name": "Yasin Shokrollahi"
                    },
                    {
                        "authorId": "2253397980",
                        "name": "Sahar Yarmohammadtoosky"
                    },
                    {
                        "authorId": "2225837555",
                        "name": "Matthew M. Nikahd"
                    },
                    {
                        "authorId": "2106110132",
                        "name": "Pengfei Dong"
                    },
                    {
                        "authorId": "2253859290",
                        "name": "Xianqi Li"
                    },
                    {
                        "authorId": "2151996389",
                        "name": "Linxia Gu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9484cec311dabf63d43246912f90373579217292",
                "externalIds": {
                    "DOI": "10.1007/s42979-023-02204-2",
                    "CorpusId": 263158130
                },
                "corpusId": 263158130,
                "publicationVenue": {
                    "id": "7a7dc89b-e1a6-44df-a496-46c330a87840",
                    "name": "SN Computer Science",
                    "type": "journal",
                    "alternate_names": [
                        "SN Comput Sci"
                    ],
                    "issn": "2661-8907",
                    "alternate_issns": [
                        "2662-995X"
                    ],
                    "url": "https://link.springer.com/journal/42979"
                },
                "url": "https://www.semanticscholar.org/paper/9484cec311dabf63d43246912f90373579217292",
                "title": "ViT: Quantifying Chest X-Ray Images Using Vision Transformer & XAI Technique",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2088958187",
                        "name": "Y. Salini"
                    },
                    {
                        "authorId": "2248337896",
                        "name": "J. HariKiran"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8273bd168474897e1b7f628946a33541c720ab26",
                "externalIds": {
                    "ArXiv": "2309.16108",
                    "CorpusId": 263139621
                },
                "corpusId": 263139621,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8273bd168474897e1b7f628946a33541c720ab26",
                "title": "Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words",
                "abstract": "Vision Transformer (ViT) has emerged as a powerful architecture in the realm of modern computer vision. However, its application in certain imaging fields, such as microscopy and satellite imaging, presents unique challenges. In these domains, images often contain multiple channels, each carrying semantically distinct and independent information. Furthermore, the model must demonstrate robustness to sparsity in input channels, as they may not be densely available during training or testing. In this paper, we propose a modification to the ViT architecture that enhances reasoning across the input channels and introduce Hierarchical Channel Sampling (HCS) as an additional regularization technique to ensure robustness when only partial channels are presented during test time. Our proposed model, ChannelViT, constructs patch tokens independently from each input channel and utilizes a learnable channel embedding that is added to the patch tokens, similar to positional embeddings. We evaluate the performance of ChannelViT on ImageNet, JUMP-CP (microscopy cell imaging), and So2Sat (satellite imaging). Our results show that ChannelViT outperforms ViT on classification tasks and generalizes well, even when a subset of input channels is used during testing. Across our experiments, HCS proves to be a powerful regularizer, independent of the architecture employed, suggesting itself as a straightforward technique for robust ViT training. Lastly, we find that ChannelViT generalizes effectively even when there is limited access to all channels during training, highlighting its potential for multi-channel imaging under real-world conditions with sparse sensors.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145854784",
                        "name": "Yu Bao"
                    },
                    {
                        "authorId": "2248190581",
                        "name": "Srinivasan Sivanandan"
                    },
                    {
                        "authorId": "3302673",
                        "name": "Theofanis Karaletsos"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fda6100a3c1cc4c46fae5b57227903b73ae21116",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-08035",
                    "ArXiv": "2309.08035",
                    "DOI": "10.48550/arXiv.2309.08035",
                    "CorpusId": 262013082
                },
                "corpusId": 262013082,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fda6100a3c1cc4c46fae5b57227903b73ae21116",
                "title": "Interpretability-Aware Vision Transformer",
                "abstract": "Vision Transformers (ViTs) have become prominent models for solving various vision tasks. However, the interpretability of ViTs has not kept pace with their promising performance. While there has been a surge of interest in developing {\\it post hoc} solutions to explain ViTs' outputs, these methods do not generalize to different downstream tasks and various transformer architectures. Furthermore, if ViTs are not properly trained with the given data and do not prioritize the region of interest, the {\\it post hoc} methods would be less effective. Instead of developing another {\\it post hoc} approach, we introduce a novel training procedure that inherently enhances model interpretability. Our interpretability-aware ViT (IA-ViT) draws inspiration from a fresh insight: both the class patch and image patches consistently generate predicted distributions and attention maps. IA-ViT is composed of a feature extractor, a predictor, and an interpreter, which are trained jointly with an interpretability-aware training objective. Consequently, the interpreter simulates the behavior of the predictor and provides a faithful explanation through its single-head self-attention mechanism. Our comprehensive experimental results demonstrate the effectiveness of IA-ViT in several image classification tasks, with both qualitative and quantitative evaluations of model performance and interpretability. Source code is available from: https://github.com/qiangyao1988/IA-ViT.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2062242240",
                        "name": "Yao Qiang"
                    },
                    {
                        "authorId": "46651935",
                        "name": "Chengyin Li"
                    },
                    {
                        "authorId": "4386787",
                        "name": "Prashant Khanduri"
                    },
                    {
                        "authorId": "39895985",
                        "name": "D. Zhu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "da279c477f12408d2b23737cce1ab15b0079b595",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-03631",
                    "ArXiv": "2309.03631",
                    "DOI": "10.48550/arXiv.2309.03631",
                    "CorpusId": 261582733
                },
                "corpusId": 261582733,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/da279c477f12408d2b23737cce1ab15b0079b595",
                "title": "Insights Into the Inner Workings of Transformer Models for Protein Function Prediction",
                "abstract": "Motivation: We explored how explainable AI (XAI) can help to shed light into the inner workings of neural networks for protein function prediction, by extending the widely used XAI method of integrated gradients such that latent representations inside of transformer models, which were finetuned to Gene Ontology term and Enzyme Commission number prediction, can be inspected too. Results: The approach enabled us to identify amino acids in the sequences that the transformers pay particular attention to, and to show that these relevant sequence parts reflect expectations from biology and chemistry, both in the embedding layer and inside of the model, where we identified transformer heads with a statistically significant correspondence of attribution maps with ground truth sequence annotations (e.g., transmembrane regions, active sites) across many proteins. Availability and Implementation: Source code can be accessed at https://github.com/markuswenzel/xai-proteins .",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "113782311",
                        "name": "M. Wenzel"
                    },
                    {
                        "authorId": "2239130064",
                        "name": "Erik Gr\u00fcner"
                    },
                    {
                        "authorId": "1481095347",
                        "name": "Nils Strodthoff"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "The relevancy map loss, uses a CLIP-based relevancy [11] to provide rough estimation for the localization map"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "89fcc1d56837ad32169d1ccf92805a16516b8205",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-03874",
                    "ArXiv": "2309.03874",
                    "DOI": "10.48550/arXiv.2309.03874",
                    "CorpusId": 261582510
                },
                "corpusId": 261582510,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/89fcc1d56837ad32169d1ccf92805a16516b8205",
                "title": "Box-based Refinement for Weakly Supervised and Unsupervised Localization Tasks",
                "abstract": "It has been established that training a box-based detector network can enhance the localization performance of weakly supervised and unsupervised methods. Moreover, we extend this understanding by demonstrating that these detectors can be utilized to improve the original network, paving the way for further advancements. To accomplish this, we train the detectors on top of the network output instead of the image data and apply suitable loss backpropagation. Our findings reveal a significant improvement in phrase grounding for the ``what is where by looking'' task, as well as various methods of unsupervised object discovery. Our code is available at https://github.com/eyalgomel/box-based-refinement.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2238208483",
                        "name": "Eyal Gomel"
                    },
                    {
                        "authorId": "1438948620",
                        "name": "Tal Shaharabany"
                    },
                    {
                        "authorId": "145128145",
                        "name": "Lior Wolf"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Attention rollout [1, 9] is a popular way to understand whether attention modules can provide such explanations [19, 34]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8864519b5b05c2d28eb2ae329b03ba9b9d209cf9",
                "externalIds": {
                    "ArXiv": "2309.03173",
                    "DBLP": "journals/corr/abs-2309-03173",
                    "DOI": "10.48550/arXiv.2309.03173",
                    "CorpusId": 261463486
                },
                "corpusId": 261463486,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8864519b5b05c2d28eb2ae329b03ba9b9d209cf9",
                "title": "PDiscoNet: Semantically consistent part discovery for fine-grained recognition",
                "abstract": "Fine-grained classification often requires recognizing specific object parts, such as beak shape and wing patterns for birds. Encouraging a fine-grained classification model to first detect such parts and then using them to infer the class could help us gauge whether the model is indeed looking at the right details better than with interpretability methods that provide a single attribution map. We propose PDiscoNet to discover object parts by using only image-level class labels along with priors encouraging the parts to be: discriminative, compact, distinct from each other, equivariant to rigid transforms, and active in at least some of the images. In addition to using the appropriate losses to encode these priors, we propose to use part-dropout, where full part feature vectors are dropped at once to prevent a single part from dominating in the classification, and part feature vector modulation, which makes the information coming from each part distinct from the perspective of the classifier. Our results on CUB, CelebA, and PartImageNet show that the proposed method provides substantially better part discovery performance than previous methods while not requiring any additional hyper-parameter tuning and without penalizing the classification performance. The code is available at https://github.com/robertdvdk/part_detection.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2237150166",
                        "name": "Robert van der Klis"
                    },
                    {
                        "authorId": "40894329",
                        "name": "Stephan Alaniz"
                    },
                    {
                        "authorId": "38286801",
                        "name": "Massimiliano Mancini"
                    },
                    {
                        "authorId": "10373561",
                        "name": "C. Dantas"
                    },
                    {
                        "authorId": "2243268263",
                        "name": "Dino Ienco"
                    },
                    {
                        "authorId": "2893664",
                        "name": "Zeynep Akata"
                    },
                    {
                        "authorId": "2237150511",
                        "name": "Diego Marcos"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Both methods have been applied to derive the relevance score of inputs in Transformer-based models (Wu & Ong, 2021; Chefer et al., 2021)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "26089bdfdbca1e6eaaceca71e3116b715bec6d47",
                "externalIds": {
                    "ArXiv": "2309.01029",
                    "DBLP": "journals/corr/abs-2309-01029",
                    "DOI": "10.48550/arXiv.2309.01029",
                    "CorpusId": 261530292
                },
                "corpusId": 261530292,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/26089bdfdbca1e6eaaceca71e3116b715bec6d47",
                "title": "Explainability for Large Language Models: A Survey",
                "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional machine learning models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2237987232",
                        "name": "Haiyan Zhao"
                    },
                    {
                        "authorId": "2237948828",
                        "name": "Hanjie Chen"
                    },
                    {
                        "authorId": "145338224",
                        "name": "F. Yang"
                    },
                    {
                        "authorId": "47717322",
                        "name": "Ninghao Liu"
                    },
                    {
                        "authorId": "13689700",
                        "name": "Huiqi Deng"
                    },
                    {
                        "authorId": "22561596",
                        "name": "Hengyi Cai"
                    },
                    {
                        "authorId": "2237948548",
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "authorId": "2136400100",
                        "name": "Dawei Yin"
                    },
                    {
                        "authorId": "2237804196",
                        "name": "Mengnan Du"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3be71116590c49eabdec0f31e3e56c19bc3c3d35",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-00252",
                    "ArXiv": "2309.00252",
                    "DOI": "10.48550/arXiv.2309.00252",
                    "CorpusId": 261493752
                },
                "corpusId": 261493752,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3be71116590c49eabdec0f31e3e56c19bc3c3d35",
                "title": "Interpretable Medical Imagery Diagnosis with Self-Attentive Transformers: A Review of Explainable AI for Health Care",
                "abstract": "Recent advancements in artificial intelligence (AI) have facilitated its widespread adoption in primary medical services, addressing the demand-supply imbalance in healthcare. Vision Transformers (ViT) have emerged as state-of-the-art computer vision models, benefiting from self-attention modules. However, compared to traditional machine-learning approaches, deep-learning models are complex and are often treated as a\"black box\"that can cause uncertainty regarding how they operate. Explainable Artificial Intelligence (XAI) refers to methods that explain and interpret machine learning models' inner workings and how they come to decisions, which is especially important in the medical domain to guide the healthcare decision-making process. This review summarises recent ViT advancements and interpretative approaches to understanding the decision-making process of ViT, enabling transparency in medical diagnosis applications.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2237422809",
                        "name": "Tin Lai"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "90d8e66a46915ef5d6ac1b5996e8dcd61b2c1a69",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-14575",
                    "ArXiv": "2308.14575",
                    "DOI": "10.48550/arXiv.2308.14575",
                    "CorpusId": 261243179
                },
                "corpusId": 261243179,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/90d8e66a46915ef5d6ac1b5996e8dcd61b2c1a69",
                "title": "Referring Image Segmentation Using Text Supervision",
                "abstract": "Existing Referring Image Segmentation (RIS) methods typically require expensive pixel-level or box-level annotations for supervision. In this paper, we observe that the referring texts used in RIS already provide sufficient information to localize the target object. Hence, we propose a novel weakly-supervised RIS framework to formulate the target localization problem as a classification process to differentiate between positive and negative text expressions. While the referring text expressions for an image are used as positive expressions, the referring text expressions from other images can be used as negative expressions for this image. Our framework has three main novelties. First, we propose a bilateral prompt method to facilitate the classification process, by harmonizing the domain discrepancy between visual and linguistic features. Second, we propose a calibration method to reduce noisy background information and improve the correctness of the response maps for target object localization. Third, we propose a positive response map selection strategy to generate high-quality pseudo-labels from the enhanced response maps, for training a segmentation network for RIS inference. For evaluation, we propose a new metric to measure localization accuracy. Experiments on four benchmarks show that our framework achieves promising performances to existing fully-supervised RIS methods while outperforming state-of-the-art weakly-supervised methods adapted from related areas. Code is available at https://github.com/fawnliu/TRIS.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47185625",
                        "name": "Fang Liu"
                    },
                    {
                        "authorId": "2108173014",
                        "name": "Yuhao Liu"
                    },
                    {
                        "authorId": "3460073",
                        "name": "Yuqiu Kong"
                    },
                    {
                        "authorId": "2117101328",
                        "name": "Ke Xu"
                    },
                    {
                        "authorId": "50081215",
                        "name": "L. Zhang"
                    },
                    {
                        "authorId": "1714354",
                        "name": "Baocai Yin"
                    },
                    {
                        "authorId": "2229808348",
                        "name": "Gerhard P. Hancke"
                    },
                    {
                        "authorId": "1726262",
                        "name": "Rynson W. H. Lau"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "For the intepretation of the attention in ShE, SE, DE, BERT, and its variants we used the tool developed by (Chefer et al., 2021)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "81d0303db4cbd755945669ab9c42938d63b3c987",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-12272",
                    "ArXiv": "2308.12272",
                    "DOI": "10.48550/arXiv.2308.12272",
                    "CorpusId": 261076309
                },
                "corpusId": 261076309,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/81d0303db4cbd755945669ab9c42938d63b3c987",
                "title": "Simple is Better and Large is Not Enough: Towards Ensembling of Foundational Language Models",
                "abstract": "Foundational Language Models (FLMs) have advanced natural language processing (NLP) research. Current researchers are developing larger FLMs (e.g., XLNet, T5) to enable contextualized language representation, classification, and generation. While developing larger FLMs has been of significant advantage, it is also a liability concerning hallucination and predictive uncertainty. Fundamentally, larger FLMs are built on the same foundations as smaller FLMs (e.g., BERT); hence, one must recognize the potential of smaller FLMs which can be realized through an ensemble. In the current research, we perform a reality check on FLMs and their ensemble on benchmark and real-world datasets. We hypothesize that the ensembling of FLMs can influence the individualistic attention of FLMs and unravel the strength of coordination and cooperation of different FLMs. We utilize BERT and define three other ensemble techniques: {Shallow, Semi, and Deep}, wherein the Deep-Ensemble introduces a knowledge-guided reinforcement learning approach. We discovered that the suggested Deep-Ensemble BERT outperforms its large variation i.e. BERTlarge, by a factor of many times using datasets that show the usefulness of NLP in sensitive fields, such as mental health.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2150504227",
                        "name": "Nancy Tyagi"
                    },
                    {
                        "authorId": "151484341",
                        "name": "Aidin Shiri"
                    },
                    {
                        "authorId": "2215467580",
                        "name": "Surjodeep Sarkar"
                    },
                    {
                        "authorId": "1833378844",
                        "name": "A. Umrawal"
                    },
                    {
                        "authorId": "1491238594",
                        "name": "Manas Gaur"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "The current effective methods are Rollout [1], TransAtt [7], GradCAM [30], PRLP [38] and GenAtt [6]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7e3ab72799aaff4933c8107ae77394fe92bcd2ee",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-10240",
                    "ArXiv": "2308.10240",
                    "DOI": "10.48550/arXiv.2308.10240",
                    "CorpusId": 261049493
                },
                "corpusId": 261049493,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7e3ab72799aaff4933c8107ae77394fe92bcd2ee",
                "title": "Generic Attention-model Explainability by Weighted Relevance Accumulation",
                "abstract": "Attention-based transformer models have achieved remarkable progress in multi-modal tasks, such as visual question answering. The explainability of attention-based methods has recently attracted wide interest as it can explain the inner changes of attention tokens by accumulating relevancy across attention layers. Current methods simply update relevancy by equally accumulating the token relevancy before and after the attention processes. However, the importance of token values is usually different during relevance accumulation. In this paper, we propose a weighted relevancy strategy, which takes the importance of token values into consideration, to reduce distortion when equally accumulating relevance. To evaluate our method, we propose a unified CLIP-based two-stage model, named CLIPmapper, to process Vision-and-Language tasks through CLIP encoder and a following mapper. CLIPmapper consists of self-attention, cross-attention, single-modality, and cross-modality attention, thus it is more suitable for evaluating our generic explainability method. Extensive perturbation tests on visual question answering and image captioning validate that our explainability method outperforms existing methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2161249651",
                        "name": "Yiming Huang"
                    },
                    {
                        "authorId": "2175479135",
                        "name": "Ao Jia"
                    },
                    {
                        "authorId": "2108452179",
                        "name": "Xiaodan Zhang"
                    },
                    {
                        "authorId": "2168548350",
                        "name": "Jiawei Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Chefer et al. (Chefer, Gur, and Wolf 2021) allocated local correlations based on deep Taylor decomposition, subsequently propagating these correlations across layers, involving both attention and residual connections."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9537f23dcdd5fcb501a1cb2cc8c5ef644641a4d7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-08333",
                    "ArXiv": "2308.08333",
                    "DOI": "10.48550/arXiv.2308.08333",
                    "CorpusId": 260926413
                },
                "corpusId": 260926413,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9537f23dcdd5fcb501a1cb2cc8c5ef644641a4d7",
                "title": "Improving Depth Gradient Continuity in Transformers: A Comparative Study on Monocular Depth Estimation with CNN",
                "abstract": "Monocular depth estimation is an ongoing challenge in computer vision. Recent progress with Transformer models has demonstrated notable advantages over conventional CNNs in this area. However, there's still a gap in understanding how these models prioritize different regions in 2D images and how these regions affect depth estimation performance. To explore the differences between Transformers and CNNs, we employ a sparse pixel approach to contrastively analyze the distinctions between the two. Our findings suggest that while Transformers excel in handling global context and intricate textures, they lag behind CNNs in preserving depth gradient continuity. To further enhance the performance of Transformer models in monocular depth estimation, we propose the Depth Gradient Refinement (DGR) module that refines depth estimation through high-order differentiation, feature fusion, and recalibration. Additionally, we leverage optimal transport theory, treating depth maps as spatial probability distributions, and employ the optimal transport distance as a loss function to optimize our model. Experimental results demonstrate that models integrated with the plug-and-play Depth Gradient Refinement (DGR) module and the proposed loss function enhance performance without increasing complexity and computational costs. This research not only offers fresh insights into the distinctions between Transformers and CNNs in depth estimation but also paves the way for novel depth estimation methodologies.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2110069712",
                        "name": "Jiawei Yao"
                    },
                    {
                        "authorId": "2231653396",
                        "name": "Tong Wu"
                    },
                    {
                        "authorId": "2144525229",
                        "name": "Xiaofeng Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "result"
            ],
            "contexts": [
                "Class-specific interpretability can be obtained by extensions like Contrastive-LRP (CLRP) and Softmax-Gradient- LRP (SGLRP) where the results of the class to be visualized are contrasted with the results of all other classes, to emphasize the differences and produce a class-dependent heatmap [100].",
                "associations among features and output, lack of propagated relevancy through the attention layers partially addressed through a new layer-propagation strategy, and unstable interpretability with covariate shift [100, 101]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fea19ea351f65115c470bdd6a2446d7e41ee799f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-08407",
                    "ArXiv": "2308.08407",
                    "DOI": "10.48550/arXiv.2308.08407",
                    "CorpusId": 260926590
                },
                "corpusId": 260926590,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fea19ea351f65115c470bdd6a2446d7e41ee799f",
                "title": "Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities",
                "abstract": "Recent advancements in AI applications to healthcare have shown incredible promise in surpassing human performance in diagnosis and disease prognosis. With the increasing complexity of AI models, however, concerns regarding their opacity, potential biases, and the need for interpretability. To ensure trust and reliability in AI systems, especially in clinical risk prediction models, explainability becomes crucial. Explainability is usually referred to as an AI system's ability to provide a robust interpretation of its decision-making logic or the decisions themselves to human stakeholders. In clinical risk prediction, other aspects of explainability like fairness, bias, trust, and transparency also represent important concepts beyond just interpretability. In this review, we address the relationship between these concepts as they are often used together or interchangeably. This review also discusses recent progress in developing explainable models for clinical risk prediction, highlighting the importance of quantitative and clinical evaluation and validation across multiple common modalities in clinical practice. It emphasizes the need for external validation and the combination of diverse interpretability methods to enhance trust and fairness. Adopting rigorous testing, such as using synthetic datasets with known generative factors, can further improve the reliability of explainability methods. Open access and code-sharing resources are essential for transparency and reproducibility, enabling the growth and trustworthiness of explainable research. While challenges exist, an end-to-end approach to explainability in clinical risk prediction, incorporating stakeholders from clinicians to developers, is essential for success.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2031505256",
                        "name": "Munib Mesinovic"
                    },
                    {
                        "authorId": "40041965",
                        "name": "P. Watkinson"
                    },
                    {
                        "authorId": "2163214121",
                        "name": "Ting Zhu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9d8e5e44edb85302e246a38fa354d5d3f97f407f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-06248",
                    "ArXiv": "2308.06248",
                    "DOI": "10.48550/arXiv.2308.06248",
                    "CorpusId": 260866112
                },
                "corpusId": 260866112,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9d8e5e44edb85302e246a38fa354d5d3f97f407f",
                "title": "FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods",
                "abstract": "The field of explainable artificial intelligence (XAI) aims to uncover the inner workings of complex deep neural models. While being crucial for safety-critical domains, XAI inherently lacks ground-truth explanations, making its automatic evaluation an unsolved problem. We address this challenge by proposing a novel synthetic vision dataset, named FunnyBirds, and accompanying automatic evaluation protocols. Our dataset allows performing semantically meaningful image interventions, e.g., removing individual object parts, which has three important implications. First, it enables analyzing explanations on a part level, which is closer to human comprehension than existing methods that evaluate on a pixel level. Second, by comparing the model output for inputs with removed parts, we can estimate ground-truth part importances that should be reflected in the explanations. Third, by mapping individual explanations into a common space of part importances, we can analyze a variety of different explanation types in a single common framework. Using our tools, we report results for 24 different combinations of neural models and XAI methods, demonstrating the strengths and weaknesses of the assessed methods in a fully automatic and systematic manner.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2230789042",
                        "name": "Robin Hesse"
                    },
                    {
                        "authorId": "1412432168",
                        "name": "Simone Schaub-Meyer"
                    },
                    {
                        "authorId": "145920814",
                        "name": "S. Roth"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0aea41e5f5d6abccc462323b6b79fec01ce02ff7",
                "externalIds": {
                    "DBLP": "journals/hcisys/YangWWCHLLYWGAK23",
                    "DOI": "10.1007/s44230-023-00038-y",
                    "CorpusId": 260817864
                },
                "corpusId": 260817864,
                "publicationVenue": {
                    "id": "096a4eed-8279-4e94-abd3-1c0403372743",
                    "name": "Human-Centric Intelligent Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Human-centric Intell Syst"
                    ],
                    "issn": "2667-1336"
                },
                "url": "https://www.semanticscholar.org/paper/0aea41e5f5d6abccc462323b6b79fec01ce02ff7",
                "title": "Survey on Explainable AI: From Approaches, Limitations and Applications Aspects",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144835131",
                        "name": "Wenli Yang"
                    },
                    {
                        "authorId": "2030591854",
                        "name": "Yuchen Wei"
                    },
                    {
                        "authorId": "2109394207",
                        "name": "Hanyu Wei"
                    },
                    {
                        "authorId": "2187427814",
                        "name": "Yanyu Chen"
                    },
                    {
                        "authorId": "1752868605",
                        "name": "Guangtai Huang"
                    },
                    {
                        "authorId": "2229540863",
                        "name": "Xiang Li"
                    },
                    {
                        "authorId": "2143498408",
                        "name": "Renjie Li"
                    },
                    {
                        "authorId": "2051457724",
                        "name": "Naimeng Yao"
                    },
                    {
                        "authorId": "2144708134",
                        "name": "Xinyi Wang"
                    },
                    {
                        "authorId": "48531491",
                        "name": "Xiaodong Gu"
                    },
                    {
                        "authorId": "2229017122",
                        "name": "Muhammad Bilal Amin"
                    },
                    {
                        "authorId": "2229034626",
                        "name": "Byeong Kang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "67c53554ab7dddae58043afc7d8d5e75642ecea1",
                "externalIds": {
                    "PubMedCentral": "10523985",
                    "DOI": "10.1002/cam4.6437",
                    "CorpusId": 260774254,
                    "PubMed": "37559500"
                },
                "corpusId": 260774254,
                "publicationVenue": {
                    "id": "aa254d07-8401-40bf-bc00-2e8203afca3f",
                    "name": "Cancer Medicine",
                    "type": "journal",
                    "alternate_names": [
                        "Cancer Med"
                    ],
                    "issn": "2045-7634",
                    "url": "http://onlinelibrary.wiley.com/journal/10.1002/(ISSN)2045-7634"
                },
                "url": "https://www.semanticscholar.org/paper/67c53554ab7dddae58043afc7d8d5e75642ecea1",
                "title": "Identification of lymph node metastasis in pre\u2010operation cervical cancer patients by weakly supervised deep learning from histopathological whole\u2010slide biopsy images",
                "abstract": "Lymph node metastasis (LNM) significantly impacts the prognosis of individuals diagnosed with cervical cancer, as it is closely linked to disease recurrence and mortality, thereby impacting therapeutic schedule choices for patients. However, accurately predicting LNM prior to treatment remains challenging. Consequently, this study seeks to utilize digital pathological features extracted from histopathological slides of primary cervical cancer patients to preoperatively predict the presence of LNM.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47362536",
                        "name": "Qingqing Liu"
                    },
                    {
                        "authorId": "2075423485",
                        "name": "Nan Jiang"
                    },
                    {
                        "authorId": "9700664",
                        "name": "Yiping Hao"
                    },
                    {
                        "authorId": "49532791",
                        "name": "Chunyan Hao"
                    },
                    {
                        "authorId": "49337119",
                        "name": "W. Wang"
                    },
                    {
                        "authorId": "40161766",
                        "name": "T. Bian"
                    },
                    {
                        "authorId": "2109074947",
                        "name": "Xiaohong Wang"
                    },
                    {
                        "authorId": "7178351",
                        "name": "Huadong Li"
                    },
                    {
                        "authorId": "2256617063",
                        "name": "Yan Zhang"
                    },
                    {
                        "authorId": "2228423568",
                        "name": "Yanjun Kang"
                    },
                    {
                        "authorId": "48572097",
                        "name": "Fengxiang Xie"
                    },
                    {
                        "authorId": "2228970722",
                        "name": "Yawen Li"
                    },
                    {
                        "authorId": "2228251522",
                        "name": "XuJi Jiang"
                    },
                    {
                        "authorId": "2228147783",
                        "name": "Yuan Feng"
                    },
                    {
                        "authorId": "2182563702",
                        "name": "Zhonghao Mao"
                    },
                    {
                        "authorId": "2230124348",
                        "name": "Qi Wang"
                    },
                    {
                        "authorId": "16140495",
                        "name": "Q. Gao"
                    },
                    {
                        "authorId": "2194999920",
                        "name": "Wenjing Zhang"
                    },
                    {
                        "authorId": "144585957",
                        "name": "B. Cui"
                    },
                    {
                        "authorId": "35325785",
                        "name": "Taotao Dong"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Similar variants such as positive / negative perturbations (Chefer et al., 2021a) or using masking in (Hase et al., 2021) have also been used for evaluating the explainability of methods.",
                "\u2026short-term memory (LSTM) architectures (Li et al., 2016; Arras et al., 2017; Ka\u0301da\u0301r et al., 2017) and more complex state-of-the-art transformerstack-based architectures (Guan et al., 2019; Wallace et al., 2019; De Cao et al., 2020; Chefer et al., 2021b; Hase et al., 2021; Feldhus et al., 2021)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "94abb4a364c7572173f01b9657a1f5f11aa13608",
                "externalIds": {
                    "DBLP": "conf/icml/HouC23",
                    "ArXiv": "2308.05219",
                    "DOI": "10.48550/arXiv.2308.05219",
                    "CorpusId": 260775611
                },
                "corpusId": 260775611,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/94abb4a364c7572173f01b9657a1f5f11aa13608",
                "title": "Decoding Layer Saliency in Language Transformers",
                "abstract": "In this paper, we introduce a strategy for identifying textual saliency in large-scale language models applied to classification tasks. In visual networks where saliency is more well-studied, saliency is naturally localized through the convolutional layers of the network; however, the same is not true in modern transformer-stack networks used to process natural language. We adapt gradient-based saliency methods for these networks, propose a method for evaluating the degree of semantic coherence of each layer, and demonstrate consistent improvement over numerous other methods for textual saliency on multiple benchmark classification datasets. Our approach requires no additional training or access to labelled data, and is comparatively very computationally efficient.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152023094",
                        "name": "Elizabeth M. Hou"
                    },
                    {
                        "authorId": "3355010",
                        "name": "Greg Casta\u00f1\u00f3n"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "91b7e05303e9100d6cca9b37625c3855f02948aa",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-04321",
                    "ArXiv": "2308.04321",
                    "DOI": "10.48550/arXiv.2308.04321",
                    "CorpusId": 260704231
                },
                "corpusId": 260704231,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/91b7e05303e9100d6cca9b37625c3855f02948aa",
                "title": "All-pairs Consistency Learning for Weakly Supervised Semantic Segmentation",
                "abstract": "In this work, we propose a new transformer-based regularization to better localize objects for Weakly supervised semantic segmentation (WSSS). In image-level WSSS, Class Activation Map (CAM) is adopted to generate object localization as pseudo segmentation labels. To address the partial activation issue of the CAMs, consistency regularization is employed to maintain activation intensity invariance across various image augmentations. However, such methods ignore pair-wise relations among regions within each CAM, which capture context and should also be invariant across image views. To this end, we propose a new all-pairs consistency regularization (ACR). Given a pair of augmented views, our approach regularizes the activation intensities between a pair of augmented views, while also ensuring that the affinity across regions within each view remains consistent. We adopt vision transformers as the self-attention mechanism naturally embeds pair-wise affinity. This enables us to simply regularize the distance between the attention matrices of augmented image pairs. Additionally, we introduce a novel class-wise localization method that leverages the gradients of the class token. Our method can be seamlessly integrated into existing WSSS methods using transformers without modifying the architectures. We evaluate our method on PASCAL VOC and MS COCO datasets. Our method produces noticeably better class localization maps (67.3% mIoU on PASCAL VOC train), resulting in superior WSSS performances.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "8397429",
                        "name": "Weixuan Sun"
                    },
                    {
                        "authorId": "2135952852",
                        "name": "Yanhao Zhang"
                    },
                    {
                        "authorId": "2171650015",
                        "name": "Zhen Qin"
                    },
                    {
                        "authorId": "46271490",
                        "name": "Zheyuan Liu"
                    },
                    {
                        "authorId": "47768647",
                        "name": "Lin Cheng"
                    },
                    {
                        "authorId": "88985728",
                        "name": "Fanyi Wang"
                    },
                    {
                        "authorId": "2015152",
                        "name": "Yiran Zhong"
                    },
                    {
                        "authorId": "1712576",
                        "name": "N. Barnes"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "53efa7ef5263644c9a70f35d1c6cad346f07336d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-00255",
                    "ArXiv": "2308.00255",
                    "DOI": "10.1145/3581783.3611762",
                    "CorpusId": 260350893
                },
                "corpusId": 260350893,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/53efa7ef5263644c9a70f35d1c6cad346f07336d",
                "title": "LGViT: Dynamic Early Exiting for Accelerating Vision Transformer",
                "abstract": "Recently, the efficient deployment and acceleration of powerful vision transformers (ViTs) on resource-limited edge devices for providing multimedia services have become attractive tasks. Although early exiting is a feasible solution for accelerating inference, most works focus on convolutional neural networks (CNNs) and transformer models in natural language processing (NLP).Moreover, the direct application of early exiting methods to ViTs may result in substantial performance degradation. To tackle this challenge, we systematically investigate the efficacy of early exiting in ViTs and point out that the insufficient feature representations in shallow internal classifiers and the limited ability to capture target semantic information in deep internal classifiers restrict the performance of these methods. We then propose an early exiting framework for general ViTs termed LGViT, which incorporates heterogeneous exiting heads, namely, local perception head and global aggregation head, to achieve an efficiency-accuracy trade-off. In particular, we develop a novel two-stage training scheme, including end-to-end training and self-distillation with the backbone frozen to generate early exiting ViTs, which facilitates the fusion of global and local information extracted by the two types of heads. We conduct extensive experiments using three popular ViT backbones on three vision datasets. Results demonstrate that our LGViT can achieve competitive performance with approximately 1.8 $\\times$ speed-up.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "120330536",
                        "name": "Guanyu Xu"
                    },
                    {
                        "authorId": "2226257596",
                        "name": "Jiawei Hao"
                    },
                    {
                        "authorId": "152148573",
                        "name": "Li Shen"
                    },
                    {
                        "authorId": "1823518756",
                        "name": "Han Hu"
                    },
                    {
                        "authorId": "2150649639",
                        "name": "Yong Luo"
                    },
                    {
                        "authorId": "2117122541",
                        "name": "Hui Lin"
                    },
                    {
                        "authorId": "2244160",
                        "name": "J. Shen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "\u2022 Exploiting the intepretability capabilites of the transformer model to provide explanations of its results [9].",
                "Additionally, the transformer model offers inherent interpretability properties [9], allowing for explaining algorithms to provide insight about how a certain result was achieved."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "547e7cf61ada3a3b92de878c62402af84ef02474",
                "externalIds": {
                    "DBLP": "conf/csr2/CasajusSetienBL23",
                    "DOI": "10.1109/CSR57506.2023.10224965",
                    "CorpusId": 261313329
                },
                "corpusId": 261313329,
                "publicationVenue": {
                    "id": "334fe43b-8712-418a-80a4-064bf088c79e",
                    "name": "Computer Science Symposium in Russia",
                    "type": "conference",
                    "alternate_names": [
                        "CSR",
                        "Corporate Social Responsibility & Sustainable Development",
                        "Corp Soc Responsib  Sustain Dev",
                        "Int Conf Cyber Secur Resil",
                        "Comput Sci Symp Russ",
                        "International Conference on Cyber Security and Resilience"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=610"
                },
                "url": "https://www.semanticscholar.org/paper/547e7cf61ada3a3b92de878c62402af84ef02474",
                "title": "Anomaly-Based Intrusion Detection in IIoT Networks Using Transformer Models",
                "abstract": "With the increase of device connectivity in Industry 4.0, securing industrial networks to defend them against cyberattacks has become a primary concern. Motivated by the huge data generated by devices in industrial environments, artificial intelligence has emerged as a promising complement to traditional cybersecurity. In order to gain insight about the possibility of cyberattacks, we propose a novel methodology to analyze industrial network traffic in real time exploiting the sequence modelling capabilities of the transformer architecture, widely used by the GPT model family for sequential language generation. We demonstrate that our method provides state-of-the art performance with promising explainability potential.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2187815663",
                        "name": "Jorge Casaj\u00fas-Seti\u00e9n"
                    },
                    {
                        "authorId": "1687365",
                        "name": "C. Bielza"
                    },
                    {
                        "authorId": "144999310",
                        "name": "P. Larra\u00f1aga"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "upon previous research in this field [57] and presents a graph Transformer relevancy map."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ba9fa7814d6babd0d26ceab1167a236db599b230",
                "externalIds": {
                    "ArXiv": "2307.15019",
                    "DBLP": "journals/corr/abs-2307-15019",
                    "DOI": "10.48550/arXiv.2307.15019",
                    "CorpusId": 260202745
                },
                "corpusId": 260202745,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ba9fa7814d6babd0d26ceab1167a236db599b230",
                "title": "Self-Supervised Graph Transformer for Deepfake Detection",
                "abstract": "Deepfake detection methods have shown promising results in recognizing forgeries within a given dataset, where training and testing take place on the in-distribution dataset. However, their performance deteriorates significantly when presented with unseen samples. As a result, a reliable deepfake detection system must remain impartial to forgery types, appearance, and quality for guaranteed generalizable detection performance. Despite various attempts to enhance cross-dataset generalization, the problem remains challenging, particularly when testing against common post-processing perturbations, such as video compression or blur. Hence, this study introduces a deepfake detection framework, leveraging a self-supervised pre-training model that delivers exceptional generalization ability, withstanding common corruptions and enabling feature explainability. The framework comprises three key components: a feature extractor based on vision Transformer architecture that is pre-trained via self-supervised contrastive learning methodology, a graph convolution network coupled with a Transformer discriminator, and a graph Transformer relevancy map that provides a better understanding of manipulated regions and further explains the model's decision. To assess the effectiveness of the proposed framework, several challenging experiments are conducted, including in-data distribution performance, cross-dataset, cross-manipulation generalization, and robustness against common post-production perturbations. The results achieved demonstrate the remarkable effectiveness of the proposed deepfake detection framework, surpassing the current state-of-the-art approaches.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "9247763",
                        "name": "Aminollah Khormali"
                    },
                    {
                        "authorId": "2149088287",
                        "name": "Ji Yuan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "methods, which are IG [30], BlurIG [64], GuidedIG [35], I-GOS [31], LIME [46], XRAI [38], GradCAM [6], GradCAM++ [28], Score-CAM [39], Transformer Explainability [65].",
                "Transformer Explainability [65] propose a novel way to compute relevancy for the transformer network."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "79134b6ff9488ef31b723db0f05a940435b47eff",
                "externalIds": {
                    "DBLP": "journals/tip/LiuLZXC23",
                    "DOI": "10.1109/TIP.2023.3297404",
                    "CorpusId": 260163621,
                    "PubMed": "37490377"
                },
                "corpusId": 260163621,
                "publicationVenue": {
                    "id": "e117fa7f-05b7-4dd6-b64b-0a0a7c0393d8",
                    "name": "IEEE Transactions on Image Processing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Image Process"
                    ],
                    "issn": "1057-7149",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=83",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/79134b6ff9488ef31b723db0f05a940435b47eff",
                "title": "Prediction With Visual Evidence: Sketch Classification Explanation via Stroke-Level Attributions",
                "abstract": "Sketch classification models have been extensively investigated by designing a task-driven deep neural network. Despite their successful performances, few works have attempted to explain the prediction of sketch classifiers. To explain the prediction of classifiers, an intuitive way is to visualize the activation maps via computing the gradients. However, visualization based explanations are constrained by several factors when directly applying them to interpret the sketch classifiers: (i) low-semantic visualization regions for human understanding. and (ii) neglecting of the inter-class correlations among distinct categories. To address these issues, we introduce a novel explanation method to interpret the decision of sketch classifiers with stroke-level evidences. Specifically, to achieve stroke-level semantic regions, we first develop a sketch parser that parses the sketch into strokes while preserving their geometric structures. Then, we design a counterfactual map generator to discover the stroke-level principal components for a specific category. Finally, based on the counterfactual feature maps, our model could explain the question of \u201cwhy the sketch is classified as X\u201d by providing positive and negative semantic explanation evidences. Experiments conducted on two public sketch benchmarks, Sketchy-COCO and TU-Berlin, demonstrate the effectiveness of our proposed model. Furthermore, our model could provide more discriminative and human understandable explanations compared with these existing works.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2195060916",
                        "name": "Sixuan Liu"
                    },
                    {
                        "authorId": "2109624802",
                        "name": "Jingzhi Li"
                    },
                    {
                        "authorId": "2108906565",
                        "name": "Hua Zhang"
                    },
                    {
                        "authorId": "12901980",
                        "name": "Long Xu"
                    },
                    {
                        "authorId": "2182161936",
                        "name": "Xiaochun Cao"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Hybrid models can potentially be also used to improve model interpretability [22-25]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "779c1bcb46fbd255dcf03a4aec9ee1ff6a3bfc58",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-12775",
                    "ArXiv": "2307.12775",
                    "DOI": "10.48550/arXiv.2307.12775",
                    "CorpusId": 260126133
                },
                "corpusId": 260126133,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/779c1bcb46fbd255dcf03a4aec9ee1ff6a3bfc58",
                "title": "Is attention all you need in medical image analysis? A review",
                "abstract": "Medical imaging is a key component in clinical diagnosis, treatment planning and clinical trial design, accounting for almost 90% of all healthcare data. CNNs achieved performance gains in medical image analysis (MIA) over the last years. CNNs can efficiently model local pixel interactions and be trained on small-scale MI data. The main disadvantage of typical CNN models is that they ignore global pixel relationships within images, which limits their generalisation ability to understand out-of-distribution data with different 'global' information. The recent progress of Artificial Intelligence gave rise to Transformers, which can learn global relationships from data. However, full Transformer models need to be trained on large-scale data and involve tremendous computational complexity. Attention and Transformer compartments (Transf/Attention) which can well maintain properties for modelling global relationships, have been proposed as lighter alternatives of full Transformers. Recently, there is an increasing trend to co-pollinate complementary local-global properties from CNN and Transf/Attention architectures, which led to a new era of hybrid models. The past years have witnessed substantial growth in hybrid CNN-Transf/Attention models across diverse MIA problems. In this systematic review, we survey existing hybrid CNN-Transf/Attention models, review and unravel key architectural designs, analyse breakthroughs, and evaluate current and future opportunities as well as challenges. We also introduced a comprehensive analysis framework on generalisation opportunities of scientific and clinical impact, based on which new data-driven domain generalisation and adaptation methods can be stimulated.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "6846362",
                        "name": "G. Papanastasiou"
                    },
                    {
                        "authorId": "1832186",
                        "name": "Nikolaos Dikaios"
                    },
                    {
                        "authorId": "2110324580",
                        "name": "Jiahao Huang"
                    },
                    {
                        "authorId": "2109431217",
                        "name": "Chengjia Wang"
                    },
                    {
                        "authorId": "2149521749",
                        "name": "Guang Yang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[15] is adopted to interpret the transformer model and highlight important words.",
                "For the text modality, we employ the interpretability of transformers approach [15]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0e5f991979725299bf5a87e9d5a87cdf9a16cd3d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-12199",
                    "ArXiv": "2307.12199",
                    "DOI": "10.48550/arXiv.2307.12199",
                    "CorpusId": 260125903
                },
                "corpusId": 260125903,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0e5f991979725299bf5a87e9d5a87cdf9a16cd3d",
                "title": "Leveraging Historical Medical Records as a Proxy via Multimodal Modeling and Visualization to Enrich Medical Diagnostic Learning",
                "abstract": "Simulation-based Medical Education (SBME) has been developed as a cost-effective means of enhancing the diagnostic skills of novice physicians and interns, thereby mitigating the need for resource-intensive mentor-apprentice training. However, feedback provided in most SBME is often directed towards improving the operational proficiency of learners, rather than providing summative medical diagnoses that result from experience and time. Additionally, the multimodal nature of medical data during diagnosis poses significant challenges for interns and novice physicians, including the tendency to overlook or over-rely on data from certain modalities, and difficulties in comprehending potential associations between modalities. To address these challenges, we present DiagnosisAssistant, a visual analytics system that leverages historical medical records as a proxy for multimodal modeling and visualization to enhance the learning experience of interns and novice physicians. The system employs elaborately designed visualizations to explore different modality data, offer diagnostic interpretive hints based on the constructed model, and enable comparative analyses of specific patients. Our approach is validated through two case studies and expert interviews, demonstrating its effectiveness in enhancing medical training.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220050109",
                        "name": "Yang Ouyang"
                    },
                    {
                        "authorId": "2224665420",
                        "name": "Yuchen Wu"
                    },
                    {
                        "authorId": "1500698462",
                        "name": "Hengbao Wang"
                    },
                    {
                        "authorId": "2220076649",
                        "name": "Chenyang Zhang"
                    },
                    {
                        "authorId": "79493779",
                        "name": "Furui Cheng"
                    },
                    {
                        "authorId": "145762675",
                        "name": "Chang Jiang"
                    },
                    {
                        "authorId": "153893691",
                        "name": "Lixia Jin"
                    },
                    {
                        "authorId": "7186062",
                        "name": "Yuanwu Cao"
                    },
                    {
                        "authorId": "2108647348",
                        "name": "Qu Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "3) The perturbation test: This test consists of two experiments: Most Relevant First Perturbation (MRFP) and Least Relevant First Perturbation (LRFP) as described in the work by Hila\u2019s method [46]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "40c86b3f758a2021cbd364c5942ee87c3896d25b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-09050",
                    "ArXiv": "2307.09050",
                    "DOI": "10.48550/arXiv.2307.09050",
                    "CorpusId": 259951049
                },
                "corpusId": 259951049,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/40c86b3f758a2021cbd364c5942ee87c3896d25b",
                "title": "R-Cut: Enhancing Explainability in Vision Transformers with Relationship Weighted Out and Cut",
                "abstract": "Transformer-based models have gained popularity in the field of natural language processing (NLP) and are extensively utilized in computer vision tasks and multi-modal models such as GPT4. This paper presents a novel method to enhance the explainability of Transformer-based image classification models. Our method aims to improve trust in classification results and empower users to gain a deeper understanding of the model for downstream tasks by providing visualizations of class-specific maps. We introduce two modules: the ``Relationship Weighted Out\"and the ``Cut\"modules. The ``Relationship Weighted Out\"module focuses on extracting class-specific information from intermediate layers, enabling us to highlight relevant features. Additionally, the ``Cut\"module performs fine-grained feature decomposition, taking into account factors such as position, texture, and color. By integrating these modules, we generate dense class-specific visual explainability maps. We validate our method with extensive qualitative and quantitative experiments on the ImageNet dataset. Furthermore, we conduct a large number of experiments on the LRN dataset, specifically designed for automatic driving danger alerts, to evaluate the explainability of our method in complex backgrounds. The results demonstrate a significant improvement over previous methods. Moreover, we conduct ablation experiments to validate the effectiveness of each module. Through these experiments, we are able to confirm the respective contributions of each module, thus solidifying the overall effectiveness of our proposed approach.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2217228356",
                        "name": "Yingjie Niu"
                    },
                    {
                        "authorId": "145573466",
                        "name": "Ming Ding"
                    },
                    {
                        "authorId": "2142714606",
                        "name": "Maoning Ge"
                    },
                    {
                        "authorId": "2066181657",
                        "name": "Robin Karlsson"
                    },
                    {
                        "authorId": "2108079411",
                        "name": "Yuxiao Zhang"
                    },
                    {
                        "authorId": "153621979",
                        "name": "K. Takeda"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[65] leverage a transformer interpretability method [218] to generate coarse relevance maps for each category, which are then refined by test-time-augmentation"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b78a9007186f8db25e1890757959b5b0513e220c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-09220",
                    "ArXiv": "2307.09220",
                    "DOI": "10.48550/arXiv.2307.09220",
                    "CorpusId": 259950727
                },
                "corpusId": 259950727,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b78a9007186f8db25e1890757959b5b0513e220c",
                "title": "A Survey on Open-Vocabulary Detection and Segmentation: Past, Present, and Future",
                "abstract": "As the most fundamental tasks of computer vision, object detection and segmentation have made tremendous progress in the deep learning era. Due to the expensive manual labeling, the annotated categories in existing datasets are often small-scale and pre-defined, i.e., state-of-the-art detectors and segmentors fail to generalize beyond the closed-vocabulary. To resolve this limitation, the last few years have witnessed increasing attention toward Open-Vocabulary Detection (OVD) and Segmentation (OVS). In this survey, we provide a comprehensive review on the past and recent development of OVD and OVS. To this end, we develop a taxonomy according to the type of task and methodology. We find that the permission and usage of weak supervision signals can well discriminate different methodologies, including: visual-semantic space mapping, novel visual feature synthesis, region-aware training, pseudo-labeling, knowledge distillation-based, and transfer learning-based. The proposed taxonomy is universal across different tasks, covering object detection, semantic/instance/panoptic segmentation, 3D scene and video understanding. In each category, its main principles, key challenges, development routes, strengths, and weaknesses are thoroughly discussed. In addition, we benchmark each task along with the vital components of each method. Finally, several promising directions are provided to stimulate future research.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2118508373",
                        "name": "Chaoyang Zhu"
                    },
                    {
                        "authorId": "2118172180",
                        "name": "Long Chen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Vision transformers [61, 62], on the other hand, have demonstrated superior performance and should be investigated [63] further."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3f324c5862cd44e317a0b4585c24261a67169938",
                "externalIds": {
                    "ArXiv": "2307.08003",
                    "DBLP": "journals/corr/abs-2307-08003",
                    "DOI": "10.48550/arXiv.2307.08003",
                    "CorpusId": 259937193
                },
                "corpusId": 259937193,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3f324c5862cd44e317a0b4585c24261a67169938",
                "title": "SHAMSUL: Simultaneous Heatmap-Analysis to investigate Medical Significance Utilizing Local interpretability methods",
                "abstract": "The interpretability of deep neural networks has become a subject of great interest within the medical and healthcare domain. This attention stems from concerns regarding transparency, legal and ethical considerations, and the medical significance of predictions generated by these deep neural networks in clinical decision support systems. To address this matter, our study delves into the application of four well-established interpretability methods: Local Interpretable Model-agnostic Explanations (LIME), Shapley Additive exPlanations (SHAP), Gradient-weighted Class Activation Mapping (Grad-CAM), and Layer-wise Relevance Propagation (LRP). Leveraging the approach of transfer learning with a multi-label-multi-class chest radiography dataset, we aim to interpret predictions pertaining to specific pathology classes. Our analysis encompasses both single-label and multi-label predictions, providing a comprehensive and unbiased assessment through quantitative and qualitative investigations, which are compared against human expert annotation. Notably, Grad-CAM demonstrates the most favorable performance in quantitative evaluation, while the LIME heatmap segmentation visualization exhibits the highest level of medical significance. Our research highlights the strengths and limitations of these interpretability methods and suggests that a multimodal-based approach, incorporating diverse sources of information beyond chest radiography images, could offer additional insights for enhancing interpretability in the medical domain.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "153494049",
                        "name": "Mahbub Ul Alam"
                    },
                    {
                        "authorId": "2223594236",
                        "name": "Jaakko Hollm'en"
                    },
                    {
                        "authorId": "2199959517",
                        "name": "J\u00f3n R. Baldvinsson"
                    },
                    {
                        "authorId": "2627161",
                        "name": "R. Rahmani"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Recent work has introduced the assignment of a local relevancy score [21]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "99b0c3a18050889c591e1db6d51ca01298638437",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-05979",
                    "ArXiv": "2307.05979",
                    "DOI": "10.48550/arXiv.2307.05979",
                    "CorpusId": 259837280
                },
                "corpusId": 259837280,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/99b0c3a18050889c591e1db6d51ca01298638437",
                "title": "Transformers in Reinforcement Learning: A Survey",
                "abstract": "Transformers have significantly impacted domains like natural language processing, computer vision, and robotics, where they improve performance compared to other neural networks. This survey explores how transformers are used in reinforcement learning (RL), where they are seen as a promising solution for addressing challenges such as unstable training, credit assignment, lack of interpretability, and partial observability. We begin by providing a brief domain overview of RL, followed by a discussion on the challenges of classical RL algorithms. Next, we delve into the properties of the transformer and its variants and discuss the characteristics that make them well-suited to address the challenges inherent in RL. We examine the application of transformers to various aspects of RL, including representation learning, transition and reward function modeling, and policy optimization. We also discuss recent research that aims to enhance the interpretability and efficiency of transformers in RL, using visualization techniques and efficient training strategies. Often, the transformer architecture must be tailored to the specific needs of a given application. We present a broad overview of how transformers have been adapted for several applications, including robotics, medicine, language modeling, cloud computing, and combinatorial optimization. We conclude by discussing the limitations of using transformers in RL and assess their potential for catalyzing future breakthroughs in this field.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1491233100",
                        "name": "Pranav Agarwal"
                    },
                    {
                        "authorId": "40525193",
                        "name": "A. Rahman"
                    },
                    {
                        "authorId": "1399252442",
                        "name": "P. St-Charles"
                    },
                    {
                        "authorId": "2080106096",
                        "name": "Simon J. D. Prince"
                    },
                    {
                        "authorId": "3127597",
                        "name": "S. Kahou"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                ", by analyzing gradients [4, 47, 53] and attentions [11, 31, 59]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2fd631d788cdca483e162878a87104dacac05975",
                "externalIds": {
                    "DBLP": "conf/issta/Xu0YSS0TS0023",
                    "DOI": "10.1145/3597926.3598121",
                    "CorpusId": 259844843
                },
                "corpusId": 259844843,
                "publicationVenue": {
                    "id": "289bfdda-eab3-4c9a-97be-ef1e0f9ddfc0",
                    "name": "International Symposium on Software Testing and Analysis",
                    "type": "conference",
                    "alternate_names": [
                        "ISSTA",
                        "Int Symp Softw Test Anal"
                    ],
                    "url": "https://dl.acm.org/conference/issta"
                },
                "url": "https://www.semanticscholar.org/paper/2fd631d788cdca483e162878a87104dacac05975",
                "title": "Improving Binary Code Similarity Transformer Models by Semantics-Driven Instruction Deemphasis",
                "abstract": "Given a function in the binary executable form, binary code similarity analysis determines a set of similar functions from a large pool of candidate functions. These similar functions are usually compiled from the same source code with different compilation setups. Such analysis has a large number of applications, such as malware detection, code clone detection, and automatic software patching. The state-of-the art methods utilize complex Deep Learning models such as Transformer models. We observe that these models suffer from undesirable instruction distribution biases caused by specific compiler conventions. We develop a novel technique to detect such biases and repair them by removing the corresponding instructions from the dataset and finetuning the models. This entails synergy between Deep Learning model analysis and program analysis. Our results show that we can substantially improve the state-of-the-art models\u2019 performance by up to 14.4% in the most challenging cases where test data may be out of the distributions of training data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2028614325",
                        "name": "Xiangzhe Xu"
                    },
                    {
                        "authorId": "2188773882",
                        "name": "Shiwei Feng"
                    },
                    {
                        "authorId": "2107568083",
                        "name": "Yapeng Ye"
                    },
                    {
                        "authorId": "2052467415",
                        "name": "Guangyu Shen"
                    },
                    {
                        "authorId": "66513455",
                        "name": "Zian Su"
                    },
                    {
                        "authorId": "46378881",
                        "name": "Siyuan Cheng"
                    },
                    {
                        "authorId": "48927894",
                        "name": "Guanhong Tao"
                    },
                    {
                        "authorId": "2226677",
                        "name": "Qingkai Shi"
                    },
                    {
                        "authorId": "2220690905",
                        "name": "Zhuo Zhang"
                    },
                    {
                        "authorId": "2156004395",
                        "name": "Xiangyu Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "[39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 74, 75, 76, 77] I think the formatting may have gotten screwed up (or Gerrit made it look ugly) [ ] below assignments also should be removed",
                "To improve explainability using attention-based mechanisms, recent works have proposed transformer-based sequence-to-sequence models [47], [48]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a33977ec0d652bc0b319af6618d4ffbc99d0e91d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-03386",
                    "ArXiv": "2307.03386",
                    "DOI": "10.48550/arXiv.2307.03386",
                    "CorpusId": 259375549
                },
                "corpusId": 259375549,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a33977ec0d652bc0b319af6618d4ffbc99d0e91d",
                "title": "ToxiSpanSE: An Explainable Toxicity Detection in Code Review Comments",
                "abstract": "Background: The existence of toxic conversations in open-source platforms can degrade relationships among software developers and may negatively impact software product quality. To help mitigate this, some initial work has been done to detect toxic comments in the Software Engineering (SE) domain. Aims: Since automatically classifying an entire text as toxic or non-toxic does not help human moderators to understand the specific reason(s) for toxicity, we worked to develop an explainable toxicity detector for the SE domain. Method: Our explainable toxicity detector can detect specific spans of toxic content from SE texts, which can help human moderators by automatically highlighting those spans. This toxic span detection model, ToxiSpanSE, is trained with the 19,651 code review (CR) comments with labeled toxic spans. Our annotators labeled the toxic spans within 3,757 toxic CR samples. We explored several types of models, including one lexicon-based approach and five different transformer-based encoders. Results: After an extensive evaluation of all models, we found that our fine-tuned RoBERTa model achieved the best score with 0.88 $F1$, 0.87 precision, and 0.93 recall for toxic class tokens, providing an explainable toxicity classifier for the SE domain. Conclusion: Since ToxiSpanSE is the first tool to detect toxic spans in the SE domain, this tool will pave a path to combat toxicity in the SE community.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1471434369",
                        "name": "Jaydeb Sarker"
                    },
                    {
                        "authorId": "2117276459",
                        "name": "Sayma Sultana"
                    },
                    {
                        "authorId": "48408604",
                        "name": "Steven R. Wilson"
                    },
                    {
                        "authorId": "2517154",
                        "name": "Amiangshu Bosu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Many studies focus on visualizing the attention maps to interpret the principle of the network, especially for the transformer modules [1,6]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1b7dd80616ace40b49cf83f8f66a48df6e4dc90f",
                "externalIds": {
                    "ArXiv": "2307.00885",
                    "DBLP": "conf/miccai/HanZHDWGLTM23",
                    "DOI": "10.48550/arXiv.2307.00885",
                    "CorpusId": 259316599
                },
                "corpusId": 259316599,
                "publicationVenue": {
                    "id": "61a709e3-2060-423c-8de5-ffd3885aa31c",
                    "name": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                    "type": "conference",
                    "alternate_names": [
                        "Medical Image Computing and Computer-Assisted Intervention",
                        "MICCAI",
                        "Med Image Comput Comput Interv",
                        "Int Conf Med Image Comput Comput Interv"
                    ],
                    "url": "http://www.miccai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/1b7dd80616ace40b49cf83f8f66a48df6e4dc90f",
                "title": "An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis",
                "abstract": "Multi-sequence MRI is valuable in clinical settings for reliable diagnosis and treatment prognosis, but some sequences may be unusable or missing for various reasons. To address this issue, MRI synthesis is a potential solution. Recent deep learning-based methods have achieved good performance in combining multiple available sequences for missing sequence synthesis. Despite their success, these methods lack the ability to quantify the contributions of different input sequences and estimate the quality of generated images, making it hard to be practical. Hence, we propose an explainable task-specific synthesis network, which adapts weights automatically for specific sequence generation tasks and provides interpretability and reliability from two sides: (1) visualize the contribution of each input sequence in the fusion stage by a trainable task-specific weighted average module; (2) highlight the area the network tried to refine during synthesizing by a task-specific attention module. We conduct experiments on the BraTS2021 dataset of 1251 subjects, and results on arbitrary sequence synthesis indicate that the proposed method achieves better performance than the state-of-the-art methods. Our code is available at \\url{https://github.com/fiy2W/mri_seq2seq}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2112708227",
                        "name": "Luyi Han"
                    },
                    {
                        "authorId": "2146331891",
                        "name": "Tianyu Zhang"
                    },
                    {
                        "authorId": "2108831879",
                        "name": "Yunzhi Huang"
                    },
                    {
                        "authorId": "10669778",
                        "name": "Haoran Dou"
                    },
                    {
                        "authorId": "153316152",
                        "name": "Xin Wang"
                    },
                    {
                        "authorId": "48146495",
                        "name": "Yuan Gao"
                    },
                    {
                        "authorId": "2828701",
                        "name": "Chun-Ta Lu"
                    },
                    {
                        "authorId": "2221011241",
                        "name": "Tan Tao"
                    },
                    {
                        "authorId": "1794036",
                        "name": "R. Mann"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Unlike TextShield, which relies on computationally intensive saliency factors [9] and lacks the utilization of attention cues inherent in transformer architecture, ITDT is specifically designed to tackle the challenges posed by large and complex transformer-based models."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "56530e6761adaffc3f607356d39ce7252b872f2e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-01225",
                    "ArXiv": "2307.01225",
                    "DOI": "10.48550/arXiv.2307.01225",
                    "CorpusId": 259342638
                },
                "corpusId": 259342638,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/56530e6761adaffc3f607356d39ce7252b872f2e",
                "title": "Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT)",
                "abstract": "Transformer-based text classifiers like BERT, Roberta, T5, and GPT-3 have shown impressive performance in NLP. However, their vulnerability to adversarial examples poses a security risk. Existing defense methods lack interpretability, making it hard to understand adversarial classifications and identify model vulnerabilities. To address this, we propose the Interpretability and Transparency-Driven Detection and Transformation (IT-DT) framework. It focuses on interpretability and transparency in detecting and transforming textual adversarial examples. IT-DT utilizes techniques like attention maps, integrated gradients, and model feedback for interpretability during detection. This helps identify salient features and perturbed words contributing to adversarial classifications. In the transformation phase, IT-DT uses pre-trained embeddings and model feedback to generate optimal replacements for perturbed words. By finding suitable substitutions, we aim to convert adversarial examples into non-adversarial counterparts that align with the model's intended behavior while preserving the text's meaning. Transparency is emphasized through human expert involvement. Experts review and provide feedback on detection and transformation results, enhancing decision-making, especially in complex scenarios. The framework generates insights and threat intelligence empowering analysts to identify vulnerabilities and improve model robustness. Comprehensive experiments demonstrate the effectiveness of IT-DT in detecting and transforming adversarial examples. The approach enhances interpretability, provides transparency, and enables accurate identification and successful transformation of adversarial inputs. By combining technical analysis and human expertise, IT-DT significantly improves the resilience and trustworthiness of transformer-based text classifiers against adversarial attacks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "25681391",
                        "name": "Bushra Sabir"
                    },
                    {
                        "authorId": "2142773270",
                        "name": "M. A. Babar"
                    },
                    {
                        "authorId": "1402904203",
                        "name": "Sharif Abuadbba"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7e561bc6690b51f5da6eedf89b4b68d815e213ee",
                "externalIds": {
                    "DOI": "10.1016/j.media.2023.102894",
                    "CorpusId": 259881920,
                    "PubMed": "37562256"
                },
                "corpusId": 259881920,
                "publicationVenue": {
                    "id": "0e5a2999-db05-4ab3-83d8-b480d49b90be",
                    "name": "Medical Image Analysis",
                    "type": "journal",
                    "alternate_names": [
                        "Med Image Anal"
                    ],
                    "issn": "1361-8415",
                    "url": "https://www.journals.elsevier.com/medical-image-analysis",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/13618415"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7e561bc6690b51f5da6eedf89b4b68d815e213ee",
                "title": "MuSiC-ViT: A multi-task Siamese convolutional vision transformer for differentiating change from no-change in follow-up chest radiographs.",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2111051486",
                        "name": "Kyungjin Cho"
                    },
                    {
                        "authorId": "2116321144",
                        "name": "Jeeyoung Kim"
                    },
                    {
                        "authorId": "2156002978",
                        "name": "Ki Duk Kim"
                    },
                    {
                        "authorId": "2179712652",
                        "name": "Seungju Park"
                    },
                    {
                        "authorId": "3053231",
                        "name": "Junsik Kim"
                    },
                    {
                        "authorId": "2069434050",
                        "name": "J. Yun"
                    },
                    {
                        "authorId": "1491121675",
                        "name": "Yura Ahn"
                    },
                    {
                        "authorId": "3804463",
                        "name": "S. Oh"
                    },
                    {
                        "authorId": "40398599",
                        "name": "Sang Min Lee"
                    },
                    {
                        "authorId": "46844846",
                        "name": "J. Seo"
                    },
                    {
                        "authorId": "145979410",
                        "name": "Namkug Kim"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In order to verify the effectiveness of our proposed DynaSlim, we leverage a local relevancebased calculating method [26] to visualize the significant parts of the image that lead to a certain classification.",
                "Attention map visualization on ImageNet-1K with [26]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a2bff2d05f05265fc79b850d14e455744d88ce2c",
                "externalIds": {
                    "DBLP": "conf/icmcs/ShiGLF23",
                    "DOI": "10.1109/ICME55011.2023.00251",
                    "CorpusId": 261126725
                },
                "corpusId": 261126725,
                "publicationVenue": {
                    "id": "975e1529-a14b-436f-ad7e-a09d8eabc50d",
                    "name": "IEEE International Conference on Multimedia and Expo",
                    "type": "conference",
                    "alternate_names": [
                        "ICME",
                        "Int Conf Multimedia Expo",
                        "IEEE Int Conf Multimedia Expo",
                        "International Conference on Multimedia and Expo"
                    ],
                    "issn": "1945-7871",
                    "alternate_issns": [
                        "2330-7927"
                    ],
                    "url": "http://csdl2.computer.org/persagen/ProceedingByFilter.jsp?filter=I",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conferences.jsp",
                        "http://www.wikicfp.com/cfp/program?id=1418"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a2bff2d05f05265fc79b850d14e455744d88ce2c",
                "title": "DynaSlim: Dynamic Slimming for Vision Transformers",
                "abstract": "Vision transformers (ViTs) have achieved significant performance on various vision tasks. However, high computational and memory costs hinder their edge deployment. Existing compression methods employ static constraints between accuracy and efficiency during sparsification. The static constraints restrict the sparsification efficiency and their initialization relies heavily on human expertise. We propose a dynamic slimming strategy for ViT, DynaSlim, to achieve an adaptive accuracy-efficiency constraint during sparsification. We first equip fine-grained, adjustable sparsity weights, the scaling factor between accuracy and efficiency, for multiple dimensions, including input tokens, Multihead Self-Attention (MSA) and Multilayer Perceptron (MLP). We then employ the heuristic search for these non-differentiable factors and combine the search with regularization-based sparsification to obtain the optimal sparsed model. Finally, we compress and retrain the sparsed model under various budgets to get our resulting submodels. Experiments show that our DynaSlim outperforms previous state-of-the-art methods under different budgets. For example, we reduce both parameters and FLOPs of DeiT-B by 39% while increasing its accuracy by 1.9% on ImageNet-1K. Moreover, we demonstrate the transferability of our compressed models on several downstream datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2233500307",
                        "name": "Da Shi"
                    },
                    {
                        "authorId": "2115557074",
                        "name": "Jingsheng Gao"
                    },
                    {
                        "authorId": "145840791",
                        "name": "Ting Liu"
                    },
                    {
                        "authorId": "7727059",
                        "name": "Yuzhuo Fu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[461] show that Transformer attention is often fragmented and does not provide a robust explanation."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b44865672b3896e249b81a39cbe850286f8140c0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-00067",
                    "ArXiv": "2307.00067",
                    "DOI": "10.48550/arXiv.2307.00067",
                    "CorpusId": 259316437
                },
                "corpusId": 259316437,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b44865672b3896e249b81a39cbe850286f8140c0",
                "title": "Transformers in Healthcare: A Survey",
                "abstract": "With Artificial Intelligence (AI) increasingly permeating various aspects of society, including healthcare, the adoption of the Transformers neural network architecture is rapidly changing many applications. Transformer is a type of deep learning architecture initially developed to solve general-purpose Natural Language Processing (NLP) tasks and has subsequently been adapted in many fields, including healthcare. In this survey paper, we provide an overview of how this architecture has been adopted to analyze various forms of data, including medical imaging, structured and unstructured Electronic Health Records (EHR), social media, physiological signals, and biomolecular sequences. Those models could help in clinical diagnosis, report generation, data reconstruction, and drug/protein synthesis. We identified relevant studies using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. We also discuss the benefits and limitations of using transformers in healthcare and examine issues such as computational cost, model interpretability, fairness, alignment with human values, ethical implications, and environmental impact.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1404347990",
                        "name": "Subhash Nerella"
                    },
                    {
                        "authorId": "4872398",
                        "name": "S. Bandyopadhyay"
                    },
                    {
                        "authorId": "1600426974",
                        "name": "Jiaqing Zhang"
                    },
                    {
                        "authorId": "2145513131",
                        "name": "Miguel Contreras"
                    },
                    {
                        "authorId": "119142338",
                        "name": "Scott Siegel"
                    },
                    {
                        "authorId": "1394621662",
                        "name": "Ayseg\u00fcl Bumin"
                    },
                    {
                        "authorId": "2065419041",
                        "name": "B. Silva"
                    },
                    {
                        "authorId": "90988618",
                        "name": "Jessica Sena"
                    },
                    {
                        "authorId": "3383528",
                        "name": "B. Shickel"
                    },
                    {
                        "authorId": "5484714",
                        "name": "A. Bihorac"
                    },
                    {
                        "authorId": "1973923",
                        "name": "K. Khezeli"
                    },
                    {
                        "authorId": "2067149174",
                        "name": "P. Rashidi"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "We report the additional visual explanations and evaluations on various methods and models, including GradCAM, FullGrad, GradCAM++, WGradCAM, RAP, RSP, AGF, SGLRP, and transformer interpretability (Chefer, Gur, and Wolf 2021)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ab28e10316c060589a2d4b02500b537e02900526",
                "externalIds": {
                    "DBLP": "conf/aaai/HongNJL23",
                    "DOI": "10.1609/aaai.v37i7.25954",
                    "CorpusId": 259672927
                },
                "corpusId": 259672927,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/ab28e10316c060589a2d4b02500b537e02900526",
                "title": "Towards Better Visualizing the Decision Basis of Networks via Unfold and Conquer Attribution Guidance",
                "abstract": "Revealing the transparency of Deep Neural Networks (DNNs) has been widely studied to describe the decision mechanisms of network inner structures. In this paper, we propose a novel post-hoc framework, Unfold and Conquer Attribution Guidance (UCAG), which enhances the explainability of the network decision by spatially scrutinizing the input features with respect to the model confidence. Addressing the phenomenon of missing detailed descriptions, UCAG sequentially complies with the confidence of slices of the image, leading to providing an abundant and clear interpretation. Therefore, it is possible to enhance the representation ability of explanation by preserving the detailed descriptions of assistant input features, which are commonly overwhelmed by the main meaningful regions. We conduct numerous evaluations to validate the performance in several metrics: i) deletion and insertion, ii) (energy-based) pointing games, and iii) positive and negative density maps. Experimental results, including qualitative comparisons, demonstrate that our method outperforms the existing methods with the nature of clear and detailed explanations and applicability.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2155429791",
                        "name": "Jung-Ho Hong"
                    },
                    {
                        "authorId": "148354920",
                        "name": "Woo-Jeoung Nam"
                    },
                    {
                        "authorId": "2222489487",
                        "name": "Kyu-Sung Jeon"
                    },
                    {
                        "authorId": "2164853907",
                        "name": "Seong-Whan Lee"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "An architecture that will be explored is transformers [9], which in this context can be employed for the unsupervised matching between symbolic annotations and audio features."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "17854c23bc7feb17a595a503030a4f54e6ac7449",
                "externalIds": {
                    "ArXiv": "2306.12249",
                    "DBLP": "journals/corr/abs-2306-12249",
                    "DOI": "10.48550/arXiv.2306.12249",
                    "CorpusId": 259211962
                },
                "corpusId": 259211962,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/17854c23bc7feb17a595a503030a4f54e6ac7449",
                "title": "Knowledge-based Multimodal Music Similarity",
                "abstract": "Music similarity is an essential aspect of music retrieval, recommendation systems, and music analysis. Moreover, similarity is of vital interest for music experts, as it allows studying analogies and influences among composers and historical periods. Current approaches to musical similarity rely mainly on symbolic content, which can be expensive to produce and is not always readily available. Conversely, approaches using audio signals typically fail to provide any insight about the reasons behind the observed similarity. This research addresses the limitations of current approaches by focusing on the study of musical similarity using both symbolic and audio content. The aim of this research is to develop a fully explainable and interpretable system that can provide end-users with more control and understanding of music similarity and classification systems.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2141309560",
                        "name": "Andrea Poltronieri"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[33] proposed a visualization method for Transformer networks that integrates attention and correlation scores into multiple attention modules and includes normalization terms for non-parametric layers."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6c3a6d1671de91af197411fb958934b0fde94bf1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-12098",
                    "ArXiv": "2306.12098",
                    "DOI": "10.48550/arXiv.2306.12098",
                    "CorpusId": 259212386
                },
                "corpusId": 259212386,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6c3a6d1671de91af197411fb958934b0fde94bf1",
                "title": "MSW-Transformer: Multi-Scale Shifted Windows Transformer Networks for 12-Lead ECG Classification",
                "abstract": "Automatic classification of electrocardiogram (ECG) signals plays a crucial role in the early prevention and diagnosis of cardiovascular diseases. While ECG signals can be used for the diagnosis of various diseases, their pathological characteristics exhibit minimal variations, posing a challenge to automatic classification models. Existing methods primarily utilize convolutional neural networks to extract ECG signal features for classification, which may not fully capture the pathological feature differences of different diseases. Transformer networks have advantages in feature extraction for sequence data, but the complete network is complex and relies on large-scale datasets. To address these challenges, we propose a single-layer Transformer network called Multi-Scale Shifted Windows Transformer Networks (MSW-Transformer), which uses a multi-window sliding attention mechanism at different scales to capture features in different dimensions. The self-attention is restricted to non-overlapping local windows via shifted windows, and different window scales have different receptive fields. A learnable feature fusion method is then proposed to integrate features from different windows to further enhance model performance. Furthermore, we visualize the attention mechanism of the multi-window shifted mechanism to achieve better clinical interpretation in the ECG classification task. The proposed model achieves state-of-the-art performance on five classification tasks of the PTBXL-2020 12-lead ECG dataset, which includes 5 diagnostic superclasses, 23 diagnostic subclasses, 12 rhythm classes, 17 morphology classes, and 44 diagnosis classes, with average macro-F1 scores of 77.85%, 47.57%, 66.13%, 34.60%, and 34.29%, and average sample-F1 scores of 81.26%, 68.27%, 91.32%, 50.07%, and 63.19%, respectively.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2073540094",
                        "name": "Ren-Wei Cheng"
                    },
                    {
                        "authorId": "2292094",
                        "name": "Zhemin Zhuang"
                    },
                    {
                        "authorId": "2082327540",
                        "name": "Shuxin Zhuang"
                    },
                    {
                        "authorId": "46330520",
                        "name": "Linfu Xie"
                    },
                    {
                        "authorId": "2157960178",
                        "name": "Jingfeng Guo"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In [23], the authors employed LRP-based relevance to compute scores for each attention head in layers of a transformer which obtains state-of-the-art results, although, this method is not applicable to CNNs."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0de8b73c171fbc5e7b9aa1710cb3b8884e3bba1f",
                "externalIds": {
                    "DBLP": "conf/qomex/YangMSD23",
                    "DOI": "10.1109/QoMEX58391.2023.10178510",
                    "CorpusId": 259979064
                },
                "corpusId": 259979064,
                "publicationVenue": {
                    "id": "d745007d-d2e8-4a40-98a8-25e83ec05027",
                    "name": "International Workshop on Quality of Multimedia Experience",
                    "type": "conference",
                    "alternate_names": [
                        "Qual Multimedia Exp",
                        "Int Workshop Qual Multimedia Exp",
                        "QoMEX",
                        "Quality of Multimedia Experience"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0de8b73c171fbc5e7b9aa1710cb3b8884e3bba1f",
                "title": "Evaluating Quality of Visual Explanations of Deep Learning Models for Vision Tasks",
                "abstract": "Explainable artificial intelligence (XAI) has gained considerable attention in recent years as it aims to help humans better understand machine learning decisions, making complex black-box systems more trustworthy. Visual explanation algorithms have been designed to generate heatmaps highlighting image regions that a deep neural network focuses on to make decisions. While convolutional neural network (CNN) models typically follow similar processing operations for feature encoding, the emergence of vision transformer (ViT) has introduced a new approach to machine vision decision-making. Therefore, an important question is which architecture provides more human-understandable explanations. This paper examines the explain-ability of deep architectures, including CNN and ViT models under different vision tasks. To this end, we first performed a subjective experiment asking humans to highlight the key visual features in images that helped them to make decisions in two different vision tasks. Next, using the human-annotated images, ground-truth heatmaps were generated that were compared against heatmaps generated by explanation methods for the deep architectures. Moreover, perturbation tests were performed for objective evaluation of the deep models' explanation heatmaps. According to the results, the explanations generated from ViT are deemed more trustworthy than those produced by other CNNs, and as the features of the input image are more dispersed, the advantage of the model becomes more evident.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2194261693",
                        "name": "Yuqing Yang"
                    },
                    {
                        "authorId": "2105445",
                        "name": "Saeed Mahmoudpour"
                    },
                    {
                        "authorId": "1808586",
                        "name": "P. Schelkens"
                    },
                    {
                        "authorId": "2003112059",
                        "name": "Nikos Deligiannis"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "However, few studies have actually delved into what it is that the models actually learn (Raghu et al., 2021; Nguyen et al., 2021; Chefer et al., 2021) and even fewer have examined point clouds in particular (Zhang et al., 2019; Tayyub et al., 2022).",
                "Other works (Tayyub et al., 2022; Chefer et al., 2021) utilize gradient-based methods in order to visualize the receptive fields or the relevancy of input patches towards the model\u2019s decision.",
                "However, few studies have actually delved into what it is that the models actually learn (Raghu et al., 2021; Nguyen et al., 2021; Chefer et al., 2021) and even fewer have examined point clouds in particular (Zhang et al."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2c89260e9fa5d8f39de29a9c6981b0ec2fd474f3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-10798",
                    "ArXiv": "2306.10798",
                    "DOI": "10.48550/arXiv.2306.10798",
                    "CorpusId": 259244067
                },
                "corpusId": 259244067,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2c89260e9fa5d8f39de29a9c6981b0ec2fd474f3",
                "title": "ExpPoint-MAE: Better interpretability and performance for self-supervised point cloud transformers",
                "abstract": "In this paper we delve into the properties of transformers, attained through self-supervision, in the point cloud domain. Specifically, we evaluate the effectiveness of Masked Autoencoding as a pretraining scheme, and explore Momentum Contrast as an alternative. In our study we investigate the impact of data quantity on the learned features, and uncover similarities in the transformer's behavior across domains. Through comprehensive visualiations, we observe that the transformer learns to attend to semantically meaningful regions, indicating that pretraining leads to a better understanding of the underlying geometry. Moreover, we examine the finetuning process and its effect on the learned representations. Based on that, we devise an unfreezing strategy which consistently outperforms our baseline without introducing any other modifications to the model or the training pipeline, and achieve state-of-the-art results in the classification task among transformer models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1933552375",
                        "name": "Ioannis Romanelis"
                    },
                    {
                        "authorId": "2003351947",
                        "name": "Vlassis Fotis"
                    },
                    {
                        "authorId": "46974120",
                        "name": "K. Moustakas"
                    },
                    {
                        "authorId": "145073907",
                        "name": "A. Munteanu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "de374dc9bb0b443ef399fc36587aa1e192447466",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-09344",
                    "ArXiv": "2306.09344",
                    "DOI": "10.48550/arXiv.2306.09344",
                    "CorpusId": 259171761
                },
                "corpusId": 259171761,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/de374dc9bb0b443ef399fc36587aa1e192447466",
                "title": "DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data",
                "abstract": "Current perceptual similarity metrics operate at the level of pixels and patches. These metrics compare images in terms of their low-level colors and textures, but fail to capture mid-level similarities and differences in image layout, object pose, and semantic content. In this paper, we develop a perceptual metric that assesses images holistically. Our first step is to collect a new dataset of human similarity judgments over image pairs that are alike in diverse ways. Critical to this dataset is that judgments are nearly automatic and shared by all observers. To achieve this we use recent text-to-image models to create synthetic pairs that are perturbed along various dimensions. We observe that popular perceptual metrics fall short of explaining our new data, and we introduce a new metric, DreamSim, tuned to better align with human perception. We analyze how our metric is affected by different visual attributes, and find that it focuses heavily on foreground objects and semantic content while also being sensitive to color and layout. Notably, despite being trained on synthetic data, our metric generalizes to real images, giving strong results on retrieval and reconstruction tasks. Furthermore, our metric outperforms both prior learned metrics and recent large vision models on these tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2072785288",
                        "name": "Stephanie Fu"
                    },
                    {
                        "authorId": "48842501",
                        "name": "Netanel Tamir"
                    },
                    {
                        "authorId": "2061848423",
                        "name": "Shobhita Sundaram"
                    },
                    {
                        "authorId": "51322829",
                        "name": "Lucy Chai"
                    },
                    {
                        "authorId": "2109976035",
                        "name": "Richard Zhang"
                    },
                    {
                        "authorId": "2112779",
                        "name": "Tali Dekel"
                    },
                    {
                        "authorId": "2094770",
                        "name": "Phillip Isola"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                ", heatmaps [5, 6] and input masks [7, 8]), instead of providing a mechanistic understanding of its inner-workings."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f3e43172a84acc4ea1b05c8849c6853e0dceaa2c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-07809",
                    "ArXiv": "2306.07809",
                    "DOI": "10.48550/arXiv.2306.07809",
                    "CorpusId": 259144962
                },
                "corpusId": 259144962,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f3e43172a84acc4ea1b05c8849c6853e0dceaa2c",
                "title": "Low-Resource White-Box Semantic Segmentation of Supporting Towers on 3D Point Clouds via Signature Shape Identification",
                "abstract": "Research in 3D semantic segmentation has been increasing performance metrics, like the IoU, by scaling model complexity and computational resources, leaving behind researchers and practitioners that (1) cannot access the necessary resources and (2) do need transparency on the model decision mechanisms. In this paper, we propose SCENE-Net, a low-resource white-box model for 3D point cloud semantic segmentation. SCENE-Net identifies signature shapes on the point cloud via group equivariant non-expansive operators (GENEOs), providing intrinsic geometric interpretability. Our training time on a laptop is 85~min, and our inference time is 20~ms. SCENE-Net has 11 trainable geometrical parameters and requires fewer data than black-box models. SCENE--Net offers robustness to noisy labeling and data imbalance and has comparable IoU to state-of-the-art methods. With this paper, we release a 40~000 Km labeled dataset of rural terrain point clouds and our code implementation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2112682468",
                        "name": "Diogo Lavado"
                    },
                    {
                        "authorId": "2149926126",
                        "name": "Cl\u00e1udia Soares"
                    },
                    {
                        "authorId": "2068513462",
                        "name": "A. Micheletti"
                    },
                    {
                        "authorId": "2151859138",
                        "name": "Giovanni Bocchi"
                    },
                    {
                        "authorId": "2003481010",
                        "name": "Alex Coronati"
                    },
                    {
                        "authorId": "2167431350",
                        "name": "Manuel F. Silva"
                    },
                    {
                        "authorId": "1717431",
                        "name": "Patrizio Frosini"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Most available methods concentrate on visualizing attention scores [20] or reconstructing the attention flow [21], while other methods were introduced solely for visualizing the saliency maps of Transformers working with images [22]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c4d3e95a86bfb701fb0c3a70ca2868c48c739d70",
                "externalIds": {
                    "DOI": "10.1109/ISSC59246.2023.10162050",
                    "CorpusId": 259338741
                },
                "corpusId": 259338741,
                "publicationVenue": {
                    "id": "a04aa735-dc34-4b29-a70b-2cc395c25173",
                    "name": "Irish Signals and Systems Conference",
                    "type": "conference",
                    "alternate_names": [
                        "ISSC",
                        "Ir Signal Syst Conf"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c4d3e95a86bfb701fb0c3a70ca2868c48c739d70",
                "title": "Small, Multilingual, Explainable Transformers for Online Handwriting Decoding",
                "abstract": "A compact Transformer architecture is shown to provide an effective end-to-end framework for sentence recognition based on online handwritten gestures representing glyph strokes. Despite its reduced size, the encoder exhibits impressive transfer learning capabilities, producing latent representations that can generalise to various target grammars and output vocabularies, while also accurately performing spatio-temporal segmentation tasks. Novel visualization techniques are introduced, generalizable to other architectures and tasks, providing valuable insights and qualitative assessments of the model\u2019s learning. The model is suitable for real-time edge inference and achieves an average Levenshtein accuracy over 96% on all tested languages, even when the encoder is imported frozen from the English language.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3280438",
                        "name": "Mirco Ramo"
                    },
                    {
                        "authorId": "1874650",
                        "name": "G. Silvestre"
                    },
                    {
                        "authorId": "2631484",
                        "name": "F. Balado"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "We adopt a recent Transformer visualization method [4] to visualize the Transformer-based video encoder of our CMMT model."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "399d00a361483196f6767679db39b6d4d696c3d7",
                "externalIds": {
                    "DBLP": "conf/mir/Gao023",
                    "DOI": "10.1145/3591106.3592238",
                    "CorpusId": 259112614
                },
                "corpusId": 259112614,
                "publicationVenue": {
                    "id": "b7d34536-73df-402d-8967-50a9cdf73c01",
                    "name": "International Conference on Multimedia Retrieval",
                    "type": "conference",
                    "alternate_names": [
                        "ICMR",
                        "Int Conf Multimedia Retr"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1424"
                },
                "url": "https://www.semanticscholar.org/paper/399d00a361483196f6767679db39b6d4d696c3d7",
                "title": "CMMT: Cross-Modal Meta-Transformer for Video-Text Retrieval",
                "abstract": "Video-text retrieval has drawn great attention due to the prosperity of online video contents. Most existing methods extract the video embeddings by densely sampling abundant (generally dozens of) video clips, which acquires tremendous computational cost. To reduce the resource consumption, recent works propose to sparsely sample fewer clips from each raw video with a narrow time span. However, they still struggle to learn a reliable video representation with such locally sampled video clips, especially when testing on cross-dataset setting. In this work, to overcome this problem, we sparsely and globally (with wide time span) sample a handful of video clips from each raw video, which can be regarded as different samples of a pseudo video class (i.e., each raw video denotes a pseudo video class). From such viewpoint, we propose a novel Cross-Modal Meta-Transformer (CMMT) model that can be trained in a meta-learning paradigm. Concretely, in each training step, we conduct a cross-modal fine-grained classification task where the text queries are classified with pseudo video class prototypes (each has aggregated all sampled video clips per pseudo video class). Since each classification task is defined with different/new videos (by simulating the evaluation setting), this task-based meta-learning process enables our model to generalize well on new tasks and thus learn generalizable video/text representations. To further enhance the generalizability of our model, we induce a token-aware adaptive Transformer module to dynamically update our model (prototypes) for each individual text query. Extensive experiments on three benchmarks show that our model achieves new state-of-the-art results in cross-dataset video-text retrieval, demonstrating that it has more generalizability in video-text retrieval. Importantly, we find that our new meta-learning paradigm indeed brings improvements under both cross-dataset and in-dataset retrieval settings.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1939358",
                        "name": "Yizhao Gao"
                    },
                    {
                        "authorId": "1776220",
                        "name": "Zhiwu Lu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "For instance, we could pick a particular neuron in the embedding and trace back what parts of a particular input sequence excite that neuron using methods such as layer-wise relevance propagation [Bach et al., 2015, Chefer et al., 2021]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "56caaf598c1bf36a24385f30ca775b94cf215b6b",
                "externalIds": {
                    "ArXiv": "2306.03917",
                    "DBLP": "journals/corr/abs-2306-03917",
                    "DOI": "10.48550/arXiv.2306.03917",
                    "CorpusId": 259095948
                },
                "corpusId": 259095948,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/56caaf598c1bf36a24385f30ca775b94cf215b6b",
                "title": "Turning large language models into cognitive models",
                "abstract": "Large language models are powerful systems that excel at many tasks, ranging from translation to mathematical reasoning. Yet, at the same time, these models often show unhuman-like characteristics. In the present paper, we address this gap and ask whether large language models can be turned into cognitive models. We find that -- after finetuning them on data from psychological experiments -- these models offer accurate representations of human behavior, even outperforming traditional cognitive models in two decision-making domains. In addition, we show that their representations contain the information necessary to model behavior on the level of individual subjects. Finally, we demonstrate that finetuning on multiple tasks enables large language models to predict human behavior in a previously unseen task. Taken together, these results suggest that large, pre-trained models can be adapted to become generalist cognitive models, thereby opening up new research directions that could transform cognitive psychology and the behavioral sciences as a whole.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "32354733",
                        "name": "Marcel Binz"
                    },
                    {
                        "authorId": "49427184",
                        "name": "Eric Schulz"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "While they can make accurate predictions, understanding why they made a certain prediction is not straightforward, which could be problematic in a healthcare setting where interpretability is often necessary for clinicians to trust and act on model predictions [33]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "48121e372f2c5116b33f83fb0f20013bffa92817",
                "externalIds": {
                    "ArXiv": "2306.01249",
                    "DBLP": "journals/corr/abs-2306-01249",
                    "DOI": "10.48550/arXiv.2306.01249",
                    "CorpusId": 259064356
                },
                "corpusId": 259064356,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/48121e372f2c5116b33f83fb0f20013bffa92817",
                "title": "Transforming ECG Diagnosis: An In-depth Review of Transformer-based DeepLearning Models in Cardiovascular Disease Detection",
                "abstract": "The emergence of deep learning has significantly enhanced the analysis of electrocardiograms (ECGs), a non-invasive method that is essential for assessing heart health. Despite the complexity of ECG interpretation, advanced deep learning models outperform traditional methods. However, the increasing complexity of ECG data and the need for real-time and accurate diagnosis necessitate exploring more robust architectures, such as transformers. Here, we present an in-depth review of transformer architectures that are applied to ECG classification. Originally developed for natural language processing, these models capture complex temporal relationships in ECG signals that other models might overlook. We conducted an extensive search of the latest transformer-based models and summarize them to discuss the advances and challenges in their application and suggest potential future improvements. This review serves as a valuable resource for researchers and practitioners and aims to shed light on this innovative application in ECG interpretation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "50144429",
                        "name": "Zibin Zhao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "(2021) [3] RobustViT [4] ICE-f (Ours) ICE (Ours) Pixel-wise accuracy Mean IoU Pixel-wise accuracy Mean IoU Pixel-wise accuracy Mean IoU Pixel-wise accuracy Mean IoU Pixel-wise accuracy Mean IoU Pixel-wise accuracy Mean IoU Pixel-wise accuracy Mean IoU",
                ", relevance propagation rule, integration of propagation information, relevance, and attention scores) and solved some issues due to dependence on non-positive values and skip connections propagated in the learning process caused by the structural characteristics of vision transformers [3].",
                "Later studies have evaluated the degree to which each attention head contributes to performance [36] or integrated the relevance and attention scores in layers through the proposal of a relevance propagation rule [3].",
                "However, GradCAM has not been effectively applied to explainability visualization for vision transformers because of the structural nature of the transformers, which classify image classes using [CLS] tokens [3].",
                "Despite the advantage of this optimization, challenges to explainability visualization for vision transformers remain given their structural characteristics [3, 4]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "33cc4d534349e88fd6d396a05c0aafbdb4502a99",
                "externalIds": {
                    "DBLP": "conf/cvpr/ChoiJH23",
                    "DOI": "10.1109/CVPR52729.2023.01166",
                    "CorpusId": 260868804
                },
                "corpusId": 260868804,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/33cc4d534349e88fd6d396a05c0aafbdb4502a99",
                "title": "Adversarial Normalization: I Can visualize Everything (ICE)",
                "abstract": "Vision transformers use [CLS] tokens to predict image classes. Their explainability visualization has been studied using relevant information from [CLS] tokens or focusing on attention scores during self-attention. Such visualization, however, is challenging because of the dependence of the structure of a vision transformer on skip connections and attention operators, the instability of non-linearities in the learning process, and the limited reflection of self-attention scores on relevance. We argue that the output vectors for each input patch token in a vision transformer retain the image information of each patch location, which can facilitate the prediction of an image class. In this paper, we propose ICE (Adversarial Normalization: I Can visualize Everything), a novel method that enables a model to directly predict a class for each patch in an image; thus, advancing the effective visualization of the explainability of a vision transformer. Our method distinguishes background from foreground regions by predicting background classes for patches that do not determine image classes. We used the DeiT-S model, the most representative model employed in studies, on the explainability visualization of vision transformers. On the ImageNet-Segmentation dataset, ICE outperformed all explainability visualization methods for four cases depending on the model size. We also conducted quantitative and qualitative analyses on the tasks of weakly-supervised object localization and unsupervised object discovery. On the CUB-200-2011 and PASCALVOC07/12 datasets, ICE achieved comparable performance to the state-of-the-art methods. We incorporated ICE into the encoder of DeiT-S and improved efficiency by 44.01% on the ImageNet dataset over that achieved by the original DeiT-S model. We showed performance on the accuracy and efficiency comparable to EViT, the state-of-the-art pruning model, demonstrating the effectiveness of ICE. The code is available at https://github.com/Hanyang-HCC-Lab/ICE.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2111528165",
                        "name": "H. Choi"
                    },
                    {
                        "authorId": "1666224664",
                        "name": "Seungwan Jin"
                    },
                    {
                        "authorId": "35655049",
                        "name": "Kyungsik Han"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9ecda80a94213c4d8322ccfb34ff6e1bfc4a9390",
                "externalIds": {
                    "ArXiv": "2306.00966",
                    "DBLP": "journals/corr/abs-2306-00966",
                    "DOI": "10.48550/arXiv.2306.00966",
                    "CorpusId": 258999763
                },
                "corpusId": 258999763,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9ecda80a94213c4d8322ccfb34ff6e1bfc4a9390",
                "title": "The Hidden Language of Diffusion Models",
                "abstract": "Text-to-image diffusion models have demonstrated an unparalleled ability to generate high-quality, diverse images from a textual prompt. However, the internal representations learned by these models remain an enigma. In this work, we present Conceptor, a novel method to interpret the internal representation of a textual concept by a diffusion model. This interpretation is obtained by decomposing the concept into a small set of human-interpretable textual elements. Applied over the state-of-the-art Stable Diffusion model, Conceptor reveals non-trivial structures in the representations of concepts. For example, we find surprising visual connections between concepts, that transcend their textual semantics. We additionally discover concepts that rely on mixtures of exemplars, biases, renowned artistic styles, or a simultaneous fusion of multiple meanings of the concept. Through a large battery of experiments, we demonstrate Conceptor's ability to provide meaningful, robust, and faithful decompositions for a wide variety of abstract, concrete, and complex textual concepts, while allowing to naturally connect each decomposition element to its corresponding visual impact on the generated images. Our code will be available at: https://hila-chefer.github.io/Conceptor/",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2038268012",
                        "name": "Hila Chefer"
                    },
                    {
                        "authorId": "49618488",
                        "name": "Oran Lang"
                    },
                    {
                        "authorId": "22245981",
                        "name": "Mor Geva"
                    },
                    {
                        "authorId": "2003770160",
                        "name": "Volodymyr Polosukhin"
                    },
                    {
                        "authorId": "31114823",
                        "name": "Assaf Shocher"
                    },
                    {
                        "authorId": "144611617",
                        "name": "M. Irani"
                    },
                    {
                        "authorId": "2138834",
                        "name": "Inbar Mosseri"
                    },
                    {
                        "authorId": "145128145",
                        "name": "Lior Wolf"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9ee81e0cbe3672a8f9d72ae8692b88e38316f605",
                "externalIds": {
                    "DBLP": "conf/cvpr/HyungHKLC23",
                    "ArXiv": "2306.12570",
                    "DOI": "10.1109/CVPR52729.2023.01219",
                    "CorpusId": 259224838
                },
                "corpusId": 259224838,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9ee81e0cbe3672a8f9d72ae8692b88e38316f605",
                "title": "Local 3D Editing via 3D Distillation of CLIP Knowledge",
                "abstract": "3D content manipulation is an important computer vision task with many real-world applications (e.g., product design, cartoon generation, and 3D Avatar editing). Recently proposed 3D CANs can generate diverse photorealistic 3D-aware contents using Neural Radiance fields (NeRF). However, manipulation of NeRF still remains a challenging problem since the visual quality tends to degrade after manipulation and suboptimal control handles such as 2D semantic maps are used for manipulations. While text-guided manipulations have shown potential in 3D editing, such approaches often lack locality. To overcome these problems, we propose Local Editing NeRF (LENeRF), which only requires text inputs for fine-grained and localized manipulation. Specifically, we present three add-on modules of LENeRF, the Latent Residual Mapper, the Attention Field Network, and the Deformation Network, which are jointly usedfor local manipulations of 3D features by estimating a 3D attention field. The 3D attention field is learned in an unsupervised way, by distilling the zero-shot mask generation capability of CLIP to the 3D space with multi-view guidance. We conduct diverse experiments and thorough evaluations both quantitatively and qualitatively.11We will make our code publicly available.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2071816370",
                        "name": "J. Hyung"
                    },
                    {
                        "authorId": "121828423",
                        "name": "S. Hwang"
                    },
                    {
                        "authorId": "2154956588",
                        "name": "Daejin Kim"
                    },
                    {
                        "authorId": "2140191673",
                        "name": "Hyunji Lee"
                    },
                    {
                        "authorId": "1795455",
                        "name": "J. Choo"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "The effect described is also different than the relevancy maps obtained by explainability methods such as GradCAM [31] or recent transformer explainability methods [8]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a34b66242f3bf7cff958d1a582afa2442e6ab1e6",
                "externalIds": {
                    "DBLP": "conf/cvpr/ShaharabanyW23",
                    "DOI": "10.1109/CVPR52729.2023.00669",
                    "CorpusId": 259861751
                },
                "corpusId": 259861751,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a34b66242f3bf7cff958d1a582afa2442e6ab1e6",
                "title": "Similarity Maps for Self-Training Weakly-Supervised Phrase Grounding",
                "abstract": "A phrase grounding model receives an input image and a text phrase and outputs a suitable localization map. We present an effective way to refine a phrase ground model by considering self-similarity maps extracted from the latent representation of the model's image encoder. Our main insights are that these maps resemble localization maps and that by combining such maps, one can obtain useful pseudo-labels for performing self-training. Our results surpass, by a large margin, the state of the art in weakly supervised phrase grounding. A similar gap in performance is obtained for a recently proposed downstream task called WWbL, in which only the image is input, without any text. Our code is available at https://github.com/talshaharabany/Similarity-Maps-for-Self-Training-Weakly-Supervised-Phrase-Grounding",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1438948620",
                        "name": "Tal Shaharabany"
                    },
                    {
                        "authorId": "145128145",
                        "name": "Lior Wolf"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Note that DTA differs from the co-attention introduced in prior works [5, 6], wherein both cases, the attention is computed based on a specific task."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "134789469174e777e507e90bce9ad634798fae54",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-08071",
                    "ArXiv": "2307.08071",
                    "DOI": "10.1109/CVPRW59228.2023.00598",
                    "CorpusId": 259937044
                },
                "corpusId": 259937044,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/134789469174e777e507e90bce9ad634798fae54",
                "title": "Dense Multitask Learning to Reconfigure Comics",
                "abstract": "In this paper, we develop a MultiTask Learning (MTL) model to achieve dense predictions for comics panels to, in turn, facilitate the transfer of comics from one publication channel to another by assisting authors in the task of reconfiguring their narratives. Our MTL method can successfully identify the semantic units as well as the embedded notion of 3D in comics panels. This is a significantly challenging problem because comics comprise disparate artistic styles, illustrations, layouts, and object scales that depend on the author's creative process. Typically, dense image-based prediction techniques require a large corpus of data. Finding an automated solution for dense prediction in the comics domain, therefore, becomes more difficult with the lack of ground-truth dense annotations for the comics images. To address these challenges, we develop the following solutions- we leverage a commonly-used strategy known as unsupervised image-to-image translation, which allows us to utilize a large corpus of real-world annotations; - we utilize the results of the translations to develop our multitasking approach that is based on a vision transformer backbone and a domain transferable attention module; -we study the feasibility of integrating our MTL dense-prediction method with an existing retargeting method, thereby reconfiguring comics.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "39826361",
                        "name": "Deblina Bhattacharjee"
                    },
                    {
                        "authorId": "1735035",
                        "name": "S. S\u00fcsstrunk"
                    },
                    {
                        "authorId": "2862871",
                        "name": "M. Salzmann"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2e04f45c7b43a08268ccc58a5143b64eab6b2a33",
                "externalIds": {
                    "DBLP": "conf/cvpr/WoerlDW23",
                    "DOI": "10.1109/CVPR52729.2023.00176",
                    "CorpusId": 260085288
                },
                "corpusId": 260085288,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2e04f45c7b43a08268ccc58a5143b64eab6b2a33",
                "title": "Initialization Noise in Image Gradients and Saliency Maps",
                "abstract": "In this paper, we examine gradients of logits of image classification CNNs by input pixel values. We observe that these fluctuate considerably with training randomness, such as the random initialization of the networks. We extend our study to gradients of intermediate layers, obtained via GradCAM, as well as popular network saliency estimators such as DeepLIFT, SHAP, LIME, Integrated Gradients, and SmoothGrad. While empirical noise levels vary, qualitatively different attributions to image features are still possible with all of these, which comes with implications for interpreting such attributions, in particular when seeking data-driven explanations of the phenomenon generating the data. Finally, we demonstrate that the observed artefacts can be removed by marginalization over the initialization distribution by simple stochastic integration.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1677008487",
                        "name": "A. Woerl"
                    },
                    {
                        "authorId": "2192822564",
                        "name": "Jan Disselhoff"
                    },
                    {
                        "authorId": "1723149",
                        "name": "M. Wand"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "This deviation from the target distribution causes the attention-attenuation problem [4] between generation chains of R and G, leading to the generation of images that diverge from the reference."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "485e23e73daa2a048a2c706ace1b092a65bbe6eb",
                "externalIds": {
                    "ArXiv": "2305.18729",
                    "DBLP": "journals/corr/abs-2305-18729",
                    "DOI": "10.48550/arXiv.2305.18729",
                    "CorpusId": 258967951
                },
                "corpusId": 258967951,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/485e23e73daa2a048a2c706ace1b092a65bbe6eb",
                "title": "Real-World Image Variation by Aligning Diffusion Inversion Chain",
                "abstract": "Recent diffusion model advancements have enabled high-fidelity images to be generated using text prompts. However, a domain gap exists between generated images and real-world images, which poses a challenge in generating high-quality variations of real-world images. Our investigation uncovers that this domain gap originates from a latents' distribution gap in different diffusion processes. To address this issue, we propose a novel inference pipeline called Real-world Image Variation by ALignment (RIVAL) that utilizes diffusion models to generate image variations from a single image exemplar. Our pipeline enhances the generation quality of image variations by aligning the image generation process to the source image's inversion chain. Specifically, we demonstrate that step-wise latent distribution alignment is essential for generating high-quality variations. To attain this, we design a cross-image self-attention injection for feature interaction and a step-wise distribution normalization to align the latent features. Incorporating these alignment processes into a diffusion model allows RIVAL to generate high-quality image variations without further parameter optimization. Our experimental results demonstrate that our proposed approach outperforms existing methods with respect to semantic-condition similarity and perceptual quality. Furthermore, this generalized inference pipeline can be easily applied to other diffusion-based generation tasks, such as image-conditioned text-to-image generation and example-based image inpainting.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2145915052",
                        "name": "Yuecheng Zhang"
                    },
                    {
                        "authorId": "2087273800",
                        "name": "Jinbo Xing"
                    },
                    {
                        "authorId": "2159555294",
                        "name": "Eric Lo"
                    },
                    {
                        "authorId": "1729056",
                        "name": "Jiaya Jia"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Explainable deep learning methods have recently been effective in visualizing the internal decision-making processes of CNN and Transformer models [7], [18], [9]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "94a16bb48c36375b5d84012cab573622f73eb91d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-17451",
                    "ArXiv": "2305.17451",
                    "DOI": "10.48550/arXiv.2305.17451",
                    "CorpusId": 258960573
                },
                "corpusId": 258960573,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/94a16bb48c36375b5d84012cab573622f73eb91d",
                "title": "Analysis over vision-based models for pedestrian action anticipation",
                "abstract": "Anticipating human actions in front of autonomous vehicles is a challenging task. Several papers have recently proposed model architectures to address this problem by combining multiple input features to predict pedestrian crossing actions. This paper focuses specifically on using images of the pedestrian's context as an input feature. We present several spatio-temporal model architectures that utilize standard CNN and Transformer modules to serve as a backbone for pedestrian anticipation. However, the objective of this paper is not to surpass state-of-the-art benchmarks but rather to analyze the positive and negative predictions of these models. Therefore, we provide insights on the explainability of vision-based Transformer models in the context of pedestrian action prediction. We will highlight cases where the model can achieve correct quantitative results but falls short in providing human-like explanations qualitatively, emphasizing the importance of investing in explainability for pedestrian action anticipation problems.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1581564100",
                        "name": "Lina Achaji"
                    },
                    {
                        "authorId": "2061063803",
                        "name": "Julien Moreau"
                    },
                    {
                        "authorId": "2219041395",
                        "name": "Franccois Aioun"
                    },
                    {
                        "authorId": "2219041391",
                        "name": "F. Charpillet"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Inspired by recent work on Transformer-based image classification [29, 30], we propose a framework for regression tasks operating on molecular strings, and develop an explainable AI technique for chemical language models, using solely the model without external tools or information.",
                "Recent approaches overcome this limitation by considering all Transformer components and aggregating importance throughout the layers while retaining context [28, 29]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "13869660ad6773af0a18b34a69216afb1c7be78b",
                "externalIds": {
                    "ArXiv": "2305.16192",
                    "DBLP": "journals/corr/abs-2305-16192",
                    "DOI": "10.48550/arXiv.2305.16192",
                    "CorpusId": 258887737
                },
                "corpusId": 258887737,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/13869660ad6773af0a18b34a69216afb1c7be78b",
                "title": "Explainability Techniques for Chemical Language Models",
                "abstract": "Explainability techniques are crucial in gaining insights into the reasons behind the predictions of deep learning models, which have not yet been applied to chemical language models. We propose an explainable AI technique that attributes the importance of individual atoms towards the predictions made by these models. Our method backpropagates the relevance information towards the chemical input string and visualizes the importance of individual atoms. We focus on self-attention Transformers operating on molecular string representations and leverage a pretrained encoder for finetuning. We showcase the method by predicting and visualizing solubility in water and organic solvents. We achieve competitive model performance while obtaining interpretable predictions, which we use to inspect the pretrained model.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2219665434",
                        "name": "Stefan H\u00f6dl"
                    },
                    {
                        "authorId": "2218453039",
                        "name": "William Robinson"
                    },
                    {
                        "authorId": "1698412",
                        "name": "Yoram Bachrach"
                    },
                    {
                        "authorId": "2218444778",
                        "name": "Wilhelm Huck"
                    },
                    {
                        "authorId": "2551829",
                        "name": "Tal Kachman"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ab4036bf29853d4b6e454184730eb6d26cbc4bc7",
                "externalIds": {
                    "ArXiv": "2305.16311",
                    "DBLP": "journals/corr/abs-2305-16311",
                    "DOI": "10.48550/arXiv.2305.16311",
                    "CorpusId": 258888228
                },
                "corpusId": 258888228,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ab4036bf29853d4b6e454184730eb6d26cbc4bc7",
                "title": "Break-A-Scene: Extracting Multiple Concepts from a Single Image",
                "abstract": "Text-to-image model personalization aims to introduce a user-provided concept to the model, allowing its synthesis in diverse contexts. However, current methods primarily focus on the case of learning a single concept from multiple images with variations in backgrounds and poses, and struggle when adapted to a different scenario. In this work, we introduce the task of textual scene decomposition: given a single image of a scene that may contain several concepts, we aim to extract a distinct text token for each concept, enabling fine-grained control over the generated scenes. To this end, we propose augmenting the input image with masks that indicate the presence of target concepts. These masks can be provided by the user or generated automatically by a pre-trained segmentation model. We then present a novel two-phase customization process that optimizes a set of dedicated textual embeddings (handles), as well as the model weights, striking a delicate balance between accurately capturing the concepts and avoiding overfitting. We employ a masked diffusion loss to enable handles to generate their assigned concepts, complemented by a novel loss on cross-attention maps to prevent entanglement. We also introduce union-sampling, a training strategy aimed to improve the ability of combining multiple concepts in generated images. We use several automatic metrics to quantitatively compare our method against several baselines, and further affirm the results using a user study. Finally, we showcase several applications of our method. Project page is available at: https://omriavrahami.com/break-a-scene/",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2107086356",
                        "name": "Omri Avrahami"
                    },
                    {
                        "authorId": "3451442",
                        "name": "Kfir Aberman"
                    },
                    {
                        "authorId": "2416503",
                        "name": "Ohad Fried"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    },
                    {
                        "authorId": "70018371",
                        "name": "D. Lischinski"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "14b7b46b2b159d037869a892ae81fea94bdf1826",
                "externalIds": {
                    "ArXiv": "2305.16526",
                    "DBLP": "journals/corr/abs-2305-16526",
                    "DOI": "10.48550/arXiv.2305.16526",
                    "CorpusId": 258947438
                },
                "corpusId": 258947438,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/14b7b46b2b159d037869a892ae81fea94bdf1826",
                "title": "Extending Explainable Boosting Machines to Scientific Image Data",
                "abstract": "As the deployment of computer vision technology becomes increasingly common in applications of consequence such as medicine or science, the need for explanations of the system output has become a focus of great concern. Unfortunately, many state-of-the-art computer vision models are opaque, making their use challenging from an explanation standpoint, and current approaches to explaining these opaque models have stark limitations and have been the subject of serious criticism. In contrast, Explainable Boosting Machines (EBMs) are a class of models that are easy to interpret and achieve performance on par with the very best-performing models, however, to date EBMs have been limited solely to tabular data. Driven by the pressing need for interpretable models in science, we propose the use of EBMs for scientific image data. Inspired by an important application underpinning the development of quantum technologies, we apply EBMs to cold-atom soliton image data, and, in doing so, demonstrate EBMs for image data for the first time. To tabularize the image data we employ Gabor Wavelet Transform-based techniques that preserve the spatial structure of the data. We show that our approach provides better explanations than other state-of-the-art explainability methods for images.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "84535893",
                        "name": "D. Schug"
                    },
                    {
                        "authorId": "38809174",
                        "name": "Sai Yerramreddy"
                    },
                    {
                        "authorId": "145727186",
                        "name": "R. Caruana"
                    },
                    {
                        "authorId": "34143351",
                        "name": "Craig S. Greenberg"
                    },
                    {
                        "authorId": "3246840",
                        "name": "Justyna P. Zwolak"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "There have been several methods proposed in the literature for generating attention visualizations in classification tasks, such as Attention Roll-Out [31] and Gradient Attention Roll-Out [32]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3f0a30abbcdccab2eeb08abbc9e22d06f9030449",
                "externalIds": {
                    "ArXiv": "2305.08551",
                    "DBLP": "journals/corr/abs-2305-08551",
                    "DOI": "10.48550/arXiv.2305.08551",
                    "CorpusId": 263352309
                },
                "corpusId": 263352309,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3f0a30abbcdccab2eeb08abbc9e22d06f9030449",
                "title": "Enhancing Performance of Vision Transformers on Small Datasets through Local Inductive Bias Incorporation",
                "abstract": "Vision transformers (ViTs) achieve remarkable performance on large datasets, but tend to perform worse than convolutional neural networks (CNNs) when trained from scratch on smaller datasets, possibly due to a lack of local inductive bias in the architecture. Recent studies have therefore added locality to the architecture and demonstrated that it can help ViTs achieve performance comparable to CNNs in the small-size dataset regime. Existing methods, however, are architecture-specific or have higher computational and memory costs. Thus, we propose a module called Local InFormation Enhancer (LIFE) that extracts patch-level local information and incorporates it into the embeddings used in the self-attention block of ViTs. Our proposed module is memory and computation efficient, as well as flexible enough to process auxiliary tokens such as the classification and distillation tokens. Empirical results show that the addition of the LIFE module improves the performance of ViTs on small image classification datasets. We further demonstrate how the effect can be extended to downstream tasks, such as object detection and semantic segmentation. In addition, we introduce a new visualization method, Dense Attention Roll-Out, specifically designed for dense prediction tasks, allowing the generation of class-specific attention maps utilizing the attention maps of all tokens.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "30686917",
                        "name": "Ibrahim Batuhan Akkaya"
                    },
                    {
                        "authorId": "122174284",
                        "name": "S. Kathiresan"
                    },
                    {
                        "authorId": "2219805641",
                        "name": "Elahe Arani"
                    },
                    {
                        "authorId": "2107033749",
                        "name": "Bahram Zonooz"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[50] reassigned a trainable relevancy map to the input image and propagate it through all the self-attention layers."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7e9a8bb2b1f303efafa8226322b6cba846d6c750",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-03919",
                    "ArXiv": "2305.03919",
                    "DOI": "10.48550/arXiv.2305.03919",
                    "CorpusId": 258556930
                },
                "corpusId": 258556930,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7e9a8bb2b1f303efafa8226322b6cba846d6c750",
                "title": "DBAT: Dynamic Backward Attention Transformer for Material Segmentation with Cross-Resolution Patches",
                "abstract": "The objective of dense material segmentation is to identify the material categories for every image pixel. Recent studies adopt image patches to extract material features. Although the trained networks can improve the segmentation performance, their methods choose a fixed patch resolution which fails to take into account the variation in pixel area covered by each material. In this paper, we propose the Dynamic Backward Attention Transformer (DBAT) to aggregate cross-resolution features. The DBAT takes cropped image patches as input and gradually increases the patch resolution by merging adjacent patches at each transformer stage, instead of fixing the patch resolution during training. We explicitly gather the intermediate features extracted from cross-resolution patches and merge them dynamically with predicted attention masks. Experiments show that our DBAT achieves an accuracy of 86.85%, which is the best performance among state-of-the-art real-time models. Like other successful deep learning solutions with complex architectures, the DBAT also suffers from lack of interpretability. To address this problem, this paper examines the properties that the DBAT makes use of. By analysing the cross-resolution features and the attention weights, this paper interprets how the DBAT learns from image patches. We further align features to semantic labels, performing network dissection, to infer that the proposed model can extract material-related features better than other methods. We show that the DBAT model is more robust to network initialisation, and yields fewer variable predictions compared to other models. The project code is available at https://github.com/heng-yuwen/Dynamic-Backward-Attention-Transformer.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2134394216",
                        "name": "Yuwen Heng"
                    },
                    {
                        "authorId": "2336372",
                        "name": "S. Dasmahapatra"
                    },
                    {
                        "authorId": "65879236",
                        "name": "Hansung Kim"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Therefore, the relevance map that corresponds to the CLS token links each of the tokens to the CLS token, and the strength of this link can be intuitively considered as an indicator of the contribution of each token to the classification [60].",
                "(b) Bottom: feature map visualization using Transformer Attribution method [60]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b5b181b1adc1523e53ccc263133d3c4bb123ff3f",
                "externalIds": {
                    "DBLP": "journals/tifs/LiuTYZWLLZLG23",
                    "ArXiv": "2305.03277",
                    "DOI": "10.1109/TIFS.2023.3296330",
                    "CorpusId": 258546890
                },
                "corpusId": 258546890,
                "publicationVenue": {
                    "id": "d406a3f4-dc05-43be-b1f6-812f29de9c0e",
                    "name": "IEEE Transactions on Information Forensics and Security",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Inf Forensics Secur"
                    ],
                    "issn": "1556-6013",
                    "url": "http://www.ieee.org/organizations/society/sp/tifs.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=10206",
                        "http://www.signalprocessingsociety.org/publications/periodicals/forensics/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b5b181b1adc1523e53ccc263133d3c4bb123ff3f",
                "title": "FM-ViT: Flexible Modal Vision Transformers for Face Anti-Spoofing",
                "abstract": "The availability of handy multi-modal (i.e., RGB-D) sensors has brought about a surge of face anti-spoofing research. However, the current multi-modal face presentation attack detection (PAD) has two defects: (1) The framework based on multi-modal fusion requires providing modalities consistent with the training input, which seriously limits the deployment scenario. (2) The performance of ConvNet-based model on high fidelity datasets is increasingly limited. In this work, we present a pure transformer-based framework, dubbed the Flexible Modal Vision Transformer (FM-ViT), for face anti-spoofing to flexibly target any single-modal (i.e., RGB) attack scenarios with the help of available multi-modal data. Specifically, FM-ViT retains a specific branch for each modality to capture different modal information and introduces the Cross-Modal Transformer Block (CMTB), which consists of two cascaded attentions named Multi-headed Mutual-Attention (MMA) and Fusion-Attention (MFA) to guide each modal branch to mine potential features from informative patch tokens, and to learn modality-agnostic liveness features by enriching the modal information of own CLS token, respectively. Experiments demonstrate that the single model trained based on FM-ViT can not only flexibly evaluate different modal samples, but also outperforms existing single-modal frameworks by a large margin, and approaches the multi-modal frameworks introduced with smaller FLOPs and model parameters.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "144701473",
                        "name": "Ajian Liu"
                    },
                    {
                        "authorId": "9645431",
                        "name": "Zichang Tan"
                    },
                    {
                        "authorId": "1915941538",
                        "name": "Zitong Yu"
                    },
                    {
                        "authorId": "50016072",
                        "name": "Chenxu Zhao"
                    },
                    {
                        "authorId": "145121530",
                        "name": "Jun Wan"
                    },
                    {
                        "authorId": "48503912",
                        "name": "Yanyan Liang"
                    },
                    {
                        "authorId": "145754448",
                        "name": "Zhen Lei"
                    },
                    {
                        "authorId": "2109578297",
                        "name": "Du Zhang"
                    },
                    {
                        "authorId": "34679741",
                        "name": "S. Li"
                    },
                    {
                        "authorId": "1822413",
                        "name": "G. Guo"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[5] also argues that the intermediate artifacts of self-attention, such as queries and keys, are underexplored."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "61326b7f5ff4f574ce470217b4d6502c629191dc",
                "externalIds": {
                    "ArXiv": "2305.03210",
                    "DBLP": "journals/corr/abs-2305-03210",
                    "DOI": "10.48550/arXiv.2305.03210",
                    "CorpusId": 258546743
                },
                "corpusId": 258546743,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/61326b7f5ff4f574ce470217b4d6502c629191dc",
                "title": "AttentionViz: A Global View of Transformer Attention",
                "abstract": "Transformer models are revolutionizing machine learning, but their inner workings remain mysterious. In this work, we present a new visualization technique designed to help researchers understand the self-attention mechanism in transformers that allows these models to learn rich, contextual relationships between elements of a sequence. The main idea behind our method is to visualize a joint embedding of the query and key vectors used by transformer models to compute attention. Unlike previous attention visualization techniques, our approach enables the analysis of global patterns across multiple input sequences. We create an interactive visualization tool, AttentionViz (demo: http://attentionviz.com), based on these joint query-key embeddings, and use it to study attention mechanisms in both language and vision transformers. We demonstrate the utility of our approach in improving model understanding and offering new insights about query-key interactions through several application scenarios and expert feedback.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2170076557",
                        "name": "Catherine Yeh"
                    },
                    {
                        "authorId": "2155141122",
                        "name": "Yida Chen"
                    },
                    {
                        "authorId": "66348205",
                        "name": "Aoyu Wu"
                    },
                    {
                        "authorId": "2216498927",
                        "name": "Cynthia Chen"
                    },
                    {
                        "authorId": "2203369547",
                        "name": "Fernanda Vi'egas"
                    },
                    {
                        "authorId": "145233583",
                        "name": "M. Wattenberg"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In addition, considering the architecture difference between CNNs and transformers, some works [5, 33] adopt the Deep Taylor Decomposition [36] principle, based on which attention and relevancy scores are integrated across multiple layers for generating class-aware activation maps."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "26ff6f5d9b900466c3259250e64be407ac740bd5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-03112",
                    "ArXiv": "2305.03112",
                    "DOI": "10.48550/arXiv.2305.03112",
                    "CorpusId": 258546695
                },
                "corpusId": 258546695,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/26ff6f5d9b900466c3259250e64be407ac740bd5",
                "title": "Mitigating Undisciplined Over-Smoothing in Transformer for Weakly Supervised Semantic Segmentation",
                "abstract": "A surge of interest has emerged in weakly supervised semantic segmentation due to its remarkable efficiency in recent years. Existing approaches based on transformers mainly focus on exploring the affinity matrix to boost CAMs with global relationships. While in this work, we first perform a scrupulous examination towards the impact of successive affinity matrices and discover that they possess an inclination toward sparsification as the network approaches convergence, hence disclosing a manifestation of over-smoothing. Besides, it has been observed that enhanced attention maps tend to evince a substantial amount of extraneous background noise in deeper layers. Drawing upon this, we posit a daring conjecture that the undisciplined over-smoothing phenomenon introduces a noteworthy quantity of semantically irrelevant background noise, causing performance degradation. To alleviate this issue, we propose a novel perspective that highlights the objects of interest by investigating the regions of the trait, thereby fostering an extensive comprehension of the successive affinity matrix. Consequently, we suggest an adaptive re-activation mechanism (AReAM) that alleviates the issue of incomplete attention within the object and the unbounded background noise. AReAM accomplishes this by supervising high-level attention with shallow affinity matrices, yielding promising results. Exhaustive experiments conducted on the commonly used dataset manifest that segmentation results can be greatly improved through our proposed AReAM, which imposes restrictions on each affinity matrix in deep layers to make it attentive to semantic regions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2158100742",
                        "name": "Jingxuan He"
                    },
                    {
                        "authorId": "26953623",
                        "name": "Lechao Cheng"
                    },
                    {
                        "authorId": "2082794",
                        "name": "Chaowei Fang"
                    },
                    {
                        "authorId": "39901030",
                        "name": "Dingwen Zhang"
                    },
                    {
                        "authorId": "145549600",
                        "name": "Zhangye Wang"
                    },
                    {
                        "authorId": "2024172297",
                        "name": "Wei Chen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "For explaining BERT, we employ the transformer visualization method proposed in Chefer et al. (2021) to map back from the [CLS] activation concepts to input tokens.",
                "Similarly, the codebase used for replicating the visualization method (Chefer et al., 2021) and the baseline method (Chen et al., 2018) are licensed under the MIT license, which allows for redistribution of the code.",
                "We use the transformer visualization approach (Chefer et al., 2021) and Grad-CAM (Selvaraju et al., 2017), which rely on the gradients generated from the red path."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cfce5f9641d31121dd5d092c5380a9818526b62f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-02160",
                    "ArXiv": "2305.02160",
                    "DOI": "10.48550/arXiv.2305.02160",
                    "CorpusId": 258461083
                },
                "corpusId": 258461083,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cfce5f9641d31121dd5d092c5380a9818526b62f",
                "title": "Explaining Language Models' Predictions with High-Impact Concepts",
                "abstract": "The emergence of large-scale pretrained language models has posed unprecedented challenges in deriving explanations of why the model has made some predictions. Stemmed from the compositional nature of languages, spurious correlations have further undermined the trustworthiness of NLP systems, leading to unreliable model explanations that are merely correlated with the output predictions. To encourage fairness and transparency, there exists an urgent demand for reliable explanations that allow users to consistently understand the model's behavior. In this work, we propose a complete framework for extending concept-based interpretability methods to NLP. Specifically, we propose a post-hoc interpretability method for extracting predictive high-level features (concepts) from the pretrained model's hidden layer activations. We optimize for features whose existence causes the output predictions to change substantially, \\ie generates a high impact. Moreover, we devise several evaluation metrics that can be universally applied. Extensive experiments on real and synthetic tasks demonstrate that our method achieves superior results on {predictive impact}, usability, and faithfulness compared to the baselines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2091437375",
                        "name": "Ruochen Zhao"
                    },
                    {
                        "authorId": "2708940",
                        "name": "Shafiq R. Joty"
                    },
                    {
                        "authorId": "2108095839",
                        "name": "Yongjie Wang"
                    },
                    {
                        "authorId": "2116132591",
                        "name": "Tan Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "With the trained BrainNPT model, for any given brain network, we can obtain the relevance scores of corresponding ROI with LRP methods.",
                "The Transformer models with the <cls> classification embedding vectors have been proven to be interpretable using self-attribution scores [40] or layer-wise relevance propagation (LRP) [41] in NLP or CV domain.",
                "In addition, the proposed model could be interpreted by layer-wise relevance propagation (LRP), which is one of the most prominent explanation techniques for deep neural networks.",
                "The BrainNPT contains <cls> classification embedding vector, the Transformer block, and fully connected layers, and it is able to use LRP to calculate the relevance scores of each ROI for the classification results.",
                "Based on LRP for interpretation of BrainNPT, we could obtain the local relevance for an input sample using deep Taylor decomposition method [42].",
                "For a trained BrainNPT model, its relevance scores of ROIs can be calculated based on LRP for Transformer\u2019s self-attention, FFN, GELU activation function and MLP. Finally, by the chain rule, the relevance score of each ROI can be estimated in the input brain network.",
                "Moreover, the pre-training strategies and the influence of the parameters of the model were further analyzed, and the trained BrainNPT model was interpreted by LRP.",
                "Considering the similarity between BrainNPT with BERT [7] and ViT [21], we adapted an LRP based interpretation method for the BrainNPT model to explore which ROIs in the brain networks have the key impact on classification."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "15bc452fa14bf0f41fde53a4fc070c77ba81c0d1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-01666",
                    "ArXiv": "2305.01666",
                    "DOI": "10.48550/arXiv.2305.01666",
                    "CorpusId": 258461029
                },
                "corpusId": 258461029,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/15bc452fa14bf0f41fde53a4fc070c77ba81c0d1",
                "title": "BrainNPT: Pre-training of Transformer networks for brain network classification",
                "abstract": "Deep learning methods have advanced quickly in brain imaging analysis over the past few years, but they are usually restricted by the limited labeled data. Pre-trained model on unlabeled data has presented promising improvement in feature learning in many domains, including natural language processing and computer vision. However, this technique is under-explored in brain network analysis. In this paper, we focused on pre-training methods with Transformer networks to leverage existing unlabeled data for brain functional network classification. First, we proposed a Transformer-based neural network, named as BrainNPT, for brain functional network classification. The proposed method leveragedtoken as a classification embedding vector for the Transformer model to effectively capture the representation of brain network. Second, we proposed a pre-training framework for BrainNPT model to leverage unlabeled brain network data to learn the structure information of brain networks. The results of classification experiments demonstrated the BrainNPT model without pre-training achieved the best performance with the state-of-the-art models, and the BrainNPT model with pre-training strongly outperformed the state-of-the-art models. The pre-training BrainNPT model improved 8.75% of accuracy compared with the model without pre-training. We further compared the pre-training strategies, analyzed the influence of the parameters of the model, and interpreted the trained model.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47756274",
                        "name": "Jinlong Hu"
                    },
                    {
                        "authorId": "2108738761",
                        "name": "Ya-Lin Huang"
                    },
                    {
                        "authorId": "144457723",
                        "name": "Nan Wang"
                    },
                    {
                        "authorId": "1752810",
                        "name": "Shoubin Dong"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Interpretability is also another crucial criteria, especially in sensitive scenarios like medical applications, and despite some efforts to improve it[13, 38], investigation of interpratibility of attention-based architectures is more challenging compared to CNNs[36]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "56ae4887db15b38c7773123efd9ef69e7bd3ef54",
                "externalIds": {
                    "ArXiv": "2304.14571",
                    "DBLP": "journals/corr/abs-2304-14571",
                    "DOI": "10.48550/arXiv.2304.14571",
                    "CorpusId": 258418132
                },
                "corpusId": 258418132,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/56ae4887db15b38c7773123efd9ef69e7bd3ef54",
                "title": "DIAMANT: Dual Image-Attention Map Encoders For Medical Image Segmentation",
                "abstract": "Although purely transformer-based architectures showed promising performance in many computer vision tasks, many hybrid models consisting of CNN and transformer blocks are introduced to fit more specialized tasks. Nevertheless, despite the performance gain of both pure and hybrid transformer-based architectures compared to CNNs in medical imaging segmentation, their high training cost and complexity make it challenging to use them in real scenarios. In this work, we propose simple architectures based on purely convolutional layers, and show that by just taking advantage of the attention map visualizations obtained from a self-supervised pretrained vision transformer network (e.g., DINO) one can outperform complex transformer-based networks with much less computation costs. The proposed architecture is composed of two encoder branches with the original image as input in one branch and the attention map visualizations of the same image from multiple self-attention heads from a pre-trained DINO model (as multiple channels) in the other branch. The results of our experiments on two publicly available medical imaging datasets show that the proposed pipeline outperforms U-Net and the state-of-the-art medical image segmentation models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1882505527",
                        "name": "Yousef Yeganeh"
                    },
                    {
                        "authorId": "2169482",
                        "name": "Azade Farshad"
                    },
                    {
                        "authorId": "2176770240",
                        "name": "Peter Weinberger"
                    },
                    {
                        "authorId": "145774206",
                        "name": "Seyed-Ahmad Ahmadi"
                    },
                    {
                        "authorId": "3419364",
                        "name": "E. Adeli"
                    },
                    {
                        "authorId": "145587209",
                        "name": "N. Navab"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "The ROAR protocol and its variants have been used to evaluate attribution methods across a range of tasks, including image domain tasks Chefer et al. [2021], Yang et al. [2020], natural language processing Ismail et al. [2021], Zhang et al.",
                "The ROAR protocol and its variants have been used to evaluate attribution methods across a range of tasks, including image domain tasks Chefer et al. [2021], Yang et al. [2020], natural language processing Ismail et al. [2021], Zhang et al. [2021], graph structure analysis Funke et al. [2022], and time-series analysis Liang et al. [2020], Ismail et al. [2020]. The ROAR protocol has been established as the primary evaluation methodology in Meng et al.",
                "The ROAR protocol and its variants have been used to evaluate attribution methods across a range of tasks, including image domain tasks Chefer et al. [2021], Yang et al.",
                "The ROAR protocol and its variants have been used to evaluate attribution methods across a range of tasks, including image domain tasks Chefer et al. [2021], Yang et al. [2020], natural language processing Ismail et al. [2021], Zhang et al. [2021], graph structure analysis Funke et al. [2022], and time-series analysis Liang et al.",
                "The ROAR protocol and its variants have been used to evaluate attribution methods across a range of tasks, including image domain tasks Chefer et al. [2021], Yang et al. [2020], natural language processing Ismail et al. [2021], Zhang et al. [2021], graph structure analysis Funke et al.",
                "The ROAR protocol and its variants have been used to evaluate attribution methods across a range of tasks, including image domain tasks Chefer et al. [2021], Yang et al. [2020], natural language processing Ismail et al. [2021], Zhang et al. [2021], graph structure analysis Funke et al. [2022], and time-series analysis Liang et al. [2020], Ismail et al.",
                "The ROAR protocol and its variants have been used to evaluate attribution methods across a range of tasks, including image domain tasks Chefer et al. [2021], Yang et al. [2020], natural language processing Ismail et al."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "6a8ec7b39423d912ed58ea9d9911eec0a91d8c43",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-13836",
                    "ArXiv": "2304.13836",
                    "DOI": "10.48550/arXiv.2304.13836",
                    "CorpusId": 258352437
                },
                "corpusId": 258352437,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6a8ec7b39423d912ed58ea9d9911eec0a91d8c43",
                "title": "On Pitfalls of RemOve-And-Retrain: Data Processing Inequality Perspective",
                "abstract": "Approaches for appraising feature importance approximations, alternatively referred to as attribution methods, have been established across an extensive array of contexts. The development of resilient techniques for performance benchmarking constitutes a critical concern in the sphere of explainable deep learning. This study scrutinizes the dependability of the RemOve-And-Retrain (ROAR) procedure, which is prevalently employed for gauging the performance of feature importance estimates. The insights gleaned from our theoretical foundation and empirical investigations reveal that attributions containing lesser information about the decision function may yield superior results in ROAR benchmarks, contradicting the original intent of ROAR. This occurrence is similarly observed in the recently introduced variant RemOve-And-Debias (ROAD), and we posit a persistent pattern of blurriness bias in ROAR attribution metrics. Our findings serve as a warning against indiscriminate use on ROAR metrics. The code is available as open source.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2217315691",
                        "name": "J. Song"
                    },
                    {
                        "authorId": "32665890",
                        "name": "Keumgang Cha"
                    },
                    {
                        "authorId": "11027268",
                        "name": "Junghoon Seo"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fac0021b96f4f7c8b924cabfe7eaa3d58775d963",
                "externalIds": {
                    "DBLP": "journals/sensors/KulbackiSCRKKW23",
                    "PubMedCentral": "10181781",
                    "DOI": "10.3390/s23094258",
                    "CorpusId": 258348158,
                    "PubMed": "37177461"
                },
                "corpusId": 258348158,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fac0021b96f4f7c8b924cabfe7eaa3d58775d963",
                "title": "Intelligent Video Analytics for Human Action Recognition: The State of Knowledge",
                "abstract": "The paper presents a comprehensive overview of intelligent video analytics and human action recognition methods. The article provides an overview of the current state of knowledge in the field of human activity recognition, including various techniques such as pose-based, tracking-based, spatio-temporal, and deep learning-based approaches, including visual transformers. We also discuss the challenges and limitations of these techniques and the potential of modern edge AI architectures to enable real-time human action recognition in resource-constrained environments.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3043492",
                        "name": "Marek Kulbacki"
                    },
                    {
                        "authorId": "2554241",
                        "name": "J. Segen"
                    },
                    {
                        "authorId": "1684306",
                        "name": "Z. Chaczko"
                    },
                    {
                        "authorId": "1708570",
                        "name": "J. Rozenblit"
                    },
                    {
                        "authorId": "2220159124",
                        "name": "Michal Kulbacki"
                    },
                    {
                        "authorId": "1747826",
                        "name": "R. Klempous"
                    },
                    {
                        "authorId": "2062949094",
                        "name": "Konrad Wojciechowski"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                ", CAM [49], and GradCAM [33] for convolutional neural networks; [7] for Transformers [40]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4833b15d617ee2a44bfe326bb397e7424a0a8e21",
                "externalIds": {
                    "ArXiv": "2304.10131",
                    "DBLP": "journals/corr/abs-2304-10131",
                    "DOI": "10.48550/arXiv.2304.10131",
                    "CorpusId": 258236219
                },
                "corpusId": 258236219,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4833b15d617ee2a44bfe326bb397e7424a0a8e21",
                "title": "Learning Bottleneck Concepts in Image Classification",
                "abstract": "Interpreting and explaining the behavior of deep neural networks is critical for many tasks. Explainable AI provides a way to address this challenge, mostly by providing per-pixel relevance to the decision. Yet, interpreting such explanations may require expert knowledge. Some recent attempts toward interpretability adopt a concept-based framework, giving a higher-level relationship between some concepts and model decisions. This paper proposes Bottleneck Concept Learner (BotCL), which represents an image solely by the presence/absence of concepts learned through training over the target task without explicit supervision over the concepts. It uses self-supervision and tailored regularizers so that learned concepts can be human-understandable. Using some image classification tasks as our testbed, we demonstrate BotCL's potential to rebuild neural networks for better interpretability11Code is avaliable at https://github.com/wbw520/BotCL and a simple demo is available at https://botcl.liangzhili.com/.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2153213890",
                        "name": "Bowen Wang"
                    },
                    {
                        "authorId": "47681301",
                        "name": "Liangzhi Li"
                    },
                    {
                        "authorId": "2210102679",
                        "name": "Yuta Nakashima"
                    },
                    {
                        "authorId": "2124415764",
                        "name": "Hajime Nagahara"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Therefore, we employ an improved Layer-wise Relevance Propagation (LRP) method [16] which integrates the weighted attention relevance for each MSA block.",
                "In addition, we employ an improved Layer-wise Relevance Propagation (LRP) method [16] for the interpretation of the spectral transformer.",
                "According to [16], the relevance rule can be described as"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "c3b74c518c70ac94baf936d05d82b529e470ac82",
                "externalIds": {
                    "DBLP": "conf/isbi/JinYYWXZHGLSHT23",
                    "DOI": "10.1109/ISBI53787.2023.10230416",
                    "CorpusId": 261434774
                },
                "corpusId": 261434774,
                "publicationVenue": {
                    "id": "a38e0d3d-6929-4868-b4e4-af8bbacf711e",
                    "name": "IEEE International Symposium on Biomedical Imaging",
                    "type": "conference",
                    "alternate_names": [
                        "ISBI",
                        "International Symposium on Biomedical Imaging",
                        "Int Symp Biomed Imaging",
                        "IEEE Int Symp Biomed Imaging"
                    ],
                    "issn": "1945-7928",
                    "alternate_issns": [
                        "1945-8452"
                    ],
                    "url": "http://www.biomedicalimaging.org/",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conferences.jsp"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c3b74c518c70ac94baf936d05d82b529e470ac82",
                "title": "Strain-Level Identification and Analysis of Avian Coronavirus Using Raman Spectroscopy and Interpretable Machine Learning",
                "abstract": "Strain-level identification of viruses is important for decision making in public health management. Recently, Raman spectroscopy has attained great attention in virus identification since it enables rapid and label-free analysis. In this paper, we present an interpretable machine learning approach for strain-level identification of avian coronaviruses based on Raman spectra. Specifically, we design a spectral transformer to classify the Raman spectra of 32 avian coronavirus strains. After training, relevance maps can be generated through gradient and relevance propagation to further understand the contribution of each wavenumber to the identification. Experimental results show that the proposed method outperforms several machine learning and deep learning baseline models, and achieves 72.72% accuracy in the 32-class identification problem. The relevance maps generated reveal some wavenumber ranges that are important for the identification of almost all strains, and these ranges correlate with Raman peak ranges for lipids, nucleic acids, and proteins.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2236939228",
                        "name": "Peng Jin"
                    },
                    {
                        "authorId": "1687095",
                        "name": "Y. Yeh"
                    },
                    {
                        "authorId": "2000120834",
                        "name": "Jiarong Ye"
                    },
                    {
                        "authorId": "2121314861",
                        "name": "Ziyang Wang"
                    },
                    {
                        "authorId": "13012540",
                        "name": "Yuan Xue"
                    },
                    {
                        "authorId": "2237103168",
                        "name": "Na Zhang"
                    },
                    {
                        "authorId": "2237066382",
                        "name": "Shengxi Huang"
                    },
                    {
                        "authorId": "2580269",
                        "name": "E. Ghedin"
                    },
                    {
                        "authorId": "47146770",
                        "name": "Huaguang Lu"
                    },
                    {
                        "authorId": "39770774",
                        "name": "A. Schmitt"
                    },
                    {
                        "authorId": "122132149",
                        "name": "Sharon X. Huang"
                    },
                    {
                        "authorId": "2136377597",
                        "name": "Mauricio Terrones"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Gradient-based methods have also been used to visualize ViT models [73, 13]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "32348e3051d06b085bd7fd6b23dc4b97adecbaf4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-08733",
                    "ArXiv": "2304.08733",
                    "DOI": "10.48550/arXiv.2304.08733",
                    "CorpusId": 258187020
                },
                "corpusId": 258187020,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/32348e3051d06b085bd7fd6b23dc4b97adecbaf4",
                "title": "Do humans and machines have the same eyes? Human-machine perceptual differences on image classification",
                "abstract": "Trained computer vision models are assumed to solve vision tasks by imitating human behavior learned from training labels. Most efforts in recent vision research focus on measuring the model task performance using standardized benchmarks. Limited work has been done to understand the perceptual difference between humans and machines. To fill this gap, our study first quantifies and analyzes the statistical distributions of mistakes from the two sources. We then explore human vs. machine expertise after ranking tasks by difficulty levels. Even when humans and machines have similar overall accuracies, the distribution of answers may vary. Leveraging the perceptual difference between humans and machines, we empirically demonstrate a post-hoc human-machine collaboration that outperforms humans or machines alone.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2190694980",
                        "name": "Minghao Liu"
                    },
                    {
                        "authorId": "103410241",
                        "name": "Jiaheng Wei"
                    },
                    {
                        "authorId": "40457423",
                        "name": "Yang Liu"
                    },
                    {
                        "authorId": "2111092733",
                        "name": "James Davis"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[5], Attention Rollout [1], and Grad-CAM [19].",
                "The former is characterized by the use of the gradients of a model\u2019s output with respect to a layer\u2019s input as an indicator of importance [19, 21, 22], while the latter relies on the Deep Taylor Decomposition method [16] to recursively break down the model\u2019s output into the contributions of each layer [3, 5, 20]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "516c9f1eeb17a7d5a74b0f1bfaccad0664f01e37",
                "externalIds": {
                    "DBLP": "conf/cvpr/NalmpantisPGPA23",
                    "ArXiv": "2304.06391",
                    "DOI": "10.1109/CVPRW59228.2023.00388",
                    "CorpusId": 258108264
                },
                "corpusId": 258108264,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/516c9f1eeb17a7d5a74b0f1bfaccad0664f01e37",
                "title": "Vision DiffMask: Faithful Interpretation of Vision Transformers with Differentiable Patch Masking",
                "abstract": "The lack of interpretability of the Vision Transformer may hinder its use in critical real-world applications despite its effectiveness. To overcome this issue, we propose a post-hoc interpretability method called Vision DiffMask, which uses the activations of the model\u2019s hidden layers to predict the relevant parts of the input that contribute to its final predictions. Our approach uses a gating mechanism to identify the minimal subset of the original input that preserves the predicted distribution over classes. We demonstrate the faithfulness of our method, by introducing a faithfulness task, and comparing it to other state-of-the-art attribution methods on CIFAR-10 and ImageNet-1K, achieving compelling results. To aid reproducibility and further extension of our work, we open source our implementation here.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2139701509",
                        "name": "A. Nalmpantis"
                    },
                    {
                        "authorId": "31713235",
                        "name": "Apostolos Panagiotopoulos"
                    },
                    {
                        "authorId": "2214279295",
                        "name": "John Gkountouras"
                    },
                    {
                        "authorId": "49321317",
                        "name": "Konstantinos Papakostas"
                    },
                    {
                        "authorId": "2782694",
                        "name": "Wilker Aziz"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Recently, a Layerwise Relevance Propagation for Transformers (TransLRP) approach [6] was introduced to",
                "utilized a perturbation metric that demonstrated the superiority of their method over others significantly [6].",
                "We aim to perform a quantitative evaluation of the interpretation method adapted specifically to ViTs [6], in comparison to model-agnostic [25] and attention-based interpretation methods [4], on the example of medical imaging.",
                "For TransLRP we utilize the implementation from the original work [6].",
                "As a current state-of-the-art approach to explaining ViTs, we rely on the TransLRP algorithm proposed in [6].",
                "Several XAI methods have been proposed or adapted for ViTs [1, 6], yet a rigorous and standardized evaluation of these methods in terms of their quality of explanations is still lacking."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "1bba254ecfc356a3e383db7af48794ece8bea3f0",
                "externalIds": {
                    "ArXiv": "2304.06133",
                    "DBLP": "conf/cvpr/KomorowskiBB23",
                    "DOI": "10.1109/CVPRW59228.2023.00383",
                    "CorpusId": 258107832
                },
                "corpusId": 258107832,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1bba254ecfc356a3e383db7af48794ece8bea3f0",
                "title": "Towards Evaluating Explanations of Vision Transformers for Medical Imaging",
                "abstract": "As deep learning models increasingly find applications in critical domains such as medical imaging, the need for transparent and trustworthy decision-making becomes paramount. Many explainability methods provide insights into how these models make predictions by attributing importance to input features. As Vision Transformer (ViT) becomes a promising alternative to convolutional neural networks for image classification, its interpretability remains an open research question. This paper investigates the performance of various interpretation methods on a ViT applied to classify chest X-ray images. We introduce the notion of evaluating faithfulness, sensitivity, and complexity of ViT explanations. The obtained results indicate that Layerwise relevance propagation for transformers outperforms Local interpretable model-agnostic explanations and Attention visualization, providing a more accurate and reliable representation of what a ViT has actually learned. Our findings provide insights into the applicability of ViT explanations in medical imaging and highlight the importance of using appropriate evaluation criteria for comparing them.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "9140985",
                        "name": "P. Komorowski"
                    },
                    {
                        "authorId": "1453583908",
                        "name": "Hubert Baniecki"
                    },
                    {
                        "authorId": "144356944",
                        "name": "P. Biecek"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "The problem of noisy activation is pervasive across various research papers, including explainability methods for convolutional neural networks (CNNs) [47,56], vision transformers [9], and CLIP [31,10].",
                "with previous explainability works, including similarity map of original CLIP, Grad-CAM [47] for CNN, pLRP [28] implemented by [9] for multiple layers, Bi-",
                "CLIP; see results of Bi-Modal [8] built upon explainability of ViT [9] and gScoreCAM [10] for CLIP-based localization in Fig.",
                "Recent methods [9] have focused on explainability for vision transformers [16] based on self-attention and gradient."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "682ec56de6a1f4df4bb95bcb992250a4311ae09f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-05653",
                    "ArXiv": "2304.05653",
                    "DOI": "10.48550/arXiv.2304.05653",
                    "CorpusId": 258079047
                },
                "corpusId": 258079047,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/682ec56de6a1f4df4bb95bcb992250a4311ae09f",
                "title": "CLIP Surgery for Better Explainability with Enhancement in Open-Vocabulary Tasks",
                "abstract": "Contrastive Language-Image Pre-training (CLIP) is a powerful multimodal large vision model that has demonstrated significant benefits for downstream tasks, including many zero-shot learning and text-guided vision tasks. However, we notice some severe problems regarding the model's explainability, which undermines its credibility and impedes related tasks. Specifically, we find CLIP prefers the background regions than the foregrounds according to the predicted similarity map, which contradicts human understanding. Besides, there are obvious noisy activations on the visualization results at irrelevant positions. To address these two issues, we conduct in-depth analyses and reveal the reasons with new findings and evidences. Based on these insights, we propose the CLIP Surgery, a method that enables surgery-like modifications for the inference architecture and features, for better explainability and enhancement in multiple open-vocabulary tasks. The proposed method has significantly improved the explainability of CLIP for both convolutional networks and vision transformers, surpassing existing methods by large margins. Besides, our approach also demonstrates remarkable improvements in open-vocabulary segmentation and multi-label recognition tasks. For examples, the mAP improvement on NUS-Wide multi-label recognition is 4.41% without any additional training, and our CLIP Surgery surpasses the state-of-the-art method by 8.74% at mIoU on Cityscapes open-vocabulary semantic segmentation. Furthermore, our method benefits other tasks including multimodal visualization and interactive segmentation like Segment Anything Model (SAM). The code is available at https://github.com/xmed-lab/CLIP_Surgery",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2153682292",
                        "name": "Yi Li"
                    },
                    {
                        "authorId": "2155981033",
                        "name": "Hualiang Wang"
                    },
                    {
                        "authorId": "2107552104",
                        "name": "Yiqun Duan"
                    },
                    {
                        "authorId": "48569608",
                        "name": "X. Li"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4cc38369647d1cb0ceef7011aed9bc5176ab411f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-04902",
                    "ArXiv": "2304.04902",
                    "DOI": "10.59275/j.melba.2023-553a",
                    "CorpusId": 258060261
                },
                "corpusId": 258060261,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4cc38369647d1cb0ceef7011aed9bc5176ab411f",
                "title": "Weakly Supervised Intracranial Hemorrhage Segmentation using Head-Wise Gradient-Infused Self-Attention Maps from a Swin Transformer in Categorical Learning",
                "abstract": "Intracranial hemorrhage (ICH) is a life-threatening medical emergency that requires timely and accurate diagnosis for effective treatment and improved patient survival rates. While deep learning techniques have emerged as the leading approach for medical image analysis and processing, the most commonly employed supervised learning often requires large, high-quality annotated datasets that can be costly to obtain, particularly for pixel/voxel-wise image segmentation. To address this challenge and facilitate ICH treatment decisions, we introduce a novel weakly supervised method for ICH segmentation, utilizing a Swin transformer trained on an ICH classification task with categorical labels. Our approach leverages a hierarchical combination of head-wise gradient-infused self-attention maps to generate accurate image segmentation. Additionally, we conducted an exploratory study on different learning strategies and showed that binary ICH classification has a more positive impact on self-attention maps compared to full ICH subtyping. With a mean Dice score of 0.44, our technique achieved similar ICH segmentation performance as the popular U-Net and Swin-UNETR models with full supervision and outperformed a similar weakly supervised approach using GradCAM, demonstrating the excellent potential of the proposed framework in challenging medical image segmentation tasks. Our code is available at https://github.com/HealthX-Lab/HGI-SAM",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "90986328",
                        "name": "Amir Rasoulian"
                    },
                    {
                        "authorId": "49423545",
                        "name": "Soorena Salari"
                    },
                    {
                        "authorId": "2160866049",
                        "name": "Yiming Xiao"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[9] proposed a new method for information propagation within Transformer model components based on LRP attribution, which comprehensively understands the decision-making and inference processes within the model."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2e46788980581212aba95915a287aa38acd635f3",
                "externalIds": {
                    "ArXiv": "2304.04354",
                    "DBLP": "journals/corr/abs-2304-04354",
                    "DOI": "10.48550/arXiv.2304.04354",
                    "CorpusId": 258048607
                },
                "corpusId": 258048607,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2e46788980581212aba95915a287aa38acd635f3",
                "title": "ViT-Calibrator: Decision Stream Calibration for Vision Transformer",
                "abstract": "A surge of interest has emerged in utilizing Transformers in diverse vision tasks owing to its formidable performance. However, existing approaches primarily focus on optimizing internal model architecture designs that often entail significant trial and error with high burdens. In this work, we propose a new paradigm dubbed Decision Stream Calibration that boosts the performance of general Vision Transformers. To achieve this, we shed light on the information propagation mechanism in the learning procedure by exploring the correlation between different tokens and the relevance coefficient of multiple dimensions. Upon further analysis, it was discovered that 1) the final decision is associated with tokens of foreground targets, while token features of foreground target will be transmitted into the next layer as much as possible, and the useless token features of background area will be eliminated gradually in the forward propagation. 2) Each category is solely associated with specific sparse dimensions in the tokens. Based on the discoveries mentioned above, we designed a two-stage calibration scheme, namely ViT-Calibrator, including token propagation calibration stage and dimension propagation calibration stage. Extensive experiments on commonly used datasets show that the proposed approach can achieve promising results. The source codes are given in the supplements.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2213718755",
                        "name": "Lin Chen"
                    },
                    {
                        "authorId": "2207750353",
                        "name": "Zhijie Jia"
                    },
                    {
                        "authorId": "2046931991",
                        "name": "Tian Qiu"
                    },
                    {
                        "authorId": "26953623",
                        "name": "Lechao Cheng"
                    },
                    {
                        "authorId": "2052835090",
                        "name": "Jie Lei"
                    },
                    {
                        "authorId": "7357719",
                        "name": "Zunlei Feng"
                    },
                    {
                        "authorId": "2075692",
                        "name": "Min-Gyoo Song"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a92488e32c9f5a85d58ecd06169db9e095f7bccc",
                "externalIds": {
                    "ArXiv": "2304.03696",
                    "CorpusId": 258041192
                },
                "corpusId": 258041192,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a92488e32c9f5a85d58ecd06169db9e095f7bccc",
                "title": "MOPA: Modular Object Navigation with PointGoal Agents",
                "abstract": "We propose a simple but effective modular approach MOPA (Modular ObjectNav with PointGoal agents) to systematically investigate the inherent modularity of the object navigation task in Embodied AI. MOPA consists of four modules: (a) an object detection module trained to identify objects from RGB images, (b) a map building module to build a semantic map of the observed objects, (c) an exploration module enabling the agent to explore the environment, and (d) a navigation module to move to identified target objects. We show that we can effectively reuse a pretrained PointGoal agent as the navigation model instead of learning to navigate from scratch, thus saving time and compute. We also compare various exploration strategies for MOPA and find that a simple uniform strategy significantly outperforms more advanced exploration methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1752600856",
                        "name": "Sonia Raychaudhuri"
                    },
                    {
                        "authorId": "1900358319",
                        "name": "Tommaso Campari"
                    },
                    {
                        "authorId": "10680632",
                        "name": "Unnat Jain"
                    },
                    {
                        "authorId": "2295141",
                        "name": "M. Savva"
                    },
                    {
                        "authorId": "145830541",
                        "name": "Angel X. Chang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fc0ef4f5bdf67d97a698844f84d7fa69ebb6e9cd",
                "externalIds": {
                    "DOI": "10.1117/12.2654310",
                    "CorpusId": 257991524
                },
                "corpusId": 257991524,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fc0ef4f5bdf67d97a698844f84d7fa69ebb6e9cd",
                "title": "Single-view cone beam CT reconstruction with Swin transformer based deep learning",
                "abstract": "Cone beam CT (CBCT) imaging with sparse-view can effectively reduce the radiation dose risk. The convolution-based end-to-end deep learning methods have been used in single-view CBCT image reconstruction, which can minimize radiation dose and achieve fast CBCT imaging. However, these methods ignore the mismatch between the local feature extraction ability of convolutional neural network (CNN) and the global features of the projection image. To address this issue, we propose a novel deep learning network architecture based on Swin Transformer for single-view CBCT reconstruction. First, we use the Swin Transformer network block to construct a single-view projection feature extraction module, then through the feature transformation module, we convert the 2D features learned from projection into 3D feature tensors, and finally get the 3D volume image in the generative network. This paper is the first attempt to use the Swin Transformer model for single-view CBCT reconstruction. Experimental results with the mouse datasets demonstrate that the proposed model outperforms the convolution-based end-to-end deep learning model in reducing artifacts and preserving the image accuracy.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2213784609",
                        "name": "Shien Huang"
                    },
                    {
                        "authorId": "1682580",
                        "name": "Yonghong Song"
                    },
                    {
                        "authorId": "48726702",
                        "name": "Junyan Rong"
                    },
                    {
                        "authorId": "20857478",
                        "name": "Tianshuai Liu"
                    },
                    {
                        "authorId": "145252512",
                        "name": "Dong Huang"
                    },
                    {
                        "authorId": "2156273817",
                        "name": "Hongbing Lu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[9, 8], however, individual attention maps provide limited representation of the overall behavior of the model.",
                "In transformer-based architectures, the extracted attention maps from each layer represent a good indicator of support activations for the final classification, but they do not reflect the combined attention scores and the other components of the transformer model [9]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5a94211bb94fd7feaeb79011044b84f0e230ef2b",
                "externalIds": {
                    "ArXiv": "2304.14505",
                    "DBLP": "journals/corr/abs-2304-14505",
                    "DOI": "10.48550/arXiv.2304.14505",
                    "CorpusId": 258418283
                },
                "corpusId": 258418283,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5a94211bb94fd7feaeb79011044b84f0e230ef2b",
                "title": "Transformer-based interpretable multi-modal data fusion for skin lesion classification",
                "abstract": "A lot of deep learning (DL) research these days is mainly focused on improving quantitative metrics regardless of other factors. In human-centered applications, like skin lesion classification in dermatology, DL-driven clinical decision support systems are still in their infancy due to the limited transparency of their decision-making process. Moreover, the lack of procedures that can explain the behavior of trained DL algorithms leads to almost no trust from clinical physicians. To diagnose skin lesions, dermatologists rely on visual assessment of the disease and the data gathered from the patient's anamnesis. Data-driven algorithms dealing with multi-modal data are limited by the separation of feature-level and decision-level fusion procedures required by convolutional architectures. To address this issue, we enable single-stage multi-modal data fusion via the attention mechanism of transformer-based architectures to aid in diagnosing skin diseases. Our method beats other state-of-the-art single- and multi-modal DL architectures in image-rich and patient-data-rich environments. Additionally, the choice of the architecture enables native interpretability support for the classification task both in the image and metadata domain with no additional modifications necessary.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2174667546",
                        "name": "Theodor Cheslerean-Boghiu"
                    },
                    {
                        "authorId": "2215779896",
                        "name": "Melia-Evelina Fleischmann"
                    },
                    {
                        "authorId": "2138420994",
                        "name": "Theresa Willem"
                    },
                    {
                        "authorId": "20823452",
                        "name": "Tobias Lasser"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cab4a514d473cfc066dc0be01e35c7a5fbcf7fcb",
                "externalIds": {
                    "DOI": "10.1101/2023.03.31.532253",
                    "CorpusId": 257927583
                },
                "corpusId": 257927583,
                "publicationVenue": {
                    "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
                    "name": "bioRxiv",
                    "type": "journal",
                    "url": "http://biorxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/cab4a514d473cfc066dc0be01e35c7a5fbcf7fcb",
                "title": "Clinical Phenotype Prediction From Single-cell RNA-seq Data using Attention-Based Neural Networks",
                "abstract": "Motivation A patient\u2019s disease phenotype can be driven and determined by specific groups of cells whose marker genes are either unknown, or can only be detected at late-stage using conventional bulk assays such as RNA-Seq technology. Recent advances in single-cell RNA sequencing (scRNA-seq) enable gene expression profiling in cell-level resolution, and therefore have the potential to identify those cells driving the disease phenotype even while the number of these cells is small. However, most existing methods rely heavily on accurate cell type detection, and the number of available annotated samples is usually too small for training deep learning predictive models. Results Here we propose the method ScRAT for clinical phenotype prediction using scRNA-seq data. To train ScRAT with a limited number of samples of different phenotypes, such as COVID and non-COVID, ScRAT first applies a mixup module to increase the number of training samples. A multi-head attention mechanism is employed to learn the most informative cells for each phenotype without relying on a given cell type annotation. Using three public COVID datasets, we show that ScRAT outperforms other phenotype prediction methods. The performance edge of ScRAT over its competitors increases as the number of training samples decreases, indicating the efficacy of our sample mixup. Critical cell types detected based on high-attention cells also support novel findings in the original papers and the recent literature. This suggests that ScRAT overcomes the challenge of missing marker genes and limited sample number with great potential revealing novel molecular mechanisms and/or therapies.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2195055368",
                        "name": "Yuzhen Mao"
                    },
                    {
                        "authorId": "1959139",
                        "name": "Yen-Yi Lin"
                    },
                    {
                        "authorId": "12125147",
                        "name": "N. Wong"
                    },
                    {
                        "authorId": "144296776",
                        "name": "S. Volik"
                    },
                    {
                        "authorId": "1752803465",
                        "name": "F. Sar"
                    },
                    {
                        "authorId": "144164888",
                        "name": "C. Collins"
                    },
                    {
                        "authorId": "1766588",
                        "name": "M. Ester"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Consequently, the class-regional token attention relevance scores [2] are observed in left Fig.",
                "(a) Relevance maps [2] visualizing attention between [CLS] and regional token (Left: vanilla ViT, Right: SAT)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "06f8a72d75c01d07be670e44ff94657f834c8b90",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-16557",
                    "ArXiv": "2303.16557",
                    "DOI": "10.48550/arXiv.2303.16557",
                    "CorpusId": 257804930
                },
                "corpusId": 257804930,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/06f8a72d75c01d07be670e44ff94657f834c8b90",
                "title": "Self-accumulative Vision Transformer for Bone Age Assessment Using the Sauvegrain Method",
                "abstract": "This study presents a novel approach to bone age assessment (BAA) using a multi-view, multi-task classification model based on the Sauvegrain method. A straightforward solution to automating the Sauvegrain method, which assesses a maturity score for each landmark in the elbow and predicts the bone age, is to train classifiers independently to score each region of interest (RoI), but this approach limits the accessible information to local morphologies and increases computational costs. As a result, this work proposes a self-accumulative vision transformer (SAT) that mitigates anisotropic behavior, which usually occurs in multi-view, multi-task problems and limits the effectiveness of a vision transformer, by applying token replay and regional attention bias. A number of experiments show that SAT successfully exploits the relationships between landmarks and learns global morphological features, resulting in a mean absolute error of BAA that is 0.11 lower than that of the previous work. Additionally, the proposed SAT has four times reduced parameters than an ensemble of individual classifiers of the previous work. Lastly, this work also provides informative implications for clinical practice, improving the accuracy and efficiency of BAA in diagnosing abnormal growth in adolescents.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2166907133",
                        "name": "Hong-Jun Choi"
                    },
                    {
                        "authorId": "2059662364",
                        "name": "Dongbin Na"
                    },
                    {
                        "authorId": "2111051486",
                        "name": "Kyungjin Cho"
                    },
                    {
                        "authorId": "2212859447",
                        "name": "Byunguk Bae"
                    },
                    {
                        "authorId": "27074567",
                        "name": "Seo Taek Kong"
                    },
                    {
                        "authorId": "81793351",
                        "name": "Hyun-Suk An"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Visualizing attention is the basis of saliency map approaches specific to Computer Vision for Vision Transformers [24, 25]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "888a502ea0c37dbace57305cb55197749d2705ea",
                "externalIds": {
                    "ArXiv": "2303.15190",
                    "DBLP": "journals/corr/abs-2303-15190",
                    "DOI": "10.48550/arXiv.2303.15190",
                    "CorpusId": 257767013
                },
                "corpusId": 257767013,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/888a502ea0c37dbace57305cb55197749d2705ea",
                "title": "Evaluating self-attention interpretability through human-grounded experimental protocol",
                "abstract": "Attention mechanisms have played a crucial role in the development of complex architectures such as Transformers in natural language processing. However, Transformers remain hard to interpret and are considered as black-boxes. This paper aims to assess how attention coefficients from Transformers can help in providing interpretability. A new attention-based interpretability method called CLaSsification-Attention (CLS-A) is proposed. CLS-A computes an interpretability score for each word based on the attention coefficient distribution related to the part specific to the classification task within the Transformer architecture. A human-grounded experiment is conducted to evaluate and compare CLS-A to other interpretability methods. The experimental protocol relies on the capacity of an interpretability method to provide explanation in line with human reasoning. Experiment design includes measuring reaction times and correct response rates by human subjects. CLS-A performs comparably to usual interpretability methods regarding average participant reaction time and accuracy. The lower computational cost of CLS-A compared to other interpretability methods and its availability by design within the classifier make it particularly interesting. Data analysis also highlights the link between the probability score of a classifier prediction and adequate explanations. Finally, our work confirms the relevancy of the use of CLS-A and shows to which extent self-attention contains rich information to explain Transformer classifiers.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2212794108",
                        "name": "Milan Bhan"
                    },
                    {
                        "authorId": "2149705451",
                        "name": "Nina Achache"
                    },
                    {
                        "authorId": "2212283861",
                        "name": "Victor Legrand"
                    },
                    {
                        "authorId": "2338391",
                        "name": "A. Blangero"
                    },
                    {
                        "authorId": "21265854",
                        "name": "N. Chesneau"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "6 with the Transformer model\u2019s attention visualization tool provided by Chefer [30].",
                "We show the attention distribution of ViT and SViT on images in Fig.6 with the Transformer model\u2019s attention visualization tool provided by Chefer [30]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "75c0cbadf456978441a608575bf17b9351ead493",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-14645",
                    "ArXiv": "2303.14645",
                    "DOI": "10.48550/arXiv.2303.14645",
                    "CorpusId": 257766442
                },
                "corpusId": 257766442,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/75c0cbadf456978441a608575bf17b9351ead493",
                "title": "Sector Patch Embedding: An Embedding Module Conforming to The Distortion Pattern of Fisheye Image",
                "abstract": "Fisheye cameras suffer from image distortion while having a large field of view(LFOV). And this fact leads to poor performance on some fisheye vision tasks. One of the solutions is to optimize the current vision algorithm for fisheye images. However, most of the CNN-based methods and the Transformer-based methods lack the capability of leveraging distortion information efficiently. In this work, we propose a novel patch embedding method called Sector Patch Embedding(SPE), conforming to the distortion pattern of the fisheye image. Furthermore, we put forward a synthetic fisheye dataset based on the ImageNet-1K and explore the performance of several Transformer models on the dataset. The classification top-1 accuracy of ViT and PVT is improved by 0.75% and 2.8% with SPE respectively. The experiments show that the proposed sector patch embedding method can better perceive distortion and extract features on the fisheye images. Our method can be easily adopted to other Transformer-based models. Source code is at https://github.com/IN2-ViAUn/Sector-Patch-Embedding.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2107658954",
                        "name": "Dian Yang"
                    },
                    {
                        "authorId": "89460562",
                        "name": "Jiadong Tang"
                    },
                    {
                        "authorId": "2143443111",
                        "name": "Yu Gao"
                    },
                    {
                        "authorId": "2143685743",
                        "name": "Yi Yang"
                    },
                    {
                        "authorId": "35556335",
                        "name": "M. Fu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "result"
            ],
            "contexts": [
                "Specifically, we compare the results using raw attention, CAM (Zhou et al., 2016), and Transformer-LRP (Chefer et al., 2021) in Table 4.",
                ", 2016), and Transformer-LRP (Chefer et al., 2021) in Table 4.",
                "To figure out how interpretability contributes to the model prediction, we adopt the positive and negative perturbation tests presented in (Chefer et al., 2021) to give both quantitative and qualitative evaluations on the interpretability.",
                "Decomposition-based approaches (Bach et al., 2015; Montavon et al., 2017; Chefer et al., 2021) propagate the final prediction to the input following the Deep Taylor Decomposition (Montavon et al., 2017).",
                "3 EVALUATION OF THE INTERPRETABILITY To figure out how interpretability contributes to the model prediction, we adopt the positive and negative perturbation tests presented in (Chefer et al., 2021) to give both quantitative and qualitative evaluations on the interpretability.",
                "Although Luo et al. (2016) propose to measure the ERF for CNNs, it cannot be directly implemented to Transformer-base models.",
                "Decomposition-based approaches (Bach et al., 2015; Montavon et al., 2017; Chefer et al., 2021) propagate the final prediction to the input following the Deep Taylor Decomposition (Montavon et al.",
                "As most previous methods focus on CNNs, Chefer et al. (2021) propose ViT-LRP tailored for vision Transformers."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "eed4caa4d1dc631cf6317903b410408abeef21f0",
                "externalIds": {
                    "ArXiv": "2303.06635",
                    "DBLP": "conf/iclr/ZhangXL0SS23",
                    "DOI": "10.48550/arXiv.2303.06635",
                    "CorpusId": 257496457
                },
                "corpusId": 257496457,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/eed4caa4d1dc631cf6317903b410408abeef21f0",
                "title": "Schema Inference for Interpretable Image Classification",
                "abstract": "In this paper, we study a novel inference paradigm, termed as schema inference, that learns to deductively infer the explainable predictions by rebuilding the prior deep neural network (DNN) forwarding scheme, guided by the prevalent philosophical cognitive concept of schema. We strive to reformulate the conventional model inference pipeline into a graph matching policy that associates the extracted visual concepts of an image with the pre-computed scene impression, by analogy with human reasoning mechanism via impression matching. To this end, we devise an elaborated architecture, termed as SchemaNet, as a dedicated instantiation of the proposed schema inference concept, that models both the visual semantics of input instances and the learned abstract imaginations of target categories as topological relational graphs. Meanwhile, to capture and leverage the compositional contributions of visual semantics in a global view, we also introduce a universal Feat2Graph scheme in SchemaNet to establish the relational graphs that contain abundant interaction information. Both the theoretical analysis and the experimental results on several benchmarks demonstrate that the proposed schema inference achieves encouraging performance and meanwhile yields a clear picture of the deductive process leading to the predictions. Our code is available at https://github.com/zhfeing/SchemaNet-PyTorch.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1739347431",
                        "name": "Haofei Zhang"
                    },
                    {
                        "authorId": "2065788410",
                        "name": "Mengqi Xue"
                    },
                    {
                        "authorId": "2109002553",
                        "name": "Xiaokang Liu"
                    },
                    {
                        "authorId": "145937448",
                        "name": "Kaixuan Chen"
                    },
                    {
                        "authorId": "40403685",
                        "name": "Jie Song"
                    },
                    {
                        "authorId": "2152127912",
                        "name": "Mingli Song"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e60b6836b45ad0ae02a5fa663c8c31119f0c0a94",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-04935",
                    "ArXiv": "2303.04935",
                    "DOI": "10.1109/CVPR52729.2023.02333",
                    "CorpusId": 257427497
                },
                "corpusId": 257427497,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e60b6836b45ad0ae02a5fa663c8c31119f0c0a94",
                "title": "X-Pruner: eXplainable Pruning for Vision Transformers",
                "abstract": "Recently vision transformer models have become prominent models for a range of tasks. These models, however, usually suffer from intensive computational costs and heavy memory requirements, making them impractical for deployment on edge platforms. Recent studies have proposed to prune transformers in an unexplainable manner, which overlook the relationship between internal units of the model and the target class, thereby leading to inferior performance. To alleviate this problem, we propose a novel explainable pruning framework dubbed X-Pruner, which is designed by considering the explainability of the pruning criterion. Specifically, to measure each prunable unit's contribution to predicting each target class, a novel explainability-aware mask is proposed and learned in an end-to-end manner. Then, to preserve the most informative units and learn the layer-wise pruning rate, we adaptively search the layer-wise threshold that differentiates between unpruned and pruned units based on their explainability-aware mask values. To verify and evaluate our method, we apply the X-Pruner on representative transformer models including the DeiT and Swin Transformer. Comprehensive simulation results demonstrate that the proposed X-Pruner outperforms the state-of-the-art black-box methods with significantly reduced computational costs and slight performance degradation. Code is available at https://github.com/vickyyu90/XPruner.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2175073154",
                        "name": "Lu Yu"
                    },
                    {
                        "authorId": "2052727473",
                        "name": "Wei Xiang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "\u2026challenging vision related tasks such as image segmentation [Amit et al. 2021], domain adaptation [Song et al. 2022], image editing [Avrahami et al. 2022; Hertz et al. 2022; Tumanyan et al. 2022a], personalization [Gal\net al. 2022, 2023; Ruiz et al. 2022], and explainability [Chefer et al. 2021]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6be1fa0513ce9f758997149a96d6181c2d51ce78",
                "externalIds": {
                    "DBLP": "journals/tog/IluzVHBCS23",
                    "ArXiv": "2303.01818",
                    "DOI": "10.1145/3592123",
                    "CorpusId": 257353586
                },
                "corpusId": 257353586,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6be1fa0513ce9f758997149a96d6181c2d51ce78",
                "title": "Word-As-Image for Semantic Typography",
                "abstract": "A word-as-image is a semantic typography technique where a word illustration presents a visualization of the meaning of the word, while also preserving its readability. We present a method to create word-as-image illustrations automatically. This task is highly challenging as it requires semantic understanding of the word and a creative idea of where and how to depict these semantics in a visually pleasing and legible manner. We rely on the remarkable ability of recent large pretrained language-vision models to distill textual concepts visually. We target simple, concise, black-and-white designs that convey the semantics clearly. We deliberately do not change the color or texture of the letters and do not use embellishments. Our method optimizes the outline of each letter to convey the desired concept, guided by a pretrained Stable Diffusion model. We incorporate additional loss terms to ensure the legibility of the text and the preservation of the style of the font. We show high quality and engaging results on numerous examples and compare to alternative techniques. Code and demo will be available at our project page.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "114105645",
                        "name": "Shira Iluz"
                    },
                    {
                        "authorId": "1630270601",
                        "name": "Yael Vinker"
                    },
                    {
                        "authorId": "51437320",
                        "name": "Amir Hertz"
                    },
                    {
                        "authorId": "1850521",
                        "name": "Daniel Berio"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    },
                    {
                        "authorId": "2947946",
                        "name": "Ariel Shamir"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b7e0cda123d73a6f5521d388b830b445fdf8c84c",
                "externalIds": {
                    "ArXiv": "2303.01871",
                    "DBLP": "journals/corr/abs-2303-01871",
                    "DOI": "10.1148/ryai.220187",
                    "CorpusId": 257289817,
                    "PubMed": "37035429"
                },
                "corpusId": 257289817,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b7e0cda123d73a6f5521d388b830b445fdf8c84c",
                "title": "Attention-based Saliency Maps Improve Interpretability of Pneumothorax Classification",
                "abstract": "Purpose\nTo investigate the chest radiograph classification performance of vision transformers (ViTs) and interpretability of attention-based saliency maps, using the example of pneumothorax classification.\n\n\nMaterials and Methods\nIn this retrospective study, ViTs were fine-tuned for lung disease classification using four public datasets: CheXpert, Chest X-Ray 14, MIMIC CXR, and VinBigData. Saliency maps were generated using transformer multimodal explainability and gradient-weighted class activation mapping (GradCAM). Classification performance was evaluated on the Chest X-Ray 14, VinBigData, and Society for Imaging Informatics in Medicine-American College of Radiology (SIIM-ACR) Pneumothorax Segmentation datasets using the area under the receiver operating characteristic curve (AUC) analysis and compared with convolutional neural networks (CNNs). The explainability methods were evaluated with positive and negative perturbation, sensitivity-n, effective heat ratio, intra-architecture repeatability, and interarchitecture reproducibility. In the user study, three radiologists classified 160 chest radiographs with and without saliency maps for pneumothorax and rated their usefulness.\n\n\nResults\nViTs had comparable chest radiograph classification AUCs compared with state-of-the-art CNNs: 0.95 (95% CI: 0.94, 0.95) versus 0.83 (95%, CI 0.83, 0.84) on Chest X-Ray 14, 0.84 (95% CI: 0.77, 0.91) versus 0.83 (95% CI: 0.76, 0.90) on VinBigData, and 0.85 (95% CI: 0.85, 0.86) versus 0.87 (95% CI: 0.87, 0.88) on SIIM-ACR. Both saliency map methods unveiled a strong bias toward pneumothorax tubes in the models. Radiologists found 47% of the attention-based and 39% of the GradCAM saliency maps useful. The attention-based methods outperformed GradCAM on all metrics.\n\n\nConclusion\nViTs performed similarly to CNNs in chest radiograph classification, and their attention-based saliency maps were more useful to radiologists and outperformed GradCAM.Keywords: Conventional Radiography, Thorax, Diagnosis, Supervised Learning, Convolutional Neural Network (CNN) Online supplemental material is available for this article. \u00a9 RSNA, 2023.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2180022520",
                        "name": "Alessandro Wollek"
                    },
                    {
                        "authorId": "2112166232",
                        "name": "R. Graf"
                    },
                    {
                        "authorId": "1860958691",
                        "name": "Sa\u0161a \u010ce\u010datka"
                    },
                    {
                        "authorId": "1828831326",
                        "name": "N. Fink"
                    },
                    {
                        "authorId": "2138420994",
                        "name": "Theresa Willem"
                    },
                    {
                        "authorId": "31463178",
                        "name": "B. Sabel"
                    },
                    {
                        "authorId": "20823452",
                        "name": "Tobias Lasser"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                ", 2022), data augmentation (Shorten & Khoshgoftaar, 2019), interpretability (Chefer et al., 2021), self-supervised pretraining (He et al.",
                "\u2026of fast-evolving and well-studied computer vision techniques in the time series domain, such as better model architecture (Liu et al., 2022), data augmentation (Shorten & Khoshgoftaar, 2019), interpretability (Chefer et al., 2021), self-supervised pretraining (He et al., 2022), to name a few."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "668fe862cbfcab26b67dd81b4cefa6c0cf11adab",
                "externalIds": {
                    "ArXiv": "2303.12799",
                    "DBLP": "journals/corr/abs-2303-12799",
                    "DOI": "10.48550/arXiv.2303.12799",
                    "CorpusId": 257687717
                },
                "corpusId": 257687717,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/668fe862cbfcab26b67dd81b4cefa6c0cf11adab",
                "title": "Time Series as Images: Vision Transformer for Irregularly Sampled Time Series",
                "abstract": "Irregularly sampled time series are becoming increasingly prevalent in various domains, especially in medical applications. Although different highly-customized methods have been proposed to tackle irregularity, how to effectively model their complicated dynamics and high sparsity is still an open problem. This paper studies the problem from a whole new perspective: transforming irregularly sampled time series into line graph images and adapting powerful vision transformers to perform time series classification in the same way as image classification. Our approach largely simplifies algorithm designs without assuming prior knowledge and can be potentially extended as a general-purpose framework. Despite its simplicity, we show that it substantially outperforms state-of-the-art specialized algorithms on several popular healthcare and human activity datasets. Especially in the challenging leave-sensors-out setting where a subset of variables is masked during testing, the performance improvement is up to 54.0\\% in absolute F1 score points. Our code and data are available at \\url{https://github.com/Leezekun/ViTST}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2168519132",
                        "name": "Zekun Li"
                    },
                    {
                        "authorId": "50341591",
                        "name": "SHIYANG LI"
                    },
                    {
                        "authorId": "1740249",
                        "name": "Xifeng Yan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "3 Self-Attention Self-Attention (SA) draws on the attention mechanism of the human brain when looking at objects, and only selects some key information inputs for processing to improve the efficiency of the neural network[16], [17]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d6a78a832912cd006261b97965a8159ce98f157c",
                "externalIds": {
                    "DOI": "10.1117/12.2667212",
                    "CorpusId": 257308105
                },
                "corpusId": 257308105,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d6a78a832912cd006261b97965a8159ce98f157c",
                "title": "Near-Earth aircraft wake vortex recognition based on multiple LIDAR and transformer",
                "abstract": "Along with the rapid development of the air transportation industry, the impact of aircraft wake vortices on flight safety and airport capacity has become increasingly prominent. In this paper, we propose a transformer-based model to solve the problem of multiple LIDAR wake vortex detection and recognition in airports. By setting up multiple Doppler LIDARs in the near-Earth flight areas of different runways of Shenzhen Baoan Airport (SZX), a large amount of accurate wind field data is captured for wake vortex data collection. In the deep learning framework, the radial velocity sequence obtained from the LIDAR is used as the input of the transformer. Meanwhile, local meteorological information and LIDAR operating parameters are introduced into the model, providing prior knowledge at different observation points. The experimental results show that the model has unified modeling for different LIDAR wake vortex detection, and has obtained excellent recognition results.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2075362869",
                        "name": "Weijun Pan"
                    },
                    {
                        "authorId": "46991242",
                        "name": "An-ning Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Some more recent works have also proposed versions of post-hoc algorithms tailored for the transformer model.(27,28)"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "826536dec2667b42c55d7708dcd545bb16925a3a",
                "externalIds": {
                    "DOI": "10.1117/1.JEI.32.2.020801",
                    "CorpusId": 258323714
                },
                "corpusId": 258323714,
                "publicationVenue": {
                    "id": "c677ab24-0c04-487d-83e2-c252af9479c8",
                    "name": "Journal of Electronic Imaging (JEI)",
                    "type": "journal",
                    "alternate_names": [
                        "J Electron Imaging (JEI",
                        "Journal of Electronic Imaging",
                        "J Electron Imaging"
                    ],
                    "issn": "1017-9909",
                    "url": "https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging",
                    "alternate_urls": [
                        "http://electronicimaging.spiedigitallibrary.org/journal.aspx"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/826536dec2667b42c55d7708dcd545bb16925a3a",
                "title": "Computing and evaluating saliency maps for image classification: a tutorial",
                "abstract": "Abstract. Facing the black-box nature of deep learning models for image classification, a popular trend in the literature proposes methods to generate explanations in the form of heat maps indicating the areas that played an important role in the models\u2019 decisions. Such explanations are called saliency maps and constitute an active field of research, given that many fundamental questions are yet to be answered: how to compute them efficiently? How to evaluate them? What exactly can they be used for? Given the increasing rate at which papers are produced and the vast amount of literature that is already existing, we propose our study to help newcomers become part of this community and to contribute to the research field. First, the two existing approaches to generate saliency maps are discussed, namely post-hoc methods and attention models. Post-hoc methods are generic algorithms that can be applied to any model from a given class without requiring fine-tuning. On the contrary, attention models are ad-hoc architectures that generate a saliency map during the inference phase to guide the decision. We show that both approaches can be divided into several subcategories and illustrate each of them with one important model or method. Second, we present the current methodologies used to evaluate saliency maps, including objective and subjective protocols, depending on whether or not they involve users. Among objective methods, we notably detail faithfulness metrics and propose an implementation featuring the faithfulness metrics discussed in this paper (https://github.com/TristanGomez44/metrics-saliency-maps).",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2070904522",
                        "name": "T. Gomez"
                    },
                    {
                        "authorId": "1790706",
                        "name": "H. Mouch\u00e8re"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "For instance, in [38], layer-wise relevance propagation is applied to transformers, and [8]"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "44b5014b38beb0628c129f08374b173e91d5b0d3",
                "externalIds": {
                    "ArXiv": "2302.14278",
                    "DBLP": "journals/corr/abs-2302-14278",
                    "DOI": "10.48550/arXiv.2302.14278",
                    "CorpusId": 257233090
                },
                "corpusId": 257233090,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/44b5014b38beb0628c129f08374b173e91d5b0d3",
                "title": "Multi-Layer Attention-Based Explainability via Transformers for Tabular Data",
                "abstract": "We propose a graph-oriented attention-based explainability method for tabular data. Tasks involving tabular data have been solved mostly using traditional tree-based machine learning models which have the challenges of feature selection and engineering. With that in mind, we consider a transformer architecture for tabular data, which is amenable to explainability, and present a novel way to leverage self-attention mechanism to provide explanations by taking into account the attention matrices of all layers as a whole. The matrices are mapped to a graph structure where groups of features correspond to nodes and attention values to arcs. By finding the maximum probability paths in the graph, we identify groups of features providing larger contributions to explain the model's predictions. To assess the quality of multi-layer attention-based explanations, we compare them with popular attention-, gradient-, and perturbation-based explanability methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2210069597",
                        "name": "Andrea Trevino Gavito"
                    },
                    {
                        "authorId": "1753376",
                        "name": "D. Klabjan"
                    },
                    {
                        "authorId": "2711744",
                        "name": "J. Utke"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "mean-intersection-over-union (mIoU) metric [72] and compare with eight explanation approaches."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "419d0ca86aedd3dfd6a8bf12999b1885c1e8a860",
                "externalIds": {
                    "DOI": "10.1109/TIP.2023.3246793",
                    "CorpusId": 257131559,
                    "PubMed": "37027685"
                },
                "corpusId": 257131559,
                "publicationVenue": {
                    "id": "e117fa7f-05b7-4dd6-b64b-0a0a7c0393d8",
                    "name": "IEEE Transactions on Image Processing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Image Process"
                    ],
                    "issn": "1057-7149",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=83",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/419d0ca86aedd3dfd6a8bf12999b1885c1e8a860",
                "title": "Learning Patch-Channel Correspondence for Interpretable Face Forgery Detection",
                "abstract": "Beyond high accuracy, good interpretability is very critical to deploy a face forgery detection model for visual content analysis. In this paper, we propose learning patch-channel correspondence to facilitate interpretable face forgery detection. Patch-channel correspondence aims to transform the latent features of a facial image into multi-channel interpretable features where each channel mainly encoders a corresponding facial patch. Towards this end, our approach embeds a feature reorganization layer into a deep neural network and simultaneously optimizes classification task and correspondence task via alternate optimization. The correspondence task accepts multiple zero-padding facial patch images and represents them into channel-aware interpretable representations. The task is solved by step-wisely learning channel-wise decorrelation and patch-channel alignment. Channel-wise decorrelation decouples latent features for class-specific discriminative channels to reduce feature complexity and channel correlation, while patch-channel alignment then models the pairwise correspondence between feature channels and facial patches. In this way, the learned model can automatically discover corresponding salient features associated to potential forgery regions during inference, providing discriminative localization of visualized evidences for face forgery detection while maintaining high detection accuracy. Extensive experiments on popular benchmarks clearly demonstrate the effectiveness of the proposed approach in interpreting face forgery detection without sacrificing accuracy. The source code is available at https://github.com/Jae35/IFFD.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51300273",
                        "name": "Yingying Hua"
                    },
                    {
                        "authorId": "2091415343",
                        "name": "Ruixin Shi"
                    },
                    {
                        "authorId": "2108815093",
                        "name": "Pengju Wang"
                    },
                    {
                        "authorId": "39646508",
                        "name": "Shiming Ge"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "498cc16f23413c66b17b4bffc8475a4079cb312c",
                "externalIds": {
                    "ArXiv": "2302.07265",
                    "DBLP": "journals/corr/abs-2302-07265",
                    "DOI": "10.48550/arXiv.2302.07265",
                    "CorpusId": 256846994
                },
                "corpusId": 256846994,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/498cc16f23413c66b17b4bffc8475a4079cb312c",
                "title": "The Meta-Evaluation Problem in Explainable AI: Identifying Reliable Estimators with MetaQuantus",
                "abstract": "One of the unsolved challenges in the field of Explainable AI (XAI) is determining how to most reliably estimate the quality of an explanation method in the absence of ground truth explanation labels. Resolving this issue is of utmost importance as the evaluation outcomes generated by competing evaluation methods (or ''quality estimators''), which aim at measuring the same property of an explanation method, frequently present conflicting rankings. Such disagreements can be challenging for practitioners to interpret, thereby complicating their ability to select the best-performing explanation method. We address this problem through a meta-evaluation of different quality estimators in XAI, which we define as ''the process of evaluating the evaluation method''. Our novel framework, MetaQuantus, analyses two complementary performance characteristics of a quality estimator: its resilience to noise and reactivity to randomness, thus circumventing the need for ground truth labels. We demonstrate the effectiveness of our framework through a series of experiments, targeting various open questions in XAI such as the selection and hyperparameter optimisation of quality estimators. Our work is released under an open-source license (https://github.com/annahedstroem/MetaQuantus) to serve as a development tool for XAI- and Machine Learning (ML) practitioners to verify and benchmark newly constructed quality estimators in a given explainability context. With this work, we provide the community with clear and theoretically-grounded guidance for identifying reliable evaluation methods, thus facilitating reproducibility in the field.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "50418294",
                        "name": "Anna Hedstr\u00f6m"
                    },
                    {
                        "authorId": "116143717",
                        "name": "P. Bommer"
                    },
                    {
                        "authorId": "2205658423",
                        "name": "Kristoffer K. Wickstrom"
                    },
                    {
                        "authorId": "1699054",
                        "name": "W. Samek"
                    },
                    {
                        "authorId": "3633358",
                        "name": "S. Lapuschkin"
                    },
                    {
                        "authorId": "2116161062",
                        "name": "Marina M.-C. H\u00f6hne"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                ", CAM or Grad-CAM) to other backbones like Transformer [9, 36] and graph neural networks [3, 12, 55].",
                "While most of these visualization approaches to interpretation of model predictions were originally developed for image classification models, they have been extended or modified for other tasks [11, 72] or other deep learning models [9, 38].",
                "For the\nother type of deep learning model backbone Transformer and its variants (e.g., ViT [14], Swin Transformer [42]), since most items in the input sequence at each model layer correspond to components (e.g., words for a sentence input, image patches for an image input) of the original input, the final model prediction also largely depends on the collection of local features of the original input.",
                "In contrast, for each representative baseline method, the importance maps often\nchange over model backbones and even may not work for the Transformer backbone ViT and SwinT.",
                "Substantial efforts are often required to adapt one visualization approach to various tasks (e.g., image caption) with different model backbones (e.g., Transformer backbone) or input formats (e.g., sequence of items).",
                "Because the proposed PAMI framework can consider the\nwell-trained model as a black-box, it can potentially work for various backbone structures (e.g., both CNN and Transformer backbones).",
                "In contrast, the majority of interpretation methods were proposed for the CNN backbone, and specific modifications are often required when applying existing interpretation methods (e.g., CAM or Grad-CAM) to other backbones like Transformer [9, 36] and graph neural networks [3, 12, 55].",
                "[9] Hila Chefer, Shir Gur, and Lior Wolf."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "208e861d55125a57c943b666b0bcaed879aa540a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-03318",
                    "ArXiv": "2302.03318",
                    "DOI": "10.48550/arXiv.2302.03318",
                    "CorpusId": 256627527
                },
                "corpusId": 256627527,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/208e861d55125a57c943b666b0bcaed879aa540a",
                "title": "PAMI: partition input and aggregate outputs for model interpretation",
                "abstract": "There is an increasing demand for interpretation of model predictions especially in high-risk applications. Various visualization approaches have been proposed to estimate the part of input which is relevant to a specific model prediction. However, most approaches require model structure and parameter details in order to obtain the visualization results, and in general much effort is required to adapt each approach to multiple types of tasks particularly when model backbone and input format change over tasks. In this study, a simple yet effective visualization framework called PAMI is proposed based on the observation that deep learning models often aggregate features from local regions for model predictions. The basic idea is to mask majority of the input and use the corresponding model output as the relative contribution of the preserved input part to the original model prediction. For each input, since only a set of model outputs are collected and aggregated, PAMI does not require any model detail and can be applied to various prediction tasks with different model backbones and input formats. Extensive experiments on multiple tasks confirm the proposed method performs better than existing visualization approaches in more precisely finding class-specific input regions, and when applied to different model backbones and input formats. The source code will be released publicly.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2156688166",
                        "name": "Wei Shi"
                    },
                    {
                        "authorId": "2204634891",
                        "name": "Wentao Zhang"
                    },
                    {
                        "authorId": "2152975427",
                        "name": "Weishi Zheng"
                    },
                    {
                        "authorId": "2192231278",
                        "name": "Ruixuan Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In Figure 8, we further conduct visualization experiments(Chefer, Gur, and Wolf 2021) to show the effectiveness of X-ReID."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cd226950e66cd1ac8628ece5c76d513a487bf311",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-02075",
                    "ArXiv": "2302.02075",
                    "DOI": "10.48550/arXiv.2302.02075",
                    "CorpusId": 256616099
                },
                "corpusId": 256616099,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cd226950e66cd1ac8628ece5c76d513a487bf311",
                "title": "X-ReID: Cross-Instance Transformer for Identity-Level Person Re-Identification",
                "abstract": "Currently, most existing person re-identification methods use Instance-Level features, which are extracted only from a single image. However, these Instance-Level features can easily ignore the discriminative information due to the appearance of each identity varies greatly in different images. Thus, it is necessary to exploit Identity-Level features, which can be shared across different images of each identity. In this paper, we propose to promote Instance-Level features to Identity-Level features by employing cross-attention to incorporate information from one image to another of the same identity, thus more unified and discriminative pedestrian information can be obtained. We propose a novel training framework named X-ReID. Specifically, a Cross Intra-Identity Instances module (IntraX) fuses different intra-identity instances to transfer Identity-Level knowledge and make Instance-Level features more compact. A Cross Inter-Identity Instances module (InterX) involves hard positive and hard negative instances to improve the attention response to the same identity instead of different identity, which minimizes intra-identity variation and maximizes inter-identity variation. Extensive experiments on benchmark datasets show the superiority of our method over existing works. Particularly, on the challenging MSMT17, our proposed method gains 1.1% mAP improvements when compared to the second place.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2174869132",
                        "name": "Leqi Shen"
                    },
                    {
                        "authorId": "2055131273",
                        "name": "Tao He"
                    },
                    {
                        "authorId": "34811036",
                        "name": "Yuchen Guo"
                    },
                    {
                        "authorId": "38329336",
                        "name": "Guiguang Ding"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "587c11117655859d137d9a538d99c0d8fd1de296",
                "externalIds": {
                    "DOI": "10.1002/alz.12948",
                    "CorpusId": 256577151,
                    "PubMed": "36735865"
                },
                "corpusId": 256577151,
                "publicationVenue": {
                    "id": "c7d391f8-08f8-4dc2-8081-c3d96a19f620",
                    "name": "Alzheimer's & Dementia",
                    "type": "journal",
                    "alternate_names": [
                        "Alzheimers  Dement",
                        "Alzheimers & Dementia",
                        "Alzheimer's  Dement"
                    ],
                    "issn": "1552-5260",
                    "url": "https://www.alzheimersanddementia.com/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/science/journal/15525260"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/587c11117655859d137d9a538d99c0d8fd1de296",
                "title": "Interpretable machine learning for dementia: A systematic review",
                "abstract": "Machine learning research into automated dementia diagnosis is becoming increasingly popular but so far has had limited clinical impact. A key challenge is building robust and generalizable models that generate decisions that can be reliably explained. Some models are designed to be inherently \u201cinterpretable,\u201d whereas post hoc \u201cexplainability\u201d methods can be used for other models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2204412926",
                        "name": "Sophie A. Martin"
                    },
                    {
                        "authorId": "2204353620",
                        "name": "Florence J Townend"
                    },
                    {
                        "authorId": "2496051",
                        "name": "F. Barkhof"
                    },
                    {
                        "authorId": "2069465101",
                        "name": "J. Cole"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Explainability refers to the ability of ViT to provide insights into its decisionmaking process, which is crucial for building trust in the model [1, 4, 47]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "03d22caf64831d1c48a5a25b3f886196a13d1dce",
                "externalIds": {
                    "ArXiv": "2301.13803",
                    "DBLP": "journals/corr/abs-2301-13803",
                    "DOI": "10.48550/arXiv.2301.13803",
                    "CorpusId": 256416070
                },
                "corpusId": 256416070,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/03d22caf64831d1c48a5a25b3f886196a13d1dce",
                "title": "Fairness-aware Vision Transformer via Debiased Self-Attention",
                "abstract": "Vision Transformer (ViT) has recently gained significant interest in solving computer vision (CV) problems due to its capability of extracting informative features and modeling long-range dependencies through the self-attention mechanism. To fully realize the advantages of ViT in real-world applications, recent works have explored the trustworthiness of ViT, including its robustness and explainability. However, another desiderata, fairness has not yet been adequately addressed in the literature. We establish that the existing fairness-aware algorithms (primarily designed for CNNs) do not perform well on ViT. This necessitates the need for developing our novel framework via Debiased Self-Attention (DSA). DSA is a fairness-through-blindness approach that enforces ViT to eliminate spurious features correlated with the sensitive attributes for bias mitigation. Notably, adversarial examples are leveraged to locate and mask the spurious features in the input image patches. In addition, DSA utilizes an attention weights alignment regularizer in the training objective to encourage learning informative features for target prediction. Importantly, our DSA framework leads to improved fairness guarantees over prior works on multiple prediction tasks without compromising target prediction performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2062242240",
                        "name": "Yao Qiang"
                    },
                    {
                        "authorId": "46651935",
                        "name": "Chengyin Li"
                    },
                    {
                        "authorId": "4386787",
                        "name": "Prashant Khanduri"
                    },
                    {
                        "authorId": "39895985",
                        "name": "D. Zhu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Specifically, we use IntGrad (Sundararajan et al., 2017) with n=32 steps, \u2018Input\u00d7Gradient\u2019 (IxG), cf. Adebayo et al. (2018), as well as an adapted GradCAM (Selvaraju et al., 2017) as in Chefer et al. (2021).",
                "The significant gains in quantitative interpretability metrics reported by Chefer et al. (2021) highlight the importance of such holistic explanations.",
                "Second, we evaluate two pixel perturbation metrics, cf. Chefer et al. (2021).",
                "In response, various improvements over attention rollout have been proposed, such as GradSAM (Barkan et al., 2021) or an LRP-based explanation method (Chefer et al., 2021), that were designed to more accurately reflect the computations of all model components.",
                ", 2021) or an LRP-based explanation method (Chefer et al., 2021), that were designed to more accurately reflect the computations of all model components.",
                "Further, we evaluate architecture-agnostic methods such as Integrated Gradients (IntGrad) (Sundararajan et al., 2017), adapted GradCAM (Selvaraju et al., 2017) as in Chefer et al. (2021), and \u2018Input\u00d7Gradient\u2019 (IxG), cf. Adebayo et al. (2018).",
                "In contrast to attention explanations, which are not class-specific (Chefer et al., 2021), we find the model-inherent explanations of B-cos ViTs to be highly detailed and class-specific.",
                "For the last, we rely on the implementation provided by Chefer et al. (2021).",
                "For all these transformer-specific explanations, we rely on the implementation provided by Chefer et al. (2021).",
                "However, as transformers consist of many additional components, explanations derived from attention alone have been found insufficient to explain the full models (Bastings & Filippova, 2020; Chefer et al., 2021).",
                "First, we follow Chefer et al. (2021) and evaluate common transformerspecific explanations such as the attention in the final layer (FinAtt), attention rollout (Rollout) (Abnar & Zuidema, 2020), a transformer-specific LRP implementation (CheferLRP) proposed by Chefer et al. (2021), \u2018partial\u2026",
                "The configurations of the ViTs follow the conventional specifications for ViTs of size Ti, S, and B, cf. Chefer et al. (2021).",
                "On the conventional ViTs, we further evaluate LRP-based explanations: partial LRP (pLRP) Voita et al. (2019) and the transformer-specific LRP adaptation by Chefer et al. (2021) (CheferLRP).",
                "For all models, we rely on the implementation by Chefer et al. (2021), which we use unchanged for the conventional ViTs and modify as we describe below for the B-cos ViTs (C.1.1).",
                "However, instead of deriving an explanation \u2018post-hoc\u2019 as in Chefer et al. (2021), we explicitly design our models to be holistically explainable.",
                "\u2026explanations such as the attention in the final layer (FinAtt), attention rollout (Rollout) (Abnar & Zuidema, 2020), a transformer-specific LRP implementation (CheferLRP) proposed by Chefer et al. (2021), \u2018partial LRP\u2019(pLRP) (Voita et al., 2019), and \u2018GradSAM\u2019 (Barkan et al., 2021)."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "c07f51a1c5e63420eaab2f016a2757eb44a86ebd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-08669",
                    "ArXiv": "2301.08669",
                    "DOI": "10.48550/arXiv.2301.08669",
                    "CorpusId": 256080584
                },
                "corpusId": 256080584,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c07f51a1c5e63420eaab2f016a2757eb44a86ebd",
                "title": "Holistically Explainable Vision Transformers",
                "abstract": "Transformers increasingly dominate the machine learning landscape across many tasks and domains, which increases the importance for understanding their outputs. While their attention modules provide partial insight into their inner workings, the attention scores have been shown to be insufficient for explaining the models as a whole. To address this, we propose B-cos transformers, which inherently provide holistic explanations for their decisions. Specifically, we formulate each model component - such as the multi-layer perceptrons, attention layers, and the tokenisation module - to be dynamic linear, which allows us to faithfully summarise the entire transformer via a single linear transform. We apply our proposed design to Vision Transformers (ViTs) and show that the resulting models, dubbed Bcos-ViTs, are highly interpretable and perform competitively to baseline ViTs on ImageNet. Code will be made available soon.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "35698126",
                        "name": "Moritz D Boehle"
                    },
                    {
                        "authorId": "1739548",
                        "name": "Mario Fritz"
                    },
                    {
                        "authorId": "48920094",
                        "name": "B. Schiele"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e4dae627fbc7df3a52dede899869775f220ad7cf",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-08110",
                    "ArXiv": "2301.08110",
                    "DOI": "10.48550/arXiv.2301.08110",
                    "CorpusId": 255999631
                },
                "corpusId": 255999631,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e4dae627fbc7df3a52dede899869775f220ad7cf",
                "title": "AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation",
                "abstract": "Generative transformer models have become increasingly complex, with large numbers of parameters and the ability to process multiple input modalities. Current methods for explaining their predictions are resource-intensive. Most crucially, they require prohibitively large amounts of extra memory, since they rely on backpropagation which allocates almost twice as much GPU memory as the forward pass. This makes it difficult, if not impossible, to use them in production. We present AtMan that provides explanations of generative transformer models at almost no extra cost. Specifically, AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction. Instead of using backpropagation, AtMan applies a parallelizable token-based search method based on cosine similarity neighborhood in the embedding space. Our exhaustive experiments on text and image-text benchmarks demonstrate that AtMan outperforms current state-of-the-art gradient-based methods on several metrics while being computationally efficient. As such, AtMan is suitable for use in large model inference deployments.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "32616828",
                        "name": "Mayukh Deb"
                    },
                    {
                        "authorId": "2905059",
                        "name": "Bj\u00f6rn Deiseroth"
                    },
                    {
                        "authorId": "2024731554",
                        "name": "Samuel Weinbach"
                    },
                    {
                        "authorId": "2166299958",
                        "name": "Manuel Brack"
                    },
                    {
                        "authorId": "40896023",
                        "name": "P. Schramowski"
                    },
                    {
                        "authorId": "2066493115",
                        "name": "K. Kersting"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Transformer visualizations are not limited to the only ones listed here as new techniques are continually suggested in scientific publications which shows how versatile Transformer models are (Chefer et al., 2021)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8ca5ce6f3e59cdeb31941c3b726fa885746ba5ee",
                "externalIds": {
                    "PubMedCentral": "9848389",
                    "DOI": "10.7554/eLife.82819",
                    "CorpusId": 255966856,
                    "PubMed": "36651724"
                },
                "corpusId": 255966856,
                "publicationVenue": {
                    "id": "07365b9a-c0ce-4dd3-b93b-a02e1c81e0c6",
                    "name": "eLife",
                    "type": "journal",
                    "issn": "2050-084X",
                    "url": "https://epub.uni-regensburg.de/40444/",
                    "alternate_urls": [
                        "https://elifesciences.org/",
                        "https://elife.elifesciences.org/",
                        "http://elifesciences.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8ca5ce6f3e59cdeb31941c3b726fa885746ba5ee",
                "title": "Transformer-based deep learning for predicting protein properties in the life sciences",
                "abstract": "Recent developments in deep learning, coupled with an increasing number of sequenced proteins, have led to a breakthrough in life science applications, in particular in protein property prediction. There is hope that deep learning can close the gap between the number of sequenced proteins and proteins with known properties based on lab experiments. Language models from the field of natural language processing have gained popularity for protein property predictions and have led to a new computational revolution in biology, where old prediction results are being improved regularly. Such models can learn useful multipurpose representations of proteins from large open repositories of protein sequences and can be used, for instance, to predict protein properties. The field of natural language processing is growing quickly because of developments in a class of models based on a particular model\u2014the Transformer model. We review recent developments and the use of large-scale Transformer models in applications for predicting protein characteristics and how such models can be used to predict, for example, post-translational modifications. We review shortcomings of other deep learning models and explain how the Transformer models have quickly proven to be a very promising way to unravel information hidden in the sequences of amino acids.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2064480300",
                        "name": "A. Chandra"
                    },
                    {
                        "authorId": "1405297291",
                        "name": "Laura T\u00fcnnermann"
                    },
                    {
                        "authorId": "40136362",
                        "name": "Tommy L\u00f6fstedt"
                    },
                    {
                        "authorId": "80558856",
                        "name": "Regina Gratz"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "ViT-B\nFake-CAM [39] 62.8 54.0 57.7 47.9 99.8 28.6 0.87\nGrad-CAM [46] 79.6 74.3 29.4 45.0 58.1 31.0 3.27 Grad-CAM++ [11] 84.2 80.6 14.8 23.8 51.4 27.3 4.15 Score-CAM [56] 77.6 71.6 46.0 54.3 66.1 33.1 3.14 XGrad-CAM [20] 82.0 76.9 19.6 41.3 52.8 28.5 3.31 Layer-CAM [26] 70.7 63.9 20.6 50.5 60.7 32.6 1.44 ExPerturbation [18] 71.5 64.9 35.9 44.6 62.3 35.3 1.34 RawAtt [16] 72.4 64.8 18.5 50.4 55.4 31.6 1.68 Rollout [1] 67.6 58.8 36.9 50.7 57.8 30.0 1.16 TIBAV [12] 70.1 63.1 26.6 58.8 66.1 35.0 1.23 Opti-CAM (ours) 64.4 54.6 54.5 48.0 58.2 28.7 0.98\nDeiT-B\nFake-CAM [39] 61.4 54.0 57.7 47.9 99.8 28.7 0.83\nGrad-CAM [46] 65.5 60.3 44.3 47.2 62.8 30.2 1.20 Grad-CAM++ [11] 70.6 67.2 34.3 43.6 57.7 30.3 2.14 Score-CAM [56] 79.9 76.2 31.9 43.8 63.4 32.2 3.14 XGrad-CAM [20] 82.0 78.4 19.5 44.1 53.4 28.8 3.03 Layer-CAM [26] 80.2 77.3 17.6 50.8 62.7 35.1 3.15 ExPerturbation [18] 69.9 64.3 36.2 44.2 63.1 35.5 1.16 RawAtt [16] 73.5 68.2 5.9 48.1 46.5 27.3 1.91 Rollout [1] 63.9 57.0 27.8 47.9 36.5 27.2 0.94 TIBAV [12] 68.2 62.2 28.1 59.6 64.1 33.5 1.08 Opti-CAM 62.3 55.1 53.9 48.0 55.1 28.8 0.84\nTable A8: Localization metrics with ViT and DeiT on ImageNet validation set.",
                "TIBAV [12], which is designed for transformers, outperforms the other methods on DeiT and ViT.",
                "METHOD DEIT-B VIT-B\nI\u2191 D\u2193 I\u2191 D\u2193\nFake-CAM [39] 57.5 34.2 57.4 33.3\nGrad-CAM [46] 61.8 17.5 62.9 19.8 Grad-CAM++ [11] 60.5 21.9 56.7 29.3 Score-CAM [56] 60.6 24.4 66.5 15.1 XGrad-CAM [20] 55.2 31.1 55.6 26.5 Layer-CAM [26] 61.6 21.2 62.9 14.6 ExPerturbation [18] 62.1 27.0 64.4 18.4 RawAtt [16] 56.3 29.3 62.2 17.9 Rollout [1] 56.7 32.8 64.8 15.2 TIBAV [12] 63.7 16.3 66.1 14.1 Opti-CAM (ours) 59.2 22.8 60.5 22.0\nTable A6: I/D: insertion/deletion [36] scores on ImageNet validation set; \u2193 / \u2191: lower / higher is better.\nobservation holds for deletion.",
                "For transformer models, we also compare against raw attention [16], rollout [1] and TIBAV [12]7.",
                "TIBAV [12] uses both instance-specific and class-specific information."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "fcc080a18a4ff5fb0ff06be5f9c6bc65b0d292e4",
                "externalIds": {
                    "ArXiv": "2301.07002",
                    "DBLP": "journals/corr/abs-2301-07002",
                    "DOI": "10.48550/arXiv.2301.07002",
                    "CorpusId": 255942630
                },
                "corpusId": 255942630,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fcc080a18a4ff5fb0ff06be5f9c6bc65b0d292e4",
                "title": "Opti-CAM: Optimizing saliency maps for interpretability",
                "abstract": "Methods based on class activation maps (CAM) provide a simple mechanism to interpret predictions of convolutional neural networks by using linear combinations of feature maps as saliency maps. By contrast, masking-based methods optimize a saliency map directly in the image space or learn it by training another network on additional data. In this work we introduce Opti-CAM, combining ideas from CAM-based and masking-based approaches. Our saliency map is a linear combination of feature maps, where weights are optimized per image such that the logit of the masked image for a given class is maximized. We also \ufb01x a fundamental \ufb02aw in two of the most com-mon evaluation metrics of attribution methods. On several datasets, Opti-CAM largely outperforms other CAM-based approaches according to the most relevant classi\ufb01cation metrics. We provide empirical evidence supporting that localization and classi\ufb01er interpretability are not necessarily aligned.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2119078004",
                        "name": "Hanwei Zhang"
                    },
                    {
                        "authorId": "2057997370",
                        "name": "Felipe Torres"
                    },
                    {
                        "authorId": "1993738",
                        "name": "R. Sicre"
                    },
                    {
                        "authorId": "1744904",
                        "name": "Yannis Avrithis"
                    },
                    {
                        "authorId": "1818853",
                        "name": "S. Ayache"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "acab728e7827b0cf7179cc44457b95ff27600c71",
                "externalIds": {
                    "DBLP": "conf/ijcai/0081PLQZ23",
                    "ArXiv": "2301.06989",
                    "DOI": "10.48550/arXiv.2301.06989",
                    "CorpusId": 255942778
                },
                "corpusId": 255942778,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/acab728e7827b0cf7179cc44457b95ff27600c71",
                "title": "Negative Flux Aggregation to Estimate Feature Attributions",
                "abstract": "There are increasing demands for understanding deep neural networks' (DNNs) behavior spurred by growing security and/or transparency concerns. Due to multi-layer nonlinearity of the deep neural network architectures, explaining DNN predictions still remains as an open problem, preventing us from gaining a deeper understanding of the mechanisms. To enhance the explainability of DNNs, we estimate the input feature's attributions to the prediction task using divergence and flux. Inspired by the divergence theorem in vector analysis, we develop a novel Negative Flux Aggregation (NeFLAG) formulation and an efficient approximation algorithm to estimate attribution map. Unlike the previous techniques, ours doesn't rely on fitting a surrogate model nor need any path integration of gradients. Both qualitative and quantitative experiments demonstrate a superior performance of NeFLAG in generating more faithful attribution maps than the competing methods. Our code is available at https://github.com/xinli0928/NeFLAG.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "50080172",
                        "name": "X. Li"
                    },
                    {
                        "authorId": "1727055",
                        "name": "Deng Pan"
                    },
                    {
                        "authorId": "46651935",
                        "name": "Chengyin Li"
                    },
                    {
                        "authorId": "2062242240",
                        "name": "Yao Qiang"
                    },
                    {
                        "authorId": "39895985",
                        "name": "D. Zhu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "79e07495df577219ec343d68a23e91f9bb4f2e2c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-03831",
                    "ArXiv": "2301.03831",
                    "DOI": "10.48550/arXiv.2301.03831",
                    "CorpusId": 245668630
                },
                "corpusId": 245668630,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/79e07495df577219ec343d68a23e91f9bb4f2e2c",
                "title": "Dynamic Grained Encoder for Vision Transformers",
                "abstract": "Transformers, the de-facto standard for language modeling, have been recently applied for vision tasks. This paper introduces sparse queries for vision transformers to exploit the intrinsic spatial redundancy of natural images and save computational costs. Specifically, we propose a Dynamic Grained Encoder for vision transformers, which can adaptively assign a suitable number of queries to each spatial region. Thus it achieves a fine-grained representation in discriminative regions while keeping high efficiency. Besides, the dynamic grained encoder is compatible with most vision transformer frameworks. Without bells and whistles, our encoder allows the state-of-the-art vision transformers to reduce computational complexity by 40%-60% while maintaining comparable performance on image classification. Extensive experiments on object detection and segmentation further demonstrate the generalizability of our approach. Code is available at https://github.com/StevenGrove/vtpack.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2150597337",
                        "name": "Lin Song"
                    },
                    {
                        "authorId": "1734973476",
                        "name": "Songyang Zhang"
                    },
                    {
                        "authorId": "2144363412",
                        "name": "Songtao Liu"
                    },
                    {
                        "authorId": "6000385",
                        "name": "Zeming Li"
                    },
                    {
                        "authorId": "33913193",
                        "name": "Xuming He"
                    },
                    {
                        "authorId": "47217841",
                        "name": "Hongbin Sun"
                    },
                    {
                        "authorId": "2032184078",
                        "name": "Jian Sun"
                    },
                    {
                        "authorId": "2144620206",
                        "name": "Nanning Zheng"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "As an active research topic, current attention visualization methods mainly focus on dot-product attention [1, 11]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "be989dda3af590f09d56a73a30e45ceb39018c9f",
                "externalIds": {
                    "DBLP": "conf/wacv/TianNB23",
                    "DOI": "10.1109/WACV56688.2023.00107",
                    "CorpusId": 256652844
                },
                "corpusId": 256652844,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/be989dda3af590f09d56a73a30e45ceb39018c9f",
                "title": "Fashion Image Retrieval with Text Feedback by Additive Attention Compositional Learning",
                "abstract": "Effective fashion image retrieval with text feedback stands to impact a range of real-world applications, such as e-commerce. Given a source image and text feedback that describes the desired modifications to that image, the goal is to retrieve the target images that resemble the source yet satisfy the given modifications by composing a multi-modal (image-text) query. We propose a novel solution to this problem, Additive Attention Compositional Learning (AACL), that uses a multi-modal transformer-based architecture and effectively models the image-text contexts. Specifically, we propose a novel image-text composition module based on additive attention that can be seamlessly plugged into deep neural networks. We also introduce a new challenging benchmark derived from the Shopping100k dataset. AACL is evaluated on three large-scale datasets (FashionIQ, Fashion200k, and Shopping100k), each with strong baselines. Extensive experiments show that AACL achieves new state-of-the-art results on all three datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1423690004",
                        "name": "Yuxin Tian"
                    },
                    {
                        "authorId": "145211099",
                        "name": "S. Newsam"
                    },
                    {
                        "authorId": "145908678",
                        "name": "K. Boakye"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "60c2397ffd568d6db3aef6e87a1666c293055dca",
                "externalIds": {
                    "PubMedCentral": "10214058",
                    "DOI": "10.1177/17562848231170945",
                    "CorpusId": 258851599,
                    "PubMed": "37251086"
                },
                "corpusId": 258851599,
                "publicationVenue": {
                    "id": "acdf09ed-68cc-4c8a-b2ab-3c6c4873cc9e",
                    "name": "Therapeutic Advances in Gastroenterology",
                    "type": "journal",
                    "alternate_names": [
                        "Ther Adv Gastroenterol"
                    ],
                    "issn": "1756-283X",
                    "url": "https://journals.sagepub.com/home/tag",
                    "alternate_urls": [
                        "http://tag.sagepub.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/60c2397ffd568d6db3aef6e87a1666c293055dca",
                "title": "Development and validation of a deep learning-based approach to predict the Mayo endoscopic score of ulcerative colitis",
                "abstract": "Background: The ulcerative colitis (UC) Mayo endoscopy score is a useful tool for evaluating the severity of UC in patients in clinical practice. Objectives: We aimed to develop and validate a deep learning-based approach to automatically predict the Mayo endoscopic score using UC endoscopic images. Design: A multicenter, diagnostic retrospective study. Methods: We collected 15120 colonoscopy images of 768 UC patients from two hospitals in China and developed a deep model based on a vision transformer named the UC-former. The performance of the UC-former was compared with that of six endoscopists on the internal test set. Furthermore, multicenter validation from three hospitals was also carried out to evaluate UC-former\u2019s generalization performance. Results: On the internal test set, the areas under the curve of Mayo 0, Mayo 1, Mayo 2, and Mayo 3 achieved by the UC-former were 0.998, 0.984, 0.973, and 0.990, respectively. The accuracy (ACC) achieved by the UC-former was 90.8%, which is higher than that achieved by the best senior endoscopist. For three multicenter external validations, the ACC was 82.4%, 85.0%, and 83.6%, respectively. Conclusions: The developed UC-former could achieve high ACC, fidelity, and stability to evaluate the severity of UC, which may provide potential application in clinical practice. Registration: This clinical trial was registered at the ClinicalTrials.gov (trial registration number: NCT05336773)",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2149459102",
                        "name": "Jing Qi"
                    },
                    {
                        "authorId": "88718707",
                        "name": "Guangcong Ruan"
                    },
                    {
                        "authorId": "2056742524",
                        "name": "Yi Ping"
                    },
                    {
                        "authorId": "2116551193",
                        "name": "Zhifeng Xiao"
                    },
                    {
                        "authorId": "46578450",
                        "name": "Kaijun Liu"
                    },
                    {
                        "authorId": "2117233207",
                        "name": "Yi Cheng"
                    },
                    {
                        "authorId": "15736787",
                        "name": "Rongbei Liu"
                    },
                    {
                        "authorId": "2117925407",
                        "name": "Bingqiang Zhang"
                    },
                    {
                        "authorId": "46691792",
                        "name": "M. Zhi"
                    },
                    {
                        "authorId": "2108870249",
                        "name": "Jun-rong Chen"
                    },
                    {
                        "authorId": "2057534043",
                        "name": "Fang Xiao"
                    },
                    {
                        "authorId": "2218430093",
                        "name": "Tingting Zhao"
                    },
                    {
                        "authorId": "2218071980",
                        "name": "Jiaxing Li"
                    },
                    {
                        "authorId": "2118749131",
                        "name": "Zhou Zhang"
                    },
                    {
                        "authorId": "2219054575",
                        "name": "Yuxin Zou"
                    },
                    {
                        "authorId": "39384562",
                        "name": "Q. Cao"
                    },
                    {
                        "authorId": "2066451448",
                        "name": "Y. Nian"
                    },
                    {
                        "authorId": "2165082837",
                        "name": "Yanling Wei"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "The interpretability of the Transformer-based model is closely tied to the basis of this model, which is the mechanism of attention [4], as the attention matrix provides deep insight into the object and relations in it [5]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "51312dea4943e7df48f5e5ea000d34235b7a46d7",
                "externalIds": {
                    "ArXiv": "2212.14246",
                    "CorpusId": 256662154
                },
                "corpusId": 256662154,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/51312dea4943e7df48f5e5ea000d34235b7a46d7",
                "title": "Robust representations of oil wells' intervals via sparse attention mechanism",
                "abstract": "Determining the characteristics of newly drilled wells (e.g. reservoir formation properties) is a major challenge. One of the corresponding tasks is a well-interval similarity assessment: if we can learn to predict which oilfields are rich and which are not by comparing them with existing ones, this will lead to significant cost reductions. There are three main requirements for applying machine learning to oil&gas data: high quality even for unreliable data, low manual effort and interpretability of the model itself. Neural networks can be used to address these challenges. The use of a self-supervised paradigm leads to automatic model construction. However, existing approaches lack interpretability, and their quality prevents their use in applications. In particular, existing approaches like LSTM suffer from short-term memory, paying more attention to the end of a sequence. Instead, neural networks with Transformer architecture cast their attention over all sequences to make a decision. To make them more efficient in terms of computational time and more robust to noisy or absent values, we introduce a limited attention mechanism similar to that of the Informer architecture that considers only top correspondences. We run experiments on an open dataset with more than $20$ wells, making our experiments reliable and suitable for industrial use. The best results were obtained with our adaptation of the Informer variant of Transformer with ROC AUC $0.982$. It outperforms classical approaches with ROC AUC $0.824$, recurrent neural networks (RNNs) with ROC AUC $0.934$ and the direct use of Transformer with ROC AUC $0.961$. We show that well-interval representations obtained by Informer are of higher quality than those extracted by RNNs. Moreover, the obtained attention is interpretable, as it corresponds to the importance of a particular part of an interval for the similarity estimation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2154418801",
                        "name": "Alina Rogulina"
                    },
                    {
                        "authorId": "15002046",
                        "name": "N. Baramiia"
                    },
                    {
                        "authorId": "2193473650",
                        "name": "Valerii Kornilov"
                    },
                    {
                        "authorId": "2042557906",
                        "name": "Sergey Petrakov"
                    },
                    {
                        "authorId": "2139684212",
                        "name": "A. Zaytsev"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "The comparison of attention maps in FBKD and FBKD-ProC-KD (ours) by using the Transformer Interpretability method [43]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1c716198bbcd675e4f2fa0ec6aef7ef8cfe869cf",
                "externalIds": {
                    "ArXiv": "2212.13180",
                    "DBLP": "journals/corr/abs-2212-13180",
                    "DOI": "10.48550/arXiv.2212.13180",
                    "CorpusId": 255125462
                },
                "corpusId": 255125462,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1c716198bbcd675e4f2fa0ec6aef7ef8cfe869cf",
                "title": "Prototype-guided Cross-task Knowledge Distillation for Large-scale Models",
                "abstract": "Recently, large-scale pre-trained models have shown their advantages in many tasks. However, due to the huge computational complexity and storage requirements, it is challenging to apply the large-scale model to real scenes. A common solution is knowledge distillation which regards the large-scale model as a teacher model and helps to train a small student model to obtain a competitive performance. Cross-task Knowledge distillation expands the application scenarios of the large-scale pre-trained model. Existing knowledge distillation works focus on directly mimicking the final prediction or the intermediate layers of the teacher model, which represent the global-level characteristics and are task-specific. To alleviate the constraint of different label spaces, capturing invariant intrinsic local object characteristics (such as the shape characteristics of the leg and tail of the cattle and horse) plays a key role. Considering the complexity and variability of real scene tasks, we propose a Prototype-guided Cross-task Knowledge Distillation (ProC-KD) approach to transfer the intrinsic local-level object knowledge of a large-scale teacher network to various task scenarios. First, to better transfer the generalized knowledge in the teacher model in cross-task scenarios, we propose a prototype learning module to learn from the essential feature representation of objects in the teacher model. Secondly, for diverse downstream tasks, we propose a task-adaptive feature augmentation module to enhance the features of the student model with the learned generalization prototype features and guide the training of the student model to improve its generalization ability. The experimental results on various visual tasks demonstrate the effectiveness of our approach for large-scale model cross-task knowledge distillation scenes.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2158676096",
                        "name": "Deng Li"
                    },
                    {
                        "authorId": "48352212",
                        "name": "Aming Wu"
                    },
                    {
                        "authorId": "144622313",
                        "name": "Yahong Han"
                    },
                    {
                        "authorId": "2149898858",
                        "name": "Qingwen Tian"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "We provide the attention relevance maps [9] for the same image as shown in Figure 13 in the main paper, but for all three classes present in the image, in Figure 28.",
                "Attention relevance (as in [9]) can significantly change at different patch sizes for both ViT and FlexiViT.",
                "Attention relevance patterns across scales We find that decreasing the patch size results in attention relevance [9] to concentrate into a larger number of smaller areas throughout the image.",
                "Top: Attention relevance (as in [9]) can significantly change at different patch sizes.",
                "2 [9] Hila Chefer, Shir Gur, and Lior Wolf."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "925fe4b2225e534888a2c78c9f6539a8e4e58d59",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-08013",
                    "ArXiv": "2212.08013",
                    "DOI": "10.1109/CVPR52729.2023.01393",
                    "CorpusId": 254685937
                },
                "corpusId": 254685937,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/925fe4b2225e534888a2c78c9f6539a8e4e58d59",
                "title": "FlexiViT: One Model for All Patch Sizes",
                "abstract": "Vision Transformers convert images to sequences by slicing them into patches. The size of these patches controls a speed/accuracy tradeoff, with smaller patches leading to higher accuracy at greater computational cost, but changing the patch size typically requires retraining the model. In this paper, we demonstrate that simply randomizing the patch size at training time leads to a single set of weights that performs well across a wide range of patch sizes, making it possible to tailor the model to different compute budgets at deployment time. We extensively evaluate the resulting model, which we call FlexiViT, on a wide range of tasks, including classification, image-text retrieval, open-world detection, panoptic segmentation, and semantic segmentation, concluding that it usually matches, and sometimes outperforms, standard ViT models trained at a single patch size in an otherwise identical setup. Hence, FlexiViT training is a simple drop-in improvement for ViT that makes it easy to add compute-adaptive capabilities to most models relying on a ViT backbone architecture. Code and pre-trained models are available at github.com/google-research/big_vision.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "39611591",
                        "name": "L. Beyer"
                    },
                    {
                        "authorId": "7991830",
                        "name": "Pavel Izmailov"
                    },
                    {
                        "authorId": "144629422",
                        "name": "Alexander Kolesnikov"
                    },
                    {
                        "authorId": "2062862676",
                        "name": "Mathilde Caron"
                    },
                    {
                        "authorId": "40464924",
                        "name": "Simon Kornblith"
                    },
                    {
                        "authorId": "2743563",
                        "name": "Xiaohua Zhai"
                    },
                    {
                        "authorId": "46352821",
                        "name": "Matthias Minderer"
                    },
                    {
                        "authorId": "143902495",
                        "name": "M. Tschannen"
                    },
                    {
                        "authorId": "2922782",
                        "name": "Ibrahim M. Alabdulmohsin"
                    },
                    {
                        "authorId": "1696719",
                        "name": "Filip Pavetic"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "D.2 Measuring negative contribution While Shapley values estimate both the positive and the negative contributions of input tokens towards the model prediction \u2013 which is relevant for foil words \u2013, attention (Chefer et al., 2021a) allows for positive-only relevance assessments.",
                "In Figures 10 and 11, we have visualised CLIPs attention-based relevancy for the image-caption and foil examples shown in Figures 2 to 7 using the method of Chefer et al. (2021a).",
                "\u2026to assign relevancy values for image and text tokens, research strives to generate simple explanations that represent the most important tokens and tend to inhibit the rest, as can be seen on the progress from Chefer et al. (2021b) to Chefer et al. (2021a) (cf. Figure 4 in Chefer et al. (2021a))."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "082690629dd27d2763c4366f9783cf0435879b79",
                "externalIds": {
                    "ACL": "2023.acl-long.223",
                    "DBLP": "conf/acl/ParcalabescuF23",
                    "ArXiv": "2212.08158",
                    "DOI": "10.48550/arXiv.2212.08158",
                    "CorpusId": 254823126
                },
                "corpusId": 254823126,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/082690629dd27d2763c4366f9783cf0435879b79",
                "title": "MM-SHAP: A Performance-agnostic Metric for Measuring Multimodal Contributions in Vision and Language Models & Tasks",
                "abstract": "Vision and language models (VL) are known to exploit unrobust indicators in individual modalities (e.g., introduced by distributional biases) instead of focusing on relevant information in each modality. That a unimodal model achieves similar accuracy on a VL task to a multimodal one, indicates that so-called unimodal collapse occurred. However, accuracy-based tests fail to detect e.g., when the model prediction is wrong, while the model used relevant information from a modality.Instead, we propose MM-SHAP, a performance-agnostic multimodality score based on Shapley values that reliably quantifies in which proportions a multimodal model uses individual modalities. We apply MM-SHAP in two ways: (1) to compare models for their average degree of multimodality, and (2) to measure for individual models the contribution of individual modalities for different tasks and datasets.Experiments with six VL models \u2013 LXMERT, CLIP and four ALBEF variants \u2013 on four VL tasks highlight that unimodal collapse can occur to different degrees and in different directions, contradicting the wide-spread assumption that unimodal collapse is one-sided. Based on our results, we recommend MM-SHAP for analysing multimodal tasks, to diagnose and guide progress towards multimodal integration. Code available at https://github.com/Heidelberg-NLP/MM-SHAP.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "79647482",
                        "name": "Letitia Parcalabescu"
                    },
                    {
                        "authorId": "143876555",
                        "name": "A. Frank"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "CAM-, or propagation-based methods [39].",
                "However, some of these methods are classagnostic in practical applications [39].",
                "Propagation-based methods [39, 44-51] mostly rely on the deep Taylor decomposition (DTD) framework [44]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bae5d7db6cb03ab716e8be48abb51b959d0511eb",
                "externalIds": {
                    "ArXiv": "2212.06299",
                    "DBLP": "journals/corr/abs-2212-06299",
                    "DOI": "10.48550/arXiv.2212.06299",
                    "CorpusId": 254591801,
                    "PubMed": "37405891"
                },
                "corpusId": 254591801,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bae5d7db6cb03ab716e8be48abb51b959d0511eb",
                "title": "Interpretable Diabetic Retinopathy Diagnosis based on Biomarker Activation Map",
                "abstract": "OBJECTIVE\nDeep learning classifiers provide the most accurate means of automatically diagnosing diabetic retinopathy (DR) based on optical coherence tomography (OCT) and its angiography (OCTA). The power of these models is attributable in part to the inclusion of hidden layers that provide the complexity required to achieve a desired task. However, hidden layers also render algorithm outputs difficult to interpret. Here we introduce a novel biomarker activation map (BAM) framework based on generative adversarial learning that allows clinicians to verify and understand classifiers' decision-making.\n\n\nMETHODS\nA data set including 456 macular scans were graded as non-referable or referable DR based on current clinical standards. A DR classifier that was used to evaluate our BAM was first trained based on this data set. The BAM generation framework was designed by combing two U-shaped generators to provide meaningful interpretability to this classifier. The main generator was trained to take referable scans as input and produce an output that would be classified by the classifier as non-referable. The BAM is then constructed as the difference image between the output and input of the main generator. To ensure that the BAM only highlights classifier-utilized biomarkers an assistant generator was trained to do the opposite, producing scans that would be classified as referable by the classifier from non-referable scans.\n\n\nRESULTS\nThe generated BAMs highlighted known pathologic features including nonperfusion area and retinal fluid.\n\n\nCONCLUSION/SIGNIFICANCE\nA fully interpretable classifier based on these highlights could help clinicians better utilize and verify automated DR diagnosis.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "11612861",
                        "name": "P. Zang"
                    },
                    {
                        "authorId": "11567514",
                        "name": "T. Hormel"
                    },
                    {
                        "authorId": "2146041336",
                        "name": "Jie Wang"
                    },
                    {
                        "authorId": "51358432",
                        "name": "Yukun Guo"
                    },
                    {
                        "authorId": "50735027",
                        "name": "Steven T. Bailey"
                    },
                    {
                        "authorId": "3779452",
                        "name": "C. Flaxel"
                    },
                    {
                        "authorId": "8312601",
                        "name": "David Huang"
                    },
                    {
                        "authorId": "39118829",
                        "name": "T. Hwang"
                    },
                    {
                        "authorId": "3830567",
                        "name": "Yali Jia"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Raghu et al. (2021) compare how the internal representation structure and use of spatial information differs between ViTs and CNNs. Chefer et al. (2021) produce \u2018image relevance maps\u2019 (which resemble saliency maps) to promote interpretability of ViTs.",
                "Furthermore, the final layer in ViTs appears to behave as a learned global pooling operation that aggregates information from all patches, which is similar to its explicit averagepooling counterpart in CNNs.",
                "In other words, ViTs learn to preserve spatial information,\ndespite lacking the inductive bias of CNNs. Spatial information in patches of deep layers has been\nexplored in Raghu et al. (2021) through the CKA similarity measure, and we further show that spatial information is in fact present in individual channels.",
                "After probing the role of spatial information, we delve into the behavioral differences between ViTs and CNNs.",
                "When performing activation maximizing visualizations, we notice that ViTs consistently generate higher quality image backgrounds than CNNs.",
                "As extensive work has been done to understand the workings of convolutional networks, including similar feature visualization and image reconstruction techniques to those used here, we may be able to learn more about ViT behavior via direct comparison to CNNs.",
                "Chefer et al. (2021) produce \u2018image relevance maps\u2019 (which resemble saliency maps) to promote interpretability of ViTs.",
                "Given their rapid proliferation, there is naturally great interest in how ViTs work and how they may differ from CNNs."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "41d3b9617772fda44cd81a3a11eead7236a0c01b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-06727",
                    "ArXiv": "2212.06727",
                    "DOI": "10.48550/arXiv.2212.06727",
                    "CorpusId": 254591270
                },
                "corpusId": 254591270,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/41d3b9617772fda44cd81a3a11eead7236a0c01b",
                "title": "What do Vision Transformers Learn? A Visual Exploration",
                "abstract": "Vision transformers (ViTs) are quickly becoming the de-facto architecture for computer vision, yet we understand very little about why they work and what they learn. While existing studies visually analyze the mechanisms of convolutional neural networks, an analogous exploration of ViTs remains challenging. In this paper, we first address the obstacles to performing visualizations on ViTs. Assisted by these solutions, we observe that neurons in ViTs trained with language model supervision (e.g., CLIP) are activated by semantic concepts rather than visual features. We also explore the underlying differences between ViTs and CNNs, and we find that transformers detect image background features, just like their convolutional counterparts, but their predictions depend far less on high-frequency information. On the other hand, both architecture types behave similarly in the way features progress from abstract patterns in early layers to concrete objects in late layers. In addition, we show that ViTs maintain spatial information in all layers except the final layer. In contrast to previous works, we show that the last layer most likely discards the spatial information and behaves as a learned global pooling operation. Finally, we conduct large-scale visualizations on a wide range of ViT variants, including DeiT, CoaT, ConViT, PiT, Swin, and Twin, to validate the effectiveness of our method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "115752784",
                        "name": "Amin Ghiasi"
                    },
                    {
                        "authorId": "2075283338",
                        "name": "Hamid Kazemi"
                    },
                    {
                        "authorId": "5493115",
                        "name": "Eitan Borgnia"
                    },
                    {
                        "authorId": "145653742",
                        "name": "Steven Reich"
                    },
                    {
                        "authorId": "1643697854",
                        "name": "Manli Shu"
                    },
                    {
                        "authorId": "121592562",
                        "name": "Micah Goldblum"
                    },
                    {
                        "authorId": "145771261",
                        "name": "A. Wilson"
                    },
                    {
                        "authorId": "2066530698",
                        "name": "T. Goldstein"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "We visualize the saliency maps [12] of DeiT-S and OAMixer on top of it, trained on ImageNet9.",
                "First, the advance of unsupervised [11, 36, 50] and weakly-supervised [12, 43, 60] saliency detection significantly reduced the labeling cost of objects."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9671054b27698aafa4ea680480dc2ecdd25a14d6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-06595",
                    "ArXiv": "2212.06595",
                    "DOI": "10.48550/arXiv.2212.06595",
                    "CorpusId": 254591707
                },
                "corpusId": 254591707,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9671054b27698aafa4ea680480dc2ecdd25a14d6",
                "title": "OAMixer: Object-aware Mixing Layer for Vision Transformers",
                "abstract": "Patch-based models, e.g., Vision Transformers (ViTs) and Mixers, have shown impressive results on various visual recognition tasks, alternating classic convolutional networks. While the initial patch-based models (ViTs) treated all patches equally, recent studies reveal that incorporating inductive bias like spatiality bene\ufb01ts the representations. However, most prior works solely focused on the location of patches, overlooking the scene structure of images. Thus, we aim to further guide the interaction of patches using the object information. Speci\ufb01cally, we propose OAMixer (object-aware mixing layer), which cali-brates the patch mixing layers of patch-based models based on the object labels. Here, we obtain the object labels in unsupervised or weakly-supervised manners, i.e., no additional human-annotating cost is necessary. Using the object labels, OAMixer computes a reweighting mask with a learnable scale parameter that intensi\ufb01es the interaction of patches containing similar objects and applies the mask to the patch mixing layers. By learning an object-centric representation, we demonstrate that OAMixer improves the classi\ufb01cation accuracy and background robustness of various patch-based models, including ViTs, MLP-Mixers, and ConvMixers. Moreover, we show that OAMixer enhances various downstream tasks, including large-scale classi\ufb01cation, self-supervised learning, and multi-object recognition, verifying the generic applicability of OAMixer. 1",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115457459",
                        "name": "H. Kang"
                    },
                    {
                        "authorId": "9962692",
                        "name": "Sangwoo Mo"
                    },
                    {
                        "authorId": "143720148",
                        "name": "Jinwoo Shin"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "The last two columns are the attention maps extracted by the method [93] to demonstrate the focus area of the model without and with the proposed AP modules."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "72a34970c762c7de0895985aca7d76315bd4ad71",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-05463",
                    "ArXiv": "2212.05463",
                    "DOI": "10.1109/TAFFC.2022.3226473",
                    "CorpusId": 254343286
                },
                "corpusId": 254343286,
                "publicationVenue": {
                    "id": "a88e2a6c-903f-42e1-8dfc-f2547e32020a",
                    "name": "IEEE Transactions on Affective Computing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Affect Comput"
                    ],
                    "issn": "1949-3045",
                    "url": "https://ieeexplore.ieee.org/document/7160715/",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=5165369"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/72a34970c762c7de0895985aca7d76315bd4ad71",
                "title": "Vision Transformer with Attentive Pooling for Robust Facial Expression Recognition",
                "abstract": "Facial Expression Recognition (FER) in the wild is an extremely challenging task. Recently, some Vision Transformers (ViT) have been explored for FER, but most of them perform inferiorly compared to Convolutional Neural Networks (CNN). This is mainly because the new proposed modules are difficult to converge well from scratch due to lacking inductive bias and easy to focus on the occlusion and noisy areas. TransFER, a representative transformer-based method for FER, alleviates this with multi-branch attention dropping but brings excessive computations. On the contrary, we present two attentive pooling (AP) modules to pool noisy features directly. The AP modules include Attentive Patch Pooling (APP) and Attentive Token Pooling (ATP). They aim to guide the model to emphasize the most discriminative features while reducing the impacts of less relevant features. The proposed APP is employed to select the most informative patches on CNN features, and ATP discards unimportant tokens in ViT. Being simple to implement and without learnable parameters, the APP and ATP intuitively reduce the computational cost while boosting the performance by ONLY pursuing the most discriminative features. Qualitative results demonstrate the motivations and effectiveness of our attentive poolings. Besides, quantitative results on six in-the-wild datasets outperform other state-of-the-art methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40830909",
                        "name": "Fanglei Xue"
                    },
                    {
                        "authorId": "2224668502",
                        "name": "Qiangchang Wang"
                    },
                    {
                        "authorId": "9645431",
                        "name": "Zichang Tan"
                    },
                    {
                        "authorId": "9249617",
                        "name": "Zhongsong Ma"
                    },
                    {
                        "authorId": "1822413",
                        "name": "G. Guo"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d7c9b620b977e92e04497b33fc7c9d85ed992b93",
                "externalIds": {
                    "PubMedCentral": "9839963",
                    "DOI": "10.1016/j.isci.2022.105872",
                    "CorpusId": 255115512,
                    "PubMed": "36647383"
                },
                "corpusId": 255115512,
                "publicationVenue": {
                    "id": "60a698a8-aea0-41b9-86bd-e979ded8bc8d",
                    "name": "iScience",
                    "type": "journal",
                    "issn": "2589-0042",
                    "url": "https://www.cell.com/iscience/home",
                    "alternate_urls": [
                        "http://www.cell.com/iscience/home",
                        "https://www.sciencedirect.com/journal/iscience"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d7c9b620b977e92e04497b33fc7c9d85ed992b93",
                "title": "Vision transformer-based weakly supervised histopathological image analysis of primary brain tumors",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "66545108",
                        "name": "Zhongxiao Li"
                    },
                    {
                        "authorId": "46298821",
                        "name": "Yu-wei Cong"
                    },
                    {
                        "authorId": "2145230207",
                        "name": "Xin Chen"
                    },
                    {
                        "authorId": "35189366",
                        "name": "Jiping Qi"
                    },
                    {
                        "authorId": "2157278648",
                        "name": "Jingxian Sun"
                    },
                    {
                        "authorId": "2165913257",
                        "name": "Tao Yan"
                    },
                    {
                        "authorId": "2109770555",
                        "name": "He Yang"
                    },
                    {
                        "authorId": "2135235138",
                        "name": "Junsi Liu"
                    },
                    {
                        "authorId": "113889770",
                        "name": "Enzhou Lu"
                    },
                    {
                        "authorId": "2143591683",
                        "name": "Lixiang Wang"
                    },
                    {
                        "authorId": "2109393472",
                        "name": "Jiafeng Li"
                    },
                    {
                        "authorId": "2198264939",
                        "name": "Hong Hu"
                    },
                    {
                        "authorId": "145107889",
                        "name": "Chen Zhang"
                    },
                    {
                        "authorId": "2112717835",
                        "name": "Quan Yang"
                    },
                    {
                        "authorId": "10822676",
                        "name": "Jiawei Yao"
                    },
                    {
                        "authorId": "1393376353",
                        "name": "Penglei Yao"
                    },
                    {
                        "authorId": "1720817729",
                        "name": "Qiuyi Jiang"
                    },
                    {
                        "authorId": "2109166499",
                        "name": "Wenwu Liu"
                    },
                    {
                        "authorId": "2119025983",
                        "name": "Jiangning Song"
                    },
                    {
                        "authorId": "145006560",
                        "name": "L. Carin"
                    },
                    {
                        "authorId": "2109253936",
                        "name": "Yupeng Chen"
                    },
                    {
                        "authorId": "3058672",
                        "name": "Shiguang Zhao"
                    },
                    {
                        "authorId": "2198273175",
                        "name": "Xin Gao"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "C V\n] 2\n7 N\nov 2\nterpretability methods, like CAM [46] and Transformerinterpretability [5], can support such an argument, such as in the work of [61].",
                "terpretability methods, like CAM [46] and Transformerinterpretability [5], can support such an argument, such as in the work of [61]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4bae689ade260c1624406b5bf2d58d637a0c5aa9",
                "externalIds": {
                    "DBLP": "conf/icml/Luo0W0023",
                    "ArXiv": "2211.14813",
                    "DOI": "10.48550/arXiv.2211.14813",
                    "CorpusId": 254043520
                },
                "corpusId": 254043520,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4bae689ade260c1624406b5bf2d58d637a0c5aa9",
                "title": "SegCLIP: Patch Aggregation with Learnable Centers for Open-Vocabulary Semantic Segmentation",
                "abstract": "Recently, the contrastive language-image pre-training, e.g., CLIP, has demonstrated promising results on various downstream tasks. The pre-trained model can capture enriched visual concepts for images by learning from a large scale of text-image data. However, transferring the learned visual knowledge to open-vocabulary semantic segmentation is still under-explored. In this paper, we propose a CLIP-based model named SegCLIP for the topic of open-vocabulary segmentation in an annotation-free manner. The SegCLIP achieves segmentation based on ViT and the main idea is to gather patches with learnable centers to semantic regions through training on text-image pairs. The gathering operation can dynamically capture the semantic groups, which can be used to generate the final segmentation results. We further propose a reconstruction loss on masked patches and a superpixel-based KL loss with pseudo-labels to enhance the visual representation. Experimental results show that our model achieves comparable or superior segmentation accuracy on the PASCAL VOC 2012 (+0.3% mIoU), PASCAL Context (+2.3% mIoU), and COCO (+2.2% mIoU) compared with baselines. We release the code at https://github.com/ArrowLuo/SegCLIP.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "35347136",
                        "name": "Huaishao Luo"
                    },
                    {
                        "authorId": "3299718",
                        "name": "Junwei Bao"
                    },
                    {
                        "authorId": "2115860568",
                        "name": "Youzheng Wu"
                    },
                    {
                        "authorId": "144137069",
                        "name": "Xiaodong He"
                    },
                    {
                        "authorId": "2118910985",
                        "name": "Tianrui Li"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "35c7586457f8a158a967a3d73207ee8ecd1e8eb6",
                "externalIds": {
                    "ArXiv": "2211.14305",
                    "DBLP": "conf/cvpr/AvrahamiHGGTPLF23",
                    "DOI": "10.1109/CVPR52729.2023.01762",
                    "CorpusId": 254018089
                },
                "corpusId": 254018089,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/35c7586457f8a158a967a3d73207ee8ecd1e8eb6",
                "title": "SpaText: Spatio-Textual Representation for Controllable Image Generation",
                "abstract": "Recent text-to-image diffusion models are able to generate convincing results of unprecedented quality. However, it is nearly impossible to control the shapes of different regions/objects or their layout in a fine-grained fashion. Previous attempts to provide such controls were hindered by their reliance on a fixed set of labels. To this end, we present SpaText \u2014 a new method for text-to-image generation using open-vocabulary scene control. In addition to a global text prompt that describes the entire scene, the user provides a segmentation map where each region of interest is annotated by a free-form natural language description. Due to lack of large-scale datasets that have a detailed textual description for each region in the image, we choose to leverage the current large-scale text-to-image datasets and base our approach on a novel CLIP-based spatio-textual representation, and show its effectiveness on two state-of-the-art diffusion models: pixel-based and latent-based. In addition, we show how to extend the classifier-free guidance method in diffusion models to the multi-conditional case and present an alternative accelerated inference algorithm. Finally, we offer several automatic evaluation metrics and use them, in addition to FID scores and a user study, to evaluate our method and show that it achieves state-of-the-art results on image generation with free-form textual scene control.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2107086356",
                        "name": "Omri Avrahami"
                    },
                    {
                        "authorId": "2161662704",
                        "name": "Thomas Hayes"
                    },
                    {
                        "authorId": "90840812",
                        "name": "Oran Gafni"
                    },
                    {
                        "authorId": "2118343423",
                        "name": "Sonal Gupta"
                    },
                    {
                        "authorId": "2188620",
                        "name": "Yaniv Taigman"
                    },
                    {
                        "authorId": "153432684",
                        "name": "Devi Parikh"
                    },
                    {
                        "authorId": "70018371",
                        "name": "D. Lischinski"
                    },
                    {
                        "authorId": "2416503",
                        "name": "Ohad Fried"
                    },
                    {
                        "authorId": "1557290137",
                        "name": "Xiaoyue Yin"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Finally, we perform visualization experiments using the (Chefer, Gur, and Wolf 2021) method to show the focused areas of the model."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a8adfb137332a61893417609563897abe9307a11",
                "externalIds": {
                    "DBLP": "conf/aaai/LiSL23",
                    "ArXiv": "2211.13977",
                    "DOI": "10.48550/arXiv.2211.13977",
                    "CorpusId": 254018126
                },
                "corpusId": 254018126,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/a8adfb137332a61893417609563897abe9307a11",
                "title": "CLIP-ReID: Exploiting Vision-Language Model for Image Re-Identification without Concrete Text Labels",
                "abstract": "Pre-trained vision-language models like CLIP have recently shown superior performances on various downstream tasks, including image classification and segmentation. However, in fine-grained image re-identification (ReID), the labels are indexes, lacking concrete text descriptions. Therefore, it remains to be determined how such models could be applied to these tasks. This paper first finds out that simply fine-tuning the visual model initialized by the image encoder in CLIP, has already obtained competitive performances in various ReID tasks. Then we propose a two-stage strategy to facilitate a better visual representation. The key idea is to fully exploit the cross-modal description ability in CLIP through a set of learnable text tokens for each ID and give them to the text encoder to form ambiguous descriptions. In the first training stage, image and text encoders from CLIP keep fixed, and only the text tokens are optimized from scratch by the contrastive loss computed within a batch. In the second stage, the ID-specific text tokens and their encoder become static, providing constraints for fine-tuning the image encoder. With the help of the designed loss in the downstream task, the image encoder is able to represent data as vectors in the feature embedding accurately. The effectiveness of the proposed strategy is validated on several datasets for the person or vehicle ReID tasks. Code is available at https://github.com/Syliz517/CLIP-ReID.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48831152",
                        "name": "Siyuan Li"
                    },
                    {
                        "authorId": "30135277",
                        "name": "Li Sun"
                    },
                    {
                        "authorId": "48934067",
                        "name": "Qingli Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "When ViT first demonstrate its powerful performance to outperform previous CNN-based baselines [8], its unique model structure has attracted extensive focus from researchers to understand its interpretability from different aspects, including: observing the attention map of Transformer outputs [3,15], computing the relevancy of different attention heads in Transformer networks [5, 26]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "17fc99b691d6fe1f4a84c774f01e86a2aae2c2df",
                "externalIds": {
                    "ArXiv": "2211.08543",
                    "DBLP": "journals/corr/abs-2211-08543",
                    "DOI": "10.48550/arXiv.2211.08543",
                    "CorpusId": 253553294
                },
                "corpusId": 253553294,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/17fc99b691d6fe1f4a84c774f01e86a2aae2c2df",
                "title": "Demystify Self-Attention in Vision Transformers from a Semantic Perspective: Analysis and Application",
                "abstract": "Self-attention mechanisms, especially multi-head self-attention (MSA), have achieved great success in many fields such as computer vision and natural language processing. However, many existing vision transformer (ViT) works simply inherent transformer designs from NLP to adapt vision tasks, while ignoring the fundamental difference between ``how MSA works in image and language settings''. Language naturally contains highly semantic structures that are directly interpretable by humans. Its basic unit (word) is discrete without redundant information, which readily supports interpretable studies on MSA mechanisms of language transformer. In contrast, visual data exhibits a fundamentally different structure: Its basic unit (pixel) is a natural low-level representation with significant redundancies in the neighbourhood, which poses obvious challenges to the interpretability of MSA mechanism in ViT. In this paper, we introduce a typical image processing technique, i.e., scale-invariant feature transforms (SIFTs), which maps low-level representations into mid-level spaces, and annotates extensive discrete keypoints with semantically rich information. Next, we construct a weighted patch interrelation analysis based on SIFT keypoints to capture the attention patterns hidden in patches with different semantic concentrations Interestingly, we find this quantitative analysis is not only an effective complement to the interpretability of MSA mechanisms in ViT, but can also be applied to 1) spurious correlation discovery and ``prompting'' during model inference, 2) and guided model pre-training acceleration. Experimental results on both applications show significant advantages over baselines, demonstrating the efficacy of our method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108654373",
                        "name": "Leijie Wu"
                    },
                    {
                        "authorId": "2162793936",
                        "name": "Song Guo"
                    },
                    {
                        "authorId": "2190937933",
                        "name": "Yaohong Ding"
                    },
                    {
                        "authorId": "2156915817",
                        "name": "Junxiao Wang"
                    },
                    {
                        "authorId": "50232004",
                        "name": "Wenchao Xu"
                    },
                    {
                        "authorId": "2179090902",
                        "name": "Richard Xu"
                    },
                    {
                        "authorId": "2122117250",
                        "name": "Jiewei Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "We applied the method described in [6] to interpret the learned representations from three selfsupervised learning approaches.",
                "Furthermore, we visualize the attention from the pre-trained weights using the method described in [6]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "997105b4a626ea871293dfe85ecd438489c30af7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-06012",
                    "ArXiv": "2211.06012",
                    "DOI": "10.48550/arXiv.2211.06012",
                    "CorpusId": 253498983
                },
                "corpusId": 253498983,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/997105b4a626ea871293dfe85ecd438489c30af7",
                "title": "Masked Contrastive Representation Learning",
                "abstract": "Masked image modelling (e.g., Masked AutoEncoder) and contrastive learning (e.g., Momentum Contrast) have shown impressive performance on unsupervised visual representation learning. This work presents Masked Contrastive Representation Learning (MACRL) for self-supervised visual pre-training. In particular, MACRL leverages the effectiveness of both masked image modelling and contrastive learning. We adopt an asymmetric setting for the siamese network (i.e., encoder-decoder structure in both branches), where one branch with higher mask ratio and stronger data augmentation, while the other adopts weaker data corruptions. We optimize a contrastive learning objective based on the learned features from the encoder in both branches. Furthermore, we minimize the $L_1$ reconstruction loss according to the decoders' outputs. In our experiments, MACRL presents superior results on various vision benchmarks, including CIFAR-10, CIFAR-100, Tiny-ImageNet, and two other ImageNet subsets. Our framework provides unified insights on self-supervised visual pre-training and future research.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46461580",
                        "name": "Yuan Yao"
                    },
                    {
                        "authorId": "18145464",
                        "name": "Nandakishor Desai"
                    },
                    {
                        "authorId": "145389998",
                        "name": "M. Palaniswami"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "In comparison with CNNbased neural networks, the recently developed attention-based transformer models represent a potential paradigm change in the middle of the 2020s.(25) When compared with CNNs, the recovered features from transformers can more accurately reflect long-range dependency within the sequence, and they also carry more semantic information."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d664870d80a536986634d6dc1f74104e9a2c58e2",
                "externalIds": {
                    "DOI": "10.1117/1.JRS.17.022203",
                    "CorpusId": 253403575
                },
                "corpusId": 253403575,
                "publicationVenue": {
                    "id": "e1fe9a1d-d67b-4f64-a192-472091321f8e",
                    "name": "Journal of Applied Remote Sensing",
                    "type": "journal",
                    "alternate_names": [
                        "J Appl Remote Sens"
                    ],
                    "issn": "1931-3195",
                    "url": "https://www.spiedigitallibrary.org/journals/journal-of-applied-remote-sensing",
                    "alternate_urls": [
                        "http://remotesensing.spiedigitallibrary.org/journal.aspx"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d664870d80a536986634d6dc1f74104e9a2c58e2",
                "title": "Deformable patch-based-multi-layer perceptron Mixer model for forest fire aerial image classification",
                "abstract": "Abstract. Unmanned aerial vehicles (UAVs) that include mounted camera sensors enable a wide range of remote sensing application deployments. Due to UAVs\u2019 capacity to explore distant locations such as woods, situational awareness for applications, such as search and rescue in wildfires, estimation of endangered flora and fauna, and emergency responses have undergone a paradigm change. A multi-layer perceptron (MLP)-Mixer architecture is suggested for classifying burned piles in dense forests. Convolutional neural networks (CNNs) and the more recent attention-based transformer models have produced cutting-edge outcomes in picture prediction. Convolutions and attention are used in the MLP Mixer architecture in an effort to overcome their drawbacks and improve performance. By including a new module of DePatch in the suggested MLP Mixer model, which separates the input images in a deformable pattern to identify forest fires at an early stage, the shallow learning of CNN layers and fixed-size patch embedding in transformers were eliminated. This suggests that the DePatch-based MLP classification model outperforms transformer approaches in terms of performance, achieving a substantial accuracy of 77.23. Our proposed classification system was evaluated on the pile photos dataset taken during a burning pile of debris in an Arizona pine forest.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47311778",
                        "name": "Payal Mittal"
                    },
                    {
                        "authorId": "2064350",
                        "name": "Akashdeep Sharma"
                    },
                    {
                        "authorId": "2157632867",
                        "name": "Raman Singh"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a5e787c54ef97d40744f9a95d41f7ce8ccae05cd",
                "externalIds": {
                    "DBLP": "conf/ijcai/Xie0CZ23",
                    "ArXiv": "2211.03064",
                    "DOI": "10.48550/arXiv.2211.03064",
                    "CorpusId": 253383736
                },
                "corpusId": 253383736,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/a5e787c54ef97d40744f9a95d41f7ce8ccae05cd",
                "title": "ViT-CX: Causal Explanation of Vision Transformers",
                "abstract": "Despite the popularity of Vision Transformers (ViTs) and eXplainable AI (XAI), only a few explanation methods have been designed specially for ViTs thus far. They mostly use attention weights of the [CLS] token on patch embeddings and often produce unsatisfactory saliency maps. This paper proposes a novel method for explaining ViTs called ViT-CX. It is based on patch embeddings, rather than attentions paid to them, and their causal impacts on the model output. Other characteristics of ViTs such as causal overdetermination are considered in the design of ViT-CX. The empirical results show that ViT-CX produces more meaningful saliency maps and does a better job revealing all important evidence for the predictions than previous methods. The explanation generated by ViT-CX also shows significantly better faithfulness to the model. The codes and appendix are available at https://github.com/vaynexie/CausalX-ViT.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "4868695",
                        "name": "Weiyan Xie"
                    },
                    {
                        "authorId": "2118890136",
                        "name": "Xiao-hui Li"
                    },
                    {
                        "authorId": "3151540",
                        "name": "Caleb Chen Cao"
                    },
                    {
                        "authorId": "2190105006",
                        "name": "Nevin L.Zhang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "333e8f75d47ea91be8ce630a07346e586448ef5a",
                "externalIds": {
                    "DOI": "10.1016/j.matt.2022.10.003",
                    "CorpusId": 253312312,
                    "PubMed": "36817352"
                },
                "corpusId": 253312312,
                "publicationVenue": {
                    "id": "f5755c39-9039-422b-a57e-fc6bab51c706",
                    "name": "Matter",
                    "type": "journal",
                    "issn": "2318-0846",
                    "alternate_issns": [
                        "2590-2385"
                    ],
                    "url": "http://unibr.com.br/revistamatter/?page_id=130",
                    "alternate_urls": [
                        "https://www.cell.com/matter/home"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/333e8f75d47ea91be8ce630a07346e586448ef5a",
                "title": "An automated biomateriomics platform for sustainable programmable materials discovery.",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2189743815",
                        "name": "Nicolas A. Lee"
                    },
                    {
                        "authorId": "152909131",
                        "name": "Sabrina Shen"
                    },
                    {
                        "authorId": "2273480",
                        "name": "M. Buehler"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The flexibility of Transformers for various input modalities [226] and the inherent interpretability of learned attention scores [227], [228] lend themselves well to the self-driving domain."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "26c3f19c6cbe34652a143a1db16d71436a930a69",
                "externalIds": {
                    "DBLP": "journals/tits/KotserubaT22",
                    "DOI": "10.1109/TITS.2022.3186613",
                    "CorpusId": 250506092
                },
                "corpusId": 250506092,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/26c3f19c6cbe34652a143a1db16d71436a930a69",
                "title": "Attention for Vision-Based Assistive and Automated Driving: A Review of Algorithms and Datasets",
                "abstract": "Driving safety has been a concern since the first cars appeared on the streets. Driver inattention has been singled out as a major cause of accidents early on. This is hardly surprising, as drivers routinely perform other tasks in addition to controlling the vehicle. Decades of research into what causes lapses or misdirection of drivers\u2019 attention resulted in improvements in road safety through better design of infrastructure, driver training programs, in- vehicle interfaces, and, more recently, the development of driving assistance systems (ADAS) and driving automation. This review focuses on the methods for modeling and detecting spatio-temporal aspects of drivers\u2019 attention, i. e. where and when they look, for the two latter categories of applications. We start with a brief theoretical background on human visual attention, methods for recording and measuring attention in the driving context, types of driver inattention, and factors causing it. We then discuss machine learning approaches for 1) modeling gaze for assistive and self-driving applications and 2) detecting gaze for driver monitoring. Following the overview of state-of-the-art models, we provide an extensive list of publicly available datasets that feature recordings of drivers\u2019 gaze and other attention-related annotations. We conclude with a general overview of the remaining challenges, such as data availability and quality, evaluation methods, and the limited scope of attention modeling, and outline steps toward rectifying some of these issues. Categorized and annotated lists of the reviewed models and datasets are available at https://github.com/ykotseruba/attention_and_driving",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3468296",
                        "name": "Iuliia Kotseruba"
                    },
                    {
                        "authorId": "1727853",
                        "name": "John K. Tsotsos"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "\u2026DeepLift, LRP, etc. (Simonyan et al., 2014; Smilkov et al., 2017; Shrikumar et al., 2017; Sundararajan et al., 2017; Xu et al., 2020; Selvaraju et al., 2017; Bach et al., 2015; Shrikumar et al., 2016; Chefer et al., 2021; Montavon et al., 2017; Shrikumar et al., 2017; Schwab & Karlen, 2019)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ac983aa2f778cdb017df3e13430ec17802a54cf3",
                "externalIds": {
                    "ArXiv": "2210.17426",
                    "CorpusId": 258967747
                },
                "corpusId": 258967747,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ac983aa2f778cdb017df3e13430ec17802a54cf3",
                "title": "Trade-off Between Efficiency and Consistency for Removal-based Explanations",
                "abstract": "In the current landscape of explanation methodologies, most predominant approaches, such as SHAP and LIME, employ removal-based techniques to evaluate the impact of individual features by simulating various scenarios with specific features omitted. Nonetheless, these methods primarily emphasize efficiency in the original context, often resulting in general inconsistencies. In this paper, we demonstrate that such inconsistency is an inherent aspect of these approaches by establishing the Impossible Trinity Theorem, which posits that interpretability, efficiency and consistency cannot hold simultaneously. Recognizing that the attainment of an ideal explanation remains elusive, we propose the utilization of interpretation error as a metric to gauge inconsistencies and inefficiencies. To this end, we present two novel algorithms founded on the standard polynomial basis, aimed at minimizing interpretation error. Our empirical findings indicate that the proposed methods achieve a substantial reduction in interpretation error, up to 31.8 times lower when compared to alternative techniques.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108464109",
                        "name": "Yifan Zhang"
                    },
                    {
                        "authorId": "2110436433",
                        "name": "Haowei He"
                    },
                    {
                        "authorId": "144459366",
                        "name": "Zhiyuan Tan"
                    },
                    {
                        "authorId": "2116944866",
                        "name": "Yang Yuan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "The recent approaches presented in [23] and [24] go beyond interpretability solely based on attention weights and generate model explanations by utilizing all components in the transformer architecture."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "70e924e07fa678bd0019b670fe39f349809e36c2",
                "externalIds": {
                    "ArXiv": "2210.13167",
                    "DBLP": "journals/corr/abs-2210-13167",
                    "DOI": "10.48550/arXiv.2210.13167",
                    "CorpusId": 253098518
                },
                "corpusId": 253098518,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/70e924e07fa678bd0019b670fe39f349809e36c2",
                "title": "Exploring Self-Attention for Crop-type Classification Explainability",
                "abstract": "Automated crop-type classification using Sentinel-2 satellite time series is essential to support agriculture monitoring. Recently, deep learning models based on transformer encoders became a promising approach for crop-type classification. Using explainable machine learning to reveal the inner workings of these models is an important step towards improving stakeholders' trust and efficient agriculture monitoring. In this paper, we introduce a novel explainability framework that aims to shed a light on the essential crop disambiguation patterns learned by a state-of-the-art transformer encoder model. More specifically, we process the attention weights of a trained transformer encoder to reveal the critical dates for crop disambiguation and use domain knowledge to uncover the phenological events that support the model performance. We also present a sensitivity analysis approach to understand better the attention capability for revealing crop-specific phenological events. We report compelling results showing that attention patterns strongly relate to key dates, and consequently, to the critical phenological events for crop-type classification. These findings might be relevant for improving stakeholder trust and optimizing agriculture monitoring processes. Additionally, our sensitivity analysis demonstrates the limitation of attention weights for identifying the important events in the crop phenology as we empirically show that the unveiled phenological events depend on the other crops in the data considered during training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "19183479",
                        "name": "Ivica Obadic"
                    },
                    {
                        "authorId": "46525320",
                        "name": "R. Roscher"
                    },
                    {
                        "authorId": "34475405",
                        "name": "Dario Augusto Borges Oliveira"
                    },
                    {
                        "authorId": "2125159330",
                        "name": "Xiao Xiang Zhu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "In the future, other than class activation maps, we will seek to explore the explainability for Vision Transformers in multi-label classification tasks, with the help of self-attention derived from the Transformer architectures [12, 79, 1, 13]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "249e00445585586214e27d1f4ade032533132d0a",
                "externalIds": {
                    "ArXiv": "2210.12843",
                    "DBLP": "journals/corr/abs-2210-12843",
                    "DOI": "10.1109/WACV56688.2023.00358",
                    "CorpusId": 253098023
                },
                "corpusId": 253098023,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/249e00445585586214e27d1f4ade032533132d0a",
                "title": "Delving into Masked Autoencoders for Multi-Label Thorax Disease Classification",
                "abstract": "Vision Transformer (ViT) has become one of the most popular neural architectures due to its great scalability, computational efficiency, and compelling performance in many vision tasks. However, ViT has shown inferior performance to Convolutional Neural Network (CNN) on medical tasks due to its data-hungry nature and the lack of an-notated medical data. In this paper, we pre-train ViTs on 266,340 chest X-rays using Masked Autoencoders (MAE) which reconstruct missing pixels from a small part of each image. For comparison, CNNs are also pre-trained on the same 266,340 X-rays using advanced self-supervised methods (e.g. MoCo v2). The results show that our pre-trained ViT performs comparably (sometimes better) to the state-of-the-art CNN (DenseNet-121) for multi-label thorax dis-ease classification. This performance is attributed to the strong recipes extracted from our empirical studies for pre-training and fine-tuning ViT. The pre-training recipe signifies that medical reconstruction requires a much smaller proportion of an image (10% vs. 25%) and a more moderate random resized crop range (0.5\u223c1.0 vs. 0.2\u223c1.0) compared with natural imaging. Furthermore, we remark that in-domain transfer learning is preferred whenever possible. The fine-tuning recipe discloses that layer-wise LR decay, RandAug magnitude, and DropPath rate are significant factors to consider. We hope that this study can direct future research on the application of Transformers to a larger variety of medical imaging tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1384522627",
                        "name": "Junfei Xiao"
                    },
                    {
                        "authorId": "48442730",
                        "name": "Yutong Bai"
                    },
                    {
                        "authorId": "145081362",
                        "name": "A. Yuille"
                    },
                    {
                        "authorId": "2198519",
                        "name": "Zongwei Zhou"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1eaca22499c3b31cb01246bc9852f47e724c9762",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-11318",
                    "ArXiv": "2210.11318",
                    "DOI": "10.1145/3626186",
                    "CorpusId": 253018670
                },
                "corpusId": 253018670,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1eaca22499c3b31cb01246bc9852f47e724c9762",
                "title": "A Survey of Computer Vision Technologies In Urban and Controlled-environment Agriculture",
                "abstract": "In the evolution of agriculture to its next stage, Agriculture 5.0, artificial intelligence will play a central role. Controlled-environment agriculture, or CEA, is a special form of urban and suburban agricultural practice that offers numerous economic, environmental, and social benefits, including shorter transportation routes to population centers, reduced environmental impact, and increased productivity. Due to its ability to control environmental factors, CEA couples well with computer vision (CV) in the adoption of real-time monitoring of the plant conditions and autonomous cultivation and harvesting. The objective of this paper is to familiarize CV researchers with agricultural applications and agricultural practitioners with the solutions offered by CV. We identify five major CV applications in CEA, analyze their requirements and motivation, and survey the state of the art as reflected in 68 technical papers using deep learning methods. In addition, we discuss five key subareas of computer vision and how they related to these CEA problems, as well as fourteen vision-based CEA datasets. We hope the survey will help researchers quickly gain a bird-eye view of the striving research area and will spark inspiration for new research and development.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152365014",
                        "name": "Jiayun Luo"
                    },
                    {
                        "authorId": "1728712",
                        "name": "Boyang Albert Li"
                    },
                    {
                        "authorId": "2188346915",
                        "name": "Cyril Leung"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "\u2019s method [190] can reach better performance.",
                "[190] proposed a novel method for computing the correlation of the Transformers network.",
                "[190] proposed to explore attention visualization to interpret the principle of Transformer."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "88a6a6e0194b99b3df98a7b3a884747b10f76be4",
                "externalIds": {
                    "DBLP": "journals/mms/FengT23",
                    "DOI": "10.1007/s00530-022-01003-8",
                    "CorpusId": 253027533
                },
                "corpusId": 253027533,
                "publicationVenue": {
                    "id": "d1997ea9-9d41-4458-9280-94feb013bd15",
                    "name": "Multimedia Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Multimedia Syst"
                    ],
                    "issn": "0942-4962",
                    "url": "http://www.springer.com/computer/information+systems+and+applications/journal/530?changeHeader",
                    "alternate_urls": [
                        "https://link.springer.com/journal/530",
                        "http://www.springer.com/computer/information+systems+and+applications/journal/530?changeHeader="
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/88a6a6e0194b99b3df98a7b3a884747b10f76be4",
                "title": "A survey of visual neural networks: current trends, challenges and opportunities",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2056811337",
                        "name": "Ping Feng"
                    },
                    {
                        "authorId": "2173392",
                        "name": "Zhenjun Tang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "For non-parametric layers (add layer and matrix multiplication), the authors of [7].",
                "To circumvent the black-box nature of the Deep Neural Networks, recent studies have tried explaining their decisions [8, 9, 10, 11, 7].",
                "The computation of the weighted attention map is inspired by [15] and [7].",
                "On the contrary, their method is class-specific, as [7] integrates the relevance scores with the gradient of attention w.",
                "Our method is compared with the state-of-the-art explainers with a recently adapted version of Relevance Propagation for the transformers [7] being amongst them.",
                "In [16], the authors of [7] extended their work for coattention methods performing multimodal input (images and text) and encoder-decoder attention.",
                "Heat maps built upon explanation maps are obtained on a number of state-of-the-art methods such as Grad-Cam [10], adapted relevance propagation [7], rollout method [15], and our self-attention weighted method (SAW).",
                "al [7] proposed a transformer explanation inspired by LRP [11] method, which was developed for explaining the decisions of CNNs."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "72c916685c9d1151cffa1db87c93e9f5afef39be",
                "externalIds": {
                    "DBLP": "conf/icip/MallickBZ22",
                    "DOI": "10.1109/ICIP46576.2022.9897347",
                    "CorpusId": 253331185
                },
                "corpusId": 253331185,
                "publicationVenue": {
                    "id": "b6369c33-5d70-463c-8e82-95a54efa3cc8",
                    "name": "International Conference on Information Photonics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Image Process",
                        "ICIP",
                        "Int Conf Inf Photonics",
                        "International Conference on Image Processing"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/72c916685c9d1151cffa1db87c93e9f5afef39be",
                "title": "I Saw: A Self-Attention Weighted Method for Explanation of Visual Transformers",
                "abstract": "Recently, visual transformers have shown promising results in tasks such as image classification, segmentation, object detection, etc. The explanation of their decision remains a challenge. This paper focuses on exploiting self-attention for an explanation. We propose a generalized interpretation of the transformers i.e model agnostic but class-specific explanations. The main principle is in the use and weighting self-attention maps of a visual transformer. To evaluate it, we use the popular hypothesis that an explanation is good if it correlates with human perception of a visual scene. Thus, the method has been evaluated against the Gaze Fixation Density Maps obtained in a psycho-visual experiment on a public database. It has been compared with other popular explainers such as Grad-Cam, LRP, Rollout, and Adaptive Relevance methods. The proposed method outperforms the best baseline by 2% in a standard Pearson Correlation Coefficient (PCC) metric.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2068396961",
                        "name": "Rupayan Mallick"
                    },
                    {
                        "authorId": "1381345946",
                        "name": "J. Benois-Pineau"
                    },
                    {
                        "authorId": "1961187",
                        "name": "A. Zemmari"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3e7805d27c2652b178d2925d2712f2daeec6040a",
                "externalIds": {
                    "DBLP": "journals/tmi/MahapatraPR23",
                    "DOI": "10.1109/TMI.2022.3215017",
                    "CorpusId": 252896812,
                    "PubMed": "36240033"
                },
                "corpusId": 252896812,
                "publicationVenue": {
                    "id": "e0cda45d-3074-4ac0-80b8-e5250df00b89",
                    "name": "IEEE Transactions on Medical Imaging",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Med Imaging"
                    ],
                    "issn": "0278-0062",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=42",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=42"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3e7805d27c2652b178d2925d2712f2daeec6040a",
                "title": "Graph Node Based Interpretability Guided Sample Selection for Active Learning",
                "abstract": "While supervised learning techniques have demonstrated state-of-the-art performance in many medical image analysis tasks, the role of sample selection is important. Selecting the most informative samples contributes to the system attaining optimum performance with minimum labeled samples, which translates to fewer expert interventions and cost. Active Learning (AL) methods for informative sample selection are effective in boosting performance of computer aided diagnosis systems when limited labels are available. Conventional approaches to AL have mostly focused on the single label setting where a sample has only one disease label from the set of possible labels. These approaches do not perform optimally in the multi-label setting where a sample can have multiple disease labels (e.g. in chest X-ray images). In this paper we propose a novel sample selection approach based on graph analysis to identify informative samples in a multi-label setting. For every analyzed sample, each class label is denoted as a separate node of a graph. Building on findings from interpretability of deep learning models, edge interactions in this graph characterize similarity between corresponding interpretability saliency map model encodings. We explore different types of graph aggregation to identify informative samples for active learning. We apply our method to public chest X-ray and medical image datasets, and report improved results over state-of-the-art AL techniques in terms of model performance, learning rates, and robustness.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "143784265",
                        "name": "D. Mahapatra"
                    },
                    {
                        "authorId": "5808616",
                        "name": "A. Poellinger"
                    },
                    {
                        "authorId": "21119833",
                        "name": "M. Reyes"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "In particular, Yuan et al. (2021) and Chefer et al. (2021) are promising examples of approaches based, partially or entirely, on gradient information.",
                "In particular, there have been some approaches based, partially or entirely, on gradient information, such as GradCam Yuan et al. (2021); Chefer et al. (2021)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "317208b423d24d52ba04221cfb46956962364e22",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-05506",
                    "ArXiv": "2210.05506",
                    "DOI": "10.48550/arXiv.2210.05506",
                    "CorpusId": 252815866
                },
                "corpusId": 252815866,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/317208b423d24d52ba04221cfb46956962364e22",
                "title": "Extracting Meaningful Attention on Source Code: An Empirical Study of Developer and Neural Model Code Exploration",
                "abstract": "The high effectiveness of neural models of code, such as OpenAI Codex and AlphaCode, suggests coding capabilities of models that are at least comparable to those of humans. However, previous work has only used these models for their raw completion, ignoring how the model reasoning, in the form of attention weights, can be used for other downstream tasks. Disregarding the attention weights means discarding a considerable portion of what those models compute when queried. To profit more from the knowledge embedded in these large pre-trained models, this work compares multiple approaches to post-process these valuable attention weights for supporting code exploration. Specifically, we compare to which extent the transformed attention signal of CodeGen, a large and publicly available pretrained neural model, agrees with how developers look at and explore code when each answering the same sense-making questions about code. At the core of our experimental evaluation, we collect, manually annotate, and open-source a novel eye-tracking dataset comprising 25 developers answering sense-making questions on code over 92 sessions. We empirically evaluate five attention-agnostic heuristics and ten attention-based post processing approaches of the attention signal against our ground truth of developers exploring code, including the novel concept of follow-up attention which exhibits the highest agreement. Beyond the dataset contribution and the empirical study, we also introduce a novel practical application of the attention signal of pre-trained models with completely analytical solutions, going beyond how neural models' attention mechanisms have traditionally been used.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2126465230",
                        "name": "Matteo Paltenghi"
                    },
                    {
                        "authorId": "40211404",
                        "name": "Rahul Pandita"
                    },
                    {
                        "authorId": "34968683",
                        "name": "Austin Z. Henley"
                    },
                    {
                        "authorId": "24939618",
                        "name": "Albert Ziegler"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Specifically, given a vision transformer model F and a sampled pair (x,y), we first acquire the class activated matrix C of the b\u2212th block of F following [6], which can be formulated as:"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1c947a0e33093052dba13be968e2cbab7554d197",
                "externalIds": {
                    "DBLP": "conf/mm/WangWYGWLL22",
                    "DOI": "10.1145/3503161.3547989",
                    "CorpusId": 252782229
                },
                "corpusId": 252782229,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1c947a0e33093052dba13be968e2cbab7554d197",
                "title": "Generating Transferable Adversarial Examples against Vision Transformers",
                "abstract": "Vision transformers (ViTs) are prevailing among several visual recognition tasks, therefore drawing intensive interest in generating adversarial examples against them. Different from CNNs, ViTs enjoy unique architectures, e.g., self-attention and image-embedding, which are commonly-shared features among various types of transformer-based models. However, existing adversarial methods suffer from weak transferable attacking ability due to the overlook of these architectural features. To address the problem, we propose an Architecture-oriented Transferable Attacking (ATA) framework to generate transferable adversarial examples by activating the uncertain attention and perturbing the sensitive embedding.Specifically, we first locate the patch-wise attentional regions that mostly affect model perception, therefore intensively activating the uncertainty of the attention mechanism and confusing the model decisions in turn.Furthermore, we search the pixel-wise attacking positions that are more likely to derange the embedded tokens using sensitive embedding perturbation, which could serve as a strong transferable attacking pattern.By jointly confusing the unique yet widely-used architectural features among transformer-based models, we can activate strong attacking transferability among diverse ViTs. Extensive experiments on large-scale dataset ImageNet using various popular transformers demonstrate that our ATA outperforms other baselines by large margins (at least +15% Attack Success Rate). Our code is available at https://github.com/nlsde-safety-team/ATA",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115830211",
                        "name": "Yuxuan Wang"
                    },
                    {
                        "authorId": "2109618469",
                        "name": "Jiakai Wang"
                    },
                    {
                        "authorId": "46401744",
                        "name": "Zixin Yin"
                    },
                    {
                        "authorId": "152217579",
                        "name": "Ruihao Gong"
                    },
                    {
                        "authorId": "2208109045",
                        "name": "Jingyi Wang"
                    },
                    {
                        "authorId": "153152072",
                        "name": "Aishan Liu"
                    },
                    {
                        "authorId": "6820648",
                        "name": "Xianglong Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In this experiment, we verify the improvement of AdsCVLR compared with single modal models on these \u201chard\u201d samples through a visualization method [1]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d165bb48f51b8ff7d4dd178fb8c03a24bc415581",
                "externalIds": {
                    "DBLP": "conf/mm/ZhuHZPLS0SDDZZZ22",
                    "DOI": "10.1145/3503161.3548226",
                    "CorpusId": 252783115
                },
                "corpusId": 252783115,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d165bb48f51b8ff7d4dd178fb8c03a24bc415581",
                "title": "AdsCVLR: Commercial Visual-Linguistic Representation Modeling in Sponsored Search",
                "abstract": "Sponsored search advertisements (ads) appear next to search results when consumers look for products and services on search engines. As the fundamental basis of search ads, relevance modeling has attracted increasing attention due to the significant research challenges and tremendous practical value. In this paper, we address the problem of multi-modal modeling in sponsored search, which models the relevance between user query and commercial ads with multi-modal structured information. To solve this problem, we propose a transformer architecture with Ads data on Commercial Visual-Linguistic Representation (AdsCVLR) with contrastive learning that naturally extends the transformer encoder with the complementary multi-modal inputs, serving as a strong aggregator of image-text features. We also make a public advertising dataset, which includes 480K labeled query-ad pairwise data with structured information of image, title, seller, description, and so on. Empirically, we evaluate the AdsCVLR model over the large industry dataset, and the experimental results of online/offline tests show the superiority of our method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2116513872",
                        "name": "Yongjie Zhu"
                    },
                    {
                        "authorId": "2187322388",
                        "name": "Chunhui Han"
                    },
                    {
                        "authorId": "1945466206",
                        "name": "Yu-Wei Zhan"
                    },
                    {
                        "authorId": "2139619289",
                        "name": "Bochen Pang"
                    },
                    {
                        "authorId": "49969609",
                        "name": "Zhaoju Li"
                    },
                    {
                        "authorId": "2118180377",
                        "name": "Hao Sun"
                    },
                    {
                        "authorId": "2118155941",
                        "name": "Si Li"
                    },
                    {
                        "authorId": "35580784",
                        "name": "Boxin Shi"
                    },
                    {
                        "authorId": "46429989",
                        "name": "Nan Duan"
                    },
                    {
                        "authorId": "2066621592",
                        "name": "Weiwei Deng"
                    },
                    {
                        "authorId": "2124601065",
                        "name": "Ruofei Zhang"
                    },
                    {
                        "authorId": "2146643615",
                        "name": "Liangjie Zhang"
                    },
                    {
                        "authorId": "2181348993",
                        "name": "Qi Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "We make a visualization on each level in Figure 6, we use the method proposed in [40,41] to generate the visualization maps."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a832ff78c1fb755e2b71a7fa7f0547e0fccd7942",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-03899",
                    "ArXiv": "2210.03899",
                    "DOI": "10.48550/arXiv.2210.03899",
                    "CorpusId": 252780341
                },
                "corpusId": 252780341,
                "publicationVenue": {
                    "id": "a8f26d13-e373-4e48-b57b-ef89bf48f4db",
                    "name": "Asian Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Asian Conf Comput Vis",
                        "ACCV"
                    ],
                    "url": "http://www.cvl.iis.u-tokyo.ac.jp/afcv/"
                },
                "url": "https://www.semanticscholar.org/paper/a832ff78c1fb755e2b71a7fa7f0547e0fccd7942",
                "title": "Multi-Scale Wavelet Transformer for Face Forgery Detection",
                "abstract": "Currently, many face forgery detection methods aggregate spatial and frequency features to enhance the generalization ability and gain promising performance under the cross-dataset scenario. However, these methods only leverage one level frequency information which limits their expressive ability. To overcome these limitations, we propose a multi-scale wavelet transformer framework for face forgery detection. Specifically, to take full advantage of the multi-scale and multi-frequency wavelet representation, we gradually aggregate the multi-scale wavelet representation at different stages of the backbone network. To better fuse the frequency feature with the spatial features, frequency-based spatial attention is designed to guide the spatial feature extractor to concentrate more on forgery traces. Meanwhile, cross-modality attention is proposed to fuse the frequency features with the spatial features. These two attention modules are calculated through a unified transformer block for efficiency. A wide variety of experiments demonstrate that the proposed method is efficient and effective for both within and cross datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Jie Liu"
                    },
                    {
                        "authorId": "2109838714",
                        "name": "Jingjing Wang"
                    },
                    {
                        "authorId": "2151333065",
                        "name": "Peng Zhang"
                    },
                    {
                        "authorId": "13817352",
                        "name": "Chunmao Wang"
                    },
                    {
                        "authorId": "50322310",
                        "name": "Di Xie"
                    },
                    {
                        "authorId": "3290437",
                        "name": "Shiliang Pu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "Ablation experiments and a variety of representation analyses have been applied to understand the role of the attention mechanism in NLP tasks as well as in transformer-based vision models (Chefer et al., 2021; Manning et al., 2020; Michel et al., 2019; Voita et al., 2019)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a5378175d31d3dd8fa004037df663aa00f236a0b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-00400",
                    "ArXiv": "2210.00400",
                    "DOI": "10.48550/arXiv.2210.00400",
                    "CorpusId": 252683821
                },
                "corpusId": 252683821,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a5378175d31d3dd8fa004037df663aa00f236a0b",
                "title": "Systematic Generalization and Emergent Structures in Transformers Trained on Structured Tasks",
                "abstract": "Transformer networks have seen great success in natural language processing and machine vision, where task objectives such as next word prediction and image classification benefit from nuanced context sensitivity across high-dimensional inputs. However, there is an ongoing debate about how and when transformers can acquire highly structured behavior and achieve systematic generalization. Here, we explore how well a causal transformer can perform a set of algorithmic tasks, including copying, sorting, and hierarchical compositions of these operations. We demonstrate strong generalization to sequences longer than those used in training by replacing the standard positional encoding typically used in transformers with labels arbitrarily paired with items in the sequence. We search for the layer and head configuration sufficient to solve these tasks, then probe for signs of systematic processing in latent representations and attention patterns. We show that two-layer transformers learn reliable solutions to multi-level problems, develop signs of task decomposition, and encode input items in a way that encourages the exploitation of shared computation across related tasks. These results provide key insights into how attention layers support structured computation both within a task and across multiple tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2144435606",
                        "name": "Yuxuan Li"
                    },
                    {
                        "authorId": "1701656",
                        "name": "James L. McClelland"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "Besides, Zabari & Hoshen (2021) obtains segments via interpretability Chefer et al. (2021b) based on gradient, but owing to the limited localization quality, additional unsupervised segmentation algorithm is required.",
                "To get rid of costly pixel-level annotations, some works generate the segments by retrieval Shin et al. (2022), grouping Xu et al. (2022) or unsupervised segmentation Zabari & Hoshen (2021) with gradient-based interpretability method Chefer et al. (2021b).",
                "We firstly compare our dense ITSM with smoothed min pooling with gradient based visualization method Bi-Module Chefer et al. (2021a), and our method performs much better than it."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e642628b46c783024c9aeea2616936b4d70730cf",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-13558",
                    "ArXiv": "2209.13558",
                    "DOI": "10.48550/arXiv.2209.13558",
                    "CorpusId": 252545060
                },
                "corpusId": 252545060,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e642628b46c783024c9aeea2616936b4d70730cf",
                "title": "FreeSeg: Free Mask from Interpretable Contrastive Language-Image Pretraining for Semantic Segmentation",
                "abstract": "Fully supervised semantic segmentation learns from dense masks, which requires heavy annotation cost for closed set. In this paper, we use natural language as supervision without any pixel-level annotation for open world segmentation. We call the proposed framework as FreeSeg, where the mask is freely available from raw feature map of pretraining model. Compared with zero-shot or openset segmentation, FreeSeg doesn\u2019t require any annotated masks, and it widely predicts categories beyond class-agnostic unsupervised segmentation. Specifically, FreeSeg obtains free mask from Image-Text Similarity Map (ITSM) of Interpretable Contrastive Language-Image Pretraining (ICLIP). And our core improvements are the smoothed min pooling for dense ICLIP, with the partial label and pixel strategies for segmentation. Furthermore, FreeSeg is very straight forward without complex design like grouping, clustering or retrieval. Besides the simplicity, the performances of FreeSeg surpass previous state-of-the-art at large margins, e.g. 13.4% higher at mIoU on VOC dataset in the same settings.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2153682292",
                        "name": "Yi Li"
                    },
                    {
                        "authorId": "2149485220",
                        "name": "Huifeng Yao"
                    },
                    {
                        "authorId": null,
                        "name": "Hualiang Wang"
                    },
                    {
                        "authorId": "48569608",
                        "name": "X. Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Head operations commonly found in the literature are averaging [6, 12, 33] and summing [34, 35] the attention matrices of each head.",
                "A recent method that does not solely rely on raw attention to provide explanations, is combining relevance and gradient information [6].",
                "Operations concerning the resulting matrices of self-attention layers include averaging [35] and multiplying [6].",
                "Other transformer-specific interpretability approaches combine attention with gradient information [6] or compute new attentions based on the network\u2019s residual connections [7].",
                "To obtain the final interpretation vector, a common approach is to consider the attention that each input token receives from the special [CLS] token that is prepended at the beginning of sequences in text classification tasks [6, 12]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "608244ccc7cdae31d63c533d2086747ee0f5b16a",
                "externalIds": {
                    "ArXiv": "2209.10876",
                    "DOI": "10.1007/s10618-023-00962-4",
                    "CorpusId": 262733410
                },
                "corpusId": 262733410,
                "publicationVenue": {
                    "id": "d263025a-9eaf-443f-9bbf-72377e8d22a6",
                    "name": "Data mining and knowledge discovery",
                    "type": "journal",
                    "alternate_names": [
                        "Data Mining and Knowledge Discovery",
                        "Data Min Knowl Discov",
                        "Data min knowl discov"
                    ],
                    "issn": "1384-5810",
                    "url": "https://www.springer.com/computer/database+management+&+information+retrieval/journal/10618",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10618",
                        "http://www.springer.com/computer/database+management+&+information+retrieval/journal/10618"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/608244ccc7cdae31d63c533d2086747ee0f5b16a",
                "title": "An attention matrix for every decision: faithfulness-based arbitration among multiple attention-based interpretations of transformers in text classification",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1500412311",
                        "name": "Nikolaos Mylonas"
                    },
                    {
                        "authorId": "2223815010",
                        "name": "Ioannis Mollas"
                    },
                    {
                        "authorId": "2502501",
                        "name": "Grigorios Tsoumakas"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "285d13bf3cbe6a8a0f164f584d84f8b74067271f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-11326",
                    "ArXiv": "2209.11326",
                    "DOI": "10.48550/arXiv.2209.11326",
                    "CorpusId": 252519203
                },
                "corpusId": 252519203,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/285d13bf3cbe6a8a0f164f584d84f8b74067271f",
                "title": "Towards Faithful Model Explanation in NLP: A Survey",
                "abstract": "End-to-end neural Natural Language Processing (NLP) models are notoriously difficult to understand. This has given rise to numerous efforts towards model explainability in recent years. One desideratum of model explanation is faithfulness, i.e. an explanation should accurately represent the reasoning process behind the model's prediction. In this survey, we review over 110 model explanation methods in NLP through the lens of faithfulness. We first discuss the definition and evaluation of faithfulness, as well as its significance for explainability. We then introduce recent advances in faithful explanation, grouping existing approaches into five categories: similarity methods, analysis of model-internal structures, backpropagation-based methods, counterfactual intervention, and self-explanatory models. For each category, we synthesize its representative studies, strengths, and weaknesses. Finally, we summarize their common virtues and remaining challenges, and reflect on future work directions towards faithful explainability in NLP.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1904906987",
                        "name": "QING LYU"
                    },
                    {
                        "authorId": "2817917",
                        "name": "Marianna Apidianaki"
                    },
                    {
                        "authorId": "1763608",
                        "name": "Chris Callison-Burch"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "[10] expand the class-agnostic self-attention of rollout to class-specific.",
                "[9] (Bi-Model) is the latest method based on [10] for transformer, and [33] (Grad-CAM) is the most used gradient based method.",
                "Note, Bi-Model [9] is based on the latest explainability method [10] for ViT.",
                "Besides, Bi-Model [9] based on [10] treats CLIP as ViT [14] and explains it with self-attention, even the quality of self-attention in CLIP is bad (see Fig.",
                "Most existing methods for visual explainability are designed for convolutional networks [41, 44] or vision transformers [9, 10].",
                "Another explainable method [9] treats CLIP as vision transformer [10] and relies on the self-attention of ViT [14] to explain the images, followed by rollout [1] to expand classagnostic attention map into class-specific."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "bfdcd7a3d415de2e222828ce478672e4d6b46d53",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-07046",
                    "ArXiv": "2209.07046",
                    "DOI": "10.48550/arXiv.2209.07046",
                    "CorpusId": 252280465
                },
                "corpusId": 252280465,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bfdcd7a3d415de2e222828ce478672e4d6b46d53",
                "title": "Exploring Visual Interpretability for Contrastive Language-Image Pre-training",
                "abstract": "Contrastive Language-Image Pre-training (CLIP) learns rich representations via readily available supervision of natural language. It improves the performance of downstream vision tasks, including but not limited to the zeroshot, long tail, segmentation, retrieval, caption, and video. However, the visual explainability of CLIP is rarely studied, especially for the raw feature map. To provide visual explanations of its predictions, we propose the Image-Text Similarity Map (ITSM). Based on it, we surprisingly find that CLIP prefers the background regions than the foregrounds, and shows erroneous visualization results against human understanding. This phenomenon is universal for both vision transformers and convolutional networks, which suggests this problem is unique and not owing to certain network. Experimentally, we find the devil is in the pooling part, where inappropriate pooling methods lead to a phenomenon called semantic shift. For this problem, we propose the Explainable Contrastive Language-Image Pretraining (ECLIP), which corrects the explainability via the Masked Max Pooling. Specifically, to avoid the semantic shift, we replace the original attention pooling by max pooling to focus on the confident foreground, with guidance from free attention during training. Experiments on three datasets suggest that ECLIP greatly improves the explainability of CLIP, and beyond previous explainability methods at large margins. The code will be released later.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2153682292",
                        "name": "Yi Li"
                    },
                    {
                        "authorId": "2155981033",
                        "name": "Hualiang Wang"
                    },
                    {
                        "authorId": "2107552104",
                        "name": "Yiqun Duan"
                    },
                    {
                        "authorId": "2143534537",
                        "name": "Han Xu"
                    },
                    {
                        "authorId": "2116433129",
                        "name": "Xiaomeng Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The literature (Chefer et al. 2021) proposed a DTD-based decomposition algorithm to solve the problems caused by residual connection and matrix multiplication operations.",
                "The power of these methods has led to their adoption in the field of language and vision (Chefer et al. 2021; Lu et al. 2019)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5b5908e9d5552123d04f575687587455e19187c0",
                "externalIds": {
                    "DBLP": "journals/jaihc/XuYDL23",
                    "DOI": "10.1007/s12652-022-04354-2",
                    "CorpusId": 251942826
                },
                "corpusId": 251942826,
                "publicationVenue": {
                    "id": "60538382-994e-41df-b512-6ff16fedd7cc",
                    "name": "Journal of Ambient Intelligence and Humanized Computing",
                    "type": "journal",
                    "alternate_names": [
                        "J Ambient Intell Humaniz Comput"
                    ],
                    "issn": "1868-5137",
                    "url": "http://www.springer.com/engineering/journal/12652",
                    "alternate_urls": [
                        "https://link.springer.com/journal/12652"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5b5908e9d5552123d04f575687587455e19187c0",
                "title": "Attribution rollout: a new way to interpret visual transformer",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2183447948",
                        "name": "Li Xu"
                    },
                    {
                        "authorId": "2116538002",
                        "name": "Xin Yan"
                    },
                    {
                        "authorId": "2113832334",
                        "name": "Weiyue Ding"
                    },
                    {
                        "authorId": "2109371144",
                        "name": "Zechao Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[28], which leads to low credibility of the final results."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0a74050a118013ef12807fcbcf99897cacc1c581",
                "externalIds": {
                    "DBLP": "journals/tmi/HuangHTMFXMSQ23",
                    "DOI": "10.1109/TMI.2022.3202248",
                    "CorpusId": 251865991,
                    "PubMed": "36018875"
                },
                "corpusId": 251865991,
                "publicationVenue": {
                    "id": "e0cda45d-3074-4ac0-80b8-e5250df00b89",
                    "name": "IEEE Transactions on Medical Imaging",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Med Imaging"
                    ],
                    "issn": "0278-0062",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=42",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=42"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0a74050a118013ef12807fcbcf99897cacc1c581",
                "title": "A ViT-AMC Network With Adaptive Model Fusion and Multiobjective Optimization for Interpretable Laryngeal Tumor Grading From Histopathological Images",
                "abstract": "The tumor grading of laryngeal cancer pathological images needs to be accurate and interpretable. The deep learning model based on the attention mechanism-integrated convolution (AMC) block has good inductive bias capability but poor interpretability, whereas the deep learning model based on the vision transformer (ViT) block has good interpretability but weak inductive bias ability. Therefore, we propose an end-to-end ViT-AMC network (ViT-AMCNet) with adaptive model fusion and multiobjective optimization that integrates and fuses the ViT and AMC blocks. However, existing model fusion methods often have negative fusion: 1). There is no guarantee that the ViT and AMC blocks will simultaneously have good feature representation capability. 2). The difference in feature representations learning between the ViT and AMC blocks is not obvious, so there is much redundant information in the two feature representations. Accordingly, we first prove the feasibility of fusing the ViT and AMC blocks based on Hoeffding\u2019s inequality. Then, we propose a multiobjective optimization method to solve the problem that ViT and AMC blocks cannot simultaneously have good feature representation. Finally, an adaptive model fusion method integrating the metrics block and the fusion block is proposed to increase the differences between feature representations and improve the deredundancy capability. Our methods improve the fusion ability of ViT-AMCNet, and experimental results demonstrate that ViT-AMCNet significantly outperforms state-of-the-art methods. Importantly, the visualized interpretive maps are closer to the region of interest of concern by pathologists, and the generalization ability is also excellent. Our code is publicly available at https://github.com/Baron-Huang/ViT-AMCNet.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1490938442",
                        "name": "Pan Huang"
                    },
                    {
                        "authorId": "145688449",
                        "name": "Peng He"
                    },
                    {
                        "authorId": "50991143",
                        "name": "Sukun Tian"
                    },
                    {
                        "authorId": "2088199107",
                        "name": "Mingrui Ma"
                    },
                    {
                        "authorId": "2056810826",
                        "name": "P. Feng"
                    },
                    {
                        "authorId": "2092636978",
                        "name": "Hualiang Xiao"
                    },
                    {
                        "authorId": "2112431",
                        "name": "F. Mercaldo"
                    },
                    {
                        "authorId": "1762147",
                        "name": "A. Santone"
                    },
                    {
                        "authorId": "144745977",
                        "name": "Jin Qin"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Here the method described in [3] is used, the corresponding values per token are referred to as \u201cattention weights\u201d here."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "55c8cb65e470e7c7aaf2892162466c8d428b2b91",
                "externalIds": {
                    "PubMedCentral": "9436379",
                    "DOI": "10.1073/pnas.2122636119",
                    "CorpusId": 251865581,
                    "PubMed": "36018838"
                },
                "corpusId": 251865581,
                "publicationVenue": {
                    "id": "bb95bf2e-8383-4748-bf9d-d6906d091085",
                    "name": "Proceedings of the National Academy of Sciences of the United States of America",
                    "type": "journal",
                    "alternate_names": [
                        "PNAS",
                        "PNAS online",
                        "Proceedings of the National Academy of Sciences of the United States of America.",
                        "Proc National Acad Sci",
                        "Proceedings of the National Academy of Sciences",
                        "Proc National Acad Sci u s Am"
                    ],
                    "issn": "0027-8424",
                    "alternate_issns": [
                        "1091-6490"
                    ],
                    "url": "https://www.jstor.org/journal/procnatiacadscie",
                    "alternate_urls": [
                        "http://www.pnas.org/",
                        "https://www.pnas.org/",
                        "http://www.jstor.org/journals/00278424.html",
                        "www.pnas.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/55c8cb65e470e7c7aaf2892162466c8d428b2b91",
                "title": "Taxonomic classification of DNA sequences beyond sequence similarity using deep neural networks",
                "abstract": "Significance The correct assignment of DNA sequences to their origin is an important task. However, only a fraction of all species are available in today\u2019s databases and thus easily assignable. Therefore, we present a method that is particularly good at classifying sequences for which there are no closely related species in databases. For this purpose, we use a deep learning approach to learn, at first, the \u201clanguage\u201d of DNA to subsequently distinguish the \u201clanguage\u201d structure of different groups of organisms, for example, bacteria and viruses. Using this approach, we achieve comparable quality to previous methods for sequences with close relatives in the database and superior quality for new species.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "90183777",
                        "name": "Florian Mock"
                    },
                    {
                        "authorId": "2119076544",
                        "name": "Fleming Kretschmer"
                    },
                    {
                        "authorId": "2119068694",
                        "name": "Anton Kriese"
                    },
                    {
                        "authorId": "1715474",
                        "name": "Sebastian B\u00f6cker"
                    },
                    {
                        "authorId": "2453279",
                        "name": "M. Marz"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "For ViTs, some reasons the decision-making process via gradients [10, 14, 15], attributions [6, 41] and redundancy reduction [26]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e983cdf156d932819a032837a9e11c8ee38566d6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-10431",
                    "ArXiv": "2208.10431",
                    "DOI": "10.48550/arXiv.2208.10431",
                    "CorpusId": 251718906
                },
                "corpusId": 251718906,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e983cdf156d932819a032837a9e11c8ee38566d6",
                "title": "ProtoPFormer: Concentrating on Prototypical Parts in Vision Transformers for Interpretable Image Recognition",
                "abstract": "Prototypical part network (ProtoPNet) has drawn wide attention and boosted many follow-up studies due to its self-explanatory property for explainable artificial intelligence (XAI). However, when directly applying ProtoPNet on vision transformer (ViT) backbones, learned prototypes have a\"distraction\"problem: they have a relatively high probability of being activated by the background and pay less attention to the foreground. The powerful capability of modeling long-term dependency makes the transformer-based ProtoPNet hard to focus on prototypical parts, thus severely impairing its inherent interpretability. This paper proposes prototypical part transformer (ProtoPFormer) for appropriately and effectively applying the prototype-based method with ViTs for interpretable image recognition. The proposed method introduces global and local prototypes for capturing and highlighting the representative holistic and partial features of targets according to the architectural characteristics of ViTs. The global prototypes are adopted to provide the global view of objects to guide local prototypes to concentrate on the foreground while eliminating the influence of the background. Afterwards, local prototypes are explicitly supervised to concentrate on their respective prototypical visual parts, increasing the overall interpretability. Extensive experiments demonstrate that our proposed global and local prototypes can mutually correct each other and jointly make final decisions, which faithfully and transparently reason the decision-making processes associatively from the whole and local perspectives, respectively. Moreover, ProtoPFormer consistently achieves superior performance and visualization results over the state-of-the-art (SOTA) prototype-based baselines. Our code has been released at https://github.com/zju-vipa/ProtoPFormer.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2065788410",
                        "name": "Mengqi Xue"
                    },
                    {
                        "authorId": "1720973203",
                        "name": "Qihan Huang"
                    },
                    {
                        "authorId": "1739347431",
                        "name": "Haofei Zhang"
                    },
                    {
                        "authorId": "26953623",
                        "name": "Lechao Cheng"
                    },
                    {
                        "authorId": "40403685",
                        "name": "Jie Song"
                    },
                    {
                        "authorId": "49227857",
                        "name": "Ming-hui Wu"
                    },
                    {
                        "authorId": "2152127912",
                        "name": "Mingli Song"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b463e9eaeb032103bcd57abe6f6df1c67dd804cb",
                "externalIds": {
                    "DOI": "10.1101/2022.08.18.22278971",
                    "CorpusId": 251669287
                },
                "corpusId": 251669287,
                "publicationVenue": {
                    "id": "d5e5b5e7-54b1-4f53-82fc-4853f3e71c58",
                    "name": "medRxiv",
                    "type": "journal",
                    "url": "https://www.medrxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/b463e9eaeb032103bcd57abe6f6df1c67dd804cb",
                "title": "Aiding Oral Squamous Cell Carcinoma diagnosis using Deep learning ConvMixer network",
                "abstract": "In recent years, Oral squamous cell carcinoma (OSCC) has become one of the world's most prevalent cancers, and it is becoming more prevalent in many populations. The high incidence rate, late diagnosis, and inadequate treatment planning continue to be major concerns. Despite the enhancement in the applications of deep learning algorithms for the medical field, late diagnosis, and approaches toward precision medicine for OSCC patients remain a challenge. Due to a lack of datasets and trained models with low computational costs, the diagnosis, an important cornerstone, is still done manually by oncologists. Although Convolutional neural networks (CNNs) have become the dominant architecture for vision tasks for plenty of years, recent experiments show that Transformer-based models, most noticeably the Vision Transformer (ViT), may out-compete them in some settings. Therefore, in this research, a method called ConvMixer, which combines great features from CNNs and patches based on ViT was applied to an original very small dataset of only 1224 images in total for 2 classes, Normal epithelium of the oral cavity (Normal) and OSCC, 696 slides for 400x magnification and 528 slides for 100x magnification. However, the proposed models with small parameters and data augmentation performed magnificently, with 400x magnification (Accuracy: 99.81% - F1score: 99.87%) and 100x magnification (Accuracy: 99.62% - F1score: 99.77%).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2242612842",
                        "name": "Nguyen Quoc"
                    },
                    {
                        "authorId": "2242609117",
                        "name": "Toan"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "To visualize the regions of the AS-OCT images that contributed to the model\u2019s decisions, Gradient-weighted Class Activation Mapping (Grad-CAM) [39] will be extracted from the first LN of the last block of the transformer encoder."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "33778be0dbd10abe3fc57db96b2bc2a8f3b35260",
                "externalIds": {
                    "PubMedCentral": "9371292",
                    "DOI": "10.1371/journal.pone.0270493",
                    "CorpusId": 251495076,
                    "PubMed": "35951641"
                },
                "corpusId": 251495076,
                "publicationVenue": {
                    "id": "0aed7a40-85f3-4c66-9e1b-c1556c57001b",
                    "name": "PLoS ONE",
                    "type": "journal",
                    "alternate_names": [
                        "Plo ONE",
                        "PLOS ONE",
                        "PLO ONE"
                    ],
                    "issn": "1932-6203",
                    "url": "https://journals.plos.org/plosone/",
                    "alternate_urls": [
                        "http://www.plosone.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/33778be0dbd10abe3fc57db96b2bc2a8f3b35260",
                "title": "Predicting demographic characteristics from anterior segment OCT images with deep learning: A study protocol",
                "abstract": "Introduction Anterior segment optical coherence tomography (AS-OCT) is a non-contact, rapid, and high-resolution in vivo modality for imaging of the eyeball\u2019s anterior segment structures. Because progressive anterior segment deformation is a hallmark of certain eye diseases such as angle-closure glaucoma, identification of AS-OCT structural changes over time is fundamental to their diagnosis and monitoring. Detection of pathologic damage, however, relies on the ability to differentiate it from normal, age-related structural changes. Methods and analysis This proposed large-scale, retrospective cross-sectional study will determine whether demographic characteristics including age can be predicted from deep learning analysis of AS-OCT images; it will also assess the importance of specific anterior segment areas of the eyeball to the prediction. We plan to extract, from SUPREME\u00ae, a clinical data warehouse (CDW) of Seoul National University Hospital (SNUH; Seoul, South Korea), a list of patients (at least 2,000) who underwent AS-OCT imaging between 2008 and 2020. AS-OCT images as well as demographic characteristics including age, gender, height, weight and body mass index (BMI) will be collected from electronic medical records (EMRs). The dataset of horizontal AS-OCT images will be split into training (80%), validation (10%), and test (10%) datasets, and a Vision Transformer (ViT) model will be built to predict demographics. Gradient-weighted Class Activation Mapping (Grad-CAM) will be used to visualize the regions of AS-OCT images that contributed to the model\u2019s decisions. The accuracy, sensitivity, specificity, and area under the receiver operating characteristic (ROC) curve (AUC) will be applied to evaluate the model performance. Conclusion This paper presents a study protocol for prediction of demographic characteristics from AS-OCT images of the eyeball using a deep learning model. The results of this study will aid clinicians in understanding and identifying age-related structural changes and other demographics-based structural differences. Trial registration Registration ID with open science framework: 10.17605/OSF.IO/FQ46X.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "39256453",
                        "name": "Y. Lee"
                    },
                    {
                        "authorId": "41133631",
                        "name": "Sukkyu Sun"
                    },
                    {
                        "authorId": "2538007",
                        "name": "Young Kook Kim"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "And [49], [80], [219] expanded on this approach toward the goal of multi-step attribution across multiple layers."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2c709ef6186bd607494a3344c903552ea500e449",
                "externalIds": {
                    "ArXiv": "2207.13243",
                    "DBLP": "journals/corr/abs-2207-13243",
                    "DOI": "10.1109/SaTML54575.2023.00039",
                    "CorpusId": 251104722
                },
                "corpusId": 251104722,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2c709ef6186bd607494a3344c903552ea500e449",
                "title": "Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks",
                "abstract": "The last decade of machine learning has seen drastic increases in scale and capabilities. Deep neural networks (DNNs) are increasingly being deployed in the real world. However, they are difficult to analyze, raising concerns about using them without a rigorous understanding of how they function. Effective tools for interpreting them will be important for building more trustworthy AI by helping to identify problems, fix bugs, and improve basic understanding. In particular, \u201cinner\u201d interpretability techniques, which focus on explaining the internal components of DNNs, are well-suited for developing a mechanistic understanding, guiding manual modifications, and reverse engineering solutions. Much recent work has focused on DNN interpretability, and rapid progress has thus far made a thorough systematization of methods difficult. In this survey, we review over 300 works with a focus on inner interpretability tools. We introduce a taxonomy that classifies methods by what part of the network they help to explain (weights, neurons, subnetworks, or latent representations) and whether they are implemented during (intrinsic) or after (post hoc) training. To our knowledge, we are also the first to survey a number of connections between interpretability research and work in adversarial robustness, continual learning, modularity, network compression, and studying the human visual system. We discuss key challenges and argue that the status quo in interpretability research is largely unproductive. Finally, we highlight the importance of future work that emphasizes diagnostics, debugging, adversaries, and benchmarking in order to make interpretability tools more useful to engineers in practical applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2179318557",
                        "name": "Tilman Raukur"
                    },
                    {
                        "authorId": "120892153",
                        "name": "A. Ho"
                    },
                    {
                        "authorId": "2103487700",
                        "name": "Stephen Casper"
                    },
                    {
                        "authorId": "1397904824",
                        "name": "Dylan Hadfield-Menell"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Recent studies have therefore been attempting to improve the quality and sharpness of the maps by focusing on objects, for example, by combining Grad-CAM and LRP [25], applying LRP to Vision Transformers [5], improving ABN with Score-CAM [26], and even human intervention [33] or additional supervision [27]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "af36c48d69e01e5bd5b2de88d84ccfbf9c185578",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-13306",
                    "ArXiv": "2207.13306",
                    "DOI": "10.1587/transinf.2022EDP7138",
                    "CorpusId": 251105159
                },
                "corpusId": 251105159,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/af36c48d69e01e5bd5b2de88d84ccfbf9c185578",
                "title": "Object-ABN: Learning to Generate Sharp Attention Maps for Action Recognition",
                "abstract": "In this paper we propose an extension of the Attention Branch Network (ABN) by using instance segmentation for generating sharper attention maps for action recognition. Methods for visual explanation such as Grad-CAM usually generate blurry maps which are not intuitive for humans to understand, particularly in recognizing actions of people in videos. Our proposed method, Object-ABN, tackles this issue by introducing a new mask loss that makes the generated attention maps close to the instance segmentation result. Further the PC loss and multiple attention maps are introduced to enhance the sharpness of the maps and improve the performance of classification. Experimental results with UCF101 and SSv2 shows that the generated maps by the proposed method are much clearer qualitatively and quantitatively than those of the original ABN.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2161242789",
                        "name": "Tomoya Nitta"
                    },
                    {
                        "authorId": "134790239",
                        "name": "Tsubasa Hirakawa"
                    },
                    {
                        "authorId": "1687968",
                        "name": "H. Fujiyoshi"
                    },
                    {
                        "authorId": "2060243644",
                        "name": "Toru Tamaki"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "The latest researches\u2019 results [60]\u2013[65]"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e0e3299d34a3625c98d833ef8597f4f48be6ebe6",
                "externalIds": {
                    "DOI": "10.1109/ICIVC55077.2022.9886081",
                    "CorpusId": 252387391
                },
                "corpusId": 252387391,
                "publicationVenue": {
                    "id": "8fb9dbeb-ccee-4192-8549-ccfdf8c5706b",
                    "name": "International Conference on Image, Vision and Computing",
                    "type": "conference",
                    "alternate_names": [
                        "ICIVC",
                        "Int Conf Image Vis Comput"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e0e3299d34a3625c98d833ef8597f4f48be6ebe6",
                "title": "Review of Human Violence Recognition Algorithms",
                "abstract": "Violent behavior recognition is a specific research direction of human behavior recognition. Published reviews mainly focuses on the development of deep learning in the field of behavior recognition, and the attention to violent behavior recognition is low. According to the different methods used, this paper analyzes various algorithms from the perspective of violence recognition based on manual descriptor and deep learning, and introduces the commonly used datasets. Finally, the characteristics of different models and algorithms are summarized, and discuss potential problems and future work in the field of violence recognition.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2162411573",
                        "name": "Youshan Zhang"
                    },
                    {
                        "authorId": "2166960701",
                        "name": "Shaozhe Guo"
                    },
                    {
                        "authorId": "2154404915",
                        "name": "Yong Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In contrast to pure self-attention based ViTs, the enhanced visual interpretability of hybrid ViTs [28, 54] allows us to extract these salient regions utilizing general methods such as gradient based class activation maps (Grad-CAM) [4, 38]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2a688f8594bda539800367543f879f9730113d58",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-00777",
                    "ArXiv": "2208.00777",
                    "DOI": "10.1109/CVPRW59228.2023.00240",
                    "CorpusId": 251223760
                },
                "corpusId": 251223760,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2a688f8594bda539800367543f879f9730113d58",
                "title": "D3Former: Debiased Dual Distilled Transformer for Incremental Learning",
                "abstract": "In class incremental learning (CIL) setting, groups of classes are introduced to a model in each learning phase. The goal is to learn a unified model performant on all the classes observed so far. Given the recent popularity of Vision Transformers (ViTs) in conventional classification settings, an interesting question is to study their continual learning behaviour. In this work, we develop a Debiased Dual Distilled Transformer for CIL dubbed D3Former. The proposed model leverages a hybrid nested ViT design to ensure data efficiency and scalability to small as well as large datasets. In contrast to a recent ViT based CIL approach, our D3Former does not dynamically expand its architecture when new tasks are learned and remains suitable for a large number of incremental tasks. The improved CIL behaviour of D3Former owes to two fundamental changes to the ViT design. First, we treat the incremental learning as a long-tail classification problem where the majority samples from new classes vastly outnumber the limited exemplars available for old classes. To avoid the bias against the minority old classes, we propose to dynamically adjust logits to emphasize on retaining the representations relevant to old tasks. Second, we propose to preserve the configuration of spatial attention maps as the learning progresses across tasks. This helps in reducing catastrophic forgetting by constraining the model to retain the attention on the most discriminative regions. D3Former obtains favorable results on incremental versions of CIFAR-100, MNIST, SVHN, and ImageNet datasets. Code is available at https://tinyurl.com/d3former.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40360972",
                        "name": "Abdel-rahman Mohamed"
                    },
                    {
                        "authorId": "2121406309",
                        "name": "Rushali Grandhe"
                    },
                    {
                        "authorId": "2179880769",
                        "name": "KJ Joseph"
                    },
                    {
                        "authorId": "2179960157",
                        "name": "Salman A. Khan"
                    },
                    {
                        "authorId": "2358803",
                        "name": "F. Khan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "It is a interesting future direction to investigate how different VLMs [7\u201311], their training procedure [43], and different relevancy approaches [18, 22, 44, 45] affect the performance of different downstream 3D scene understanding tasks."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d5c4550d285b57111e52c5956dbc40942d36b117",
                "externalIds": {
                    "DBLP": "conf/corl/HaS22",
                    "ArXiv": "2207.11514",
                    "DOI": "10.48550/arXiv.2207.11514",
                    "CorpusId": 251040433
                },
                "corpusId": 251040433,
                "publicationVenue": {
                    "id": "fbfbf10a-faa4-4d2a-85be-3ac660454ce3",
                    "name": "Conference on Robot Learning",
                    "type": "conference",
                    "alternate_names": [
                        "CoRL",
                        "Conf Robot Learn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d5c4550d285b57111e52c5956dbc40942d36b117",
                "title": "Semantic Abstraction: Open-World 3D Scene Understanding from 2D Vision-Language Models",
                "abstract": "We study open-world 3D scene understanding, a family of tasks that require agents to reason about their 3D environment with an open-set vocabulary and out-of-domain visual inputs - a critical skill for robots to operate in the unstructured 3D world. Towards this end, we propose Semantic Abstraction (SemAbs), a framework that equips 2D Vision-Language Models (VLMs) with new 3D spatial capabilities, while maintaining their zero-shot robustness. We achieve this abstraction using relevancy maps extracted from CLIP, and learn 3D spatial and geometric reasoning skills on top of those abstractions in a semantic-agnostic manner. We demonstrate the usefulness of SemAbs on two open-world 3D scene understanding tasks: 1) completing partially observed objects and 2) localizing hidden objects from language descriptions. Experiments show that SemAbs can generalize to novel vocabulary, materials/lighting, classes, and domains (i.e., real-world scans) from training on limited 3D synthetic data. Code and data is available at https://semantic-abstraction.cs.columbia.edu/",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9091886",
                        "name": "Huy Ha"
                    },
                    {
                        "authorId": "3340170",
                        "name": "Shuran Song"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "03e9c708d5754ee8c67547fa435807888a07f753",
                "externalIds": {
                    "DOI": "10.1109/ISPDS56360.2022.9874093",
                    "CorpusId": 252113129
                },
                "corpusId": 252113129,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/03e9c708d5754ee8c67547fa435807888a07f753",
                "title": "Cervical cell classification based on attention mechanism and multi-resolution feature fusion",
                "abstract": "Liquid-based thin-layer cell smears are very important for the early screening and prevention of cervical cancer, and computer-aided diagnosis can reduce the workload of pathologists. The cell classification method based on deep learning can process data efficiently. However, most classification methods are based on a single resolution for recognition. When the resolution is low, the processing speed of the whole slide image is faster, but lack of picture details, which makes the identification inaccurate. When the resolution is high, it takes more time to process the whole slide image, but with more image detail. To this end, we propose a model based on Attention Mechanism and Multi-resolution Feature Fusion Module (AMFM), which combine the advantages of various resolutions to classify cervical cells. Experiments show that the accuracy is increased by 3.93% and the AUC is improved by 0.022 on the four-classification task of the cervical cell compared to the model based on a single resolution.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49403006",
                        "name": "Jingya Yu"
                    },
                    {
                        "authorId": "50248653",
                        "name": "Guoyou Wang"
                    },
                    {
                        "authorId": "8679233",
                        "name": "Shenghua Cheng"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Our intuition is that the neural activation map of even a partially trained classification network can better localize some part of an object [40,4] than using naive score averaging."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d97becb3abec3c6646f0524827bbde3b6b8cf598",
                "externalIds": {
                    "ArXiv": "2207.08409",
                    "DBLP": "conf/eccv/LiuLZ0022",
                    "DOI": "10.48550/arXiv.2207.08409",
                    "CorpusId": 250627606
                },
                "corpusId": 250627606,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/d97becb3abec3c6646f0524827bbde3b6b8cf598",
                "title": "TokenMix: Rethinking Image Mixing for Data Augmentation in Vision Transformers",
                "abstract": "CutMix is a popular augmentation technique commonly used for training modern convolutional and transformer vision networks. It was originally designed to encourage Convolution Neural Networks (CNNs) to focus more on an image's global context instead of local information, which greatly improves the performance of CNNs. However, we found it to have limited benefits for transformer-based architectures that naturally have a global receptive field. In this paper, we propose a novel data augmentation technique TokenMix to improve the performance of vision transformers. TokenMix mixes two images at token level via partitioning the mixing region into multiple separated parts. Besides, we show that the mixed learning target in CutMix, a linear combination of a pair of the ground truth labels, might be inaccurate and sometimes counter-intuitive. To obtain a more suitable target, we propose to assign the target score according to the content-based neural activation maps of the two images from a pre-trained teacher model, which does not need to have high performance. With plenty of experiments on various vision transformer architectures, we show that our proposed TokenMix helps vision transformers focus on the foreground area to infer the classes and enhances their robustness to occlusion, with consistent performance gains. Notably, we improve DeiT-T/S/B with +1% ImageNet top-1 accuracy. Besides, TokenMix enjoys longer training, which achieves 81.2% top-1 accuracy on ImageNet with DeiT-S trained for 400 epochs. Code is available at https://github.com/Sense-X/TokenMix.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2337079",
                        "name": "Jihao Liu"
                    },
                    {
                        "authorId": "152847208",
                        "name": "B. Liu"
                    },
                    {
                        "authorId": "145798292",
                        "name": "Hang Zhou"
                    },
                    {
                        "authorId": "47893312",
                        "name": "Hongsheng Li"
                    },
                    {
                        "authorId": "2146400394",
                        "name": "Yu Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Although it is wrong to equate attention scores with explanation [44], it can offer plausible and meaningful interpretations [45], [46]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d8ecabb06862519333a9ae3ba11e7183130abb9e",
                "externalIds": {
                    "ArXiv": "2207.06959",
                    "DBLP": "journals/corr/abs-2207-06959",
                    "DOI": "10.48550/arXiv.2207.06959",
                    "CorpusId": 250526686
                },
                "corpusId": 250526686,
                "publicationVenue": {
                    "id": "c6840156-ee10-4d78-8832-7f8909811576",
                    "name": "IEEE Transactions on Knowledge and Data Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Knowl Data Eng"
                    ],
                    "issn": "1041-4347",
                    "url": "https://www.computer.org/web/tkde",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=69"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d8ecabb06862519333a9ae3ba11e7183130abb9e",
                "title": "Spatiotemporal Propagation Learning for Network-Wide Flight Delay Prediction",
                "abstract": "Demystifying the delay propagation mechanisms among multiple airports is fundamental to precise and interpretable delay prediction, which is crucial during decision-making for all aviation industry stakeholders. The principal challenge lies in effectively leveraging the spatiotemporal dependencies and exogenous factors related to the delay propagation. However, previous works only consider limited spatiotemporal patterns with few factors. To promote more comprehensive propagation modeling for delay prediction, we propose SpatioTemporal Propagation Network (STPN), a space-time separable graph convolutional network, which is novel in spatiotemporal dependency capturing. From the aspect of spatial relation modeling, we propose a multi-graph convolution model considering both geographic proximity and airline schedule. From the aspect of temporal dependency capturing, we propose a multi-head self-attentional mechanism that can be learned end-to-end and explicitly reason multiple kinds of temporal dependency of delay time series. We show that the joint spatial and temporal learning models yield a sum of the Kronecker product, which factors the spatiotemporal dependence into the sum of several spatial and temporal adjacency matrices. By this means, STPN allows cross-talk of spatial and temporal factors for modeling delay propagation. Furthermore, a squeeze and excitation module is added to each layer of STPN to boost meaningful spatiotemporal features. To this end, we apply STPN to multi-step ahead arrival and departure delay prediction in large-scale airport networks. To validate the effectiveness of our model, we experiment with two real-world delay datasets, including U.S and China flight delays; and we show that STPN outperforms state-of-the-art methods. In addition, counterfactuals produced by STPN show that it learns explainable delay propagation patterns.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1641939256",
                        "name": "Yuankai Wu"
                    },
                    {
                        "authorId": "2118616775",
                        "name": "Hongyu Yang"
                    },
                    {
                        "authorId": "2156012350",
                        "name": "Yi Lin"
                    },
                    {
                        "authorId": "2118903301",
                        "name": "Hong Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[13] proposed a layer-wise relevance propagation (LRP) method by introducing a relevancy propagation rule that is applicable to both positive and negative contributions.",
                "[13] proposed a layer-wise relevance propagation (LRP) method to compute different relevance of the attention heads throughout the transformer."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1914a3a1ccc383a1a6cc33f5c91f926e7aac794a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-05358",
                    "ArXiv": "2207.05358",
                    "DOI": "10.48550/arXiv.2207.05358",
                    "CorpusId": 250451438
                },
                "corpusId": 250451438,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1914a3a1ccc383a1a6cc33f5c91f926e7aac794a",
                "title": "eX-ViT: A Novel eXplainable Vision Transformer for Weakly Supervised Semantic Segmentation",
                "abstract": "Recently vision transformer models have become prominent models for a range of vision tasks. These models, however, are usually opaque with weak feature interpretability. Moreover, there is no method currently built for an intrinsically interpretable transformer, which is able to explain its reasoning process and provide a faithful explanation. To close these crucial gaps, we propose a novel vision transformer dubbed the eXplainable Vision Transformer (eX-ViT), an intrinsically interpretable transformer model that is able to jointly discover robust interpretable features and perform the prediction. Specifically, eX-ViT is composed of the Explainable Multi-Head Attention (E-MHA) module, the Attribute-guided Explainer (AttE) module and the self-supervised attribute-guided loss. The E-MHA tailors explainable attention weights that are able to learn semantically interpretable representations from local patches in terms of model decisions with noise robustness. Meanwhile, AttE is proposed to encode discriminative attribute features for the target object through diverse attribute discovery, which constitutes faithful evidence for the model's predictions. In addition, a self-supervised attribute-guided loss is developed for our eX-ViT, which aims at learning enhanced representations through the attribute discriminability mechanism and attribute diversity mechanism, to localize diverse and discriminative attributes and generate more robust explanations. As a result, we can uncover faithful and robust interpretations with diverse attributes through the proposed eX-ViT.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2112484303",
                        "name": "Lu Yu"
                    },
                    {
                        "authorId": "2052727822",
                        "name": "Wei Xiang"
                    },
                    {
                        "authorId": "2115219377",
                        "name": "Juan Fang"
                    },
                    {
                        "authorId": "2109381756",
                        "name": "Yi-Ping Phoebe Chen"
                    },
                    {
                        "authorId": "32816039",
                        "name": "Lianhua Chi"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Alternatives include methods to compute and propagate trained attention-based token relevancy scores (Chefer et al., 2021), or to generate higher-level conceptual explanations (Rigotti et al., 2021).",
                "Alternatives include methods to compute and propagate trained attention-based token relevancy scores (Chefer et al., 2021), or to generate higher-level conceptual explanations (Rigotti et al."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c85bb5fba72f85b852a455e70d569a301367f21d",
                "externalIds": {
                    "DOI": "10.1101/2022.07.07.499217",
                    "CorpusId": 250459257
                },
                "corpusId": 250459257,
                "publicationVenue": {
                    "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
                    "name": "bioRxiv",
                    "type": "journal",
                    "url": "http://biorxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/c85bb5fba72f85b852a455e70d569a301367f21d",
                "title": "SNVformer: An Attention-based Deep Neural Network for GWAS Data",
                "abstract": "Despite being the widely-used gold standard for linking common genetic variations to phenotypes and disease, genome-wide association studies (GWAS) suffer major limitations, partially attributable to the reliance on simple, typically linear, models of genetic effects. More elaborate methods, such as epistasis-aware models, typically struggle with the scale of GWAS data. In this paper, we build on recent advances in neural networks employing Transformer-based architectures to enable such models at a large scale. As a first step towards replacing linear GWAS with a more expressive approximation, we demonstrate prediction of gout, a painful form of inflammatory arthritis arising when monosodium urate crystals form in the joints under high serum urate conditions, from Single Nucleotide Variants (SNVs) using a scalable (long input) variant of the Transformer architecture. Furthermore, we show that sparse SNVs can be efficiently used by these Transformer-based networks without expanding them to a full genome. By appropriately encoding SNVs, we are able to achieve competitive initial performance, with an AUROC of 83% when classifying a balanced test set using genotype and demographic information. Moreover, the confidence with which the network makes its prediction is a good indication of the prediction accuracy. Our results indicate a number of opportunities for extension, enabling full genome-scale data analysis using more complex and accurate genotype-phenotype association models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1580351587",
                        "name": "Kieran Elmes"
                    },
                    {
                        "authorId": "1413955731",
                        "name": "Diana Benavides-Prado"
                    },
                    {
                        "authorId": "2151063121",
                        "name": "N. Tan"
                    },
                    {
                        "authorId": "2175833020",
                        "name": "T. Nguyen"
                    },
                    {
                        "authorId": "31237591",
                        "name": "N. Sumpter"
                    },
                    {
                        "authorId": "39580039",
                        "name": "M. Leask"
                    },
                    {
                        "authorId": "2582677",
                        "name": "Michael Witbrock"
                    },
                    {
                        "authorId": "144086085",
                        "name": "A. Gavryushkin"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "For each layer, we present the patch-to-patch attention matrix (size: 180\u00d7180) calculated by the rollout method in [32].",
                "[32] enhanced this method by adding an additional identical matrix before multiplication to simulate the effect of residual connection of MSA."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "775fc1fe716099c537b125d6b8145a988f6950e4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-03927",
                    "ArXiv": "2207.03927",
                    "DOI": "10.48550/arXiv.2207.03927",
                    "CorpusId": 250408242
                },
                "corpusId": 250408242,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/775fc1fe716099c537b125d6b8145a988f6950e4",
                "title": "BAST: Binaural Audio Spectrogram Transformer for Binaural Sound Localization",
                "abstract": "Accurate sound localization in a reverberation environment is essential for human auditory perception. Recently, Convolutional Neural Networks (CNNs) have been utilized to model the binaural human auditory pathway. However, CNN shows barriers in capturing the global acoustic features. To address this issue, we propose a novel end-to-end Binaural Audio Spectrogram Transformer (BAST) model to predict the sound azimuth in both anechoic and reverberation environments. Two modes of implementation, i.e. BAST-SP and BAST-NSP corresponding to BAST model with shared and non-shared parameters respectively, are explored. Our model with subtraction interaural integration and hybrid loss achieves an angular distance of 1.29 degrees and a Mean Square Error of 1e-3 at all azimuths, significantly surpassing CNN based model. The exploratory analysis of the BAST's performance on the left-right hemifields and anechoic and reverberation environments shows its generalization ability as well as the feasibility of binaural Transformers in sound localization. Furthermore, the analysis of the attention maps is provided to give additional insights on the interpretation of the localization process in a natural reverberant environment.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2175558528",
                        "name": "Sheng Kuang"
                    },
                    {
                        "authorId": "2047183326",
                        "name": "Kiki van der Heijden"
                    },
                    {
                        "authorId": "2950749",
                        "name": "S. Mehrkanoon"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "(d) visualizes the impact of each spatial location on the final prediction in the DeiT-S model [47] using the visualization method proposed in [4]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "968f628c3d42dbfd16fd4516e61cfedc16612310",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-01580",
                    "ArXiv": "2207.01580",
                    "DOI": "10.1109/TPAMI.2023.3263826",
                    "CorpusId": 250264184,
                    "PubMed": "37030709"
                },
                "corpusId": 250264184,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/968f628c3d42dbfd16fd4516e61cfedc16612310",
                "title": "Dynamic Spatial Sparsification for Efficient Vision Transformers and Convolutional Neural Networks",
                "abstract": "In this paper, we present a new approach for model acceleration by exploiting spatial sparsity in visual data. We observe that the final prediction in vision Transformers is only based on a subset of the most informative regions, which is sufficient for accurate image recognition. Based on this observation, we propose a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input to accelerate vision Transformers. Specifically, we devise a lightweight prediction module to estimate the importance of each token given the current features. The module is added to different layers to prune redundant tokens hierarchically. While the framework is inspired by our observation of the sparse attention in vision Transformers, we find that the idea of adaptive and asymmetric computation can be a general solution for accelerating various architectures. We extend our method to hierarchical models including CNNs and hierarchical vision Transformers as well as more complex dense prediction tasks. To handle structured feature maps, we formulate a generic dynamic spatial sparsification framework with progressive sparsification and asymmetric computation for different spatial locations. By applying lightweight fast paths to less informative features and expressive slow paths to important locations, we can maintain the complete structure of feature maps while significantly reducing the overall computations. Extensive experiments on diverse modern architectures and different visual tasks demonstrate the effectiveness of our proposed framework. By hierarchically pruning 66% of the input tokens, our method greatly reduces 31% $\\sim$\u223c 35% FLOPs and improves the throughput by over 40% while the drop of accuracy is within 0.5% for various vision Transformers. By introducing asymmetric computation, a similar acceleration can be achieved on modern CNNs and Swin Transformers. Moreover, our method achieves promising results on more complex tasks including semantic segmentation and object detection. Our results clearly demonstrate that dynamic spatial sparsification offers a new and more effective dimension for model acceleration. Code is available at https://github.com/raoyongming/DynamicViT.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "39358728",
                        "name": "Yongming Rao"
                    },
                    {
                        "authorId": "2124814824",
                        "name": "Zuyan Liu"
                    },
                    {
                        "authorId": "2118223312",
                        "name": "Wenliang Zhao"
                    },
                    {
                        "authorId": "48128428",
                        "name": "Jie Zhou"
                    },
                    {
                        "authorId": "1697700",
                        "name": "Jiwen Lu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1df8ce9e21c544a8ba0911e3e7825abc752236eb",
                "externalIds": {
                    "DBLP": "conf/aaai/GiangSJ23",
                    "ArXiv": "2207.00328",
                    "DOI": "10.1609/aaai.v37i2.25341",
                    "CorpusId": 251903558
                },
                "corpusId": 251903558,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/1df8ce9e21c544a8ba0911e3e7825abc752236eb",
                "title": "TopicFM: Robust and Interpretable Topic-Assisted Feature Matching",
                "abstract": "This study addresses an image-matching problem in challenging cases, such as large scene variations or textureless scenes. To gain robustness to such situations, most previous studies have attempted to encode the global contexts of a scene via graph neural networks or transformers. However, these contexts do not explicitly represent high-level contextual information, such as structural shapes or semantic instances; therefore, the encoded features are still not sufficiently discriminative in challenging scenes. We propose a novel image-matching method that applies a topic-modeling strategy to encode high-level contexts in images. The proposed method trains latent semantic instances called topics. It explicitly models an image as a multinomial distribution of topics, and then performs probabilistic feature matching. This approach improves the robustness of matching by focusing on the same semantic areas between the images. In addition, the inferred topics provide interpretability for matching the results, making our method explainable. Extensive experiments on outdoor and indoor datasets show that our method outperforms other state-of-the-art methods, particularly in challenging cases.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2145154617",
                        "name": "Khang Truong Giang"
                    },
                    {
                        "authorId": "2133985",
                        "name": "Soohwan Song"
                    },
                    {
                        "authorId": "2146863514",
                        "name": "Sung-Guk Jo"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3c3b1dbfd5685c9f34300943f4568f4a79b70008",
                "externalIds": {
                    "DOI": "10.1117/1.JEI.31.4.043030",
                    "CorpusId": 251417563
                },
                "corpusId": 251417563,
                "publicationVenue": {
                    "id": "c677ab24-0c04-487d-83e2-c252af9479c8",
                    "name": "Journal of Electronic Imaging (JEI)",
                    "type": "journal",
                    "alternate_names": [
                        "J Electron Imaging (JEI",
                        "Journal of Electronic Imaging",
                        "J Electron Imaging"
                    ],
                    "issn": "1017-9909",
                    "url": "https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging",
                    "alternate_urls": [
                        "http://electronicimaging.spiedigitallibrary.org/journal.aspx"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3c3b1dbfd5685c9f34300943f4568f4a79b70008",
                "title": "Not all temporal shift modules are profitable",
                "abstract": "Abstract. With the increasing coverage of video surveillance systems in modern society, demand for using artificial intelligence algorithm to replace humans in violent behavior recognition has also become stronger. By moving some channels in the temporal dimension, temporary shift module (TSM) can achieve the performance of three-dimensional convolution neural network (CNN) with the complexity of two-dimensional CNN, and extract the temporal and spatial information at the same time. Our intuition is that too many temporary shift modules may fuse too much action information in each frame, which weakens the capability of CNN on spatiotemporal information extraction. To verify the aforementioned conjecture, we adjusted the network structure based on TSM, proposed partial TSM, selected the optimal model through experiments, and verified the performance of the algorithm on multiple datasets and our expanded datasets. The proposed optimal model not only reduced the memory usage of hardware but also achieved higher accuracy on multiple datasets with 77.3% running time. Meanwhile, we achieved state-of-the-art performance of 91% on RWF-2000 dataset.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2162411573",
                        "name": "Youshan Zhang"
                    },
                    {
                        "authorId": "2154404915",
                        "name": "Yong Li"
                    },
                    {
                        "authorId": "2166960701",
                        "name": "Shaozhe Guo"
                    },
                    {
                        "authorId": "153461571",
                        "name": "Q. Liang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fe727f89a2545cb2facb1de88b3e35717b570893",
                "externalIds": {
                    "DBLP": "conf/ijcai/NiuWWW22",
                    "DOI": "10.24963/ijcai.2022/102",
                    "CorpusId": 250639920
                },
                "corpusId": 250639920,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/fe727f89a2545cb2facb1de88b3e35717b570893",
                "title": "AttExplainer: Explain Transformer via Attention by Reinforcement Learning",
                "abstract": "Transformer and its variants, built based on attention mechanisms, have recently achieved remarkable performance in many NLP tasks. Most existing works on Transformer explanation tend to reveal and utilize the attention matrix with human subjective intuitions in a qualitative manner. However, the huge size of dimensions directly challenges these methods to quantitatively analyze the attention matrix. Therefore, in this paper, we propose a novel reinforcement learning (RL) based framework for Transformer explanation via attention matrix, namely AttExplainer. The RL agent learns to perform step-by-step masking operations by observing the change in attention matrices. We have adapted our method to two scenarios, perturbation-based model explanation and text adversarial attack. Experiments on three widely used text classification benchmarks validate the effectiveness of the proposed method compared to state-of-the-art baselines. Additional studies show that our method is highly transferable and consistent with human intuition. The code of this paper is available at https://github.com/niuzaisheng/AttExplainer .",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2174434599",
                        "name": "Runliang Niu"
                    },
                    {
                        "authorId": "122835755",
                        "name": "Zhepei Wei"
                    },
                    {
                        "authorId": "2152543613",
                        "name": "Yan Wang"
                    },
                    {
                        "authorId": "2151570641",
                        "name": "Qi Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "It is also quite challenging to interpret vision transformers\u2019 decisions [66], e."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d9aaa84dd512f98d56dbc498b9757932e08af8b5",
                "externalIds": {
                    "PubMedCentral": "9312955",
                    "DOI": "10.3390/biomedicines10071551",
                    "CorpusId": 250203545,
                    "PubMed": "35884859"
                },
                "corpusId": 250203545,
                "publicationVenue": {
                    "id": "abc0c598-b988-4841-a8c5-65bfa6f3df77",
                    "name": "Biomedicines",
                    "type": "journal",
                    "issn": "2227-9059",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-327040",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-327040",
                        "https://www.mdpi.com/journal/biomedicines"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d9aaa84dd512f98d56dbc498b9757932e08af8b5",
                "title": "Multi-Channel Vision Transformer for Epileptic Seizure Prediction",
                "abstract": "Epilepsy is a neurological disorder that causes recurrent seizures and sometimes loss of awareness. Around 30% of epileptic patients continue to have seizures despite taking anti-seizure medication. The ability to predict the future occurrence of seizures would enable the patients to take precautions against probable injuries and administer timely treatment to abort or control impending seizures. In this study, we introduce a Transformer-based approach called Multi-channel Vision Transformer (MViT) for automated and simultaneous learning of the spatio-temporal-spectral features in multi-channel EEG data. Continuous wavelet transform, a simple yet efficient pre-processing approach, is first used for turning the time-series EEG signals into image-like time-frequency representations named Scalograms. Each scalogram is split into a sequence of fixed-size non-overlapping patches, which are then fed as inputs to the MViT for EEG classification. Extensive experiments on three benchmark EEG datasets demonstrate the superiority of the proposed MViT algorithm over the state-of-the-art seizure prediction methods, achieving an average prediction sensitivity of 99.80% for surface EEG and 90.28\u201391.15% for invasive EEG data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3035995",
                        "name": "R. Hussein"
                    },
                    {
                        "authorId": "2108256964",
                        "name": "Soojin Lee"
                    },
                    {
                        "authorId": "1477327788",
                        "name": "R. Ward"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "To investigate the performance of our model, we used the class activation map (CAM) [55] to visualize the attention maps generated by our ACSI-Net."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5615526866d6f875c2346e1b933f57fa0782d8d1",
                "externalIds": {
                    "PubMedCentral": "9324190",
                    "DBLP": "journals/entropy/LiZZ22",
                    "DOI": "10.3390/e24070882",
                    "CorpusId": 250126272,
                    "PubMed": "35885106"
                },
                "corpusId": 250126272,
                "publicationVenue": {
                    "id": "8270cfe1-3713-4325-a7bd-c6a87eed889e",
                    "name": "Entropy",
                    "type": "journal",
                    "issn": "1099-4300",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-155606",
                    "alternate_urls": [
                        "http://www.mdpi.com/journal/entropy/",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-155606",
                        "https://www.mdpi.com/journal/entropy"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5615526866d6f875c2346e1b933f57fa0782d8d1",
                "title": "Facial Expression Recognition: One Attention-Modulated Contextual Spatial Information Network",
                "abstract": "Facial expression recognition (FER) in the wild is a challenging task due to some uncontrolled factors such as occlusion, illumination, and pose variation. The current methods perform well in controlled conditions. However, there are still two issues with the in-the-wild FER task: (i) insufficient descriptions of long-range dependency of expression features in the facial information space and (ii) not finely refining subtle inter-classes distinction from multiple expressions in the wild. To overcome the above issues, an end-to-end model for FER, named attention-modulated contextual spatial information network (ACSI-Net), is presented in this paper, with the manner of embedding coordinate attention (CA) modules into a contextual convolutional residual network (CoResNet). Firstly, CoResNet is constituted by arranging contextual convolution (CoConv) blocks of different levels to integrate facial expression features with long-range dependency, which generates a holistic representation of spatial information on facial expression. Then, the CA modules are inserted into different stages of CoResNet, at each of which the subtle information about facial expression acquired from CoConv blocks is first modulated by the corresponding CA module across channels and spatial locations and then flows into the next layer. Finally, to highlight facial regions related to expression, a CA module located at the end of the whole network, which produces attentional masks to multiply by input feature maps, is utilized to focus on salient regions. Different from other models, the ACSI-Net is capable of exploring intrinsic dependencies between features and yielding a discriminative representation for facial expression classification. Extensive experimental results on AffectNet and RAF_DB datasets demonstrate its effectiveness and competitiveness compared to other FER methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2157332957",
                        "name": "Xue Li"
                    },
                    {
                        "authorId": "2118159972",
                        "name": "Chunhua Zhu"
                    },
                    {
                        "authorId": "2114904481",
                        "name": "F. Zhou"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "With the introduction of the deep neural network model with attention, its attention weights also play an important role in the XAI field[23,50,6]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "68ca0c2bee8445deabcfd3e31d55ba0c846f75d4",
                "externalIds": {
                    "ArXiv": "2206.11126",
                    "DBLP": "journals/corr/abs-2206-11126",
                    "DOI": "10.48550/arXiv.2206.11126",
                    "CorpusId": 249926451
                },
                "corpusId": 249926451,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/68ca0c2bee8445deabcfd3e31d55ba0c846f75d4",
                "title": "Explanation-based Counterfactual Retraining(XCR): A Calibration Method for Black-box Models",
                "abstract": "With the rapid development of eXplainable Artificial Intelligence (XAI), a long line of past work has shown concerns about the Out-of-Distribution (OOD) problem in perturbation-based post-hoc XAI models and explanations are socially misaligned. We explore the limitations of post-hoc explanation methods that use approximators to mimic the behavior of black-box models. Then we propose eXplanation-based Counterfactual Retraining (XCR), which extracts feature importance fastly. XCR applies the explanations generated by the XAI model as counterfactual input to retrain the black-box model to address OOD and social misalignment problems. Evaluation of popular image datasets shows that XCR can improve model performance when only retaining 12.5% of the most crucial features without changing the black-box model structure. Furthermore, the evaluation of the benchmark of corruption datasets shows that the XCR is very helpful for improving model robustness and positively impacts the calibration of OOD problems. Even though not calibrated in the validation set like some OOD calibration methods, the corrupted data metric outperforms existing methods. Our method also beats current OOD calibration methods on the OOD calibration metric if calibration on the validation set is applied.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2171440524",
                        "name": "Liu Zhendong"
                    },
                    {
                        "authorId": "2155573126",
                        "name": "Wenyu Jiang"
                    },
                    {
                        "authorId": "39509574",
                        "name": "Yan Zhang"
                    },
                    {
                        "authorId": "2155371393",
                        "name": "Chongjun Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "617769c0ebeef10208ec92e2aec4a363d7f82799",
                "externalIds": {
                    "ArXiv": "2206.09753",
                    "CorpusId": 258298653
                },
                "corpusId": 258298653,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/617769c0ebeef10208ec92e2aec4a363d7f82799",
                "title": "Visualizing and Understanding Contrastive Learning",
                "abstract": "Contrastive learning has revolutionized the field of computer vision, learning rich representations from unlabeled data, which generalize well to diverse vision tasks. Consequently, it has become increasingly important to explain these approaches and understand their inner workings mechanisms. Given that contrastive models are trained with interdependent and interacting inputs and aim to learn invariance through data augmentation, the existing methods for explaining single-image systems (e.g., image classification models) are inadequate as they fail to account for these factors. Additionally, there is a lack of evaluation metrics designed to assess pairs of explanations, and no analytical studies have been conducted to investigate the effectiveness of different techniques used to explaining contrastive learning. In this work, we design visual explanation methods that contribute towards understanding similarity learning tasks from pairs of images. We further adapt existing metrics, used to evaluate visual explanations of image classification systems, to suit pairs of explanations and evaluate our proposed methods with these metrics. Finally, we present a thorough analysis of visual explainability methods for contrastive learning, establish their correlation with downstream tasks and demonstrate the potential of our approaches to investigate their merits and drawbacks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "32095408",
                        "name": "Fawaz Sammani"
                    },
                    {
                        "authorId": "1568871637",
                        "name": "B. Joukovsky"
                    },
                    {
                        "authorId": "2003112059",
                        "name": "Nikos Deligiannis"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The third term considers the explainability map of CLIP, given the input image and the caption [13] as a guide for the foreground mask."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "85ad34478f8c25cc5f666ec2f74ef7bb707816eb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-09358",
                    "ArXiv": "2206.09358",
                    "DOI": "10.48550/arXiv.2206.09358",
                    "CorpusId": 249890126
                },
                "corpusId": 249890126,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/85ad34478f8c25cc5f666ec2f74ef7bb707816eb",
                "title": "What is Where by Looking: Weakly-Supervised Open-World Phrase-Grounding without Text Inputs",
                "abstract": "Given an input image, and nothing else, our method returns the bounding boxes of objects in the image and phrases that describe the objects. This is achieved within an open world paradigm, in which the objects in the input image may not have been encountered during the training of the localization mechanism. Moreover, training takes place in a weakly supervised setting, where no bounding boxes are provided. To achieve this, our method combines two pre-trained networks: the CLIP image-to-text matching score and the BLIP image captioning tool. Training takes place on COCO images and their captions and is based on CLIP. Then, during inference, BLIP is used to generate a hypothesis regarding various regions of the current image. Our work generalizes weakly supervised segmentation and phrase grounding and is shown empirically to outperform the state of the art in both domains. It also shows very convincing results in the novel task of weakly-supervised open-world purely visual phrase-grounding presented in our work. For example, on the datasets used for benchmarking phrase-grounding, our method results in a very modest degradation in comparison to methods that employ human captions as an additional input. Our code is available at https://github.com/talshaharabany/what-is-where-by-looking and a live demo can be found at https://replicate.com/talshaharabany/what-is-where-by-looking.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1438948620",
                        "name": "Tal Shaharabany"
                    },
                    {
                        "authorId": "2142470821",
                        "name": "Yoad Tewel"
                    },
                    {
                        "authorId": "145128145",
                        "name": "Lior Wolf"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "We present visualizations of target class activation maps using the recent Transformer Explainability [2]\nfor several images in Figure 5 to showcase the behavior of SPViT.",
                "Figure 5: Visualization using Transformer Explainability [2].",
                "We present visualizations of target class activation maps using the recent Transformer Explainability [2] for several images in Figure 5 to showcase the behavior of SPViT."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "0fda2f1ada85dda45cbb3aa926620bbbd9c476b9",
                "externalIds": {
                    "DBLP": "conf/bmvc/ZhouXLWW0K022",
                    "ArXiv": "2206.07662",
                    "DOI": "10.48550/arXiv.2206.07662",
                    "CorpusId": 249674814
                },
                "corpusId": 249674814,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/0fda2f1ada85dda45cbb3aa926620bbbd9c476b9",
                "title": "SP-ViT: Learning 2D Spatial Priors for Vision Transformers",
                "abstract": "Recently, transformers have shown great potential in image classification and established state-of-the-art results on the ImageNet benchmark. However, compared to CNNs, transformers converge slowly and are prone to overfitting in low-data regimes due to the lack of spatial inductive biases. Such spatial inductive biases can be especially beneficial since the 2D structure of an input image is not well preserved in transformers. In this work, we present Spatial Prior-enhanced Self-Attention (SP-SA), a novel variant of vanilla Self-Attention (SA) tailored for vision transformers. Spatial Priors (SPs) are our proposed family of inductive biases that highlight certain groups of spatial relations. Unlike convolutional inductive biases, which are forced to focus exclusively on hard-coded local regions, our proposed SPs are learned by the model itself and take a variety of spatial relations into account. Specifically, the attention score is calculated with emphasis on certain kinds of spatial relations at each head, and such learned spatial foci can be complementary to each other. Based on SP-SA we propose the SP-ViT family, which consistently outperforms other ViT models with similar GFlops or parameters. Our largest model SP-ViT-L achieves a record-breaking 86.3% Top-1 accuracy with a reduction in the number of parameters by almost 50% compared to previous state-of-the-art model (150M for SP-ViT-L vs 271M for CaiT-M-36) among all ImageNet-1K models trained on 224x224 and fine-tuned on 384x384 resolution w/o extra data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2110219110",
                        "name": "Yuxuan Zhou"
                    },
                    {
                        "authorId": "41022741",
                        "name": "Wangmeng Xiang"
                    },
                    {
                        "authorId": "46651287",
                        "name": "C. Li"
                    },
                    {
                        "authorId": "2136790887",
                        "name": "Biao Wang"
                    },
                    {
                        "authorId": "152512722",
                        "name": "Xihan Wei"
                    },
                    {
                        "authorId": "2152836694",
                        "name": "Lei Zhang"
                    },
                    {
                        "authorId": "3316866",
                        "name": "M. Keuper"
                    },
                    {
                        "authorId": "2075382133",
                        "name": "Xia Hua"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "38ea64f4b5fa71942cd358c92ef14995bb51d72e",
                "externalIds": {
                    "ArXiv": "2206.05488",
                    "DBLP": "journals/corr/abs-2206-05488",
                    "DOI": "10.48550/arXiv.2206.05488",
                    "CorpusId": 249626215
                },
                "corpusId": 249626215,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/38ea64f4b5fa71942cd358c92ef14995bb51d72e",
                "title": "Kaggle Kinship Recognition Challenge: Introduction of Convolution-Free Model to boost conventional",
                "abstract": "Convolutional neural networks (CNNs) have achieved remarkable success in computer vision, making them a dominant classifier for ensemble learning in numerous Kaggle competitions. Nevertheless, this work aims to explore a convolution-free base classifier that can be used to widen the variations of the conventional ensemble classifier. Specifically, we propose Vision Transformers as base classifiers to combine with CNNs for a unique ensemble solution in Kaggle\u2019s kinship recognition. The idea behind the proposed solution is based on the belief that: If we achieve a lower correlation between base classifiers by using a particular method, such as by introducing a different model which calculates the prediction using different methods, the final ensemble classifier can achieve a greater boost. In this paper, we verify our proposed idea by implementing and optimizing variants of the Vision Transformer model on top of the existing CNN models. The combined models achieve better scores than conventional ensemble classifiers based solely on CNN variants. We demonstrate that highly optimized CNN ensembles publicly available on the Kaggle Discussion board can easily achieve a significant boost in ROC score by simply introducing variants of the Vision Transformer model to the ensemble because of its low correlation magnitude between the CNNs in this experiment. In addition, we also conclude that the convolution-free Vision Transformer may even be used as an additional booster classifier to an already highly optimal ensemble of only CNN models to further boost the competitor\u2019s ranking.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2170076482",
                        "name": "Mingchuan Tian"
                    },
                    {
                        "authorId": "121214539",
                        "name": "Guang-shou Teng"
                    },
                    {
                        "authorId": "1500377102",
                        "name": "Yipeng Bao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "These methods achieve mixed results in quantitative benchmarks, whether for object localization or the removal of influential features [42, 25, 49, 10, 31], and they are somewhat insensitive to the randomization of model parameters [2].",
                "Among the baselines, RISE and LRP remain most competitive, but ViT Shapley performs best for both datasets.",
                "Among those shown here, LRP is most similar to ViT Shapley, but they disagree in several cases.",
                "Attention last This approach calculates the attention directed from each image token into the class token in the final self-attention layer, summed across attention heads [1, 10].",
                "Other methods modify the gradient backpropagation algorithm to generate attribution scores that satisfy certain properties [4, 53, 3], including the layer-wise relevance propagation (LRP) approach that was recently extended to transformers [10].",
                "Recent work has disputed the role of attention as an indicator of feature importance [51, 29, 63, 10], and we find in our experiments that attention is a poor proxy for the effect of removing features from a model.",
                "For attention-based methods, we use attention rollout [1] and the last layer\u2019s attention directed to the class token [1, 10].",
                "Layer-wise relevance propagation (LRP) Originally described as a set of constraints for a modified backpropagation routine [4], LRP has since been implemented for a variety of network layers and architectures, and it was recently adapted to ViTs [10].",
                "We show results for attention last, attention rollout, Vanilla Gradients, Integrated Gradients, SmoothGrad, LRP, leave-one-out, and ViT Shapley only; we excluded VarGrad, GradCAM and RISE because their results were less visually appealing.",
                "We used an implementation provided by prior work [10].",
                "Our evaluation was conducted on a GeForce RTX 2080 Ti GPU, with minibatches of 16 samples for attention last, attention rollout and ViT Shapley; batch size of 1 for Vanilla Gradients, GradCAM, LRP, leave-one-out and RISE; and internal minibatching for SmoothGrad, IntGrad and VarGrad (implemented via Captum [35]).",
                "Similar to prior work [10], we did not use attention flow [1] due to the computational cost.",
                "(\u2191)\nAttention last - - - Attention rollout - - -\nGradCAM 0.021 (0.002) 0.005 (0.000) -0.672 (0.015) IntGrad 0.008 (0.001) 0.004 (0.000) 0.294 (0.022) Vanilla 0.006 (0.001) 0.020 (0.001) -0.682 (0.015) SmoothGrad 0.006 (0.001) 0.006 (0.001) -0.683 (0.015) VarGrad 0.006 (0.001) 0.006 (0.001) -0.680 (0.015) LRP 0.004 (0.001) 0.022 (0.001) -0.680 (0.015)\nLeave-one-out 0.013 (0.002) 0.003 (0.000) -0.017 (0.028) RISE 0.023 (0.003) 0.002 (0.000) -0.681 (0.015) ViT Shapley 0.093 (0.004) 0.001 (0.000) 0.672 (0.014)\nRandom 0.005 (0.001) 0.005 (0.001) -",
                "RISE and LRP tend to be the most competitive baselines, and perhaps surprisingly, certain other methods fail to outperform a random baseline (GradCAM, SmoothGrad, VarGrad).",
                "Next, for gradient-based methods, we use Vanilla Gradients [54], Integrated Gradients [57], SmoothGrad [55], VarGrad [25], LRP [10] and GradCAM [50].",
                "We used existing LRP and GradCAM implementations for ViTs [10, 23], and the remaining gradient-based methods were run using the Captum package [35]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "a71b9fc58d8df11d4ae725568308fb5ace3da24a",
                "externalIds": {
                    "DBLP": "conf/iclr/Covert0L23",
                    "ArXiv": "2206.05282",
                    "DOI": "10.48550/arXiv.2206.05282",
                    "CorpusId": 249626076
                },
                "corpusId": 249626076,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a71b9fc58d8df11d4ae725568308fb5ace3da24a",
                "title": "Learning to Estimate Shapley Values with Vision Transformers",
                "abstract": "Transformers have become a default architecture in computer vision, but understanding what drives their predictions remains a challenging problem. Current explanation approaches rely on attention values or input gradients, but these provide a limited view of a model's dependencies. Shapley values offer a theoretically sound alternative, but their computational cost makes them impractical for large, high-dimensional models. In this work, we aim to make Shapley values practical for vision transformers (ViTs). To do so, we first leverage an attention masking approach to evaluate ViTs with partial information, and we then develop a procedure to generate Shapley value explanations via a separate, learned explainer model. Our experiments compare Shapley values to many baseline methods (e.g., attention rollout, GradCAM, LRP), and we find that our approach provides more accurate explanations than existing methods for ViTs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "115697510",
                        "name": "Ian Covert"
                    },
                    {
                        "authorId": "2125803151",
                        "name": "Chanwoo Kim"
                    },
                    {
                        "authorId": "2180463",
                        "name": "Su-In Lee"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Following the evaluation scheme in (Samek et al., 2016; Feng et al., 2018; Chefer et al., 2020), given an input x and an attribution map, we rank the map elements by ascending importance."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a547d08097eac6f272f41f33ac97999339a98206",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-02761",
                    "ArXiv": "2206.02761",
                    "DOI": "10.48550/arXiv.2206.02761",
                    "CorpusId": 249395292
                },
                "corpusId": 249395292,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a547d08097eac6f272f41f33ac97999339a98206",
                "title": "Dual Decomposition of Convex Optimization Layers for Consistent Attention in Medical Images",
                "abstract": "A key concern in integrating machine learning models in medicine is the ability to interpret their reasoning. Popular explainability methods have demonstrated satisfactory results in natural image recognition, yet in medical image analysis, many of these approaches provide partial and noisy explanations. Recently, attention mechanisms have shown compelling results both in their predictive performance and in their interpretable qualities. A fundamental trait of attention is that it leverages salient parts of the input which contribute to the model's prediction. To this end, our work focuses on the explanatory value of attention weight distributions. We propose a multi-layer attention mechanism that enforces consistent interpretations between attended convolutional layers using convex optimization. We apply duality to decompose the consistency constraints between the layers by reparameterizing their attention probability distributions. We further suggest learning the dual witness by optimizing with respect to our objective; thus, our implementation uses standard back-propagation, hence it is highly efficient. While preserving predictive performance, our proposed method leverages weakly annotated medical imaging data and provides complete and faithful explanations to the model's prediction.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "32171897",
                        "name": "Tom Ron"
                    },
                    {
                        "authorId": "1413859242",
                        "name": "M. Weiler-Sagie"
                    },
                    {
                        "authorId": "1918412",
                        "name": "Tamir Hazan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1cb5a1fce0b65b616e69cc5ffd4e43e03d259e97",
                "externalIds": {
                    "DBLP": "journals/mia/LiCTWLZ23",
                    "ArXiv": "2206.01136",
                    "DOI": "10.48550/arXiv.2206.01136",
                    "CorpusId": 249282648,
                    "PubMed": "36738650"
                },
                "corpusId": 249282648,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1cb5a1fce0b65b616e69cc5ffd4e43e03d259e97",
                "title": "Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives",
                "abstract": "Transformer, one of the latest technological advances of deep learning, has gained prevalence in natural language processing or computer vision. Since medical imaging bear some resemblance to computer vision, it is natural to inquire about the status quo of Transformers in medical imaging and ask the question: can the Transformer models transform medical imaging? In this paper, we attempt to make a response to the inquiry. After a brief introduction of the fundamentals of Transformers, especially in comparison with convolutional neural networks (CNNs), and highlighting key defining properties that characterize the Transformers, we offer a comprehensive review of the state-of-the-art Transformer-based approaches for medical imaging and exhibit current research progresses made in the areas of medical image segmentation, recognition, detection, registration, reconstruction, enhancement, etc. In particular, what distinguishes our review lies in its organization based on the Transformer's key defining properties, which are mostly derived from comparing the Transformer and CNN, and its type of architecture, which specifies the manner in which the Transformer and CNN are combined, all helping the readers to best understand the rationale behind the reviewed approaches. We conclude with discussions of future perspectives.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152753361",
                        "name": "Jun Li"
                    },
                    {
                        "authorId": "47740582",
                        "name": "Junyu Chen"
                    },
                    {
                        "authorId": "46556781",
                        "name": "Yucheng Tang"
                    },
                    {
                        "authorId": "2133475009",
                        "name": "Bennett A. Landman"
                    },
                    {
                        "authorId": "2107323185",
                        "name": "S. K. Zhou"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Metrics and dataset are taken from [9].",
                "the combination of gradients and attention values has been shown to produce a viable interpretation of the model\u2019s prediction [8, 9].",
                "Second, we conduct segmentation tests following [9] to assess the effect of our method on the level of agreement between the relevancy maps and the foreground segmentation maps.",
                "Segmentation tests Since our motivation is to encourage the relevance to focus less on the background and more on as much of the foreground as possible, we test the resemblance of the resulting relevance maps to the segmentation maps following [9]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "257f9f3dbe2bac1ae242728827f8a861bd8469fd",
                "externalIds": {
                    "DBLP": "conf/nips/CheferSW22",
                    "ArXiv": "2206.01161",
                    "DOI": "10.48550/arXiv.2206.01161",
                    "CorpusId": 249282278
                },
                "corpusId": 249282278,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/257f9f3dbe2bac1ae242728827f8a861bd8469fd",
                "title": "Optimizing Relevance Maps of Vision Transformers Improves Robustness",
                "abstract": "It has been observed that visual classification models often rely mostly on the image background, neglecting the foreground, which hurts their robustness to distribution changes. To alleviate this shortcoming, we propose to monitor the model's relevancy signal and manipulate it such that the model is focused on the foreground object. This is done as a finetuning step, involving relatively few samples consisting of pairs of images and their associated foreground masks. Specifically, we encourage the model's relevancy map (i) to assign lower relevance to background regions, (ii) to consider as much information as possible from the foreground, and (iii) we encourage the decisions to have high confidence. When applied to Vision Transformer (ViT) models, a marked improvement in robustness to domain shifts is observed. Moreover, the foreground masks can be obtained automatically, from a self-supervised variant of the ViT model itself; therefore no additional supervision is required.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2038268012",
                        "name": "Hila Chefer"
                    },
                    {
                        "authorId": "38211837",
                        "name": "Idan Schwartz"
                    },
                    {
                        "authorId": "145128145",
                        "name": "Lior Wolf"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d6cd05d16157351b7084aa821adf1054582f1262",
                "externalIds": {
                    "DBLP": "journals/isci/HeLXYCAA22",
                    "DOI": "10.1016/j.ins.2022.06.091",
                    "CorpusId": 250203107
                },
                "corpusId": 250203107,
                "publicationVenue": {
                    "id": "e46002a1-d7a6-4681-aae9-36bc3a6a1f93",
                    "name": "Information Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Information Scientist",
                        "Inf Sci"
                    ],
                    "issn": "0020-0255",
                    "alternate_issns": [
                        "0020-0263"
                    ],
                    "url": "http://www.sciencedirect.com/science/journal/00200255"
                },
                "url": "https://www.semanticscholar.org/paper/d6cd05d16157351b7084aa821adf1054582f1262",
                "title": "Deconv-transformer (DecT): A histopathological image classification model for breast cancer based on color deconvolution and transformer architecture",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2149074389",
                        "name": "Zhu He"
                    },
                    {
                        "authorId": "1388825335",
                        "name": "Mingwei Lin"
                    },
                    {
                        "authorId": "1741576",
                        "name": "Zeshui Xu"
                    },
                    {
                        "authorId": "2113321644",
                        "name": "Zhiqiang Yao"
                    },
                    {
                        "authorId": "2155543044",
                        "name": "Hong Chen"
                    },
                    {
                        "authorId": "9319144",
                        "name": "A. Alhudhaif"
                    },
                    {
                        "authorId": "51477868",
                        "name": "Fayadh S. Alenezi"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "We visualize the activated area of our M3T network based on transformer interpretability technique [7].",
                "Third, we visualize the activated area in 3D MRI images the transformer interpretability methods [7]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "91ad42be584dc86c0576157c32502e7ec5288c86",
                "externalIds": {
                    "DBLP": "conf/cvpr/JangH22",
                    "DOI": "10.1109/CVPR52688.2022.02006",
                    "CorpusId": 250551564
                },
                "corpusId": 250551564,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/91ad42be584dc86c0576157c32502e7ec5288c86",
                "title": "M3T: three-dimensional Medical image classifier using Multi-plane and Multi-slice Transformer",
                "abstract": "In this study, we propose a three-dimensional Medical image classifier using Multi-plane and Multi-slice Trans-former (M3T) network to classify Alzheimer's disease (AD) in 3D MRI images. The proposed network synergically com-bines 3D CNN, 2D CNN, and Transformer for accurate AD classification. The 3D CNN is used to perform natively 3D representation learning, while 2D CNN is used to utilize the pre-trained weights on large 2D databases and 2D repre-sentation learning. It is possible to efficiently extract the lo-cality information for AD-related abnormalities in the local brain using CNN networks with inductive bias. The trans-former network is also used to obtain attention relationships among multi-plane (axial, coronal, and sagittal) and multi-slice images after CNN. It is also possible to learn the ab-normalities distributed over the wider region in the brain using the transformer without inductive bias. In this ex-periment, we used a training dataset from the Alzheimer's Disease Neuroimaging Initiative (ADNI) which contains a total of 4,786 3D T1-weighted MRI images. For the validation data, we used dataset from three different institutions: The Australian Imaging, Biomarker and Lifestyle Flagship Study of Ageing (AIBL), The Open Access Series of Imaging Studies (OASIS), and some set of ADNI data indepen-dent from the training dataset. Our proposed M3T is compared to conventional 3D classification networks based on an area under the curve (AUC) and classification accuracy for AD classification. This study represents that the pro-posed network M3T achieved the highest performance in multi-institutional validation database, and demonstrates the feasibility of the method to efficiently combine CNN and Transformer for 3D medical images.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9331782",
                        "name": "Jinseong Jang"
                    },
                    {
                        "authorId": "2905840",
                        "name": "D. Hwang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a81e603028ba872d5b742f601d4a32a34d729eeb",
                "externalIds": {
                    "DBLP": "journals/cgf/HuesmannL22",
                    "DOI": "10.1111/cgf.14548",
                    "CorpusId": 251136383
                },
                "corpusId": 251136383,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a81e603028ba872d5b742f601d4a32a34d729eeb",
                "title": "SimilarityNet: A Deep Neural Network for Similarity Analysis Within Spatio\u2010temporal Ensembles",
                "abstract": "Latent feature spaces of deep neural networks are frequently used to effectively capture semantic characteristics of a given dataset. In the context of spatio\u2010temporal ensemble data, the latent space represents a similarity space without the need of an explicit definition of a field similarity measure. Commonly, these networks are trained for specific data within a targeted application. We instead propose a general training strategy in conjunction with a deep neural network architecture, which is readily applicable to any spatio\u2010temporal ensemble data without re\u2010training. The latent\u2010space visualization allows for a comprehensive visual analysis of patterns and temporal evolution within the ensemble. With the use of SimilarityNet, we are able to perform similarity analyses on large\u2010scale spatio\u2010temporal ensembles in less than a second on commodity consumer hardware. We qualitatively compare our results to visualizations with established field similarity measures to document the interpretability of our latent space visualizations and show that they are feasible for an in\u2010depth basic understanding of the underlying temporal evolution of a given ensemble.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "82384337",
                        "name": "Karim Huesmann"
                    },
                    {
                        "authorId": "1802442",
                        "name": "L. Linsen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[9] study relevancy for Transformer networks in computer vision by assigning local relevance based on the Deep Taylor Decomposition principle [27]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4f161f3cf6a272061600c71cc2e8a325753a38f0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-15389",
                    "ArXiv": "2205.15389",
                    "DOI": "10.48550/arXiv.2205.15389",
                    "CorpusId": 249210171
                },
                "corpusId": 249210171,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4f161f3cf6a272061600c71cc2e8a325753a38f0",
                "title": "Attention Flows for General Transformers",
                "abstract": "In this paper, we study the computation of how much an input token in a Transformer model influences its prediction. We formalize a method to construct a flow network out of the attention values of encoder-only Transformer models and extend it to general Transformer architectures including an auto-regressive decoder. We show that running a maxflow algorithm on the flow network construction yields Shapley values, which determine the impact of a player in cooperative game theory. By interpreting the input tokens in the flow network as players, we can compute their influence on the total attention flow leading to the decoder's decision. Additionally, we provide a library that computes and visualizes the attention flow of arbitrary Transformer models. We show the usefulness of our implementation on various models trained on natural language processing and reasoning tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1412480273",
                        "name": "Niklas Metzger"
                    },
                    {
                        "authorId": "112959493",
                        "name": "Christopher Hahn"
                    },
                    {
                        "authorId": "2106411503",
                        "name": "Julian Siber"
                    },
                    {
                        "authorId": "50115259",
                        "name": "Frederik Schmitt"
                    },
                    {
                        "authorId": "1719407",
                        "name": "B. Finkbeiner"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0953ada119f384f328b6102e6b7963b3bde7cc9e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-15290",
                    "ArXiv": "2205.15290",
                    "DOI": "10.48550/arXiv.2205.15290",
                    "CorpusId": 249192263
                },
                "corpusId": 249192263,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0953ada119f384f328b6102e6b7963b3bde7cc9e",
                "title": "Zero-Shot and Few-Shot Learning for Lung Cancer Multi-Label Classification using Vision Transformer",
                "abstract": "Lung cancer is the leading cause of cancer-related death worldwide. Lung adenocarcinoma (LUAD) and lung squamous cell carcinoma (LUSC) are the most common histologic subtypes of non-small-cell lung cancer (NSCLC). Histology is an essential tool for lung cancer diagnosis. Pathologists make classifications according to the dominant subtypes. Although morphology remains the standard for diagnosis, significant tool needs to be developed to elucidate the diagnosis. In our study, we utilize the pre-trained Vision Transformer (ViT) model to classify multiple label lung cancer on histologic slices (from dataset LC25000), in both Zero-Shot and Few-Shot settings. Then we compare the performance of Zero-Shot and Few-Shot ViT on accuracy, precision, recall, sensitivity and specificity. Our study show that the pre-trained ViT model has a good performance in Zero-Shot setting, a competitive accuracy ($99.87\\%$) in Few-Shot setting ({epoch = 1}) and an optimal result ($100.00\\%$ on both validation set and test set) in Few-Shot seeting ({epoch = 5}).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2439700",
                        "name": "F. Guo"
                    },
                    {
                        "authorId": "65858592",
                        "name": "Yingfang Fan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "[24] visualize attention layer in an image more clearly.",
                "For ViT, attention visualization and interpretability [24] are important factors in its success, but for MLP-Mixer and similar methods, it\u2019s unclear what are learned from training."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b6d01a6942b30cfded51871653e431d8437deb1f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-14477",
                    "ArXiv": "2205.14477",
                    "DOI": "10.48550/arXiv.2205.14477",
                    "CorpusId": 249191762
                },
                "corpusId": 249191762,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b6d01a6942b30cfded51871653e431d8437deb1f",
                "title": "MDMLP: Image Classification from Scratch on Small Datasets with MLP",
                "abstract": "The attention mechanism has become a go-to technique for natural language processing and computer vision tasks. Recently, the MLP-Mixer and other MLP-based architectures, based simply on multi-layer perceptrons (MLPs), are also powerful compared to CNNs and attention techniques and raises a new research direction. However, the high capability of the MLP-based networks severely relies on large volume of training data, and lacks of explanation ability compared to the Vision Transformer (ViT) or ConvNets. When trained on small datasets, they usually achieved inferior results than ConvNets. To resolve it, we present (i) multi-dimensional MLP (MDMLP), a conceptually simple and lightweight MLP-based architecture yet achieves SOTA when training from scratch on small-size datasets; (ii) multi-dimension MLP Attention Tool (MDAttnTool), a novel and efficient attention mechanism based on MLPs. Even without strong data augmentation, MDMLP achieves 90.90% accuracy on CIFAR10 with only 0.3M parameters, while the well-known MLP-Mixer achieves 85.45% with 17.1M parameters. In addition, the lightweight MDAttnTool highlights objects in images, indicating its explanation power. Our code is available at https://github.com/Amoza-Theodore/MDMLP.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2041744341",
                        "name": "Tianxu Lv"
                    },
                    {
                        "authorId": "9148956",
                        "name": "Chongyang Bai"
                    },
                    {
                        "authorId": "2136111348",
                        "name": "Chaojie Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fd06e7aaf9131c9a625d374841844adce6e0ed09",
                "externalIds": {
                    "ArXiv": "2205.11631",
                    "ACL": "2022.emnlp-main.599",
                    "DBLP": "journals/corr/abs-2205-11631",
                    "DOI": "10.48550/arXiv.2205.11631",
                    "CorpusId": 249017500
                },
                "corpusId": 249017500,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/fd06e7aaf9131c9a625d374841844adce6e0ed09",
                "title": "Towards Opening the Black Box of Neural Machine Translation: Source and Target Interpretations of the Transformer",
                "abstract": "In Neural Machine Translation (NMT), each token prediction is conditioned on the source sentence and the target prefix (what has been previously translated at a decoding step). However, previous work on interpretability in NMT has mainly focused solely on source sentence tokens\u2019 attributions. Therefore, we lack a full understanding of the influences of every input token (source sentence and target prefix) in the model predictions. In this work, we propose an interpretability method that tracks input tokens\u2019 attributions for both contexts. Our method, which can be extended to any encoder-decoder Transformer-based model, allows us to better comprehend the inner workings of current NMT models. We apply the proposed method to both bilingual and multilingual Transformers and present insights into their behaviour.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1751450782",
                        "name": "Javier Ferrando"
                    },
                    {
                        "authorId": "2003752849",
                        "name": "Gerard I. G\u00e1llego"
                    },
                    {
                        "authorId": "2117714386",
                        "name": "Belen Alastruey"
                    },
                    {
                        "authorId": "144483761",
                        "name": "Carlos Escolano"
                    },
                    {
                        "authorId": "1398996347",
                        "name": "M. Costa-juss\u00e0"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Our technique was inspired by the recent work by Chefer and colleagues [39], who used",
                "Following the propagation procedure of relevance and gradients by Chefer and colleagues [39], GraphCAM computes the gradient \u2207 A(l) and layer relevance R(nl ) with respect to a target class for each attention map A(l), where nl is the layer that corresponds to the softmax operation in Eq."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e2c16ffbb1a590d8cc04f7bfbe080b611484a75e",
                "externalIds": {
                    "PubMedCentral": "9670036",
                    "DBLP": "journals/tmi/ZhengGGBBBK22",
                    "ArXiv": "2205.09671",
                    "DOI": "10.1109/TMI.2022.3176598",
                    "CorpusId": 248887420,
                    "PubMed": "35594209"
                },
                "corpusId": 248887420,
                "publicationVenue": {
                    "id": "e0cda45d-3074-4ac0-80b8-e5250df00b89",
                    "name": "IEEE Transactions on Medical Imaging",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Med Imaging"
                    ],
                    "issn": "0278-0062",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=42",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=42"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e2c16ffbb1a590d8cc04f7bfbe080b611484a75e",
                "title": "A Graph-Transformer for Whole Slide Image Classification",
                "abstract": "Deep learning is a powerful tool for whole slide image (WSI) analysis. Typically, when performing supervised deep learning, a WSI is divided into small patches, trained and the outcomes are aggregated to estimate disease grade. However, patch-based methods introduce label noise during training by assuming that each patch is independent with the same label as the WSI and neglect overall WSI-level information that is significant in disease grading. Here we present a Graph-Transformer (GT) that fuses a graph-based representation of an WSI and a vision transformer for processing pathology images, called GTP, to predict disease grade. We selected 4,818 WSIs from the Clinical Proteomic Tumor Analysis Consortium (CPTAC), the National Lung Screening Trial (NLST), and The Cancer Genome Atlas (TCGA), and used GTP to distinguish adenocarcinoma (LUAD) and squamous cell carcinoma (LSCC) from adjacent non-cancerous tissue (normal). First, using NLST data, we developed a contrastive learning framework to generate a feature extractor. This allowed us to compute feature vectors of individual WSI patches, which were used to represent the nodes of the graph followed by construction of the GTP framework. Our model trained on the CPTAC data achieved consistently high performance on three-label classification (normal versus LUAD versus LSCC: mean accuracy = 91.2 \u00b1 2.5%) based on five-fold cross-validation, and mean accuracy = 82.3 \u00b1 1.0% on external test data (TCGA). We also introduced a graph-based saliency mapping technique, called GraphCAM, that can identify regions that are highly associated with the class label. Our findings demonstrate GTP as an interpretable and effective deep learning framework for WSI-level classification.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9085030",
                        "name": "Yi Zheng"
                    },
                    {
                        "authorId": "32065369",
                        "name": "R. Gindra"
                    },
                    {
                        "authorId": "2165662979",
                        "name": "Emily Green"
                    },
                    {
                        "authorId": "15483353",
                        "name": "E. Burks"
                    },
                    {
                        "authorId": "1723703",
                        "name": "Margrit Betke"
                    },
                    {
                        "authorId": "2555118",
                        "name": "J. Beane"
                    },
                    {
                        "authorId": "1887282",
                        "name": "V. Kolachalama"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Prior works [6, 63, 1, 14] using transformer networks in natural language have re-purposed the attention weights in the later layers as an mechanism to introspect model logic."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "83f87f1cccf84255060bcee1e507735f7ff3a699",
                "externalIds": {
                    "DBLP": "conf/nips/WuG22",
                    "ArXiv": "2205.09735",
                    "DOI": "10.48550/arXiv.2205.09735",
                    "CorpusId": 248887787
                },
                "corpusId": 248887787,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/83f87f1cccf84255060bcee1e507735f7ff3a699",
                "title": "Foundation Posteriors for Approximate Probabilistic Inference",
                "abstract": "Probabilistic programs provide an expressive representation language for generative models. Given a probabilistic program, we are interested in the task of posterior inference: estimating a latent variable given a set of observed variables. Existing techniques for inference in probabilistic programs often require choosing many hyper-parameters, are computationally expensive, and/or only work for restricted classes of programs. Here we formulate inference as masked language modeling: given a program, we generate a supervised dataset of variables and assignments, and randomly mask a subset of the assignments. We then train a neural network to unmask the random values, defining an approximate posterior distribution. By optimizing a single neural network across a range of programs we amortize the cost of training, yielding a\"foundation\"posterior able to do zero-shot inference for new programs. The foundation posterior can also be fine-tuned for a particular program and dataset by optimizing a variational inference objective. We show the efficacy of the approach, zero-shot and fine-tuned, on a benchmark of STAN programs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1700653",
                        "name": "Mike Wu"
                    },
                    {
                        "authorId": "144002017",
                        "name": "Noah D. Goodman"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Another work uses a Deep Taylor Decomposition approach to visualize portions of input image leading to a particular ViT prediction (Chefer et al., 2021)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d163cca5cfea5d967873d34023554e3d1771716b",
                "externalIds": {
                    "DBLP": "conf/icml/SahinerEOPMP22",
                    "ArXiv": "2205.08078",
                    "DOI": "10.48550/arXiv.2205.08078",
                    "CorpusId": 248834078
                },
                "corpusId": 248834078,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d163cca5cfea5d967873d34023554e3d1771716b",
                "title": "Unraveling Attention via Convex Duality: Analysis and Interpretations of Vision Transformers",
                "abstract": "Vision transformers using self-attention or its proposed alternatives have demonstrated promising results in many image related tasks. However, the underpinning inductive bias of attention is not well understood. To address this issue, this paper analyzes attention through the lens of convex duality. For the non-linear dot-product self-attention, and alternative mechanisms such as MLP-mixer and Fourier Neural Operator (FNO), we derive equivalent finite-dimensional convex problems that are interpretable and solvable to global optimality. The convex programs lead to {\\it block nuclear-norm regularization} that promotes low rank in the latent feature and token dimensions. In particular, we show how self-attention networks implicitly clusters the tokens, based on their latent similarity. We conduct experiments for transferring a pre-trained transformer backbone for CIFAR-100 classification by fine-tuning a variety of convex attention heads. The results indicate the merits of the bias induced by attention compared with the existing MLP or linear heads.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2742407",
                        "name": "Arda Sahiner"
                    },
                    {
                        "authorId": "19278348",
                        "name": "Tolga Ergen"
                    },
                    {
                        "authorId": "2027017491",
                        "name": "Batu Mehmet Ozturkler"
                    },
                    {
                        "authorId": "2060416777",
                        "name": "J. Pauly"
                    },
                    {
                        "authorId": "33002157",
                        "name": "M. Mardani"
                    },
                    {
                        "authorId": "3173667",
                        "name": "Mert Pilanci"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Note that our shared attention differs from the coattention introduced in prior works [7], where the value and key are passed via a skip connection from the encoder layers."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4b9b3c538999410c58e229ca15437a693cbb03c5",
                "externalIds": {
                    "ArXiv": "2205.08303",
                    "DBLP": "conf/cvpr/BhattacharjeeZS22",
                    "DOI": "10.1109/CVPR52688.2022.01172",
                    "CorpusId": 248834301
                },
                "corpusId": 248834301,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4b9b3c538999410c58e229ca15437a693cbb03c5",
                "title": "MuIT: An End-to-End Multitask Learning Transformer",
                "abstract": "We propose an end-to-end Multitask Learning Transformer framework, named MulT, to simultaneously learn multiple high-level vision tasks, including depth estimation, semantic segmentation, reshading, surface normal estimation, 2D keypoint detection, and edge detection. Based on the Swin transformer model, our framework encodes the input image into a shared representation and makes predictions for each vision task using task-specific transformer-based decoder heads. At the heart of our approach is a shared attention mechanism modeling the dependencies across the tasks. We evaluate our model on several multitask benchmarks, showing that our MulT framework outperforms both the state-of-the art multitask convolutional neural network models and all the respective single task transformer models. Our experiments further highlight the benefits of sharing attention across all the tasks, and demonstrate that our MulT model is robust and generalizes well to new domains. Our project website is at https://ivrl.github.io/MulT/.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "39826361",
                        "name": "Deblina Bhattacharjee"
                    },
                    {
                        "authorId": "2146324900",
                        "name": "Tong Zhang"
                    },
                    {
                        "authorId": "1735035",
                        "name": "S. S\u00fcsstrunk"
                    },
                    {
                        "authorId": "2862871",
                        "name": "M. Salzmann"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7f80de0744e0e537c185fdbf4e7f4e1003b3383e",
                "externalIds": {
                    "ArXiv": "2206.02661",
                    "DBLP": "journals/corr/abs-2206-02661",
                    "DOI": "10.48550/arXiv.2206.02661",
                    "CorpusId": 249394530
                },
                "corpusId": 249394530,
                "publicationVenue": {
                    "id": "7dc964d5-49e6-4c37-b1c4-a7f0de1fa425",
                    "name": "International Conference on Web and Social Media",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Weblogs Soc Media",
                        "International Conference on Weblogs and Social Media",
                        "Int Conf Web Soc Media",
                        "ICWSM"
                    ],
                    "url": "http://www.aaai.org/Library/ICWSM/icwsm-library.php"
                },
                "url": "https://www.semanticscholar.org/paper/7f80de0744e0e537c185fdbf4e7f4e1003b3383e",
                "title": "Evaluating Deep Taylor Decomposition for Reliability Assessment in the Wild",
                "abstract": "We argue that we need to evaluate model interpretability methods 'in the wild', i.e., in situations where professionals make critical decisions, and models can potentially assist them. We present an in-the-wild evaluation of token attribution based on Deep Taylor Decomposition, with professional journalists performing reliability assessments. We find that using this method in conjunction with RoBERTa-Large, fine-tuned on the Gossip Corpus, led to faster and better human decision-making, as well as a more critical attitude toward news sources among the journalists. We present a comparison of human and model rationales, as well as a qualitative analysis of the journalists' experiences with machine-in-the-loop decision making.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "6547490",
                        "name": "Stephanie Brandl"
                    },
                    {
                        "authorId": "2064295987",
                        "name": "Daniel Hershcovich"
                    },
                    {
                        "authorId": "1700187",
                        "name": "Anders S\u00f8gaard"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a2b1cdb053b4529be028370be5e491a9eadb8192",
                "externalIds": {
                    "DBLP": "journals/cbm/QiuHZC022",
                    "DOI": "10.1016/j.compbiomed.2022.105628",
                    "CorpusId": 248898124,
                    "PubMed": "35609472"
                },
                "corpusId": 248898124,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a2b1cdb053b4529be028370be5e491a9eadb8192",
                "title": "FGAM: A pluggable light-weight attention module for medical image segmentation",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2150449901",
                        "name": "Zhongxi Qiu"
                    },
                    {
                        "authorId": "97121888",
                        "name": "Yan Hu"
                    },
                    {
                        "authorId": "2167509773",
                        "name": "Jiayi Zhang"
                    },
                    {
                        "authorId": "2157187683",
                        "name": "Xiaoshan Chen"
                    },
                    {
                        "authorId": "2155403819",
                        "name": "Jiang Liu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e10f7e643779ff0ff01448721e4ae9bdc908c7fb",
                "externalIds": {
                    "DBLP": "journals/cmpb/SuLXH22",
                    "DOI": "10.1016/j.cmpb.2022.106903",
                    "CorpusId": 249033953,
                    "PubMed": "35636358"
                },
                "corpusId": 249033953,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e10f7e643779ff0ff01448721e4ae9bdc908c7fb",
                "title": "YOLO-LOGO: A transformer-based YOLO segmentation model for breast mass detection and segmentation in digital mammograms",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2254201298",
                        "name": "Yongye Su"
                    },
                    {
                        "authorId": "2034188172",
                        "name": "Qian Liu"
                    },
                    {
                        "authorId": "2256590078",
                        "name": "Wentao Xie"
                    },
                    {
                        "authorId": "2250232936",
                        "name": "Pingzhao Hu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Regarding the DeiT, we refer the reader to [216], 1335 which proposed a framework to generate LRP attributions for 1336 Transformer-based architectures."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "41b6cc4acedea461646ea85426f4f750a753a33b",
                "externalIds": {
                    "DBLP": "journals/access/GoncalvesRTC22",
                    "ArXiv": "2204.12406",
                    "DOI": "10.1109/ACCESS.2022.3206449",
                    "CorpusId": 248391921
                },
                "corpusId": 248391921,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/41b6cc4acedea461646ea85426f4f750a753a33b",
                "title": "A Survey on Attention Mechanisms for Medical Applications: are we Moving Toward Better Algorithms?",
                "abstract": "The increasing popularity of attention mechanisms in deep learning algorithms for computer vision and natural language processing made these models attractive to other research domains. In healthcare, there is a strong need for tools that may improve the routines of the clinicians and the patients. Naturally, the use of attention-based algorithms for medical applications occurred smoothly. However, being healthcare a domain that depends on high-stake decisions, the scientific community must ponder if these high-performing algorithms fit the needs of medical applications. With this motto, this paper extensively reviews the use of attention mechanisms in machine learning methods (including Transformers) for several medical applications based on the types of tasks that may integrate several works pipelines of the medical domain. This work distinguishes itself from its predecessors by proposing a critical analysis of the claims and potentialities of attention mechanisms presented in the literature through an experimental case study on medical image classification with three different use cases. These experiments focus on the integrating process of attention mechanisms into established deep learning architectures, the analysis of their predictive power, and a visual assessment of their saliency maps generated by post-hoc explanation methods. This paper concludes with a critical analysis of the claims and potentialities presented in the literature about attention mechanisms and proposes future research lines in medical applications that may benefit from these frameworks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2061326129",
                        "name": "Tiago Gon\u00e7alves"
                    },
                    {
                        "authorId": "1413472908",
                        "name": "Isabel Rio-Torto"
                    },
                    {
                        "authorId": "144977907",
                        "name": "L. Teixeira"
                    },
                    {
                        "authorId": "2075457734",
                        "name": "J. S. Cardoso"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "From increasing users\u2019 trust [9, 27, 39] and improving interpretability [4, 10, 19, 20, 24] to debugging [31]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7f4bd0aae76ee987a07b025c438c705b18966e49",
                "externalIds": {
                    "DBLP": "conf/www/MalkielGBCWK22",
                    "ArXiv": "2208.06612",
                    "DOI": "10.1145/3485447.3512045",
                    "CorpusId": 248367592
                },
                "corpusId": 248367592,
                "publicationVenue": {
                    "id": "e07422f9-c065-40c3-a37b-75e98dce79fe",
                    "name": "The Web Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Web Conf",
                        "WWW"
                    ],
                    "url": "http://www.iw3c2.org/"
                },
                "url": "https://www.semanticscholar.org/paper/7f4bd0aae76ee987a07b025c438c705b18966e49",
                "title": "Interpreting BERT-based Text Similarity via Activation and Saliency Maps",
                "abstract": "Recently, there has been growing interest in the ability of Transformer-based models to produce meaningful embeddings of text with several applications, such as text similarity. Despite significant progress in the field, the explanations for similarity predictions remain challenging, especially in unsupervised settings. In this work, we present an unsupervised technique for explaining paragraph similarities inferred by pre-trained BERT models. By looking at a pair of paragraphs, our technique identifies important words that dictate each paragraph\u2019s semantics, matches between the words in both paragraphs, and retrieves the most important pairs that explain the similarity between the two. The method, which has been assessed by extensive human evaluations and demonstrated on datasets comprising long and complex paragraphs, has shown great promise, providing accurate interpretations that correlate better with human perceptions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46252132",
                        "name": "Itzik Malkiel"
                    },
                    {
                        "authorId": "1441128149",
                        "name": "Dvir Ginzburg"
                    },
                    {
                        "authorId": "48797862",
                        "name": "Oren Barkan"
                    },
                    {
                        "authorId": "27743758",
                        "name": "Avi Caciularu"
                    },
                    {
                        "authorId": "40389676",
                        "name": "Jonathan Weill"
                    },
                    {
                        "authorId": "1683070",
                        "name": "Noam Koenigstein"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "783c4b8bbd2c27aee76651d42c866e3b1272c150",
                "externalIds": {
                    "ArXiv": "2205.10226",
                    "DBLP": "conf/acl/EberleBPS22",
                    "ACL": "2022.acl-long.296",
                    "DOI": "10.48550/arXiv.2205.10226",
                    "CorpusId": 248780273
                },
                "corpusId": 248780273,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/783c4b8bbd2c27aee76651d42c866e3b1272c150",
                "title": "Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?",
                "abstract": "Learned self-attention functions in state-of-the-art NLP models often correlate with human attention. We investigate whether self-attention in large-scale pre-trained language models is as predictive of human eye fixation patterns during task-reading as classical cognitive models of human attention. We compare attention functions across two task-specific reading datasets for sentiment analysis and relation extraction. We find the predictiveness of large-scale pre-trained self-attention for human attention depends on \u2018what is in the tail\u2019, e.g., the syntactic nature of rare contexts.Further, we observe that task-specific fine-tuning does not increase the correlation with human task-specific reading. Through an input reduction experiment we give complementary insights on the sparsity and fidelity trade-off, showing that lower-entropy attention vectors are more faithful.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1557932201",
                        "name": "Oliver Eberle"
                    },
                    {
                        "authorId": "6547490",
                        "name": "Stephanie Brandl"
                    },
                    {
                        "authorId": "2137836312",
                        "name": "Jonas Pilot"
                    },
                    {
                        "authorId": "1700187",
                        "name": "Anders S\u00f8gaard"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "30540790bb4c8815c7d6336115bfce5f7d67fb85",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-09840",
                    "ArXiv": "2204.09840",
                    "DOI": "10.48550/arXiv.2204.09840",
                    "CorpusId": 248300176
                },
                "corpusId": 248300176,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/30540790bb4c8815c7d6336115bfce5f7d67fb85",
                "title": "Multi-Tier Platform for Cognizing Massive Electroencephalogram",
                "abstract": "An end-to-end platform assembling multiple tiers is built for precisely cognizing brain activities. Being fed massive electroencephalogram (EEG) data, the time-frequency spectrograms are conventionally projected into the episode-wise feature matrices (seen as tier-1). A spiking neural network (SNN) based tier is designed to distill the principle information in terms of spike-streams from the rare features, which maintains the temporal implication in the nature of EEGs. The proposed tier-3 transposes time- and space-domain of spike patterns from the SNN; and feeds the transposed pattern-matrices into an artificial neural network (ANN, Transformer specifically) known as tier-4, where a special spanning topology is proposed to match the two-dimensional input form. In this manner, cognition such as classification is conducted with high accuracy. For proof-of-concept, the sleep stage scoring problem is demonstrated by introducing multiple EEG datasets with the largest comprising 42,560 hours recorded from 5,793 subjects. From experiment results, our platform achieves the general cognition overall accuracy of 87% by leveraging sole EEG, which is 2% superior to the state-of-the-art. Moreover, our developed multi-tier methodology offers visible and graphical interpretations of the temporal characteristics of EEG by identifying the critical episodes, which is demanded in neurodynamics but hardly appears in conventional cognition scenarios.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2117203726",
                        "name": "Zheng Chen"
                    },
                    {
                        "authorId": "1750926460",
                        "name": "Lingwei Zhu"
                    },
                    {
                        "authorId": "2155486181",
                        "name": "Ziwei Yang"
                    },
                    {
                        "authorId": "7265846",
                        "name": "Renyuan Zhang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3ab64c8e9e05fd2426d6cc51f16d931279b862df",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-08227",
                    "ArXiv": "2204.08227",
                    "DOI": "10.48550/arXiv.2204.08227",
                    "CorpusId": 248227452
                },
                "corpusId": 248227452,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/3ab64c8e9e05fd2426d6cc51f16d931279b862df",
                "title": "The Devil is in the Frequency: Geminated Gestalt Autoencoder for Self-Supervised Visual Pre-Training",
                "abstract": "The self-supervised Masked Image Modeling (MIM) schema, following \"mask-and-reconstruct\" pipeline of recovering contents from masked image, has recently captured the increasing interest in the community, owing to the excellent ability of learning visual representation from unlabeled data. Aiming at learning representations with high semantics abstracted, a group of works attempts to reconstruct non-semantic pixels with large-ratio masking strategy, which may suffer from \"over-smoothing\" problem, while others directly infuse semantics into targets in off-line way requiring extra data. Different from them, we shift the perspective to the Fourier domain which naturally has global perspective and present a new Masked Image Modeling (MIM), termed Geminated Gestalt Autoencoder (Ge^2-AE) for visual pre-training. Specifically, we equip our model with geminated decoders in charge of reconstructing image contents from both pixel and frequency space, where each other serves as not only the complementation but also the reciprocal constraints. Through this way, more robust representations can be learned in the pre-trained encoders, of which the effectiveness is confirmed by the juxtaposing experimental results on downstream recognition tasks. We also conduct several quantitative and qualitative experiments to investigate the learning behavior of our method. To our best knowledge, this is the first MIM work to solve the visual pre-training through the lens of frequency domain.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48446712",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "2110312091",
                        "name": "Xinghua Jiang"
                    },
                    {
                        "authorId": "2153898418",
                        "name": "Xin Li"
                    },
                    {
                        "authorId": "39710397",
                        "name": "Antai Guo"
                    },
                    {
                        "authorId": "3336726",
                        "name": "Deqiang Jiang"
                    },
                    {
                        "authorId": "2064646914",
                        "name": "Bo Ren"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Class-specific behavior is also introduced using Contrastive-LRP [12] and Softmax-Gradient LRP [15].",
                "Post-hoc work [1, 7] has interpreted and visualized",
                "In the second row we observe that the negative contributions from the noisy background observed in the attention maps of DeiT-B + pADL + AR are alleviated in ViTOL-GAR/LRP.\nVisualization on CUB: In Figure 5, first row, we showcase five random example images from the CUB dataset.",
                "For each of the example images, we visualize the baseline DeiT-B with AR, GAR and LRP in the second, third and fourth columns, and, DeiT-B + p-ADL with AR, ViTOL with LRP and ViTOL with GAR attention maps in the final three columns.",
                "In Section 4, we also showcase results for an alternative post-hoc approach called Layer Relevance Propagation [7].",
                "We observe that our approaches with a DeiT-B backbone with p-ADL + (a) GAR and (b) LRP significantly outperform the other WSOL approaches.",
                "[7] proposed an alternate method of assigning local relevance based on the DTD principle to generate class dependent attention maps.",
                "[7], this overlooks the fact that GELU [14] activation is used in all intermediate layers.",
                "[7] examined the problems in attention rollout and proposed a method that",
                "In the second row, we overlay the attention maps obtained from ViTOL-LRP for these images.",
                "We compare this against attention rollout (AR) mechanism [1] and layer relevance propogation (LRP) [7] for generating class dependent attention maps.",
                "Training and Testing details: On ImageNet-1K, for baseline models, we use the DeiT-B and DeiT-S ImageNet pre-trained weights and evaluate on all the methods, namely, AR, GAR and LRP as stated in Table 1, 2 and 3.",
                "We observe that attention maps generated for ViTOL with GAR/LRP show dependency with the class, are noisefree and cover the complete object of interest.",
                "Other ablations: Some other ablation studies include i) comparison of GAR and LRP across different transformer backbones, ii) detailed study of patch drop masks for the pADL layer and iii) effect of changing embedding drop rate\nand drop threshold hyper-parameters for p-ADL layer.",
                "For more details we refer the reader to [7]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "d4fec6b3345d1e48b8988075396fe4c753981e61",
                "externalIds": {
                    "ArXiv": "2204.06772",
                    "DBLP": "journals/corr/abs-2204-06772",
                    "DOI": "10.1109/CVPRW56347.2022.00455",
                    "CorpusId": 248177857
                },
                "corpusId": 248177857,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d4fec6b3345d1e48b8988075396fe4c753981e61",
                "title": "ViTOL: Vision Transformer for Weakly Supervised Object Localization",
                "abstract": "Weakly supervised object localization (WSOL) aims at predicting object locations in an image using only image-level category labels. Common challenges that image classification models encounter when localizing objects are, (a) they tend to look at the most discriminative features in an image that confines the localization map to a very small region, (b) the localization maps are class agnostic, and the models highlight objects of multiple classes in the same image and, (c) the localization performance is affected by background noise. To alleviate the above challenges we introduce the following simple changes through our proposed method ViTOL. We leverage the vision-based transformer for self-attention and introduce a patch-based attention dropout layer (p-ADL) to increase the coverage of the localization map and a gradient attention rollout mechanism to generate class-dependent attention maps. We conduct extensive quantitative, qualitative and ablation experiments on the ImageNet-1K and CUB datasets. We achieve state-of-the-art MaxBoxAcc-V2 localization scores of 70.47% and 73.17% on the two datasets respectively. Code is available on https://github.com/Saurav-31/ViTOL.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2162604498",
                        "name": "Saurav Gupta"
                    },
                    {
                        "authorId": "2162473538",
                        "name": "Sourav Lakhotia"
                    },
                    {
                        "authorId": "40829961",
                        "name": "Abhay Rawat"
                    },
                    {
                        "authorId": "46182292",
                        "name": "Rahul Tallamraju"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "00a124ecdb12e186383ee47c24515fde66d436c0",
                "externalIds": {
                    "ArXiv": "2204.05591",
                    "DBLP": "journals/corr/abs-2204-05591",
                    "DOI": "10.48550/arXiv.2204.05591",
                    "CorpusId": 248118530,
                    "PubMed": "35985360"
                },
                "corpusId": 248118530,
                "publicationVenue": {
                    "id": "a015ea7c-fb55-4ef1-9328-e10d1ca225c9",
                    "name": "Survey of ophthalmology",
                    "type": "journal",
                    "alternate_names": [
                        "Survey of Ophthalmology",
                        "Surv ophthalmol",
                        "Surv Ophthalmol"
                    ],
                    "issn": "0039-6257",
                    "url": "https://www.journals.elsevier.com/survey-of-ophthalmology",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00396257"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/00a124ecdb12e186383ee47c24515fde66d436c0",
                "title": "Automatic detection of glaucoma via fundus imaging and artificial intelligence: A review",
                "abstract": "Glaucoma is a leading cause of irreversible vision impairment globally, and cases are continuously rising worldwide. Early detection is crucial, allowing timely intervention that can prevent further visual field loss. To detect glaucoma, examination of the optic nerve head via fundus imaging can be performed, at the center of which is the assessment of the optic cup and disc boundaries. Fundus imaging is non-invasive and low-cost; however, the image examination relies on subjective, time-consuming, and costly expert assessments. A timely question to ask is: \"Can artificial intelligence mimic glaucoma assessments made by experts?\". Specifically, can artificial intelligence automatically find the boundaries of the optic cup and disc (providing a so-called segmented fundus image) and then use the segmented image to identify glaucoma with high accuracy? We conducted a comprehensive review on artificial intelligence-enabled glaucoma detection frameworks that produce and use segmented fundus images and summarized the advantages and disadvantages of such frameworks. We identified 36 relevant papers from 2011-2021 and 2 main approaches: 1) logical rule-based frameworks, based on a set of rules; and 2) machine learning/statistical modelling based frameworks. We critically evaluated the state-of-art of the 2 approaches, identified gaps in the literature and pointed at areas for future research.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2162187664",
                        "name": "Lauren Coan"
                    },
                    {
                        "authorId": "2000539",
                        "name": "Bryan M. Williams"
                    },
                    {
                        "authorId": "2134982195",
                        "name": "V. Adithya"
                    },
                    {
                        "authorId": "3443967",
                        "name": "S. Upadhyaya"
                    },
                    {
                        "authorId": "1799314",
                        "name": "S. Czanner"
                    },
                    {
                        "authorId": "144366136",
                        "name": "R. Venkatesh"
                    },
                    {
                        "authorId": "2939591",
                        "name": "C. Willoughby"
                    },
                    {
                        "authorId": "40474965",
                        "name": "K. Srinivasan"
                    },
                    {
                        "authorId": "2219560",
                        "name": "G. Czanner"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "However, the literature on Transformer explainability is relatively sparse and most methods focus on pure self-attention architectures [1, 8]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a9ab78ff9424794cd4de4bd1ff5a87e721a79ac4",
                "externalIds": {
                    "ArXiv": "2204.04908",
                    "DBLP": "journals/corr/abs-2204-04908",
                    "DOI": "10.48550/arXiv.2204.04908",
                    "CorpusId": 248084956
                },
                "corpusId": 248084956,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/a9ab78ff9424794cd4de4bd1ff5a87e721a79ac4",
                "title": "No Token Left Behind: Explainability-Aided Image Classification and Generation",
                "abstract": "The application of zero-shot learning in computer vision has been revolutionized by the use of image-text matching models. The most notable example, CLIP, has been widely used for both zero-shot classification and guiding generative models with a text prompt. However, the zero-shot use of CLIP is unstable with respect to the phrasing of the input text, making it necessary to carefully engineer the prompts used. We find that this instability stems from a selective similarity score, which is based only on a subset of the semantically meaningful input tokens. To mitigate it, we present a novel explainability-based approach, which adds a loss term to ensure that CLIP focuses on all relevant semantic parts of the input, in addition to employing the CLIP similarity loss used in previous works. When applied to one-shot classification through prompt engineering, our method yields an improvement in the recognition rate, without additional training or fine-tuning. Additionally, we show that CLIP guidance of generative models using our method significantly improves the generated images. Finally, we demonstrate a novel use of CLIP guidance for text-based image generation with spatial conditioning on object location, by requiring the image explainability heatmap for each object to be confined to a pre-determined bounding box.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2134839079",
                        "name": "Roni Paiss"
                    },
                    {
                        "authorId": "2038268012",
                        "name": "Hila Chefer"
                    },
                    {
                        "authorId": "145128145",
                        "name": "Lior Wolf"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Attention Visualization: In order to visualize the parts of the facial image that contributes to the category clarification, we apply the visualization method of [6] to visualize the attention maps in the transformer."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2e000ec0c9594599cfe5aca4d063c62f4a168ef0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-04083",
                    "ArXiv": "2204.04083",
                    "DOI": "10.48550/arXiv.2204.04083",
                    "CorpusId": 248069142
                },
                "corpusId": 248069142,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2e000ec0c9594599cfe5aca4d063c62f4a168ef0",
                "title": "POSTER: A Pyramid Cross-Fusion Transformer Network for Facial Expression Recognition",
                "abstract": "Facial expression recognition (FER) is an important task in computer vision, having practical applications in areas such as human-computer interaction, education, healthcare, and online monitoring. In this challenging FER task, there are three key issues especially prevalent: inter-class similarity, intra-class discrepancy, and scale sensitivity. While existing works typically address some of these issues, none have fully addressed all three challenges in a unified framework. In this paper, we propose a two-stream Pyramid crOss-fuSion TransformER network (POSTER), that aims to holistically solve all three issues. Specifically, we design a transformer-based cross-fusion method that enables effective collaboration of facial landmark features and image features to maximize proper attention to salient facial regions. Furthermore, POSTER employs a pyramid structure to promote scale invariance. Extensive experimental results demonstrate that our POSTER achieves new state-of-the-art results on RAF-DB (92.05%), FERPlus (91.62%), as well as AffectNet 7 class (67.31%) and 8 class (63.34%). The code is available at https://github.com/zczcwh/POSTER.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2113919899",
                        "name": "Ce Zheng"
                    },
                    {
                        "authorId": "1422036273",
                        "name": "Mat'ias Mendieta"
                    },
                    {
                        "authorId": "2141809453",
                        "name": "Chen Chen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "An identity matrix I is added to avoid self-inhibition of patches [40].",
                "Unlike conventional gradient-based visualization techniques [39], we use an attention-oriented visualization similar to a visual work [40] to highlight the FT patches the model pays the most attention to by inferring both the gradient information and the relevance from the final classification decision to each attention layer."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ca7f35a0c16774cc6d52cabe1a4f4fe77dab9a46",
                "externalIds": {
                    "ArXiv": "2204.03173",
                    "DOI": "10.1109/TNSRE.2023.3243589",
                    "CorpusId": 250071984,
                    "PubMed": "37022825"
                },
                "corpusId": 250071984,
                "publicationVenue": {
                    "id": "416b088d-de72-4dbf-959e-8551984d9676",
                    "name": "IEEE transactions on neural systems and rehabilitation engineering",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Neural Syst Rehabilitation Eng",
                        "IEEE trans neural syst rehabilitation eng",
                        "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
                    ],
                    "issn": "1534-4320",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=7333",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7333"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ca7f35a0c16774cc6d52cabe1a4f4fe77dab9a46",
                "title": "Automated Sleep Staging via Parallel Frequency-Cut Attention",
                "abstract": "Stage-based sleep screening is a widely-used tool in both healthcare and neuroscientific research, as it allows for the accurate assessment of sleep patterns and stages. In this paper, we propose a novel framework that is based on authoritative guidance in sleep medicine and is designed to automatically capture the time-frequency characteristics of sleep electroencephalogram (EEG) signals in order to make staging decisions. Our framework consists of two main phases: a feature extraction process that partitions the input EEG spectrograms into a sequence of time-frequency patches, and a staging phase that searches for correlations between the extracted features and the defining characteristics of sleep stages. To model the staging phase, we utilize a Transformer model with an attention-based module, which allows for the extraction of global contextual relevance among time-frequency patches and the use of this relevance for staging decisions. The proposed method is validated on the large-scale Sleep Heart Health Study dataset and achieves new state-of-the-art results for the wake, N2, and N3 stages, with respective F1 scores of 0.93, 0.88, and 0.87 using only EEG signals. Our method also demonstrates high inter-rater reliability, with a kappa score of 0.80. Moreover, we provide visualizations of the correspondence between sleep staging decisions and features extracted by our method, which enhances the interpretability of the proposal. Overall, our work represents a significant contribution to the field of automated sleep staging and has important implications for both healthcare and neuroscience research.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2117203726",
                        "name": "Zheng Chen"
                    },
                    {
                        "authorId": "2155486181",
                        "name": "Ziwei Yang"
                    },
                    {
                        "authorId": "1750926460",
                        "name": "Lingwei Zhu"
                    },
                    {
                        "authorId": "2154939940",
                        "name": "Wei Chen"
                    },
                    {
                        "authorId": "46526573",
                        "name": "T. Tamura"
                    },
                    {
                        "authorId": "2467259",
                        "name": "N. Ono"
                    },
                    {
                        "authorId": "1399354648",
                        "name": "M. Altaf-Ul-Amin"
                    },
                    {
                        "authorId": "2105905006",
                        "name": "Shigehiko Kanaya"
                    },
                    {
                        "authorId": "2108435519",
                        "name": "Ming Huang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "In brief, Zabari & Hoshen (2021) create a set of query-driven relevance maps for an image, coupled with transformer interpretability methods (Chefer et al., 2021)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0791a0441e1f672c43aecb2d6708fbc8725c8cad",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-01694",
                    "ArXiv": "2204.01694",
                    "DOI": "10.48550/arXiv.2204.01694",
                    "CorpusId": 247939764
                },
                "corpusId": 247939764,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/0791a0441e1f672c43aecb2d6708fbc8725c8cad",
                "title": "\"This is my unicorn, Fluffy\": Personalizing frozen vision-language representations",
                "abstract": "Large Vision&Language models pretrained on web-scale data provide representations that are invaluable for numerous V&L problems. However, it is unclear how they can be used for reasoning about user-specific visual concepts in unstructured language. This problem arises in multiple domains, from personalized image retrieval to personalized interaction with smart devices. We introduce a new learning setup called Personalized Vision&Language (PerVL) with two new benchmark datasets for retrieving and segmenting user-specific\"personalized\"concepts\"in the wild\". In PerVL, one should learn personalized concepts (1) independently of the downstream task (2) allowing a pretrained model to reason about them with free language, and (3) does not require personalized negative examples. We propose an architecture for solving PerVL that operates by extending the input vocabulary of a pretrained model with new word embeddings for the new personalized concepts. The model can then reason about them by simply using them in a sentence. We demonstrate that our approach learns personalized visual concepts from a few examples and can effectively apply them in image retrieval and semantic segmentation using rich textual queries.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "22021547",
                        "name": "Niv Cohen"
                    },
                    {
                        "authorId": "134639223",
                        "name": "Rinon Gal"
                    },
                    {
                        "authorId": "2656186",
                        "name": "E. Meirom"
                    },
                    {
                        "authorId": "1732280",
                        "name": "Gal Chechik"
                    },
                    {
                        "authorId": "34815079",
                        "name": "Y. Atzmon"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Attempts [55, 42, 9] have been made to interpret image classification models.",
                "[9] went beyond attention visualization by adopting gradients and the propagation of relevancy scores."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a9ba2a82a84ee759a1063e97cce5981d03db47c1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-01186",
                    "ArXiv": "2204.01186",
                    "DOI": "10.48550/arXiv.2204.01186",
                    "CorpusId": 247940117
                },
                "corpusId": 247940117,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/a9ba2a82a84ee759a1063e97cce5981d03db47c1",
                "title": "Revisiting a kNN-based Image Classification System with High-capacity Storage",
                "abstract": "In existing image classification systems that use deep neural networks, the knowledge needed for image classification is implicitly stored in model parameters. If users want to update this knowledge, then they need to fine-tune the model parameters. Moreover, users cannot verify the validity of inference results or evaluate the contribution of knowledge to the results. In this paper, we investigate a system that stores knowledge for image classification, such as image feature maps, labels, and original images, not in model parameters but in external high-capacity storage. Our system refers to the storage like a database when classifying input images. To increase knowledge, our system updates the database instead of fine-tuning model parameters, which avoids catastrophic forgetting in incremental learning scenarios. We revisit a kNN (k-Nearest Neighbor) classifier and employ it in our system. By analyzing the neighborhood samples referred by the kNN algorithm, we can interpret how knowledge learned in the past is used for inference results. Our system achieves 79.8% top-1 accuracy on the ImageNet dataset without fine-tuning model parameters after pretraining, and 90.8% accuracy on the Split CIFAR-100 dataset in the task incremental learning setting.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2047738",
                        "name": "K. Nakata"
                    },
                    {
                        "authorId": "20556792",
                        "name": "Youyang Ng"
                    },
                    {
                        "authorId": "2441156",
                        "name": "D. Miyashita"
                    },
                    {
                        "authorId": "38136807",
                        "name": "A. Maki"
                    },
                    {
                        "authorId": "145008078",
                        "name": "Yu Lin"
                    },
                    {
                        "authorId": "49192096",
                        "name": "J. Deguchi"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6157aded1cc77adfa428ae315abca22b5fcea69f",
                "externalIds": {
                    "PubMedCentral": "9008134",
                    "DOI": "10.3389/fnsys.2022.800280",
                    "CorpusId": 247799011,
                    "PubMed": "35431820"
                },
                "corpusId": 247799011,
                "publicationVenue": {
                    "id": "ba12d37f-ecd3-47c2-a173-22fb1bae2ece",
                    "name": "Frontiers in Systems Neuroscience",
                    "type": "journal",
                    "alternate_names": [
                        "Front Syst Neurosci"
                    ],
                    "issn": "1662-5137",
                    "url": "https://www.frontiersin.org/journals/systems-neuroscience",
                    "alternate_urls": [
                        "http://www.frontiersin.org/systemsneuroscience/",
                        "http://www.frontiersin.org/systems_neuroscience"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6157aded1cc77adfa428ae315abca22b5fcea69f",
                "title": "Understanding Is a Process",
                "abstract": "How do we gauge understanding? Tests of understanding, such as Turing's imitation game, are numerous; yet, attempts to achieve a state of understanding are not satisfactory assessments. Intelligent agents designed to pass one test of understanding often fall short of others. Rather than approaching understanding as a system state, in this paper, we argue that understanding is a process that changes over time and experience. The only window into the process is through the lens of natural language. Usefully, failures of understanding reveal breakdowns in the process. We propose a set of natural language-based probes that can be used to map the degree of understanding a human or intelligent system has achieved through combinations of successes and failures.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2684131",
                        "name": "L. Blaha"
                    },
                    {
                        "authorId": "122860492",
                        "name": "Mitchell Abrams"
                    },
                    {
                        "authorId": "3390221",
                        "name": "Sarah A Bibyk"
                    },
                    {
                        "authorId": "3202888",
                        "name": "Claire Bonial"
                    },
                    {
                        "authorId": "11880712",
                        "name": "B. M. Hartzler"
                    },
                    {
                        "authorId": "2160853175",
                        "name": "Christopher D. Hsu"
                    },
                    {
                        "authorId": "2011661",
                        "name": "S. Khemlani"
                    },
                    {
                        "authorId": "2116964118",
                        "name": "Jayde King"
                    },
                    {
                        "authorId": "47793058",
                        "name": "R. St. Amant"
                    },
                    {
                        "authorId": "145209749",
                        "name": "J. Trafton"
                    },
                    {
                        "authorId": "2160754294",
                        "name": "Rachel Wong"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "[7] proposed a method for visualizing self-attention models by calculating a LRP [1]-based relevancy score for each attention head in each layer, and propagating relevancies through the network."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "abc6e5b86406e87b09a9520e1757d6db4c7fa08a",
                "externalIds": {
                    "ArXiv": "2203.17247",
                    "DBLP": "journals/corr/abs-2203-17247",
                    "DOI": "10.1109/CVPR52688.2022.02072",
                    "CorpusId": 247839845
                },
                "corpusId": 247839845,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/abc6e5b86406e87b09a9520e1757d6db4c7fa08a",
                "title": "VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers",
                "abstract": "Breakthroughs in transformer-based models have revolutionized not only the NLP field, but also vision and multimodal systems. However, although visualization and interpretability tools have become available for NLP models, internal mechanisms of vision and multimodal transformers remain largely opaque. With the success of these transformers, it is increasingly critical to understand their inner workings, as unraveling these black-boxes will lead to more capable and trustworthy models. To contribute to this quest, we propose VL-InterpreT, which provides novel interactive visualizations for interpreting the attentions and hidden representations in multimodal transformers. VL-InterpreT is a task agnostic and integrated tool that (1) tracks a variety of statistics in attention heads throughout all layers for both vision and language components, (2) visualizes cross-modal and intra-modal attentions through easily readable heatmaps, and (3) plots the hidden representations of vision and language tokens as they pass through the transformer layers. In this paper, we demonstrate the functionalities of VL-InterpreT through the analysis of KD-VLP, an end-to-end pretraining vision-language multimodal transformer-based model, in the tasks of Visual Commonsense Reasoning (VCR) and WebQA, two visual question answering benchmarks. Furthermore, we also present a few interesting findings about multimodal transformer behaviors that were learned through our tool.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1816753600",
                        "name": "Estelle Aflalo"
                    },
                    {
                        "authorId": "2160888992",
                        "name": "Meng Du"
                    },
                    {
                        "authorId": "3156011",
                        "name": "Shao-Yen Tseng"
                    },
                    {
                        "authorId": "2108078299",
                        "name": "Yongfei Liu"
                    },
                    {
                        "authorId": "2151101534",
                        "name": "Chenfei Wu"
                    },
                    {
                        "authorId": "46429989",
                        "name": "Nan Duan"
                    },
                    {
                        "authorId": "95340164",
                        "name": "Vasudev Lal"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "More recent work visualizes the attention maps in transformers [8,9,47]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "02b7b90144851c9193918433dcfccd70e005b289",
                "externalIds": {
                    "DBLP": "conf/eccv/ThomasZC22",
                    "ArXiv": "2203.15704",
                    "DOI": "10.48550/arXiv.2203.15704",
                    "CorpusId": 247778943
                },
                "corpusId": 247778943,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/02b7b90144851c9193918433dcfccd70e005b289",
                "title": "Fine-Grained Visual Entailment",
                "abstract": "Visual entailment is a recently proposed multimodal reasoning task where the goal is to predict the logical relationship of a piece of text to an image. In this paper, we propose an extension of this task, where the goal is to predict the logical relationship of fine-grained knowledge elements within a piece of text to an image. Unlike prior work, our method is inherently explainable and makes logical predictions at different levels of granularity. Because we lack fine-grained labels to train our method, we propose a novel multi-instance learning approach which learns a fine-grained labeling using only sample-level supervision. We also impose novel semantic structural constraints which ensure that fine-grained predictions are internally semantically consistent. We evaluate our method on a new dataset of manually annotated knowledge elements and show that our method achieves 68.18\\% accuracy at this challenging task while significantly outperforming several strong baselines. Finally, we present extensive qualitative results illustrating our method's predictions and the visual evidence our method relied on. Our code and annotated dataset can be found here: https://github.com/SkrighYZ/FGVE.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2150796972",
                        "name": "Christopher Thomas"
                    },
                    {
                        "authorId": "2760404",
                        "name": "Yipeng Zhang"
                    },
                    {
                        "authorId": "2122374530",
                        "name": "Shih-Fu Chang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "We visualize the attention maps of transformers using Transformer Explainability [6].",
                "12, we visualize the attention maps of different transformer models on spoof images using Transformer Explainability [6].",
                "7, we visualize the attention maps of different transformers on spoof images using Transformer Explainability [6]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "7b7f360325742fb9acde9e6d6f6dd3970ed4b995",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-12175",
                    "ArXiv": "2203.12175",
                    "DOI": "10.48550/arXiv.2203.12175",
                    "CorpusId": 247619150
                },
                "corpusId": 247619150,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/7b7f360325742fb9acde9e6d6f6dd3970ed4b995",
                "title": "Adaptive Transformers for Robust Few-shot Cross-domain Face Anti-spoofing",
                "abstract": "While recent face anti-spoofing methods perform well under the intra-domain setups, an effective approach needs to account for much larger appearance variations of images acquired in complex scenes with different sensors for robust performance. In this paper, we present adaptive vision transformers (ViT) for robust cross-domain face antispoofing. Specifically, we adopt ViT as a backbone to exploit its strength to account for long-range dependencies among pixels. We further introduce the ensemble adapters module and feature-wise transformation layers in the ViT to adapt to different domains for robust performance with a few samples. Experiments on several benchmark datasets show that the proposed models achieve both robust and competitive performance against the state-of-the-art methods for cross-domain face anti-spoofing using a few samples.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115636909",
                        "name": "Hsin-Ping Huang"
                    },
                    {
                        "authorId": "3232265",
                        "name": "Deqing Sun"
                    },
                    {
                        "authorId": "1614039598",
                        "name": "Yaojie Liu"
                    },
                    {
                        "authorId": "39336289",
                        "name": "Wen-Sheng Chu"
                    },
                    {
                        "authorId": "14002400",
                        "name": "Taihong Xiao"
                    },
                    {
                        "authorId": "2150063945",
                        "name": "Jinwei Yuan"
                    },
                    {
                        "authorId": "2595180",
                        "name": "Hartwig Adam"
                    },
                    {
                        "authorId": "37144787",
                        "name": "Ming-Hsuan Yang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "One future direction is to interpret the Transformer by visualizing the attention heatmaps based on the deep Taylor decomposition principle [70]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9b0404425af712a31d15d181eacc66cced8ccc66",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-06598",
                    "ArXiv": "2204.06598",
                    "DOI": "10.1109/TMI.2022.3161739",
                    "CorpusId": 247628586,
                    "PubMed": "35320092"
                },
                "corpusId": 247628586,
                "publicationVenue": {
                    "id": "e0cda45d-3074-4ac0-80b8-e5250df00b89",
                    "name": "IEEE Transactions on Medical Imaging",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Med Imaging"
                    ],
                    "issn": "0278-0062",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=42",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=42"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9b0404425af712a31d15d181eacc66cced8ccc66",
                "title": "Deep Relation Learning for Regression and Its Application to Brain Age Estimation",
                "abstract": "Most deep learning models for temporal regression directly output the estimation based on single input images, ignoring the relationships between different images. In this paper, we propose deep relation learning for regression, aiming to learn different relations between a pair of input images. Four non-linear relations are considered: \u201ccumulative relation,\u201d \u201crelative relation,\u201d \u201cmaximal relation\u201d and \u201cminimal relation.\u201d These four relations are learned simultaneously from one deep neural network which has two parts: feature extraction and relation regression. We use an efficient convolutional neural network to extract deep features from the pair of input images and apply a Transformer for relation learning. The proposed method is evaluated on a merged dataset with 6,049 subjects with ages of 0\u201397 years using 5-fold cross-validation for the task of brain age estimation. The experimental results have shown that the proposed method achieved a mean absolute error (MAE) of 2.38 years, which is lower than the MAEs of 8 other state-of-the-art algorithms with statistical significance (p<0.05) in paired T-test (two-side).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115303290",
                        "name": "Sheng He"
                    },
                    {
                        "authorId": "2160104321",
                        "name": "Yanfang Feng"
                    },
                    {
                        "authorId": "2249910331",
                        "name": "P. Grant"
                    },
                    {
                        "authorId": "2227890",
                        "name": "Yangming Ou"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "On the other hand, explaining trained ViTs requires non-trivial and sophisticated methods [4] following the trend of eXplainable AI (XAI) [18] that has been extensively studied with convolutional neural networks.",
                "As pointed out in the Improved LRP [4], reducing the explanation to only the attentions scores may be myopic since many other components are ignored.",
                "As pointed out in the Im-\nproved LRP [4], reducing the explanation to only the attentions scores may be myopic since many other components are ignored."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d0cbdf7d40fe6ba2133e0442c27702270efa1268",
                "externalIds": {
                    "DBLP": "conf/cvpr/GraingerPSCL023",
                    "ArXiv": "2203.11987",
                    "DOI": "10.1109/CVPR52729.2023.01781",
                    "CorpusId": 258040914
                },
                "corpusId": 258040914,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d0cbdf7d40fe6ba2133e0442c27702270efa1268",
                "title": "PaCa-ViT: Learning Patch-to-Cluster Attention in Vision Transformers",
                "abstract": "Vision Transformers (ViTs) are built on the assumption of treating image patches as \u201cvisual tokens\u201d and learn patch-to-patch attention. The patch embedding based tokenizer has a semantic gap with respect to its counterpart, the textual tokenizer. The patch-to-patch attention suffers from the quadratic complexity issue, and also makes it non-trivial to explain learned ViTs. To address these issues in ViT, this paper proposes to learn Patch-to-Cluster attention (PaCa) in ViT. Queries in our PaCa-ViT starts with patches, while keys and values are directly based on clustering (with a predefined small number of clusters). The clusters are learned end-to-end, leading to better tokenizers and inducing joint clustering-for-attention and attention-for-clustering for better and interpretable models. The quadratic complexity is relaxed to linear complexity. The proposed PaCa module is used in designing efficient and interpretable ViT backbones and semantic segmentation head networks. In experiments, the proposed methods are tested on ImageNet-1k image classification, MS-COCO object detection and instance segmentation and MIT-ADE20k semantic segmentation. Compared with the prior art, it obtains better performance in all the three benchmarks than the SWin [32] and the PVTs [47], [48] by significant margins in ImageNet-1k and MIT-ADE20k. It is also significantly more efficient than PVT models in MS-COCO and MIT-ADE20k due to the linear complexity. The learned clusters are semantically meaningful. Code and model checkpoints are available at https:/github.com/iVMCL/PaCaViT.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2159674931",
                        "name": "Ryan Grainger"
                    },
                    {
                        "authorId": "1390008236",
                        "name": "Thomas Paniagua"
                    },
                    {
                        "authorId": "47684568",
                        "name": "Xi Song"
                    },
                    {
                        "authorId": "1803047",
                        "name": "Naresh P. Cuntoor"
                    },
                    {
                        "authorId": "2214162981",
                        "name": "Mun Wai Lee"
                    },
                    {
                        "authorId": "47353858",
                        "name": "Tianfu Wu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "742b195fb4c2868a4e60012c8e0bf7db43bb5650",
                "externalIds": {
                    "DBLP": "conf/cvpr/GadreWISS23",
                    "ArXiv": "2203.10421",
                    "DOI": "10.1109/CVPR52729.2023.02219",
                    "CorpusId": 254636632
                },
                "corpusId": 254636632,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/742b195fb4c2868a4e60012c8e0bf7db43bb5650",
                "title": "CoWs on Pasture: Baselines and Benchmarks for Language-Driven Zero-Shot Object Navigation",
                "abstract": "For robots to be generally useful, they must be able to find arbitrary objects described by people (i.e., be language-driven) even without expensive navigation training on in-domain data (i.e., perform zero-shot inference). We explore these capabilities in a unified setting: language- driven zero-shot object navigation (L-ZSON). Inspired by the recent success of open-vocabulary models for image classification, we investigate a straightforward framework, CLIP on Wheels (CoW), to adapt open-vocabulary models to this task without fine-tuning. To better evaluate L-ZSON, we introduce the Pasturebenchmark, which considers finding uncommon objects, objects described by spatial and appearance attributes, and hidden objects described relative to visible objects. We conduct an in-depth empirical study by directly deploying 22 CoW baselines across Habitat, Robothor,and Pasture. In total, we evaluate over 90k navigation episodes and find that (1) CoW baselines often struggle to leverage language descriptions but are proficient at finding uncommon objects. (2) A simple Co W, with CLIP-based object localization and classical exploration-and no additional training-matches the navigation efficiency of a state-of-the-art ZSON method trained for 500M steps on HabitatMp3d data. This same CoW provides a 15.6 percentage point improvement in success over a state-of-the-art ROBOTHOR ZSON model.11For code, data, and videos, see cow.cs.columbia.edu/",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1387466862",
                        "name": "S. Gadre"
                    },
                    {
                        "authorId": "52193502",
                        "name": "Mitchell Wortsman"
                    },
                    {
                        "authorId": "1387994137",
                        "name": "Gabriel Ilharco"
                    },
                    {
                        "authorId": "152772922",
                        "name": "Ludwig Schmidt"
                    },
                    {
                        "authorId": "3340170",
                        "name": "Shuran Song"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6838cf75a2f743e4811f8fb1a61341e9203cf3de",
                "externalIds": {
                    "ArXiv": "2203.08421",
                    "DBLP": "journals/corr/abs-2203-08421",
                    "DOI": "10.48550/arXiv.2203.08421",
                    "CorpusId": 247476075
                },
                "corpusId": 247476075,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6838cf75a2f743e4811f8fb1a61341e9203cf3de",
                "title": "WegFormer: Transformers for Weakly Supervised Semantic Segmentation",
                "abstract": "Although convolutional neural networks (CNNs) have achieved remarkable progress in weakly supervised semantic segmentation (WSSS), the effective receptive field of CNN is insufficient to capture global context information, leading to sub-optimal results. Inspired by the great success of Transformers in fundamental vision areas, this work for the first time introduces Transformer to build a simple and effective WSSS framework, termed WegFormer. Unlike existing CNN-based methods, WegFormer uses Vision Transformer (ViT) as a classifier to produce high-quality pseudo segmentation masks. To this end, we introduce three tailored components in our Transformer-based framework, which are (1) a Deep Taylor Decomposition (DTD) to generate attention maps, (2) a soft erasing module to smooth the attention maps, and (3) an efficient potential object mining (EPOM) to filter noisy activation in the background. Without any bells and whistles, WegFormer achieves state-of-the-art 70.5% mIoU on the PASCAL VOC dataset, significantly outperforming the previous best method. We hope WegFormer provides a new perspective to tap the potential of Transformer in weakly supervised semantic segmentation. Code will be released.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2107918881",
                        "name": "Chunmeng Liu"
                    },
                    {
                        "authorId": "41020000",
                        "name": "Enze Xie"
                    },
                    {
                        "authorId": "2117833005",
                        "name": "Wenjia Wang"
                    },
                    {
                        "authorId": "71074736",
                        "name": "Wenhai Wang"
                    },
                    {
                        "authorId": "71300641",
                        "name": "Guangya Li"
                    },
                    {
                        "authorId": "144389940",
                        "name": "P. Luo"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Tools of interpreting ViTs in [6] are adopted to produce visualization.",
                "Figure 5 is generated by an advanced ViT interpretable approach [6]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3a5b7838b5348315572a8c1aa8c33deea16f159d",
                "externalIds": {
                    "ArXiv": "2203.06345",
                    "DBLP": "journals/corr/abs-2203-06345",
                    "DOI": "10.1109/CVPR52688.2022.01171",
                    "CorpusId": 247446934
                },
                "corpusId": 247446934,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3a5b7838b5348315572a8c1aa8c33deea16f159d",
                "title": "The Principle of Diversity: Training Stronger Vision Transformers Calls for Reducing All Levels of Redundancy",
                "abstract": "Vision transformers (ViTs) have gained increasing popularity as they are commonly believed to own higher mod-eling capacity and representation flexibility, than traditional convolutional networks. However, it is questionable whether such potential has been fully unleashed in prac-tice, as the learned ViTs often suffer from over-smoothening, yielding likely redundant models. Recent works made pre-liminary attempts to identify and alleviate such redundancy, e.g., via regularizing embedding similarity or re-injecting convolution-like structures. However, a \u201chead-to-toe as-sessment\u201d regarding the extent of redundancy in ViTs, and how much we could gain by thoroughly mitigating such, has been absent for this field. This paper, for the first time, systematically studies the ubiquitous existence of re-dundancy at all three levels: patch embedding, attention map, and weight space. In view of them, we advocate a principle of diversity for training ViTs, by presenting cor-responding regularizers that encourage the representation diversity and coverage at each of those levels, that enabling capturing more discriminative information. Extensive ex-periments on ImageNet with a number of ViT backbones validate the effectiveness of our proposals, largely eliminating the observed ViT redundancy and significantly boosting the model generalization. For example, our diversified DeiT obtains 0.70% ~ 1.76% accuracy boosts on ImageNet with highly reduced similarity. Our codes are fully available in https://github.com/VITA-Group/Diverse-ViT.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2034263179",
                        "name": "Tianlong Chen"
                    },
                    {
                        "authorId": "2109338656",
                        "name": "Zhenyu (Allen) Zhang"
                    },
                    {
                        "authorId": "145215470",
                        "name": "Yu Cheng"
                    },
                    {
                        "authorId": "2072795428",
                        "name": "A. Awadallah"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                ", [4] proposed the visualization method of attention weight based on a specific formulation while maintaining the total relevancy in each layer."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6736387c87457b68e7a931d7cd6143436f91ed72",
                "externalIds": {
                    "DBLP": "conf/das/NagataOHU22",
                    "ArXiv": "2203.05338",
                    "DOI": "10.48550/arXiv.2203.05338",
                    "CorpusId": 247362804
                },
                "corpusId": 247362804,
                "publicationVenue": {
                    "id": "02d53b80-30d7-493c-9453-ed7406056b31",
                    "name": "International Workshop on Document Analysis Systems",
                    "type": "conference",
                    "alternate_names": [
                        "DAS",
                        "Document Analysis Systems",
                        "Int Workshop Doc Anal Syst",
                        "Doc Anal Syst"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=647"
                },
                "url": "https://www.semanticscholar.org/paper/6736387c87457b68e7a931d7cd6143436f91ed72",
                "title": "TrueType Transformer: Character and Font Style Recognition in Outline Format",
                "abstract": "We propose TrueType Transformer (T3), which can perform character and font style recognition in an outline format. The outline format, such as TrueType, represents each character as a sequence of control points of stroke contours and is frequently used in born-digital documents. T3 is organized by a deep neural network, so-called Transformer. Transformer is originally proposed for sequential data, such as text, and therefore appropriate for handling the outline data. In other words, T3 directly accepts the outline data without converting it into a bitmap image. Consequently, T3 realizes a resolution-independent classification. Moreover, since the locations of the control points represent the fine and local structures of the font style, T3 is suitable for font style classification, where such structures are very important. In this paper, we experimentally show the applicability of T3 in character and font style recognition tasks, while observing how the individual control points contribute to classification results.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2056677367",
                        "name": "Yusuke Nagata"
                    },
                    {
                        "authorId": "2158366609",
                        "name": "Jinki Otao"
                    },
                    {
                        "authorId": "2064706229",
                        "name": "Daichi Haraguchi"
                    },
                    {
                        "authorId": "1809705",
                        "name": "S. Uchida"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Finally, Abnar and Zuidema (2020) proposed the attention rollout method, which measures the mixing of information by linearly combining attention matrices, a method that has been extended to Transformers in the visual domain (Chefer et al., 2021a,b)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bb1c9cb431e771660cffdda1d80a7f15ff40c764",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-04212",
                    "ArXiv": "2203.04212",
                    "ACL": "2022.emnlp-main.595",
                    "DOI": "10.48550/arXiv.2203.04212",
                    "CorpusId": 247315171
                },
                "corpusId": 247315171,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/bb1c9cb431e771660cffdda1d80a7f15ff40c764",
                "title": "Measuring the Mixing of Contextual Information in the Transformer",
                "abstract": "The Transformer architecture aggregates input information through the self-attention mechanism, but there is no clear understanding of how this information is mixed across the entire model. Additionally, recent works have demonstrated that attention weights alone are not enough to describe the flow of information. In this paper, we consider the whole attention block \u2013multi-head attention, residual connection, and layer normalization\u2013 and define a metric to measure token-to-token interactions within each layer. Then, we aggregate layer-wise interpretations to provide input attribution scores for model predictions. Experimentally, we show that our method, ALTI (Aggregation of Layer-wise Token-to-token Interactions), provides more faithful explanations and increased robustness than gradient-based methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1751450782",
                        "name": "Javier Ferrando"
                    },
                    {
                        "authorId": "2003752849",
                        "name": "Gerard I. G\u00e1llego"
                    },
                    {
                        "authorId": "1398996347",
                        "name": "M. Costa-juss\u00e0"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Considering the powerful function of the skip-connection [27], we concatenate the two outputs together and utilize the convolution with 1 kernel size to align the scale of the feature map.",
                "The derived transformer models have shown a flexible adoption in many fields [26], [27]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7ce62e7df8f0c01b961064caebc70b967c0f37c9",
                "externalIds": {
                    "DBLP": "journals/titb/ZhangYLS0V22",
                    "DOI": "10.1109/JBHI.2022.3156585",
                    "CorpusId": 247293313,
                    "PubMed": "35255000"
                },
                "corpusId": 247293313,
                "publicationVenue": {
                    "id": "eac74c9c-a5c0-417d-8088-8164a6a8bfb3",
                    "name": "IEEE journal of biomedical and health informatics",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Journal of Biomedical and Health Informatics",
                        "IEEE j biomed health informatics",
                        "IEEE J Biomed Health Informatics"
                    ],
                    "issn": "2168-2194",
                    "url": "https://ieeexplore.ieee.org/xpl/aboutJournal.jsp?punumber=6221020",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221020"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7ce62e7df8f0c01b961064caebc70b967c0f37c9",
                "title": "CMS2-Net: Semi-Supervised Sleep Staging for Diverse Obstructive Sleep Apnea Severity",
                "abstract": "Although the development of computer-aided algorithms for sleep staging is integrated into automatic detection of sleep disorders, most supervised deep learning-based models might suffer from insufficient labeled data. While the adoption of semi-supervised learning (SSL) can mitigate the issue, the SSL models are still limited to the lack of discriminative feature extraction for diverse obstructive sleep apnea (OSA) severity. This model deterioration might be exacerbated during the domain adaptation. Such exploration on the alleviation of domain-shift of SSL model between different OSA conditions has attracted more and more attentions from the clinic. In this work, a co-attention meta sleep staging network (CMS2-net) is proposed to simultaneously deal with two issues: the inter-class disparity problem and the intra-class selection problem. Within CMS2-net, a co-attention module and a triple-classifier are designed to explicitly refine the coarse feature representations by identifying the class boundary inconsistency. Moreover, the mutual information with meta contrastive variance is introduced to supervise the gradient stream from a multi-scale view. The performance of the proposed framework is demonstrated on both public and local datasets. Furthermore, our approach achieves the state-of-the-art SSL results on both datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2051661284",
                        "name": "Chuanhao Zhang"
                    },
                    {
                        "authorId": "2157977835",
                        "name": "Wenwen Yu"
                    },
                    {
                        "authorId": "2154570462",
                        "name": "Yamei Li"
                    },
                    {
                        "authorId": "2152993190",
                        "name": "Hongqiang Sun"
                    },
                    {
                        "authorId": "46867833",
                        "name": "Yuan Zhang"
                    },
                    {
                        "authorId": "2148239294",
                        "name": "M. de Vos"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Unlike original LRP and [7], where the decomposition starts from the classifier output corresponding to the target class, we have a similarity model that rather measures how similar graph embeddings of the time-snapshot graphs Gt andGt+1 are.",
                "In order to explain the output and pave the way for a better explanation of our model, we utilize the idea from [7]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "77c88030ede1a9ccc2ccce9abecd97e6838e1e9a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-01830",
                    "ArXiv": "2203.01830",
                    "PubMedCentral": "9899240",
                    "DOI": "10.1038/s41598-023-29098-7",
                    "CorpusId": 247223191,
                    "PubMed": "36739319"
                },
                "corpusId": 247223191,
                "publicationVenue": {
                    "id": "f99f77b7-b1b6-44d3-984a-f288e9884b9b",
                    "name": "Scientific Reports",
                    "type": "journal",
                    "alternate_names": [
                        "Sci Rep"
                    ],
                    "issn": "2045-2322",
                    "url": "http://www.nature.com/srep/",
                    "alternate_urls": [
                        "http://www.nature.com/srep/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/77c88030ede1a9ccc2ccce9abecd97e6838e1e9a",
                "title": "Understanding microbiome dynamics via interpretable graph representation learning",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "30606399",
                        "name": "K. Melnyk"
                    },
                    {
                        "authorId": "2051939455",
                        "name": "Kuba Weimann"
                    },
                    {
                        "authorId": "3024170",
                        "name": "T. Conrad"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "On the other hand, the transformer model enjoys higher visual interpretability by the virtue of its inherent selfattention block [22]\u2013[24]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3074f2f3a7dbaec548e00d8c2f6522059cbe729c",
                "externalIds": {
                    "ArXiv": "2202.13517",
                    "DBLP": "journals/corr/abs-2202-13517",
                    "DOI": "10.1088/1361-6560/acc000",
                    "CorpusId": 247158080,
                    "PubMed": "36854190"
                },
                "corpusId": 247158080,
                "publicationVenue": {
                    "id": "d5594aad-095f-4587-802a-b011732c7100",
                    "name": "Physics in Medicine and Biology",
                    "type": "journal",
                    "alternate_names": [
                        "Phys Med Biology"
                    ],
                    "issn": "0031-9155",
                    "url": "http://www.iop.org/EJ/journal/0031-9155",
                    "alternate_urls": [
                        "https://iopscience.iop.org/journal/0031-9155",
                        "http://iopscience.iop.org/0031-9155",
                        "http://iopscience.org/pmb"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3074f2f3a7dbaec548e00d8c2f6522059cbe729c",
                "title": "CTformer: convolution-free Token2Token dilated vision transformer for low-dose CT denoising",
                "abstract": "Objective. Low-dose computed tomography (LDCT) denoising is an important problem in CT research. Compared to the normal dose CT, LDCT images are subjected to severe noise and artifacts. Recently in many studies, vision transformers have shown superior feature representation ability over the convolutional neural networks (CNNs). However, unlike CNNs, the potential of vision transformers in LDCT denoising was little explored so far. Our paper aims to further explore the power of transformer for the LDCT denoising problem. Approach. In this paper, we propose a Convolution-free Token2Token Dilated Vision Transformer (CTformer) for LDCT denoising. The CTformer uses a more powerful token rearrangement to encompass local contextual information and thus avoids convolution. It also dilates and shifts feature maps to capture longer-range interaction. We interpret the CTformer by statically inspecting patterns of its internal attention maps and dynamically tracing the hierarchical attention flow with an explanatory graph. Furthermore, overlapped inference mechanism is employed to effectively eliminate the boundary artifacts that are common for encoder-decoder-based denoising models. Main results. Experimental results on Mayo dataset suggest that the CTformer outperforms the state-of-the-art denoising methods with a low computational overhead. Significance. The proposed model delivers excellent denoising performance on LDCT. Moreover, low computational cost and interpretability make the CTformer promising for clinical applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2111243924",
                        "name": "Dayang Wang"
                    },
                    {
                        "authorId": "47177956",
                        "name": "Fenglei Fan"
                    },
                    {
                        "authorId": "2109666517",
                        "name": "Zhan Wu"
                    },
                    {
                        "authorId": "144207288",
                        "name": "R. Liu"
                    },
                    {
                        "authorId": "39586294",
                        "name": "Fei Wang"
                    },
                    {
                        "authorId": "2148642422",
                        "name": "Hengyong Yu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The distributed representations learned by such models result in significant performance gains on a range of tasks, however come with the drawback of storing world knowledge implicitly within their parameters, making post-hoc modification [8] and interpretability [4] challenging."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "15115f67452f3305b69e6886cee98ac466d42cd5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-11233",
                    "ArXiv": "2202.11233",
                    "DOI": "10.1109/CVPR52688.2022.00683",
                    "CorpusId": 247058346
                },
                "corpusId": 247058346,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/15115f67452f3305b69e6886cee98ac466d42cd5",
                "title": "Retrieval Augmented Classification for Long-Tail Visual Recognition",
                "abstract": "We introduce Retrieval Augmented Classification (RAC), a generic approach to augmenting standard image classification pipelines with an explicit retrieval module. RAC consists of a standard base image encoder fused with a parallel retrieval branch that queries a non-parametric external memory of pre-encoded images and associated text snippets. We apply RAC to the problem of long-tail classification and demonstrate a significant improvement over previous state-of-the-art on Places365-LT and iNaturalist-2018 (14.5% and 6.7% respectively), despite using only the training datasets themselves as the external information source. We demonstrate that RAC's retrieval module, without prompting, learns a high level of accuracy on tail classes. This, in turn, frees the base encoder to focus on common classes, and improve its performance thereon. RAC represents an alternative approach to utilizing large, pretrained models without requiring fine-tuning, as well as a first step towards more effectively making use of external memory within common computer vision architectures.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2065043639",
                        "name": "Alex Long"
                    },
                    {
                        "authorId": "2006490058",
                        "name": "Wei Yin"
                    },
                    {
                        "authorId": "144722114",
                        "name": "Thalaiyasingam Ajanthan"
                    },
                    {
                        "authorId": "153072227",
                        "name": "V. Nguyen"
                    },
                    {
                        "authorId": "33305173",
                        "name": "Pulak Purkait"
                    },
                    {
                        "authorId": "144751013",
                        "name": "Ravi Garg"
                    },
                    {
                        "authorId": "2155886213",
                        "name": "Alan Blair"
                    },
                    {
                        "authorId": "1780381",
                        "name": "Chunhua Shen"
                    },
                    {
                        "authorId": "5546141",
                        "name": "A. Hengel"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "\u2018Generic Attention Explainability\u2019 (GAE) by Chefer et al. (2021a) propagates attention gradients together with gradients from other parts of the network, resulting in state-of-the art performance in explaining Transformer architectures.",
                "Other approaches to LRP / gradient propagation in Transformer blocks can be found in (Chefer et al., 2021a;b), where the relevancy scores are obtained by combining attention scores with LRP or attention gradients.",
                "New explanation techniques, such as attention rollouts (Abnar & Zuidema, 2020), or other generic ways to aggregate attention information (Chefer et al., 2021a;b) have also been developed for a similar purpose."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "577a350ade92578913245e2d474aeabcb576e6d6",
                "externalIds": {
                    "DBLP": "conf/icml/AliSEMMW22",
                    "ArXiv": "2202.07304",
                    "CorpusId": 246863594
                },
                "corpusId": 246863594,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/577a350ade92578913245e2d474aeabcb576e6d6",
                "title": "XAI for Transformers: Better Explanations through Conservative Propagation",
                "abstract": "Transformers have become an important workhorse of machine learning, with numerous applications. This necessitates the development of reliable methods for increasing their transparency. Multiple interpretability methods, often based on gradient information, have been proposed. We show that the gradient in a Transformer reflects the function only locally, and thus fails to reliably identify the contribution of input features to the prediction. We identify Attention Heads and LayerNorm as main reasons for such unreliable explanations and propose a more stable way for propagation through these layers. Our proposal, which can be seen as a proper extension of the well-established LRP method to Transformers, is shown both theoretically and empirically to overcome the deficiency of a simple gradient-based approach, and achieves state-of-the-art explanation performance on a broad range of Transformer models and datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2112239702",
                        "name": "Ameen Ali"
                    },
                    {
                        "authorId": "90387439",
                        "name": "Thomas Schnake"
                    },
                    {
                        "authorId": "1557932201",
                        "name": "Oliver Eberle"
                    },
                    {
                        "authorId": "144535526",
                        "name": "G. Montavon"
                    },
                    {
                        "authorId": "2113612432",
                        "name": "Klaus-Robert M\u00fcller"
                    },
                    {
                        "authorId": "48519520",
                        "name": "Lior Wolf"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Combining Q16 with explainable AI methods such as [Chefer et al. 2021] to explain the reasons is likely to improve the datasheet."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "69764587c8422a8f439e85bbf47b7748f880d4ed",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-06675",
                    "ArXiv": "2202.06675",
                    "DOI": "10.1145/3531146.3533192",
                    "CorpusId": 246823108
                },
                "corpusId": 246823108,
                "publicationVenue": {
                    "id": "cdc83875-a82d-445c-b097-cbe91afe99a8",
                    "name": "Conference on Fairness, Accountability and Transparency",
                    "type": "conference",
                    "alternate_names": [
                        "FAccT",
                        "Conf Fairness Account Transpar"
                    ],
                    "url": "https://facctconference.org/"
                },
                "url": "https://www.semanticscholar.org/paper/69764587c8422a8f439e85bbf47b7748f880d4ed",
                "title": "Can Machines Help Us Answering Question 16 in Datasheets, and In Turn Reflecting on Inappropriate Content?",
                "abstract": "This paper contains images and descriptions that are offensive in nature. Large datasets underlying much of current machine learning raise serious issues concerning inappropriate content such as offensive, insulting, threatening, or might otherwise cause anxiety. This calls for increased dataset documentation, e.g., using datasheets. They, among other topics, encourage to reflect on the composition of the datasets. So far, this documentation, however, is done manually and therefore can be tedious and error-prone, especially for large image datasets. Here we ask the arguably \u201ccircular\u201d question of whether a machine can help us reflect on inappropriate content, answering Question 16 in Datasheets. To this end, we propose to use the information stored in pre-trained transformer models to assist us in the documentation process. Specifically, prompt-tuning based on a dataset of socio-moral values steers CLIP to identify potentially inappropriate content, therefore reducing human labor. We then document the inappropriate images found using word clouds, based on captions generated using a vision-language model. The documentations of two popular, large-scale computer vision datasets\u2014ImageNet and OpenImages\u2014produced this way suggest that machines can indeed help dataset creators to answer Question 16 on inappropriate image content.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40896023",
                        "name": "P. Schramowski"
                    },
                    {
                        "authorId": "41127724",
                        "name": "Christopher Tauchmann"
                    },
                    {
                        "authorId": "2066493115",
                        "name": "K. Kersting"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "\u2026this, many works hence develop explanation methods by combining attention weights with additional attributions, such as the gradients (Chefer et al., 2021a; Hao et al., 2021), layer-wise relevance (Abnar & Zuidema, 2020; Chefer et al., 2021b), or the norm of\ninput vectors (Kobayashi et al., 2020).",
                "Correspondence to: Haoliang Li <haoliang.li1991@gmail.com>.\ndararajan et al., 2017; Smilkov et al., 2017; Shrikumar et al., 2017; Chefer et al., 2021a; Hao et al., 2021; Kobayashi et al., 2020).",
                "In line with this, many works hence develop explanation methods by combining attention weights with additional attributions, such as the gradients (Chefer et al., 2021a; Hao et al., 2021), layer-wise relevance (Abnar & Zuidema, 2020; Chefer et al., 2021b), or the norm of\ninput vectors (Kobayashi et\u2026",
                "\u2026we particularly adopt four explanation methods: Partial Layer-wise Relevance Propagation (PLRP) (Voita et al., 2019), Attention Rollout (Abnar & Zuidema, 2020), Transformer Attention Attribution (TransAtt) (Chefer et al., 2021b), and Generic Attention Attribution (GenAtt) (Chefer et al., 2021a).",
                "Note that in transformer-based architectures, these methods are implemented based on the last attention layer\u2019s output following (Chefer et al., 2021a)."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "5fdb05e17fd503b8dbdbadc338e0a00829929dcc",
                "externalIds": {
                    "DBLP": "conf/icml/LiuLGKL022",
                    "ArXiv": "2201.12114",
                    "CorpusId": 246411233
                },
                "corpusId": 246411233,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5fdb05e17fd503b8dbdbadc338e0a00829929dcc",
                "title": "Rethinking Attention-Model Explainability through Faithfulness Violation Test",
                "abstract": "Attention mechanisms are dominating the explainability of deep models. They produce probability distributions over the input, which are widely deemed as feature-importance indicators. However, in this paper, we find one critical limitation in attention explanations: weakness in identifying the polarity of feature impact. This would be somehow misleading -- features with higher attention weights may not faithfully contribute to model predictions; instead, they can impose suppression effects. With this finding, we reflect on the explainability of current attention-based techniques, such as Attentio$\\odot$Gradient and LRP-based attention explanations. We first propose an actionable diagnostic methodology (henceforth faithfulness violation test) to measure the consistency between explanation weights and the impact polarity. Through the extensive experiments, we then show that most tested explanation methods are unexpectedly hindered by the faithfulness violation issue, especially the raw attention. Empirical analyses on the factors affecting violation issues further provide useful observations for adopting explanation methods in attention models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "35435925",
                        "name": "Y. Liu"
                    },
                    {
                        "authorId": "144878149",
                        "name": "Haoliang Li"
                    },
                    {
                        "authorId": "30921555",
                        "name": "Yangyang Guo"
                    },
                    {
                        "authorId": "144332826",
                        "name": "Chen Kong"
                    },
                    {
                        "authorId": "2152914623",
                        "name": "Jing Li"
                    },
                    {
                        "authorId": "2144190725",
                        "name": "Shiqi Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "proposed an explainability method in which relevancy is assigned to attention maps and then propagated throughout all blocks [7]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e65c7311597a6b856e8d3bf5f2246e9cfad3c846",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-11808",
                    "ArXiv": "2201.11808",
                    "CorpusId": 246411223
                },
                "corpusId": 246411223,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e65c7311597a6b856e8d3bf5f2246e9cfad3c846",
                "title": "LAP: An Attention-Based Module for Faithful Interpretation and Knowledge Injection in Convolutional Neural Networks",
                "abstract": "Despite the state-of-the-art performance of deep convolutional neural networks, they are susceptible to bias and malfunction in unseen situations. The complex computation behind their reasoning is not sufficiently human-understandable to develop trust. External explainer methods have tried to interpret the network decisions in a human-understandable way, but they are accused of fallacies due to their assumptions and simplifications. On the other side, the inherent self-interpretability of models, while being more robust to the mentioned fallacies, cannot be applied to the already trained models. In this work, we propose a new attention-based pooling layer, called Local Attention Pooling (LAP), that accomplishes self-interpretability and the possibility for knowledge injection while improving the model's performance. Moreover, several weakly-supervised knowledge injection methodologies are provided to enhance the process of training. We verified our claims by evaluating several LAP-extended models on three different datasets, including Imagenet. The proposed framework offers more valid human-understandable and more faithful-to-the-model interpretations than the commonly used white-box explainer methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2123058871",
                        "name": "Rassa Ghavami Modegh"
                    },
                    {
                        "authorId": "1396417490",
                        "name": "Ahmadali Salimi"
                    },
                    {
                        "authorId": "1688652",
                        "name": "H. Rabiee"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "24aa57dae649b6683d8f5bc8deaf2ff549cdacc4",
                "externalIds": {
                    "ArXiv": "2201.09873",
                    "DBLP": "journals/mia/ShamshadKZKHKF23",
                    "DOI": "10.1016/j.media.2023.102802",
                    "CorpusId": 246240729,
                    "PubMed": "37315483"
                },
                "corpusId": 246240729,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/24aa57dae649b6683d8f5bc8deaf2ff549cdacc4",
                "title": "Transformers in Medical Imaging: A Survey",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "29434078",
                        "name": "Fahad Shamshad"
                    },
                    {
                        "authorId": "152973423",
                        "name": "Salman Hameed Khan"
                    },
                    {
                        "authorId": "3323621",
                        "name": "Syed Waqas Zamir"
                    },
                    {
                        "authorId": "2115774820",
                        "name": "Muhammad Haris Khan"
                    },
                    {
                        "authorId": "145684318",
                        "name": "Munawar Hayat"
                    },
                    {
                        "authorId": "2358803",
                        "name": "F. Khan"
                    },
                    {
                        "authorId": "1929093",
                        "name": "H. Fu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Few works have tried to interpret Transformers further than this for vision [207], and so far within the literature of VTs we only find a limited subset of works that visualize these attention activations for specific samples [66], [69], [99], [130]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3e906906d475c73b6d8ce24ac5ebdac9979fd01b",
                "externalIds": {
                    "DBLP": "journals/pami/SelvaJENMC23",
                    "ArXiv": "2201.05991",
                    "DOI": "10.1109/TPAMI.2023.3243465",
                    "CorpusId": 246015436,
                    "PubMed": "37022830"
                },
                "corpusId": 246015436,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3e906906d475c73b6d8ce24ac5ebdac9979fd01b",
                "title": "Video Transformers: A Survey",
                "abstract": "Transformer models have shown great success handling long-range interactions, making them a promising tool for modeling video. However, they lack inductive biases and scale quadratically with input length. These limitations are further exacerbated when dealing with the high dimensionality introduced by the temporal dimension. While there are surveys analyzing the advances of Transformers for vision, none focus on an in-depth analysis of video-specific designs. In this survey, we analyze the main contributions and trends of works leveraging Transformers to model video. Specifically, we delve into how videos are handled at the input level first. Then, we study the architectural changes made to deal with video more efficiently, reduce redundancy, re-introduce useful inductive biases, and capture long-term temporal dynamics. In addition, we provide an overview of different training regimes and explore effective self-supervised learning strategies for video. Finally, we conduct a performance comparison on the most common benchmark for Video Transformers (i.e., action classification), finding them to outperform 3D ConvNets even with less computational complexity.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2073004411",
                        "name": "Javier Selva"
                    },
                    {
                        "authorId": "2075449066",
                        "name": "A. S. Johansen"
                    },
                    {
                        "authorId": "7855312",
                        "name": "Sergio Escalera"
                    },
                    {
                        "authorId": "2143163793",
                        "name": "Kamal Nasrollahi"
                    },
                    {
                        "authorId": "1700569",
                        "name": "T. Moeslund"
                    },
                    {
                        "authorId": "2150345448",
                        "name": "Albert Clap'es"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "As stated in [76], using solely the attention map to explain the model\u2019s reasoning is naive, since there are a lot more layers and processes in a model that contribute to its decisions."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5cfed5ba84dd91ecc5fa9215f30ecc551632ba9a",
                "externalIds": {
                    "ArXiv": "2201.03016",
                    "DBLP": "journals/tgrs/BountosMP22",
                    "DOI": "10.1109/TGRS.2022.3180891",
                    "CorpusId": 249532574
                },
                "corpusId": 249532574,
                "publicationVenue": {
                    "id": "70628d6a-97aa-4571-9701-bc0eb3989c32",
                    "name": "IEEE Transactions on Geoscience and Remote Sensing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Geosci Remote Sens"
                    ],
                    "issn": "0196-2892",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=36",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5cfed5ba84dd91ecc5fa9215f30ecc551632ba9a",
                "title": "Learning from Synthetic InSAR with Vision Transformers: The case of volcanic unrest detection",
                "abstract": "The detection of early signs of volcanic unrest preceding an eruption, in the form of ground deformation in Interferometric Synthetic Aperture Radar (InSAR) data is critical for assessing volcanic hazard. In this work we treat this as a binary classification problem of InSAR images, and propose a novel deep learning methodology that exploits a rich source of synthetically generated interferograms to train quality classifiers that perform equally well in real interferograms. The imbalanced nature of the problem, with orders of magnitude fewer positive samples, coupled with the lack of a curated database with labeled InSAR data, sets a challenging task for conventional deep learning architectures. We propose a new framework for domain adaptation, in which we learn class prototypes from synthetic data with vision transformers. We report detection accuracy that amounts to the highest reported accuracy on a large test set for volcanic unrest detection. Moreover, we built upon this knowledge by learning a new, non-linear, projection between the learnt representations and prototype space, using pseudo labels produced by our model from an unlabeled real InSAR dataset. This leads to the new state of the art with 97.1% accuracy on our test set. We demonstrate the robustness of our approach by training a simple ResNet-18 Convolutional Neural Network on the unlabeled real InSAR dataset with pseudo-labels generated from our top transformer-prototype model. Our methodology provides a significant improvement in performance without the need of manually labeling any sample, opening the road for further exploitation of synthetic InSAR data in various remote sensing applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2007384465",
                        "name": "N. Bountos"
                    },
                    {
                        "authorId": "145741802",
                        "name": "D. Michail"
                    },
                    {
                        "authorId": "49032213",
                        "name": "I. Papoutsis"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "We adopt the method presented in [4] which employs Deep Taylor Decomposition to calculate local relevance and then propagates these relevancy scores through the layers to generate a final relevancy map.",
                "Class wise visualisation of ImageNet-1K images with method presented in [4]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5ab70d95ca49702a3dd49b39d9396d8136b52311",
                "externalIds": {
                    "DBLP": "conf/cvpr/ChavanS0LCX22",
                    "ArXiv": "2201.00814",
                    "DOI": "10.1109/CVPR52688.2022.00488",
                    "CorpusId": 245650313
                },
                "corpusId": 245650313,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5ab70d95ca49702a3dd49b39d9396d8136b52311",
                "title": "Vision Transformer Slimming: Multi-Dimension Searching in Continuous Optimization Space",
                "abstract": "This paper explores the feasibility of finding an optimal sub-model from a vision transformer and introduces a pure vision transformer slimming (ViT-Slim) framework. It can search a sub-structure from the original model end-to-end across multiple dimensions, including the input tokens, MHSA and MLP modules with state-of-the-art performance. Our method is based on a learnable and unified \u21131sparsity constraint with pre-defined factors to reflect the global importance in the continuous searching space of different dimensions. The searching process is highly efficient through a single-shot training scheme. For instance, on DeiT-S, ViT-Slim only takes ~43 GPU hours for the searching process, and the searched structure is flexible with diverse dimensionalities in different modules. Then, a budget threshold is employed according to the requirements of accuracy-FLOPs trade-off on running devices, and a retraining process is performed to obtain the final model. The extensive experiments show that our ViT-Slim can compress up to 40% of parameters and 40% FLOPs on various vision transformers while increasing the accuracy by ~0.6% on ImageNet. We also demonstrate the advantage of our searched models on several downstream datasets. Our code is available at https://github.com/Arnav0400/ViT-Slim.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1585915155",
                        "name": "Arnav Chavan"
                    },
                    {
                        "authorId": "145314568",
                        "name": "Zhiqiang Shen"
                    },
                    {
                        "authorId": "2109168016",
                        "name": "Zhuang Liu"
                    },
                    {
                        "authorId": "2109370860",
                        "name": "Zechun Liu"
                    },
                    {
                        "authorId": "145210800",
                        "name": "Kwang-Ting Cheng"
                    },
                    {
                        "authorId": "2064963077",
                        "name": "Eric P. Xing"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Previous work has used Rollout to produce outputagnostic visualizations of the attention weights [14, 8].",
                "lowed by methods focused on networks trained for classification [8, 7].",
                "used as the foundation for output-specific visualizations for Transformer networks used for classification [8, 7]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ce58bc73d777f871fdfbd0c97df7ae6158c73ea6",
                "externalIds": {
                    "DBLP": "conf/wacv/BlackSPS22",
                    "DOI": "10.1109/WACV51458.2022.00160",
                    "CorpusId": 246868774
                },
                "corpusId": 246868774,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/ce58bc73d777f871fdfbd0c97df7ae6158c73ea6",
                "title": "Visualizing Paired Image Similarity in Transformer Networks",
                "abstract": "Transformer architectures have shown promise for a wide range of computer vision tasks, including image embedding. As was the case with convolutional neural networks and other models, explainability of the predictions is a key concern, but visualization approaches tend to be architecture-specific. In this paper, we introduce a new method for producing interpretable visualizations that, given a pair of images encoded with a Transformer, show which regions contributed to their similarity. Additionally, for the task of image retrieval, we compare the performance of Transformer and ResNet models of similar capacity and show that while they have similar performance in aggregate, the retrieved results and the visual explanations for those results are quite different. Code is available at https://github.com/vidarlab/xformer-paired-viz.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1452228516",
                        "name": "Samuel Black"
                    },
                    {
                        "authorId": "15017879",
                        "name": "Abby Stylianou"
                    },
                    {
                        "authorId": "143857761",
                        "name": "Robert Pless"
                    },
                    {
                        "authorId": "1690110",
                        "name": "Richard Souvenir"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Take Transformers [26] for example, a naive approach of averaging the attention layer weights for each token would lead to the dilution of signal and would not consider the interaction between layers [27]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "12754143d4ab3c8f52bbb02f2b3cbced0851db7f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-14928",
                    "ArXiv": "2112.14928",
                    "DOI": "10.2139/ssrn.4084594",
                    "CorpusId": 245634260
                },
                "corpusId": 245634260,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/12754143d4ab3c8f52bbb02f2b3cbced0851db7f",
                "title": "An empirical user-study of text-based nonverbal annotation systems for human-human conversations",
                "abstract": "the substantial increase in the number of online human-human conversations and the usefulness of multimodal transcripts, there is a rising need for automated multimodal transcription systems to help us better understand the conversations. In this paper, we evaluated three methods to perform multimodal transcription. They were (1) Jefferson -- an existing manual system used widely by the linguistics community, (2) MONAH -- a system that aimed to make multimodal transcripts accessible and automated, (3) MONAH+ -- a system that builds on MONAH that visualizes machine attention. Based on 104 participants responses, we found that (1) all text-based methods significantly reduced the amount of information for the human users, (2) MONAH was found to be more usable than Jefferson, (3) Jefferson's relative strength was in chronemics (pace / delay) and paralinguistics (pitch / volume) annotations, whilst MONAH's relative strength was in kinesics (body language) annotations, (4) enlarging words' font-size based on machine attention was confusing human users as loudness. These results pose considerations for researchers designing a multimodal annotation system for the masses who would like a fully-automated or human-augmented conversational analysis system.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47964485",
                        "name": "Joshua Y. Kim"
                    },
                    {
                        "authorId": "1763220",
                        "name": "K. Yacef"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The token representations [84,87,10,5] in early and middle layers are insufficiently encoded, which makes token pruning quite difficult."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "722d71a19e4049b30a03d1028158881560432135",
                "externalIds": {
                    "DBLP": "conf/eccv/KongDMMNSSYRTQW22",
                    "ArXiv": "2112.13890",
                    "DOI": "10.1007/978-3-031-20083-0_37",
                    "CorpusId": 245537400
                },
                "corpusId": 245537400,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/722d71a19e4049b30a03d1028158881560432135",
                "title": "SPViT: Enabling Faster Vision Transformers via Latency-Aware Soft Token Pruning",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "32409528",
                        "name": "Zhenglun Kong"
                    },
                    {
                        "authorId": "2052289825",
                        "name": "Peiyan Dong"
                    },
                    {
                        "authorId": "151480882",
                        "name": "Xiaolong Ma"
                    },
                    {
                        "authorId": "2148338860",
                        "name": "Xin Meng"
                    },
                    {
                        "authorId": "48643324",
                        "name": "Wei Niu"
                    },
                    {
                        "authorId": null,
                        "name": "Mengshu Sun"
                    },
                    {
                        "authorId": "2042633100",
                        "name": "Bin Ren"
                    },
                    {
                        "authorId": "39449475",
                        "name": "Minghai Qin"
                    },
                    {
                        "authorId": "2112388851",
                        "name": "H. Tang"
                    },
                    {
                        "authorId": "46393431",
                        "name": "Yanzhi Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "However, unlike our mechanism, selfattention layers do not distinguish between classes on the same image without additional steps [10]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c78ffe94ad3d5b41595ab6474a924c429f1420a6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-13692",
                    "ArXiv": "2112.13692",
                    "CorpusId": 245502238
                },
                "corpusId": 245502238,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c78ffe94ad3d5b41595ab6474a924c429f1420a6",
                "title": "Augmenting Convolutional networks with attention-based aggregation",
                "abstract": "We show how to augment any convolutional network with an attention-based global map to achieve non-local reasoning. We replace the final average pooling by an attention-based aggregation layer akin to a single transformer block, that weights how the patches are involved in the classification decision. We plug this learned aggregation layer with a simplistic patch-based convolutional network parametrized by 2 parameters (width and depth). In contrast with a pyramidal design, this architecture family maintains the input patch resolution across all the layers. It yields surprisingly competitive trade-offs between accuracy and complexity, in particular in terms of memory consumption, as shown by our experiments on various computer vision tasks: object classification, image segmentation and detection.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2146374461",
                        "name": "Hugo Touvron"
                    },
                    {
                        "authorId": "51021910",
                        "name": "M. Cord"
                    },
                    {
                        "authorId": "1388811741",
                        "name": "Alaaeldin El-Nouby"
                    },
                    {
                        "authorId": "2329288",
                        "name": "Piotr Bojanowski"
                    },
                    {
                        "authorId": "2319608",
                        "name": "Armand Joulin"
                    },
                    {
                        "authorId": "2282478",
                        "name": "Gabriel Synnaeve"
                    },
                    {
                        "authorId": "2065248680",
                        "name": "Herv'e J'egou"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Techniques have been proposed [3, 16], but they enable the analysis of influential tokens from the input text, rather than words, or they display multiple attention weights relative to the different layers and attention mechanisms inside the model.",
                "We contribute to the field by building on the method in [3] to associate consolidated attention weights to words containing the tokens and extending this analysis to the whole test set so that one can leverage this knowledge to assess the behavior of a stance classifier in terms of domain characteristics.",
                "To get the attention weights for each token, we use the modified LRP technique proposed in [3].",
                "The technique proposed in [3] leverages LRP (Layer-wise Relevance Propagation) to overcome this limitation and summarizes the attention weights using information related to both the relevance and gradient.",
                "The attention weights are obtained using the adapted LRP technique proposed in [3] from the results obtained by the classifier.",
                "The interpretability mechanism expands the attention weights consolidation proposed in [3] in two ways: a) relating tokens to their original words in each instance; and b) proposing an attention score that is significant with regard to a documents set, referred",
                "It extends the technique of [3], which consolidates at instance level the attention weights of the relevant tokens throughout the whole network."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "1e65d1a622ce8cb30ad4ad2bafc20a4bd9ba5892",
                "externalIds": {
                    "DBLP": "conf/webi/SaenzB21",
                    "DOI": "10.1145/3486622.3493966",
                    "CorpusId": 248151012
                },
                "corpusId": 248151012,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1e65d1a622ce8cb30ad4ad2bafc20a4bd9ba5892",
                "title": "Assessing the use of attention weights to interpret BERT-based stance classification",
                "abstract": "BERT models are currently state-of-the-art solutions for various tasks, including stance classification. However, these models are a black box for their users. Some proposals have leveraged the weights assigned by the internal attention mechanisms of these models for interpretability purposes. However, whether the attention weights help the interpretability of the model is still a matter of debate, with positions in favor and against. This work proposes an attention-based interpretability mechanism to identify the most influential words for stances predicted using BERT-based models. We target stances expressed in Twitter using the Portuguese language and assess the proposed mechanism using a case study regarding stances on COVID-19 vaccination in the Brazilian context. The interpretation mechanism traces tokens\u2019 attentions back to words, assigning a newly proposed metric referred to as absolute word attention. Through this metric, we assess several aspects to determine if we can find important words for the classification and with meaning for the domain. We developed a broad experimental setting that involved three datasets with tweets in Brazilian Portuguese and three BERT models with support for this language. Our results are encouraging, as we were able to identify 52-82% of words with high absolute attention contributing positively to stance classification. The interpretability mechanism proved to be helpful to understand the influence of words in the classification, and they revealed intrinsic properties of the domain and representative arguments of the stances.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2092671419",
                        "name": "Carlos Abel C\u00f3rdova S\u00e1enz"
                    },
                    {
                        "authorId": "49629811",
                        "name": "Karin Becker"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Averaging over the models, we\u2019d achieve 59.74% accuracy with the BM-GAE model and 58.33% using the TRF method compared with 51.04% average accuracy amongst all E-BERT injection models as seen in Table 3.",
                "We see that for 7 of the 9 models, questions which include E-BERT entities amongst their top 5 using BM-GAE provide better accuracy than those using the TRF method.",
                "The method uses the model\u2019s attention layers to produce relevancy maps for each of the interactions between the input modalities in the network and is a generalization of TRF [4] without Layer-wise Relevance Propagation [1] which itself was shown to be effective on single-modality Transformers that utilize self-attention such as VilBERT[14].",
                "TRF EXPLANATION\nNERagro\nNERper\nKVQAmeta\nOKVQA, both without the need to redo any costly pre-training.",
                "[421]:\nKVQAmeta\nTRF EXPLANATION\nNERagro\nNERper\nentity Fisher Morgan10 is in fact both a singer and actor, the later which KVQAmeta is actually the only model to predict correctly.",
                "Over all models E-BERT entities appear in the top 5 most important tokens using TRF more than BM-GAE (10.35 vs 8.59",
                "We extract visual and text explanations using BM-GAE and TRF on our KVQA models."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "43250eafc113dd90fd24d1371b43e2fe1e53b868",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-06888",
                    "ArXiv": "2112.06888",
                    "DOI": "10.1145/3487553.3524648",
                    "CorpusId": 245124118
                },
                "corpusId": 245124118,
                "publicationVenue": {
                    "id": "e07422f9-c065-40c3-a37b-75e98dce79fe",
                    "name": "The Web Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Web Conf",
                        "WWW"
                    ],
                    "url": "http://www.iw3c2.org/"
                },
                "url": "https://www.semanticscholar.org/paper/43250eafc113dd90fd24d1371b43e2fe1e53b868",
                "title": "Improving and Diagnosing Knowledge-Based Visual Question Answering via Entity Enhanced Knowledge Injection",
                "abstract": "Knowledge-Based Visual Question Answering (KBVQA) is a bi-modal task requiring external world knowledge in order to correctly answer a text question and associated image. Recent single modality text work has shown knowledge injection into pre-trained language models, specifically entity enhanced knowledge graph embeddings, can improve performance on downstream entity-centric tasks. In this work, we empirically study how and whether such methods, applied in a bi-modal setting, can improve an existing VQA system\u2019s performance on the KBVQA task. We experiment with two large publicly available VQA datasets, (1) KVQA which contains mostly rare Wikipedia entities and (2) OKVQA which is less entity-centric and more aligned with common sense reasoning. Both lack explicit entity spans, and we study the effect of different weakly supervised and manual methods for obtaining them. Additionally, we analyze how recently proposed bi-modal and single modal attention explanations are affected by the incorporation of such entity enhanced representations. Our results show substantially improved performance on the KBVQA task without the need for additional costly pre-training, and we provide insights for when entity knowledge injection helps improve a model\u2019s understanding. We provide code and enhanced datasets for reproducibility1.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1405390891",
                        "name": "Diego Garcia-Olano"
                    },
                    {
                        "authorId": "115412405",
                        "name": "Yasumasa Onoe"
                    },
                    {
                        "authorId": "1379535322",
                        "name": "J. Ghosh"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4799ea215e65e66a9a170303861763662d04e8b1",
                "externalIds": {
                    "DBLP": "conf/aaai/GatLSH22",
                    "ArXiv": "2112.04895",
                    "DOI": "10.1609/aaai.v36i1.19948",
                    "CorpusId": 245005847
                },
                "corpusId": 245005847,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/4799ea215e65e66a9a170303861763662d04e8b1",
                "title": "Latent Space Explanation by Intervention",
                "abstract": "The success of deep neural nets heavily relies on their ability to encode complex relations between their input and their output. While this property serves to fit the training data well, it also obscures the mechanism that drives prediction. This study aims to reveal hidden concepts by employing an intervention mechanism that shifts the predicted class based on discrete variational autoencoders. An explanatory model then visualizes the encoded information from any hidden layer and its corresponding intervened representation. By the assessment of differences between the original representation and the intervened representation, one can determine the concepts that can alter the class, hence providing interpretability. We demonstrate the effectiveness of our approach on CelebA, where we show various visualizations for bias in the data and suggest different interventions to reveal and change bias.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2064713742",
                        "name": "Itai Gat"
                    },
                    {
                        "authorId": "50976892",
                        "name": "Guy Lorberbom"
                    },
                    {
                        "authorId": "38211837",
                        "name": "Idan Schwartz"
                    },
                    {
                        "authorId": "1918412",
                        "name": "Tamir Hazan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "FOR METHODS THAT PURSUE OPTIMAL CLASSIFICATION PERFORMANCE WITHOUT A SPECIFIED ATTENTION VISUALISATION MECHANISM, WE RESORT TO GRADCAM [68] AND TRANSRELEVANCE [10] AS TWO GENERIC WAYS FOR CNN AND TRANSFORMER TYPE OF MODELS, WHICH COVERED MOST NETWORK BACKBONE CHOICES ACROSS DISCIPLINES IN COMPUTER VISION WORLD NOWADAYS.",
                "For works that do not specify a concrete model visualisation approach, we adopt GradCAM [68] and TransRelevance [10] as two generic ways for CNN and Transformer type of models respectively."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "43365435cdc6c9382f42ab8efadfd4c14119acd3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-02747",
                    "ArXiv": "2112.02747",
                    "DOI": "10.1109/TPAMI.2023.3274593",
                    "CorpusId": 244908946,
                    "PubMed": "37159309"
                },
                "corpusId": 244908946,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/43365435cdc6c9382f42ab8efadfd4c14119acd3",
                "title": "Making a Bird AI Expert Work for You and Me",
                "abstract": "As powerful as fine-grained visual classification (FGVC) is, responding your query with a bird name of \u201cWhip-poor-will\u201d or \u201cMallard\u201d probably does not make much sense. This however commonly accepted in the literature, underlines a fundamental question interfacing AI and human \u2013 what constitutes transferable knowledge for human to learn from AI? This paper sets out to answer this very question using FGVC as a test bed. Specifically, we envisage a scenario where a trained FGVC model (the AI expert) functions as a knowledge provider in enabling average people (you and me) to become better domain experts ourselves. Assuming an AI expert trained using expert human labels, we anchor our focus on asking and providing solutions for two questions: (i) what is the best transferable knowledge we can extract from AI, and (ii) what is the most practical means to measure the gains in expertise given that knowledge? We propose to represent knowledge as highly discriminative visual regions that are expert-exclusive and instantiate it via a novel multi-stage learning framework. A human study of 15,000 trials shows our method is able to consistently improve people of divergent bird expertise to recognise once unrecognisable birds. We further propose a crude but benchmarkable metric TEMI and therefore allow future efforts in this direction to be comparable to ours without the need of large-scale human studies.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "67146777",
                        "name": "Dongliang Chang"
                    },
                    {
                        "authorId": "3407337",
                        "name": "Kaiyue Pang"
                    },
                    {
                        "authorId": "1557389697",
                        "name": "Ruoyi Du"
                    },
                    {
                        "authorId": "1755773",
                        "name": "Zhanyu Ma"
                    },
                    {
                        "authorId": "1705408",
                        "name": "Yi-Zhe Song"
                    },
                    {
                        "authorId": "2117222700",
                        "name": "Jun Guo"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "TIBS is essentially a thresholded interpretability map, which despite its knowledge distillation from CLIP, generated relatively noisy segmentations.",
                "Transformer Interpretability Based Segmentation (TIBS) [6] .",
                "The mapping between the text prompts to per-category relevance maps is performed by utilizing a recently developed transformer interpretation method [6].",
                "The trends are similar to those of PASCAL VOC, our method improves over TIBS."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "d6fb5948be7b89d71620f25047c193a55d0d77d5",
                "externalIds": {
                    "ArXiv": "2112.03185",
                    "DBLP": "journals/corr/abs-2112-03185",
                    "CorpusId": 244908512
                },
                "corpusId": 244908512,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d6fb5948be7b89d71620f25047c193a55d0d77d5",
                "title": "Semantic Segmentation In-the-Wild Without Seeing Any Segmentation Examples",
                "abstract": "Semantic segmentation is a key computer vision task that has been actively researched for decades. In recent years, supervised methods have reached unprecedented accuracy, however they require many pixel-level annotations for every new class category which is very time-consuming and expensive. Additionally, the ability of current semantic segmentation networks to handle a large number of categories is limited. That means that images containing rare class categories are unlikely to be well segmented by current methods. In this paper we propose a novel approach for creating semantic segmentation masks for every object, without the need for training segmentation networks or seeing any segmentation masks. Our method takes as input the image-level labels of the class categories present in the image; they can be obtained automatically or manually. We utilize a vision-language embedding model (specifically CLIP) to create a rough segmentation map for each class, using model interpretability methods. We refine the maps using a test-time augmentation technique. The output of this stage provides pixel-level pseudo-labels, instead of the manual pixel-level labels required by supervised methods. Given the pseudo-labels, we utilize single-image segmentation techniques to obtain high-quality output segmentation masks. Our method is shown quantitatively and qualitatively to outperform methods that use a similar amount of supervision. Our results are particularly remarkable for images containing rare categories.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1630345313",
                        "name": "Nir Zabari"
                    },
                    {
                        "authorId": "2776254",
                        "name": "Yedid Hoshen"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d3cf012f6b7a8b1e11914a7ce774a0ccd2999607",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-02841",
                    "ArXiv": "2112.02841",
                    "CorpusId": 244908580
                },
                "corpusId": 244908580,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d3cf012f6b7a8b1e11914a7ce774a0ccd2999607",
                "title": "GETAM: Gradient-weighted Element-wise Transformer Attention Map for Weakly-supervised Semantic segmentation",
                "abstract": "Weakly Supervised Semantic Segmentation (WSSS) is challenging, particularly when image-level labels are used to supervise pixel level prediction. To bridge their gap, a Class Activation Map (CAM) is usually generated to provide pixel level pseudo labels. CAMs in Convolutional Neural Networks suffer from partial activation ie, only the most discriminative regions are activated. Transformer based methods, on the other hand, are highly effective at exploring global context with long range dependency modeling, potentially alleviating the\"partial activation\"issue. In this paper, we propose the first transformer based WSSS approach, and introduce the Gradient weighted Element wise Transformer Attention Map (GETAM). GETAM shows fine scale activation for all feature map elements, revealing different parts of the object across transformer layers. Further, we propose an activation aware label completion module to generate high quality pseudo labels. Finally, we incorporate our methods into an end to end framework for WSSS using double backward propagation. Extensive experiments on PASCAL VOC and COCO demonstrate that our results beat the state-of-the-art end-to-end approaches by a significant margin, and outperform most multi-stage methods.m most multi-stage methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3466875",
                        "name": "Weixuan Sun"
                    },
                    {
                        "authorId": "2155698491",
                        "name": "Jing Zhang"
                    },
                    {
                        "authorId": "46271490",
                        "name": "Zheyuan Liu"
                    },
                    {
                        "authorId": "2015152",
                        "name": "Yiran Zhong"
                    },
                    {
                        "authorId": "1712576",
                        "name": "N. Barnes"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "To be more specific, inspired by [5], we first obtain the class token xcls i and patch token x patch i of the whole image Ii by ViT encoder:"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5a4246ebd0d0bf3b080e7102eb3ec3a74462ea51",
                "externalIds": {
                    "ArXiv": "2112.01515",
                    "DBLP": "journals/corr/abs-2112-01515",
                    "DOI": "10.1007/978-3-031-19818-2_5",
                    "CorpusId": 244798653
                },
                "corpusId": 244798653,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/5a4246ebd0d0bf3b080e7102eb3ec3a74462ea51",
                "title": "TransFGU: A Top-down Approach to Fine-Grained Unsupervised Semantic Segmentation",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1693997019",
                        "name": "Zhao-Heng Yin"
                    },
                    {
                        "authorId": "8120382",
                        "name": "Pichao Wang"
                    },
                    {
                        "authorId": "2145903446",
                        "name": "Fan Wang"
                    },
                    {
                        "authorId": "153030252",
                        "name": "Xianzhe Xu"
                    },
                    {
                        "authorId": "2111475",
                        "name": "Hanling Zhang"
                    },
                    {
                        "authorId": "1706574",
                        "name": "Hao Li"
                    },
                    {
                        "authorId": "2146543534",
                        "name": "Rong Jin"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "77d96d55ec9effcd33517a46e60ed6fb3081ba10",
                "externalIds": {
                    "DOI": "10.1016/j.cose.2021.102574",
                    "CorpusId": 245026394
                },
                "corpusId": 245026394,
                "publicationVenue": {
                    "id": "d56ebe2f-b08f-4b5c-ae3a-63144701ed04",
                    "name": "Computers & security",
                    "type": "journal",
                    "alternate_names": [
                        "Comput  Secur",
                        "Computers & Security",
                        "Comput  secur"
                    ],
                    "issn": "0167-4048",
                    "url": "http://www.sciencedirect.com/science/journal/01674048"
                },
                "url": "https://www.semanticscholar.org/paper/77d96d55ec9effcd33517a46e60ed6fb3081ba10",
                "title": "A privacy preservation method for multiple-source unstructured data in online social networks",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108755860",
                        "name": "Chenguang Wang"
                    },
                    {
                        "authorId": "49769821",
                        "name": "Zhu Tianqing"
                    },
                    {
                        "authorId": "46277805",
                        "name": "P. Xiong"
                    },
                    {
                        "authorId": "2053309101",
                        "name": "Wei Ren"
                    },
                    {
                        "authorId": "2840539",
                        "name": "Kim-Kwang Raymond Choo"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "By visualizing attention maps of class activation based on method [4], in Fig."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fc2e7fcdc1bd773f1eb097ae67c8f736108276e3",
                "externalIds": {
                    "DBLP": "conf/cvpr/0003JLBJ022",
                    "ArXiv": "2111.12994",
                    "DOI": "10.1109/CVPR52688.2022.01176",
                    "CorpusId": 244709229
                },
                "corpusId": 244709229,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fc2e7fcdc1bd773f1eb097ae67c8f736108276e3",
                "title": "NomMer: Nominate Synergistic Context in Vision Transformer for Visual Recognition",
                "abstract": "Recently, Vision Transformers (ViT), with the self-attention (SA) as the de facto ingredients, have demon-strated great potential in the computer vision community. For the sake of trade-off between efficiency and performance, a group of works merely perform SA operation within local patches, whereas the global contextual information is abandoned, which would be indispensable for visual recognition tasks. To solve the issue, the subsequent global-local ViTs take a stab at marrying local SA with global one in parallel or alternative way in the model. Nevertheless, the exhaustively combined local and global context may exist redundancy for various visual data, and the receptive field within each layer is fixed. Alternatively, a more graceful way is that global and local context can adaptively contribute per se to accommodate different visual data. To achieve this goal, we in this paper propose a novel ViT architecture, termed NomMer, which can dynamically Nominate the synergistic global-local context in vision transforMer. By investigating the working pattern of NomMer, we further explore what context information is focused. Beneficial from this \u201cdynamic nomination\u201d mechanism, without bells and whistles, the NomMer can not only achieve 84.5% Top-1 classification accuracy on ImageNet with only 73M parameters, but also show promising performance on dense prediction tasks, i.e., object detection and semantic segmentation. The code and models are publicly available at https://github.com/TencentYoutuResearch/VisualRecognition-NomMer.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48446712",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "2110312091",
                        "name": "Xinghua Jiang"
                    },
                    {
                        "authorId": "2153898418",
                        "name": "Xin Li"
                    },
                    {
                        "authorId": "2111842238",
                        "name": "Zhimin Bao"
                    },
                    {
                        "authorId": "3336726",
                        "name": "Deqiang Jiang"
                    },
                    {
                        "authorId": "2064646914",
                        "name": "Bo Ren"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Following [1, 5], rollout applies matrix multiplication across all 12 blocks\u2019 attention matrices.",
                "Then we follow [1, 5] to compute the attention rollout, which aggregate the attention matrices from all blocks by matrix multiplications."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b39495876b494412e0918898db8f988e9f5fd69d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-09833",
                    "ArXiv": "2111.09833",
                    "DOI": "10.1109/CVPR52688.2022.01182",
                    "CorpusId": 244346829
                },
                "corpusId": 244346829,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b39495876b494412e0918898db8f988e9f5fd69d",
                "title": "TransMix: Attend to Mix for Vision Transformers",
                "abstract": "Mixup-based augmentation has been found to be effective for generalizing models during training, especially for Vision Transformers (ViTs) since they can easily overfit. However, previous mixup-based methods have an underlying prior knowledge that the linearly interpolated ratio of targets should be kept the same as the ratio proposed in input interpolation. This may lead to a strange phenomenon that sometimes there is no valid object in the mixed image due to the random process in augmentation but there is still response in the label space. To bridge such gap between the input and label spaces, we propose TransMix, which mixes labels based on the attention maps of Vision Transformers. The confidence of the label will be larger if the corresponding input image is weighted higher by the attention map. TransMix is embarrassingly simple and can be implemented in just a few lines of code without introducing any extra parameters and FLOPs to ViT-based models. Experimental results show that our method can consistently improve various ViT-based models at scales on ImageNet classification. After pre-trained with TransMix on ImageNet, the ViT-based models also demonstrate better transferability to semantic segmentation, object detection and instance segmentation. TransMix also exhibits to be more robust when evaluating on 4 different benchmarks. Code is publicly available at https://github.com/Beckschen/TransMix.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "90972805",
                        "name": "Jieneng Chen"
                    },
                    {
                        "authorId": "2115306116",
                        "name": "Shuyang Sun"
                    },
                    {
                        "authorId": "153146760",
                        "name": "Ju He"
                    },
                    {
                        "authorId": "143635540",
                        "name": "Philip H. S. Torr"
                    },
                    {
                        "authorId": "145081362",
                        "name": "A. Yuille"
                    },
                    {
                        "authorId": "47651566",
                        "name": "S. Bai"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "Input saliency Bastings & Filippova (2020) and attribution-propagation Chefer et al. (2020) methods have also been studied as potential tools for model interpretability."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1da81224d2a781b88186d81872755535e82fce5c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-15253",
                    "ArXiv": "2110.15253",
                    "CorpusId": 240070481
                },
                "corpusId": 240070481,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1da81224d2a781b88186d81872755535e82fce5c",
                "title": "Understanding How Encoder-Decoder Architectures Attend",
                "abstract": "Encoder-decoder networks with attention have proven to be a powerful way to solve many sequence-to-sequence tasks. In these networks, attention aligns encoder and decoder states and is often used for visualizing network behavior. However, the mechanisms used by networks to generate appropriate attention matrices are still mysterious. Moreover, how these mechanisms vary depending on the particular architecture used for the encoder and decoder (recurrent, feed-forward, etc.) are also not well understood. In this work, we investigate how encoder-decoder networks solve different sequence-to-sequence tasks. We introduce a way of decomposing hidden states over a sequence into temporal (independent of input) and input-driven (independent of sequence position) components. This reveals how attention matrices are formed: depending on the task requirements, networks rely more heavily on either the temporal or input-driven components. These findings hold across both recurrent and feed-forward architectures despite their differences in forming the temporal components. Overall, our results provide new insight into the inner workings of attention-based encoder-decoder networks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "153088347",
                        "name": "Kyle Aitken"
                    },
                    {
                        "authorId": "96641652",
                        "name": "V. Ramasesh"
                    },
                    {
                        "authorId": "145144022",
                        "name": "Yuan Cao"
                    },
                    {
                        "authorId": "2333223",
                        "name": "Niru Maheswaranathan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "The transformer encoders are arranged after a convolution feature\nhttps://github.com/hila-chefer/Transformer-Explainability\nextractor.",
                "Left to right: input image, rollout [62], raw-attention, GradCAM [72], LRP [73], partial LRP [63], and Transformer-Explainability [23].",
                "We also briefly describe recent developments in visualizing feature maps of ViT models [23, 62, 63], which help to better understand the working mechanism of ViT models.",
                "8, the latest tools specialized for MSA modules and ViT models, namely, partial LRP [63] and TransformerExplainability [23] , can generate better results for feature map visualization than the visualization methods for CNN.",
                "Several researchers have achieved promising progress in unveiling the power of transformer models, from such perspectives as information bottlenecks [149, 150] and better visualization tools [23, 63].",
                "Visualization Transformer-Explainability [23] A better tool to visualize feature maps from ViT models"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "5e2180e4ce9d218cccb1c78a93a863d5f967d907",
                "externalIds": {
                    "DBLP": "journals/cvm/XuWLDSZTDHX22",
                    "DOI": "10.1007/s41095-021-0247-3",
                    "CorpusId": 240006659
                },
                "corpusId": 240006659,
                "publicationVenue": {
                    "id": "d2dfc02a-9028-4345-b6cf-556b76ac435b",
                    "name": "Computational Visual Media",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Vis Media"
                    ],
                    "issn": "2096-0433",
                    "url": "http://www.springer.com/41095",
                    "alternate_urls": [
                        "https://link.springer.com/journal/41095"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5e2180e4ce9d218cccb1c78a93a863d5f967d907",
                "title": "Transformers in computational visual media: A survey",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2125063007",
                        "name": "Yifan Xu"
                    },
                    {
                        "authorId": "2152788915",
                        "name": "Huapeng Wei"
                    },
                    {
                        "authorId": "1506975125",
                        "name": "Minxuan Lin"
                    },
                    {
                        "authorId": "49873367",
                        "name": "Yingying Deng"
                    },
                    {
                        "authorId": "2126419",
                        "name": "Kekai Sheng"
                    },
                    {
                        "authorId": "2516588",
                        "name": "Mengdan Zhang"
                    },
                    {
                        "authorId": "1443761295",
                        "name": "Fan Tang"
                    },
                    {
                        "authorId": "38690089",
                        "name": "Weiming Dong"
                    },
                    {
                        "authorId": "1835006",
                        "name": "Feiyue Huang"
                    },
                    {
                        "authorId": "145194969",
                        "name": "Changsheng Xu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Examples of XAI in research fields outside of MI include: visualizing word embeddings in Natural Language Processing [41\u201343], inspecting decision-making processes in reinforcement learning [44\u201346], visualizing pixel importances [47,48], or segmenting in computer vision [49,50]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a590928a15bc6dced42bb8ec4222044e9b270306",
                "externalIds": {
                    "DOI": "10.1007/s40192-021-00247-y",
                    "CorpusId": 239478601
                },
                "corpusId": 239478601,
                "publicationVenue": {
                    "id": "c6a8c769-7ca1-4f70-9487-1cd3481e43ed",
                    "name": "Integrating Materials and Manufacturing Innovation",
                    "alternate_names": [
                        "Integrating Mater Manuf Innov"
                    ],
                    "issn": "2193-9764",
                    "url": "https://link.springer.com/journal/40192"
                },
                "url": "https://www.semanticscholar.org/paper/a590928a15bc6dced42bb8ec4222044e9b270306",
                "title": "CrabNet for Explainable Deep Learning in Materials Science: Bridging the Gap Between Academia and Industry",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50013769",
                        "name": "A. Wang"
                    },
                    {
                        "authorId": "2152411887",
                        "name": "Mahamad Salah Mahmoud"
                    },
                    {
                        "authorId": "40843882",
                        "name": "Mathias Czasny"
                    },
                    {
                        "authorId": "144959484",
                        "name": "A. Gurlo"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Our technique was inspired by the recent work by Chefer and colleagues [26], who used the deep Taylor decomposition principle to assign local relevance scores",
                "Following the propagation procedure of relevance and gradients by Chefer and colleagues [26], GraphCAM computes the gradient \u2207A and layer relevance Rl with respect to a target class for each attention map A, where nl is the layer that corresponds to the softmax operation in Eq."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c626c7c6fdf0f7b0e23dd42e9ff1fc6d1d8c5326",
                "externalIds": {
                    "DOI": "10.1101/2021.10.15.21265060",
                    "CorpusId": 239014882
                },
                "corpusId": 239014882,
                "publicationVenue": {
                    "id": "d5e5b5e7-54b1-4f53-82fc-4853f3e71c58",
                    "name": "medRxiv",
                    "type": "journal",
                    "url": "https://www.medrxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/c626c7c6fdf0f7b0e23dd42e9ff1fc6d1d8c5326",
                "title": "A deep learning based graph-transformer for whole slide image classification",
                "abstract": "Deep learning is a powerful tool for assessing pathology data obtained from digitized biopsy slides. In the context of supervised learning, most methods typically divide a whole slide image (WSI) into patches, aggregate convolutional neural network outcomes on them and estimate overall disease grade. However, patch-based methods introduce label noise in training by assuming that each patch is independent with the same label as the WSI and neglect the important contextual information that is significant in disease grading. Here we present a Graph-Transformer (GT) based framework for processing pathology data, called GTP, that interprets morphological and spatial information at the WSI-level to predict disease grade. To demonstrate the applicability of our approach, we selected 3,024 hematoxylin and eosin WSIs of lung tumors and with normal histology from the Clinical Proteomic Tumor Analysis Consortium, the National Lung Screening Trial, and The Cancer Genome Atlas, and used GTP to distinguish adenocarcinoma (LUAD) and squamous cell carcinoma (LSCC) from those that have normal histology. Our model achieved consistently high performance on binary (tumor versus normal: mean overall accuracy = 0.975+/-0.013) as well as three-label (normal versus LUAD versus LSCC: mean accuracy = 0.932+/-0.019) classification on held-out test data, underscoring the power of GT-based deep learning for WSI-level classification. We also introduced a graph-based saliency mapping technique, called GraphCAM, that captures regional as well as contextual information and allows our model to highlight WSI regions that are highly associated with the class label. Taken together, our findings demonstrate GTP as a novel interpretable and effective deep learning framework for WSI-level classification.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1391155462",
                        "name": "Y. Zheng"
                    },
                    {
                        "authorId": "32065369",
                        "name": "R. Gindra"
                    },
                    {
                        "authorId": "1723703",
                        "name": "Margrit Betke"
                    },
                    {
                        "authorId": "2555118",
                        "name": "J. Beane"
                    },
                    {
                        "authorId": "1887282",
                        "name": "V. Kolachalama"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Applying explainable AI methods such as (Chefer et al., 2021) to explain the reasoning process could lead to further improvement of the curation process."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a92ac3993bc4e8ea51c91550cea4207a9d50b080",
                "externalIds": {
                    "ArXiv": "2110.04222",
                    "DBLP": "journals/corr/abs-2110-04222",
                    "CorpusId": 238531664
                },
                "corpusId": 238531664,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a92ac3993bc4e8ea51c91550cea4207a9d50b080",
                "title": "Inferring Offensiveness In Images From Natural Language Supervision",
                "abstract": "Probing or fine-tuning (large-scale) pre-trained models results in state-of-the-art performance for many NLP tasks and, more recently, even for computer vision tasks when combined with image data. Unfortunately, these approaches also entail severe risks. In particular, large image datasets automatically scraped from the web may contain derogatory terms as categories and offensive images, and may also underrepresent specific classes. Consequently, there is an urgent need to carefully document datasets and curate their content. Unfortunately, this process is tedious and error-prone. We show that pre-trained transformers themselves provide a methodology for the automated curation of large-scale vision datasets. Based on human-annotated examples and the implicit knowledge of a CLIP based model, we demonstrate that one can select relevant prompts for rating the offensiveness of an image. In addition to e.g. privacy violation and pornographic content previously identified in ImageNet, we demonstrate that our approach identifies further inappropriate and potentially offensive content.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "40896023",
                        "name": "P. Schramowski"
                    },
                    {
                        "authorId": "2066493115",
                        "name": "K. Kersting"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "We contribute to the field by building on the solution proposed in [Chefer et al. 2021] to associate consolidated attention weights to words containing the tokens so that one can leverage this knowledge to assess the behavior of a stance classifier in terms of the domain characteristics and the contribution of this words to the correct classification of the stances.",
                "The technique proposed in [Chefer et al. 2021] leverages LRP (Layer-wise Relevance Propagation) to overcome this limitation and summarizes the attention weights using information related to both the relevance and gradient.",
                "The interpretability mechanism proposed expands the attention consolidation proposed in [Chefer et al. 2021] by relating tokens to the original words and associating them to two word attention metrics: absolute and relative.",
                "Some\ntechniques have been proposed [Chefer et al. 2021, Vig 2019].",
                "To get the attention weights for each token, we use the modified LRP technique proposed in [Chefer et al. 2021].",
                "The attention weights are obtained using the adapted LRP technique proposed in [Chefer et al. 2021] from the results obtained by the classifier.",
                "It extends the technique of [Chefer et al. 2021], which consolidates the attention weights of the relevant tokens throughout the whole network by relating these tokens and the respective weights to the original words of the input text.",
                "We contribute to the field by building on the solution proposed in [Chefer et al. 2021] to associate consolidated attention weights to words containing the tokens so that one can leverage this knowledge to assess the behavior of a stance classifier in terms of the domain characteristics and the\u2026"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "e43a360728e34e43934707665db0f7be02c8e5a7",
                "externalIds": {
                    "DBLP": "conf/sbbd/SaenzB21",
                    "DOI": "10.5753/sbbd.2021.17867",
                    "CorpusId": 245184963
                },
                "corpusId": 245184963,
                "publicationVenue": {
                    "id": "ae70daa0-c404-4e18-ac73-f8a37c17fdd9",
                    "name": "Brazilian Symposium on Databases",
                    "type": "conference",
                    "alternate_names": [
                        "SBBD",
                        "Braz Symp Database"
                    ],
                    "url": "http://www.sbc.org.br/"
                },
                "url": "https://www.semanticscholar.org/paper/e43a360728e34e43934707665db0f7be02c8e5a7",
                "title": "Interpreting BERT-based stance classification: a case study about the Brazilian COVID vaccination",
                "abstract": "The actions to control the COVID-19 pandemics should be based on scientific facts. However, Brazil is facing a politically polarized scenario that has influenced the population\u2019s behavior regarding social distance or vaccination issues. This paper addresses this subject by proposing a BERT-based stance classification model and an attention-based mechanism to identify the influential words for stance classification. The interpretation mechanism traces tokens\u2019 attentions back to words, assigning word attention scores (absolute and relative). We use these metrics to assess if words with high attention weights correspond to domain intrinsic properties and contribute to the correct classification of stances. Our experiments revealed good results for stance classification (F1=0.752), and that 74% of the top-100 words with the highest absolute attention are representative of the arguments that support the investigated stances.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2092671419",
                        "name": "Carlos Abel C\u00f3rdova S\u00e1enz"
                    },
                    {
                        "authorId": "49629811",
                        "name": "Karin Becker"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Moreover, while the interpretability of Transformers is a research problem on its own [4], MM-MIL has an explicit mechanism for model interpretability."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a3fb7dd2cd27c1cc35e61bc9fbcf986ae148792e",
                "externalIds": {
                    "ArXiv": "2109.12307",
                    "DBLP": "journals/corr/abs-2109-12307",
                    "DOI": "10.1145/3474085.3475418",
                    "CorpusId": 237940152
                },
                "corpusId": 237940152,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a3fb7dd2cd27c1cc35e61bc9fbcf986ae148792e",
                "title": "Multi-Modal Multi-Instance Learning for Retinal Disease Recognition",
                "abstract": "This paper attacks an emerging challenge of multi-modal retinal disease recognition. Given a multi-modal case consisting of a color fundus photo (CFP) and an array of OCT B-scan images acquired during an eye examination, we aim to build a deep neural network that recognizes multiple vision-threatening diseases for the given case. As the diagnostic efficacy of CFP and OCT is disease-dependent, the network's ability of being both selective and interpretable is important. Moreover, as both data acquisition and manual labeling are extremely expensive in the medical domain, the network has to be relatively lightweight for learning from a limited set of labeled multi-modal samples. Prior art on retinal disease recognition focuses either on a single disease or on a single modality, leaving multi-modal fusion largely underexplored. We propose in this paper Multi-Modal Multi-Instance Learning (MM-MIL) for selectively fusing CFP and OCT modalities. Its lightweight architecture (as compared to current multi-head attention modules) makes it suited for learning from relatively small-sized datasets. For an effective use of MM-MIL, we propose to generate a pseudo sequence of CFPs by over sampling a given CFP. The benefits of this tactic include well balancing instances across modalities, increasing the resolution of the CFP input, and finding out regions of the CFP most relevant with respect to the final diagnosis. Extensive experiments on a real-world dataset consisting of 1,206 multi-modal cases from 1,193 eyes of 836 subjects demonstrate the viability of the proposed model.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2144393226",
                        "name": "Xirong Li"
                    },
                    {
                        "authorId": "2145499010",
                        "name": "Yang Zhou"
                    },
                    {
                        "authorId": "2146044469",
                        "name": "Jie Wang"
                    },
                    {
                        "authorId": "5379645",
                        "name": "Hailan Lin"
                    },
                    {
                        "authorId": "1596488705",
                        "name": "Jianchun Zhao"
                    },
                    {
                        "authorId": "50792192",
                        "name": "Dayong Ding"
                    },
                    {
                        "authorId": "152262144",
                        "name": "Weihong Yu"
                    },
                    {
                        "authorId": "2029541",
                        "name": "You-xin Chen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "To further investigate the effectiveness of our approach, we employ the method [5] to visualize the attention maps generated by our TransFER.",
                "Attention visualization [5] of different expressions on some example face images from AffectNet dataset.",
                "Attention visualizations [5] on two example images: Sur-"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "ebf221bf7260e2d27b243b15909d89196f62f39b",
                "externalIds": {
                    "ArXiv": "2108.11116",
                    "DBLP": "journals/corr/abs-2108-11116",
                    "DOI": "10.1109/ICCV48922.2021.00358",
                    "CorpusId": 237292746
                },
                "corpusId": 237292746,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/ebf221bf7260e2d27b243b15909d89196f62f39b",
                "title": "TransFER: Learning Relation-aware Facial Expression Representations with Transformers",
                "abstract": "Facial expression recognition (FER) has received increasing interest in computer vision. We propose the Trans-FER model which can learn rich relation-aware local representations. It mainly consists of three components: Multi-Attention Dropping (MAD), ViT-FER, and Multi-head Self-Attention Dropping (MSAD). First, local patches play an important role in distinguishing various expressions, however, few existing works can locate discriminative and diverse local patches. This can cause serious problems when some patches are invisible due to pose variations or viewpoint changes. To address this issue, the MAD is proposed to randomly drop an attention map. Consequently, models are pushed to explore diverse local patches adaptively. Second, to build rich relations between different local patches, the Vision Transformers (ViT) are used in FER, called ViT-FER. Since the global scope is used to reinforce each local patch, a better representation is obtained to boost the FER performance. Thirdly, the multi-head self-attention allows ViT to jointly attend to features from different information subspaces at different positions. Given no explicit guidance, however, multiple self-attentions may extract similar relations. To address this, the MSAD is proposed to randomly drop one self-attention module. As a result, models are forced to learn rich relations among diverse local patches. Our proposed TransFER model outperforms the state-of-the-art methods on several FER benchmarks, showing its effectiveness and usefulness.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "40830909",
                        "name": "Fanglei Xue"
                    },
                    {
                        "authorId": "2224668502",
                        "name": "Qiangchang Wang"
                    },
                    {
                        "authorId": "1822413",
                        "name": "G. Guo"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "80884479ed1ba4a88d1823a78903dd9bdbca0d51",
                "externalIds": {
                    "PubMedCentral": "8472428",
                    "DOI": "10.3390/plants10091759",
                    "CorpusId": 238200442,
                    "PubMed": "34579292"
                },
                "corpusId": 238200442,
                "publicationVenue": {
                    "id": "e53a7f00-880e-4975-9953-95f2e540dbb7",
                    "name": "Plants",
                    "issn": "2223-7747",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-247598",
                    "alternate_urls": [
                        "https://www.mdpi.com/journal/plants",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-247598"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/80884479ed1ba4a88d1823a78903dd9bdbca0d51",
                "title": "Quinoa Phenotyping Methodologies: An International Consensus",
                "abstract": "Quinoa is a crop originating in the Andes but grown more widely and with the genetic potential for significant further expansion. Due to the phenotypic plasticity of quinoa, varieties need to be assessed across years and multiple locations. To improve comparability among field trials across the globe and to facilitate collaborations, components of the trials need to be kept consistent, including the type and methods of data collected. Here, an internationally open-access framework for phenotyping a wide range of quinoa features is proposed to facilitate the systematic agronomic, physiological and genetic characterization of quinoa for crop adaptation and improvement. Mature plant phenotyping is a central aspect of this paper, including detailed descriptions and the provision of phenotyping cards to facilitate consistency in data collection. High-throughput methods for multi-temporal phenotyping based on remote sensing technologies are described. Tools for higher-throughput post-harvest phenotyping of seeds are presented. A guideline for approaching quinoa field trials including the collection of environmental data and designing layouts with statistical robustness is suggested. To move towards developing resources for quinoa in line with major cereal crops, a database was created. The Quinoa Germinate Platform will serve as a central repository of data for quinoa researchers globally.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "91850135",
                        "name": "Clara S Stanschewski"
                    },
                    {
                        "authorId": "36587260",
                        "name": "Elodie Rey"
                    },
                    {
                        "authorId": "13254607",
                        "name": "Gabriele Fiene"
                    },
                    {
                        "authorId": "90592194",
                        "name": "Evan B. Craine"
                    },
                    {
                        "authorId": "90005564",
                        "name": "Gordon B. Wellman"
                    },
                    {
                        "authorId": "6001310",
                        "name": "V. Melino"
                    },
                    {
                        "authorId": "2130005785",
                        "name": "Dilan S R Patiranage"
                    },
                    {
                        "authorId": "145118138",
                        "name": "K. Johansen"
                    },
                    {
                        "authorId": "4412209",
                        "name": "Sandra M. Schm\u00f6ckel"
                    },
                    {
                        "authorId": "2973601",
                        "name": "D. Bertero"
                    },
                    {
                        "authorId": "49229060",
                        "name": "H. Oakey"
                    },
                    {
                        "authorId": "1903201429",
                        "name": "Carla Colque-Little"
                    },
                    {
                        "authorId": "6750902",
                        "name": "I. Afzal"
                    },
                    {
                        "authorId": "2335401",
                        "name": "S. Raubach"
                    },
                    {
                        "authorId": "33145497",
                        "name": "Nathan D. Miller"
                    },
                    {
                        "authorId": "51940571",
                        "name": "J. Streich"
                    },
                    {
                        "authorId": "5105069",
                        "name": "D. B. Amby"
                    },
                    {
                        "authorId": "4084829",
                        "name": "N. Emrani"
                    },
                    {
                        "authorId": "1500728446",
                        "name": "Mark Warmington"
                    },
                    {
                        "authorId": "123183185",
                        "name": "Magdi A. A. Mousa"
                    },
                    {
                        "authorId": "2107535422",
                        "name": "D. Wu"
                    },
                    {
                        "authorId": "143601258",
                        "name": "Daniel A. Jacobson"
                    },
                    {
                        "authorId": "144214173",
                        "name": "C. Andreasen"
                    },
                    {
                        "authorId": "144215085",
                        "name": "C. Jung"
                    },
                    {
                        "authorId": "46290711",
                        "name": "K. Murphy"
                    },
                    {
                        "authorId": "4566919",
                        "name": "D. Bazile"
                    },
                    {
                        "authorId": "144750829",
                        "name": "M. Tester"
                    },
                    {
                        "authorId": "2130024227",
                        "name": "On Behalf Of The Quinoa Phenotyping Consortium"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Although they achieve promising results on image classification [50, 2], visual question answering [31], and image generation [1], they cannot explain how visual similarity is composed.",
                "To improve the transparency of deep visual models, many efforts have been made recently by either explaining the existing models [50, 31, 1, 2] or modifying models to achieve better interpretability [46, 47]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "022e5a449e70ce84c94ddb4e0c6658434441e6d4",
                "externalIds": {
                    "ArXiv": "2108.05889",
                    "DBLP": "journals/corr/abs-2108-05889",
                    "DOI": "10.1109/ICCV48922.2021.00974",
                    "CorpusId": 236987081
                },
                "corpusId": 236987081,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/022e5a449e70ce84c94ddb4e0c6658434441e6d4",
                "title": "Towards Interpretable Deep Metric Learning with Structural Matching",
                "abstract": "How do the neural networks distinguish two images? It is of critical importance to understand the matching mechanism of deep models for developing reliable intelligent systems for many risky visual applications such as surveillance and access control. However, most existing deep metric learning methods match the images by comparing feature vectors, which ignores the spatial structure of images and thus lacks interpretability. In this paper, we present a deep interpretable metric learning (DIML) method for more transparent embedding learning. Unlike conventional metric learning methods based on feature vector comparison, we propose a structural matching strategy that explicitly aligns the spatial embeddings by computing an optimal matching flow between feature maps of the two images. Our method enables deep models to learn metrics in a more human-friendly way, where the similarity of two images can be decomposed to several part-wise similarities and their contributions to the overall similarity. Our method is model-agnostic, which can be applied to off-the-shelf backbone networks and metric learning methods. We evaluate our method on three major benchmarks of deep metric learning including CUB200-2011, Cars196, and Stanford Online Products, and achieve substantial improvements over popular metric learning methods with better interpretability. Code is available at https://github.com/wl-zhao/DIML.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118223312",
                        "name": "Wenliang Zhao"
                    },
                    {
                        "authorId": "39358728",
                        "name": "Yongming Rao"
                    },
                    {
                        "authorId": "2142663191",
                        "name": "Ziyi Wang"
                    },
                    {
                        "authorId": "1697700",
                        "name": "Jiwen Lu"
                    },
                    {
                        "authorId": "2108485135",
                        "name": "Jie Zhou"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "For overall comparisons with the state-of-the-art methods (Rao et al. 2021; Tang et al. 2021; Chen et al. 2021; Pan et al. 2021), we conduct the token selection and slow-fast token updating from the fifth layer of DeiT and the third layer (excluding the convolution layers) of LeViT, respectively.",
                "In Table 1, we compare our method with existing token pruning methods (Rao et al. 2021; Pan et al. 2021; Tang et al. 2021; Chen et al. 2021)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d045133e6e022684329ff944d67f91888be1bc3b",
                "externalIds": {
                    "ArXiv": "2108.01390",
                    "DBLP": "conf/aaai/XuZZSLDZXS22",
                    "DOI": "10.1609/aaai.v36i3.20202",
                    "CorpusId": 236881638
                },
                "corpusId": 236881638,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/d045133e6e022684329ff944d67f91888be1bc3b",
                "title": "Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer",
                "abstract": "Vision transformers (ViTs) have recently received explosive popularity, but the huge computational cost is still a severe issue. Since the computation complexity of ViT is quadratic with respect to the input sequence length, a mainstream paradigm for computation reduction is to reduce the number of tokens. Existing designs include structured spatial compression that uses a progressive shrinking pyramid to reduce the computations of large feature maps, and unstructured token pruning that dynamically drops redundant tokens. However, the limitation of existing token pruning lies in two folds: 1) the incomplete spatial structure caused by pruning is not compatible with structured spatial compression that is commonly used in modern deep-narrow transformers; 2) it usually requires a time-consuming pre-training procedure. To tackle the limitations and expand the applicable scenario of token pruning, we present Evo-ViT, a self-motivated slow-fast token evolution approach for vision transformers. Specifically, we conduct unstructured instance-wise token selection by taking advantage of the simple and effective global class attention that is native to vision transformers. Then, we propose to update the selected informative tokens and uninformative tokens with different computation paths, namely, slow-fast updating. Since slow-fast updating mechanism maintains the spatial structure and information flow, Evo-ViT can accelerate vanilla transformers of both flat and deep-narrow structures from the very beginning of the training process. Experimental results demonstrate that our method significantly reduces the computational cost of vision transformers while maintaining comparable performance on image classification. For example, our method accelerates DeiT-S by over 60% throughput while only sacrificing 0.4% top-1 accuracy on ImageNet-1K, outperforming current token pruning methods on both accuracy and efficiency.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2110154407",
                        "name": "Yifan Xu"
                    },
                    {
                        "authorId": "2051266898",
                        "name": "Zhijie Zhang"
                    },
                    {
                        "authorId": "2516588",
                        "name": "Mengdan Zhang"
                    },
                    {
                        "authorId": "2126419",
                        "name": "Kekai Sheng"
                    },
                    {
                        "authorId": "2149140038",
                        "name": "Ke Li"
                    },
                    {
                        "authorId": "38690089",
                        "name": "Weiming Dong"
                    },
                    {
                        "authorId": "2108911758",
                        "name": "Liqing Zhang"
                    },
                    {
                        "authorId": "2155590336",
                        "name": "Changsheng Xu"
                    },
                    {
                        "authorId": "143900241",
                        "name": "Xing Sun"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In order to analyse the explainability properties of our proposed method, we use the Gradient Attention Rollout algorithm as outlined in [87].",
                "Visualization of different cases (normal, Pneumonia, COVID-19) considered in this study and their associated critical factors in decision making by xViTCOS as identified using the explanability method laid out in [87] for transformers [16]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bcc8ab809cdb8315f33317b17e67848711c366ca",
                "externalIds": {
                    "PubMedCentral": "8691725",
                    "MAG": "3185555919",
                    "DOI": "10.1109/JTEHM.2021.3134096",
                    "CorpusId": 237830836,
                    "PubMed": "34956741"
                },
                "corpusId": 237830836,
                "publicationVenue": {
                    "id": "5b1ab359-1a03-4a1b-b41d-332a3fbfd5b3",
                    "name": "IEEE Journal of Translational Engineering in Health and Medicine",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE J Transl Eng Health Med"
                    ],
                    "issn": "2168-2372",
                    "url": "http://health.embs.org/",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/aboutJournal.jsp?punumber=6221039",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6221039"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bcc8ab809cdb8315f33317b17e67848711c366ca",
                "title": "xViTCOS: Explainable Vision Transformer Based COVID-19 Screening Using Radiography",
                "abstract": "Objective: Since its outbreak, the rapid spread of COrona VIrus Disease 2019 (COVID-19) across the globe has pushed the health care system in many countries to the verge of collapse. Therefore, it is imperative to correctly identify COVID-19 positive patients and isolate them as soon as possible to contain the spread of the disease and reduce the ongoing burden on the healthcare system. The primary COVID-19 screening test, RT-PCR although accurate and reliable, has a long turn-around time. In the recent past, several researchers have demonstrated the use of Deep Learning (DL) methods on chest radiography (such as X-ray and CT) for COVID-19 detection. However, existing CNN based DL methods fail to capture the global context due to their inherent image-specific inductive bias. Methods: Motivated by this, in this work, we propose the use of vision transformers (instead of convolutional networks) for COVID-19 screening using the X-ray and CT images. We employ a multi-stage transfer learning technique to address the issue of data scarcity. Furthermore, we show that the features learned by our transformer networks are explainable. Results: We demonstrate that our method not only quantitatively outperforms the recent benchmarks but also focuses on meaningful regions in the images for detection (as confirmed by Radiologists), aiding not only in accurate diagnosis of COVID-19 but also in localization of the infected area. The code for our implementation can be found here - https://github.com/arnabkmondal/xViTCOS. Conclusion: The proposed method will help in timely identification of COVID-19 and efficient utilization of limited resources.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2064036757",
                        "name": "A. Mondal"
                    },
                    {
                        "authorId": "46763438",
                        "name": "A. Bhattacharjee"
                    },
                    {
                        "authorId": "2128239746",
                        "name": "Parag Singla"
                    },
                    {
                        "authorId": "2127763983",
                        "name": "Prathosh Ap"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "Visualization of the weights in self-attention has widely been used to provide the semantic relationships between different elements of data [27, 28]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "123694c4b7e4135c75105fb4251ca7e48816c792",
                "externalIds": {
                    "ArXiv": "2106.15516",
                    "DOI": "10.1021/acsomega.3c05753",
                    "CorpusId": 259275324
                },
                "corpusId": 259275324,
                "publicationVenue": {
                    "id": "d516f81c-009b-445d-b748-c827efa137d3",
                    "name": "ACS Omega",
                    "alternate_names": [
                        "AC Omega"
                    ],
                    "issn": "2470-1343",
                    "url": "https://epub.uni-regensburg.de/36538/",
                    "alternate_urls": [
                        "https://pubs.acs.org/journal/acsodf"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/123694c4b7e4135c75105fb4251ca7e48816c792",
                "title": "GeoT: A Geometry-Aware Transformer for Reliable Molecular Property Prediction and Chemically Interpretable Representation Learning",
                "abstract": "In recent years, molecular representation learning has emerged as a key area of focus in various chemical tasks. However, many existing models fail to fully consider the geometric information of molecular structures, resulting in less intuitive representations. Moreover, the widely used message-passing mechanism is limited to provide the interpretation of experimental results from a chemical perspective. To address these challenges, we introduce a novel Transformer-based framework for molecular representation learning, named the Geometry-aware Transformer (GeoT). GeoT learns molecular graph structures through attention-based mechanisms specifically designed to offer reliable interpretability, as well as molecular property prediction. Consequently, GeoT can generate attention maps of interatomic relationships associated with training objectives. In addition, GeoT demonstrates comparable performance to MPNN-based models while achieving reduced computational complexity. Our comprehensive experiments, including an empirical simulation, reveal that GeoT effectively learns the chemical insights into molecular structures, bridging the gap between artificial intelligence and molecular sciences.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "97113956",
                        "name": "Bumju Kwak"
                    },
                    {
                        "authorId": "2115930893",
                        "name": "J. Park"
                    },
                    {
                        "authorId": "2220735231",
                        "name": "Taewon Kang"
                    },
                    {
                        "authorId": "91019090",
                        "name": "Jeonghee Jo"
                    },
                    {
                        "authorId": "2821668",
                        "name": "Byunghan Lee"
                    },
                    {
                        "authorId": "2999019",
                        "name": "Sungroh Yoon"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "On the other hand, previous works [6, 8] have already shown the vulnerable interpretability of the original vision transformer, where the raw attention comes from the architecture sometimes fails to perceive the informative region of the input images."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e2f2662f0734e2edc2b4b36a734de111c7f8d54d",
                "externalIds": {
                    "DBLP": "conf/nips/PanPJWFO21",
                    "ArXiv": "2106.12620",
                    "CorpusId": 235623729
                },
                "corpusId": 235623729,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e2f2662f0734e2edc2b4b36a734de111c7f8d54d",
                "title": "IA-RED2: Interpretability-Aware Redundancy Reduction for Vision Transformers",
                "abstract": "The self-attention-based model, transformer, is recently becoming the leading backbone in the field of computer vision. In spite of the impressive success made by transformers in a variety of vision tasks, it still suffers from heavy computation and intensive memory costs. To address this limitation, this paper presents an Interpretability-Aware REDundancy REDuction framework (IA-RED$^2$). We start by observing a large amount of redundant computation, mainly spent on uncorrelated input patches, and then introduce an interpretable module to dynamically and gracefully drop these redundant patches. This novel framework is then extended to a hierarchical structure, where uncorrelated tokens at different stages are gradually removed, resulting in a considerable shrinkage of computational cost. We include extensive experiments on both image and video tasks, where our method could deliver up to 1.4x speed-up for state-of-the-art models like DeiT and TimeSformer, by only sacrificing less than 0.7% accuracy. More importantly, contrary to other acceleration approaches, our method is inherently interpretable with substantial visual evidence, making vision transformer closer to a more human-understandable architecture while being lighter. We demonstrate that the interpretability that naturally emerged in our framework can outperform the raw attention learned by the original visual transformer, as well as those generated by off-the-shelf interpretation methods, with both qualitative and quantitative results. Project Page: http://people.csail.mit.edu/bpan/ia-red/.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35654996",
                        "name": "Bowen Pan"
                    },
                    {
                        "authorId": null,
                        "name": "Yifan Jiang"
                    },
                    {
                        "authorId": "1819152",
                        "name": "R. Panda"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "1723233",
                        "name": "R. Feris"
                    },
                    {
                        "authorId": "143868587",
                        "name": "A. Oliva"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "(c) visualizes the impact of each spatial location on the final prediction in the DeiT-S model [25] using the visualization method proposed in [3]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dbdcabd0444ad50b68ee09e30f39b66e9068f5d2",
                "externalIds": {
                    "DBLP": "conf/nips/RaoZLLZH21",
                    "ArXiv": "2106.02034",
                    "CorpusId": 235313562
                },
                "corpusId": 235313562,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/dbdcabd0444ad50b68ee09e30f39b66e9068f5d2",
                "title": "DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification",
                "abstract": "Attention is sparse in vision transformers. We observe the final prediction in vision transformers is only based on a subset of most informative tokens, which is sufficient for accurate image recognition. Based on this observation, we propose a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input. Specifically, we devise a lightweight prediction module to estimate the importance score of each token given the current features. The module is added to different layers to prune redundant tokens hierarchically. To optimize the prediction module in an end-to-end manner, we propose an attention masking strategy to differentiably prune a token by blocking its interactions with other tokens. Benefiting from the nature of self-attention, the unstructured sparse tokens are still hardware friendly, which makes our framework easy to achieve actual speed-up. By hierarchically pruning 66% of the input tokens, our method greatly reduces 31%~37% FLOPs and improves the throughput by over 40% while the drop of accuracy is within 0.5% for various vision transformers. Equipped with the dynamic token sparsification framework, DynamicViT models can achieve very competitive complexity/accuracy trade-offs compared to state-of-the-art CNNs and vision transformers on ImageNet. Code is available at https://github.com/raoyongming/DynamicViT",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "39358728",
                        "name": "Yongming Rao"
                    },
                    {
                        "authorId": "2118223312",
                        "name": "Wenliang Zhao"
                    },
                    {
                        "authorId": "67215934",
                        "name": "Benlin Liu"
                    },
                    {
                        "authorId": "1697700",
                        "name": "Jiwen Lu"
                    },
                    {
                        "authorId": "49178343",
                        "name": "Jie Zhou"
                    },
                    {
                        "authorId": "1793529",
                        "name": "Cho-Jui Hsieh"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Images from different classes are visualized in Figure 5 using Transformer Attribution method [6] on DeiT-Tiny.",
                "2 [6] Hila Chefer, Shir Gur, and Lior Wolf.",
                "Visualization using Transformer Attribution [6]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "fb987ebe5ff5276fbbe6a5c5b16b6bfd759afa37",
                "externalIds": {
                    "MAG": "3169938586",
                    "DBLP": "conf/eccv/WangWWLCLJ22",
                    "ArXiv": "2106.00515",
                    "DOI": "10.1007/978-3-031-20053-3_17",
                    "CorpusId": 235266033
                },
                "corpusId": 235266033,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/fb987ebe5ff5276fbbe6a5c5b16b6bfd759afa37",
                "title": "KVT: k-NN Attention for Boosting Vision Transformers",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "8120382",
                        "name": "Pichao Wang"
                    },
                    {
                        "authorId": "2118294665",
                        "name": "Xue Wang"
                    },
                    {
                        "authorId": "1716453",
                        "name": "F. Wang"
                    },
                    {
                        "authorId": "2115912472",
                        "name": "Ming Lin"
                    },
                    {
                        "authorId": "1993649162",
                        "name": "Shuning Chang"
                    },
                    {
                        "authorId": "2106507853",
                        "name": "Wen Xie"
                    },
                    {
                        "authorId": "1706574",
                        "name": "Hao Li"
                    },
                    {
                        "authorId": "144723884",
                        "name": "Rong Jin"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7093016e02e8f4580ffea18396f95d129d37858d",
                "externalIds": {
                    "ArXiv": "2104.10858",
                    "DBLP": "conf/nips/JiangHYZSJWF21",
                    "CorpusId": 235377078
                },
                "corpusId": 235377078,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7093016e02e8f4580ffea18396f95d129d37858d",
                "title": "All Tokens Matter: Token Labeling for Training Better Vision Transformers",
                "abstract": "In this paper, we present token labeling -- a new training objective for training high-performance vision transformers (ViTs). Different from the standard training objective of ViTs that computes the classification loss on an additional trainable class token, our proposed one takes advantage of all the image patch tokens to compute the training loss in a dense manner. Specifically, token labeling reformulates the image classification problem into multiple token-level recognition problems and assigns each patch token with an individual location-specific supervision generated by a machine annotator. Experiments show that token labeling can clearly and consistently improve the performance of various ViT models across a wide spectrum. For a vision transformer with 26M learnable parameters serving as an example, with token labeling, the model can achieve 84.4% Top-1 accuracy on ImageNet. The result can be further increased to 86.4% by slightly scaling the model size up to 150M, delivering the minimal-sized model among previous models (250M+) reaching 86%. We also show that token labeling can clearly improve the generalization capability of the pre-trained models on downstream tasks with dense prediction, such as semantic segmentation. Our code and all the training details will be made publicly available at https://github.com/zihangJiang/TokenLabeling.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "97234041",
                        "name": "Zihang Jiang"
                    },
                    {
                        "authorId": "3298532",
                        "name": "Qibin Hou"
                    },
                    {
                        "authorId": "2087091296",
                        "name": "Li Yuan"
                    },
                    {
                        "authorId": "18119920",
                        "name": "Daquan Zhou"
                    },
                    {
                        "authorId": "145356288",
                        "name": "Yujun Shi"
                    },
                    {
                        "authorId": "2103483",
                        "name": "Xiaojie Jin"
                    },
                    {
                        "authorId": "1754818",
                        "name": "Anran Wang"
                    },
                    {
                        "authorId": "1698982",
                        "name": "Jiashi Feng"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "409e010e6859f764486b6f0fb6f088a817398bdf",
                "externalIds": {
                    "ArXiv": "2104.10935",
                    "CorpusId": 245335371
                },
                "corpusId": 245335371,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/409e010e6859f764486b6f0fb6f088a817398bdf",
                "title": "SoT: Delving Deeper into Classification Head for Transformer",
                "abstract": "Transformer models are not only successful in natural language processing (NLP) but also demonstrate high potential in computer vision (CV). Despite great advance, most of works only focus on improvement of architectures but pay little attention to the classification head. For years transformer models base exclusively on classification token to construct the final classifier, without explicitly harnessing high-level word tokens. In this paper, we propose a novel transformer model called second-order transformer (SoT), exploiting simultaneously the classification token and word tokens for the classifier. Specifically, we empirically disclose that high-level word tokens contain rich information, which per se are very competent with the classifier and moreover, are complementary to the classification token. To effectively harness such rich information, we propose multi-headed global cross-covariance pooling with singular value power normalization, which shares similar philosophy and thus is compatible with the transformer block, better than commonly used pooling methods. Then, we study comprehensively how to explicitly combine word tokens with classification token for building the final classification head. For CV tasks, our SoT significantly improves state-of-the-art vision transformers on challenging benchmarks including ImageNet and ImageNet-A. For NLP tasks, through fine-tuning based on pretrained language transformers including GPT and BERT, our SoT greatly boosts the performance on widely used tasks such as CoLA and RTE. Code will be available at https://peihuali.org/SoT",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144418234",
                        "name": "Jiangtao Xie"
                    },
                    {
                        "authorId": "47689605",
                        "name": "Rui Zeng"
                    },
                    {
                        "authorId": "2108995796",
                        "name": "Qilong Wang"
                    },
                    {
                        "authorId": "2112252436",
                        "name": "Ziqi Zhou"
                    },
                    {
                        "authorId": "40426020",
                        "name": "P. Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "We use the attention as the explanation [29, 3]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "13c9adaaa44e75f0b6cbbd1ff3f55c2f804669e3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-07954",
                    "ArXiv": "2104.07954",
                    "CorpusId": 233289541
                },
                "corpusId": 233289541,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/13c9adaaa44e75f0b6cbbd1ff3f55c2f804669e3",
                "title": "Towards Human-Understandable Visual Explanations: Imperceptible High-frequency Cues Can Better Be Removed",
                "abstract": "Explainable AI (XAI) methods focus on explaining what a neural network has learned - in other words, identifying the features that are the most influential to the prediction. In this paper, we call them\"distinguishing features\". However, whether a human can make sense of the generated explanation also depends on the perceptibility of these features to humans. To make sure an explanation is human-understandable, we argue that the capabilities of humans, constrained by the Human Visual System (HVS) and psychophysics, need to be taken into account. We propose the {\\em human perceptibility principle for XAI}, stating that, to generate human-understandable explanations, neural networks should be steered towards focusing on human-understandable cues during training. We conduct a case study regarding the classification of real vs. fake face images, where many of the distinguishing features picked up by standard neural networks turn out not to be perceptible to humans. By applying the proposed principle, a neural network with human-understandable explanations is trained which, in a user study, is shown to better align with human intuition. This is likely to make the AI more trustworthy and opens the door to humans learning from machines. In the case study, we specifically investigate and analyze the behaviour of the human-imperceptible high spatial frequency features in neural networks and XAI methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50394968",
                        "name": "Kaili Wang"
                    },
                    {
                        "authorId": "2111981",
                        "name": "Jos\u00e9 Oramas"
                    },
                    {
                        "authorId": "1704728",
                        "name": "T. Tuytelaars"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "For the interpretability of the classification model, we adopted a visualization method of saliency map tailored for ViT suggested by (Chefer et al., 2020), which computes relevancy for Transformer network."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fefd5ad3ee8b3b7a780143741e095e5a9d655d73",
                "externalIds": {
                    "ArXiv": "2104.07235",
                    "DBLP": "journals/mia/ParkKOSLKMLY22",
                    "PubMedCentral": "8566090",
                    "DOI": "10.1016/j.media.2021.102299",
                    "CorpusId": 233241180,
                    "PubMed": "34814058"
                },
                "corpusId": 233241180,
                "publicationVenue": {
                    "id": "0e5a2999-db05-4ab3-83d8-b480d49b90be",
                    "name": "Medical Image Analysis",
                    "type": "journal",
                    "alternate_names": [
                        "Med Image Anal"
                    ],
                    "issn": "1361-8415",
                    "url": "https://www.journals.elsevier.com/medical-image-analysis",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/13618415"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fefd5ad3ee8b3b7a780143741e095e5a9d655d73",
                "title": "Multi-task vision transformer using low-level chest X-ray feature corpus for COVID-19 diagnosis and severity quantification",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2107948510",
                        "name": "Sangjoon Park"
                    },
                    {
                        "authorId": "2109334279",
                        "name": "Gwanghyun Kim"
                    },
                    {
                        "authorId": "50048593",
                        "name": "Y. Oh"
                    },
                    {
                        "authorId": "46844846",
                        "name": "J. Seo"
                    },
                    {
                        "authorId": "40398599",
                        "name": "Sang Min Lee"
                    },
                    {
                        "authorId": "2152674480",
                        "name": "Jin Hwan Kim"
                    },
                    {
                        "authorId": "2113900807",
                        "name": "Sungjun Moon"
                    },
                    {
                        "authorId": "66190703",
                        "name": "Jae-Kwang Lim"
                    },
                    {
                        "authorId": "30547794",
                        "name": "Jong-Chul Ye"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Since each such map is comprised of h heads, we follow [5] and use gradients to average across heads.",
                ", [5, 1]) heavily rely on self-attention, and do not provide adaptations to any other form of attention, which is commonly used in multi-modal Transformers.",
                "In some cases, when self-attention is prominent, the recent method by Chefer et al. [5] is the only method that can provide comparable results.",
                "6 we account for the fact that the tokens were already contextualized in previous attention layers by applying matrix multiplication with the aggregated self-attention matrix R, as done in [1, 5].",
                "Our explainability prescription is easier to implement than existing methods, such as [5], and can be readily applied to any attention-based architecture.",
                "Following [5] we remove the negative contributions before averaging.",
                "As noted by Chefer et al [5], the computation in each attention head mixes queries, keys, and values and cannot be fully captured by considering only the inner products of queries and keys, which is what is referred to as attention.",
                "[5] provide a comprehensive treatment of the information propagation within all components of the Transformer model, which back-propagates the information through all layers from the decision back to the input.",
                "[5] demonstrates that this method fails to distinguish between positive and negative contributions to the decision, leading to an accumulation of relevancy scores across the layers in cases where these should be cancelled out.",
                "We present baselines of three classes, following [5]: attention map baselines, gradient baselines, and relevancy map baselines.",
                "For our gradient baselines, we use the Grad-CAM [32] adaptation described in [5], i.",
                "In contrast to these methods, Chefer et al. [5] provide a comprehensive treatment of the information propagation\nwithin all components of the Transformer model, which back-propagates the information through all layers from the decision back to the input."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "7ec5f207263100ea2d45db595712f611a74bafd9",
                "externalIds": {
                    "ArXiv": "2103.15679",
                    "DBLP": "conf/iccv/CheferGW21",
                    "DOI": "10.1109/ICCV48922.2021.00045",
                    "CorpusId": 232417173
                },
                "corpusId": 232417173,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/7ec5f207263100ea2d45db595712f611a74bafd9",
                "title": "Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers",
                "abstract": "Transformers are increasingly dominating multi-modal reasoning tasks, such as visual question answering, achieving state-of-the-art results thanks to their ability to contextualize information using the self-attention and co-attention mechanisms. These attention modules also play a role in other computer vision tasks including object detection and image segmentation. Unlike Transformers that only use self-attention, Transformers with co-attention require to consider multiple attention maps in parallel in order to highlight the information that is relevant to the prediction in the model\u2019s input. In this work, we propose the first method to explain prediction by any Transformer-based architecture, including bi-modal Transformers and Transformers with co-attentions. We provide generic solutions and apply these to the three most commonly used of these architectures: (i) pure self-attention, (ii) self-attention combined with co-attention, and (iii) encoder-decoder attention. We show that our method is superior to all existing methods which are adapted from single modality explainability. Our code is available at: https://github.com/hila-chefer/Transformer-MM-Explainability.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2038268012",
                        "name": "Hila Chefer"
                    },
                    {
                        "authorId": "47509360",
                        "name": "Shir Gur"
                    },
                    {
                        "authorId": "48519520",
                        "name": "Lior Wolf"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "There also exist interpretation tools that specifically leverage the transformer architecture (Chefer et al., 2021, 2020)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7cc88a1a904e8bb6edc1123c0800d1c5a0ea435d",
                "externalIds": {
                    "MAG": "3170470779",
                    "ArXiv": "2103.15949",
                    "DBLP": "journals/corr/abs-2103-15949",
                    "ACL": "2021.deelio-1.1",
                    "DOI": "10.18653/V1/2021.DEELIO-1.1",
                    "CorpusId": 232417301
                },
                "corpusId": 232417301,
                "publicationVenue": {
                    "id": "bc50a9b5-eb11-4d2b-a7d8-3933854a3273",
                    "name": "Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out",
                    "type": "conference",
                    "alternate_names": [
                        "DeeLIO",
                        "Workshop Knowl Extr Integr Deep Learn Archit Deep Learn Inside"
                    ],
                    "url": "https://aclanthology.org/venues/deelio/"
                },
                "url": "https://www.semanticscholar.org/paper/7cc88a1a904e8bb6edc1123c0800d1c5a0ea435d",
                "title": "Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors",
                "abstract": "Transformer networks have revolutionized NLP representation learning since they were introduced. Though a great effort has been made to explain the representation in transformers, it is widely recognized that our understanding is not sufficient. One important reason is that there lack enough visualization tools for detailed analysis. In this paper, we propose to use dictionary learning to open up these \u2018black boxes\u2019 as linear superpositions of transformer factors. Through visualization, we demonstrate the hierarchical semantic structures captured by the transformer factors, e.g., word-level polysemy disambiguation, sentence-level pattern formation, and long-range dependency. While some of these patterns confirm the conventional prior linguistic knowledge, the rest are relatively unexpected, which may provide new insights. We hope this visualization tool can bring further knowledge and a better understanding of how transformer networks work. The code is available at: https://github.com/zeyuyun1/TransformerVis.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2061343349",
                        "name": "Zeyu Yun"
                    },
                    {
                        "authorId": "2109184424",
                        "name": "Yubei Chen"
                    },
                    {
                        "authorId": "1708655",
                        "name": "B. Olshausen"
                    },
                    {
                        "authorId": "1688882",
                        "name": "Yann LeCun"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "As well, these ideas have also been used in Vision Transformers [48] for explaining image classification models [34, 185] or bi-modal transformer models [33].",
                "Furthermore, extended LRPs [34, 169] can be helpful to interpret Transformer models [45, 48, 159]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3d9f067d97cf21f3b0c4d406ccff98b06abafb5c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-10689",
                    "ArXiv": "2103.10689",
                    "DOI": "10.1007/s10115-022-01756-8",
                    "CorpusId": 232290756
                },
                "corpusId": 232290756,
                "publicationVenue": {
                    "id": "1f55639d-134e-44ae-b050-ccf2a6676bc5",
                    "name": "Knowledge and Information Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Inf Syst"
                    ],
                    "issn": "0219-3116",
                    "url": "https://link.springer.com/journal/10115"
                },
                "url": "https://www.semanticscholar.org/paper/3d9f067d97cf21f3b0c4d406ccff98b06abafb5c",
                "title": "Interpretable deep learning: interpretation, interpretability, trustworthiness, and beyond",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48568841",
                        "name": "Xuhong Li"
                    },
                    {
                        "authorId": "40518823",
                        "name": "Haoyi Xiong"
                    },
                    {
                        "authorId": "2155445773",
                        "name": "Xingjian Li"
                    },
                    {
                        "authorId": "2117921638",
                        "name": "Xuanyu Wu"
                    },
                    {
                        "authorId": "2115476207",
                        "name": "Xiao Zhang"
                    },
                    {
                        "authorId": "2118971193",
                        "name": "Ji Liu"
                    },
                    {
                        "authorId": "2143957850",
                        "name": "Jiang Bian"
                    },
                    {
                        "authorId": "1721158",
                        "name": "D. Dou"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Different from other methods relying on attention maps or heuristic propagation of attention, the method proposed by [3] assigns local relevance with deep Taylor decomposition, and propagate the local relevance throughout the layers."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7e88c31dde15e2f6fb40147b3d7bee86d8431570",
                "externalIds": {
                    "ArXiv": "2103.07055",
                    "DBLP": "journals/corr/abs-2103-07055",
                    "CorpusId": 232222257
                },
                "corpusId": 232222257,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7e88c31dde15e2f6fb40147b3d7bee86d8431570",
                "title": "Vision Transformer for COVID-19 CXR Diagnosis using Chest X-ray Feature Corpus",
                "abstract": "Under the global COVID-19 crisis, developing robust diagnosis algorithm for COVID-19 using CXR is hampered by the lack of the well-curated COVID-19 data set, although CXR data with other disease are abundant. This situation is suitable for vision transformer architecture that can exploit the abundant unlabeled data using pre-training. However, the direct use of existing vision transformer that uses the corpus generated by the ResNet is not optimal for correct feature embedding. To mitigate this problem, we propose a novel vision Transformer by using the low-level CXR feature corpus that are obtained to extract the abnormal CXR features. Specifically, the backbone network is trained using large public datasets to obtain the abnormal features in routine diagnosis such as consolidation, glass-grass opacity (GGO), etc. Then, the embedded features from the backbone network are used as corpus for vision transformer training. We examine our model on various external test datasets acquired from totally different institutions to assess the generalization ability. Our experiments demonstrate that our method achieved the state-of-art performance and has better generalization capability, which are crucial for a widespread deployment.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2107948510",
                        "name": "Sangjoon Park"
                    },
                    {
                        "authorId": "2109334279",
                        "name": "Gwanghyun Kim"
                    },
                    {
                        "authorId": "50048593",
                        "name": "Y. Oh"
                    },
                    {
                        "authorId": "46844846",
                        "name": "J. Seo"
                    },
                    {
                        "authorId": "40398599",
                        "name": "Sang Min Lee"
                    },
                    {
                        "authorId": "2152674480",
                        "name": "Jin Hwan Kim"
                    },
                    {
                        "authorId": "2113900807",
                        "name": "Sungjun Moon"
                    },
                    {
                        "authorId": "66190703",
                        "name": "Jae-Kwang Lim"
                    },
                    {
                        "authorId": "2998762",
                        "name": "J. C. Ye"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Fong & Vedaldi (2017) introduce a method for explaining classifiers based on meaningful perturbation and Chefer et al. (2021) introduce a method for improving interpretation for transformer-based classifiers."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4922e89201273b4040cfa5c90a5ab2906d725146",
                "externalIds": {
                    "ArXiv": "2103.00370",
                    "DBLP": "conf/iclr/HamiltonLFZF22",
                    "CorpusId": 239024909
                },
                "corpusId": 239024909,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4922e89201273b4040cfa5c90a5ab2906d725146",
                "title": "Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning",
                "abstract": "Visual search, recommendation, and contrastive similarity learning power technologies that impact billions of users worldwide. Modern model architectures can be complex and difficult to interpret, and there are several competing techniques one can use to explain a search engine's behavior. We show that the theory of fair credit assignment provides a $\\textit{unique}$ axiomatic solution that generalizes several existing recommendation- and metric-explainability techniques in the literature. Using this formalism, we show when existing approaches violate\"fairness\"and derive methods that sidestep these shortcomings and naturally handle counterfactual information. More specifically, we show existing approaches implicitly approximate second-order Shapley-Taylor indices and extend CAM, GradCAM, LIME, SHAP, SBSM, and other methods to search engines. These extensions can extract pairwise correspondences between images from trained $\\textit{opaque-box}$ models. We also introduce a fast kernel-based method for estimating Shapley-Taylor indices that require orders of magnitude fewer function evaluations to converge. Finally, we show that these game-theoretic measures yield more consistent explanations for image similarity architectures.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47359380",
                        "name": "Mark Hamilton"
                    },
                    {
                        "authorId": "23451726",
                        "name": "Scott M. Lundberg"
                    },
                    {
                        "authorId": "2152837332",
                        "name": "Lei Zhang"
                    },
                    {
                        "authorId": "2072785288",
                        "name": "Stephanie Fu"
                    },
                    {
                        "authorId": "1768236",
                        "name": "W. Freeman"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d96c0d6c609f460f6761fe56113a461be719989b",
                "externalIds": {
                    "ArXiv": "2102.07824",
                    "DBLP": "conf/aaai/NaimanA23",
                    "DOI": "10.1609/aaai.v37i8.26111",
                    "CorpusId": 257102309
                },
                "corpusId": 257102309,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/d96c0d6c609f460f6761fe56113a461be719989b",
                "title": "An Operator Theoretic Approach for Analyzing Sequence Neural Networks",
                "abstract": "Analyzing the inner mechanisms of deep neural networks is a fundamental task in machine learning. Existing work provides limited analysis or it depends on local theories, such as fixed-point analysis. In contrast, we propose to analyze trained neural networks using an operator theoretic approach which is rooted in Koopman theory, the Koopman Analysis of Neural Networks (KANN). Key to our method is the Koopman operator, which is a linear object that globally represents the dominant behavior of the network dynamics. The linearity of the Koopman operator facilitates analysis via its eigenvectors and eigenvalues. Our method reveals that the latter eigendecomposition holds semantic information related to the neural network inner workings. For instance, the eigenvectors highlight positive and negative n-grams in the sentiments analysis task; similarly, the eigenvectors capture the salient features of healthy heart beat signals in the ECG classification problem.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "65787565",
                        "name": "Ilana D Naiman"
                    },
                    {
                        "authorId": "1927247",
                        "name": "Omri Azencot"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a0aaadf96cd5b5477aac4e4a8c34139b58785bf5",
                "externalIds": {
                    "ArXiv": "2108.01210",
                    "DBLP": "journals/corr/abs-2108-01210",
                    "DOI": "10.51628/001c.27358",
                    "CorpusId": 231714114
                },
                "corpusId": 231714114,
                "publicationVenue": {
                    "id": "027ffd21-ebb0-4af8-baf5-911124292fd0",
                    "name": "bioRxiv",
                    "type": "journal",
                    "url": "http://biorxiv.org/"
                },
                "url": "https://www.semanticscholar.org/paper/a0aaadf96cd5b5477aac4e4a8c34139b58785bf5",
                "title": "Representation learning for neural population activity with Neural Data Transformers",
                "abstract": "Neural population activity is theorized to reflect an underlying dynamical structure. This structure can be accurately captured using state space models with explicit dynamics, such as those based on recurrent neural networks (RNNs). However, using recurrence to explicitly model dynamics necessitates sequential processing of data, slowing real-time applications such as brain-computer interfaces. Here we introduce the Neural Data Transformer (NDT), a non-recurrent alternative. We test the NDT\u2019s ability to capture autonomous dynamical systems by applying it to synthetic datasets with known dynamics and data from monkey motor cortex during a reaching task well-modeled by RNNs. The NDT models these datasets as well as state-of-the-art recurrent models. Further, its non-recurrence enables 3.9ms inference, well within the loop time of real-time applications and more than 6 times faster than recurrent baselines on the monkey reaching dataset. These results suggest that an explicit dynamics model is not necessary to model autonomous neural population dynamics. Code github.com/snel-repo/neural-data-transformers.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1801888368",
                        "name": "Joel Ye"
                    },
                    {
                        "authorId": "2231933",
                        "name": "C. Pandarinath"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3a906b77fa218adc171fecb28bb81c24c14dcc7b",
                "externalIds": {
                    "DBLP": "journals/csur/KhanNHZKS22",
                    "ArXiv": "2101.01169",
                    "DOI": "10.1145/3505244",
                    "CorpusId": 230435805
                },
                "corpusId": 230435805,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3a906b77fa218adc171fecb28bb81c24c14dcc7b",
                "title": "Transformers in Vision: A Survey",
                "abstract": "Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks, e.g., Long short-term memory. Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text, and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization), and three-dimensional analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges toward the application of transformer models in computer vision.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "152973423",
                        "name": "Salman Hameed Khan"
                    },
                    {
                        "authorId": "40894826",
                        "name": "Muzammal Naseer"
                    },
                    {
                        "authorId": "145684318",
                        "name": "Munawar Hayat"
                    },
                    {
                        "authorId": "3323621",
                        "name": "Syed Waqas Zamir"
                    },
                    {
                        "authorId": "2358803",
                        "name": "F. Khan"
                    },
                    {
                        "authorId": "145103012",
                        "name": "M. Shah"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "We also notice a recent paper [10] that develops LRPbased [2] method to compute relevance to explain the predictions of Transformer."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6c9710487ffe7161bf5a7b4e436ee6a76fc114cf",
                "externalIds": {
                    "ArXiv": "2012.14214",
                    "DBLP": "conf/iccv/YangQNY21",
                    "DOI": "10.1109/ICCV48922.2021.01159",
                    "CorpusId": 236428790
                },
                "corpusId": 236428790,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/6c9710487ffe7161bf5a7b4e436ee6a76fc114cf",
                "title": "TransPose: Keypoint Localization via Transformer",
                "abstract": "While CNN-based models have made remarkable progress on human pose estimation, what spatial dependencies they capture to localize keypoints remains unclear. In this work, we propose a model called Trans-Pose, which introduces Transformer for human pose estimation. The attention layers built in Transformer enable our model to capture long-range relationships efficiently and also can reveal what dependencies the predicted key-points rely on. To predict keypoint heatmaps, the last attention layer acts as an aggregator, which collects contributions from image clues and forms maximum positions of keypoints. Such a heatmap-based localization approach via Transformer conforms to the principle of Activation Maximization [19]. And the revealed dependencies are image-specific and fine-grained, which also can provide evidence of how the model handles special cases, e.g., occlusion. The experiments show that TransPose achieves 75.8 AP and 75.0 AP on COCO validation and test-dev sets, while being more lightweight and faster than mainstream CNN architectures. The TransPose model also transfers very well on MPII benchmark, achieving superior performance on the test set when fine-tuned with small training costs. Code and pre-trained models are publicly available1.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2115257611",
                        "name": "Sen Yang"
                    },
                    {
                        "authorId": "2007084",
                        "name": "Zhibin Quan"
                    },
                    {
                        "authorId": "2054208112",
                        "name": "Mu Nie"
                    },
                    {
                        "authorId": "2345507",
                        "name": "Wankou Yang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The current literature usually analyzes the effect in an intuitive way [55], [23]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d40c77c010c8dbef6142903a02f2a73a85012d5d",
                "externalIds": {
                    "DBLP": "journals/pami/00020C0GLTXXXYZ23",
                    "ArXiv": "2012.12556",
                    "DOI": "10.1109/TPAMI.2022.3152247",
                    "CorpusId": 236986986,
                    "PubMed": "35180075"
                },
                "corpusId": 236986986,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d40c77c010c8dbef6142903a02f2a73a85012d5d",
                "title": "A Survey on Vision Transformer",
                "abstract": "Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3826388",
                        "name": "Kai Han"
                    },
                    {
                        "authorId": "2108702980",
                        "name": "Yunhe Wang"
                    },
                    {
                        "authorId": "2118023932",
                        "name": "Hanting Chen"
                    },
                    {
                        "authorId": "1736061",
                        "name": "Xinghao Chen"
                    },
                    {
                        "authorId": "2148899357",
                        "name": "Jianyuan Guo"
                    },
                    {
                        "authorId": "2125024057",
                        "name": "Zhenhua Liu"
                    },
                    {
                        "authorId": "103603255",
                        "name": "Yehui Tang"
                    },
                    {
                        "authorId": "1569696821",
                        "name": "An Xiao"
                    },
                    {
                        "authorId": "1691522",
                        "name": "Chunjing Xu"
                    },
                    {
                        "authorId": "2127897462",
                        "name": "Yixing Xu"
                    },
                    {
                        "authorId": "2116369043",
                        "name": "Zhaohui Yang"
                    },
                    {
                        "authorId": "2108440680",
                        "name": "Yiman Zhang"
                    },
                    {
                        "authorId": "143719920",
                        "name": "D. Tao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "In [9], the authors proposed a way to visualize the relevancy maps for Transformer networks."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "15adfb10072ab06d1621c90f9ce844bd1d9b23cd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2011-08019",
                    "ArXiv": "2011.08019",
                    "MAG": "3103670834",
                    "DOI": "10.1109/IJCB52358.2021.9484333",
                    "CorpusId": 226965093
                },
                "corpusId": 226965093,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/15adfb10072ab06d1621c90f9ce844bd1d9b23cd",
                "title": "On the Effectiveness of Vision Transformers for Zero-shot Face Anti-Spoofing",
                "abstract": "The vulnerability of face recognition systems to presentation attacks has limited their application in security-critical scenarios. Automatic methods of detecting such malicious attempts are essential for the safe use of facial recognition technology. Although various methods have been suggested for detecting such attacks, most of them over-fit the training set and fail in generalizing to unseen attacks and environments. In this work, we use transfer learning from the vision transformer model for the zero-shot anti-spoofing task. The effectiveness of the proposed approach is demonstrated through experiments in publicly available datasets. The proposed approach outperforms the state-of-the-art methods in the zero-shot protocols in the HQ-WMCA and SiW-M datasets by a large margin. Besides, the model achieves a significant boost in cross-database performance as well.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3110004",
                        "name": "Anjith George"
                    },
                    {
                        "authorId": "145607451",
                        "name": "S. Marcel"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "Specifically, we compare the results using raw attention, CAM (Zhou et al., 2016), and Transformer-LRP (Chefer et al., 2021) in Table 4.",
                ", 2016), and Transformer-LRP (Chefer et al., 2021) in Table 4.",
                "To figure out how interpretability contributes to the model prediction, we adopt the positive and negative perturbation tests presented in (Chefer et al., 2021) to give both quantitative and qualitative evaluations on the interpretability.",
                "Decomposition-based approaches (Bach et al., 2015; Montavon et al., 2017; Chefer et al., 2021) propagate the final prediction to the input following the Deep Taylor Decomposition (Montavon et al., 2017).",
                "3 EVALUATION OF THE INTERPRETABILITY To figure out how interpretability contributes to the model prediction, we adopt the positive and negative perturbation tests presented in (Chefer et al., 2021) to give both quantitative and qualitative evaluations on the interpretability.",
                "Although Luo et al. (2016) propose to measure the ERF for CNNs, it cannot be directly implemented to Transformer-base models.",
                "Decomposition-based approaches (Bach et al., 2015; Montavon et al., 2017; Chefer et al., 2021) propagate the final prediction to the input following the Deep Taylor Decomposition (Montavon et al.",
                "As most previous methods focus on CNNs, Chefer et al. (2021) propose ViT-LRP tailored for vision Transformers."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "6f87dd6430559cd6783abde9384edb86357a028f",
                "externalIds": {
                    "CorpusId": 257497153
                },
                "corpusId": 257497153,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6f87dd6430559cd6783abde9384edb86357a028f",
                "title": "S CHEMA I NFERENCE FOR I NTERPRETABLE I MAGE C LASSIFICATION",
                "abstract": "In this paper, we study a novel inference paradigm, termed as schema inference, that learns to deductively infer the explainable predictions by rebuilding the prior deep neural network (DNN) forwarding scheme, guided by the prevalent philosophical cognitive concept of schema. We strive to reformulate the conventional model inference pipeline into a graph matching policy that associates the extracted visual concepts of an image with the pre-computed scene impression, by analogy with human reasoning mechanism via impression matching. To this end, we devise an elaborated architecture, termed as SchemaNet, as a dedicated instantiation of the proposed schema inference concept, that models both the visual semantics of input instances and the learned abstract imaginations of target categories as topological relational graphs. Meanwhile, to capture and leverage the compositional contributions of visual semantics in a global view, we also introduce a universal Feat2Graph scheme in SchemaNet to establish the relational graphs that contain abundant interaction information. Both the theoretical analysis and the experimental results on several benchmarks demonstrate that the proposed schema inference achieves encouraging performance and meanwhile yields a clear picture of the deductive process leading to the predictions. Our code is available at https://github.com/zhfeing/SchemaNet-PyTorch.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1739347431",
                        "name": "Haofei Zhang"
                    },
                    {
                        "authorId": "2065788410",
                        "name": "Mengqi Xue"
                    },
                    {
                        "authorId": "2109002553",
                        "name": "Xiaokang Liu"
                    },
                    {
                        "authorId": "38644044",
                        "name": "Kai Chen"
                    },
                    {
                        "authorId": "40403685",
                        "name": "Jie Song"
                    },
                    {
                        "authorId": "2152127912",
                        "name": "Mingli Song"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "The following six explanation methods are used as the baselines for a comparison in our experiments: Raw Attention Map (RAM), Attention Rollout (Abnar & Zuidema, 2020) (Rollout), Generic Attribution (Chefer et al., 2021b) (GA), GradCAM (Selvaraju et al., 2017) (CAM), Partial LRP (Voita et al., 2019) (PLRP) and Transformer Attribution (Chefer et al., 2021a) (TA).",
                "Some attention-based explanation methods also have been proposed for the Transformers (Michel et al., 2019; Abnar & Zuidema, 2020; Chefer et al., 2021a;b; Hao et al., 2021), despite the disputation about the legitimacy of attentions being an explanation (Jain & Wallace, 2019; Wiegreffe & Pinter,\u2026",
                "Transformer attribution (Chefer et al., 2021a) (TA) Transformer attribution method is a state-of-the-art class-specific explanation method for Transformer.",
                "Generic Attribution (Chefer et al., 2021b) generalizes the idea of Rollout and adds the gradient information to each attention map, while Transformer Attribution (Chefer et al., 2021a) exploits LRP (Binder et al., 2016) and gradients together for getting the explanations.",
                "Generic attribution (Chefer et al., 2021b) (GA) Generic attribution extends the usage of Transformer attribution to co-attention and self-attention based models, such as VisualBERT, LXMERT etc. and propose a more generic relevancy update rule.",
                "\u2026a comparison in our experiments: Raw Attention Map (RAM), Attention Rollout (Abnar & Zuidema, 2020) (Rollout), Generic Attribution (Chefer et al., 2021b) (GA), GradCAM (Selvaraju et al., 2017) (CAM), Partial LRP (Voita et al., 2019) (PLRP) and Transformer Attribution (Chefer et al., 2021a) (TA).",
                "We show in Figure 1 some visualizations with Generic Attribution to different types of Vision Transformers (Dosovitskiy et al., 2021; He et al., 2022).",
                "Following previous works (Abnar & Zuidema, 2020; Chefer et al., 2021a;b; Samek et al., 2017; Vu et al., 2019; DeYoung et al., 2020), we prepare three types of tests for the trustworthiness evaluation:\nPerturbation Tests gradually mask out the tokens of input according to the explanation results and\u2026",
                "\u2026baselines for a comparison in our experiments: Raw Attention Map (RAM), Attention Rollout (Abnar & Zuidema, 2020) (Rollout), Generic Attribution (Chefer et al., 2021b) (GA), GradCAM (Selvaraju et al., 2017) (CAM), Partial LRP (Voita et al., 2019) (PLRP) and Transformer Attribution (Chefer et\u2026",
                "Transformer\nAttribution (Chefer et al., 2021a) and Generic Attribution (Chefer et al., 2021b) combine the gradients with layer-wise relevance propagation (Binder et al., 2016) (LRP) or attention maps along a rolling out path, and eliminate the negative components in each attention block.",
                "Our approach outperforms other strong baselines (e.g., (Abnar & Zuidema, 2020; Chefer et al., 2021a;b)) through quantitative metrics and qualitative visualizations, and shows better applicability to various settings.",
                "Following the work of Chefer et al. (2021a), we use a weighted gradient map of the last attention block, which corresponds to the [CLS] token ."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "7920371e6b296460183b00f8d31796a5664db582",
                "externalIds": {
                    "DBLP": "journals/tmlr/ChenLYDX23",
                    "CorpusId": 253082859
                },
                "corpusId": 253082859,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7920371e6b296460183b00f8d31796a5664db582",
                "title": "Beyond Intuition: Rethinking Token Attributions inside Transformers",
                "abstract": "The multi-head attention mechanism, or rather the Transformer-based models have always been under the spotlight, not only in the domain of text processing, but also for computer vision. Several works have recently been proposed around exploring the token attributions along the intrinsic decision process. However, the ambiguity of the expression formulation can lead to an accumulation of error, which makes the interpretation less trustworthy and less applicable to different variants. In this work, we propose a novel method to approximate token contributions inside Transformers. We start from the partial derivative to each token, divide the interpretation process into attention perception and reasoning feedback with the chain rule and explore each part individually with explicit mathematical derivations. In attention perception, we propose the head-wise and token-wise approximations in order to learn how the tokens interact to form the pooled vector. As for reasoning feedback, we adopt a noise-decreasing strategy by applying the integrated gradients to the last attention map. Our method is further validated qualitatively and quantitatively through the faithfulness evaluations across different settings: single modality (BERT and ViT) and bi-modality (CLIP), different model sizes (ViT-L) and different pooling strategies (ViT-MAE) to demonstrate the broad applicability and clear improvements over existing methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2115328887",
                        "name": "Jiamin Chen"
                    },
                    {
                        "authorId": "48568841",
                        "name": "Xuhong Li"
                    },
                    {
                        "authorId": "2109352263",
                        "name": "Lei Yu"
                    },
                    {
                        "authorId": "1721158",
                        "name": "D. Dou"
                    },
                    {
                        "authorId": "72795417",
                        "name": "H. Xiong"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "\u2026especially popular in computer vision, the more advanced methods have been successfully implemented and validated for transformer models for NLP by Chefer et al. (2021), who showed that their improved implementation of LRP gives better classspecific explanations compared to roll-out because the\u2026",
                "Further mentions of LRP in this paper follow the implementation of Chefer et al. (2021).",
                "This implementation (Chefer et al., 2021)\n2https://github.com/INK-USC/DIG 3https://github.com/cdpierse/transformers-interpret 4https://github.com/hila-chefer/Transformer-\nExplainability 5For each text, we use the explanation for unseen data, meaning that the attributions were generated with the\u2026"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "b45e5b84fe14a0fffef7f6254a57b5f0116ab7e7",
                "externalIds": {
                    "ACL": "2023.wassa-1.28",
                    "DBLP": "conf/wassa/MaladryLHH23",
                    "DOI": "10.18653/v1/2023.wassa-1.28",
                    "CorpusId": 260063038
                },
                "corpusId": 260063038,
                "publicationVenue": {
                    "id": "322bf662-0263-48b5-bf26-03b114a182dc",
                    "name": "Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",
                    "type": "conference",
                    "alternate_names": [
                        "WASSA",
                        "Workshop Comput Approach Subj Sentim Soc Media Anal"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b45e5b84fe14a0fffef7f6254a57b5f0116ab7e7",
                "title": "A Fine Line Between Irony and Sincerity: Identifying Bias in Transformer Models for Irony Detection",
                "abstract": "In this paper we investigate potential bias in fine-tuned transformer models for irony detection. Bias is defined in this research as spurious associations between word n-grams and class labels, that can cause the system to rely too much on superficial cues and miss the essence of the irony. For this purpose, we looked for correlations between class labels and words that are prone to trigger irony, such as positive adjectives, intensifiers and topical nouns. Additionally, we investigate our irony model\u2019s predictions before and after manipulating the data set through irony trigger replacements. We further support these insights with state-of-the-art explainability techniques (Layer Integrated Gradients, Discretized Integrated Gradients and Layer-wise Relevance Propagation). Both approaches confirm the hypothesis that transformer models generally encode correlations between positive sentiments and ironic texts, with even higher correlations between vividly expressed sentiment and irony. Based on these insights, we implemented a number of modification strategies to enhance the robustness of our irony classifier.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2165226292",
                        "name": "Aaron Maladry"
                    },
                    {
                        "authorId": "2846808",
                        "name": "Els Lefever"
                    },
                    {
                        "authorId": "2165226266",
                        "name": "Cynthia Van Hee"
                    },
                    {
                        "authorId": "2653729",
                        "name": "Veronique Hoste"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fddbfdb795cccf12aa9ca51398a04e305a0fb89b",
                "externalIds": {
                    "DBLP": "journals/tifs/ZhaoWHCLT23",
                    "DOI": "10.1109/TIFS.2023.3239223",
                    "CorpusId": 256223372
                },
                "corpusId": 256223372,
                "publicationVenue": {
                    "id": "d406a3f4-dc05-43be-b1f6-812f29de9c0e",
                    "name": "IEEE Transactions on Information Forensics and Security",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Inf Forensics Secur"
                    ],
                    "issn": "1556-6013",
                    "url": "http://www.ieee.org/organizations/society/sp/tifs.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=10206",
                        "http://www.signalprocessingsociety.org/publications/periodicals/forensics/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fddbfdb795cccf12aa9ca51398a04e305a0fb89b",
                "title": "ISTVT: Interpretable Spatial-Temporal Video Transformer for Deepfake Detection",
                "abstract": "With the rapid development of Deepfake synthesis technology, our information security and personal privacy have been severely threatened in recent years. To achieve a robust Deepfake detection, researchers attempt to exploit the joint spatial-temporal information in the videos, like using recurrent networks and 3D convolutional networks. However, these spatial-temporal models remain room to improve. Another general challenge for spatial-temporal models is that people do not clearly understand what these spatial-temporal models really learn. To address these two challenges, in this paper, we propose an Interpretable Spatial-Temporal Video Transformer (ISTVT), which consists of a novel decomposed spatial-temporal self-attention and a self-subtract mechanism to capture spatial artifacts and temporal inconsistency for robust Deepfake detection. Thanks to this decomposition, we propose to interpret ISTVT by visualizing the discriminative regions for both spatial and temporal dimensions via the relevance (the pixel-wise importance on the input) propagation algorithm. We conduct extensive experiments on large-scale datasets, including FaceForensics++, FaceShifter, DeeperForensics, Celeb-DF, and DFDC datasets. Our strong performance of intra-dataset and cross-dataset Deepfake detection demonstrates the effectiveness and robustness of our method, and our visualization-based interpretability offers people insights into our model.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2050550",
                        "name": "Cairong Zhao"
                    },
                    {
                        "authorId": "2182968724",
                        "name": "Chutian Wang"
                    },
                    {
                        "authorId": "2189742472",
                        "name": "Guosheng Hu"
                    },
                    {
                        "authorId": "2035571489",
                        "name": "Haonan Chen"
                    },
                    {
                        "authorId": "2202745150",
                        "name": "Chun Liu"
                    },
                    {
                        "authorId": "2192672187",
                        "name": "Jinhui Tang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "and restrain unimportant features [14]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "750fd14cb82784c82b8741b3532ef342453fc75c",
                "externalIds": {
                    "DBLP": "journals/tim/MaHWYWQ23",
                    "DOI": "10.1109/TIM.2023.3244798",
                    "CorpusId": 256884433
                },
                "corpusId": 256884433,
                "publicationVenue": {
                    "id": "3edbd5e0-8799-420a-9ca8-b35c646c354f",
                    "name": "IEEE Transactions on Instrumentation and Measurement",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Instrum Meas"
                    ],
                    "issn": "0018-9456",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=19",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=19"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/750fd14cb82784c82b8741b3532ef342453fc75c",
                "title": "Automatic Modulation Classification in Impulsive Noise: Hyperbolic-Tangent Cyclic Spectrum and Multibranch Attention Shuffle Network",
                "abstract": "Automatic modulation classification plays an essential role in cognitive communication systems. Traditional automatic modulation classification approaches are primarily developed under Gaussian noise assumptions. Nevertheless, recent empirical studies show that impulsive noise has emerged in numerous wireless communication systems. The bursty nature of impulsive noise fundamentally challenges the applicability of the traditional automatic modulation classification approaches. To accurately identify the modulation schemes in impulsive noise environment, in this article, we propose a novel modulation classification approach through using hyperbolic-tangent cyclic spectrum and multibranch attention shuffle neural networks. First, based on the designed hyperbolic-tangent autocorrelation function, hyperbolic-tangent cyclic spectrum is proposed to effectively suppress the impulsive noise and extract the discriminating features. Then, based on the hyperbolic-tangent cyclic spectrum, a novel deep shuffle neural network is proposed as a classifier to perform the modulation classification through the multibranch attention mechanism to reweight all the features. Both the numerical and real-data experimental results demonstrate that the proposed algorithm can correctly classify modulation schemes with high accuracy and robustness in impulsive noise.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "40790369",
                        "name": "Jitong Ma"
                    },
                    {
                        "authorId": "2208490053",
                        "name": "Mutian Hu"
                    },
                    {
                        "authorId": "2118914265",
                        "name": "Tianyu Wang"
                    },
                    {
                        "authorId": "48598660",
                        "name": "Zhengyan Yang"
                    },
                    {
                        "authorId": "144652877",
                        "name": "Liangtian Wan"
                    },
                    {
                        "authorId": "145986209",
                        "name": "T. Qiu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "845721156d35f472152c749c0afcb2ee15dfe44f",
                "externalIds": {
                    "CorpusId": 259267280
                },
                "corpusId": 259267280,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/845721156d35f472152c749c0afcb2ee15dfe44f",
                "title": "Activation Sparsity: An Insight into the Interpretability of Trained Transformers",
                "abstract": "Transformer models have demonstrated outstanding performance in natural language processing tasks. However, their lack of interpretability remains a persistent challenge. In October 2022, it was discovered that transformer models exhibit activation sparsity, which can aid in improving their interpretability. Activation sparsity encourages certain neurons in the network to remain inactive, promoting a sparse representation of input data and identifying the most important input features for the model\u2019s predictions. Building on this finding, our project investigates the impact of activation sparsity on the interpretability of transformer models. Our research addresses two primary questions: (i) what activation sparsity in transformer architecture represents, and (ii) what impact activation sparsity has on interpretability. We propose modifying the transformer architecture that integrates activation sparsity into the self-attention mechanism. Our experimental evaluation using Hugging Face\u2019s T5 encoder-decoder model on various supervised learning tasks demonstrates that our modified transformer model significantly improves interpretability while maintaining comparable performance on standard evaluation metrics. We also observe that activation sparsity has a more significant impact on interpretability in complex tasks like machine translation. Our findings have important implications for the design of transformer models, highlighting the potential benefits of incorporating activation sparsity to improve interpretability. 1 Key Information to include \u2022 Mentor: Abhinav Garg (External Collaborators: N/A Sharing project: N/A)",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220684813",
                        "name": "Anushree Aggarwal"
                    },
                    {
                        "authorId": "2220692039",
                        "name": "Carolyn Akua Asante Dartey"
                    },
                    {
                        "authorId": "2220691133",
                        "name": "Jaime Eli Mizrachi"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "79dd9e62f0e3ba35ce54d98b32750933e91a972a",
                "externalIds": {
                    "CorpusId": 259282776
                },
                "corpusId": 259282776,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/79dd9e62f0e3ba35ce54d98b32750933e91a972a",
                "title": "Representations and Computations in Transformers that Support Generalization on Structured Tasks",
                "abstract": "Transformers have shown remarkable success in natural language processing and computer vision, serving as the foundation of large language and multimodal models. These networks can capture nuanced context sensitivity across high-dimensional language tokens or image pixels, but it remains unclear how highly structured behavior and systematic generalization can arise in these systems. Here, we explore the solution process a causal transformer discovers as it learns to solve a set of algorithmic tasks involving copying, sorting, and hierarchical compositions of these operations. We search for the minimal layer and head configuration sufficient to solve these tasks and unpack the roles of the attention heads, as well as how token representations are reweighted across layers to complement these roles. Our results provide new insights into how attention layers in transformers support structured computation within and across tasks: 1) Replacing fixed position labels with labels sampled from a larger set enables strong length generalization and faster learning. The learnable embeddings of these labels develop different representations, capturing sequence order if necessary, depending on task demand. 2) Two-layer transformers can learn reliable solutions to the multi-level problems we explore. The first layer tends to transform the input representation to allow the second layer to share computation across repeated components within a task or across related tasks. 3) We introduce an analysis pipeline that quantifies how the representation space in a given layer prioritizes different aspects of each item. We show that these representations prioritize information needed to guide attention relative to information that only requires downstream readout.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144435606",
                        "name": "Yuxuan Li"
                    },
                    {
                        "authorId": "1701656",
                        "name": "James L. McClelland"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "af1d28eb31384b2b70e8600e9a2146be67072761",
                "externalIds": {
                    "DBLP": "journals/tgrs/SalazarMOP23",
                    "DOI": "10.1109/TGRS.2023.3285820",
                    "CorpusId": 259299646
                },
                "corpusId": 259299646,
                "publicationVenue": {
                    "id": "70628d6a-97aa-4571-9701-bc0eb3989c32",
                    "name": "IEEE Transactions on Geoscience and Remote Sensing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Geosci Remote Sens"
                    ],
                    "issn": "0196-2892",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=36",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/af1d28eb31384b2b70e8600e9a2146be67072761",
                "title": "Self-Supervised Learning for Seismic Data: Enhancing Model Interpretability With Seismic Attributes",
                "abstract": "Deep learning (DL) has shown great potential in geosciences, such as seismic data processing and interpretation, improving decision-making and reducing analysis time. However, DL faces two main challenges. First, many DL models rely on labeled data, which can be time-consuming to obtain. Second, the predictions from these models often lack interpretability, making it difficult to use them for high-value decisions. To address these limitations, we propose a novel workflow that eliminates the need for labeled data and enables interpretation of the results, highlighting key geological features. The proposed workflow trains a vision transformer (ViT) to produce six attention maps, focusing on diverse and relevant regions, by assigning higher attention values. We first train the ViT using a modified distillation with no labels (DINO) method specifically designed for the seismic domain and monitor for overfitting. Then, to evaluate the focus of each attention head (AH), we use nine seismic attributes as predictor features for the assigned attention using a gradient boosting model. Finally, the method samples the seismic attributes in stationary regions of the attention maps and calculates Shapley additive explanations (SHAP) values to determine the most impactful attributes on the attention prediction. Each AH can concentrate on unique geological features of the input seismic image, as indicated by different relationships between SHAP values and seismic attributes. Additionally, regardless of location, each AH can detect the same geologically significant pattern based on the attributes used. The proposed workflow enables the interpretability of the model\u2019s importance, guided by expert knowledge through seismic attributes.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2118306851",
                        "name": "Jose J. Salazar"
                    },
                    {
                        "authorId": "2121055376",
                        "name": "Eduardo Maldonado-Cruz"
                    },
                    {
                        "authorId": "2214248295",
                        "name": "Jesus Ochoa"
                    },
                    {
                        "authorId": "2127628",
                        "name": "M. Pyrcz"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "\u2026of fast-evolving and well-studied computer vision techniques in the time series domain, such as better model architecture (Liu et al., 2022), data augmentation (Shorten & Khoshgoftaar, 2019), interpretability (Chefer et al., 2021), self-supervised learning (He et al., 2022), to name a few.",
                ", 2022), data augmentation (Shorten & Khoshgoftaar, 2019), interpretability (Chefer et al., 2021), self-supervised learning (He et al."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "97e24e67aa8a4254f1778b8e469fcc3d9bb73425",
                "externalIds": {
                    "CorpusId": 259311557
                },
                "corpusId": 259311557,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/97e24e67aa8a4254f1778b8e469fcc3d9bb73425",
                "title": "IRREGULARLY SAMPLED TIME SERIES",
                "abstract": "Irregularly sampled time series are becoming increasingly prevalent in various domains, especially in medical applications. Although different highly-customized methods have been proposed to tackle irregularity, how to effectively model their complicated dynamics and high sparsity is still an open problem. This paper studies the problem from a whole new perspective: transforming irregularly sampled time series into line graph images and adapting powerful vision transformers to perform time series classification in the same way as image classification. Our approach largely simplifies algorithm designs without assuming prior knowledge and can be potentially extended as a general-purpose framework. Despite its simplicity, we show that it substantially outperforms state-of-the-art specialized algorithms on several popular healthcare and human activity datasets. Our code and data are available at https://github.com/Leezekun/ViTST.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2168519132",
                        "name": "Zekun Li"
                    },
                    {
                        "authorId": "50341591",
                        "name": "SHIYANG LI"
                    },
                    {
                        "authorId": "1740249",
                        "name": "Xifeng Yan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9662c5784d20927c5e9404a102128a0394dfaa02",
                "externalIds": {
                    "DOI": "10.14428/esann/2023.es2023-152",
                    "CorpusId": 262050835
                },
                "corpusId": 262050835,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9662c5784d20927c5e9404a102128a0394dfaa02",
                "title": "Fine-tuning is not (always) overfitting artifacts",
                "abstract": ". Since their release, transformers, and in particular fine-tuned transformers are widely used for text-related classification tasks. However, only a few studies try to understand how fine-tuning actually works and existing alternatives, such as feature-based transformers, are often overlooked. In this work, we study a French transformer model, Camem-BERT, to compare the fine-tuned and feature-based approaches in terms of their performances, interpretability and embedding space. We observe that while fine-tuning has a limited impact on performances in our case study, it significantly affects the intepretability (by better isolating words that are intuitively connected to the classification task) and embedding space (by summarizing the majority of the relevant information into a fewer dimensions) of the results. We conclude by highlighting open questions regarding the generalization potential of fine-tuned embeddings.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2098835202",
                        "name": "J\u00e9r\u00e9mie Bogaert"
                    },
                    {
                        "authorId": "2199751772",
                        "name": "Emmanuel Jean"
                    },
                    {
                        "authorId": "2243211023",
                        "name": "Cyril de Bodt"
                    },
                    {
                        "authorId": "1706533",
                        "name": "Fran\u00e7ois-Xavier Standaert"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "In the future, more advanced approaches such as [29], and [30], can be incorporated."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "48285421bce66b4720aa925347e508d0ca30f370",
                "externalIds": {
                    "DOI": "10.54364/aaiml.2023.1181",
                    "CorpusId": 263683004
                },
                "corpusId": 263683004,
                "publicationVenue": {
                    "id": "860ccb2d-f6df-4923-bb38-3649a5f7740c",
                    "name": "Advances in Artificial Intelligence and Machine Learning",
                    "alternate_names": [
                        "Adv Artif Intell Mach Learn"
                    ],
                    "issn": "2582-9793"
                },
                "url": "https://www.semanticscholar.org/paper/48285421bce66b4720aa925347e508d0ca30f370",
                "title": "Faster, Stronger, and More Interpretable: Massive Transformer Architectures for Vision-Language Tasks",
                "abstract": "Abstract",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2254866835",
                        "name": "Tong Chen"
                    },
                    {
                        "authorId": "2254478687",
                        "name": "Sicong Liu"
                    },
                    {
                        "authorId": "2254660275",
                        "name": "Zhiran Chen"
                    },
                    {
                        "authorId": "2255434567",
                        "name": "Wenyan Hu"
                    },
                    {
                        "authorId": "2222540406",
                        "name": "Dachi Chen"
                    },
                    {
                        "authorId": "2255194849",
                        "name": "Yuanxin Wang"
                    },
                    {
                        "authorId": "2254458281",
                        "name": "Qi Lyu"
                    },
                    {
                        "authorId": "2222592447",
                        "name": "Cindy X. Le"
                    },
                    {
                        "authorId": "2254866406",
                        "name": "Wenping Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "20dc76a99ce75de708fe23e16980a3e585464eb0",
                "externalIds": {
                    "DBLP": "conf/acl/BrinnerZ23",
                    "DOI": "10.18653/v1/2023.findings-acl.867",
                    "CorpusId": 259859057
                },
                "corpusId": 259859057,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/20dc76a99ce75de708fe23e16980a3e585464eb0",
                "title": "Model Interpretability and Rationale Extraction by Input Mask Optimization",
                "abstract": ",",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2200520053",
                        "name": "Marc Brinner"
                    },
                    {
                        "authorId": "1721364",
                        "name": "Sina Zarrie\u00df"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "contexts": [
                "Meanwhile, most Transformer explanation methods are post-hoc and focus on the vanilla ViT models in image classification [6].",
                "The clusters are semantically meaningful (with some corresponding object parts) and show comparable or qualitatively better results to the state-of-the-art Improved LRP [6].",
                "On the other hand, explaining trained ViT models requires nontrivial and sophisticated methods [6] following the trend of eXplainable AI (XAI) [18] that has been extensively studied with convolutional neural networks.",
                "Results of Rollout [1], raw attention, GradCAM [34], LRP [5], partial LRP [41] and the Improved LRP [6] are reproduced from [6], which all use the pretained ViT-B model [12].",
                "As pointed out in the Improved LRP [6], reducing the explanation to only the attentions scores may be myopic since many other components are ignored.",
                "There are mainly two categories of approaches: gradient based methods such as the GradCAM method [34] that is built on the CAM [59] and attribution based methods built on the deep Taylor decomposition framework [30] such as the Layerwise Relevance Propagation (LRP) method [5]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "8c4a4e33193dff9960a6c3f1b42ddf841ad3b702",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-11987",
                    "DOI": "10.48550/arXiv.2203.11987",
                    "CorpusId": 247618812
                },
                "corpusId": 247618812,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8c4a4e33193dff9960a6c3f1b42ddf841ad3b702",
                "title": "Learning Patch-to-Cluster Attention in Vision Transformer",
                "abstract": "The Vision Transformer (ViT) model is built on the assumption of treating image patches as \u201cvisual tokens\u201d and learning patch-to-patch attention. The patch embedding based tokenizer is a workaround in practice and has a semantic gap with respect to its counterpart, the textual tokenizer. The patch-to-patch attention suffers from the quadratic complexity issue, and also makes it non-trivial to explain learned ViT models. To address these issues in ViT models, this paper proposes to learn patch-to-cluster attention (PaCa) based ViT models. Queries in our PaCa-ViT are based on patches, while keys and values are based on clustering (with a prede\ufb01ned small number of clusters). The clusters are learned end-to-end, leading to better tokenizers and realizing joint clustering-for-attention and attention-for-clustering when deployed in ViT models. The quadratic complexity is relaxed to linear complexity. Also, directly visualizing the learned clusters can reveal how a trained ViT model learns to perform a task (e.g., object detection). In experiments, the proposed PaCa-ViT is tested on CIFAR-100 and ImageNet-1000 image classi\ufb01cation, and MS-COCO object detection and instance segmentation. Compared with prior arts, it obtains better performance in classi\ufb01cation and comparable performance in detection and segmentation. It is signi\ufb01cantly more ef\ufb01cient in COCO due to the linear complexity. The learned clusters are also semantically meaningful and shed light on designing more discriminative yet interpretable ViT models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2159674931",
                        "name": "Ryan Grainger"
                    },
                    {
                        "authorId": "1390008236",
                        "name": "Thomas Paniagua"
                    },
                    {
                        "authorId": "47684568",
                        "name": "Xi Song"
                    },
                    {
                        "authorId": "47353858",
                        "name": "Tianfu Wu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "6, to further understand the proposed method, we visualize the feature maps of our HFI-Net by using Attention Map [82] on various manipulation faces.",
                "6, to further understand the proposed method, we visualize the feature maps of our HFI-Net by using Attention Map [82] on various\nmanipulation faces.",
                "The visualization experiments of our method via Attention Map [82]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "b5bb4d7df5a63ea3ed45a62b2a727a51dd7f9d12",
                "externalIds": {
                    "DBLP": "journals/tifs/MiaoTCYG22",
                    "DOI": "10.1109/TIFS.2022.3198275",
                    "CorpusId": 251527509
                },
                "corpusId": 251527509,
                "publicationVenue": {
                    "id": "d406a3f4-dc05-43be-b1f6-812f29de9c0e",
                    "name": "IEEE Transactions on Information Forensics and Security",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Inf Forensics Secur"
                    ],
                    "issn": "1556-6013",
                    "url": "http://www.ieee.org/organizations/society/sp/tifs.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=10206",
                        "http://www.signalprocessingsociety.org/publications/periodicals/forensics/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b5bb4d7df5a63ea3ed45a62b2a727a51dd7f9d12",
                "title": "Hierarchical Frequency-Assisted Interactive Networks for Face Manipulation Detection",
                "abstract": "Recently, face manipulation techniques have caused increasing trust concerns in our society. Although current face manipulation detection methods achieve impressive performance regarding intra-dataset evaluation, they are struggling to improve the generalization and robustness ability. To address this issue, we propose a novel Hierarchical Frequency-assisted Interactive Networks (HFI-Net) to explore comprehensive frequency-related forgery cues for face manipulation detection. At first, we formulate HFI-Net as a dual-branch network to take full advantage of both CNN and transformer for capturing local details and global context information, respectively. Considering the forged faces are easy to show flaws in the frequency domain, a novel Frequency-based Feature Refinement (FFR) module is proposed to learn frequency-based attention from RGB features. FFR module emphasizes forgery cues and suppresses the pristine semantics information by keeping middle-high frequency features while discarding the low-frequency ones. Based on FFR, we further develop a co- sharing Global-Local Interaction (GLI) module to conduct frequency-assisted interactions while capturing complementarity among dual branches. Lastly, we further implement the GLI module in each stage of the network to effectively explore multi-level frequency artifacts. Extensive experiments are conducted on several popular benchmarks including FaceForensics++, Celeb-DF, DeepFake-TIMIT, DFDC, UADFV, and DeeperForensics-1.0, which shows that our model outperforms the state-of-the-art, especially in unseen datasets, manipulations, and perturbations evaluation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2053820540",
                        "name": "Changtao Miao"
                    },
                    {
                        "authorId": "9645431",
                        "name": "Zichang Tan"
                    },
                    {
                        "authorId": "3127351",
                        "name": "Q. Chu"
                    },
                    {
                        "authorId": "1708598",
                        "name": "Nenghai Yu"
                    },
                    {
                        "authorId": "2067614206",
                        "name": "Guodong Guo"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "To interpret the inner working mechanism of Transformers, it is essential to understand how the information of each input token flows through each intermediate layer and finally reaches the output.",
                "Thus, attention alone, without considering the skip connection, is not sufficient to characterize the inner working mechanism of Transformers.",
                "Transformers have advanced the state-of-the-art on a variety of natural language processing tasks [1, 2] and see increasing popularity in the field of computer vision [3, 4].",
                "Third, the individual feature attribution-based approaches [15, 14, 29, 30] cannot capture the pairwise interactions of feature since gradients or relevance scores are calculated independently for each individual feature.",
                "Others [15, 14] apply LRP aiming to dissect the information flows via layer-wise back-propagation.",
                "While it somewhat outperforms the rollout method in specific scenarios, it is not ready to support large-scale evaluations [15].",
                "This work addresses the major issues in generating faithful and confident explanations for Transformers via a novel attentive class activation tokens approach.",
                "There has been a growing body of work on using LRP to explain Transformers [14, 15].",
                "Although some gradient-based methods [20, 21, 22, 23] have been proposed to leverage salience for explaining Transformer\u2019s output, most of them still focus on the gradients of attention weights, i.e., Grads and AttGrads as shown in Figure 2.",
                "Since there are various versions of Transformer architectures, e.g., ViT [3] and Swin Transformer [4], which are much different from Transformers used on NLP tasks, it opens up new avenues to extend our AttCAT to explain these models prediction.",
                "This has motivated new research on explaining Transformers output to assist trustworthy human decision-making [10, 11, 12, 13, 14, 15, 16, 17].",
                "[15] provide a comprehensive treatment of the information propagation within all components of the Transformer model, which back-propagates the information through all layers from the output back to the input.",
                "The self-attention mechanism [18] in Transformers assigns a pairwise score capturing the relative importance between every two tokens or image patches as attention weights.",
                "This observation motivates us to interpret the inner working mechanism of Transformers via disentangling the information flow Transformer.",
                "Among all the compared methods, the attention-based methods (i.e., RawAtt and Rollout) perform worst since attention weights alone without considering the magnitudes of feature values are not adequate to analyze the inner working mechanism of Transformers.",
                "Thus, the rollout operation used in [13, 15] will attenuate the impact scores at shallower layers (i.",
                "\u2022 Our AttCAT exploits both the self-attention mechanism and skip connection to explain the inner working mechanism of Transformers via disentangling information flows between intermediate layers."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "26c89113585741975e932e493c562c3114cc6b96",
                "externalIds": {
                    "DBLP": "conf/nips/QiangPLLJZ22",
                    "CorpusId": 253115122
                },
                "corpusId": 253115122,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/26c89113585741975e932e493c562c3114cc6b96",
                "title": "AttCAT: Explaining Transformers via Attentive Class Activation Tokens",
                "abstract": "Transformers have improved the state-of-the-art in various natural language processing and computer vision tasks. However, the success of the Transformer model has not yet been duly explained. Current explanation techniques, which dissect either the self-attention mechanism or gradient-based attribution, do not necessarily provide a faithful explanation of the inner workings of Transformers due to the following reasons: first, attention weights alone without considering the magnitudes of feature values are not adequate to reveal the self-attention mechanism; second, whereas most Transformer explanation techniques utilize self-attention module, the skip-connection module, contributing a significant portion of information flows in Transformers, has not yet been sufficiently exploited in explanation; third, the gradient-based attribution of individual feature does not incorporate interaction among features in explaining the model\u2019s output. In order to tackle the above problems, we propose a novel Transformer explanation technique via attentive class activation tokens, aka, AttCAT, leveraging encoded features, their gradients, and their attention weights to generate a faithful and confident explanation for Transformer\u2019s output. Extensive experiments are conducted to demonstrate the superior performance of AttCAT, which generalizes well to different Transformer architectures, evaluation metrics, datasets, and tasks, to the baseline methods. Our code is available at: https://github.com/qiangyao1988/AttCAT.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2062242240",
                        "name": "Yao Qiang"
                    },
                    {
                        "authorId": "1727055",
                        "name": "Deng Pan"
                    },
                    {
                        "authorId": "46651935",
                        "name": "Chengyin Li"
                    },
                    {
                        "authorId": "50080172",
                        "name": "X. Li"
                    },
                    {
                        "authorId": "38681770",
                        "name": "Rhongho Jang"
                    },
                    {
                        "authorId": "39895985",
                        "name": "D. Zhu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "769fa500401007225bdc73438827f8016b287c14",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-03016",
                    "CorpusId": 245837670
                },
                "corpusId": 245837670,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/769fa500401007225bdc73438827f8016b287c14",
                "title": "Learning class prototypes from Synthetic InSAR with Vision Transformers",
                "abstract": "\u2014The detection of early signs of volcanic unrest preceding an eruption, in the form of ground deformation in Interferometric Synthetic Aperture Radar (InSAR) data is critical for assessing volcanic hazard. In this work we treat this as a binary classi\ufb01cation problem of InSAR images, and propose a novel deep learning methodology that exploits a rich source of synthetically generated interferograms to train quality classi\ufb01ers that perform equally well in real interferograms. The imbalanced nature of the problem, with orders of magnitude fewer positive samples, coupled with the lack of a curated database with labeled InSAR data, sets a challenging task for conventional deep learning architectures. We propose a new framework for domain adaptation, in which we learn class prototypes from synthetic data with vision transformers. We report detection accuracy that surpasses the state of the art on volcanic unrest detection. Moreover, we built upon this knowledge by learning a new, non-linear, projection between the learnt representations and prototype space, using pseudo labels produced by our model from an unlabeled real InSAR dataset. This leads to the new state of the art with 97 . 1% accuracy on our test set. We demonstrate the robustness of our approach by training a simple ResNet-18 Convolutional Neural Network on the unlabeled real InSAR dataset with pseudo-labels generated from our top transformer-prototype model. Our methodology provides a signi\ufb01cant improvement in performance without the need of manually labeling any sample, opening the road for further exploitation of synthetic InSAR data in various remote sensing applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2007384465",
                        "name": "N. Bountos"
                    },
                    {
                        "authorId": "145741802",
                        "name": "D. Michail"
                    },
                    {
                        "authorId": "49032213",
                        "name": "I. Papoutsis"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "52289f27ef3ce416ae360691d91fc8608f995cc7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-10421",
                    "DOI": "10.48550/arXiv.2203.10421",
                    "CorpusId": 247594857
                },
                "corpusId": 247594857,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/52289f27ef3ce416ae360691d91fc8608f995cc7",
                "title": "CLIP on Wheels: Zero-Shot Object Navigation as Object Localization and Exploration",
                "abstract": "Households across the world contain arbitrary objects: from mate gourds and coffee mugs to sitars and guitars. Considering this diversity, robot perception must handle a large variety of semantic objects without additional fine-tuning to be broadly applicable in homes. Recently, zero-shot models have demonstrated impressive performance in image classification of arbitrary objects (i.e., classifying images at inference with categories not explicitly seen during training). In this paper, we translate the success of zero-shot vision models (e.g., CLIP) to the popular embodied AI task of object navigation. In our setting, an agent must find an arbitrary goal object, specified via text, in unseen environments coming from different datasets. Our key insight is to modularize the task into zero-shot object localization and exploration. Employing this philosophy, we design CLIP on Wheels (CoW) baselines for the task and evaluate each zero-shot model in both Habitat and RoboTHOR simulators. We find that a straightfor-ward CoW, with CLIP-based object localization plus classical exploration, and no additional training , often outperforms learnable approaches in terms of success, efficiency, and robustness to dataset distribution shift. This CoW achieves 6.3% SPL in Habitat and 10.0% SPL in RoboTHOR, when tested zero-shot on all categories. On a subset of four RoboTHOR categories considered in prior work, the same CoW shows a 16.1 percentage point improvement in Success over the learnable state-of-the-art baseline.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1387466862",
                        "name": "S. Gadre"
                    },
                    {
                        "authorId": "52193502",
                        "name": "Mitchell Wortsman"
                    },
                    {
                        "authorId": "2123694087",
                        "name": "Gabriel Ilharco"
                    },
                    {
                        "authorId": "152772922",
                        "name": "Ludwig Schmidt"
                    },
                    {
                        "authorId": "3340170",
                        "name": "Shuran Song"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "In addition, the identity matrix (I) is used to avoid the self-inhibition of each patch [61].",
                "[61] to highlight the FT patches that the model is attending to by inferring both the gradient and the relevance from the final classification decision for each attention layer."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "520bc228c5dfcab5b13d1444b42fcbb2800446b7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-03173",
                    "DOI": "10.48550/arXiv.2204.03173",
                    "CorpusId": 248006078
                },
                "corpusId": 248006078,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/520bc228c5dfcab5b13d1444b42fcbb2800446b7",
                "title": "Enhancement on Model Interpretability and Sleep Stage Scoring Performance with A Novel Pipeline Based on Deep Neural Network",
                "abstract": "\u2014Considering the natural frequency characteristics in sleep medicine, this paper \ufb01rst proposes a time-frequency framework for the representation learning of the electroencephalogram (EEG) following the de\ufb01nition of the American Academy of Sleep Medicine. To meet the temporal-random and transient nature of the de\ufb01ning characteristics of sleep stages, we further design a context-sensitive \ufb02exible pipeline that automatically adapts to the attributes of data itself. That is, the input EEG spectrogram is partitioned into a sequence of patches in the time and frequency axes, and then input to a delicate deep learning network for further representation learning to extract the stage-dependent features, which are used in the classi\ufb01cation step \ufb01nally. The proposed pipeline is validated against a large database, i.e., the Sleep Heart Health Study (SHHS), and the results demonstrate that the com-petitive performance for the wake, N2, and N3 stages outperforms the state-of-art works, with the F1 scores being 0.93, 0.88, and 0.87, respectively, and the proposed method has a high inter-rater reliability of 0.80 kappa. Importantly, we visualize the stage scoring process of the model decision with the Layer-wise Relevance Prop- agation (LRP) method, which shows that the proposed pipeline is more sensitive and perceivable in the decision-making process than the baseline pipelines. Therefore, the pipeline together with the LRP method can provide better model interpretability, which is important for clinical support.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2117203726",
                        "name": "Zheng Chen"
                    },
                    {
                        "authorId": "2155486181",
                        "name": "Ziwei Yang"
                    },
                    {
                        "authorId": "2108435519",
                        "name": "Ming Huang"
                    },
                    {
                        "authorId": "46526573",
                        "name": "T. Tamura"
                    },
                    {
                        "authorId": "2467259",
                        "name": "N. Ono"
                    },
                    {
                        "authorId": "1399354648",
                        "name": "M. Altaf-Ul-Amin"
                    },
                    {
                        "authorId": "35149657",
                        "name": "S. Kanaya"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "LRP-implementations follow [34] for ResNet and [35], [36] for transformers."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fe0b5c7df3bd32afc4bbda993919b216b6ef0271",
                "externalIds": {
                    "CorpusId": 248085067
                },
                "corpusId": 248085067,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fe0b5c7df3bd32afc4bbda993919b216b6ef0271",
                "title": "From CNNs to Vision Transformers \u2013 A Comprehensive Evaluation of Deep Learning Models for Histopathology",
                "abstract": "\u2014While machine learning is currently transforming the \ufb01eld of histopathology, the domain lacks a comprehensive evaluation of state-of-the-art models based on essential but complementary quality requirements beyond a mere classi\ufb01cation accuracy. In order to \ufb01ll this gap, we conducted an extensive evaluation by benchmarking a wide range of classi\ufb01cation models, including recent vision transformers, convolutional neural networks and hybrid models comprising transformer and convolutional models. We thoroughly tested the models on \ufb01ve widely used histopathology datasets containing whole slide images of breast, gastric, and colorectal cancer and developed a novel approach using an image-to-image translation model to assess the robustness of a cancer classi\ufb01cation model against stain variations. Further, we extended existing interpretability methods to previously unstudied models and systematically reveal insights of the models\u2019 classi\ufb01cation strategies that allow for plausibility checks and systematic comparisons. The study resulted in speci\ufb01c model recommendations for practitioners as well as putting forward a general methodology to quantify a model\u2019s quality according to complementary requirements that can be transferred to future model architectures.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2162045357",
                        "name": "Maximilian Springenberg"
                    },
                    {
                        "authorId": "2078668598",
                        "name": "A. Frommholz"
                    },
                    {
                        "authorId": "113782311",
                        "name": "M. Wenzel"
                    },
                    {
                        "authorId": "2077593031",
                        "name": "Eva Weicken"
                    },
                    {
                        "authorId": "2144402742",
                        "name": "Jackie Ma"
                    },
                    {
                        "authorId": "1481095347",
                        "name": "Nils Strodthoff"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Fong & Vedaldi (2017) introduce a method for explaining classifiers based on meaningful perturbation and Chefer et al. (2021) introduce a method for improving interpretation for transformer-based classifiers."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "209b28f6f829e7386e14c72ecf27a49588e2245d",
                "externalIds": {
                    "CorpusId": 248545115
                },
                "corpusId": 248545115,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/209b28f6f829e7386e14c72ecf27a49588e2245d",
                "title": "A XIOMATIC E XPLANATIONS FOR V ISUAL S EARCH , R ETRIEVAL , AND S IMILARITY L EARNING",
                "abstract": "Visual search, recommendation, and contrastive similarity learning power tech-nologies that impact billions of users worldwide. Modern model architectures can be complex and dif\ufb01cult to interpret, and there are several competing techniques one can use to explain a search engine\u2019s behavior. We show that the theory of fair credit assignment provides a unique axiomatic solution that generalizes several existing recommendation- and metric-explainability techniques in the literature. Using this formalism, we show when existing approaches violate \u201cfairness\u201d and derive methods that sidestep these shortcomings and naturally handle counterfactual information. More speci\ufb01cally, we show existing approaches implicitly approximate second-order Shapley-Taylor indices and extend CAM, GradCAM, LIME, SHAP, SBSM, and other methods to search engines. These extensions can extract pairwise correspondences between images from trained opaque-box models. We also introduce a fast kernel-based method for estimating Shapley-Taylor indices that require orders of magnitude fewer function evaluations to converge. Finally, we show that these game-theoretic measures yield more consistent explanations for image similarity architectures. show that Shapley values of a particular class of value functions generalize many \ufb01rst-order methods, and this allows us to \ufb01x issues present in existing approaches and extend these approaches to counterfactual explanations. For second order methods we show that Shapley-Taylor indices generalize the work of Zhu et al. (2019) and use our framework to introduce generalizations of LIME, SHAP, and GradCAM. We apply these methods to extract image correspondences from opaque-box similarity models, a feat not yet presented in the literature. To accelerate estimation higher order Shapley-Taylor indices, we contribute a new weighting kernel that requires 10 \u00d7 fewer function evaluations. Finally, we show this game-theoretic formalism yields methods that are more \u201cfaithful\u201d to the underlying model and better satisfy ef\ufb01ciency axioms across several visual similarity methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47359380",
                        "name": "Mark Hamilton"
                    },
                    {
                        "authorId": "23451726",
                        "name": "Scott M. Lundberg"
                    },
                    {
                        "authorId": "2072785288",
                        "name": "Stephanie Fu"
                    },
                    {
                        "authorId": "1452981772",
                        "name": "Lei Zhang"
                    },
                    {
                        "authorId": "2053571409",
                        "name": "W. T. Freeman"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "For example, LRP rules have been derived for LSTMs [46], networks with Batch Normalization [47, 48], Multi-Head Attention [5, 49] and Transformers [50]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3ffce765f4af5ea4fe7f9716a07df5971b198448",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-09753",
                    "DOI": "10.48550/arXiv.2206.09753",
                    "CorpusId": 249890408
                },
                "corpusId": 249890408,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3ffce765f4af5ea4fe7f9716a07df5971b198448",
                "title": "Visualizing and Understanding Self-Supervised Vision Learning",
                "abstract": "Self-Supervised vision learning has revolutionized deep learning, becoming the next big challenge in the domain and rapidly closing the gap with supervised methods on large computer vision bench-marks. With current models and training data exponentially growing, explaining and understanding these models becomes pivotal. We study the problem of explainable arti\ufb01cial intelligence in the domain of self-supervised learning for vision tasks, and present methods to understand networks trained with self-supervision and their inner workings. Given the huge diversity of self-supervised vision pretext tasks, we narrow our focus on understanding paradigms which learn from two views of the same image, and mainly aim to understand the pretext task. Our work focuses on explaining similarity learning, and is easily extendable to all other pretext tasks. We study two popular self-supervised vision models: SimCLR [1] and Barlow Twins [2]. We develop a total of six methods for visualizing and understanding these models: Perturbation-based methods (conditional occlusion, context-agnostic conditional occlusion and pairwise occlusion), Interaction-CAM, Feature Visualization, Model Difference Visualization, Averaged Transforms and Pixel Invaraince. Finally, we evaluate these explanations by translating well-known evaluation metrics tailored towards supervised image classi\ufb01cation systems involving a single image, into the domain of self-supervised learning where two images are involved. Code is at: https://github.com/fawazsammani/xai-ssl",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "32095408",
                        "name": "Fawaz Sammani"
                    },
                    {
                        "authorId": "1568871637",
                        "name": "B. Joukovsky"
                    },
                    {
                        "authorId": "2003112059",
                        "name": "Nikos Deligiannis"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Interpretability of vision models has been an active research recently [3, 11, 46, 65]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "daf318c007593263cafd91678b55a3368bf3ff38",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-00328",
                    "DOI": "10.48550/arXiv.2207.00328",
                    "CorpusId": 250243816
                },
                "corpusId": 250243816,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/daf318c007593263cafd91678b55a3368bf3ff38",
                "title": "TopicFM: Robust and Interpretable Feature Matching with Topic-assisted",
                "abstract": "Finding correspondences across images is an important task in many visual applications. Recent state-of-the-art methods focus on end-to-end learning-based architectures designed in a coarse-to-\ufb01ne manner. They use a very deep CNN or multi-block Transformer to learn robust representation, which requires high computation power. Moreover, these methods learn features without reasoning about objects, shapes inside images, thus lacks of interpretability. In this paper, we propose an architecture for image matching which is ef\ufb01cient, robust, and interpretable. More speci\ufb01-cally, we introduce a novel feature matching module called TopicFM which can roughly organize same spatial structure across images into a topic and then augment the features inside each topic for accurate matching. To infer topics, we \ufb01rst learn global embedding of topics and then use a latent-variable model to detect-then-assign the image structures into topics. Our method can only perform matching in co-visibility regions to reduce computations. Extensive experiments in both outdoor and indoor datasets show that our method outperforms the recent methods in terms of matching performance and computational ef\ufb01ciency. The code is available at https://github.com/TruongKhang/ TopicFM .",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2145154617",
                        "name": "Khang Truong Giang"
                    },
                    {
                        "authorId": "2133985",
                        "name": "Soohwan Song"
                    },
                    {
                        "authorId": "2146863514",
                        "name": "Sung-Guk Jo"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Using the same method with the main material [3], we also visualize the activated area of our M3T network."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d6af81d369524e81063f1ce6ffad409539026632",
                "externalIds": {
                    "CorpusId": 250317658
                },
                "corpusId": 250317658,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d6af81d369524e81063f1ce6ffad409539026632",
                "title": "Supplementary Material for CVPR 2022 paper # 9084",
                "abstract": "We compared M3T with conventional 3D classification methods and visualize the ROC curve plot with the area under curve (AUC) values. The conventional methods include 3D ResNet (50, 101, 152) [5], 3D DenseNet121 [6], , I3D [2] , MRNet [1], and MedicalNet [4] used in our main material. We also combined some 3D CNN networks with a transformer like the model used in the main material. The ROC graphs are presented in Fig. 3. The results show that our proposed M3T (black line) achieves the highest performance of AUC value in all test datasets. Especially, the performance differences in AIBL and OASIS between M3T and the other models are more than those in the ADNI dataset, which indicates the proposed model is strong against overfitting to the training dataset. Furthermore, the curves of our method are closed to the ideal graphs where the AUC value is 1, which means that it has higher sensitivity and specificity values than the other methods in classifying Alzheimer\u2019s Disease.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9331782",
                        "name": "Jinseong Jang"
                    },
                    {
                        "authorId": "2905840",
                        "name": "D. Hwang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2b8b1ac2cdeea7189648d15c14c7bcd74b600b69",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-00507",
                    "DOI": "10.2139/ssrn.4207369",
                    "CorpusId": 251979651
                },
                "corpusId": 251979651,
                "publicationVenue": {
                    "id": "75d7a8c1-d871-42db-a8e4-7cf5146fdb62",
                    "name": "Social Science Research Network",
                    "type": "journal",
                    "alternate_names": [
                        "SSRN, Social Science Research Network (SSRN) home page",
                        "SSRN Electronic Journal",
                        "Soc Sci Res Netw",
                        "SSRN",
                        "SSRN Home Page",
                        "SSRN Electron J",
                        "Social Science Electronic Publishing presents Social Science Research Network"
                    ],
                    "issn": "1556-5068",
                    "url": "http://www.ssrn.com/",
                    "alternate_urls": [
                        "www.ssrn.com/",
                        "https://fatcat.wiki/container/tol7woxlqjeg5bmzadeg6qrg3e",
                        "https://www.wikidata.org/wiki/Q53949192",
                        "www.ssrn.com/en",
                        "http://www.ssrn.com/en/",
                        "http://umlib.nl/ssrn",
                        "umlib.nl/ssrn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2b8b1ac2cdeea7189648d15c14c7bcd74b600b69",
                "title": "A Dataset for Detecting Real-World Environmental Claims",
                "abstract": "In this paper, we introduce an expert-annotated dataset for detecting real-world environmental claims made by listed companies. We train and release baseline models for detecting environmental claims using this new dataset. We further preview potential applications of our dataset: We use our \ufb01ne-tuned model to detect environmental claims made in answer sections of quarterly earning calls between 2012 and 2020 \u2013 and we \ufb01nd that the amount of environmental claims steadily increased since the Paris Agreement in 2015.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "146552774",
                        "name": "Dominik Stammbach"
                    },
                    {
                        "authorId": "2023644816",
                        "name": "Nicolas Webersinke"
                    },
                    {
                        "authorId": "2006205621",
                        "name": "J. Bingler"
                    },
                    {
                        "authorId": "2156865999",
                        "name": "Mathias Kraus"
                    },
                    {
                        "authorId": "3073566",
                        "name": "Markus Leippold"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c01d0335f1bcc449ddebf1354a0654069504c089",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-10876",
                    "DOI": "10.48550/arXiv.2209.10876",
                    "CorpusId": 252438689
                },
                "corpusId": 252438689,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c01d0335f1bcc449ddebf1354a0654069504c089",
                "title": "Improving Attention-Based Interpretability of Text Classification Transformers",
                "abstract": "Transformers are widely used in NLP, where they consistently achieve state-of-the-art performance. This is due to their attention-based architecture, which allows them to model rich linguistic relations between words. However, transformers are dif\ufb01cult to interpret. Being able to provide reasoning for its decisions is an important property for a model in domains where human lives are affected, such as hate speech detection and biomedicine. With transformers \ufb01nding wide use in these \ufb01elds, the need for interpretability techniques tailored to them arises. The effectiveness of attention-based interpretability techniques for transformers in text classi\ufb01cation is studied in this work. Despite concerns about attention-based interpretations in the literature, we show that, with proper setup, attention may be used in such tasks with results comparable to state-of-the-art techniques, while also being faster and friendlier to the environment. We validate our claims with a series of experiments that employ a new feature importance metric",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1500412311",
                        "name": "Nikolaos Mylonas"
                    },
                    {
                        "authorId": "51055381",
                        "name": "Ioannis Mollas"
                    },
                    {
                        "authorId": "2502501",
                        "name": "Grigorios Tsoumakas"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "There is a tremendous amount of work on extracting saliency maps in a self-supervised (Voynov et al., 2021; Caron et al., 2021; Mo et al., 2021) or weakly-supervised (i.e., using class labels; Selvaraju et al. (2017); Chefer et al. (2021)) manner.",
                "Figure 4 visualizes the saliency maps (Chefer et al., 2021), verifying that ReMixer gives more object-centric view.",
                "First, the progress of supervised (He et al., 2017; Carion et al., 2020; Fang et al., 2021), weakly-supervised (Selvaraju et al., 2017; Chefer et al., 2021; Yun et al., 2021), and self-supervised (Voynov et al., 2021; Caron et al., 2021; Mo et al., 2021) detection significantly reduced the cost of\u2026"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a3cbfa4c5da14e4a7e3a6eedba2b2a96d257acba",
                "externalIds": {
                    "CorpusId": 253519559
                },
                "corpusId": 253519559,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a3cbfa4c5da14e4a7e3a6eedba2b2a96d257acba",
                "title": "R E M IXER : O BJECT - AWARE M IXING L AYER FOR V ISION T RANSFORMERS AND M IXERS",
                "abstract": "Patch-based models, e.g., Vision Transformers (ViTs) and Mixers, have shown impressive results on various visual recognition tasks, exceeding classic convolutional networks. While the initial patch-based models treated all patches equally, recent studies reveal that incorporating inductive biases like spatiality benefits the learned representations. However, most prior works solely focused on the position of patches, overlooking the scene structure of images. This paper aims to further guide the interaction of patches using the object information. Specifically, we propose ReMixer, which reweights the patch mixing layers based on the patch-wise object labels extracted from pretrained saliency or classification models. We apply ReMixer on various patch-based models using different patch mixing layers: ViT, MLP-Mixer, and ConvMixer, where our method consistently improves the classification accuracy and background robustness of baseline models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "143720148",
                        "name": "Jinwoo Shin"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                ", Vision Transformers) starkly di\u21b5erent learning mechanisms for visual recognition than traditional CNNs, their fine-grained attentiveness, and their subsequent human-interpretability in standard image domains [14,19,21,80]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "452741292e1ff10362a999259c5716799f2e40fd",
                "externalIds": {
                    "DBLP": "conf/eccv/MachirajuPM22",
                    "DOI": "10.1007/978-3-031-19775-8_25",
                    "CorpusId": 253121227
                },
                "corpusId": 253121227,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/452741292e1ff10362a999259c5716799f2e40fd",
                "title": "A Dataset Generation Framework for Evaluating Megapixel Image Classifiers and Their Explanations",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "13879287",
                        "name": "Gautam Machiraju"
                    },
                    {
                        "authorId": "2224052",
                        "name": "S. Plevritis"
                    },
                    {
                        "authorId": "2376538",
                        "name": "P. Mallick"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "11488e6941fda57c35459efd8d262e55b5a31340",
                "externalIds": {
                    "DBLP": "conf/ectel/Gombert22",
                    "CorpusId": 254998913
                },
                "corpusId": 254998913,
                "publicationVenue": {
                    "id": "64efda39-fbce-4563-9435-4064cc930715",
                    "name": "European Conference on Technology Enhanced Learning",
                    "type": "conference",
                    "alternate_names": [
                        "EC-TEL",
                        "Eur Conf Technol Enhanc Learn"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=826"
                },
                "url": "https://www.semanticscholar.org/paper/11488e6941fda57c35459efd8d262e55b5a31340",
                "title": "Methods and perspectives for the automated analytic assessment of free-text responses in formative scenarios",
                "abstract": "Assessment is the process of testing learners\u2019 skills and knowledge. Free-text response items are well suited for the assessment of learners\u2019 active knowledge and writing skills. However, the automatic assessment of respective responses is not trivial and requires the application of natural language processing. Accordingly, the automatic assessment of free-text responses is a widely researched topic in educational natural language processing. Most past work targets holistic scoring, the process of assigning overall scores or grades to responses. This is problematic in formative scenarios because learners require feedback rather than summative scores in such scenarios. Such feedback ideally targets specific aspects of responses, and, accordingly, automated systems which only predict holistic scores cannot be used as a basis for providing the same. What is instead needed are systems which implement analytic scoring approaches. Analytic scoring targets specific aspects of responses and scores them according to corresponding criteria. This requires different systems than addressed by the broad research on automated holistic scoring. In my PhD work which is outlined by this paper, I want to explore approaches for implementing analytic scoring systems by means of state-of-the-art natural language processing. These systems are targeted at providing a basis for feedback generation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1937843717",
                        "name": "Sebastian Gombert"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "93dc3e1bf0fbfefc0bc40e3b7ef2b72b479d3cda",
                "externalIds": {
                    "CorpusId": 256462859
                },
                "corpusId": 256462859,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/93dc3e1bf0fbfefc0bc40e3b7ef2b72b479d3cda",
                "title": "Machine Learning for Abdominal Aortic Aneurysm Characterization from Standard-Of- Care Computed Tomography Angiography Images by",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "31930500",
                        "name": "A. Salvi"
                    },
                    {
                        "authorId": "2642538",
                        "name": "E. Finol"
                    },
                    {
                        "authorId": "2040744",
                        "name": "Prahlad G. Menon"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "We propose to leverage the idea of relevancy scores [4] as the importance map for optimal transport distributions.",
                "A class-specific visualization method for self-attention models is proposed in [4]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5269fcbc48723bd66816e10e310c191ad2598d71",
                "externalIds": {
                    "DBLP": "conf/bmvc/0023LC22",
                    "CorpusId": 256902313
                },
                "corpusId": 256902313,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/5269fcbc48723bd66816e10e310c191ad2598d71",
                "title": "Beyond the CLS Token: Image Reranking using Pretrained Vision Transformers",
                "abstract": "We propose to leverage structural similarity of pretrained vision transformers for image retrieval reranking. Vision transformers have become the dominant architecture in many computer vision tasks. However, the usage of global representation ( CLS token) makes for the lack of interpretability. Since not all patches are equally important for image similarity, our idea is to exploit a pretrained model for optimal spatial weights assigned to local patch tokens. To understand the relationship between global and local representations of vision transformers, we compare multiple transformers architectures against ResNet using similarity as an indicative measure. Our analysis suggest that the usage of convolutions within vision transformers is vital to learn suitable patch embeddings for structural similarities. We also find that local patch similarity equipped with an optimal transport solver could improve image retrieval accuracy compared to the one using global similarity only. Without re-training, our evaluations with off-the-shelf pretrained vision transformers show that the use of structural similarity not only boosts retrieval performance, but also provides visualization cues for interpretable image similarity. Evaluations on three benchmarks show that our proposed structural approach outperforms the state of the art for interpretable image retrieval.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152737858",
                        "name": "Chao Zhang"
                    },
                    {
                        "authorId": "2297234",
                        "name": "Stephan Liwicki"
                    },
                    {
                        "authorId": "1745672",
                        "name": "R. Cipolla"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                ", 2017), attribution methods (Bach et al., 2015; Montavon et al., 2017; Nam et al., 2019; Gur et al., 2020; Chefer et al., 2021), and image manipulation methods (Fong et al.",
                "\u2026(Shrikumar et al., 2017; Srinivas and Fleuret, 2019; Selvaraju et al., 2017), attribution methods (Bach et al., 2015; Montavon et al., 2017; Nam et al., 2019; Gur et al., 2020; Chefer et al., 2021), and image manipulation methods (Fong et al., 2019; Fong and Vedaldi, 2017; Lundberg and Lee, 2017)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ff446646a9f3d31c1af422b7c53738b5a2ec7ba9",
                "externalIds": {
                    "DBLP": "conf/midl/AliSW22",
                    "CorpusId": 256942649
                },
                "corpusId": 256942649,
                "publicationVenue": {
                    "id": "2c1c3a62-7d6e-44b9-b225-a9ddb7ebdb8d",
                    "name": "International Conference on Medical Imaging with Deep Learning",
                    "type": "conference",
                    "alternate_names": [
                        "MIDL",
                        "Int Conf Med Imaging Deep Learn"
                    ],
                    "url": "https://www.midl.io/"
                },
                "url": "https://www.semanticscholar.org/paper/ff446646a9f3d31c1af422b7c53738b5a2ec7ba9",
                "title": "Explainability Guided COVID-19 Detection in CT Scans",
                "abstract": "Radiological examination of chest CT is an effective method for screening COVID-19 cases. In this work, we overcome three challenges in the automation of this process: (i) the limited number of supervised positive cases, (ii) the lack of region-based supervision, and (iii) variability across acquisition sites. These challenges are met by incorporating a recent augmentation solution called SnapMix, a novel explainability-driven contrastive loss for patch embedding, and by performing test-time augmentation that masks out the most relevant patches in order to analyse the prediction stability. The three techniques are complementary and are all based on utilizing the heatmaps produced by the Class Activation Mapping (CAM) explainability method. State-of-the-art performance is obtained on three different datasets for COVID detection in CT scans.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2112239702",
                        "name": "Ameen Ali"
                    },
                    {
                        "authorId": "1438948620",
                        "name": "Tal Shaharabany"
                    },
                    {
                        "authorId": "2057216785",
                        "name": "L. Wolf"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Methods raw attention rollout[11] attribution[12] Ours Ours(lend=4) Pixel accuracy 67.",
                "Based on the idea of the rollout method, the attribution method [12] computed the relevance scores of the tokens with the layer-wise relevance propagation (LRP) [13], to visualize ViT\u2019s decision process.",
                "Methods raw attention rollout[11] attribution[12] Ours Ours(lend=4) Deletion 21.",
                "Note that (i) and (ii) are obtained after binarizing each visualization, which depends on the pre-set threshold (30% of the max value is used in practice [12]), while (iii) is threshold-free.",
                "The Transformer attribution method [12] assigns local relevance scores based on LRP and propagates the relevance scores mixed with gradients through layers.",
                "Without early stopping using lend, our proposed approach produces better explanations than the rollout [11] and attribution [12] methods.",
                "The proposed approach produces better explanations than currently state-of-the-art algorithms [11, 12].",
                "Methods raw attention rollout[11] attribution[12] Ours Ours(lend=4) Positive 28.",
                "The attribution method [12] can extract category-related features and part of the target region but is not comprehensive and complete.",
                "[12] proposed a better explanation algorithm using the product of gradients and feature attributions."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "34d124291050ec7da53f4c1fec2b0d18554de58a",
                "externalIds": {
                    "CorpusId": 249246070
                },
                "corpusId": 249246070,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/34d124291050ec7da53f4c1fec2b0d18554de58a",
                "title": "Explaining Information Flow Inside Vision Transformers Using Markov Chain",
                "abstract": "Transformer-based models are receiving increasingly popularity in the field of computer vision, however, the corresponding interpretability study is less. As the simplest explainability method, visualization of attention weights exerts poor performance because of lacking association between the input and model decisions. In this study, we propose a method, named Transition Attention Maps , to generate the saliency map concerning a specific target category. The proposed approach con-nects the idea of the Markov chain, to investigate the information flow across layers of the Transformer and combine the integrated gradients to compute the relevance of input tokens for the model decisions. We compare with other explainability methods using Vision Transformer as a benchmark and demonstrate that our method achieves better performance in various aspects. We open source the implementation of our approach at https://github.com/PaddlePaddle/InterpretDL .",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2057820135",
                        "name": "Ting Yuan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "ViT is given high expectation to improve vision tasks beyond image classification, with existing studies on generative modeling [11, 27, 28, 39], video understanding [2, 38], segmentation and detection [30, 34, 53], interpretability [1, 6, 7].",
                "It gives the advantage to cheaply apply CAM-like [65] method to interpret how well learned representations measure object features without needing any particular algorithms for transformer, such as [7]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "083ca4bd4d5b231a1d7a0715ec55cc57a0f44b13",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-12723",
                    "CorpusId": 235196049
                },
                "corpusId": 235196049,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/083ca4bd4d5b231a1d7a0715ec55cc57a0f44b13",
                "title": "Aggregating Nested Transformers",
                "abstract": "Although hierarchical structures are popular in recent vision transformers, they require sophisticated designs and massive datasets to work well. In this work, we explore the idea of nesting basic local transformers on non-overlapping image blocks and aggregating them in a hierarchical manner. We \ufb01nd that the block aggregation function plays a critical role in enabling cross-block non-local information communication. This observation leads us to design a simpli\ufb01ed architecture with minor code changes upon the original vision transformer and obtains improved performance compared to existing methods. Our empirical results show that the proposed method NesT converges faster and requires much less training data to achieve good generalization. For example, a NesT with 68M parameters trained on ImageNet for 100/300 epochs achieves 82 . 3% / 83 . 8% accuracy evaluated on 224 \u00d7 224 image size, outperforming previous methods with up to 57% parameter reduction. Training a NesT with 6M parameters from scratch on CIFAR10 achieves 96% accuracy using a single GPU, setting a new state of the art for vision transformers. Beyond image classi\ufb01cation, we extend the key idea to image generation and show NesT leads to a strong decoder that is 8 \u00d7 faster than previous transformer based generators. Furthermore, we also propose a novel method for visually interpreting the learned model. Source code is available https://github.com/google-research/nested-transformer .",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2128158461",
                        "name": "Zizhao Zhang"
                    },
                    {
                        "authorId": "2119079641",
                        "name": "Han Zhang"
                    },
                    {
                        "authorId": "48096253",
                        "name": "Long Zhao"
                    },
                    {
                        "authorId": "2117180415",
                        "name": "Ting Chen"
                    },
                    {
                        "authorId": "1945962",
                        "name": "Tomas Pfister"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ef84b10efe4e0717f9391606e446da05595838cc",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-07824",
                    "CorpusId": 231933638
                },
                "corpusId": 231933638,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ef84b10efe4e0717f9391606e446da05595838cc",
                "title": "A Koopman Approach to Understanding Sequence Neural Models",
                "abstract": "Deep learning models are often treated as \u201cblack boxes\u201d. Existing approaches for understanding the decision mechanisms of neural networks provide limited explanations or depend on local theories. Recently, a data-driven framework based on Koopman theory was developed for the analysis of nonlinear dynamical systems. In this paper, we introduce a new approach to understanding trained sequence neural models: the Koopman Analysis of Neural Networks (KANN) method. At the core of our method lies the Koopman operator, which is linear, yet it encodes the dominant features of the network latent dynamics. Moreover, its eigenvectors and eigenvalues facilitate understanding: in the sentiment analysis problem, the eigenvectors highlight positive and negative n-grams; and, in the ECG classification challenge, the eigenvectors capture the dominant features of the normal beat signal.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "65787565",
                        "name": "Ilana D Naiman"
                    },
                    {
                        "authorId": "1927247",
                        "name": "Omri Azencot"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "A comparison of visualization techniques was made in (Chefer et al., 2021), and they carried out experiments to test some of these methods for Vision Transformers in image classification tasks."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cbb17fee26c79909e3ad83cb6659a26a0ddea3e5",
                "externalIds": {
                    "CorpusId": 250261983
                },
                "corpusId": 250261983,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cbb17fee26c79909e3ad83cb6659a26a0ddea3e5",
                "title": "Interpretable Video Transformers in Imitation Learning of Human Driving",
                "abstract": "Transformers applied to high-level vision tasks showcase impressive performance due to the use of self-attention sublayers for computing af\ufb01nity weights across tokens corresponding to image patches. A simple Vision Transformer encoder can also be trained with video clip inputs from popular driving datasets in a weakly supervised imitation learning task, framed as predicting future human driving actions as a time series sequence over a prediction horizon. In this paper, we propose this task as a simple, scalable method for autonomous vehicle planning to match human driving behaviour. We demonstrate initial results for this method, along with model visualizations for interpreting features in video inputs that contribute to sequence predictions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2555924",
                        "name": "Andrew M. Dai"
                    },
                    {
                        "authorId": "2057146070",
                        "name": "Wenliang Qiu"
                    },
                    {
                        "authorId": "36535493",
                        "name": "Bidisha Ghosh"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The token representations [4,10,65,68] in early and middle layers are insufficiently encoded, which makes token pruning quite difficult."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0d8be19e00af83388523baf86f8cdf682302a0d1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-13890",
                    "CorpusId": 260445709
                },
                "corpusId": 260445709,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0d8be19e00af83388523baf86f8cdf682302a0d1",
                "title": "SPViT: Enabling Faster Vision Transformers via Soft Token Pruning",
                "abstract": "Recently, Vision Transformer (ViT) has continuously es-tablished new milestones in the computer vision \ufb01eld, while the high computation and memory cost makes its propaga-tion in industrial production dif\ufb01cult. Pruning, a traditional model compression paradigm for hardware ef\ufb01ciency, has been widely applied in various DNN structures. Neverthe-less, it stays ambiguous on how to perform exclusive pruning on the ViT structure. Considering three key points: the structural characteristics, the internal data pattern of ViTs, and the related edge device deployment, we leverage the input token sparsity and propose a computation-aware soft pruning framework, which can be set up on vanilla Transformers of both \ufb02atten and CNN-type structures, such as Pooling-based ViT (PiT). More concretely, we design a dynamic attention-based multi-head token selector, which is a lightweight module for adaptive instance-wise token selection. We further introduce a soft pruning technique, which integrates the less informative tokens generated by the selector module into a package token that will participate in subsequent calculations rather than being completely dis-carded. Our framework is bound to the trade-off between accuracy and computation constraints of speci\ufb01c edge devices through our proposed computation-aware training strategy. Experimental results show that our",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "32409528",
                        "name": "Zhenglun Kong"
                    },
                    {
                        "authorId": "2052289825",
                        "name": "Peiyan Dong"
                    },
                    {
                        "authorId": "151480882",
                        "name": "Xiaolong Ma"
                    },
                    {
                        "authorId": "2148338860",
                        "name": "Xin Meng"
                    },
                    {
                        "authorId": "48643324",
                        "name": "Wei Niu"
                    },
                    {
                        "authorId": "8712588",
                        "name": "Mengshu Sun"
                    },
                    {
                        "authorId": "2042633100",
                        "name": "Bin Ren"
                    },
                    {
                        "authorId": "39449475",
                        "name": "Minghai Qin"
                    },
                    {
                        "authorId": "2168478659",
                        "name": "Hao Tang"
                    },
                    {
                        "authorId": "2143431718",
                        "name": "Yanzhi Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "185f41ee8666a8935c5d8cb5274c67652045f6aa",
                "externalIds": {
                    "CorpusId": 263767343
                },
                "corpusId": 263767343,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/185f41ee8666a8935c5d8cb5274c67652045f6aa",
                "title": "Transformers Pay Attention to Convolutions Leveraging Emerging Properties of ViTs by Dual Attention-Image Network",
                "abstract": "Although purely transformer-based architectures pre-trained on large datasets are introduced as foundation models for general computer vision tasks, hybrid models that incorporate combinations of convolution and transformer blocks showed state-of-the-art performance in more specialized tasks. Nevertheless, despite the performance gain of both pure and hybrid transformer-based architectures compared to convolutional networks, their high training cost and complexity make it challenging to use them in real scenarios. In this work, we propose a novel and simple architecture based on only convolutional layers and show that by just taking advantage of the attention map visualizations obtained from a self-supervised pretrained vision transformer network, complex transformer-based networks, and even 3D architectures are outperformed with much fewer computation costs. The proposed architecture is composed of two encoder branches with the original image as input in one branch and the attention map visualizations of the same image from multiple self-attention heads from a pre-trained DINO model in the other branch. The results of our experiments on medi-*",
                "year": null,
                "authors": [
                    {
                        "authorId": "1882505527",
                        "name": "Yousef Yeganeh"
                    },
                    {
                        "authorId": "2169482",
                        "name": "Azade Farshad"
                    },
                    {
                        "authorId": "2176770240",
                        "name": "Peter Weinberger"
                    },
                    {
                        "authorId": "145774206",
                        "name": "Seyed-Ahmad Ahmadi"
                    },
                    {
                        "authorId": "2256396182",
                        "name": "Ehsan Adeli"
                    },
                    {
                        "authorId": "145587209",
                        "name": "N. Navab"
                    }
                ]
            }
        }
    ]
}