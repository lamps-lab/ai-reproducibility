{
    "offset": 0,
    "data": [
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Although machine learning approaches have been proposed for many types of inverse problems [1, 2, 3, 4], most of them make prior assumptions on the specific form of the underlying partial differential equation (PDE) and use discretization i."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dfa5ba89364d011e9adca084afc5ca11fecf44ee",
                "externalIds": {
                    "ArXiv": "2309.16131",
                    "CorpusId": 263135582
                },
                "corpusId": 263135582,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/dfa5ba89364d011e9adca084afc5ca11fecf44ee",
                "title": "A Spectral Approach for Learning Spatiotemporal Neural Differential Equations",
                "abstract": "Rapidly developing machine learning methods has stimulated research interest in computationally reconstructing differential equations (DEs) from observational data which may provide additional insight into underlying causative mechanisms. In this paper, we propose a novel neural-ODE based method that uses spectral expansions in space to learn spatiotemporal DEs. The major advantage of our spectral neural DE learning approach is that it does not rely on spatial discretization, thus allowing the target spatiotemporal equations to contain long range, nonlocal spatial interactions that act on unbounded spatial domains. Our spectral approach is shown to be as accurate as some of the latest machine learning approaches for learning PDEs operating on bounded domains. By developing a spectral framework for learning both PDEs and integro-differential equations, we extend machine learning methods to apply to unbounded DEs and a larger class of problems.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "91847040",
                        "name": "Mingtao Xia"
                    },
                    {
                        "authorId": "2238124694",
                        "name": "Xiangting Li"
                    },
                    {
                        "authorId": "2248042608",
                        "name": "Qijing Shen"
                    },
                    {
                        "authorId": "2237986957",
                        "name": "Tom Chou"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "A similar application is a correction term within a fixed-point iterator, as outlined in [439]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "89e2413c3b5db4d48431e7d116c61ccff5e8d03e",
                "externalIds": {
                    "ArXiv": "2309.15421",
                    "CorpusId": 262940040
                },
                "corpusId": 262940040,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/89e2413c3b5db4d48431e7d116c61ccff5e8d03e",
                "title": "Deep Learning in Deterministic Computational Mechanics",
                "abstract": "The rapid growth of deep learning research, including within the field of computational mechanics, has resulted in an extensive and diverse body of literature. To help researchers identify key concepts and promising methodologies within this field, we provide an overview of deep learning in deterministic computational mechanics. Five main categories are identified and explored: simulation substitution, simulation enhancement, discretizations as neural networks, generative approaches, and deep reinforcement learning. This review focuses on deep learning methods rather than applications for computational mechanics, thereby enabling researchers to explore this field more effectively. As such, the review is not necessarily aimed at researchers with extensive knowledge of deep learning -- instead, the primary audience is researchers at the verge of entering this field or those who attempt to gain an overview of deep learning in computational mechanics. The discussed concepts are, therefore, explained as simple as possible.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "35562955",
                        "name": "L. Herrmann"
                    },
                    {
                        "authorId": "2244431426",
                        "name": "Stefan Kollmannsberger"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Other PDE modeling approaches include accurate neural PDE solvers [40, 8, 70] or other improved PINN variants such as competitive PINNs [97] and robust PINNs [3]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ae52090320b461b7e656904c90a78ae76fbc4688",
                "externalIds": {
                    "ArXiv": "2309.13167",
                    "DBLP": "journals/corr/abs-2309-13167",
                    "DOI": "10.48550/arXiv.2309.13167",
                    "CorpusId": 262464471
                },
                "corpusId": 262464471,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ae52090320b461b7e656904c90a78ae76fbc4688",
                "title": "Flow Factorized Representation Learning",
                "abstract": "A prominent goal of representation learning research is to achieve representations which are factorized in a useful manner with respect to the ground truth factors of variation. The fields of disentangled and equivariant representation learning have approached this ideal from a range of complimentary perspectives; however, to date, most approaches have proven to either be ill-specified or insufficiently flexible to effectively separate all realistic factors of interest in a learned latent space. In this work, we propose an alternative viewpoint on such structured representation learning which we call Flow Factorized Representation Learning, and demonstrate it to learn both more efficient and more usefully structured representations than existing frameworks. Specifically, we introduce a generative model which specifies a distinct set of latent probability paths that define different input transformations. Each latent flow is generated by the gradient field of a learned potential following dynamic optimal transport. Our novel setup brings new understandings to both \\textit{disentanglement} and \\textit{equivariance}. We show that our model achieves higher likelihoods on standard representation learning benchmarks while simultaneously being closer to approximately equivariant models. Furthermore, we demonstrate that the transformations learned by our model are flexibly composable and can also extrapolate to new data, implying a degree of robustness and generalizability approaching the ultimate goal of usefully factorized representation learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152601878",
                        "name": "Yue Song"
                    },
                    {
                        "authorId": "47255130",
                        "name": "Thomas Anderson Keller"
                    },
                    {
                        "authorId": "1703601",
                        "name": "N. Sebe"
                    },
                    {
                        "authorId": "2241950105",
                        "name": "Max Welling"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Other approaches of ML-based PDE solvers include the physics informed neural networks (PINNs) which utilize the physics-informed loss [40, 50, 51, 60, 61], neural operators [5, 29, 33, 34, 38, 57] , and autoregressive methods [3, 6, 14, 23, 26] , etc."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ffc8f28dc83ef500d2d885d12d3d031a8b01a6f2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-04943",
                    "ArXiv": "2309.04943",
                    "DOI": "10.48550/arXiv.2309.04943",
                    "CorpusId": 261682556
                },
                "corpusId": 261682556,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ffc8f28dc83ef500d2d885d12d3d031a8b01a6f2",
                "title": "A multi-fidelity machine learning based semi-Lagrangian finite volume scheme for linear transport equations and the nonlinear Vlasov-Poisson system",
                "abstract": "Machine-learning (ML) based discretization has been developed to simulate complex partial differential equations (PDEs) with tremendous success across various fields. These learned PDE solvers can effectively resolve the underlying solution structures of interest and achieve a level of accuracy which often requires an order-of-magnitude finer grid for a conventional numerical method using polynomial-based approximations. In a previous work in [13], we introduced a learned finite volume discretization that further incorporates the semi-Lagrangian (SL) mechanism, enabling larger CFL numbers for stability. However, the efficiency and effectiveness of such methodology heavily rely on the availability of abundant high-resolution training data, which can be prohibitively expensive to obtain. To address this challenge, in this paper, we propose a novel multi-fidelity ML-based SL method for transport equations. This method leverages a combination of a small amount of high-fidelity data and sufficient but cheaper low-fidelity data. The approach is designed based on a composite convolutional neural network architecture that explore the inherent correlation between high-fidelity and low-fidelity data. The proposed method demonstrates the capability to achieve a reasonable level of accuracy, particularly in scenarios where a single-fidelity model fails to generalize effectively. We further extend the method to the nonlinear Vlasov-Poisson system by employing high order Runge-Kutta exponential integrators. A collection of numerical tests are provided to validate the efficiency and accuracy of the proposed method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2109384982",
                        "name": "Yongsheng Chen"
                    },
                    {
                        "authorId": "2239058944",
                        "name": "Wei Guo"
                    },
                    {
                        "authorId": "2239067052",
                        "name": "Xinghui Zhong"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                ", [33] \u2013 they lack convergence guarantees and predictive uncertainty modeling, i.",
                "[33] Jun-Ting Hsieh, Shengjia Zhao, Stephan Eismann, Lucia Mirabella, and Stefano Ermon.",
                "Recent approaches can be broadly classified into three categories: (i) neural approaches that approximate the solution function of the underlying PDE [24, 66]; (ii) hybrid approaches, where neural networks either augment numerical solvers or replace parts of them [1, 2, 18, 33, 43, 79]; (iii) neural approaches in which the learned evolution operator maps the current state to a future state of the system [4, 7, 9, 21, 49, 54, 68, 87]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "01b7968c084f611b9b008713c8bceb862ab047d7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-05732",
                    "ArXiv": "2308.05732",
                    "DOI": "10.48550/arXiv.2308.05732",
                    "CorpusId": 260775609
                },
                "corpusId": 260775609,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/01b7968c084f611b9b008713c8bceb862ab047d7",
                "title": "PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers",
                "abstract": "Time-dependent partial differential equations (PDEs) are ubiquitous in science and engineering. Recently, mostly due to the high computational cost of traditional solution techniques, deep neural network based surrogates have gained increased interest. The practical utility of such neural PDE solvers relies on their ability to provide accurate, stable predictions over long time horizons, which is a notoriously hard problem. In this work, we present a large-scale analysis of common temporal rollout strategies, identifying the neglect of non-dominant spatial frequency information, often associated with high frequencies in PDE solutions, as the primary pitfall limiting stable, accurate rollout performance. Based on these insights, we draw inspiration from recent advances in diffusion models to introduce PDE-Refiner; a novel model class that enables more accurate modeling of all frequency components via a multistep refinement process. We validate PDE-Refiner on challenging benchmarks of complex fluid dynamics, demonstrating stable and accurate rollouts that consistently outperform state-of-the-art models, including neural, numerical, and hybrid neural-numerical architectures. We further demonstrate that PDE-Refiner greatly enhances data efficiency, since the denoising objective implicitly induces a novel form of spectral data augmentation. Finally, PDE-Refiner's connection to diffusion models enables an accurate and efficient assessment of the model's predictive uncertainty, allowing us to estimate when the surrogate becomes inaccurate.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1751661088",
                        "name": "Phillip Lippe"
                    },
                    {
                        "authorId": "46221215",
                        "name": "Bastiaan S. Veeling"
                    },
                    {
                        "authorId": "3410970",
                        "name": "P. Perdikaris"
                    },
                    {
                        "authorId": "145369890",
                        "name": "Richard E. Turner"
                    },
                    {
                        "authorId": "78843496",
                        "name": "Johannes Brandstetter"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Also, one can consider optimization problems or solving ordinary or partial diferential equations using numerical methods, which are frequently used in the computational and physical sciences [9, 20, 25, 32, 40, 49, 72, 73, 75, 77, 83, 86, 103, 114, 116, 117, 129, 132, 144]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5815075c70bf5113d4ab2ab4fc078913674a27ce",
                "externalIds": {
                    "DOI": "10.1145/3611383",
                    "CorpusId": 260380494
                },
                "corpusId": 260380494,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5815075c70bf5113d4ab2ab4fc078913674a27ce",
                "title": "Machine Learning and Physics: A Survey of Integrated Models",
                "abstract": "Predictive modeling of various systems around the world is extremely essential from the physics and engineering perspectives. The recognition of different systems and the capacity to predict their future behavior can lead to numerous significant applications. For the most part, physics is frequently used to model different systems. Using physical modeling can also very well help the resolution of complexity and achieve superior performance with the emerging field of novel artificial intelligence and the challenges associated with it. Physical modeling provides data and knowledge that offer meaningful and complementary understanding about the system. So, by using enriched data and training phases, the overall general integrated model achieves enhanced accuracy. The effectiveness of hybrid physics-guided or machine learning-guided models has been validated by experimental results of diverse use cases. Increased accuracy, interpretability, and transparency are the results of such hybrid models. In this paper, we provide a detailed overview of how machine learning and physics can be integrated into an interactive approach. Regarding this, we propose a classification of possible interactions between physical modeling and machine learning techniques. Our classification includes three types of approaches: (1) physics-guided machine learning (2) machine learning-guided physics, and (3) mutually-guided physics and ML. We studied the models and specifications for each of these three approaches in-depth for this survey.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2226458277",
                        "name": "Azra Seyyedi"
                    },
                    {
                        "authorId": "2793286",
                        "name": "M. Bohlouli"
                    },
                    {
                        "authorId": "2199182748",
                        "name": "SeyedEhsan Nedaaee Oskoee"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Often, these approaches speed up classical methods [68, 30, 2], augment weaknesses in end-to-end deep learning solvers [41], or provide guarantees that deep networks by themselves, do not [29, 10].",
                "On the other hand, works like [29, 46, 53] apply neural networks iteratively multiple times, improving the obtained approximation of the solution each time."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f8a753c78e7e6760401682a41b95b74b4363142d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-17486",
                    "ArXiv": "2306.17486",
                    "DOI": "10.48550/arXiv.2306.17486",
                    "CorpusId": 260285174
                },
                "corpusId": 260285174,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f8a753c78e7e6760401682a41b95b74b4363142d",
                "title": "Multigrid-Augmented Deep Learning for the Helmholtz Equation: Better Scalability with Compact Implicit Layers",
                "abstract": "We present a deep learning-based iterative approach to solve the discrete heterogeneous Helmholtz equation for high wavenumbers. Combining classical iterative multigrid solvers and convolutional neural networks (CNNs) via preconditioning, we obtain a learned neural solver that is faster and scales better than a standard multigrid solver. Our approach offers three main contributions over previous neural methods of this kind. First, we construct a multilevel U-Net-like encoder-solver CNN with an implicit layer on the coarsest grid of the U-Net, where convolution kernels are inverted. This alleviates the field of view problem in CNNs and allows better scalability. Second, we improve upon the previous CNN preconditioner in terms of the number of parameters, computation time, and convergence rates. Third, we propose a multiscale training approach that enables the network to scale to problems of previously unseen dimensions while still maintaining a reasonable training procedure. Our encoder-solver architecture can be used to generalize over different slowness models of various difficulties and is efficient at solving for many right-hand sides per slowness model. We demonstrate the benefits of our novel architecture with numerical experiments on a variety of heterogeneous two-dimensional problems at high wavenumbers.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2228823757",
                        "name": "Bar Lerer"
                    },
                    {
                        "authorId": "1419449987",
                        "name": "Ido Ben-Yair"
                    },
                    {
                        "authorId": "2784349",
                        "name": "Eran Treister"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "As an alternative to MLPs and PINNs, convolutional neural networks (CNNs) have also been used to solve PDEs, either as independent solvers [12, 13], or as part of fluid solvers, such as to perform pressure projection of incompressible fluid flows [14, 15], to model acoustic wave propagation [16], or to solve the Poisson equation for plasma fluid flows [17]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "da325e7b887fd0bbb6aca7a76e2cb4933b6504ce",
                "externalIds": {
                    "DBLP": "conf/pasc/IllarramendiBAL23",
                    "DOI": "10.1145/3592979.3593416",
                    "CorpusId": 259253604
                },
                "corpusId": 259253604,
                "publicationVenue": {
                    "id": "da7877ef-8cd5-42d7-ad87-5368bd31dd98",
                    "name": "Platform for Advanced Scientific Computing Conference",
                    "type": "conference",
                    "alternate_names": [
                        "PASC",
                        "Platf Adv Sci Comput Conf"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/da325e7b887fd0bbb6aca7a76e2cb4933b6504ce",
                "title": "Performance Study of Convolutional Neural Network Architectures for 3D Incompressible Flow Simulations",
                "abstract": "Recently, correctly handling spatial information from multiple scales has proven to be essential in Machine Learning (ML) applications on Computational Fluid Dynamics (CFD) problems. For these type of applications, Convolutional Neural Networks (CNN) that use Multiple Downsampled Branches (MDBs) to efficiently encode spatial information from different spatial scales have proven to be some of the most successful architectures. However, not many guidelines exist to build these architectures, particularly when applied to more challenging 3D configurations. Thus, this work focuses on studying the impact of the choice of the number of down-sampled branches, accuracy and performance-wise in 3D incompressible fluid test cases, where a CNN is used to solve the Poisson equation. The influence of this parameter is assessed by performing multiple trainings of Unet architectures with varying MDBs on a cloud-computing environment. These trained networks are then tested on two 3D CFD problems: a plume and a Von Karman vortex street at various operating points, where the solution of the neural network is coupled to a nonlinear advection equation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2138996792",
                        "name": "Ekhi Ajuria Illarramendi"
                    },
                    {
                        "authorId": "92033612",
                        "name": "M. Bauerheim"
                    },
                    {
                        "authorId": "49571094",
                        "name": "N. Ashton"
                    },
                    {
                        "authorId": "2220554072",
                        "name": "Coretin Lapeyre"
                    },
                    {
                        "authorId": "9448078",
                        "name": "B. Cuenot"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "87d432941b902b23a44d5eb3445b96169d2010bb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-07604",
                    "ArXiv": "2306.07604",
                    "DOI": "10.48550/arXiv.2306.07604",
                    "CorpusId": 259144899
                },
                "corpusId": 259144899,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/87d432941b902b23a44d5eb3445b96169d2010bb",
                "title": "Towards a Machine-Learned Poisson Solver for Low-Temperature Plasma Simulations in Complex Geometries",
                "abstract": "Poisson's equation plays an important role in modeling many physical systems. In electrostatic self-consistent low-temperature plasma (LTP) simulations, Poisson's equation is solved at each simulation time step, which can amount to a significant computational cost for the entire simulation. In this paper, we describe the development of a generic machine-learned Poisson solver specifically designed for the requirements of LTP simulations in complex 2D reactor geometries on structured Cartesian grids. Here, the reactor geometries can consist of inner electrodes and dielectric materials as often found in LTP simulations. The approach leverages a hybrid CNN-transformer network architecture in combination with a weighted multiterm loss function. We train the network using highly-randomized synthetic data to ensure the generalizability of the learned solver to unseen reactor geometries. The results demonstrate that the learned solver is able to produce quantitatively and qualitatively accurate solutions. Furthermore, it generalizes well on new reactor geometries such as reference geometries found in the literature. To increase the numerical accuracy of the solutions required in LTP simulations, we employ a conventional iterative solver to refine the raw predictions, especially to recover the high-frequency features not resolved by the initial prediction. With this, the proposed learned Poisson solver provides the required accuracy and is potentially faster than a pure GPU-based conventional iterative solver. This opens up new possibilities for developing a generic and high-performing learned Poisson solver for LTP systems in complex geometries.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2191400301",
                        "name": "Ihda Chaerony Siffa"
                    },
                    {
                        "authorId": "29661032",
                        "name": "M. Becker"
                    },
                    {
                        "authorId": "5734556",
                        "name": "K. Weltmann"
                    },
                    {
                        "authorId": "6106904",
                        "name": "J. Trieschmann"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In [10], iterative solvers for elliptic problems were learned using neural networks, but these were built upon existing solvers in order to guarantee high order convergence."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8d4753b5e1a2336646a42eefcf3c9abdfb5a7452",
                "externalIds": {
                    "DBLP": "journals/jscic/SandeFF23",
                    "DOI": "10.1007/s10915-023-02260-z",
                    "CorpusId": 259114765
                },
                "corpusId": 259114765,
                "publicationVenue": {
                    "id": "2fd362b7-4859-41eb-a9f2-f12a77f1c997",
                    "name": "Journal of Scientific Computing",
                    "type": "journal",
                    "alternate_names": [
                        "J Sci Comput"
                    ],
                    "issn": "0885-7474",
                    "url": "http://www.kluweronline.com/issn/0885-7474/contents",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10915"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8d4753b5e1a2336646a42eefcf3c9abdfb5a7452",
                "title": "Accelerating Explicit Time-Stepping with Spatially Variable Time Steps Through Machine Learning",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2131076741",
                        "name": "Kiera van der Sande"
                    },
                    {
                        "authorId": "1751907",
                        "name": "N. Flyer"
                    },
                    {
                        "authorId": "2502287",
                        "name": "B. Fornberg"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "While some approaches use deep learning to accelerate traditional CFD solvers (Hsieh et al., 2019; Kochkov et al., 2021a), a certain body of research treats the flow problems as problems defined over a cartesian grid or an irregular mesh and uses techniques involving convolutional or graph neural operators to predict the flow fields (Hennigh, 2017; Jiang et al.",
                "While some approaches use deep learning to accelerate traditional CFD solvers (Hsieh et al., 2019; Kochkov et al., 2021a), a certain body of research treats the flow problems as problems defined over a cartesian grid or an irregular mesh and uses techniques involving convolutional or graph neural\u2026"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e726ec54bbbadaaa1c1e7416755da4bbf1d0db23",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-06034",
                    "ArXiv": "2306.06034",
                    "DOI": "10.48550/arXiv.2306.06034",
                    "CorpusId": 259129294
                },
                "corpusId": 259129294,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e726ec54bbbadaaa1c1e7416755da4bbf1d0db23",
                "title": "RANS-PINN based Simulation Surrogates for Predicting Turbulent Flows",
                "abstract": "Physics-informed neural networks (PINNs) provide a framework to build surrogate models for dynamical systems governed by differential equations. During the learning process, PINNs incorporate a physics-based regularization term within the loss function to enhance generalization performance. Since simulating dynamics controlled by partial differential equations (PDEs) can be computationally expensive, PINNs have gained popularity in learning parametric surrogates for fluid flow problems governed by Navier-Stokes equations. In this work, we introduce RANS-PINN, a modified PINN framework, to predict flow fields (i.e., velocity and pressure) in high Reynolds number turbulent flow regimes. To account for the additional complexity introduced by turbulence, RANS-PINN employs a 2-equation eddy viscosity model based on a Reynolds-averaged Navier-Stokes (RANS) formulation. Furthermore, we adopt a novel training approach that ensures effective initialization and balance among the various components of the loss function. The effectiveness of the RANS-PINN framework is then demonstrated using a parametric PINN.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "100979416",
                        "name": "Shinjan Ghosh"
                    },
                    {
                        "authorId": "2019491887",
                        "name": "Amit Chakraborty"
                    },
                    {
                        "authorId": "2175781464",
                        "name": "Georgia Olympia Brikis"
                    },
                    {
                        "authorId": "32553531",
                        "name": "Biswadip Dey"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "20cd09f73421bae53a37c17d0ecc2c5147abf6c0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-14118",
                    "ArXiv": "2304.14118",
                    "DOI": "10.48550/arXiv.2304.14118",
                    "CorpusId": 258352559
                },
                "corpusId": 258352559,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/20cd09f73421bae53a37c17d0ecc2c5147abf6c0",
                "title": "Learning Neural PDE Solvers with Parameter-Guided Channel Attention",
                "abstract": "Scientific Machine Learning (SciML) is concerned with the development of learned emulators of physical systems governed by partial differential equations (PDE). In application domains such as weather forecasting, molecular dynamics, and inverse design, ML-based surrogate models are increasingly used to augment or replace inefficient and often non-differentiable numerical simulation algorithms. While a number of ML-based methods for approximating the solutions of PDEs have been proposed in recent years, they typically do not adapt to the parameters of the PDEs, making it difficult to generalize to PDE parameters not seen during training. We propose a Channel Attention mechanism guided by PDE Parameter Embeddings (CAPE) component for neural surrogate models and a simple yet effective curriculum learning strategy. The CAPE module can be combined with neural PDE solvers allowing them to adapt to unseen PDE parameters. The curriculum learning strategy provides a seamless transition between teacher-forcing and fully auto-regressive training. We compare CAPE in conjunction with the curriculum learning strategy using a popular PDE benchmark and obtain consistent and significant improvements over the baseline models. The experiments also show several advantages of CAPE, such as its increased ability to generalize to unseen PDE parameters without large increases inference time and parameter count.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "19909384",
                        "name": "M. Takamoto"
                    },
                    {
                        "authorId": "2819104",
                        "name": "F. Alesiani"
                    },
                    {
                        "authorId": "2780262",
                        "name": "Mathias Niepert"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2f403d194b42d10c3a438736388c8812831b1361",
                "externalIds": {
                    "ArXiv": "2304.12944",
                    "DBLP": "conf/icml/SongKSW23",
                    "DOI": "10.48550/arXiv.2304.12944",
                    "CorpusId": 258309133
                },
                "corpusId": 258309133,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2f403d194b42d10c3a438736388c8812831b1361",
                "title": "Latent Traversals in Generative Models as Potential Flows",
                "abstract": "Despite the significant recent progress in deep generative models, the underlying structure of their latent spaces is still poorly understood, thereby making the task of performing semantically meaningful latent traversals an open research challenge. Most prior work has aimed to solve this challenge by modeling latent structures linearly, and finding corresponding linear directions which result in `disentangled' generations. In this work, we instead propose to model latent structures with a learned dynamic potential landscape, thereby performing latent traversals as the flow of samples down the landscape's gradient. Inspired by physics, optimal transport, and neuroscience, these potential landscapes are learned as physically realistic partial differential equations, thereby allowing them to flexibly vary over both space and time. To achieve disentanglement, multiple potentials are learned simultaneously, and are constrained by a classifier to be distinct and semantically self-consistent. Experimentally, we demonstrate that our method achieves both more qualitatively and quantitatively disentangled trajectories than state-of-the-art baselines. Further, we demonstrate that our method can be integrated as a regularization term during training, thereby acting as an inductive bias towards the learning of structured representations, ultimately improving model likelihood on similarly structured data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152601878",
                        "name": "Yue Song"
                    },
                    {
                        "authorId": "2215270642",
                        "name": "Andy Keller"
                    },
                    {
                        "authorId": "1429806753",
                        "name": "N. Sebe"
                    },
                    {
                        "authorId": "1678311",
                        "name": "M. Welling"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fb87e864e66668d152f2ec5764c036d076ca9493",
                "externalIds": {
                    "ArXiv": "2303.16110",
                    "DBLP": "journals/corr/abs-2303-16110",
                    "DOI": "10.48550/arXiv.2303.16110",
                    "CorpusId": 257771643
                },
                "corpusId": 257771643,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fb87e864e66668d152f2ec5764c036d076ca9493",
                "title": "Invariant preservation in machine learned PDE solvers via error correction",
                "abstract": "Machine learned partial differential equation (PDE) solvers trade the reliability of standard numerical methods for potential gains in accuracy and/or speed. The only way for a solver to guarantee that it outputs the exact solution is to use a convergent method in the limit that the grid spacing $\\Delta x$ and timestep $\\Delta t$ approach zero. Machine learned solvers, which learn to update the solution at large $\\Delta x$ and/or $\\Delta t$, can never guarantee perfect accuracy. Some amount of error is inevitable, so the question becomes: how do we constrain machine learned solvers to give us the sorts of errors that we are willing to tolerate? In this paper, we design more reliable machine learned PDE solvers by preserving discrete analogues of the continuous invariants of the underlying PDE. Examples of such invariants include conservation of mass, conservation of energy, the second law of thermodynamics, and/or non-negative density. Our key insight is simple: to preserve invariants, at each timestep apply an error-correcting algorithm to the update rule. Though this strategy is different from how standard solvers preserve invariants, it is necessary to retain the flexibility that allows machine learned solvers to be accurate at large $\\Delta x$ and/or $\\Delta t$. This strategy can be applied to any autoregressive solver for any time-dependent PDE in arbitrary geometries with arbitrary boundary conditions. Although this strategy is very general, the specific error-correcting algorithms need to be tailored to the invariants of the underlying equations as well as to the solution representation and time-stepping scheme of the solver. The error-correcting algorithms we introduce have two key properties. First, by preserving the right invariants they guarantee numerical stability. Second, in closed or periodic systems they do so without degrading the accuracy of an already-accurate solver.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51932783",
                        "name": "N. McGreivy"
                    },
                    {
                        "authorId": "2169938201",
                        "name": "Ammar Hakim"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Sec-\nond, the local differential operators that span this functional space can be treated as numerically employing convolution kernels (Hsieh et al., 2018; Lin et al., 2013).",
                "Second, the local differential operators that span this functional space can be treated as numerically employing convolution kernels (Hsieh et al., 2018; Lin et al., 2013)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7e71c2e64fc36a288f936c99260121ecc24be287",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-07194",
                    "ArXiv": "2303.07194",
                    "DOI": "10.48550/arXiv.2303.07194",
                    "CorpusId": 257496259
                },
                "corpusId": 257496259,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7e71c2e64fc36a288f936c99260121ecc24be287",
                "title": "Neural Partial Differential Equations with Functional Convolution",
                "abstract": "We present a lightweighted neural PDE representation to discover the hidden structure and predict the solution of different nonlinear PDEs. Our key idea is to leverage the prior of ``translational similarity'' of numerical PDE differential operators to drastically reduce the scale of learning model and training data. We implemented three central network components, including a neural functional convolution operator, a Picard forward iterative procedure, and an adjoint backward gradient calculator. Our novel paradigm fully leverages the multifaceted priors that stem from the sparse and smooth nature of the physical PDE solution manifold and the various mature numerical techniques such as adjoint solver, linearization, and iterative procedure to accelerate the computation. We demonstrate the efficacy of our method by robustly discovering the model and accurately predicting the solutions of various types of PDEs with small-scale networks and training sets. We highlight that all the PDE examples we showed were trained with up to 8 data samples and within 325 network parameters.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2109685241",
                        "name": "Z. Wu"
                    },
                    {
                        "authorId": "1491320619",
                        "name": "Xingzhe He"
                    },
                    {
                        "authorId": "2195022734",
                        "name": "Yijun Li"
                    },
                    {
                        "authorId": "2154171530",
                        "name": "Cheng Yang"
                    },
                    {
                        "authorId": "2186805413",
                        "name": "Rui Liu"
                    },
                    {
                        "authorId": "47722204",
                        "name": "S. Xiong"
                    },
                    {
                        "authorId": "92035057",
                        "name": "Bo Zhu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Another different approach known as autoregressive methods was developed in [1, 5, 15, 17], where the PDEs are simulated iteratively, resembling conventional numerical methods with time marching."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e1e4997f0d57c186f38213d5ea4e76a02e31b691",
                "externalIds": {
                    "ArXiv": "2302.10398",
                    "DBLP": "journals/corr/abs-2302-10398",
                    "DOI": "10.48550/arXiv.2302.10398",
                    "CorpusId": 257050782
                },
                "corpusId": 257050782,
                "publicationVenue": {
                    "id": "58606051-2e63-4034-8496-9d4ed773bb49",
                    "name": "Journal of Computational Physics",
                    "type": "journal",
                    "alternate_names": [
                        "J Comput Phys"
                    ],
                    "issn": "0021-9991",
                    "url": "http://www.elsevier.com/locate/jcp",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00219991",
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622866/description#description",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e1e4997f0d57c186f38213d5ea4e76a02e31b691",
                "title": "A learned conservative semi-Lagrangian finite volume scheme for transport simulations",
                "abstract": "Semi-Lagrangian (SL) schemes are known as a major numerical tool for solving transport equations with many advantages and have been widely deployed in the fields of computational fluid dynamics, plasma physics modeling, numerical weather prediction, among others. In this work, we develop a novel machine learning-assisted approach to accelerate the conventional SL finite volume (FV) schemes. The proposed scheme avoids the expensive tracking of upstream cells but attempts to learn the SL discretization from the data by incorporating specific inductive biases in the neural network, significantly simplifying the algorithm implementation and leading to improved efficiency. In addition, the method delivers sharp shock transitions and a level of accuracy that would typically require a much finer grid with traditional transport solvers. Numerical tests demonstrate the effectiveness and efficiency of the proposed method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2109384982",
                        "name": "Yongsheng Chen"
                    },
                    {
                        "authorId": "2107247257",
                        "name": "W. Guo"
                    },
                    {
                        "authorId": "3321687",
                        "name": "Xinghui Zhong"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "85f76df0135ea32529cf382e628b38a2dc136bda",
                "externalIds": {
                    "DBLP": "conf/icml/HuangSM0GZL23",
                    "ArXiv": "2302.10255",
                    "DOI": "10.48550/arXiv.2302.10255",
                    "CorpusId": 253062211
                },
                "corpusId": 253062211,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/85f76df0135ea32529cf382e628b38a2dc136bda",
                "title": "NeuralStagger: accelerating physics-constrained neural PDE solver with spatial-temporal decomposition",
                "abstract": "Neural networks have shown great potential in accelerating the solution of partial differential equations (PDEs). Recently, there has been a growing interest in introducing physics constraints into training neural PDE solvers to reduce the use of costly data and improve the generalization ability. However, these physics constraints, based on certain finite dimensional approximations over the function space, must resolve the smallest scaled physics to ensure the accuracy and stability of the simulation, resulting in high computational costs from large input, output, and neural networks. This paper proposes a general acceleration methodology called NeuralStagger by spatially and temporally decomposing the original learning tasks into several coarser-resolution subtasks. We define a coarse-resolution neural solver for each subtask, which requires fewer computational resources, and jointly train them with the vanilla physics-constrained loss by simply arranging their outputs to reconstruct the original solution. Due to the perfect parallelism between them, the solution is achieved as fast as a coarse-resolution neural solver. In addition, the trained solvers bring the flexibility of simulating with multiple levels of resolution. We demonstrate the successful application of NeuralStagger on 2D and 3D fluid dynamics simulations, which leads to an additional $10\\sim100\\times$ speed-up. Moreover, the experiment also shows that the learned model could be well used for optimal control.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2420875",
                        "name": "Xinquan Huang"
                    },
                    {
                        "authorId": "19143277",
                        "name": "Wenlei Shi"
                    },
                    {
                        "authorId": "47580728",
                        "name": "Qi Meng"
                    },
                    {
                        "authorId": null,
                        "name": "Yue Wang"
                    },
                    {
                        "authorId": "2149397075",
                        "name": "Xiaotian Gao"
                    },
                    {
                        "authorId": "2108830650",
                        "name": "Jia Zhang"
                    },
                    {
                        "authorId": "2110264835",
                        "name": "Tie-Yan Liu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "093ca5671e581e9223a051d1c36a7b727e01668c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-10891",
                    "ArXiv": "2302.10891",
                    "DOI": "10.48550/arXiv.2302.10891",
                    "CorpusId": 257078712
                },
                "corpusId": 257078712,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/093ca5671e581e9223a051d1c36a7b727e01668c",
                "title": "An Implicit GNN Solver for Poisson-like problems",
                "abstract": "This paper presents $\\Psi$-GNN, a novel Graph Neural Network (GNN) approach for solving the ubiquitous Poisson PDE problems with mixed boundary conditions. By leveraging the Implicit Layer Theory, $\\Psi$-GNN models an ''infinitely'' deep network, thus avoiding the empirical tuning of the number of required Message Passing layers to attain the solution. Its original architecture explicitly takes into account the boundary conditions, a critical prerequisite for physical applications, and is able to adapt to any initially provided solution. $\\Psi$-GNN is trained using a ''physics-informed'' loss, and the training process is stable by design, and insensitive to its initialization. Furthermore, the consistency of the approach is theoretically proven, and its flexibility and generalization efficiency are experimentally demonstrated: the same learned model can accurately handle unstructured meshes of various sizes, as well as different boundary conditions. To the best of our knowledge, $\\Psi$-GNN is the first physics-informed GNN-based method that can handle various unstructured domains, boundary conditions and initial solutions while also providing convergence guarantees.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2191687664",
                        "name": "Matthieu Nastorg"
                    },
                    {
                        "authorId": "89936237",
                        "name": "M. Bucci"
                    },
                    {
                        "authorId": "91672048",
                        "name": "T. Faney"
                    },
                    {
                        "authorId": "2117782",
                        "name": "J. Gratien"
                    },
                    {
                        "authorId": "1926165",
                        "name": "G. Charpiat"
                    },
                    {
                        "authorId": "2066691430",
                        "name": "M. Schoenauer"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "There are researches about the so-called neural augmentation where a neural component is added to finite elements [11], multigrid solvers [10], and eikonal solvers [19]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c078b38f869c79035e51aef2cca727f3e95921d6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-04100",
                    "ArXiv": "2212.04100",
                    "DOI": "10.1109/CCIS57298.2022.10016380",
                    "CorpusId": 254408991
                },
                "corpusId": 254408991,
                "publicationVenue": {
                    "id": "b8e3fb18-a48d-49f0-96bd-38cca7124b0d",
                    "name": "International Conference on Cloud Computing and Intelligence Systems",
                    "type": "conference",
                    "alternate_names": [
                        "CCIS",
                        "Int Conf Cloud Comput Intell Syst"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c078b38f869c79035e51aef2cca727f3e95921d6",
                "title": "Physics-guided Data Augmentation for Learning the Solution Operator of Linear Differential Equations",
                "abstract": "Neural networks, especially the recent proposed neural operator models, are increasingly being used to find the solution operator of differential equations. Compared to traditional numerical solvers, they are much faster and more efficient in practical applications. However, one critical issue is that training neural operator models require large amount of ground truth data, which usually comes from the slow numerical solvers. In this paper, we propose a physics-guided data augmentation (PGDA) method to improve the accuracy and generalization of neural operator models. Training data is augmented naturally through the physical properties of differential equations such as linearity and translation. We demonstrate the advantage of PGDA on a variety of linear differential equations, showing that PGDA can improve the sample complexity and is robust to distributional shift.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46215276",
                        "name": "Yemo Li"
                    },
                    {
                        "authorId": "2194408191",
                        "name": "Yiwen Pang"
                    },
                    {
                        "authorId": "2064114309",
                        "name": "Bin Shan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "This differs from other learned PDE solvers and surrogate models, which are almost always trained for a single mesh or geometry and cannot be used when the geometry varies [11, 12, 14, 26]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bd29458e2f46280deed89b7d317cf225afb753c7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-01604",
                    "ArXiv": "2211.01604",
                    "DOI": "10.48550/arXiv.2211.01604",
                    "CorpusId": 253265115
                },
                "corpusId": 253265115,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bd29458e2f46280deed89b7d317cf225afb753c7",
                "title": "Meta-PDE: Learning to Solve PDEs Quickly Without a Mesh",
                "abstract": "Partial differential equations (PDEs) are often computationally challenging to solve, and in many settings many related PDEs must be be solved either at every timestep or for a variety of candidate boundary conditions, parameters, or geometric domains. We present a meta-learning based method which learns to rapidly solve problems from a distribution of related PDEs. We use meta-learning (MAML and LEAP) to identify initializations for a neural network representation of the PDE solution such that a residual of the PDE can be quickly minimized on a novel task. We apply our meta-solving approach to a nonlinear Poisson's equation, 1D Burgers' equation, and hyperelasticity equations with varying parameters, geometries, and boundary conditions. The resulting Meta-PDE method finds qualitatively accurate solutions to most problems within a few gradient steps; for the nonlinear Poisson and hyper-elasticity equation this results in an intermediate accuracy approximation up to an order of magnitude faster than a baseline finite element analysis (FEA) solver with equivalent accuracy. In comparison to other learned solvers and surrogate models, this meta-learning approach can be trained without supervision from expensive ground-truth data, does not require a mesh, and can even be used when the geometry and topology varies between tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2066170440",
                        "name": "Tian Qin"
                    },
                    {
                        "authorId": "1987266",
                        "name": "Alex Beatson"
                    },
                    {
                        "authorId": "4781463",
                        "name": "Deniz Oktay"
                    },
                    {
                        "authorId": "51932783",
                        "name": "N. McGreivy"
                    },
                    {
                        "authorId": "2075454640",
                        "name": "Ryan P. Adams"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "PDEs are a fundamental modeling techniques, and designing neural networks-aided solvers, particularly in high-dimensions, is of widespread usage in many scientific domains (Hsieh et al., 2019; Brandstetter et al., 2022)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "473f7750d2d576d1729e935636a9a6f3466f2151",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-12101",
                    "ArXiv": "2210.12101",
                    "DOI": "10.48550/arXiv.2210.12101",
                    "CorpusId": 253080761
                },
                "corpusId": 253080761,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/473f7750d2d576d1729e935636a9a6f3466f2151",
                "title": "Neural Network Approximations of PDEs Beyond Linearity: Representational Perspective",
                "abstract": "A burgeoning line of research leverages deep neural networks to approximate the solutions to high dimensional PDEs, opening lines of theoretical inquiry focused on explaining how it is that these models appear to evade the curse of dimensionality. However, most prior theoretical analyses have been limited to linear PDEs. In this work, we take a step towards studying the representational power of neural networks for approximating solutions to nonlinear PDEs. We focus on a class of PDEs known as \\emph{nonlinear elliptic variational PDEs}, whose solutions minimize an \\emph{Euler-Lagrange} energy functional $\\mathcal{E}(u) = \\int_\\Omega L(x, u(x), \\nabla u(x)) - f(x) u(x)dx$. We show that if composing a function with Barron norm $b$ with partial derivatives of $L$ produces a function of Barron norm at most $B_L b^p$, the solution to the PDE can be $\\epsilon$-approximated in the $L^2$ sense by a function with Barron norm $O\\left(\\left(dB_L\\right)^{\\max\\{p \\log(1/ \\epsilon), p^{\\log(1/\\epsilon)}\\}}\\right)$. By a classical result due to Barron [1993], this correspondingly bounds the size of a 2-layer neural network needed to approximate the solution. Treating $p, \\epsilon, B_L$ as constants, this quantity is polynomial in dimension, thus showing neural networks can evade the curse of dimensionality. Our proof technique involves neurally simulating (preconditioned) gradient in an appropriate Hilbert space, which converges exponentially fast to the solution of the PDE, and such that we can bound the increase of the Barron norm at each iterate. Our results subsume and substantially generalize analogous prior results for linear elliptic PDEs over a unit hypercube.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8268761",
                        "name": "Tanya Marwah"
                    },
                    {
                        "authorId": "32219137",
                        "name": "Zachary Chase Lipton"
                    },
                    {
                        "authorId": "145313246",
                        "name": "Jianfeng Lu"
                    },
                    {
                        "authorId": "3181040",
                        "name": "Andrej Risteski"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[10] utilized a convolutional neural network (CNN) to accelerate the convergence of Jacobi method."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "23d09e9ac2bf58f1eb8f4a343bfa2f44f53d5872",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-03881",
                    "ArXiv": "2210.03881",
                    "DOI": "10.48550/arXiv.2210.03881",
                    "CorpusId": 252780258
                },
                "corpusId": 252780258,
                "publicationVenue": {
                    "id": "6175efe8-6f8e-4cbe-8cee-d154f4e78627",
                    "name": "Mathematics",
                    "issn": "2227-7390",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-283014",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-283014",
                        "https://www.mdpi.com/journal/mathematics"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/23d09e9ac2bf58f1eb8f4a343bfa2f44f53d5872",
                "title": "Fourier Neural Solver for large sparse linear algebraic systems",
                "abstract": "Large sparse linear algebraic systems can be found in a variety of scientific and engineering fields and many scientists strive to solve them in an efficient and robust manner. In this paper, we propose an interpretable neural solver, the Fourier neural solver (FNS), to address them. FNS is based on deep learning and a fast Fourier transform. Because the error between the iterative solution and the ground truth involves a wide range of frequency modes, the FNS combines a stationary iterative method and frequency space correction to eliminate different components of the error. Local Fourier analysis shows that the FNS can pick up on the error components in frequency space that are challenging to eliminate with stationary methods. Numerical experiments on the anisotropic diffusion equation , convection\u2013diffusion equation, and Helmholtz equation show that the FNS is more efficient and more robust than the state-of-the-art neural solver.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2052275400",
                        "name": "Chenwei Cui"
                    },
                    {
                        "authorId": "2052123431",
                        "name": "K. Jiang"
                    },
                    {
                        "authorId": "2118113631",
                        "name": "Yun Liu"
                    },
                    {
                        "authorId": "145918751",
                        "name": "S. Shu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Various works are numerical-neural hybrid approaches where the computation graph of the solver is preserved and heuristically-chosen parameters are left for the neural network to predict (Bar-Sinai et al., 2019; Kochkov et al., 2021; Greenfeld et al., 2019; Hsieh et al., 2019; Praditia et al., 2021; Um et al., 2020; Garcia Satorras et al., 2019)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "00ffcc997b0bcf0dbf60ff04c29d701919582a62",
                "externalIds": {
                    "ArXiv": "2209.15616",
                    "DBLP": "journals/corr/abs-2209-15616",
                    "DOI": "10.48550/arXiv.2209.15616",
                    "CorpusId": 252668507
                },
                "corpusId": 252668507,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/00ffcc997b0bcf0dbf60ff04c29d701919582a62",
                "title": "Towards Multi-spatiotemporal-scale Generalized PDE Modeling",
                "abstract": "Partial differential equations (PDEs) are central to describing complex physical system simulations. Their expensive solution techniques have led to an increased interest in deep neural network based surrogates. However, the practical utility of training such surrogates is contingent on their ability to model complex multi-scale spatio-temporal phenomena. Various neural network architectures have been proposed to target such phenomena, most notably Fourier Neural Operators (FNOs), which give a natural handle over local&global spatial information via parameterization of different Fourier modes, and U-Nets which treat local and global information via downsampling and upsampling paths. However, generalizing across different equation parameters or time-scales still remains a challenge. In this work, we make a comprehensive comparison between various FNO, ResNet, and U-Net like approaches to fluid mechanics problems in both vorticity-stream and velocity function form. For U-Nets, we transfer recent architectural improvements from computer vision, most notably from object segmentation and generative modeling. We further analyze the design considerations for using FNO layers to improve performance of U-Net architectures without major degradation of computational cost. Finally, we show promising results on generalization to different PDE parameters and time-scales with a single surrogate model. Source code for our PyTorch benchmark framework is available at https://github.com/microsoft/pdearena.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "38303675",
                        "name": "Jayesh K. Gupta"
                    },
                    {
                        "authorId": "78843496",
                        "name": "Johannes Brandstetter"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "84959e211a767f902cbf1695ec54a5b50148020f",
                "externalIds": {
                    "DBLP": "conf/iclr/BrandstetterBWG23",
                    "ArXiv": "2209.04934",
                    "DOI": "10.48550/arXiv.2209.04934",
                    "CorpusId": 252199584
                },
                "corpusId": 252199584,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/84959e211a767f902cbf1695ec54a5b50148020f",
                "title": "Clifford Neural Layers for PDE Modeling",
                "abstract": "Partial differential equations (PDEs) see widespread use in sciences and engineering to describe simulation of physical processes as scalar and vector fields interacting and coevolving over time. Due to the computationally expensive nature of their standard solution methods, neural PDE surrogates have become an active research topic to accelerate these simulations. However, current methods do not explicitly take into account the relationship between different fields and their internal components, which are often correlated. Viewing the time evolution of such correlated fields through the lens of multivector fields allows us to overcome these limitations. Multivector fields consist of scalar, vector, as well as higher-order components, such as bivectors and trivectors. Their algebraic properties, such as multiplication, addition and other arithmetic operations can be described by Clifford algebras. To our knowledge, this paper presents the first usage of such multivector representations together with Clifford convolutions and Clifford Fourier transforms in the context of deep learning. The resulting Clifford neural layers are universally applicable and will find direct use in the areas of fluid dynamics, weather forecasting, and the modeling of physical systems in general. We empirically evaluate the benefit of Clifford neural layers by replacing convolution and Fourier operations in common neural PDE surrogates by their Clifford counterparts on 2D Navier-Stokes and weather modeling tasks, as well as 3D Maxwell equations. For similar parameter count, Clifford neural layers consistently improve generalization capabilities of the tested neural PDE surrogates. Source code for our PyTorch implementation is available at https://microsoft.github.io/cliffordlayers/.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "78843496",
                        "name": "Johannes Brandstetter"
                    },
                    {
                        "authorId": "9965217",
                        "name": "Rianne van den Berg"
                    },
                    {
                        "authorId": "1678311",
                        "name": "M. Welling"
                    },
                    {
                        "authorId": "38303675",
                        "name": "Jayesh K. Gupta"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "57dc42e19a3c5069360a9d49e8bbec135559775a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-13273",
                    "ArXiv": "2208.13273",
                    "DOI": "10.48550/arXiv.2208.13273",
                    "CorpusId": 251903572
                },
                "corpusId": 251903572,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/57dc42e19a3c5069360a9d49e8bbec135559775a",
                "title": "A Hybrid Iterative Numerical Transferable Solver (HINTS) for PDEs Based on Deep Operator Network and Relaxation Methods",
                "abstract": "Iterative solvers of linear systems are a key component for the numerical solutions of partial differential equations (PDEs). While there have been intensive studies through past decades on classical methods such as Jacobi, Gauss-Seidel, conjugate gradient, multigrid methods and their more advanced variants, there is still a pressing need to develop faster, more robust and reliable solvers. Based on recent advances in scientific deep learning for operator regression, we propose HINTS, a hybrid, iterative, numerical, and transferable solver for differential equations. HINTS combines standard relaxation methods and the Deep Operator Network (DeepONet). Compared to standard numerical solvers, HINTS is capable of providing faster solutions for a wide class of differential equations, while preserving the accuracy close to machine zero. Through an eigenmode analysis, we find that the individual solvers in HINTS target distinct regions in the spectrum of eigenmodes, resulting in a uniform convergence rate and hence exceptional performance of the hybrid solver overall. Moreover, HINTS applies to equations in multidimensions, and is flexible with regards to computational domain and transferable to different discretizations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "114705598",
                        "name": "Enrui Zhang"
                    },
                    {
                        "authorId": "103588611",
                        "name": "Adar Kahana"
                    },
                    {
                        "authorId": "144342630",
                        "name": "Eli Turkel"
                    },
                    {
                        "authorId": "94583808",
                        "name": "Rishikesh Ranade"
                    },
                    {
                        "authorId": "143772779",
                        "name": "Jay Pathak"
                    },
                    {
                        "authorId": "1720124",
                        "name": "G. Karniadakis"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "41dcfa296794f97331715891fdcbb05f2ad055bf",
                "externalIds": {
                    "MAG": "3147263230",
                    "DOI": "10.1109/TAP.2021.3070152",
                    "CorpusId": 234331967
                },
                "corpusId": 234331967,
                "publicationVenue": {
                    "id": "64f6bc7f-a080-473b-8e71-044beeeadb13",
                    "name": "IEEE Transactions on Antennas and Propagation",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Antenna Propag"
                    ],
                    "issn": "0018-926X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=8",
                    "alternate_urls": [
                        "https://ieeeaps.org/aps_trans/index.htm",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=8234",
                        "http://ieeexplore.ieee.org/xpl/aboutJournal.jsp?punumber=8",
                        "https://ieeexplore.ieee.org/xpl/aboutJournal.jsp?punumber=8"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/41dcfa296794f97331715891fdcbb05f2ad055bf",
                "title": "Physics Embedded Deep Neural Network for Solving Volume Integral Equation: 2-D Case",
                "abstract": "The volume integral equation (VIE) that describes the forward scattering problem is generally solved by iterative methods, such as the conjugate gradient (CG) method. In this work, we unfold the CG method into an iterative deep neural network to accelerate solving the VIE. After the dielectric scatterer\u2019s relative permittivity and the incident field are input into the network, the total field is trained to converge to the ground truth iteratively. In the neural network, Green\u2019s function is taken as an explicit operator to describe wave physics, and the fast Fourier transform (FFT) is applied to accelerate the computation of volume integrations. The global influence of all points in the space is compressed into a layer by the volume integration. In numerical tests, we validate the accuracy, efficiency, and generalization ability of the proposed neural network, and investigate the feasibility of changing the input size and the frequency in the prediction. Results show that the network is scale-independent and adaptable to predict fields in a narrow frequency band. This work provides us a new perspective of incorporating both learned parameters and physics into numerical algorithms for fast computation, and has the potential of being applied in deep-learning-based inverse scattering problems.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2090408646",
                        "name": "Rui Guo"
                    },
                    {
                        "authorId": "2067161406",
                        "name": "Tao Shan"
                    },
                    {
                        "authorId": "50707034",
                        "name": "Xiaoqian Song"
                    },
                    {
                        "authorId": "47628788",
                        "name": "Maokun Li"
                    },
                    {
                        "authorId": "2019029201",
                        "name": "Fan Yang"
                    },
                    {
                        "authorId": "143879525",
                        "name": "Shenheng Xu"
                    },
                    {
                        "authorId": "145897902",
                        "name": "A. Abubakar"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "[29] proposed to use neural networks to modify acobi-style iterative solvers.",
                "[29] Hsieh JT, Zhao S, Eismann S, Mirabella L, Ermon S."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4e7fbbf5660eba7c3f19d278b8aefe3b36bf62b9",
                "externalIds": {
                    "DBLP": "journals/cg/TangACS22",
                    "DOI": "10.1016/j.cag.2022.07.016",
                    "CorpusId": 251143046
                },
                "corpusId": 251143046,
                "publicationVenue": {
                    "id": "ca858314-5beb-4241-a7e4-f7748b3f2081",
                    "name": "Computers & graphics",
                    "type": "journal",
                    "alternate_names": [
                        "Computer Graphics",
                        "Comput graph",
                        "Computers & Graphics",
                        "Comput  Graph",
                        "Comput Graph",
                        "Computer graphics",
                        "Comput  graph"
                    ],
                    "issn": "0097-8493",
                    "alternate_issns": [
                        "0097-8930",
                        "1558-4569"
                    ],
                    "url": "https://www.sciencedirect.com/journal/computers-and-graphics",
                    "alternate_urls": [
                        "http://portal.acm.org/siggraph/newsletter",
                        "https://dl.acm.org/newsletter/siggraph",
                        "http://www.sciencedirect.com/science/journal/00978493"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4e7fbbf5660eba7c3f19d278b8aefe3b36bf62b9",
                "title": "Neural Green's function for Laplacian systems",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "102583842",
                        "name": "Jingwei Tang"
                    },
                    {
                        "authorId": "40331104",
                        "name": "V. C. Azevedo"
                    },
                    {
                        "authorId": "3408135",
                        "name": "G. Cordonnier"
                    },
                    {
                        "authorId": "1789549",
                        "name": "B. Solenthaler"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "01c27abcb2525bb6ca1912c2bc0cab13cc5771d4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-14092",
                    "ArXiv": "2206.14092",
                    "DOI": "10.48550/arXiv.2206.14092",
                    "CorpusId": 250089399
                },
                "corpusId": 250089399,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/01c27abcb2525bb6ca1912c2bc0cab13cc5771d4",
                "title": "Learning the Solution Operator of Boundary Value Problems using Graph Neural Networks",
                "abstract": "As an alternative to classical numerical solvers for partial differential equations (PDEs) subject to boundary value constraints, there has been a surge of interest in investigating neural networks that can solve such problems efficiently. In this work, we design a general solution operator for two different time-independent PDEs using graph neural networks (GNNs) and spectral graph convolutions. We train the networks on simulated data from a finite elements solver on a variety of shapes and inhomogeneities. In contrast to previous works, we focus on the ability of the trained operator to generalize to previously unseen scenarios. Specifically, we test generalization to meshes with different shapes and superposition of solutions for a different number of inhomogeneities. We find that training on a diverse dataset with lots of variation in the finite element meshes is a key ingredient for achieving good generalization results in all cases. With this, we believe that GNNs can be used to learn solution operators that generalize over a range of properties and produce solutions much faster than a generic solver. Our dataset, which we make publicly available, can be used and extended to verify the robustness of these models under varying conditions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1389925221",
                        "name": "Winfried Lotzsch"
                    },
                    {
                        "authorId": "40360982",
                        "name": "Simon Ohler"
                    },
                    {
                        "authorId": "51100085",
                        "name": "J. Otterbach"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Application IV: Physics-Informed Neural Network As a differentiation modality, HoD-Net is also useful for solving neural PDEs problems (Hsieh et al. 2019; Yang, Meng, and Karniadakis 2021) a.",
                "Application IV: Physics-Informed Neural Network As a differentiation modality, HoD-Net is also useful for solving neural PDEs problems (Hsieh et al. 2019; Yang, Meng, and Karniadakis 2021) a.k.a. physics-informed neural networks (PINNs)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e620d02d658a7c9250cfe09cbcf28f27cc9a6362",
                "externalIds": {
                    "DBLP": "conf/aaai/ShenS0JL022",
                    "DOI": "10.1609/aaai.v36i8.20799",
                    "CorpusId": 250297235
                },
                "corpusId": 250297235,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/e620d02d658a7c9250cfe09cbcf28f27cc9a6362",
                "title": "HoD-Net: High-Order Differentiable Deep Neural Networks and Applications",
                "abstract": "We introduce a deep architecture named HoD-Net to enable high-order differentiability for deep learning. HoD-Net is based on and generalizes the complex-step finite difference (CSFD) method. While similar to classic finite difference, CSFD approaches the derivative of a function from a higher-dimension complex domain, leading to highly accurate and robust differentiation computation without numerical stability issues. This method can be coupled with backpropagation and adjoint perturbation methods for an efficient calculation of high-order derivatives. We show how this numerical scheme can be leveraged in challenging deep learning problems, such as high-order network training, deep learning-based physics simulation, and neural differential equations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1712248670",
                        "name": "Siyuan Shen"
                    },
                    {
                        "authorId": "34620893",
                        "name": "Tianjia Shao"
                    },
                    {
                        "authorId": "2075359702",
                        "name": "Kun Zhou"
                    },
                    {
                        "authorId": "2956458",
                        "name": "Chenfanfu Jiang"
                    },
                    {
                        "authorId": "2140495064",
                        "name": "Feng Luo"
                    },
                    {
                        "authorId": "2118772760",
                        "name": "Yin Yang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fea901c798c09f1c77419dc6741d92ed007f478a",
                "externalIds": {
                    "DBLP": "conf/isit/WadayamaT22",
                    "DOI": "10.1109/ISIT50566.2022.9834461",
                    "CorpusId": 251324724
                },
                "corpusId": 251324724,
                "publicationVenue": {
                    "id": "234ccdc0-f58f-4f94-b86a-428d11a0c5ad",
                    "name": "International Symposium on Information Theory",
                    "type": "conference",
                    "alternate_names": [
                        "International Symposium on Information Technology",
                        "Int Symp Inf Theory",
                        "Int Symp Inf Technol",
                        "ISIT"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1719"
                },
                "url": "https://www.semanticscholar.org/paper/fea901c798c09f1c77419dc6741d92ed007f478a",
                "title": "Asymptotic Mean Squared Error of Noisy Periodical Successive Over-Relaxation",
                "abstract": "Chebyshev-periodical successive over-relaxation was recently proposed as a method of accelerating the convergence speed of fixed-point iterations. If a PSOR iteration is influenced by stochastic disturbances, such as Gaussian noise, then the behavior of the PSOR iteration deviates from the predicted behavior of the noiseless iterations, i.e., the convergence behavior of the Chebyshev-PSOR is highly sensitive to the noises. This paper presents a concise formula for the asymptotic mean squared error (AMSE) of the noisy PSOR iterations. A PSOR iteration can be regarded as a stochastic difference equation and spectral decomposition plays a key role to reveal the asymptotic behaviors of the error covariance. Based on the AMSE formula, a noise mitigation method is developed to reduce the effects of the stochastic disturbance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1984372",
                        "name": "T. Wadayama"
                    },
                    {
                        "authorId": "40401165",
                        "name": "Satoshi Takabe"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "\u2026or enhancing conventional ones in many cases [Guo et al., 2016, Zhu and Zabaras, 2018, Sirignano and Spiliopoulos, 2018, Han et al., 2018, Hsieh et al., 2019, Raissi et al., 2019, Bhatnagar et al., 2019, Bar-Sinai et al., 2019, Berner et al., 2020, Li et al., 2020a,c,b, Um et al., 2020,\u2026"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8136094f09445dd70361db3436be701b6a8405a2",
                "externalIds": {
                    "ArXiv": "2206.09418",
                    "DBLP": "journals/corr/abs-2206-09418",
                    "DOI": "10.48550/arXiv.2206.09418",
                    "CorpusId": 249889222
                },
                "corpusId": 249889222,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8136094f09445dd70361db3436be701b6a8405a2",
                "title": "LordNet: Learning to Solve Parametric Partial Differential Equations without Simulated Data",
                "abstract": "Neural operators, as a powerful approximation to the non-linear operators between in\ufb01nite-dimensional function spaces, have proved to be promising in accelerating the solution of partial differential equations (PDE). However, it requires a large amount of simulated data which can be costly to collect, resulting in a chicken-egg dilemma and limiting its usage in solving PDEs. To jump out of the dilemma, we propose a general data-free paradigm where the neural network directly learns physics from the mean squared residual (MSR) loss constructed by the discretized PDE. We investigate the physical information in the MSR loss and identify the challenge that the neural network must have the capacity to model the long range entanglements in the spatial domain of the PDE, whose patterns vary in different PDEs. Therefore, we propose the low-rank decomposition network (LordNet) which is tunable and also ef\ufb01cient to model various entanglements. Speci\ufb01cally, LordNet learns a low-rank approximation to the global entanglements with simple fully connected layers, which extracts the dominant pattern with reduced computational cost. The experiments on solving Poisson\u2019s equation and Navier-Stokes equation demonstrate that the physical constraints by the MSR loss can lead to better accuracy and generalization ability of the neural network. In addition, LordNet outperforms other modern neural network architectures in both PDEs with the fewest parameters and the fastest inference speed. For Navier-Stokes equation, the learned operator is over 50 times faster than the \ufb01nite difference solution with the same computational resources.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "19143277",
                        "name": "Wenlei Shi"
                    },
                    {
                        "authorId": "2420875",
                        "name": "Xinquan Huang"
                    },
                    {
                        "authorId": "2149397075",
                        "name": "Xiaotian Gao"
                    },
                    {
                        "authorId": "2110849142",
                        "name": "Xinran Wei"
                    },
                    {
                        "authorId": "2108830650",
                        "name": "Jia Zhang"
                    },
                    {
                        "authorId": "143901037",
                        "name": "J. Bian"
                    },
                    {
                        "authorId": "2171945065",
                        "name": "Mao Yang"
                    },
                    {
                        "authorId": "2110264835",
                        "name": "Tie-Yan Liu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ed173dc7d925822913b413e5844bda7638914f81",
                "externalIds": {
                    "DBLP": "conf/icml/ArisakaL23",
                    "ArXiv": "2206.08594",
                    "CorpusId": 256389936
                },
                "corpusId": 256389936,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ed173dc7d925822913b413e5844bda7638914f81",
                "title": "Principled Acceleration of Iterative Numerical Methods Using Machine Learning",
                "abstract": "Iterative methods are ubiquitous in large-scale scientific computing applications, and a number of approaches based on meta-learning have been recently proposed to accelerate them. However, a systematic study of these approaches and how they differ from meta-learning is lacking. In this paper, we propose a framework to analyze such learning-based acceleration approaches, where one can immediately identify a departure from classical meta-learning. We show that this departure may lead to arbitrary deterioration of model performance. Based on our analysis, we introduce a novel training method for learning-based acceleration of iterative methods. Furthermore, we theoretically prove that the proposed method improves upon the existing methods, and demonstrate its significant advantage and versatility through various numerical applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "92120993",
                        "name": "S. Arisaka"
                    },
                    {
                        "authorId": "1861517",
                        "name": "Qianxiao Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "A common and straightforward approach is to follow an Encoder-Process-Decoder (EPD) scheme to map the input solution at time t to the solution at next time step [7, 27, 57, 60, 64, 70]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "179070d3d43e97d1ce4d12127a3dc63581328809",
                "externalIds": {
                    "ArXiv": "2205.13671",
                    "DBLP": "journals/corr/abs-2205-13671",
                    "DOI": "10.48550/arXiv.2205.13671",
                    "CorpusId": 249152256
                },
                "corpusId": 249152256,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/179070d3d43e97d1ce4d12127a3dc63581328809",
                "title": "Transformer for Partial Differential Equations' Operator Learning",
                "abstract": "Data-driven learning of partial differential equations' solution operators has recently emerged as a promising paradigm for approximating the underlying solutions. The solution operators are usually parameterized by deep learning models that are built upon problem-specific inductive biases. An example is a convolutional or a graph neural network that exploits the local grid structure where functions' values are sampled. The attention mechanism, on the other hand, provides a flexible way to implicitly exploit the patterns within inputs, and furthermore, relationship between arbitrary query locations and inputs. In this work, we present an attention-based framework for data-driven operator learning, which we term Operator Transformer (OFormer). Our framework is built upon self-attention, cross-attention, and a set of point-wise multilayer perceptrons (MLPs), and thus it makes few assumptions on the sampling pattern of the input function or query locations. We show that the proposed framework is competitive on standard benchmark problems and can flexibly be adapted to randomly sampled input.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1752871628",
                        "name": "Zijie Li"
                    },
                    {
                        "authorId": "1999900316",
                        "name": "Kazem Meidani"
                    },
                    {
                        "authorId": "3614493",
                        "name": "A. Farimani"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ed9219bf3f27fecadec268844ef870cdc95e1ebd",
                "externalIds": {
                    "DBLP": "conf/icml/KanedaACK0T23",
                    "ArXiv": "2205.10763",
                    "CorpusId": 252683427
                },
                "corpusId": 252683427,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ed9219bf3f27fecadec268844ef870cdc95e1ebd",
                "title": "A Deep Conjugate Direction Method for Iteratively Solving Linear Systems",
                "abstract": "We present a novel deep learning approach to approximate the solution of large, sparse, symmetric, positive-definite linear systems of equations. These systems arise from many problems in applied science, e.g., in numerical methods for partial differential equations. Algorithms for approximating the solution to these systems are often the bottleneck in problems that require their solution, particularly for modern applications that require many millions of unknowns. Indeed, numerical linear algebra techniques have been investigated for many decades to alleviate this computational burden. Recently, data-driven techniques have also shown promise for these problems. Motivated by the conjugate gradients algorithm that iteratively selects search directions for minimizing the matrix norm of the approximation error, we design an approach that utilizes a deep neural network to accelerate convergence via data-driven improvement of the search directions. Our method leverages a carefully chosen convolutional network to approximate the action of the inverse of the linear operator up to an arbitrary constant. We train the network using unsupervised learning with a loss function equal to the $L^2$ difference between an input and the system matrix times the network evaluation, where the unspecified constant in the approximate inverse is accounted for. We demonstrate the efficacy of our approach on spatially discretized Poisson equations with millions of degrees of freedom arising in computational fluid dynamics applications. Unlike state-of-the-art learning approaches, our algorithm is capable of reducing the linear system residual to a given tolerance in a small number of iterations, independent of the problem size. Moreover, our method generalizes effectively to various systems beyond those encountered during training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2071409568",
                        "name": "Ayano Kaneda"
                    },
                    {
                        "authorId": "103541502",
                        "name": "Osman Akar"
                    },
                    {
                        "authorId": "2119701513",
                        "name": "Jingyu Chen"
                    },
                    {
                        "authorId": "2090396997",
                        "name": "Victoria Kala"
                    },
                    {
                        "authorId": "144618613",
                        "name": "David Hyde"
                    },
                    {
                        "authorId": "143667106",
                        "name": "J. Teran"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                ", 2019), or aimed at correcting iterative solvers (Hsieh et al., 2019).",
                "Other research focused on efficient simulations by learning conservation laws (Cranmer et al., 2020; Greydanus et al., 2019), or aimed at correcting iterative solvers (Hsieh et al., 2019)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b5e32b5f89a8116b8a511c7cb840206628bb6620",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-01222",
                    "ArXiv": "2205.01222",
                    "DOI": "10.48550/arXiv.2205.01222",
                    "CorpusId": 248505945
                },
                "corpusId": 248505945,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b5e32b5f89a8116b8a511c7cb840206628bb6620",
                "title": "Leveraging Stochastic Predictions of Bayesian Neural Networks for Fluid Simulations",
                "abstract": "We investigate uncertainty estimation and multimodality via the non-deterministic predictions of Bayesian neural networks (BNNs) in fluid simulations. To this end, we deploy BNNs in three challenging experimental test-cases of increasing complexity: We show that BNNs, when used as surrogate models for steady-state fluid flow predictions, provide accurate physical predictions together with sensible estimates of uncertainty. Further, we experiment with perturbed temporal sequences from Navier-Stokes simulations and evaluate the capabilities of BNNs to capture multimodal evolutions. While our findings indicate that this is problematic for large perturbations, our results show that the networks learn to correctly predict high uncertainties in such situations. Finally, we study BNNs in the context of solver interactions with turbulent plasma flows. We find that BNN-based corrector networks can stabilize coarse-grained simulations and successfully create multimodal trajectories.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "80580587",
                        "name": "Maximilian Mueller"
                    },
                    {
                        "authorId": "46424015",
                        "name": "R. Greif"
                    },
                    {
                        "authorId": "2758452",
                        "name": "F. Jenko"
                    },
                    {
                        "authorId": "2125721010",
                        "name": "Nils Thuerey"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1ba270b6d5f260fae8126e8471aa607b0733672a",
                "externalIds": {
                    "ArXiv": "2203.00614",
                    "DBLP": "journals/corr/abs-2203-00614",
                    "DOI": "10.1007/s40687-023-00378-y",
                    "CorpusId": 247188008
                },
                "corpusId": 247188008,
                "publicationVenue": {
                    "id": "d12a0066-833f-4e7a-a733-7afc7d179b43",
                    "name": "Research in the Mathematical Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Res Math Sci"
                    ],
                    "issn": "2197-9847",
                    "url": "http://www.resmathsci.com/",
                    "alternate_urls": [
                        "https://link.springer.com/article/10.1186/s40687-016-0091-8"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1ba270b6d5f260fae8126e8471aa607b0733672a",
                "title": "Side effects of learning from low-dimensional data embedded in a Euclidean space",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "27794145",
                        "name": "Juncai He"
                    },
                    {
                        "authorId": "39285779",
                        "name": "R. Tsai"
                    },
                    {
                        "authorId": "32564459",
                        "name": "Rachel A. Ward"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ebd042387a2dde05ab18b23820030841bb671966",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-07643",
                    "ArXiv": "2202.07643",
                    "CorpusId": 246863584
                },
                "corpusId": 246863584,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ebd042387a2dde05ab18b23820030841bb671966",
                "title": "Lie Point Symmetry Data Augmentation for Neural PDE Solvers",
                "abstract": "Neural networks are increasingly being used to solve partial differential equations (PDEs), replacing slower numerical solvers. However, a critical issue is that neural PDE solvers require high-quality ground truth data, which usually must come from the very solvers they are designed to replace. Thus, we are presented with a proverbial chicken-and-egg problem. In this paper, we present a method, which can partially alleviate this problem, by improving neural PDE solver sample complexity -- Lie point symmetry data augmentation (LPSDA). In the context of PDEs, it turns out that we are able to quantitatively derive an exhaustive list of data transformations, based on the Lie point symmetry group of the PDEs in question, something not possible in other application areas. We present this framework and demonstrate how it can easily be deployed to improve neural PDE solver sample complexity by an order of magnitude.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "78843496",
                        "name": "Johannes Brandstetter"
                    },
                    {
                        "authorId": "1678311",
                        "name": "M. Welling"
                    },
                    {
                        "authorId": "3471551",
                        "name": "Daniel E. Worrall"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Three important works in this area are Bar-Sinai et al. (2019), Greenfeld et al. (2019), and Hsieh et al. (2019).",
                "Hsieh et al. (2019) even have convergence guarantees for their method, something rare in deep learning."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "be8d39424a9010bfc0805385cc91edee383c2e24",
                "externalIds": {
                    "ArXiv": "2202.03376",
                    "DBLP": "conf/iclr/BrandstetterWW22",
                    "CorpusId": 246634432
                },
                "corpusId": 246634432,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/be8d39424a9010bfc0805385cc91edee383c2e24",
                "title": "Message Passing Neural PDE Solvers",
                "abstract": "The numerical solution of partial differential equations (PDEs) is difficult, having led to a century of research so far. Recently, there have been pushes to build neural--numerical hybrid solvers, which piggy-backs the modern trend towards fully end-to-end learned systems. Most works so far can only generalize over a subset of properties to which a generic solver would be faced, including: resolution, topology, geometry, boundary conditions, domain discretization regularity, dimensionality, etc. In this work, we build a solver, satisfying these properties, where all the components are based on neural message passing, replacing all heuristically designed components in the computation graph with backprop-optimized neural function approximators. We show that neural message passing solvers representationally contain some classical methods, such as finite differences, finite volumes, and WENO schemes. In order to encourage stability in training autoregressive models, we put forward a method that is based on the principle of zero-stability, posing stability as a domain adaptation problem. We validate our method on various fluid-like flow problems, demonstrating fast, stable, and accurate performance across different domain topologies, equation parameters, discretizations, etc., in 1D and 2D.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "78843496",
                        "name": "Johannes Brandstetter"
                    },
                    {
                        "authorId": "3471551",
                        "name": "Daniel E. Worrall"
                    },
                    {
                        "authorId": "1678311",
                        "name": "M. Welling"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "The literature also includes studies focused on applying deep learning techniques in multigrid and numerical PDEs [25, 20].",
                "Using, for example, the special activation function \u03c3(x) = ReLU(x) := max{0, x} \u2265 0, the above iterative process can be naturally modified to preserve the constraint (1.2):\nui = ui\u22121 + \u03c3 \u25e6Bi \u2217 \u03c3(f \u2212 A \u2217 ui\u22121), i = 1 : \u03bd. (1.4)\nThis forms the basic block of MgNet, precisely as in [15]. partial differential equations (PDEs) [52, 53], we introduce this residual\nri = f \u2212 A \u2217 ui.",
                "The literature also includes studies focused on applying deep learning techniques in multigrid and numerical PDEs [19, 16]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "4463b561b49d81a7920176f9c0dc80c564b96603",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-07441",
                    "ArXiv": "2112.07441",
                    "DOI": "10.1016/j.neunet.2023.03.011",
                    "CorpusId": 245131527,
                    "PubMed": "36947909"
                },
                "corpusId": 245131527,
                "publicationVenue": {
                    "id": "a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                    "name": "Neural Networks",
                    "type": "journal",
                    "alternate_names": [
                        "Neural Netw"
                    ],
                    "issn": "0893-6080",
                    "url": "http://www.elsevier.com/locate/neunet",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/841/description",
                        "http://www.sciencedirect.com/science/journal/08936080"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4463b561b49d81a7920176f9c0dc80c564b96603",
                "title": "An Interpretive Constrained Linear Model for ResNet and MgNet",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "27794145",
                        "name": "Juncai He"
                    },
                    {
                        "authorId": "40011783",
                        "name": "Jinchao Xu"
                    },
                    {
                        "authorId": "2146644453",
                        "name": "Lian Zhang"
                    },
                    {
                        "authorId": "1739258",
                        "name": "Jianqing Zhu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Although much recent effort has been dedicated to investigating the PINN framework for solving PDEs, other neural PDE solution techniques exist, notably techniques which learn iterators that are not solutions to PDEs themselves but rather provide a method of quickly computing such solutions [21]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8281117ffbcbb468b30af5c2995bc98cee07252c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-09930",
                    "ArXiv": "2111.09930",
                    "CorpusId": 244462911
                },
                "corpusId": 244462911,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8281117ffbcbb468b30af5c2995bc98cee07252c",
                "title": "Learning To Estimate Regions Of Attraction Of Autonomous Dynamical Systems Using Physics-Informed Neural Networks",
                "abstract": "When learning to perform motor tasks in a simulated environment, neural networks must be allowed to explore their action space to discover new potentially viable solutions. However, in an online learning scenario with physical hardware, this exploration must be constrained by relevant safety considerations in order to avoid damage to the agent's hardware and environment. We aim to address this problem by training a neural network, which we will refer to as a\"safety network\", to estimate the region of attraction (ROA) of a controlled autonomous dynamical system. This safety network can thereby be used to quantify the relative safety of proposed control actions and prevent the selection of damaging actions. Here we present our development of the safety network by training an artificial neural network (ANN) to represent the ROA of several autonomous dynamical system benchmark problems. The training of this network is predicated upon both Lyapunov theory and neural solutions to partial differential equations (PDEs). By learning to approximate the viscosity solution to a specially chosen PDE that contains the dynamics of the system of interest, the safety network learns to approximate a particular function, similar to a Lyapunov function, whose zero level set is boundary of the ROA. We train our safety network to solve these PDEs in a semi-supervised manner following a modified version of the Physics Informed Neural Network (PINN) approach, utilizing a loss function that penalizes disagreement with the PDE's initial and boundary conditions, as well as non-zero residual and variational terms. In future work we intend to apply this technique to reinforcement learning agents during motor learning tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "150064351",
                        "name": "Cody Scharzenberger"
                    },
                    {
                        "authorId": "37915616",
                        "name": "Joe Hays"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The first category is using deep neural network to improve classical numerical methods, see for example [45,47,24,20]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3376fb4428eca39044257b60a310caff00f0c015",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-02009",
                    "ArXiv": "2111.02009",
                    "CorpusId": 241033504
                },
                "corpusId": 241033504,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3376fb4428eca39044257b60a310caff00f0c015",
                "title": "Analysis of Deep Ritz Methods for Laplace Equations with Dirichlet Boundary Conditions",
                "abstract": "Deep Ritz methods (DRM) have been proven numerically to be efficient in solving partial differential equations. In this paper, we present a convergence rate in $H^{1}$ norm for deep Ritz methods for Laplace equations with Dirichlet boundary condition, where the error depends on the depth and width in the deep neural networks and the number of samples explicitly. Further we can properly choose the depth and width in the deep neural networks in terms of the number of training samples. The main idea of the proof is to decompose the total error of DRM into three parts, that is approximation error, statistical error and the error caused by the boundary penalty. We bound the approximation error in $H^{1}$ norm with $\\mathrm{ReLU}^{2}$ networks and control the statistical error via Rademacher complexity. In particular, we derive the bound on the Rademacher complexity of the non-Lipschitz composition of gradient norm with $\\mathrm{ReLU}^{2}$ network, which is of immense independent interest. We also analysis the error inducing by the boundary penalty method and give a prior rule for tuning the penalty parameter.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2070090210",
                        "name": "Chenguang Duan"
                    },
                    {
                        "authorId": "48913600",
                        "name": "Yuling Jiao"
                    },
                    {
                        "authorId": "2051930660",
                        "name": "Yanming Lai"
                    },
                    {
                        "authorId": "2125744996",
                        "name": "Xiliang Lu"
                    },
                    {
                        "authorId": "46616206",
                        "name": "Qimeng Quan"
                    },
                    {
                        "authorId": "2109753875",
                        "name": "J. Yang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Because of its multi-scale architecture, the U-Net is widely used in many problems in scientific computing, such as Navier\u2013Stokes simulations [42], and PDE-solvers [15, 44]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f08c7d6e7ad59fa8f0f5064dfeef4fae5a6b34fa",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-03682",
                    "ArXiv": "2110.03682",
                    "DOI": "10.1007/s40304-022-00331-5",
                    "CorpusId": 238419633
                },
                "corpusId": 238419633,
                "publicationVenue": {
                    "id": "f2ccdde5-01ea-4801-8a2d-35d9889dac47",
                    "name": "Communications in Mathematics and Statistics",
                    "alternate_names": [
                        "Commun Math Stat"
                    ],
                    "issn": "2194-671X",
                    "url": "https://link.springer.com/journal/40304"
                },
                "url": "https://www.semanticscholar.org/paper/f08c7d6e7ad59fa8f0f5064dfeef4fae5a6b34fa",
                "title": "Learning Invariance Preserving Moment Closure Model for Boltzmann\u2013BGK Equation",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2155011306",
                        "name": "Zhengyi Li"
                    },
                    {
                        "authorId": "145496882",
                        "name": "Bin Dong"
                    },
                    {
                        "authorId": "71698964",
                        "name": "Yanli Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6f24bb772b7432308eaaf782d9bf5376dae1c304",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-13901",
                    "ArXiv": "2109.13901",
                    "CorpusId": 238198283
                },
                "corpusId": 238198283,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6f24bb772b7432308eaaf782d9bf5376dae1c304",
                "title": "Physics-Augmented Learning: A New Paradigm Beyond Physics-Informed Learning",
                "abstract": "Integrating physical inductive biases into machine learning can improve model generalizability. We generalize the successful paradigm of physics-informed learning (PIL) into a more general framework that also includes what we term physics-augmented learning (PAL). PIL and PAL complement each other by handling discriminative and generative properties, respectively. In numerical experiments, we show that PAL performs well on examples where PIL is inapplicable or inefficient.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2145253202",
                        "name": "Ziming Liu"
                    },
                    {
                        "authorId": "2144864899",
                        "name": "Yunyue Chen"
                    },
                    {
                        "authorId": "93584228",
                        "name": "Yuanqi Du"
                    },
                    {
                        "authorId": "2011933",
                        "name": "Max Tegmark"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "[82], where an iterative solver is coupled to a deep linear network to guarantee the accuracy level of the network prediction."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "517f8f11ee266edfceac2d131738881b20a356d5",
                "externalIds": {
                    "ArXiv": "2109.09363",
                    "DBLP": "journals/corr/abs-2109-09363",
                    "DOI": "10.1017/dce.2022.2",
                    "CorpusId": 237572324
                },
                "corpusId": 237572324,
                "publicationVenue": {
                    "id": "7af5f4f9-b25e-4f92-a1b0-f3114e296482",
                    "name": "Data-Centric Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "Data-centric Eng"
                    ],
                    "issn": "2632-6736",
                    "url": "https://www.cambridge.org/core/journals/data-centric-engineering"
                },
                "url": "https://www.semanticscholar.org/paper/517f8f11ee266edfceac2d131738881b20a356d5",
                "title": "Performance and accuracy assessments of an incompressible fluid solver coupled with a deep convolutional neural network",
                "abstract": "Abstract The resolution of the Poisson equation is usually one of the most computationally intensive steps for incompressible fluid solvers. Lately, DeepLearning, and especially convolutional neural networks (CNNs), has been introduced to solve this equation, leading to significant inference time reduction at the cost of a lack of guarantee on the accuracy of the solution.This drawback might lead to inaccuracies, potentially unstable simulations and prevent performing fair assessments of the CNN speedup for different network architectures. To circumvent this issue, a hybrid strategy is developed, which couples a CNN with a traditional iterative solver to ensure a user-defined accuracy level. The CNN hybrid method is tested on two flow cases: (a) the flow around a 2D cylinder and (b) the variable-density plumes with and without obstacles (both 2D and 3D), demonstrating remarkable generalization capabilities, ensuring both the accuracy and stability of the simulations. The error distribution of the predictions using several network architectures is further investigated in the plume test case. The introduced hybrid strategy allows a systematic evaluation of the CNN performance at the same accuracy level for various network architectures. In particular, the importance of incorporating multiple scales in the network architecture is demonstrated, since improving both the accuracy and the inference performance compared with feedforward CNN architectures. Thus, in addition to the pure networks\u2019 performance evaluation, this study has also led to numerous guidelines and results on how to build neural networks and computational strategies to predict unsteady flows with both accuracy and stability requirements.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2138996792",
                        "name": "Ekhi Ajuria Illarramendi"
                    },
                    {
                        "authorId": "92033612",
                        "name": "M. Bauerheim"
                    },
                    {
                        "authorId": "9448078",
                        "name": "B. Cuenot"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "54aa0389c9d0a58afeebfa867c07151242d2576c",
                "externalIds": {
                    "ArXiv": "2109.01467",
                    "DBLP": "journals/corr/abs-2109-01467",
                    "CorpusId": 237417331
                },
                "corpusId": 237417331,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/54aa0389c9d0a58afeebfa867c07151242d2576c",
                "title": "Semi-Implicit Neural Solver for Time-dependent Partial Differential Equations",
                "abstract": "Fast and accurate solutions of time-dependent partial differential equations (PDEs) are of pivotal interest to many research fields, including physics, engineering, and biology. Generally, implicit/semi-implicit schemes are preferred over explicit ones to improve stability and correctness. However, existing semi-implicit methods are usually iterative and employ a general-purpose solver, which may be sub-optimal for a specific class of PDEs. In this paper, we propose a neural solver to learn an optimal iterative scheme in a data-driven fashion for any class of PDEs. Specifically, we modify a single iteration of a semi-implicit solver using a deep neural network. We provide theoretical guarantees for the correctness and convergence of neural solvers analogous to conventional iterative solvers. In addition to the commonly used Dirichlet boundary condition, we adopt a diffuse domain approach to incorporate a diverse type of boundary conditions, e.g., Neumann. We show that the proposed neural solver can go beyond linear PDEs and applies to a class of non-linear PDEs, where the non-linear component is non-stiff. We demonstrate the efficacy of our method on 2D and 3D scenarios. To this end, we show how our model generalizes to parameter settings, which are different from training; and achieves faster convergence than semi-implicit schemes.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1561461499",
                        "name": "Suprosanna Shit"
                    },
                    {
                        "authorId": "2065232462",
                        "name": "I. Ezhov"
                    },
                    {
                        "authorId": "2084622476",
                        "name": "Leon M\u00e4chler"
                    },
                    {
                        "authorId": "100819008",
                        "name": "R. Abinav"
                    },
                    {
                        "authorId": "1959705",
                        "name": "Jana Lipkov\u00e1"
                    },
                    {
                        "authorId": "1561434672",
                        "name": "J. Paetzold"
                    },
                    {
                        "authorId": "94576427",
                        "name": "F. Kofler"
                    },
                    {
                        "authorId": "34929811",
                        "name": "M. Piraud"
                    },
                    {
                        "authorId": "143893221",
                        "name": "Bjoern H Menze"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "[19] aims to improve an existing solver through the supplemental application of a neural network.",
                "A different direction within the optimization of multigrid methods, which has recently become popular, is applying machine learning to improve the individual solver components, such as [15, 19, 20, 22]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a0c4424f851760ead4d6957b54ce0bbd52ddf66b",
                "externalIds": {
                    "DBLP": "journals/gpem/SchmittKK21",
                    "MAG": "3198087512",
                    "DOI": "10.1007/s10710-021-09412-w",
                    "CorpusId": 239698813
                },
                "corpusId": 239698813,
                "publicationVenue": {
                    "id": "6f4efebe-b0de-4221-9037-fe8d06c85a99",
                    "name": "Genetic Programming and Evolvable Machines",
                    "type": "journal",
                    "alternate_names": [
                        "Genet Program Evolvable Mach"
                    ],
                    "issn": "1389-2576",
                    "url": "https://link.springer.com/journal/10710"
                },
                "url": "https://www.semanticscholar.org/paper/a0c4424f851760ead4d6957b54ce0bbd52ddf66b",
                "title": "EvoStencils: a grammar-based genetic programming approach for constructing efficient geometric multigrid methods",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "29915026",
                        "name": "J. Schmitt"
                    },
                    {
                        "authorId": "2315743",
                        "name": "S. Kuckuk"
                    },
                    {
                        "authorId": "35161054",
                        "name": "H. K\u00f6stler"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The first category is using deep neural network to improve classical numerical methods, see for example [40, 42, 21, 15]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fa39df4d2a04de8f138237de9c412e0679c8081c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-14478",
                    "ArXiv": "2107.14478",
                    "DOI": "10.1142/s021953052350015x",
                    "CorpusId": 236635446
                },
                "corpusId": 236635446,
                "publicationVenue": {
                    "id": "e76006a8-327b-4ebf-8d8e-3d335aab377d",
                    "name": "Analysis and Applications",
                    "type": "journal",
                    "alternate_names": [
                        "Anal Appl"
                    ],
                    "issn": "0219-5305",
                    "url": "https://www.worldscientific.com/worldscinet/aa",
                    "alternate_urls": [
                        "http://www.worldscinet.com/aa/aa.shtml"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fa39df4d2a04de8f138237de9c412e0679c8081c",
                "title": "Error Analysis of Deep Ritz Methods for Elliptic Equations",
                "abstract": "Using deep neural networks to solve PDEs has attracted a lot of attentions recently. However, why the deep learning method works is falling far behind its empirical success. In this paper, we provide a rigorous numerical analysis on deep Ritz method (DRM) \\cite{Weinan2017The} for second order elliptic equations with Drichilet, Neumann and Robin boundary condition, respectively. We establish the first nonasymptotic convergence rate in $H^1$ norm for DRM using deep networks with smooth activation functions including logistic and hyperbolic tangent functions. Our results show how to set the hyper-parameter of depth and width to achieve the desired convergence rate in terms of number of training samples.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48913600",
                        "name": "Yuling Jiao"
                    },
                    {
                        "authorId": "2051930660",
                        "name": "Yanming Lai"
                    },
                    {
                        "authorId": "2154365231",
                        "name": "Yisu Luo"
                    },
                    {
                        "authorId": "2153675654",
                        "name": "Yang Wang"
                    },
                    {
                        "authorId": "2116589060",
                        "name": "Yunfei Yang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4f82688e6fff88ce92f1c927e5118728cd7d4867",
                "externalIds": {
                    "MAG": "3184089171",
                    "DOI": "10.1109/TAP.2021.3098585",
                    "CorpusId": 237703551
                },
                "corpusId": 237703551,
                "publicationVenue": {
                    "id": "64f6bc7f-a080-473b-8e71-044beeeadb13",
                    "name": "IEEE Transactions on Antennas and Propagation",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Antenna Propag"
                    ],
                    "issn": "0018-926X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=8",
                    "alternate_urls": [
                        "https://ieeeaps.org/aps_trans/index.htm",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=8234",
                        "http://ieeexplore.ieee.org/xpl/aboutJournal.jsp?punumber=8",
                        "https://ieeexplore.ieee.org/xpl/aboutJournal.jsp?punumber=8"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4f82688e6fff88ce92f1c927e5118728cd7d4867",
                "title": "A Theory-Guided Deep Neural Network for Time Domain Electromagnetic Simulation and Inversion Using a Differentiable Programming Platform",
                "abstract": "In this communication, a trainable theory-guided recurrent neural network (RNN) equivalent to the finite-difference-time-domain (FDTD) method is exploited to formulate electromagnetic propagation, solve Maxwell\u2019s equations, and the inverse problem on differentiable programming platform Pytorch. For forward modeling, the computation efficiency is substantially improved compared to conventional FDTD implemented on MATLAB. Gradient computation becomes more precise and faster than the traditional finite difference method benefiting from the accurate and efficient automatic differentiation on the differentiable programming platform. Moreover, by setting the trainable weights of RNN as the material-related parameters, an inverse problem can be solved by training the network. Numerical results demonstrate the effectiveness and efficiency of the method for forward and inverse electromagnetic modeling.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "121617134",
                        "name": "Yanyan Hu"
                    },
                    {
                        "authorId": "3283846",
                        "name": "Yuchen Jin"
                    },
                    {
                        "authorId": "50171976",
                        "name": "Xuqing Wu"
                    },
                    {
                        "authorId": "3346124",
                        "name": "Jiefu Chen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "contexts": [
                "This is an attempt to reproduce results from [17].",
                "The other multilevel solver that we tried is a U-Net from [17].",
                "As we pointed in Section VI, we cannot be sure that we reproduce results from [17] because the paper contains omissions.",
                "Given that, the whole scheme from [17] is a generalized Richardson iteration for the preconditioned system."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "73abe93f61e64bb34f66d44dce14dfb634095b90",
                "externalIds": {
                    "DBLP": "conf/ijcnn/Fanaskov21",
                    "DOI": "10.1109/IJCNN52387.2021.9533736",
                    "CorpusId": 237599748
                },
                "corpusId": 237599748,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/73abe93f61e64bb34f66d44dce14dfb634095b90",
                "title": "Neural Multigrid Architectures",
                "abstract": "We propose a convenient matrix-free neural architecture for the multigrid method. The architecture is simple enough to be implemented in less than fifty lines of code, yet it encompasses a large number of distinct multigrid solvers. We argue that a fixed neural network without dense layers can not realize an efficient iterative method. Because of that, standard training protocols do not lead to competitive solvers. To overcome this difficulty, we use parameter sharing and serialization of layers. The resulting network can be trained on linear problems with thousands of unknowns and retains its efficiency on problems with millions of unknowns. From the point of view of numerical linear algebra network's training corresponds to finding optimal smoothers for the geometric multigrid method. We demonstrate our approach on a few second-order elliptic equations. For tested linear systems, we obtain from two to five times smaller spectral radius of the error propagation matrix compare to a basic linear multigrid with Jacobi smoother.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145620455",
                        "name": "V. Fanaskov"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Neural network of [48] is applied to speed up iterative solver for elliptic type PDEs."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "44de190cd49b8a892b3ef96fda7cc42727403455",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-00813",
                    "ArXiv": "2107.00813",
                    "CorpusId": 235727646
                },
                "corpusId": 235727646,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/44de190cd49b8a892b3ef96fda7cc42727403455",
                "title": "Cell-average based neural network method for hyperbolic and parabolic partial differential equations",
                "abstract": "Motivated by finite volume scheme, a cell-average based neural network method is proposed. The method is based on the integral or weak formulation of partial differential equations. A simple feed forward network is forced to learn the solution average evolution between two neighboring time steps. Offline supervised training is carried out to obtain the optimal network parameter set, which uniquely identifies one finite volume like neural network method. Once well trained, the network method is implemented as a finite volume scheme, thus is mesh dependent. Different to traditional numerical methods, our method can be relieved from the explicit scheme CFL restriction and can adapt to any time step size for solution evolution. For Heat equation, first order of convergence is observed and the errors are related to the spatial mesh size but are observed independent of the mesh size in time. The cell-average based neural network method can sharply evolve contact discontinuity with almost zero numerical diffusion introduced. Shock and rarefaction waves are well captured for nonlinear hyperbolic conservation laws.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "104127307",
                        "name": "Changxin Qiu"
                    },
                    {
                        "authorId": "40592398",
                        "name": "Jue Yan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b8057d8cb6a3c4240cff7856ad55db85ac57a771",
                "externalIds": {
                    "DBLP": "journals/sadm/NgomM21",
                    "MAG": "3173974052",
                    "DOI": "10.1002/sam.11531",
                    "CorpusId": 237912092
                },
                "corpusId": 237912092,
                "publicationVenue": {
                    "id": "19c7a16a-8ffe-45d4-863e-7e1cc8a78b4d",
                    "name": "Statistical analysis and data mining",
                    "type": "journal",
                    "alternate_names": [
                        "Stat Anal Data Min",
                        "Stat anal data min",
                        "Statistical Analysis and Data Mining"
                    ],
                    "issn": "1932-1864",
                    "url": "http://www3.interscience.wiley.com/journal/112701062/home",
                    "alternate_urls": [
                        "http://onlinelibrary.wiley.com/journal/10.1002/(ISSN)1932-1872",
                        "https://onlinelibrary.wiley.com/journal/19321872"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b8057d8cb6a3c4240cff7856ad55db85ac57a771",
                "title": "Fourier neural networks as function approximators and differential equation solvers",
                "abstract": "We present a Fourier neural network (FNN) that can be mapped directly to the Fourier decomposition. The choice of activation and loss function yields results that replicate a Fourier series expansion closely while preserving a straightforward architecture with a single hidden layer. The simplicity of this network architecture facilitates the integration with any other higher\u2010complexity networks, at a data pre\u2010 or postprocessing stage. We validate this FNN on naturally periodic smooth functions and on piecewise continuous periodic functions. We showcase the use of this FNN for modeling or solving partial differential equations with periodic boundary conditions. The main advantages of the current approach are the validity of the solution outside the training region, interpretability of the trained model, and simplicity of use.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "103457079",
                        "name": "M. Ngom"
                    },
                    {
                        "authorId": "144719701",
                        "name": "O. Marin"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Hybrid approaches combine an existing iterative method with a feed-forward network to compute the properties of the target system accurately while enhancing the convergence property and the solution quality of the existing method [18, 19, 32]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7a35a6061ac036dad9ae03c3b1a27435630611bf",
                "externalIds": {
                    "ArXiv": "2106.01680",
                    "DBLP": "journals/corr/abs-2106-01680",
                    "CorpusId": 235313383
                },
                "corpusId": 235313383,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7a35a6061ac036dad9ae03c3b1a27435630611bf",
                "title": "Convergent Graph Solvers",
                "abstract": "We propose the convergent graph solver (CGS), a deep learning method that learns iterative mappings to predict the properties of a graph system at its stationary state (fixed point) with guaranteed convergence. CGS systematically computes the fixed points of a target graph system and decodes them to estimate the stationary properties of the system without the prior knowledge of existing solvers or intermediate solutions. The forward propagation of CGS proceeds in three steps: (1) constructing the input dependent linear contracting iterative maps, (2) computing the fixed-points of the linear maps, and (3) decoding the fixed-points to estimate the properties. The contractivity of the constructed linear maps guarantees the existence and uniqueness of the fixed points following the Banach fixed point theorem. To train CGS efficiently, we also derive a tractable analytical expression for its gradient by leveraging the implicit function theorem. We evaluate the performance of CGS by applying it to various network-analytic and graph benchmark problems. The results indicate that CGS has competitive capabilities for predicting the stationary properties of graph systems, irrespective of whether the target systems are linear or non-linear. CGS also shows high performance for graph classification problems where the existence or the meaning of a fixed point is hard to be clearly defined, which highlights the potential of CGS as a general graph neural network architecture.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1491104517",
                        "name": "Junyoung Park"
                    },
                    {
                        "authorId": "143801538",
                        "name": "J. Choo"
                    },
                    {
                        "authorId": "2085587",
                        "name": "Jinkyoo Park"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[24] trained a convolutional network to improve a GMG algorithm for the structured Poisson problem."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "81ed800f24e9dfa2fbaf6b3ee47ff02fd0c9efa6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-01854",
                    "ArXiv": "2106.01854",
                    "CorpusId": 235313621
                },
                "corpusId": 235313621,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/81ed800f24e9dfa2fbaf6b3ee47ff02fd0c9efa6",
                "title": "Optimization-Based Algebraic Multigrid Coarsening Using Reinforcement Learning",
                "abstract": "Large sparse linear systems of equations are ubiquitous in science and engineering, such as those arising from discretizations of partial differential equations. Algebraic multigrid (AMG) methods are one of the most common methods of solving such linear systems, with an extensive body of underlying mathematical theory. A system of linear equations defines a graph on the set of unknowns and each level of a multigrid solver requires the selection of an appropriate coarse graph along with restriction and interpolation operators that map to and from the coarse representation. The efficiency of the multigrid solver depends critically on this selection and many selection methods have been developed over the years. Recently, it has been demonstrated that it is possible to directly learn the AMG interpolation and restriction operators, given a coarse graph selection. In this paper, we consider the complementary problem of learning to coarsen graphs for a multigrid solver, a necessary step in developing fully learnable AMG methods. We propose a method using a reinforcement learning (RL) agent based on graph neural networks (GNNs), which can learn to perform graph coarsening on small planar training graphs and then be applied to unstructured large planar graphs, assuming bounded node degree. We demonstrate that this method can produce better coarse graphs than existing algorithms, even as the graph size increases and other properties of the graph are varied. We also propose an efficient inference procedure for performing graph coarsening that results in linear time complexity in graph size.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "113435906",
                        "name": "Ali Taghibakhshi"
                    },
                    {
                        "authorId": "2279019",
                        "name": "S. MacLachlan"
                    },
                    {
                        "authorId": "2053925742",
                        "name": "L. Olson"
                    },
                    {
                        "authorId": "2112757181",
                        "name": "M. West"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[34] used the NN for learning PDE solvers with convergence guarantees."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2e27641dab08f12be674fddcc1b0e85481f3f6ad",
                "externalIds": {
                    "DBLP": "journals/tcasI/LiYLW21",
                    "MAG": "3137847746",
                    "DOI": "10.1109/TCSI.2021.3065561",
                    "CorpusId": 234275087
                },
                "corpusId": 234275087,
                "publicationVenue": {
                    "id": "65967e36-f7db-476f-9d00-fd080a5a8483",
                    "name": "IEEE Transactions on Circuits and Systems Part 1: Regular Papers",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Circuit Syst Part 1 Regul Pap",
                        "IEEE Trans Circuit Syst I-regular Pap",
                        "IEEE Transactions on Circuits and Systems I-regular Papers"
                    ],
                    "issn": "1549-8328",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=8919",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8919"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2e27641dab08f12be674fddcc1b0e85481f3f6ad",
                "title": "Robust PCL Discovery of Data-Driven Mean-Field Game Systems and Control Problems",
                "abstract": "Under the background of the wanton spread of the coronavirus disease (COVID-19), the pandemic is changing and hitting lives all over the world. Fortunately, Spatio-temporal processes bear essential importance in many applied scientific fields. And the disease pandemic can be viewed as Spatio-temporal dynamics processes. Generally, partial differential equations (PDEs) have been widely used to investigate interfacial dynamic processes. In this work, we use a physical-constraint neural network learning the Spatio-temporal mean-field dynamics to control the propagation of epidemics. Also, we use the AI-based algorithm physical-constraint learning (PCL) to solve the minimization problems of the mean-field game (MFG) and control (MFC) problems instead of the traditional computational method. In PCL, the PDEs are encoded into the loss function, where partial derivatives can be obtained through automatic differentiation (AD). We demonstrate how they can be applied in practice by considering the problem of controlling the propagation of epidemics. Numerical experiments on different input data are implemented to demonstrate the effectiveness and superiority of the proposed models compared to the state-of-the-art approach and illustrate how to separate the infected patients in a spatial domain effectively.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109639708",
                        "name": "Chun Li"
                    },
                    {
                        "authorId": "1758436",
                        "name": "Yunyun Yang"
                    },
                    {
                        "authorId": "2108951501",
                        "name": "Hui Liang"
                    },
                    {
                        "authorId": "1758916",
                        "name": "Boying Wu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Another example from scientific computing is [26], which deals with solving partial differential equations."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bbed0e1d9df24aa1b059e606e68de51ec33f566d",
                "externalIds": {
                    "ArXiv": "2105.05210",
                    "CorpusId": 234357844
                },
                "corpusId": 234357844,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bbed0e1d9df24aa1b059e606e68de51ec33f566d",
                "title": "Accelerated Forward-Backward Optimization using Deep Learning",
                "abstract": "We propose several deep-learning accelerated optimization solvers with convergence guarantees. We use ideas from the analysis of accelerated forward-backward schemes like FISTA, but instead of the classical approach of proving convergence for a choice of parameters, such as a step-size, we show convergence whenever the update is chosen in a specific set. Rather than picking a point in this set using some predefined method, we train a deep neural network to pick the best update. Finally, we show that the method is applicable to several cases of smooth and non-smooth optimization and show superior results to established accelerated solvers.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2841510",
                        "name": "S. Banert"
                    },
                    {
                        "authorId": "2090610132",
                        "name": "Jevgenija Rudzusika"
                    },
                    {
                        "authorId": "47982165",
                        "name": "Ozan Oktem"
                    },
                    {
                        "authorId": "40523747",
                        "name": "J. Adler"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ebc625742665d9c0317ccb08ac124c7ddfca2f11",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-03336",
                    "ArXiv": "2105.03336",
                    "DOI": "10.1007/s00498-022-00333-2",
                    "CorpusId": 234097030
                },
                "corpusId": 234097030,
                "publicationVenue": {
                    "id": "1cf80dba-739d-4ddf-9354-fd28e0171f84",
                    "name": "MCSS. Mathematics of Control, Signals and Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Math Control Signal Syst",
                        "MC Math Control Signal Syst",
                        "Mathematics of Control, Signals, and Systems"
                    ],
                    "issn": "0932-4194",
                    "url": "https://www.springer.com/journal/498",
                    "alternate_urls": [
                        "http://www.springer.com/journal/498",
                        "https://link.springer.com/journal/498"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ebc625742665d9c0317ccb08ac124c7ddfca2f11",
                "title": "Neural network architectures using min-plus algebra for solving certain high-dimensional optimal control problems and Hamilton\u2013Jacobi PDEs",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2221048",
                        "name": "J. Darbon"
                    },
                    {
                        "authorId": "2211712",
                        "name": "P. Dower"
                    },
                    {
                        "authorId": "148220583",
                        "name": "Tingwei Meng"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "37943e58d84306d8a898acee96270f123c946c1a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-04988",
                    "ArXiv": "2103.04988",
                    "DOI": "10.1016/j.rinam.2021.100201",
                    "CorpusId": 232168446
                },
                "corpusId": 232168446,
                "publicationVenue": {
                    "id": "9fd4ce69-f76a-47b9-b813-fcd8e5bf46dc",
                    "name": "Results in Applied Mathematics",
                    "type": "journal",
                    "alternate_names": [
                        "Result Appl Math"
                    ],
                    "issn": "2590-0374",
                    "url": "https://www.journals.elsevier.com/results-in-applied-mathematics/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/results-in-applied-mathematics"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/37943e58d84306d8a898acee96270f123c946c1a",
                "title": "Enhanced fifth order WENO Shock-Capturing Schemes with Deep Learning",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2052302425",
                        "name": "Tatiana Kossaczk'a"
                    },
                    {
                        "authorId": "1707694",
                        "name": "Matthias Ehrhardt"
                    },
                    {
                        "authorId": "2093700539",
                        "name": "Michael Gunther"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "An orthogonal line of work use neural networks as powerful function approximators in existing numerical PDE and linear system solvers to achieve faster convergence, generalize to different boundary conditions or larger problems, and approximate underresolved features in coarse-grained simulations (Hsieh et al., 2018; Luz et al., 2020; Bar-Sinai et al., 2019)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6c80e042dff64ad6fba6df856cd18e9cc6aa658a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-01342",
                    "ArXiv": "2103.01342",
                    "CorpusId": 232092347
                },
                "corpusId": 232092347,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6c80e042dff64ad6fba6df856cd18e9cc6aa658a",
                "title": "Reinforcement Learning for Adaptive Mesh Refinement",
                "abstract": "Large-scale finite element simulations of complex physical systems governed by partial differential equations (PDE) crucially depend on adaptive mesh refinement (AMR) to allocate computational budget to regions where higher resolution is required. Existing scalable AMR methods make heuristic refinement decisions based on instantaneous error estimation and thus do not aim for long-term optimality over an entire simulation. We propose a novel formulation of AMR as a Markov decision process and apply deep reinforcement learning (RL) to train refinement policies directly from simulation. AMR poses a new problem for RL as both the state dimension and available action set changes at every step, which we solve by proposing new policy architectures with differing generality and inductive bias. The model sizes of these policy architectures are independent of the mesh size and hence can be deployed on larger simulations than those used at train time. We demonstrate in comprehensive experiments on static function estimation and time-dependent equations that RL policies can be trained on problems without using ground truth solutions, are competitive with a widely-used error estimator, and generalize to larger, more complex, and unseen test problems.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109723412",
                        "name": "Jiachen Yang"
                    },
                    {
                        "authorId": "103407813",
                        "name": "T. Dzanic"
                    },
                    {
                        "authorId": "1814142",
                        "name": "Brenden K. Petersen"
                    },
                    {
                        "authorId": "32024672",
                        "name": "Junpei Kudo"
                    },
                    {
                        "authorId": "145358470",
                        "name": "K. Mittal"
                    },
                    {
                        "authorId": "2819139",
                        "name": "V. Tomov"
                    },
                    {
                        "authorId": "73467034",
                        "name": "Jean-Sylvain Camier"
                    },
                    {
                        "authorId": "36345161",
                        "name": "T. Zhao"
                    },
                    {
                        "authorId": "145203884",
                        "name": "H. Zha"
                    },
                    {
                        "authorId": "2601874",
                        "name": "T. Kolev"
                    },
                    {
                        "authorId": "2115697510",
                        "name": "Robert W. Anderson"
                    },
                    {
                        "authorId": "2142725",
                        "name": "D. Faissol"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "As suggested in [Hsieh et al., 2019], we also choose different iteration number k, 1 \u2264 k \u2264 b in the training, so that H learns to converge at each iteration, where larger b mimics the behavior of solving problems to higher accuracy while smaller b mimics inexpensive smoothing steps in multigrid."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2228bc91124a8b3b5e71511b9327936f95d682ad",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-12071",
                    "ArXiv": "2102.12071",
                    "DOI": "10.1137/21m1430030",
                    "CorpusId": 232035769
                },
                "corpusId": 232035769,
                "publicationVenue": {
                    "id": "0e3b51a7-21d8-477c-8918-14a55f087532",
                    "name": "SIAM Journal on Scientific Computing",
                    "type": "journal",
                    "alternate_names": [
                        "SIAM J Sci Comput"
                    ],
                    "issn": "1064-8275",
                    "url": "http://www.siam.org/journals/sisc.php",
                    "alternate_urls": [
                        "https://epubs.siam.org/journal/sjoce3"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2228bc91124a8b3b5e71511b9327936f95d682ad",
                "title": "Learning optimal multigrid smoothers via neural networks",
                "abstract": "Multigrid methods are one of the most efficient techniques for solving linear systems arising from Partial Differential Equations (PDEs) and graph Laplacians from machine learning applications. One of the key components of multigrid is smoothing, which aims at reducing high-frequency errors on each grid level. However, finding optimal smoothing algorithms is problem-dependent and can impose challenges for many problems. In this paper, we propose an efficient adaptive framework for learning optimized smoothers from operator stencils in the form of convolutional neural networks (CNNs). The CNNs are trained on small-scale problems from a given type of PDEs based on a supervised loss function derived from multigrid convergence theories, and can be applied to large-scale problems of the same class of PDEs. Numerical results on anisotropic rotated Laplacian problems demonstrate improved convergence rates and solution time compared with classical hand-crafted relaxation methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2140386828",
                        "name": "Ru Huang"
                    },
                    {
                        "authorId": "47370428",
                        "name": "Ruipeng Li"
                    },
                    {
                        "authorId": "2994506",
                        "name": "Yuanzhe Xi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Consequently, it is deeply unavoidable to find a numerical solution to the PDEs, which has always been a research hotspot by the researchers over the past few decades [9-13].",
                "2 learning tools in designing data-driven methods for solving PDEs with the improvement of the computing performance of computers [9-13]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "207ccac849e7611ff9b6b590bb9cf5485a5fce06",
                "externalIds": {
                    "MAG": "3133359221",
                    "DOI": "10.1088/1742-6596/1754/1/012228",
                    "CorpusId": 234032112
                },
                "corpusId": 234032112,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/207ccac849e7611ff9b6b590bb9cf5485a5fce06",
                "title": "The PINNs method discovery to the solution of coupled Wave- Klein-Gordon equations",
                "abstract": "Recently, the research of PDEs is regarded as one of the most important disciplines. Almost all scientific problems can be described by a differential equation, especially, many physical phenomena can be described by the system of coupled Wave-Klein-Gordon equations, which plays an important role in high-performance computing, control engineering, and electronic power system. Consequently, in our work, we use the PINNs to solve the numerical solution of coupled Wave-Klein-Gordon equations, to help us better understand the nonlinear physical phenomena, and to promote the rapid development of various fields such as in high-performance computing, control engineering, and electronic power system.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49980876",
                        "name": "Tian-Yi Wang"
                    },
                    {
                        "authorId": "143904235",
                        "name": "Xue-bin Chi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "The Poisson equation serves as an example problem in the recent literature; see [32, 11, 34].",
                "A fast, iterative PDE-solver was proposed by learning to modify each iteration of the existing solver [11]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3563948f6101c0691436d475b68dfe544b9c9248",
                "externalIds": {
                    "ArXiv": "2101.08932",
                    "CorpusId": 244954902
                },
                "corpusId": 244954902,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3563948f6101c0691436d475b68dfe544b9c9248",
                "title": "Sobolev Training for Physics Informed Neural Networks",
                "abstract": "Physics Informed Neural Networks (PINNs) is a promising application of deep learning. The smooth architecture of a fully connected neural network is appropriate for finding the solutions of PDEs; the corresponding loss function can also be intuitively designed and guarantees the convergence for various kinds of PDEs. However, the rate of convergence has been considered as a weakness of this approach. This paper proposes Sobolev-PINNs, a novel loss function for the training of PINNs, making the training substantially efficient. Inspired by the recent studies that incorporate derivative information for the training of neural networks, we develop a loss function that guides a neural network to reduce the error in the corresponding Sobolev space. Surprisingly, a simple modification of the loss function can make the training process similar to \\textit{Sobolev Training} although PINNs is not a fully supervised learning task. We provide several theoretical justifications that the proposed loss functions upper bound the error in the corresponding Sobolev spaces for the viscous Burgers equation and the kinetic Fokker--Planck equation. We also present several simulation results, which show that compared with the traditional $L^2$ loss function, the proposed loss function guides the neural network to a significantly faster convergence. Moreover, we provide the empirical evidence that shows that the proposed loss function, together with the iterative sampling techniques, performs better in solving high dimensional PDEs.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "151372987",
                        "name": "Hwijae Son"
                    },
                    {
                        "authorId": "2109878158",
                        "name": "Jin Woo Jang"
                    },
                    {
                        "authorId": "2114924549",
                        "name": "W. Han"
                    },
                    {
                        "authorId": "32013090",
                        "name": "H. Hwang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Instead of emulating the analog computing blocks, neural networks are used to find specific hyper-parameters of them or analog circuit system such as the arrangement of units or blocks [8], layout [9], and non-linear factor of crossbar arrays [2].",
                "As neural networks have the ability to find solutions of specific partial or ordinary differential equations [15, 9], we use FCNN or Neural ODE [18] as circuit equation solvers.",
                "As deep neural networks (DNNs) have the ability to solve ordinary or partial differential equations [15, 9], DNNs are used as a model to emulate the circuit behavior which can be represented by differential equations [16, 5]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d1ae9933af52b4b232aeb916dfee3b1fd9bb025b",
                "externalIds": {
                    "ArXiv": "2101.07864",
                    "DBLP": "journals/corr/abs-2101-07864",
                    "CorpusId": 231648130
                },
                "corpusId": 231648130,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d1ae9933af52b4b232aeb916dfee3b1fd9bb025b",
                "title": "SEMULATOR: Emulating the Dynamics of Crossbar Array-based Analog Neural System with Regression Neural Networks",
                "abstract": "As deep neural networks require tremendous amount of computation and memory, analog computing with emerging memory devices is a promising alternative to digital computing for edge devices. However, because of the increasing simulation time for analog computing system, it has not been explored. To overcome this issue, analytically approximated simulators are developed, but these models are inaccurate and narrow down the options for peripheral circuits for multiply-accumulate operation (MAC). In this sense, we propose a methodology, SEMULATOR (SiMULATOR by Emulating the analog computing block) which uses a deep neural network to emulate the behavior of crossbar-based analog computing system. With the proposed neural architecture, we experimentally and theoretically shows that it emulates a MAC unit for neural computation. In addition, the simulation time is incomparably reduced when it compared to the circuit simulators such as SPICE.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "7497273",
                        "name": "Chaeun Lee"
                    },
                    {
                        "authorId": "48388762",
                        "name": "Seyoung Kim"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In addition, supervised learning has also been used to guide the discretization process in a data-driven way [3] or to learn efficient iterative solvers [13]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1b5962d8a04ce69687ec2eeb63fef98c696be71b",
                "externalIds": {
                    "MAG": "3095154255",
                    "ArXiv": "2011.01456",
                    "DBLP": "journals/corr/abs-2011-01456",
                    "CorpusId": 226237604
                },
                "corpusId": 226237604,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1b5962d8a04ce69687ec2eeb63fef98c696be71b",
                "title": "Frequency-compensated PINNs for Fluid-dynamic Design Problems",
                "abstract": "Incompressible fluid flow around a cylinder is one of the classical problems in fluid-dynamics with strong relevance with many real-world engineering problems, for example, design of offshore structures or design of a pin-fin heat exchanger. Thus learning a high-accuracy surrogate for this problem can demonstrate the efficacy of a novel machine learning approach. In this work, we propose a physics-informed neural network (PINN) architecture for learning the relationship between simulation output and the underlying geometry and boundary conditions. In addition to using a physics-based regularization term, the proposed approach also exploits the underlying physics to learn a set of Fourier features, i.e. frequency and phase offset parameters, and then use them for predicting flow velocity and pressure over the spatio-temporal domain. We demonstrate this approach by predicting simulation results over out of range time interval and for novel design conditions. Our results show that incorporation of Fourier features improves the generalization performance over both temporal domain and design space.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2117882823",
                        "name": "Tongtao Zhang"
                    },
                    {
                        "authorId": "32553531",
                        "name": "Biswadip Dey"
                    },
                    {
                        "authorId": "102395044",
                        "name": "P. Kakkar"
                    },
                    {
                        "authorId": "49949943",
                        "name": "A. Dasgupta"
                    },
                    {
                        "authorId": "2019491887",
                        "name": "Amit Chakraborty"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e132a1a54c30fccca4bb6a5017a95002c8a0d9b4",
                "externalIds": {
                    "MAG": "3095353524",
                    "ArXiv": "2010.14088",
                    "DBLP": "journals/corr/abs-2010-14088",
                    "DOI": "10.1016/j.jcp.2022.110996",
                    "CorpusId": 225076076
                },
                "corpusId": 225076076,
                "publicationVenue": {
                    "id": "58606051-2e63-4034-8496-9d4ed773bb49",
                    "name": "Journal of Computational Physics",
                    "type": "journal",
                    "alternate_names": [
                        "J Comput Phys"
                    ],
                    "issn": "0021-9991",
                    "url": "http://www.elsevier.com/locate/jcp",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00219991",
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622866/description#description",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e132a1a54c30fccca4bb6a5017a95002c8a0d9b4",
                "title": "Meta-MgNet: Meta Multigrid Networks for Solving Parameterized Partial Differential Equations",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2109290651",
                        "name": "Yuyan Chen"
                    },
                    {
                        "authorId": "145496882",
                        "name": "Bin Dong"
                    },
                    {
                        "authorId": "40011783",
                        "name": "Jinchao Xu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Learnable surrogates were proposed for various scientific computing tasks in the natural sciences by exploiting fully-connected [18]\u2013[20], convolutional [21]\u2013 [23], and hybrid [24]\u2013[26] neural architectures."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1260bd8d211ef0d14b48ef14e84e3717ee971b33",
                "externalIds": {
                    "ArXiv": "2009.04240",
                    "DBLP": "journals/corr/abs-2009-04240",
                    "MAG": "3084369082",
                    "CorpusId": 221557165
                },
                "corpusId": 221557165,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1260bd8d211ef0d14b48ef14e84e3717ee971b33",
                "title": "Real-time Bayesian personalization via a learnable brain tumor growth model",
                "abstract": "Modeling of brain tumor dynamics has the potential to advance therapeutic planning. Current modeling approaches resort to numerical solvers that simulate the tumor progression according to a given differential equation. Using highly-efficient numerical solvers, a single forward simulation takes up to a few minutes of compute. At the same time, clinical applications of the tumor modeling often imply solving an inverse problem, requiring up to tens of thousands forward model evaluations when used for a Bayesian model personalization via sampling. This results in a total inference time prohibitively expensive for clinical translation. Moreover, while recent data-driven approaches become capable of emulating physics simulation, they tend to fail in generalizing over the variability of the boundary conditions imposed by the patient-specific anatomy. In this paper, we propose a learnable surrogate with anatomy encoder for simulating tumor growth which maps the biophysical model parameters directly to simulation outputs, i.e. the local tumor cell densities. We test the surrogate on Bayesian tumor model personalization for a cohort of glioma patients. Bayesian inference using the proposed neural surrogate yields estimates analogous to those obtained by solving the forward model with a regular numerical solver. The near real-time computation cost renders the proposed method suitable for clinical settings. The code is available at this https URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2065232462",
                        "name": "I. Ezhov"
                    },
                    {
                        "authorId": "1932452408",
                        "name": "T. Mot"
                    },
                    {
                        "authorId": "1561461499",
                        "name": "Suprosanna Shit"
                    },
                    {
                        "authorId": "1959705",
                        "name": "Jana Lipkov\u00e1"
                    },
                    {
                        "authorId": "1561434672",
                        "name": "J. Paetzold"
                    },
                    {
                        "authorId": "94576427",
                        "name": "F. Kofler"
                    },
                    {
                        "authorId": "2060880353",
                        "name": "F. Navarro"
                    },
                    {
                        "authorId": "1387099409",
                        "name": "M. Metz"
                    },
                    {
                        "authorId": "2364183",
                        "name": "B. Wiestler"
                    },
                    {
                        "authorId": "143893221",
                        "name": "Bjoern H Menze"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Inspired by the recently proposed deep PDE solver [2], we introduce a deep learning-based thickness solver.",
                "The thickness estimation problem is more challenging than [2], since additional procedures are involved, such as integrating lines and interpolating missing values (refer to section 2."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3d94e4d1f8c0d1de6c135b8ab208b1c162eec6b7",
                "externalIds": {
                    "MAG": "3081269842",
                    "DBLP": "journals/corr/abs-2008-11109",
                    "ArXiv": "2008.11109",
                    "DOI": "10.1007/978-3-030-68107-4_5",
                    "CorpusId": 221293208
                },
                "corpusId": 221293208,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3d94e4d1f8c0d1de6c135b8ab208b1c162eec6b7",
                "title": "Measure Anatomical Thickness from Cardiac MRI with Deep Neural Networks",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "48185960",
                        "name": "Qiaoying Huang"
                    },
                    {
                        "authorId": "32891169",
                        "name": "Eric Z. Chen"
                    },
                    {
                        "authorId": "30863081",
                        "name": "Hanchao Yu"
                    },
                    {
                        "authorId": "1734663",
                        "name": "Yimo Guo"
                    },
                    {
                        "authorId": "1846624",
                        "name": "Terrence Chen"
                    },
                    {
                        "authorId": "1711560",
                        "name": "Dimitris N. Metaxas"
                    },
                    {
                        "authorId": "3123990",
                        "name": "Shanhui Sun"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "481813caadf69883205537cd47d7b6ad6572ea04",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-12792",
                    "MAG": "3044664263",
                    "ArXiv": "2007.12792",
                    "DOI": "10.1109/MLHPCAI4S51975.2020.00013",
                    "CorpusId": 220793704
                },
                "corpusId": 220793704,
                "publicationVenue": {
                    "id": "bc17fdd4-2302-49ff-9094-d2e8e4075722",
                    "name": "Workshop on Machine Learning in High Performance Computing Environments",
                    "type": "conference",
                    "alternate_names": [
                        "MLHPC",
                        "Workshop Mach Learn High Perform Comput Environ"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/481813caadf69883205537cd47d7b6ad6572ea04",
                "title": "Deep Generative Models that Solve PDEs: Distributed Computing for Training Large Data-Free Models",
                "abstract": "Recent progress in scientific machine learning (SciML) has opened up the possibility of training novel neural network architectures that solve complex partial differential equations (PDEs). Several (nearly data free) approaches have been recently reported that successfully solve PDEs, with examples including deep feed forward networks, generative networks, and deep encoder-decoder networks. However, practical adoption of these approaches is limited by the difficulty in training these models, especially to make predictions at large output resolutions (\u2265 1024 \u00d7 1024).Here we report on a software framework for data parallel distributed deep learning that resolves the twin challenges of training these large SciML models training in reasonable time as well as distributing the storage requirements. Our framework provides several out of the box functionality including (a) loss integrity independent of number of processes, (b) synchronized batch normalization, and (c) distributed higher-order optimization methods.We show excellent scalability of this framework on both cloud as well as HPC clusters, and report on the interplay between bandwidth, network topology and bare metal vs cloud. We deploy this approach to train generative models of sizes hitherto not possible, showing that neural PDE solvers can be viably trained for practical applications. We also demonstrate that distributed higher-order optimization methods are 2\u20133 \u00d7 faster than stochastic gradient-based methods and provide minimal convergence drift with higher batch-size.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2065982189",
                        "name": "Sergio Botelho"
                    },
                    {
                        "authorId": "144931667",
                        "name": "Ameya Joshi"
                    },
                    {
                        "authorId": "116185285",
                        "name": "Biswajit Khara"
                    },
                    {
                        "authorId": "144016196",
                        "name": "S. Sarkar"
                    },
                    {
                        "authorId": "144398138",
                        "name": "C. Hegde"
                    },
                    {
                        "authorId": "2575361",
                        "name": "Santi S. Adavani"
                    },
                    {
                        "authorId": "3042938",
                        "name": "B. Ganapathysubramanian"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Prior work has shown that neural networks can learn how to solve NP-complete decision problems and optimization problems [43, 39, 23]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "edbcaf80ad4218fe04e474036484b5543c6f8f49",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-00295",
                    "MAG": "3038228403",
                    "ArXiv": "2007.00295",
                    "CorpusId": 220280768
                },
                "corpusId": 220280768,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/edbcaf80ad4218fe04e474036484b5543c6f8f49",
                "title": "Belief Propagation Neural Networks",
                "abstract": "Learned neural solvers have successfully been used to solve combinatorial optimization and decision problems. More general counting variants of these problems, however, are still largely solved with hand-crafted solvers. To bridge this gap, we introduce belief propagation neural networks (BPNNs), a class of parameterized operators that operate on factor graphs and generalize Belief Propagation (BP). In its strictest form, a BPNN layer (BPNN-D) is a learned iterative operator that provably maintains many of the desirable properties of BP for any choice of the parameters. Empirically, we show that by training BPNN-D learns to perform the task better than the original BP: it converges 1.7x faster on Ising models while providing tighter bounds. On challenging model counting problems, BPNNs compute estimates 100's of times faster than state-of-the-art handcrafted methods, while returning an estimate of comparable quality.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "50454693",
                        "name": "Jonathan Kuck"
                    },
                    {
                        "authorId": "1455756188",
                        "name": "Shuvam Chakraborty"
                    },
                    {
                        "authorId": "2109238481",
                        "name": "Hao Tang"
                    },
                    {
                        "authorId": "39109324",
                        "name": "Rachel Luo"
                    },
                    {
                        "authorId": "51453887",
                        "name": "Jiaming Song"
                    },
                    {
                        "authorId": "48229640",
                        "name": "Ashish Sabharwal"
                    },
                    {
                        "authorId": "2490652",
                        "name": "Stefano Ermon"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Existing studies have also identified discontinuities in finite-difference solutions with deep learning [46] and focused on improving the iterative behavior of linear solvers [24]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8995944de6f298c6a439675e31068ab8258722ba",
                "externalIds": {
                    "ArXiv": "2007.00016",
                    "MAG": "3098435014",
                    "DBLP": "journals/corr/abs-2007-00016",
                    "CorpusId": 220280657
                },
                "corpusId": 220280657,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8995944de6f298c6a439675e31068ab8258722ba",
                "title": "Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative PDE-Solvers",
                "abstract": "Finding accurate solutions to partial differential equations (PDEs) is a crucial task in all scientific and engineering disciplines. It has recently been shown that machine learning methods can improve the solution accuracy by correcting for effects not captured by the discretized PDE. We target the problem of reducing numerical errors of iterative PDE solvers and compare different learning approaches for finding complex correction functions. We find that previously used learning approaches are significantly outperformed by methods that integrate the solver into the training loop and thereby allow the model to interact with the PDE during training. This provides the model with realistic input distributions that take previous corrections into account, yielding improvements in accuracy with stable rollouts of several hundred recurrent evaluation steps and surpassing even tailored supervised variants. We highlight the performance of the differentiable physics networks for a wide variety of PDEs, from non-linear advection-diffusion systems to three-dimensional Navier-Stokes flows.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2679476",
                        "name": "Kiwon Um"
                    },
                    {
                        "authorId": "144179670",
                        "name": "Yun Fei"
                    },
                    {
                        "authorId": "13094542",
                        "name": "P. Holl"
                    },
                    {
                        "authorId": "117920737",
                        "name": "R. Brand"
                    },
                    {
                        "authorId": "1786445",
                        "name": "N. Th\u00fcrey"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[17] aims to improve an existing solver through the supplementary application of a neural network.",
                "A different direction within the optimization of multigrid methods, that has recently become popular, is the application of machine learning to improve the individual components of a solver, such as [12, 17, 18]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "10af5e0fa912f26dc35959a5600e6525caa3e504",
                "externalIds": {
                    "MAG": "3040239497",
                    "DBLP": "conf/gecco/SchmittKK20",
                    "DOI": "10.1145/3377930.3389811",
                    "CorpusId": 220252819
                },
                "corpusId": 220252819,
                "publicationVenue": {
                    "id": "d732841e-83f9-49ec-95ca-389e5568634b",
                    "name": "Annual Conference on Genetic and Evolutionary Computation",
                    "type": "conference",
                    "alternate_names": [
                        "GECCO",
                        "Annu Conf Genet Evol Comput",
                        "Genet Evol Comput Conf",
                        "Genetic and Evolutionary Computation Conference"
                    ],
                    "url": "http://www.sigevo.org/"
                },
                "url": "https://www.semanticscholar.org/paper/10af5e0fa912f26dc35959a5600e6525caa3e504",
                "title": "Constructing efficient multigrid solvers with genetic programming",
                "abstract": "For many linear and nonlinear systems that arise from the discretization of partial differential equations the construction of an efficient multigrid solver is a challenging task. Here we present a novel approach for the optimization of geometric multigrid methods that is based on evolutionary computation, a generic program optimization technique inspired by the principle of natural evolution. A multigrid solver is represented as a tree of mathematical expressions which we generate based on a tailored grammar. The quality of each solver is evaluated in terms of convergence and compute performance using automated local Fourier analysis (LFA) and roofline performance modeling, respectively. Based on these objectives a multi-objective optimization is performed using grammar-guided genetic programming with a non-dominated sorting based selection. To evaluate the model-based prediction and to target concrete applications, scalable implementations of an evolved solver can be automatically generated with the ExaStencils framework. We demonstrate the effectiveness of our approach by constructing multigrid solvers for a linear elastic boundary value problem that are competitive with common V- and W-cycles.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "29915026",
                        "name": "J. Schmitt"
                    },
                    {
                        "authorId": "2315743",
                        "name": "S. Kuckuk"
                    },
                    {
                        "authorId": "35161054",
                        "name": "H. K\u00f6stler"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "The authors in [12] modified existing iterative PDEs solvers with a deep neural network in order to accelerate their convergence speed.",
                "ML techniques have been used to effectively solve differential equations [5], [12], [21], [22], and [27] but also to discover underlying dynamics from data [2], [20] and [23] and even to build robust neural networks architectures based on differential equations [3], [17] and [25] ."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7a17db56d23340af97210bb420b5baf1014edf0f",
                "externalIds": {
                    "MAG": "3032060282",
                    "DBLP": "journals/corr/abs-2005-13100",
                    "ArXiv": "2005.13100",
                    "CorpusId": 218900737
                },
                "corpusId": 218900737,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7a17db56d23340af97210bb420b5baf1014edf0f",
                "title": "Approximating periodic functions and solving differential equations using a novel type of Fourier Neural Networks",
                "abstract": "Recently, machine learning tools in particular neural networks have been widely used to solve differential equations. One main advantage of using machine learning, in this case, is that one does not need to mesh the computational domain and can instead randomly draw data points to solve the differential equations of interest. In this work, we propose a simple neural network to approximate low-frequency periodic functions or seek such solutions of differential equations. To this end, we build a Fourier Neural Network (FNN) represented as a shallow neural network (i.e with one hidden layer) based on the Fourier Decomposition. As opposed to traditional neural networks, which feature activation functions such as the sigmoid, logistic, ReLU, hyperbolic tangent and softmax functions, Fourier Neural Networks are composed using sinusoidal activation functions. We propose a strategy to initialize the weights of this FNN and showcase its performance against traditional networks for function approximations and differential equations solutions.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "103457079",
                        "name": "M. Ngom"
                    },
                    {
                        "authorId": "144719701",
                        "name": "O. Marin"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Recently, machine-learning based models have been developed to either learn new discretization schemes from solution data [11, 12] or to mimic these schemes through novelties in neural network architectures [13, 14]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "84217089f7393febd46ebc374570a0ff03b676be",
                "externalIds": {
                    "MAG": "3025960736",
                    "DBLP": "journals/corr/abs-2005-08357",
                    "ArXiv": "2005.08357",
                    "DOI": "10.1016/J.CMA.2021.113722",
                    "CorpusId": 218673438
                },
                "corpusId": 218673438,
                "publicationVenue": {
                    "id": "3bfaa538-a67d-47d7-bfda-6f82748e9a29",
                    "name": "Computer Methods in Applied Mechanics and Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Method Appl Mech Eng"
                    ],
                    "issn": "0045-7825",
                    "url": "https://www.journals.elsevier.com/computer-methods-in-applied-mechanics-and-engineering",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00457825"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/84217089f7393febd46ebc374570a0ff03b676be",
                "title": "DiscretizationNet: A Machine-Learning based solver for Navier-Stokes Equations using Finite Volume Discretization",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "94583808",
                        "name": "Rishikesh Ranade"
                    },
                    {
                        "authorId": "2150315516",
                        "name": "C. Hill"
                    },
                    {
                        "authorId": "143772779",
                        "name": "Jay Pathak"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The second approach does not directly seek to find the parametric map from X to Y but rather is thought of, for fixed x \u2208 X , as being a parametrization of the solution y \u2208 Y by means of a deep neural network [19, 20, 29, 32, 44, 50]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8bd148ef2082c022a60ee02d56b0e602136761ab",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2005-03180",
                    "MAG": "3178968719",
                    "ArXiv": "2005.03180",
                    "DOI": "10.5802/SMAI-JCM.74",
                    "CorpusId": 260526711
                },
                "corpusId": 260526711,
                "publicationVenue": {
                    "id": "01b3549a-3386-470c-afe0-6cf2d6f1b336",
                    "name": "SMAI Journal of Computational Mathematics",
                    "type": "journal",
                    "alternate_names": [
                        "SMAI J Comput Math"
                    ],
                    "issn": "2426-8399",
                    "url": "https://smai-jcm.math.cnrs.fr/index.php/SMAI-JCM/",
                    "alternate_urls": [
                        "https://smai-jcm.math.cnrs.fr/index.php/SMAI-JCM/index",
                        "https://web.archive.org/web/*/http:/smai-jcm.cedram.org/smai-jcm-bin/feuilleter",
                        "http://www.numdam.org/journals/SMAI-JCM/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8bd148ef2082c022a60ee02d56b0e602136761ab",
                "title": "Model Reduction and Neural Networks for Parametric PDEs",
                "abstract": "We develop a general framework for data-driven approximation of input-output maps between infinite-dimensional spaces. The proposed approach is motivated by the recent successes of neural networks and deep learning, in combination with ideas from model reduction. This combination results in a neural network approximation which, in principle, is defined on infinite-dimensional spaces and, in practice, is robust to the dimension of finite-dimensional approximations of these spaces required for computation. For a class of input-output maps, and suitably chosen probability measures on the inputs, we prove convergence of the proposed approximation methodology. Numerically we demonstrate the effectiveness of the method on a class of parametric elliptic PDE problems, showing convergence and robustness of the approximation scheme with respect to the size of the discretization, and compare our method with existing algorithms from the literature.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "144304264",
                        "name": "K. Bhattacharya"
                    },
                    {
                        "authorId": "32067643",
                        "name": "Bamdad Hosseini"
                    },
                    {
                        "authorId": "51219644",
                        "name": "Nikola B. Kovachki"
                    },
                    {
                        "authorId": "4662911",
                        "name": "A. Stuart"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Another notable related paper is Hsieh et al. (2019), which uses a convolutional network to improve on an existing linear iterative solver.",
                "Another notable related paper is Hsieh et al. (2019), which uses a convolutional network to improve on an existing linear iterative solver. In particular, learning is applied to improve a GMG algorithm for structured Poisson problems in an end-to-end manner, by using a U-Net architecture with several downsampling and upsampling layers, and learning from supervised data. Schmitt et al. (2019) use evolutionary methods to optimize a GMG solver.",
                "Another notable related paper is Hsieh et al. (2019), which uses a convolutional network to improve on an existing linear iterative solver. In particular, learning is applied to improve a GMG algorithm for structured Poisson problems in an end-to-end manner, by using a U-Net architecture with several downsampling and upsampling layers, and learning from supervised data. Schmitt et al. (2019) use evolutionary methods to optimize a GMG solver. Katrutsa et al. (2017) optimize restriction and prolongation operators for GMG, by formulating the entire two-grid algorithm as a deep neural network, and approximately minimizing the spectral radius of the resulting iteration matrix. They evaluate their method on single instances of various structured-grid differential equations in 1D. Sun et al. (2003) use a tailored network with a single hidden layer to solve the Poisson equation on a specific mesh.",
                "Another notable related paper is Hsieh et al. (2019), which uses a convolutional network to improve on an existing linear iterative solver. In particular, learning is applied to improve a GMG algorithm for structured Poisson problems in an end-to-end manner, by using a U-Net architecture with several downsampling and upsampling layers, and learning from supervised data. Schmitt et al. (2019) use evolutionary methods to optimize a GMG solver. Katrutsa et al. (2017) optimize restriction and prolongation operators for GMG, by formulating the entire two-grid algorithm as a deep neural network, and approximately minimizing the spectral radius of the resulting iteration matrix."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "ac57e68be50d3367f0e14e9b5bb7e8c4db4aad41",
                "externalIds": {
                    "MAG": "3034265363",
                    "ArXiv": "2003.05744",
                    "DBLP": "journals/corr/abs-2003-05744",
                    "CorpusId": 212675750
                },
                "corpusId": 212675750,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ac57e68be50d3367f0e14e9b5bb7e8c4db4aad41",
                "title": "Learning Algebraic Multigrid Using Graph Neural Networks",
                "abstract": "Efficient numerical solvers for sparse linear systems are crucial in science and engineering. One of the fastest methods for solving large-scale sparse linear systems is algebraic multigrid (AMG). The main challenge in the construction of AMG algorithms is the selection of the prolongation operator -- a problem-dependent sparse matrix which governs the multiscale hierarchy of the solver and is critical to its efficiency. Over many years, numerous methods have been developed for this task, and yet there is no known single right answer except in very special cases. Here we propose a framework for learning AMG prolongation operators for linear systems with sparse symmetric positive (semi-) definite matrices. We train a single graph neural network to learn a mapping from an entire class of such matrices to prolongation operators, using an efficient unsupervised loss function. Experiments on a broad class of problems demonstrate improved convergence rates compared to classical AMG, demonstrating the potential utility of neural networks for developing sparse system solvers.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1562121845",
                        "name": "Ilay Luz"
                    },
                    {
                        "authorId": "2991672",
                        "name": "M. Galun"
                    },
                    {
                        "authorId": "3416939",
                        "name": "Haggai Maron"
                    },
                    {
                        "authorId": "1760994",
                        "name": "R. Basri"
                    },
                    {
                        "authorId": "1699099",
                        "name": "I. Yavneh"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "In this regard, Hsieh et al. (2019) and Farimani et al. (2017) have, for example, demonstrated that it is possible to learn a general PDE solver for some simple linear and elliptic PDEs.",
                "Tadmor (2012))."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "801f0817069232add1852d3c6b542e9175874e45",
                "externalIds": {
                    "ArXiv": "2003.12159",
                    "MAG": "3013116801",
                    "DBLP": "journals/corr/abs-2003-12159",
                    "CorpusId": 214693392
                },
                "corpusId": 214693392,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/801f0817069232add1852d3c6b542e9175874e45",
                "title": "Learning To Solve Differential Equations Across Initial Conditions",
                "abstract": "Recently, there has been a lot of interest in using neural networks for solving partial differential equations. A number of neural network-based partial differential equation solvers have been formulated which provide performances equivalent, and in some cases even superior, to classical solvers. However, these neural solvers, in general, need to be retrained each time the initial conditions or the domain of the partial differential equation changes. In this work, we posit the problem of approximating the solution of a fixed partial differential equation for any arbitrary initial conditions as learning a conditional probability distribution. We demonstrate the utility of our method on Burger's Equation.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2113570191",
                        "name": "Shehryar Malik"
                    },
                    {
                        "authorId": "2066185365",
                        "name": "Usman Anwar"
                    },
                    {
                        "authorId": "104492197",
                        "name": "Ali Ahmed"
                    },
                    {
                        "authorId": "3044174",
                        "name": "A. Aghasi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Recently, because of the trends for the efficient hardware implementations, neural networks have been increasingly applied in solving PDEs [6, 32, 51, 59, 60, 74, 89, 97, 99, 101, 13, 12, 14, 15, 22, 24, 31, 33, 36, 43, 47, 50, 52, 58, 63, 68, 69, 72, 73, 77, 80, 85, 86, 106, 91, 100, 102, 103, 107, 108, 109, 110] and inverse problems involving PDEs [109, 79, 78, 87, 88, 90, 92, 93, 95, 96, 94, 105, 112, 113]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "41398eb79faef56f054f5078dcc1d95164e6c1f3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2002-09750",
                    "MAG": "3092141412",
                    "ArXiv": "2002.09750",
                    "DOI": "10.1016/J.JCP.2020.109907",
                    "CorpusId": 211258709
                },
                "corpusId": 211258709,
                "publicationVenue": {
                    "id": "58606051-2e63-4034-8496-9d4ed773bb49",
                    "name": "Journal of Computational Physics",
                    "type": "journal",
                    "alternate_names": [
                        "J Comput Phys"
                    ],
                    "issn": "0021-9991",
                    "url": "http://www.elsevier.com/locate/jcp",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00219991",
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622866/description#description",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/41398eb79faef56f054f5078dcc1d95164e6c1f3",
                "title": "On some neural network architectures that can represent viscosity solutions of certain high dimensional Hamilton-Jacobi partial differential equations",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2221048",
                        "name": "J. Darbon"
                    },
                    {
                        "authorId": "148220583",
                        "name": "Tingwei Meng"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "24fad232ed20b6e9b965a655730605b5d7e2d074",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2002-03014",
                    "ArXiv": "2002.03014",
                    "MAG": "3005100040",
                    "CorpusId": 211069365
                },
                "corpusId": 211069365,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/24fad232ed20b6e9b965a655730605b5d7e2d074",
                "title": "FiniteNet: A Fully Convolutional LSTM Network Architecture for Time-Dependent Partial Differential Equations",
                "abstract": "In this work, we present a machine learning approach for reducing the error when numerically solving time-dependent partial differential equations (PDE). We use a fully convolutional LSTM network to exploit the spatiotemporal dynamics of PDEs. The neural network serves to enhance finite-difference and finite-volume methods (FDM/FVM) that are commonly used to solve PDEs, allowing us to maintain guarantees on the order of convergence of our method. We train the network on simulation data, and show that our network can reduce error by a factor of 2 to 3 compared to the baseline algorithms. We demonstrate our method on three PDEs that each feature qualitatively different dynamics. We look at the linear advection equation, which propagates its initial conditions at a constant speed, the inviscid Burgers' equation, which develops shockwaves, and the Kuramoto-Sivashinsky (KS) equation, which is chaotic.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "50021292",
                        "name": "Ben Stevens"
                    },
                    {
                        "authorId": "2462507",
                        "name": "T. Colonius"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Hsieh et al. (Hsieh et al., 2019) attempted to learn domain specific fast PDE solvers by learning how to iteratively improve the solution using a deep neural network, resulting in a 2-3 times speedup compared to state of the art solvers.",
                "[12] attempted to learn domain-specific fast PDE solvers by learning how to iteratively improve the solution using a deep neural network, resulting in a 2-speedup compared to state-of-the-art solvers."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3814865efd9d686fba414470762942d705f35710",
                "externalIds": {
                    "MAG": "3004857598",
                    "DBLP": "journals/corr/abs-2002-02521",
                    "ArXiv": "2002.02521",
                    "DOI": "10.1007/s00162-020-00531-1",
                    "CorpusId": 211066325
                },
                "corpusId": 211066325,
                "publicationVenue": {
                    "id": "fa9ec560-49d3-40c4-a4e1-9fe0a531e8d6",
                    "name": "Theoretical and Computational Fluid Dynamics",
                    "type": "journal",
                    "alternate_names": [
                        "Theor Comput Fluid Dyn"
                    ],
                    "issn": "0935-4964",
                    "url": "https://link.springer.com/journal/162"
                },
                "url": "https://www.semanticscholar.org/paper/3814865efd9d686fba414470762942d705f35710",
                "title": "Enhancement of shock-capturing methods via machine learning",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "50021292",
                        "name": "Ben Stevens"
                    },
                    {
                        "authorId": "2462507",
                        "name": "T. Colonius"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "raight forward with graph convolution operations. Another approach is to solve the problem inside a larger bounding box and then \ufb01lter out the solutions that does not belong to the region of interest [25]. Acknowledgement We would like to thank Dr. Yi Ren for the helpful suggestions in experiments and paper writing, Dr. Yuzhong Chen for proofreading the paper, and Haoyang Wei, Dr. Yang Yu for helping ",
                "ions: (1) All loading/ response observations are in image form. This assumption makes the use of convolutional neural networks as data-driven model possible. (2) Consider linear physics only. As with [24, 25], we start with simpler linear physics \ufb01rst since it is easier to prove the convergence of the proposed algorithm. Future work will extend this framework to non-linear physics and irregular mesh data.",
                "es inspired by FEA. Parallel works have been done to improve the convergence and accuracy of \ufb01nite dierence analysis (FDA) solvers to initial value problems (IVP) through learning the optimum \ufb01lters [25], or by building a hybrid model with ODE information hard-coded [44]. It is found that similarities exist between dierent FDA solvers and some neural network structures in [45]. Based on this \ufb01nding,",
                "ve been made to predict physics response or parameters with data driven models [8, 11, 12, 13, 16], and several seminal works have been done to build hybrid learning mechanisms with physics knowledge [14, 25, 41, 42, 43, 44, 45, 46].Early pioneering work has shown that the global or element stiness matrix can be learnt with neural network for simple systems [41].Based on the optimality condition of topology optimization, ecien"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "35063d2d72c6001daac49e710f32395049d2190a",
                "externalIds": {
                    "MAG": "3007593704",
                    "ArXiv": "2002.01893",
                    "DBLP": "journals/corr/abs-2002-01893",
                    "DOI": "10.1016/j.cma.2020.112892",
                    "CorpusId": 211032097
                },
                "corpusId": 211032097,
                "publicationVenue": {
                    "id": "3bfaa538-a67d-47d7-bfda-6f82748e9a29",
                    "name": "Computer Methods in Applied Mechanics and Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Method Appl Mech Eng"
                    ],
                    "issn": "0045-7825",
                    "url": "https://www.journals.elsevier.com/computer-methods-in-applied-mechanics-and-engineering",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00457825"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/35063d2d72c6001daac49e710f32395049d2190a",
                "title": "FEA-Net: A Physics-guided Data-driven Model for Efficient Mechanical Response Prediction",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "9506743",
                        "name": "Houpu Yao"
                    },
                    {
                        "authorId": "2118544323",
                        "name": "Yi Gao"
                    },
                    {
                        "authorId": "2143062695",
                        "name": "Yongming Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In this context, several works have proposed methods for improving the solution of PDE problems (Long et al., 2018; Bar-Sinai et al., 2019; Hsieh et al., 2019) or used PDE formulations for unsupervised optimization (Raissi et al.",
                "In this context, several works have proposed methods for improving the solution of PDE problems (Long et al., 2018; Bar-Sinai et al., 2019; Hsieh et al., 2019) or used PDE formulations for unsupervised optimization (Raissi et al., 2018)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6a83d327627aaa1abafb9b858110dd4876bfcb34",
                "externalIds": {
                    "MAG": "3003038987",
                    "DBLP": "journals/corr/abs-2001-07457",
                    "ArXiv": "2001.07457",
                    "CorpusId": 209334533
                },
                "corpusId": 209334533,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/6a83d327627aaa1abafb9b858110dd4876bfcb34",
                "title": "Learning to Control PDEs with Differentiable Physics",
                "abstract": "Predicting outcomes and planning interactions with the physical world are long-standing goals for machine learning. A variety of such tasks involves continuous physical systems, which can be described by partial differential equations (PDEs) with many degrees of freedom. Existing methods that aim to control the dynamics of such systems are typically limited to relatively short time frames or a small number of interaction parameters. We show that by using a differentiable PDE solver in conjunction with a novel predictor-corrector scheme, we can train neural networks to understand and control complex nonlinear physical systems over long time frames. We demonstrate that our method successfully develops an understanding of complex physical systems and learns to control them for tasks involving multiple PDEs, including the incompressible Navier-Stokes equations.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "13094542",
                        "name": "P. Holl"
                    },
                    {
                        "authorId": "145231047",
                        "name": "V. Koltun"
                    },
                    {
                        "authorId": "2045078305",
                        "name": "N. Thuerey"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7c4d76a5b0f0053dbf8f91453659678aa10b7816",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1911-10428",
                    "ArXiv": "1911.10428",
                    "MAG": "2991454810",
                    "CorpusId": 208268027
                },
                "corpusId": 208268027,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7c4d76a5b0f0053dbf8f91453659678aa10b7816",
                "title": "Constrained Linear Data-feature Mapping for Image Classification",
                "abstract": "In this paper, we propose a constrained linear data-feature mapping model as an interpretable mathematical model for image classification using convolutional neural network (CNN) such as the ResNet. From this viewpoint, we establish the detailed connections in a technical level between the traditional iterative schemes for constrained linear system and the architecture for the basic blocks of ResNet. Under these connections, we propose some natural modifications of ResNet type models which will have less parameters but still maintain almost the same accuracy as these corresponding original models. Some numerical experiments are shown to demonstrate the validity of this constrained learning data-feature mapping assumption.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "27794145",
                        "name": "Juncai He"
                    },
                    {
                        "authorId": "2109290651",
                        "name": "Yuyan Chen"
                    },
                    {
                        "authorId": "40011783",
                        "name": "Jinchao Xu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "ptimization methods [38, 39, 40, 147] and neural networks[7, 42, 81, 65, 77, 78, 98, 118, 129, 132, 134, 136]. Among these methods, neural networks have become increasingly popular tools to solve PDEs[7, 15, 14, 16, 18, 29, 31, 42, 41, 43, 46, 54, 59, 63, 64, 65, 66, 75, 77, 78, 79, 83, 90, 91, 96, 97, 98, 102, 107, 112, 113, 116, 118, 121, 129, 132, 133, 134, 136, 137, 138, 142, 144, 145, 146]and inverse problems involving PDEs [106, 105, 114, 115, 120, 124, 125, 127, 128, 126, 141, 145, 148, 149]. Their popularity is due to universal approximation theorems that state that neural networks "
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "efb5090c15648b23165cec11f162f92a9e812ce9",
                "externalIds": {
                    "MAG": "3043050416",
                    "ArXiv": "1910.09045",
                    "DOI": "10.1007/s40687-020-00215-6",
                    "CorpusId": 204800315
                },
                "corpusId": 204800315,
                "publicationVenue": {
                    "id": "d12a0066-833f-4e7a-a733-7afc7d179b43",
                    "name": "Research in the Mathematical Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Res Math Sci"
                    ],
                    "issn": "2197-9847",
                    "url": "http://www.resmathsci.com/",
                    "alternate_urls": [
                        "https://link.springer.com/article/10.1186/s40687-016-0091-8"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/efb5090c15648b23165cec11f162f92a9e812ce9",
                "title": "Overcoming the curse of dimensionality for some Hamilton\u2013Jacobi partial differential equations via neural network architectures",
                "abstract": null,
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2221048",
                        "name": "J. Darbon"
                    },
                    {
                        "authorId": "1869393",
                        "name": "G. P. Langlois"
                    },
                    {
                        "authorId": "148220583",
                        "name": "Tingwei Meng"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Motivated by [6] we construct a neural iterator from a semi-implicit update rule.",
                "[6] develop a promising way to learn numerical solver while providing a theoretical convergence guarantee.",
                "Implementation Details: Following [6], we use a three-layer convolutional neural network to model each of the Hi.",
                "On a stark contrast with previous work [6], we have several sets of parameters {\u0398i}i=1:N , \u03b4x, \u03b4t, and attached to the PDEs governing equation.",
                "2 Neural Solver We propose the following iterator using similar notation as in [6]"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "7bfcd2e30893c14558d2f0d6ad751a18b9e62610",
                "externalIds": {
                    "MAG": "2979569062",
                    "DBLP": "journals/corr/abs-1910-03452",
                    "ArXiv": "1910.03452",
                    "CorpusId": 203904897
                },
                "corpusId": 203904897,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7bfcd2e30893c14558d2f0d6ad751a18b9e62610",
                "title": "Implicit Neural Solver for Time-dependent Linear PDEs with Convergence Guarantee",
                "abstract": "Fast and accurate solution of time-dependent partial differential equations (PDEs) is of key interest in many research fields including physics, engineering, and biology. Generally, implicit schemes are preferred over the explicit ones for better stability and correctness. The existing implicit schemes are usually iterative and employ a general-purpose solver which may be sub-optimal for a specific class of PDEs. In this paper, we propose a neural solver to learn an optimal iterative scheme for a class of PDEs, in a data-driven fashion. We attain this objective by modifying an iteration of an existing semi-implicit solver using a deep neural network. Further, we prove theoretically that our approach preserves the correctness and convergence guarantees provided by the existing iterative-solvers. We also demonstrate that our model generalizes to a different parameter setting than the one seen during training and achieves faster convergence compared to the semi-implicit schemes.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "1561461499",
                        "name": "Suprosanna Shit"
                    },
                    {
                        "authorId": "1411311831",
                        "name": "Abinav Ravi Venkatakrishnan"
                    },
                    {
                        "authorId": "2065232462",
                        "name": "I. Ezhov"
                    },
                    {
                        "authorId": "1959705",
                        "name": "Jana Lipkov\u00e1"
                    },
                    {
                        "authorId": "34929811",
                        "name": "M. Piraud"
                    },
                    {
                        "authorId": "143893221",
                        "name": "Bjoern H Menze"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dc8e717489d45aafe3be1a89695cabfecb321c81",
                "externalIds": {
                    "DOI": "10.1007/s11425-019-9547-2",
                    "CorpusId": 255157728
                },
                "corpusId": 255157728,
                "publicationVenue": {
                    "id": "4c9710db-06e6-4984-9779-8788f3458646",
                    "name": "Science China Mathematics",
                    "type": "journal",
                    "alternate_names": [
                        "Sci China-mathematics",
                        "Science China-mathematics",
                        "Sci China Math"
                    ],
                    "issn": "1869-1862"
                },
                "url": "https://www.semanticscholar.org/paper/dc8e717489d45aafe3be1a89695cabfecb321c81",
                "title": "MgNet: A unified framework of multigrid and convolutional neural network",
                "abstract": null,
                "year": 2019,
                "authors": [
                    {
                        "authorId": "27794145",
                        "name": "Juncai He"
                    },
                    {
                        "authorId": "40011783",
                        "name": "Jinchao Xu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Once the iterative methods are optimized on a set of problems, generalization to different boundary conditions, domain geometries, and other similar models, is explored and can be sometimes guaranteed [23].",
                "The last group, very different from previous two, adopts NN to optimize traditional iterative methods [14,23,24,35]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6f44170c92f7dd51ac929db9bafecc40649ac5ce",
                "externalIds": {
                    "ArXiv": "1905.02789",
                    "MAG": "3007415978",
                    "DBLP": "journals/jcphy/LiLM20",
                    "DOI": "10.1016/J.JCP.2020.109338",
                    "CorpusId": 147703935
                },
                "corpusId": 147703935,
                "publicationVenue": {
                    "id": "58606051-2e63-4034-8496-9d4ed773bb49",
                    "name": "Journal of Computational Physics",
                    "type": "journal",
                    "alternate_names": [
                        "J Comput Phys"
                    ],
                    "issn": "0021-9991",
                    "url": "http://www.elsevier.com/locate/jcp",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00219991",
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/622866/description#description",
                        "http://www.idealibrary.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6f44170c92f7dd51ac929db9bafecc40649ac5ce",
                "title": "Variational training of neural network approximations of solution maps for physical models",
                "abstract": null,
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2185132",
                        "name": "Yingzhou Li"
                    },
                    {
                        "authorId": "34254827",
                        "name": "Jianfeng Lu"
                    },
                    {
                        "authorId": "2064017789",
                        "name": "Anqi Mao"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "35aab48e1045740b1a8b3992e541f51f624130bc",
                "externalIds": {
                    "DBLP": "journals/actanum/ArridgeMOS19",
                    "MAG": "2952020389",
                    "DOI": "10.1017/S0962492919000059",
                    "CorpusId": 197480023
                },
                "corpusId": 197480023,
                "publicationVenue": {
                    "id": "086b0aef-7d17-4f72-8dc3-22252e8a5201",
                    "name": "Acta Numerica",
                    "type": "journal",
                    "alternate_names": [
                        "Acta Numer"
                    ],
                    "issn": "0962-4929",
                    "url": "https://www.cambridge.org/core/journals/acta-numerica/all-issues",
                    "alternate_urls": [
                        "http://journals.cambridge.org/action/displayJournal?jid=ANU"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/35aab48e1045740b1a8b3992e541f51f624130bc",
                "title": "Solving inverse problems using data-driven models",
                "abstract": "Recent research in inverse problems seeks to develop a mathematically coherent foundation for combining data-driven models, and in particular those based on deep learning, with domain-specific knowledge contained in physical\u2013analytical models. The focus is on solving ill-posed inverse problems that are at the core of many challenging applications in the natural sciences, medicine and life sciences, as well as in engineering and industrial applications. This survey paper aims to give an account of some of the main contributions in data-driven inverse problems.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "1693952",
                        "name": "S. Arridge"
                    },
                    {
                        "authorId": "35116679",
                        "name": "P. Maass"
                    },
                    {
                        "authorId": "46200292",
                        "name": "O. \u00d6ktem"
                    },
                    {
                        "authorId": "1711104",
                        "name": "C. Sch\u00f6nlieb"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                ", 1998), many suggested to design a network to solve specific PDEs (Hsieh et al., 2019; Baque et al., 2018; Baymani et al., 2010; Berg & Nystr\u00f6m, 2018; Han et al., 2017; 2018; Katrutsa et al., 2017; Mishra, 2018; Sirignano & Spiliopoulos, 2018; Sun et al., 2003; Tang et al., 2017; Wei et al., 2018), generalizing for different choices of right hand sides, boundary conditions, and in some cases to different domain shapes.",
                "Starting with the classical paper of (Lagaris et al., 1998), many suggested to design a network to solve specific PDEs (Hsieh et al., 2019; Baque et al., 2018; Baymani et al., 2010; Berg & Nystro\u0308m, 2018; Han et al., 2017; 2018; Katrutsa et al., 2017; Mishra, 2018; Sirignano & Spiliopoulos, 2018;\u2026",
                "(Hsieh et al., 2019) proposes an elegant learning based approach to accelerate existing iterative solvers, including multigrid solvers."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5089773a2986b1916814534b0919c8055791e283",
                "externalIds": {
                    "MAG": "2949276828",
                    "DBLP": "journals/corr/abs-1902-10248",
                    "ArXiv": "1902.10248",
                    "CorpusId": 67855720
                },
                "corpusId": 67855720,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5089773a2986b1916814534b0919c8055791e283",
                "title": "Learning to Optimize Multigrid PDE Solvers",
                "abstract": "Constructing fast numerical solvers for partial differential equations (PDEs) is crucial for many scientific disciplines. A leading technique for solving large-scale PDEs is using multigrid methods. At the core of a multigrid solver is the prolongation matrix, which relates between different scales of the problem. This matrix is strongly problem-dependent, and its optimal construction is critical to the efficiency of the solver. In practice, however, devising multigrid algorithms for new problems often poses formidable challenges. In this paper we propose a framework for learning multigrid solvers. Our method learns a (single) mapping from a family of parameterized PDEs to prolongation operators. We train a neural network once for the entire class of PDEs, using an efficient and unsupervised loss function. Experiments on a broad class of 2D diffusion problems demonstrate improved convergence rates compared to the widely used Black-Box multigrid scheme, suggesting that our method successfully learned rules for constructing prolongation matrices.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "4119758",
                        "name": "D. Greenfeld"
                    },
                    {
                        "authorId": "2991672",
                        "name": "M. Galun"
                    },
                    {
                        "authorId": "1760994",
                        "name": "R. Basri"
                    },
                    {
                        "authorId": "1699099",
                        "name": "I. Yavneh"
                    },
                    {
                        "authorId": "143923265",
                        "name": "R. Kimmel"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "ity pattern are trained by minimizing the Frobenius norm of a large power of the multigrid error propagation matrix with a sampling technique similar to that used in machine learning. In Hsieh et al. [16], a linear U-net structure is proposed as a solver for linear PDEs on regular mesh. In this paper, we explore the connection between multigrid and convolutional neural networks, in several directions."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b185dc4cfd1e26058132e5b16733cb75e01cc15e",
                "externalIds": {
                    "MAG": "2963259081",
                    "ArXiv": "1901.10415",
                    "DBLP": "journals/corr/abs-1901-10415",
                    "DOI": "10.1007/s11425-019-9547-2",
                    "CorpusId": 59336404
                },
                "corpusId": 59336404,
                "publicationVenue": {
                    "id": "4c9710db-06e6-4984-9779-8788f3458646",
                    "name": "Science China Mathematics",
                    "type": "journal",
                    "alternate_names": [
                        "Sci China-mathematics",
                        "Science China-mathematics",
                        "Sci China Math"
                    ],
                    "issn": "1869-1862"
                },
                "url": "https://www.semanticscholar.org/paper/b185dc4cfd1e26058132e5b16733cb75e01cc15e",
                "title": "MgNet: A unified framework of multigrid and convolutional neural network",
                "abstract": null,
                "year": 2019,
                "authors": [
                    {
                        "authorId": "27794145",
                        "name": "Juncai He"
                    },
                    {
                        "authorId": "40011783",
                        "name": "Jinchao Xu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[21] Hsieh J T, Zhao S, Eismann S, et al."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bea70b19876fb109712c0ab751c629aeb76e2f9a",
                "externalIds": {
                    "CorpusId": 248292525
                },
                "corpusId": 248292525,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bea70b19876fb109712c0ab751c629aeb76e2f9a",
                "title": "a uni\ufb01ed framework of multigrid and convolutional neural network.",
                "abstract": "Summary: We develop a uni\ufb01ed model, known as MgNet, that simultaneously recovers some convolutional neural networks (CNN) for image classi\ufb01cation and multigrid (MG) methods for solving discretized par-tial di\ufb00erential equations (PDEs). This model is based on close connections that we have observed and uncovered between the CNN and MG methodologies. For example, pooling operation and feature extrac-tion in CNN correspond directly to restriction operation and iterative smoothers in MG, respectively. As the solution space is often the dual of the data space in PDEs, the analogous concept of feature space and data space (which are dual to each other) is introduced in CNN. With such connections and new concept in the uni\ufb01ed model, the function of various convolution operations and pooling used in CNN can be better understood. As a result, modi\ufb01ed CNN models (with fewer weights and hyperparameters) are developed that exhibit competitive and sometimes better performance in comparison with existing CNN models when applied to both CIFAR-10 and CIFAR-100 data sets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2150637668",
                        "name": "Juncai Xu"
                    },
                    {
                        "authorId": "2163049797",
                        "name": "Jinchao MgNet"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c26e0ec430ba018d94179dc108450de31b15fc00",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-10763",
                    "DOI": "10.48550/arXiv.2205.10763",
                    "CorpusId": 248986426
                },
                "corpusId": 248986426,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c26e0ec430ba018d94179dc108450de31b15fc00",
                "title": "A Deep Gradient Correction Method for Iteratively Solving Linear Systems",
                "abstract": "We present a novel deep learning approach to approximate the solution of large, sparse, symmetric, positive-de\ufb01nite linear systems of equations. These systems arise from many problems in applied science, e.g., in numerical methods for partial differential equations. Algorithms for approximating the solution to these systems are often the bottleneck in problems that require their solution, particularly for modern applications that require many millions of unknowns. Indeed, numerical linear algebra techniques have been investigated for many decades to alleviate this computational burden. Recently, data-driven techniques have also shown promise for these problems. Motivated by the conjugate gradients algorithm that iteratively selects search directions for minimizing the matrix norm of the approximation error, we design an approach that utilizes a deep neural network to accelerate convergence via data-driven improvement of the search directions. Our method leverages a carefully chosen convolutional network to approximate the action of the inverse of the linear operator up to an arbitrary constant. We train the network using unsupervised learning with a loss function equal to the L 2 difference between an input and the system matrix times the network evaluation, where the unspeci\ufb01ed constant in the approximate inverse is accounted for. We demonstrate the ef\ufb01cacy of our approach on spatially discretized Poisson equations with millions of degrees of freedom arising in computational \ufb02uid dynamics applications. Unlike state-of-the-art learning approaches, our algorithm is capable of reducing the linear system",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2071409568",
                        "name": "Ayano Kaneda"
                    },
                    {
                        "authorId": "103541502",
                        "name": "Osman Akar"
                    },
                    {
                        "authorId": "2119701513",
                        "name": "Jingyu Chen"
                    },
                    {
                        "authorId": "2090396997",
                        "name": "Victoria Kala"
                    },
                    {
                        "authorId": "144618613",
                        "name": "David Hyde"
                    },
                    {
                        "authorId": "143667106",
                        "name": "J. Teran"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "We also note that other works where neural networks replace a whole or part of numerical methods [8, 9, 10, 11, 12, 14, 15, 24] can be organized in the same way.",
                "For example, [10, 11, 20, 21, 22, 23] propose to use predicted solutions by machine learning as initial guesses of subsequent traditional numerical methods, and [24] combines a neural network and an iterative solver to accelerate it while maintaining convergence guarantee."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b593d5e5ef29599cc7e859a92e0a46abb628a11f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-08594",
                    "DOI": "10.48550/arXiv.2206.08594",
                    "CorpusId": 249847823
                },
                "corpusId": 249847823,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b593d5e5ef29599cc7e859a92e0a46abb628a11f",
                "title": "Accelerating numerical methods by gradient-based meta-solving",
                "abstract": "In science and engineering applications, it is often required to solve similar computational problems repeatedly. In such cases, we can utilize the data from previously solved problem instances to improve the e\ufb03ciency of \ufb01nding subsequent solutions. This o\ufb00ers a unique opportunity to combine machine learning (in particular, meta-learning) and scienti\ufb01c computing. To date, a variety of such domain-speci\ufb01c methods have been proposed in the literature, but a generic approach for designing these methods remains under-explored. In this paper, we tackle this issue by formulating a general framework to describe these problems, and propose a gradient-based algorithm to solve them in a uni\ufb01ed way. As an illustration of this approach, we study the adaptive generation of parameters for iterative solvers to accelerate the solution of di\ufb00erential equations. We demonstrate the performance and versatility of our method through theoretical analysis and numerical experiments, including applications to incompressible \ufb02ow simulations and an inverse problem of parameter estimation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "92120993",
                        "name": "S. Arisaka"
                    },
                    {
                        "authorId": "1861517",
                        "name": "Qianxiao Li"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5dcb27d50176d1b4e941f6690f57fd3085c20758",
                "externalIds": {
                    "CorpusId": 259977962
                },
                "corpusId": 259977962,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5dcb27d50176d1b4e941f6690f57fd3085c20758",
                "title": "Applications of machine learning to finite volume methods",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145641702",
                        "name": "B. Stevens"
                    },
                    {
                        "authorId": "1740159",
                        "name": "Yisong Yue"
                    },
                    {
                        "authorId": "2894077",
                        "name": "G. Blanquart"
                    },
                    {
                        "authorId": "101508894",
                        "name": "Ethan Pickering"
                    },
                    {
                        "authorId": "2223940930",
                        "name": "Marcus Lee"
                    },
                    {
                        "authorId": "2223964168",
                        "name": "Omar Kamal"
                    },
                    {
                        "authorId": "2061634086",
                        "name": "W. Hou"
                    },
                    {
                        "authorId": "102026890",
                        "name": "J. R. Chreim"
                    },
                    {
                        "authorId": "2528791",
                        "name": "L. Schneiders"
                    },
                    {
                        "authorId": "2223910490",
                        "name": "Mauro Rodriguez"
                    },
                    {
                        "authorId": "47897692",
                        "name": "Shunxiang Cao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "The Poisson equation serves as an example problem in the recent literature; see [10, 26, 28].",
                "[10] Jun-Ting Hsieh, Shengjia Zhao, Stephan Eismann, Lucia Mirabella, and Stefano Ermon, Learning neural PDE solvers with convergence guarantees, arXiv preprint arXiv:1906.",
                "A fast, iterative PDE-solver was proposed by learning to modify each iteration of the existing solver [10]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a28003959f23b0a0cafc6b0d89f63b067239867f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2101-08932",
                    "CorpusId": 231693215
                },
                "corpusId": 231693215,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a28003959f23b0a0cafc6b0d89f63b067239867f",
                "title": "Sobolev Training for the Neural Network Solutions of PDEs",
                "abstract": "Approximating the numerical solutions of Partial Differential Equ ations (PDEs) using neural networks is a promising application of deep learning. The smooth architecture of a fully connected neural network is appropriate for finding the solutions of PDEs; the corresponding loss function can also be intuitively designed and guarantees the convergence for various kinds of PDEs. However, the rate of convergence has been considered as a weakness of this approach. This paper introduces a novel loss function for the training of neural networks to find the solutions of PDEs, making the training substantially efficient. Inspired by the recent studies that incorporate derivative information for the training of neural networks, we develop a loss function that guides a neural network to reduce the error in the corresponding Sobolev space. Surprisingly, a simple modification of the loss function can make the training process similar to Sobolev Training although solving PDEs with neural networks is not a fully supervised learning task. We provide several theoretical justifications for such an approach for the viscous Burgers equation and the kinetic Fokker\u2013Planck equation. We also present several simulation results, which show that compared with the traditional L2 loss function, the proposed loss function guides the neural network to a significantly faster convergence. Moreover, we provide the empirical evidence that shows that the proposed loss function, together with the iterative sampling techniques, performs better in solving high dimensional PDEs.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "151372987",
                        "name": "Hwijae Son"
                    },
                    {
                        "authorId": "2109878158",
                        "name": "Jin Woo Jang"
                    },
                    {
                        "authorId": "2114924549",
                        "name": "W. Han"
                    },
                    {
                        "authorId": "32013090",
                        "name": "H. Hwang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In a recent Stanford study, the authors solved the 2D Poisson equation using a deep neural network and had 2-3 times speedup in comparison to standard iterative solvers [7]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c64cebc0396d8178c69e9246ade9595048fc705b",
                "externalIds": {
                    "CorpusId": 235356245
                },
                "corpusId": 235356245,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c64cebc0396d8178c69e9246ade9595048fc705b",
                "title": "Estimating the Heat Equation using Neural Networks Estimating the Heat Equation using Neural Networks",
                "abstract": "Partial Di\ufb00erential Equations have important uses in many \ufb01elds including physics and engineering. Due to their importance, heavy research has been done to solve these problems e\ufb03ciently and ef-fectively. However, some PDEs are still challenging to solve using classical methods, often due to the dimensionality of the problem. In recent years, it has been thought that neural networks may be able to solve these problems e\ufb00ectively. This research assesses how well a neural network can estimate a simple example PDE, the heat equation, as well as the practicality of doing so.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2107049349",
                        "name": "Nathan Jordre"
                    },
                    {
                        "authorId": "2135545",
                        "name": "M. Heroux"
                    },
                    {
                        "authorId": "2074582980",
                        "name": "J. Iverson"
                    },
                    {
                        "authorId": "34379237",
                        "name": "R. Hesse"
                    },
                    {
                        "authorId": "70475738",
                        "name": "Noreen L. Herzfeld"
                    },
                    {
                        "authorId": "2107049347",
                        "name": "Lindsey Gunnerson Gutsch"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "db642398a2fca4078e6850e383e99fe8ddd61e9f",
                "externalIds": {
                    "DBLP": "journals/vrih/ChenWWY21",
                    "DOI": "10.1016/j.vrih.2021.02.002",
                    "CorpusId": 234795682
                },
                "corpusId": 234795682,
                "publicationVenue": {
                    "id": "cb022999-2b24-4960-84ae-0b209caeaa5d",
                    "name": "Virtual Reality & Intelligent Hardware",
                    "type": "journal",
                    "alternate_names": [
                        "Virtual Real  Intell Hardw"
                    ],
                    "issn": "2096-5796",
                    "url": "http://www.keaipublishing.com/en/journals/virtual-reality-and-intelligent-hardware/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/virtual-reality-and-intelligent-hardware"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/db642398a2fca4078e6850e383e99fe8ddd61e9f",
                "title": "Data-driven simulation in fluids animation: A survey",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2146379609",
                        "name": "Qian Chen"
                    },
                    {
                        "authorId": "2144334663",
                        "name": "Yue Wang"
                    },
                    {
                        "authorId": "2155656788",
                        "name": "Hui Wang"
                    },
                    {
                        "authorId": "2143675390",
                        "name": "Xubo Yang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "There are roughly three distinct directions emerging in the literature: (1) discovery of governing equations directly from data [3, 20, 12]; (2) development of augmented numerical methods [2, 21, 7, 15, 18]; (3) surrogate modeling [4, 19, 1, 14, 13, 5]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a9d015f6f2d429a5cc29df33dafc3d25c7a9c248",
                "externalIds": {
                    "CorpusId": 231772994
                },
                "corpusId": 231772994,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a9d015f6f2d429a5cc29df33dafc3d25c7a9c248",
                "title": "Learning latent field dynamics of PDEs",
                "abstract": "We present a new approach to learning surrogate models for simulation of complex physical systems described by nonlinear partial differential equations. It aims to capture three features of PDEs: locality, time continuity and formation of elementary patterns in the solution by learning a local, low dimensional latent representation and corresponding time evolution. We show that our method achieves top performance and competitive inference speed compared to baseline methods while operating with a 4-times more compact representation. Since the models learn local representations of the solution, they generalize to different system sizes that feature qualitatively different behavior without retraining.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "40833997",
                        "name": "Dmitrii Kochkov"
                    },
                    {
                        "authorId": "1398105826",
                        "name": "Alvaro Sanchez-Gonzalez"
                    },
                    {
                        "authorId": "2119124568",
                        "name": "Jamie A. Smith"
                    },
                    {
                        "authorId": "2054956",
                        "name": "T. Pfaff"
                    },
                    {
                        "authorId": "2019153",
                        "name": "P. Battaglia"
                    },
                    {
                        "authorId": "36397553",
                        "name": "M. Brenner"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "25738b68efb631af2f80e49a86e6436f61e8c126",
                "externalIds": {
                    "DOI": "10.1016/b978-0-12-815651-3.09999-5",
                    "CorpusId": 240896774
                },
                "corpusId": 240896774,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/25738b68efb631af2f80e49a86e6436f61e8c126",
                "title": "Introduction",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2092643843",
                        "name": "D. Tarkhov"
                    },
                    {
                        "authorId": "2139628012",
                        "name": "A. Vasilyev"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "contexts": [
                "The original paper proposes to use machine learning techniques in order to find high performing update rules instead of designing them by hand [1], while still guaranteeing convergence.",
                "A common approach to build T and c is to splitA intoA = M \u2212N and by rewritingAu = f asMu = Nu+f the following updating rule naturally arises: u = M\u22121Nuk +M\u22121f For more details we refer readers to [4] or to [1].",
                "For more information we kindly refer to the original paper [1].",
                "With k \u2208 DU(1, 20) we denote the sampling of k from a discrete uniform distribution on the interval [1, 20].",
                "This is the reason why our work was focused on reproducing the results of [1], and on empirically proving the generalization of their model to arbitrary shapes and grid sizes."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "6699bff47dbf5e2fd4fdbc3f87d04bfe99d411d2",
                "externalIds": {
                    "CorpusId": 250493584
                },
                "corpusId": 250493584,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6699bff47dbf5e2fd4fdbc3f87d04bfe99d411d2",
                "title": "[Re] Learning Neural PDE Solvers with Convergence Guarantees ICLR Reproducibility Challenge 2019",
                "abstract": "Partial differential equations (PDEs) are differential equationswhich contain a-priori unkown multivariable functions and their partial derivatives. They are used to model various physical phenomena, such as heat, fluid dynamics or quantum mechanics. There are several numerical methods to solve PDEs. A common one is the finite-difference method (FDM), which approaches the differential equation by discretizing the problem space and converting the PDE to a system of linear equations. The obtained linear system can be solved using an iterative procedure which updates the solution until convergence is reached. The original paper proposes to use machine learning techniques in order to find high performing update rules instead of designing them by hand [1], while still guaranteeing convergence. In order to fulfill these requirements the learned solver is an adapted existing standard solver, from which the convergence property is inherited by enforcing that a fixed point of the original solver is a fixed point for the trained solver as well. We stress that the goal is not to find a new solver, but to optimize an existing one. To be precise the learned part operates with the residuals after applying the standard solver. This construction allows application to other existing linear iterative solvers of equivalent design. Since a linear iterative solver can be expressed as a product of convolutional operations, it is not far fetched to use the similar techniques used in deep learning in order to find such an optimal operator. In order to test this approach a solver was trained to solve a 2D Poisson equation on a square-shaped geometry with Dirichlet boundary conditions. This solver is then tested on larger geometries of two shapes and different boundary values. No significant loss of performance was observed; generalization is thus reached. For more information we kindly refer to the original paper [1].",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "48436810",
                        "name": "Francesco Bardi"
                    },
                    {
                        "authorId": "2106415166",
                        "name": "Samuel von Baussnern"
                    },
                    {
                        "authorId": "2114825242",
                        "name": "Emiljano Gjiriti"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4b89571617a3459ff2c5f03635827cb086f305d2",
                "externalIds": {
                    "CorpusId": 209336748
                },
                "corpusId": 209336748,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4b89571617a3459ff2c5f03635827cb086f305d2",
                "title": "Generative Models for Solving Nonlinear Partial Differential Equations",
                "abstract": "Partial differential equations (PDEs) describe a wide variety of physical systems. While there exist several numerical methods to solve PDEs, they are often computationally expensive, and solutions to varying boundary conditions and forcing functions need to be derived from scratch. We present a conditional generative modeling based approach to solve families of PDEs parameterized by a distribution of boundary conditions and coefficients. We validate our approach by solving a family of nonlinear PDEs: the Burgers\u2019 equation with a single trained model. We also compare with other neural network based solvers as well as standard numerical solvers and demonstrate comparable accuracy while being computationally more efficient.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "144931667",
                        "name": "Ameya Joshi"
                    },
                    {
                        "authorId": "47330662",
                        "name": "Viraj Shah"
                    },
                    {
                        "authorId": "34892652",
                        "name": "Sambuddha Ghosal"
                    },
                    {
                        "authorId": "51432607",
                        "name": "B. Pokuri"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5e1f2eafd9e1fd581c65b5f29e99ff39637ebd6b",
                "externalIds": {
                    "CorpusId": 225074409
                },
                "corpusId": 225074409,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5e1f2eafd9e1fd581c65b5f29e99ff39637ebd6b",
                "title": "Solving Partial Differential Equations with Machine Learning",
                "abstract": null,
                "year": 2019,
                "authors": [
                    {
                        "authorId": "116737767",
                        "name": "L. Sammler"
                    },
                    {
                        "authorId": "2079820203",
                        "name": "Bachelorarbeit"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8f71a75e3f47c3bf72938d12becff36b7132ff36",
                "externalIds": {
                    "CorpusId": 218538428
                },
                "corpusId": 218538428,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8f71a75e3f47c3bf72938d12becff36b7132ff36",
                "title": "SMAI-JCM SMAI Journal of Computational Mathematics Model Reduction And Neural Networks For Parametric PDEs",
                "abstract": ". We develop a general framework for data-driven approximation of input-output maps between in\ufb01nite-dimensional spaces. The proposed approach is motivated by the recent successes of neural networks and deep learning, in combination with ideas from model reduction. This combination results in a neural network approximation which, in principle, is de\ufb01ned on in\ufb01nite-dimensional spaces and, in practice, is robust to the dimension of \ufb01nite-dimensional approximations of these spaces required for computation. For a class of input-output maps, and suitably chosen probability measures on the inputs, we prove convergence of the proposed approximation methodology. We also include numerical experiments which demonstrate the e\ufb00ectiveness of the method, showing convergence and robustness of the approximation scheme with respect to the size of the discretization, and compare it with existing algorithms from the literature; our examples include the mapping from coe\ufb03cient to solution in a divergence form elliptic partial di\ufb00erential equation (PDE) problem, and the solution operator for viscous Burgers\u2019 equation. 2020 Mathematics Subject Classi\ufb01cation. 65N75, 62M45, 68T05, 60H30, 60H15. approximate",
                "year": null,
                "authors": [
                    {
                        "authorId": "144304264",
                        "name": "K. Bhattacharya"
                    },
                    {
                        "authorId": "32067643",
                        "name": "Bamdad Hosseini"
                    },
                    {
                        "authorId": "51219644",
                        "name": "Nikola B. Kovachki"
                    },
                    {
                        "authorId": "4662911",
                        "name": "A. Stuart"
                    }
                ]
            }
        }
    ]
}