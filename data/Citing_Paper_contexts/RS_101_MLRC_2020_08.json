{
    "offset": 0,
    "data": [
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "When the context classes are known, another approach is to make sure that the learned attention maps for each class do not overlap [27].",
                "With the emergence of deep learning approaches, which are capable of automatically extracting features without leveraging user knowledge, the extraction of spurious correlations induced by contextual bias has started to be perceived as a potential issue [13, 27, 29]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dd5092e10f691db2ddcde2e931c4468ee6bab08d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-12127",
                    "ArXiv": "2308.12127",
                    "DOI": "10.48550/arXiv.2308.12127",
                    "CorpusId": 261076323
                },
                "corpusId": 261076323,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/dd5092e10f691db2ddcde2e931c4468ee6bab08d",
                "title": "Masking Strategies for Background Bias Removal in Computer Vision Models",
                "abstract": "Models for fine-grained image classification tasks, where the difference between some classes can be extremely subtle and the number of samples per class tends to be low, are particularly prone to picking up background-related biases and demand robust methods to handle potential examples with out-of-distribution (OOD) backgrounds. To gain deeper insights into this critical problem, our research investigates the impact of background-induced bias on fine-grained image classification, evaluating standard backbone models such as Convolutional Neural Network (CNN) and Vision Transformers (ViT). We explore two masking strategies to mitigate background-induced bias: Early masking, which removes background information at the (input) image level, and late masking, which selectively masks high-level spatial features corresponding to the background. Extensive experiments assess the behavior of CNN and ViT models under different masking strategies, with a focus on their generalization to OOD backgrounds. The obtained findings demonstrate that both proposed strategies enhance OOD performance compared to the baseline models, with early masking consistently exhibiting the best OOD performance. Notably, a ViT variant employing GAP-Pooled Patch token-based classification combined with early masking achieves the highest OOD robustness.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2233087669",
                        "name": "Ananthu Aniraj"
                    },
                    {
                        "authorId": "10373561",
                        "name": "C. Dantas"
                    },
                    {
                        "authorId": "1789397",
                        "name": "D. Ienco"
                    },
                    {
                        "authorId": "144173388",
                        "name": "Diego Marcos"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2590d71ea093634db00c33b350fd22e41fa92ab8",
                "externalIds": {
                    "ArXiv": "2308.04553",
                    "CorpusId": 260735806
                },
                "corpusId": 260735806,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2590d71ea093634db00c33b350fd22e41fa92ab8",
                "title": "From Fake to Real: Pretraining on Balanced Synthetic Images to Prevent Bias",
                "abstract": "Visual recognition models are prone to learning spurious correlations induced by a biased training set where certain conditions $B$ (\\eg, Indoors) are over-represented in certain classes $Y$ (\\eg, Big Dogs). Synthetic data from generative models offers a promising direction to mitigate this issue by augmenting underrepresented conditions in the real dataset. However, this introduces another potential source of bias from generative model artifacts in the synthetic data. Indeed, as we will show, prior work uses synthetic data to resolve the model's bias toward $B$, but it doesn't correct the models' bias toward the pair $(B, G)$ where $G$ denotes whether the sample is real or synthetic. Thus, the model could simply learn signals based on the pair $(B, G)$ (\\eg, Synthetic Indoors) to make predictions about $Y$ (\\eg, Big Dogs). To address this issue, we propose a two-step training pipeline that we call From Fake to Real (FFR). The first step of FFR pre-trains a model on balanced synthetic data to learn robust representations across subgroups. In the second step, FFR fine-tunes the model on real data using ERM or common loss-based bias mitigation methods. By training on real and synthetic data separately, FFR avoids the issue of bias toward signals from the pair $(B, G)$. In other words, synthetic data in the first step provides effective unbiased representations that boosts performance in the second step. Indeed, our analysis of high bias setting (99.9\\%) shows that FFR improves performance over the state-of-the-art by 7-14\\% over three datasets (CelebA, UTK-Face, and SpuCO Animals).",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "92749945",
                        "name": "Maan Qraitem"
                    },
                    {
                        "authorId": "2903226",
                        "name": "Kate Saenko"
                    },
                    {
                        "authorId": "2856622",
                        "name": "Bryan A. Plummer"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "In another line of research [10, 32, 4, 9, 26, 29, 23, 33], the authors assume knowledge of the underlying data collection algorithm and provide asymptotically valid confidence intervals."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b70ebd616caf3a1bde711a902b9952c9ea26fb72",
                "externalIds": {
                    "ArXiv": "2307.07320",
                    "DBLP": "journals/corr/abs-2307-07320",
                    "DOI": "10.48550/arXiv.2307.07320",
                    "CorpusId": 259924867
                },
                "corpusId": 259924867,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b70ebd616caf3a1bde711a902b9952c9ea26fb72",
                "title": "Adaptive Linear Estimating Equations",
                "abstract": "Sequential data collection has emerged as a widely adopted technique for enhancing the efficiency of data gathering processes. Despite its advantages, such data collection mechanism often introduces complexities to the statistical inference procedure. For instance, the ordinary least squares (OLS) estimator in an adaptive linear regression model can exhibit non-normal asymptotic behavior, posing challenges for accurate inference and interpretation. In this paper, we propose a general method for constructing debiased estimator which remedies this issue. It makes use of the idea of adaptive linear estimating equations, and we establish theoretical guarantees of asymptotic normality, supplemented by discussions on achieving near-optimal asymptotic variance. A salient feature of our estimator is that in the context of multi-armed bandits, our estimator retains the non-asymptotic performance of the least square estimator while obtaining asymptotic normality property. Consequently, this work helps connect two fruitful paradigms of adaptive inference: a) non-asymptotic inference using concentration inequalities and b) asymptotic inference via asymptotic normality.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1752781323",
                        "name": "Mufang Ying"
                    },
                    {
                        "authorId": "2923207",
                        "name": "K. Khamaru"
                    },
                    {
                        "authorId": "153884534",
                        "name": "Cun-Hui Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "As such, this phenomenon of \u201ccontextual bias\u201d refers to the case where a model\u2019s attention is shifting to contextual objects which are not directly relevant to the model\u2019s goal [83].",
                "Using a biased dataset can induce a model to reference contextual objects in prediction, which is defined to be unfair [83].",
                "Consequently, using this potential vulnerability, an attacker may be able to drastically decrease model accuracy by showing the ball images without dogs [83].",
                "When CNNs are not trained properly with generalized and representative datasets, there can be various kinds of bias that can introduce several weaknesses in the model performance [29, 83].",
                "used Class Activation Maps as a \u201cweak\u201d automatic attention annotation [83].",
                "While contextual bias has become a highly crucial issue in ML and beyond [29, 40, 56, 75, 83, 104], spotting the vulnerability and steering the model is highly challenging or not even feasible [39] even for experienced ML engineers [43].",
                "In handling contextual bias, several studies outside of HCI commonly apply mathematical approaches rather than incorporating human input [75, 83].",
                "In such a case, the model\u2019s attention visualized through local explanation is on the ball rather than a dog [83]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "e623fad3e54e9e049436db4e7b59b87d1f85181e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-04036",
                    "ArXiv": "2307.04036",
                    "DOI": "10.1145/3610187",
                    "CorpusId": 259501077
                },
                "corpusId": 259501077,
                "publicationVenue": {
                    "id": "425553d6-e479-478a-8dd7-ae59d3f32b72",
                    "name": "Proceedings of the ACM on Human-Computer Interaction",
                    "alternate_names": [
                        "Proc ACM Human-computer Interact"
                    ],
                    "issn": "2573-0142",
                    "url": "https://dl.acm.org/citation.cfm?id=3120954",
                    "alternate_urls": [
                        "https://dl.acm.org/citation.cfm?id=J1598&picked=prox"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e623fad3e54e9e049436db4e7b59b87d1f85181e",
                "title": "Designing a Direct Feedback Loop between Humans and Convolutional Neural Networks through Local Explanations",
                "abstract": "The local explanation provides heatmaps on images to explain how Convolutional Neural Networks (CNNs) derive their output. Due to its visual straightforwardness, the method has been one of the most popular explainable AI (XAI) methods for diagnosing CNNs. Through our formative study (S1), however, we captured ML engineers' ambivalent perspective about the local explanation as a valuable and indispensable envision in building CNNs versus the process that exhausts them due to the heuristic nature of detecting vulnerability. Moreover, steering the CNNs based on the vulnerability learned from the diagnosis seemed highly challenging. To mitigate the gap, we designed DeepFuse, the first interactive design that realizes the direct feedback loop between a user and CNNs in diagnosing and revising CNN's vulnerability using local explanations. DeepFuse helps CNN engineers to systemically search \"unreasonable\" local explanations and annotate the new boundaries for those identified as unreasonable in a labor-efficient manner. Next, it steers the model based on the given annotation such that the model doesn't introduce similar mistakes. We conducted a two-day study (S2) with 12 experienced CNN engineers. Using DeepFuse, participants made a more accurate and \"reasonable\" model than the current state-of-the-art. Also, participants found the way DeepFuse guides case-based reasoning can practically improve their current practice. We provide implications for design that explain how future HCI-driven design can move our practice forward to make XAI-driven insights more actionable.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2113204748",
                        "name": "Tong Sun"
                    },
                    {
                        "authorId": "2110674789",
                        "name": "Yuyang Gao"
                    },
                    {
                        "authorId": "2221572233",
                        "name": "Shubham Khaladkar"
                    },
                    {
                        "authorId": "143743061",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "144000223",
                        "name": "Liang Zhao"
                    },
                    {
                        "authorId": "2108359250",
                        "name": "Younghoon Kim"
                    },
                    {
                        "authorId": "2151797416",
                        "name": "S. Hong"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "89998ef0f3e756993bda255ba00941fe30ced2c9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-01473",
                    "ArXiv": "2307.01473",
                    "DOI": "10.48550/arXiv.2307.01473",
                    "CorpusId": 259342225
                },
                "corpusId": 259342225,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/89998ef0f3e756993bda255ba00941fe30ced2c9",
                "title": "Mitigating Bias: Enhancing Image Classification by Improving Model Explanations",
                "abstract": "Deep learning models have demonstrated remarkable capabilities in learning complex patterns and concepts from training data. However, recent findings indicate that these models tend to rely heavily on simple and easily discernible features present in the background of images rather than the main concepts or objects they are intended to classify. This phenomenon poses a challenge to image classifiers as the crucial elements of interest in images may be overshadowed. In this paper, we propose a novel approach to address this issue and improve the learning of main concepts by image classifiers. Our central idea revolves around concurrently guiding the model's attention toward the foreground during the classification task. By emphasizing the foreground, which encapsulates the primary objects of interest, we aim to shift the focus of the model away from the dominant influence of the background. To accomplish this, we introduce a mechanism that encourages the model to allocate sufficient attention to the foreground. We investigate various strategies, including modifying the loss function or incorporating additional architectural components, to enable the classifier to effectively capture the primary concept within an image. Additionally, we explore the impact of different foreground attention mechanisms on model performance and provide insights into their effectiveness. Through extensive experimentation on benchmark datasets, we demonstrate the efficacy of our proposed approach in improving the classification accuracy of image classifiers. Our findings highlight the importance of foreground attention in enhancing model understanding and representation of the main concepts within images. The results of this study contribute to advancing the field of image classification and provide valuable insights for developing more robust and accurate deep-learning models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2221125797",
                        "name": "Raha Ahmadi"
                    },
                    {
                        "authorId": "144895302",
                        "name": "Mohammad Javad Rajabi"
                    },
                    {
                        "authorId": "2221573866",
                        "name": "Mohammad Khalooiem"
                    },
                    {
                        "authorId": "2884918",
                        "name": "M. Sabokrou"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Singh et al.15 devised a unique approach to address the contextual bias that exists in image classifiers.",
                "Singh et al.(15) devised a unique approach to address the contextual bias that exists in image classifiers.",
                "Contextual bias occurs when such patterns are falsely associated with certain characteristics of the image, such as geography, race, and gender.(15)",
                "Strongly relying on context can hurt the accuracy and generalizability of the model when the co-occurrence patterns are absent.(15) Although it enables the algorithm to predict objects more accurately, it introduces false positives and false negatives in the prediction in the absence of a related object."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "fd2de6758b14bcc422bc403e6351381961fab536",
                "externalIds": {
                    "DOI": "10.1117/12.2665725",
                    "CorpusId": 258392941
                },
                "corpusId": 258392941,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fd2de6758b14bcc422bc403e6351381961fab536",
                "title": "On the performance of machine learning fairness in image classification",
                "abstract": "In recent years, computer vision has made significant strides in enabling machines to perform a wide range of tasks, from image classification and segmentation to image generation and video analysis. It is a rapidly evolving field that aims to enable machines to interpret and understand visual information from the environment. One key task in computer vision is image classification, where algorithms identify and categorize objects in images based on their visual features. Image classification has a wide range of applications, from image search and recommendation systems to autonomous driving and medical diagnosis. However, recent research has highlighted the presence of bias in image classification algorithms, particularly with respect to human-sensitive attributes such as gender, race, and ethnicity. Some examples are computer programmers being predicted better in the context of men in images compared to women, and the accuracy of the algorithm being better on greyscale images compared to colored images. This discrepancy in identifying objects is developed through correlation the algorithm learns from the objects in context known as contextual bias. This bias can result in inaccurate decisions, with potential consequences in areas such as hiring, healthcare, and security. In this paper, we conduct an empirical study to investigate bias in the image classification domain based on sensitive attribute gender using deep convolutional neural networks (CNN) through transfer learning and minimize bias within the image context using data augmentation to improve overall model performance. In addition, cross-data generalization experiments are conducted to evaluate model robustness across popular open-source image datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1712241756",
                        "name": "Utsab Khakurel"
                    },
                    {
                        "authorId": "1729159",
                        "name": "D. Rawat"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "39eb1a120517c2a798dffec1c5575b9ff9846073",
                "externalIds": {
                    "ArXiv": "2305.17763",
                    "DBLP": "conf/cvpr/MinZSLDC23",
                    "DOI": "10.1109/CVPR52729.2023.02050",
                    "CorpusId": 258960477
                },
                "corpusId": 258960477,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/39eb1a120517c2a798dffec1c5575b9ff9846073",
                "title": "NeurOCS: Neural NOCS Supervision for Monocular 3D Object Localization",
                "abstract": "Monocular 3D object localization in driving scenes is a crucial task, but challenging due to its ill-posed nature. Estimating 3D coordinates for each pixel on the object surface holds great potential as it provides dense 2D-3D geometric constraints for the underlying PnP problem. However, high-quality ground truth supervision is not available in driving scenes due to sparsity and various artifacts of Lidar data, as well as the practical infeasibility of collecting per-instance CAD models. In this work, we present NeurOCS, a framework that uses instance masks and 3D boxes as input to learn 3D object shapes by means of differentiable rendering, which further serves as supervision for learning dense object coordinates. Our approach rests on insights in learning a category-level shape prior directly from real driving scenes, while properly handling single-view ambiguities. Furthermore, we study and make critical design choices to learn object coordinates more effectively from an object-centric view. Altogether, our framework leads to new state-of-the-art in monocular 3D localization that ranks 1st on the KITTI-Object [16] benchmark among published monocular methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1865761881",
                        "name": "Zhixiang Min"
                    },
                    {
                        "authorId": "2064099052",
                        "name": "Bingbing Zhuang"
                    },
                    {
                        "authorId": "1790643",
                        "name": "S. Schulter"
                    },
                    {
                        "authorId": "3248862",
                        "name": "Buyu Liu"
                    },
                    {
                        "authorId": "144678099",
                        "name": "Enrique Dunn"
                    },
                    {
                        "authorId": "2099305",
                        "name": "Manmohan Chandraker"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "tigating what is depicted in the most salient region of an image) (Hendricks et al. 2018; Jia, Lansdall-Welfare, and Cristianini 2018; Muthukumar et al. 2018; Singh et al. 2020)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "56dc4105b113a48405604bb63e4ee42080e50bdd",
                "externalIds": {
                    "ArXiv": "2305.01776",
                    "DBLP": "conf/aaai/KatzmanWSBLWB23",
                    "DOI": "10.48550/arXiv.2305.01776",
                    "CorpusId": 258461356
                },
                "corpusId": 258461356,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/56dc4105b113a48405604bb63e4ee42080e50bdd",
                "title": "Taxonomizing and Measuring Representational Harms: A Look at Image Tagging",
                "abstract": "In this paper, we examine computational approaches for measuring the \"fairness\" of image tagging systems, finding that they cluster into five distinct categories, each with its own analytic foundation. We also identify a range of normative concerns that are often collapsed under the terms \"unfairness,\" \"bias,\" or even \"discrimination\" when discussing problematic cases of image tagging. Specifically, we identify four types of representational harms that can be caused by image tagging systems, providing concrete examples of each. We then consider how different computational measurement approaches map to each of these types, demonstrating that there is not a one-to-one mapping. Our findings emphasize that no single measurement approach will be definitive and that it is not possible to infer from the use of a particular measurement approach which type of harm was intended to be measured. Lastly, equipped with this more granular understanding of the types of representational harms that can be caused by image tagging systems, we show that attempts to mitigate some of these types of harms may be in tension with one another.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "32074066",
                        "name": "Jared Katzman"
                    },
                    {
                        "authorId": "46991154",
                        "name": "Angelina Wang"
                    },
                    {
                        "authorId": "10691918",
                        "name": "M. Scheuerman"
                    },
                    {
                        "authorId": "3422038",
                        "name": "Su Lin Blodgett"
                    },
                    {
                        "authorId": "2170359427",
                        "name": "Kristen Laird"
                    },
                    {
                        "authorId": "2058607401",
                        "name": "Hanna M. Wallach"
                    },
                    {
                        "authorId": "2881033",
                        "name": "Solon Barocas"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Deep learning models are generally sensitive to object contexts and backgrounds, and learn spurious correlations that impede their ability to recognize objects and scenes in novel contexts [19, 20, 62, 75]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fa91157b042799b019cdb2b2e39ea5676fbd4292",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-15443",
                    "ArXiv": "2303.15443",
                    "DOI": "10.1109/CVPR52729.2023.01475",
                    "CorpusId": 257766329
                },
                "corpusId": 257766329,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fa91157b042799b019cdb2b2e39ea5676fbd4292",
                "title": "GeoNet: Benchmarking Unsupervised Adaptation across Geographies",
                "abstract": "In recent years, several efforts have been aimed at improving the robustness of vision models to domains and environments unseen during training. An important practical problem pertains to models deployed in a new geography that is under-represented in the training dataset, posing a direct challenge to fair and inclusive computer vision. In this paper, we study the problem of geographic robustness and make three main contributions. First, we introduce a large-scale dataset GeoNet for geographic adaptation containing benchmarks across diverse tasks like scene recognition (GeoPlaces), image classification (GeoImNet) and universal adaptation (GeoUniDA). Second, we investigate the nature of distribution shifts typical to the problem of geographic adaptation and hypothesize that the major source of domain shifts arise from significant variations in scene context (context shift), object design (design shift) and label distribution (prior shift) across geographies. Third, we conduct an extensive evaluation of several state-of-the-art unsupervised domain adaptation algorithms and architectures on GeoNet, showing that they do not suffice for geographical adaptation, and that large-scale pre-training using large vision models also does not lead to geographic robustness. Our dataset is publicly available at https://tarun005.github.io/GeoNet.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2655351",
                        "name": "Tarun Kalluri"
                    },
                    {
                        "authorId": "2110726104",
                        "name": "Wangdong Xu"
                    },
                    {
                        "authorId": "2099305",
                        "name": "Manmohan Chandraker"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "These include enforcing consistency against augmentations [38, 37, 20], smoothness [11, 31, 26], separation of classes [64, 37, 54, 32, 52], or constraining the model\u2019s attention [17, 3].",
                "2 [52] Krishna Kumar Singh, Dhruv Mahajan, Kristen Grauman, Yong Jae Lee, Matt Feiszli, and Deepti Ghadiyaram."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "86a238268b0a993300a11e75f039f274c6fc172e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-11932",
                    "ArXiv": "2303.11932",
                    "DOI": "10.48550/arXiv.2303.11932",
                    "CorpusId": 257636624
                },
                "corpusId": 257636624,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/86a238268b0a993300a11e75f039f274c6fc172e",
                "title": "Using Explanations to Guide Models",
                "abstract": "Deep neural networks are highly performant, but might base their decision on spurious or background features that co-occur with certain classes, which can hurt generalization. To mitigate this issue, the usage of 'model guidance' has gained popularity recently: for this, models are guided to be\"right for the right reasons\"by regularizing the models' explanations to highlight the right features. Experimental validation of these approaches has thus far however been limited to relatively simple and / or synthetic datasets. To gain a better understanding of which model-guiding approaches actually transfer to more challenging real-world datasets, in this work we conduct an in-depth evaluation across various loss functions, attribution methods, models, and 'guidance depths' on the PASCAL VOC 2007 and MS COCO 2014 datasets, and show that model guidance can sometimes even improve model performance. In this context, we further propose a novel energy loss, show its effectiveness in directing the model to focus on object features. We also show that these gains can be achieved even with a small fraction (e.g. 1%) of bounding box annotations, highlighting the cost effectiveness of this approach. Lastly, we show that this approach can also improve generalization under distribution shifts. Code will be made available.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "40897453",
                        "name": "Sukrut Rao"
                    },
                    {
                        "authorId": "35698126",
                        "name": "Moritz D Boehle"
                    },
                    {
                        "authorId": "2212369801",
                        "name": "Amin Parchami-Araghi"
                    },
                    {
                        "authorId": "48920094",
                        "name": "B. Schiele"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Prior work has shown that models exploit co-occurrences between an object and its context which helps overall recognition accuracy, but can hurt performance when that context is absent (Singh et al. 2020)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e6a2f67b6ddce877af5f73a4d47e2c18db2ed790",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-09608",
                    "ArXiv": "2303.09608",
                    "DOI": "10.48550/arXiv.2303.09608",
                    "CorpusId": 257622967
                },
                "corpusId": 257622967,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e6a2f67b6ddce877af5f73a4d47e2c18db2ed790",
                "title": "VEIL: Vetting Extracted Image Labels from In-the-Wild Captions for Weakly-Supervised Object Detection",
                "abstract": "The use of large-scale vision-language datasets is limited for object detection due to the negative impact of label noise on localization. Prior methods have shown how such large-scale datasets can be used for pretraining, which can provide initial signal for localization, but is insufficient without clean bounding-box data for at least some categories. We propose a technique to\"vet\"labels extracted from noisy captions, and use them for weakly-supervised object detection (WSOD). We conduct analysis of the types of label noise in captions, and train a classifier that predicts if an extracted label is actually present in the image or not. Our classifier generalizes across dataset boundaries and across categories. We compare the classifier to eleven baselines on five datasets, and demonstrate that it can improve WSOD without label vetting by 30% (31.2 to 40.5 mAP when evaluated on PASCAL VOC)",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1741370444",
                        "name": "Arushi Rai"
                    },
                    {
                        "authorId": "1770205",
                        "name": "Adriana Kovashka"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "2 [29] Krishna Kumar Singh, Dhruv Mahajan, Kristen Grauman, Yong Jae Lee, Matt Feiszli, and Deepti Ghadiyaram.",
                "Related Work Our work is related to a growing body of work studying biases in visual recognition [7, 16, 26, 29, 32, 37, 38].",
                "Visual recognition models are known to encode societal biases [5, 16, 29, 39, 42]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "16fa155e4b3ec74565283def01a12d3497074c07",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-07615",
                    "ArXiv": "2303.07615",
                    "DOI": "10.48550/arXiv.2303.07615",
                    "CorpusId": 257504863
                },
                "corpusId": 257504863,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/16fa155e4b3ec74565283def01a12d3497074c07",
                "title": "Variation of Gender Biases in Visual Recognition Models Before and After Finetuning",
                "abstract": "We introduce a framework to measure how biases change before and after fine-tuning a large scale visual recognition model for a downstream task. Deep learning models trained on increasing amounts of data are known to encode societal biases. Many computer vision systems today rely on models typically pretrained on large scale datasets. While bias mitigation techniques have been developed for tuning models for downstream tasks, it is currently unclear what are the effects of biases already encoded in a pretrained model. Our framework incorporates sets of canonical images representing individual and pairs of concepts to highlight changes in biases for an array of off-the-shelf pretrained models across model sizes, dataset sizes, and training objectives. Through our analyses, we find that (1) supervised models trained on datasets such as ImageNet-21k are more likely to retain their pretraining biases regardless of the target dataset compared to self-supervised models. We also find that (2) models finetuned on larger scale datasets are more likely to introduce new biased associations. Our results also suggest that (3) biases can transfer to finetuned models and the finetuning objective and dataset can impact the extent of transferred biases.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1819649550",
                        "name": "Jaspreet Ranjit"
                    },
                    {
                        "authorId": "1785372925",
                        "name": "Tianlu Wang"
                    },
                    {
                        "authorId": "31631000",
                        "name": "Baishakhi Ray"
                    },
                    {
                        "authorId": "2004053",
                        "name": "Vicente Ordonez"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "17133d143f15b5ee549952c0b880d0f76024c4d8",
                "externalIds": {
                    "ArXiv": "2303.06167",
                    "CorpusId": 257496198
                },
                "corpusId": 257496198,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/17133d143f15b5ee549952c0b880d0f76024c4d8",
                "title": "Overwriting Pretrained Bias with Finetuning Data",
                "abstract": "Transfer learning is beneficial by allowing the expressive features of models pretrained on large-scale datasets to be finetuned for the target task of smaller, more domain-specific datasets. However, there is a concern that these pretrained models may come with their own biases which would propagate into the finetuned model. In this work, we investigate bias when conceptualized as both spurious correlations between the target task and a sensitive attribute as well as underrepresentation of a particular group in the dataset. Under both notions of bias, we find that (1) models finetuned on top of pretrained models can indeed inherit their biases, but (2) this bias can be corrected for through relatively minor interventions to the finetuning dataset, and often with a negligible impact to performance. Our findings imply that careful curation of the finetuning dataset is important for reducing biases on a downstream task, and doing so can even compensate for bias in the pretrained model.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "46991154",
                        "name": "Angelina Wang"
                    },
                    {
                        "authorId": "2192178",
                        "name": "Olga Russakovsky"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "01404a50a3c0c065da3391c60dda49a0cab36251",
                "externalIds": {
                    "ArXiv": "2302.08571",
                    "DOI": "10.3390/ai4030039",
                    "CorpusId": 257020072
                },
                "corpusId": 257020072,
                "publicationVenue": {
                    "id": "b76366f5-0af9-45f3-8fe3-78fdb0114f67",
                    "name": "Applied Informatics",
                    "type": "conference",
                    "alternate_names": [
                        "Appl Informatics",
                        "Advances Argumentation Artificial Intelligence",
                        "AI",
                        "Can Conf Artif Intell",
                        "Adv Argum Artif Intell",
                        "Canadian Conference on Artificial Intelligence"
                    ],
                    "issn": "2196-0089",
                    "alternate_issns": [
                        "2673-2688"
                    ],
                    "url": "http://cscsi.org/",
                    "alternate_urls": [
                        "https://link.springer.com/journal/40535",
                        "http://www.applied-informatics-j.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/01404a50a3c0c065da3391c60dda49a0cab36251",
                "title": "A Comprehensive Review and a Taxonomy of Edge Machine Learning: Requirements, Paradigms, and Techniques",
                "abstract": "The union of Edge Computing (EC) and Artificial Intelligence (AI) has brought forward the Edge AI concept to provide intelligent solutions close to the end-user environment, for privacy preservation, low latency to real-time performance, and resource optimization. Machine Learning (ML), as the most advanced branch of AI in the past few years, has shown encouraging results and applications in the edge environment. Nevertheless, edge-powered ML solutions are more complex to realize due to the joint constraints from both edge computing and AI domains, and the corresponding solutions are expected to be efficient and adapted in technologies such as data processing, model compression, distributed inference, and advanced learning paradigms for Edge ML requirements. Despite the fact that a great deal of the attention garnered by Edge ML is gained in both the academic and industrial communities, we noticed the lack of a complete survey on existing Edge ML technologies to provide a common understanding of this concept. To tackle this, this paper aims at providing a comprehensive taxonomy and a systematic review of Edge ML techniques, focusing on the soft computing aspects of existing paradigms and techniques. We start by identifying the Edge ML requirements driven by the joint constraints. We then extensively survey more than twenty paradigms and techniques along with their representative work, covering two main parts: edge inference, and edge learning. In particular, we analyze how each technique fits into Edge ML by meeting a subset of the identified requirements. We also summarize Edge ML frameworks and open issues to shed light on future directions for Edge ML.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "35660603",
                        "name": "Wenbin Li"
                    },
                    {
                        "authorId": "1726350",
                        "name": "Hakim Hacid"
                    },
                    {
                        "authorId": "1967677",
                        "name": "Ebtesam Almazrouei"
                    },
                    {
                        "authorId": "145118318",
                        "name": "M. Debbah"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Such unintended correlations can cause the model to act in a biased way, such as having lower accuracy on certain sub-populations of the data [4, 7, 12, 20, 27]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f3c8ab35c29ef015a756dfb1e6b2af0907bf1264",
                "externalIds": {
                    "ArXiv": "2302.04358",
                    "DBLP": "conf/bmvc/SudhakarPKH21",
                    "DOI": "10.48550/arXiv.2302.04358",
                    "CorpusId": 244105415
                },
                "corpusId": 244105415,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/f3c8ab35c29ef015a756dfb1e6b2af0907bf1264",
                "title": "Mitigating Bias in Visual Transformers via Targeted Alignment",
                "abstract": "As transformer architectures become increasingly prevalent in computer vision, it is critical to understand their fairness implications. We perform the first study of the fairness of transformers applied to computer vision and benchmark several bias mitigation approaches from prior work. We visualize the feature space of the transformer self-attention modules and discover that a significant portion of the bias is encoded in the query matrix. With this knowledge, we propose TADeT, a targeted alignment strategy for debiasing transformers that aims to discover and remove bias primarily from query matrix features. We measure performance using Balanced Accuracy and Standard Accuracy, and fairness using Equalized Odds and Balanced Accuracy Difference. TADeT consistently leads to improved fairness over prior work on multiple attribute prediction tasks on the CelebA dataset, without compromising performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2136114132",
                        "name": "Sruthi Sudhakar"
                    },
                    {
                        "authorId": "39351028",
                        "name": "Viraj Prabhu"
                    },
                    {
                        "authorId": "2136115527",
                        "name": "Arvindkumar Krishnakumar"
                    },
                    {
                        "authorId": "50196944",
                        "name": "Judy Hoffman"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "These spurious correlations occur when a feature is statistically informative for a majority of training examples but does not actually capture the underlying relationship between the input features and the target labels [61, 54]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "03d22caf64831d1c48a5a25b3f886196a13d1dce",
                "externalIds": {
                    "ArXiv": "2301.13803",
                    "DBLP": "journals/corr/abs-2301-13803",
                    "DOI": "10.48550/arXiv.2301.13803",
                    "CorpusId": 256416070
                },
                "corpusId": 256416070,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/03d22caf64831d1c48a5a25b3f886196a13d1dce",
                "title": "Fairness-aware Vision Transformer via Debiased Self-Attention",
                "abstract": "Vision Transformer (ViT) has recently gained significant interest in solving computer vision (CV) problems due to its capability of extracting informative features and modeling long-range dependencies through the self-attention mechanism. To fully realize the advantages of ViT in real-world applications, recent works have explored the trustworthiness of ViT, including its robustness and explainability. However, another desiderata, fairness has not yet been adequately addressed in the literature. We establish that the existing fairness-aware algorithms (primarily designed for CNNs) do not perform well on ViT. This necessitates the need for developing our novel framework via Debiased Self-Attention (DSA). DSA is a fairness-through-blindness approach that enforces ViT to eliminate spurious features correlated with the sensitive attributes for bias mitigation. Notably, adversarial examples are leveraged to locate and mask the spurious features in the input image patches. In addition, DSA utilizes an attention weights alignment regularizer in the training objective to encourage learning informative features for target prediction. Importantly, our DSA framework leads to improved fairness guarantees over prior works on multiple prediction tasks without compromising target prediction performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2062242240",
                        "name": "Yao Qiang"
                    },
                    {
                        "authorId": "46651935",
                        "name": "Chengyin Li"
                    },
                    {
                        "authorId": "4386787",
                        "name": "Prashant Khanduri"
                    },
                    {
                        "authorId": "39895985",
                        "name": "D. Zhu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "\u2026Neural network representations have been shown to not generalize well to various domain shifts (e.g. pose (Alcorn et al. 2019), corruptions (Hendrycks and Dietterich 2019)) and to suffer from biases (e.g. texture (Geirhos et al. 2019), context (Singh et al. 2020), background (Xiao et al. 2021)).",
                "2019), context (Singh et al. 2020), background (Xiao et al."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "847f30c125b5cdf5b7a4f0750de5866497d7696e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-04613",
                    "ArXiv": "2212.04613",
                    "DOI": "10.48550/arXiv.2212.04613",
                    "CorpusId": 254535959
                },
                "corpusId": 254535959,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/847f30c125b5cdf5b7a4f0750de5866497d7696e",
                "title": "Contrastive View Design Strategies to Enhance Robustness to Domain Shifts in Downstream Object Detection",
                "abstract": "Contrastive learning has emerged as a competitive pretrain- ing method for object detection. Despite this progress, there has been minimal investigation into the robustness of con- trastively pretrained detectors when faced with domain shifts. To address this gap, we conduct an empirical study of con- trastive learning and out-of-domain object detection, study-ing how contrastive view design affects robustness. In partic- ular, we perform a case study of the detection-focused pretext task Instance Localization (InsLoc) and propose strategies to augment views and enhance robustness in appearance-shifted and context-shifted scenarios. Amongst these strategies, we propose changes to cropping such as altering the percent-age used, adding IoU constraints, and integrating saliency- based object priors. We also explore the addition of shortcut-reducing augmentations such as Poisson blending, texture \ufb02attening, and elastic deformation. We benchmark these strategies on abstract, weather, and context domain shifts and illustrate robust ways to combine them, in both pretraining on single-object and multi-object image datasets. Overall, our re- sults and insights show how to ensure robustness through the choice of views in contrastive learning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51150048",
                        "name": "Kyle Buettner"
                    },
                    {
                        "authorId": "1770205",
                        "name": "Adriana Kovashka"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "790a167c1c4fbb655e92b030c721728673dd1127",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-04871",
                    "ArXiv": "2212.04871",
                    "DOI": "10.48550/arXiv.2212.04871",
                    "CorpusId": 254535987
                },
                "corpusId": 254535987,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/790a167c1c4fbb655e92b030c721728673dd1127",
                "title": "Spurious Features Everywhere - Large-Scale Detection of Harmful Spurious Features in ImageNet",
                "abstract": "Benchmark performance of deep learning classifiers alone is not a reliable predictor for the performance of a deployed model. In particular, if the image classifier has picked up spurious features in the training data, its predictions can fail in unexpected ways. In this paper, we develop a framework that allows us to systematically identify spurious features in large datasets like ImageNet. It is based on our neural PCA components and their visualization. Previous work on spurious features often operates in toy settings or requires costly pixel-wise annotations. In contrast, we work with ImageNet and validate our results by showing that presence of the harmful spurious feature of a class alone is sufficient to trigger the prediction of that class. We introduce the novel dataset\"Spurious ImageNet\"which allows to measure the reliance of any ImageNet classifier on harmful spurious features. Moreover, we introduce SpuFix as a simple mitigation method to reduce the dependence of any ImageNet classifier on previously identified harmful spurious features without requiring additional labels or retraining of the model. We provide code and data at https://github.com/YanNeu/spurious_imagenet .",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2195794433",
                        "name": "Yannic Neuhaus"
                    },
                    {
                        "authorId": "49799275",
                        "name": "Maximilian Augustin"
                    },
                    {
                        "authorId": "2165469787",
                        "name": "Valentyn Boreiko"
                    },
                    {
                        "authorId": "143610806",
                        "name": "Matthias Hein"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "In the field of EGL, we have started to see several works that apply the contrastive objective to the model explanation between similar/dissimilar samples to build up the explanation objective [38, 110, 135, 163].",
                "[135] proposed to align the target model\u2019s Class Activation Maps (CAM) [173] explanation with a pre-trained model\u2019s explanation by minimizing the overlap between each classes explanation."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "025d4b73c1d0f267ca39fa9649413d88ed3cc5bc",
                "externalIds": {
                    "ArXiv": "2212.03954",
                    "DBLP": "journals/corr/abs-2212-03954",
                    "DOI": "10.48550/arXiv.2212.03954",
                    "CorpusId": 254408768
                },
                "corpusId": 254408768,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/025d4b73c1d0f267ca39fa9649413d88ed3cc5bc",
                "title": "Going Beyond XAI: A Systematic Survey for Explanation-Guided Learning",
                "abstract": "As the societal impact of Deep Neural Networks (DNNs) grows, the goals for advancing DNNs become more complex and diverse, ranging from improving a conventional model accuracy metric to infusing advanced human virtues such as fairness, accountability, transparency (FaccT), and unbiasedness. Recently, techniques in Explainable Artificial Intelligence (XAI) are attracting considerable attention, and have tremendously helped Machine Learning (ML) engineers in understanding AI models. However, at the same time, we started to witness the emerging need beyond XAI among AI communities; based on the insights learned from XAI, how can we better empower ML engineers in steering their DNNs so that the model's reasonableness and performance can be improved as intended? This article provides a timely and extensive literature overview of the field Explanation-Guided Learning (EGL), a domain of techniques that steer the DNNs' reasoning process by adding regularization, supervision, or intervention on model explanations. In doing so, we first provide a formal definition of EGL and its general learning paradigm. Secondly, an overview of the key factors for EGL evaluation, as well as summarization and categorization of existing evaluation procedures and metrics for EGL are provided. Finally, the current and potential future application areas and directions of EGL are discussed, and an extensive experimental study is presented aiming at providing comprehensive comparative studies among existing EGL models in various popular application domains, such as Computer Vision (CV) and Natural Language Processing (NLP) domains.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2110674789",
                        "name": "Yuyang Gao"
                    },
                    {
                        "authorId": "34432092",
                        "name": "Siyi Gu"
                    },
                    {
                        "authorId": "2157887017",
                        "name": "Junji Jiang"
                    },
                    {
                        "authorId": "2151797416",
                        "name": "S. Hong"
                    },
                    {
                        "authorId": "2145103541",
                        "name": "Dazhou Yu"
                    },
                    {
                        "authorId": "144000223",
                        "name": "Liang Zhao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "[27] devised a method to detach the object from the context to improve classification.",
                "6 [27] Krishna Kumar Singh, Dhruv Mahajan, Kristen Grauman, Yong Jae Lee, Matt Feiszli, and Deepti Ghadiyaram."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "529ec805e64466e3e3add31ff3c32f37111bf761",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-01470",
                    "ArXiv": "2212.01470",
                    "DOI": "10.48550/arXiv.2212.01470",
                    "CorpusId": 254247158
                },
                "corpusId": 254247158,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/529ec805e64466e3e3add31ff3c32f37111bf761",
                "title": "Prediction of Scene Plausibility",
                "abstract": "Understanding the 3D world from 2D images involves more than detection and segmentation of the objects within the scene. It also includes the interpretation of the structure and arrangement of the scene elements. Such understanding is often rooted in recognizing the physical world and its limitations, and in prior knowledge as to how similar typical scenes are arranged. In this research we pose a new challenge for neural network (or other) scene understanding algorithms - can they distinguish between plausible and implausible scenes? Plausibility can be defined both in terms of physical properties and in terms of functional and typical arrangements. Hence, we define plausibility as the probability of encountering a given scene in the real physical world. We build a dataset of synthetic images containing both plausible and implausible scenes, and test the success of various vision models in the task of recognizing and understanding plausibility.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "6181834",
                        "name": "O. Nachmias"
                    },
                    {
                        "authorId": "2416503",
                        "name": "Ohad Fried"
                    },
                    {
                        "authorId": "2947946",
                        "name": "Ariel Shamir"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "(Alvi, Zisserman, and Nella\u030aker 2018; Kim et al. 2019; McDuff et al. 2019; Singh et al. 2020; Li, Li, and Vasconcelos 2018; Li and Vasconcelos 2019) use bias labels to mit-\nigate the impact of bias labels when classifying target labels.",
                "(Singh et al. 2020) proposes overlap loss, which is measured based on the class activation map.",
                "(Alvi, Zisserman, and Nell\u00e5ker 2018; Kim et al. 2019; McDuff et al. 2019; Singh et al. 2020; Li, Li, and Vasconcelos 2018; Li and Vasconcelos 2019) use bias labels to mitigate the impact of bias labels when classifying target labels."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9fc21e2c94cb2ed51809a9c96eccce810ef22520",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-01189",
                    "ArXiv": "2212.01189",
                    "DOI": "10.48550/arXiv.2212.01189",
                    "CorpusId": 254221061
                },
                "corpusId": 254221061,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/9fc21e2c94cb2ed51809a9c96eccce810ef22520",
                "title": "Denoising after Entropy-based Debiasing A Robust Training Method for Dataset Bias with Noisy Labels",
                "abstract": "Improperly constructed datasets can result in inaccurate inferences. For instance, models trained on biased datasets perform poorly in terms of generalization (i.e., dataset bias). Recent debiasing techniques have successfully achieved generalization performance by underestimating easy-to-learn samples (i.e., bias-aligned samples) and highlighting difficult-to-learn samples (i.e., bias-conflicting samples). However, these techniques may fail owing to noisy labels, because the trained model recognizes noisy labels as difficult-to-learn and thus highlights them. In this study, we find that earlier approaches that used the provided labels to quantify difficulty could be affected by the small proportion of noisy labels. Furthermore, we find that running denoising algorithms before debiasing is ineffective because denoising algorithms reduce the impact of difficult-to-learn samples, including valuable bias-conflicting samples. Therefore, we propose an approach called denoising after entropy-based debiasing, i.e., DENEB, which has three main stages. (1) The prejudice model is trained by emphasizing (bias-aligned, clean) samples, which are selected using a Gaussian Mixture Model. (2) Using the per-sample entropy from the output of the prejudice model, the sampling probability of each sample that is proportional to the entropy is computed. (3) The final model is trained using existing denoising algorithms with the mini-batches constructed by following the computed sampling probability. Compared to existing debiasing and denoising algorithms, our method achieves better debiasing performance on multiple benchmarks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40917250",
                        "name": "Sumyeong Ahn"
                    },
                    {
                        "authorId": "70509252",
                        "name": "Se-Young Yun"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "However, numerous works [46, 47, 40] found that models suffer from contextual biases caused by co-occurrences and try to improve the object-centric generalization ability by removing such biases.",
                "Recent works [47, 40] find out that mitigating contextual biases, caused by co-occurrences of objects and context in a complex scene, would improve the generalization ability of SSL to these downstream tasks."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f1f66410422bf2609c80874f7328e82f1a4766d4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-12817",
                    "ArXiv": "2211.12817",
                    "DOI": "10.48550/arXiv.2211.12817",
                    "CorpusId": 253801956
                },
                "corpusId": 253801956,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f1f66410422bf2609c80874f7328e82f1a4766d4",
                "title": "Reason from Context with Self-supervised Learning",
                "abstract": "Self-supervised learning (SSL) learns to capture discriminative visual features useful for knowledge transfers. To better accommodate the object-centric nature of current downstream tasks such as object recognition and detection, various methods have been proposed to suppress contextual biases or disentangle objects from contexts. Nevertheless, these methods may prove inadequate in situations where object identity needs to be reasoned from associated context, such as recognizing or inferring tiny or obscured objects. As an initial effort in the SSL literature, we investigate whether and how contextual associations can be enhanced for visual reasoning within SSL regimes, by (a) proposing a new Self-supervised method with external memories for Context Reasoning (SeCo), and (b) introducing two new downstream tasks, lift-the-flap and object priming, addressing the problems of\"what\"and\"where\"in context reasoning. In both tasks, SeCo outperformed all state-of-the-art (SOTA) SSL methods by a significant margin. Our network analysis revealed that the proposed external memory in SeCo learns to store prior contextual knowledge, facilitating target identity inference in the lift-the-flap task. Moreover, we conducted psychophysics experiments and introduced a Human benchmark in Object Priming dataset (HOP). Our results demonstrate that SeCo exhibits human-like behaviors.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48032577",
                        "name": "Xinyu Liu"
                    },
                    {
                        "authorId": "2048021896",
                        "name": "Ankur Sikarwar"
                    },
                    {
                        "authorId": "2109781618",
                        "name": "J. Lim"
                    },
                    {
                        "authorId": "2066787605",
                        "name": "Gabriel Kreiman"
                    },
                    {
                        "authorId": "3480262",
                        "name": "Zenglin Shi"
                    },
                    {
                        "authorId": "2418491",
                        "name": "Mengmi Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Crowd-collection of VG images creates selection bias and crowd-annotation of these images create label-bias [29] and co-occurring-bias [25]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e008572d80e82ecb3d6be4a2bcd76432dd5378bf",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-06444",
                    "ArXiv": "2211.06444",
                    "DOI": "10.1109/CVPR52729.2023.01005",
                    "CorpusId": 253510918
                },
                "corpusId": 253510918,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e008572d80e82ecb3d6be4a2bcd76432dd5378bf",
                "title": "Probabilistic Debiasing of Scene Graphs",
                "abstract": "The quality of scene graphs generated by the state-of-the-art (SOTA) models is compromised due to the long-tail nature of the relationships and their parent object pairs. Training of the scene graphs is dominated by the majority relationships of the majority pairs and, therefore, the object-conditional distributions of relationship in the minority pairs are not preserved after the training is converged. Consequently, the biased model performs well on more frequent relationships in the marginal distribution of relationships such as \u2018on\u2019 and 'wearing\u2019, and performs poorly on the less frequent relationships such as \u2018eating\u2019 or \u2018hanging from\u2019. In this work, we propose virtual evidence incorporated within-triplet Bayesian Network (BN) to preserve the object-conditional distribution of the relationship label and to eradicate the bias created by the marginal probability of the relationships. The insufficient number of relationships in the minority classes poses a significant problem in learning the within-triplet Bayesian network. We address this insufficiency by embedding-based augmentation of triplets where we borrow samples of the minority triplet classes from its neighboring triplets in the semantic space. We perform experiments on two different datasets and achieve a significant improvement in the mean recall of the relationships. We also achieve a better balance between recall and mean recall performance compared to the SOTA de-biasing techniques of scene graph models1.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9180790",
                        "name": "Bashirul Azam Biswas"
                    },
                    {
                        "authorId": "2062772903",
                        "name": "Qian Ji"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Therefore, work was already done to decorrelate objects and their visual features to improve model generalization [41]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c1774ad4763b87d5c65f1a0b3d64b4447a4f49b8",
                "externalIds": {
                    "DBLP": "conf/semweb/MonkaHR22",
                    "ArXiv": "2210.11233",
                    "DOI": "10.1007/978-3-031-19433-7_9",
                    "CorpusId": 253018928
                },
                "corpusId": 253018928,
                "publicationVenue": {
                    "id": "efa3ff7a-4d96-44a1-a022-a683408919b6",
                    "name": "International Workshop on the Semantic Web",
                    "type": "conference",
                    "alternate_names": [
                        "SemWeb",
                        "Int Workshop Semantic Web"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c1774ad4763b87d5c65f1a0b3d64b4447a4f49b8",
                "title": "Context-Driven Visual Object Recognition Based on Knowledge Graphs",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2100108777",
                        "name": "Sebastian Monka"
                    },
                    {
                        "authorId": "2567075",
                        "name": "Lavdim Halilaj"
                    },
                    {
                        "authorId": "1748257",
                        "name": "Achim Rettinger"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                ", 2018; Edwards and Storkey, 2015; Elazar and Goldberg, 2018), causal inference (Singh et al., 2020; Kim et al., 2019) and invariant risk minimization (Adragna et al.",
                "\u2026encoder level by taking the advantage of the adversarial learning (Wang et al., 2019; Wadsworth et al., 2018; Edwards and Storkey, 2015; Elazar and Goldberg, 2018), causal inference (Singh et al., 2020; Kim et al., 2019) and invariant risk minimization (Adragna et al., 2020; Arjovsky et al., 2019).",
                "\u2026reduce the model discrimination (Wang et al., 2019; Wadsworth et al., 2018; Edwards and Storkey, 2015; Kim et al., 2019; Elazar and Goldberg, 2018; Singh et al., 2020; Zunino et al., 2021; Rieger et al., 2020; Liu and Avci, 2019; Kusner et al., 2017; Kilbertus et al., 2017; Cheng et al., 2021;\u2026",
                "Rieger et al. (2020); Zunino et al. (2021) made use of the model explainability to remove subset features that incurs bias, while Singh et al. (2020); Kim et al. (2019) concentrated on the causal fairness features to get rid of undesirable bias correlation in the training.",
                ", 2020; Buolamwini and Gebru, 2018), many efficient methods have been proposed to reduce the model discrimination (Wang et al., 2019; Wadsworth et al., 2018; Edwards and Storkey, 2015; Kim et al., 2019; Elazar and Goldberg, 2018; Singh et al., 2020; Zunino et al., 2021; Rieger et al., 2020; Liu and Avci, 2019; Kusner et al., 2017; Kilbertus et al., 2017; Cheng et al., 2021; Kang et al., 2019)."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "c1ac9bc5f9a3b43e2e76dbcf401d65162f7d108f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-06630",
                    "ArXiv": "2210.06630",
                    "DOI": "10.48550/arXiv.2210.06630",
                    "CorpusId": 252872851
                },
                "corpusId": 252872851,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c1ac9bc5f9a3b43e2e76dbcf401d65162f7d108f",
                "title": "Fairness via Adversarial Attribute Neighbourhood Robust Learning",
                "abstract": "Improving fairness between privileged and less-privileged sensitive attribute groups (e.g, {race, gender}) has attracted lots of attention. To enhance the model performs uniformly well in different sensitive attributes, we propose a principled \\underline{R}obust \\underline{A}dversarial \\underline{A}ttribute \\underline{N}eighbourhood (RAAN) loss to debias the classification head and promote a fairer representation distribution across different sensitive attribute groups. The key idea of RAAN is to mitigate the differences of biased representations between different sensitive attribute groups by assigning each sample an adversarial robust weight, which is defined on the representations of adversarial attribute neighbors, i.e, the samples from different protected groups. To provide efficient optimization algorithms, we cast the RAAN into a sum of coupled compositional functions and propose a stochastic adaptive (Adam-style) and non-adaptive (SGD-style) algorithm framework SCRAAN with provable theoretical guarantee. Extensive empirical studies on fairness-related benchmark datasets verify the effectiveness of the proposed method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40806187",
                        "name": "Q. Qi"
                    },
                    {
                        "authorId": "2599451",
                        "name": "Shervin Ardeshir"
                    },
                    {
                        "authorId": "2110289529",
                        "name": "Yi Xu"
                    },
                    {
                        "authorId": "40381920",
                        "name": "Tianbao Yang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "711878ea1d5f280baa6c6c54d3a5a19b5ae4c63e",
                "externalIds": {
                    "ArXiv": "2210.04491",
                    "DBLP": "journals/corr/abs-2210-04491",
                    "DOI": "10.48550/arXiv.2210.04491",
                    "CorpusId": 252780068
                },
                "corpusId": 252780068,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/711878ea1d5f280baa6c6c54d3a5a19b5ae4c63e",
                "title": "A survey of Identification and mitigation of Machine Learning algorithmic biases in Image Analysis",
                "abstract": "The problem of algorithmic bias in machine learning has gained a lot of attention in recent years due to its concrete and potentially hazardous implications in society. In much the same manner, biases can also alter modern industrial and safety-critical applications where machine learning are based on high dimensional inputs such as images. This issue has however been mostly left out of the spotlight in the machine learning literature. Contrarily to societal applications where a set of proxy variables can be provided by the common sense or by regulations to draw the attention on potential risks, industrial and safety-critical applications are most of the times sailing blind. The variables related to undesired biases can indeed be indirectly represented in the input data, or can be unknown, thus making them harder to tackle. This raises serious and well-founded concerns towards the commercial deployment of AI-based solutions, especially in a context where new regulations clearly address the issues opened by undesired biases in AI. Consequently, we propose here to make an overview of recent advances in this area, \ufb01rstly by presenting how such biases can demonstrate themselves, then by exploring different ways to bring them to light, and by probing different possibilities to mitigate them. We \ufb01nally present a practical remote sensing use-case of industrial Fairness.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144450552",
                        "name": "L. Risser"
                    },
                    {
                        "authorId": "150916914",
                        "name": "Agustin Picard"
                    },
                    {
                        "authorId": "2169558767",
                        "name": "Lucas Hervier"
                    },
                    {
                        "authorId": "144736570",
                        "name": "Jean-Michel Loubes"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4da50c3894dbeb86bf5a33ebb5be6447902e1a03",
                "externalIds": {
                    "DBLP": "conf/cvpr/QraitemSP23",
                    "ArXiv": "2209.15605",
                    "DOI": "10.1109/CVPR52729.2023.01945",
                    "CorpusId": 252668364
                },
                "corpusId": 252668364,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4da50c3894dbeb86bf5a33ebb5be6447902e1a03",
                "title": "Bias Mimicking: A Simple Sampling Approach for Bias Mitigation",
                "abstract": "Prior work has shown that Visual Recognition datasets frequently underrepresent bias groups $B$ (e.g. Female) within class labels $Y$ (e.g. Programmers). This dataset bias can lead to models that learn spurious correlations between class labels and bias groups such as age, gender, or race. Most recent methods that address this problem require significant architectural changes or additional loss functions requiring more hyper-parameter tuning. Alternatively, data sampling baselines from the class imbalance literature (e.g. Undersampling, Upweighting), which can often be implemented in a single line of code and often have no hyperparameters, offer a cheaper and more efficient solution. However, these methods suffer from significant shortcomings. For example, Undersampling drops a significant part of the input distribution per epoch while Oversampling repeats samples, causing overfitting. To address these shortcomings, we introduce a new class-conditioned sampling method: Bias Mimicking. The method is based on the observation that if a class $c$ bias distribution, i.e. $P_{D}(B\\vert Y=c)$ is mimicked across every $c^{\\prime}\\neq c$, then $Y$ and $B$ are statistically independent. Using this notion, BM, through a novel training procedure, ensures that the model is exposed to the entire distribution per epoch without repeating samples. Consequently, Bias Mimicking improves underrepresented groups' accuracy of sampling methods by 3% over four benchmarks while maintaining and sometimes improving performance over nonsampling methods. Code: https://github.com/mqraitem/Bias-Mimicking",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "92749945",
                        "name": "Maan Qraitem"
                    },
                    {
                        "authorId": "2903226",
                        "name": "Kate Saenko"
                    },
                    {
                        "authorId": "2856622",
                        "name": "Bryan A. Plummer"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "19616d9030bb3cc69f0cd9b4661de1a16d70f4f1",
                "externalIds": {
                    "DBLP": "conf/vissym/KwonLCLCC22",
                    "ArXiv": "2209.06357",
                    "DOI": "10.2312/evs.20221099",
                    "CorpusId": 251384230
                },
                "corpusId": 251384230,
                "publicationVenue": {
                    "id": "fec1e1ad-3ba6-4f6f-96e5-c262ada5b009",
                    "name": "Eurographics Conference on Visualization",
                    "type": "conference",
                    "alternate_names": [
                        "EuroVis",
                        "Gr Conf Vis"
                    ],
                    "url": "https://diglib.eg.org/handle/10.2312/392"
                },
                "url": "https://www.semanticscholar.org/paper/19616d9030bb3cc69f0cd9b4661de1a16d70f4f1",
                "title": "DASH: Visual Analytics for Debiasing Image Classification via User-Driven Synthetic Data Augmentation",
                "abstract": "Image classification models often learn to predict a class based on irrelevant co-occurrences between input features and an output class in training data. We call the unwanted correlations\"data biases,\"and the visual features causing data biases\"bias factors.\"It is challenging to identify and mitigate biases automatically without human intervention. Therefore, we conducted a design study to find a human-in-the-loop solution. First, we identified user tasks that capture the bias mitigation process for image classification models with three experts. Then, to support the tasks, we developed a visual analytics system called DASH that allows users to visually identify bias factors, to iteratively generate synthetic images using a state-of-the-art image-to-image translation model, and to supervise the model training process for improving the classification accuracy. Our quantitative evaluation and qualitative study with ten participants demonstrate the usefulness of DASH and provide lessons for future work.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145276140",
                        "name": "B. Kwon"
                    },
                    {
                        "authorId": "2108471861",
                        "name": "Jungsoo Lee"
                    },
                    {
                        "authorId": "2049404784",
                        "name": "Chaeyeon Chung"
                    },
                    {
                        "authorId": "1666584032",
                        "name": "Nyoungwoo Lee"
                    },
                    {
                        "authorId": "2111528050",
                        "name": "Ho-Jin Choi"
                    },
                    {
                        "authorId": "1795455",
                        "name": "J. Choo"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[51] propose a feature splitting approach to mitigate contextual bias."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c2fbcd0f7fd1edb4616c435b5e92106eb5c4945a",
                "externalIds": {
                    "DBLP": "conf/eccv/LiHX22",
                    "ArXiv": "2207.10077",
                    "DOI": "10.48550/arXiv.2207.10077",
                    "CorpusId": 250698948
                },
                "corpusId": 250698948,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/c2fbcd0f7fd1edb4616c435b5e92106eb5c4945a",
                "title": "Discover and Mitigate Unknown Biases with Debiasing Alternate Networks",
                "abstract": "Deep image classifiers have been found to learn biases from datasets. To mitigate the biases, most previous methods require labels of protected attributes (e.g., age, skin tone) as full-supervision, which has two limitations: 1) it is infeasible when the labels are unavailable; 2) they are incapable of mitigating unknown biases -- biases that humans do not preconceive. To resolve those problems, we propose Debiasing Alternate Networks (DebiAN), which comprises two networks -- a Discoverer and a Classifier. By training in an alternate manner, the discoverer tries to find multiple unknown biases of the classifier without any annotations of biases, and the classifier aims at unlearning the biases identified by the discoverer. While previous works evaluate debiasing results in terms of a single bias, we create Multi-Color MNIST dataset to better benchmark mitigation of multiple biases in a multi-bias setting, which not only reveals the problems in previous methods but also demonstrates the advantage of DebiAN in identifying and mitigating multiple biases simultaneously. We further conduct extensive experiments on real-world datasets, showing that the discoverer in DebiAN can identify unknown biases that may be hard to be found by humans. Regarding debiasing, DebiAN achieves strong bias mitigation performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48458657",
                        "name": "Zhiheng Li"
                    },
                    {
                        "authorId": "1397590190",
                        "name": "Anthony J. Hoogs"
                    },
                    {
                        "authorId": "2026123",
                        "name": "Chenliang Xu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "What blindspots do we study? We consider blindspots discovered by prior works (Hohman et al., 2019; Sohoni et al., 2020; Singh et al., 2020; Plumb et al., 2022; Singla et al., 2021) as well as novel blindspots (Adebayo et al.",
                "More broadly, finding systemic errors can help us detect algorithmic bias (Buolamwini & Gebru, 2018) or sensitivity to distribution shifts (Sagawa et al., 2020; Singh et al., 2020).",
                "We consider blindspots discovered by prior works (Hohman et al., 2019; Sohoni et al., 2020; Singh et al., 2020; Plumb et al., 2022; Singla et al., 2021) as well as novel blindspots (Adebayo et al., 2022).",
                "Some common assumptions are: \u2022 Access to metadata help define coherent subsets of the data (e.g., Kim et al., 2018; Buolamwini & Gebru, 2018; Singh et al., 2020).",
                "In Table 6 of their paper, Singh et al. (2020) organize the spurious patterns that they identify by their own measure of bias.",
                "Some common assumptions are:\n\u2022 Access to metadata help define coherent subsets of the data (e.g., Kim et al., 2018; Buolamwini & Gebru, 2018; Singh et al., 2020)."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "1b693d569432efc4a7d56662a86ab28bd4832358",
                "externalIds": {
                    "ArXiv": "2207.04104",
                    "CorpusId": 259833461
                },
                "corpusId": 259833461,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1b693d569432efc4a7d56662a86ab28bd4832358",
                "title": "Towards a More Rigorous Science of Blindspot Discovery in Image Classification Models",
                "abstract": "A growing body of work studies Blindspot Discovery Methods (\"BDM\"s): methods that use an image embedding to find semantically meaningful (i.e., united by a human-understandable concept) subsets of the data where an image classifier performs significantly worse. Motivated by observed gaps in prior work, we introduce a new framework for evaluating BDMs, SpotCheck, that uses synthetic image datasets to train models with known blindspots and a new BDM, PlaneSpot, that uses a 2D image representation. We use SpotCheck to run controlled experiments that identify factors that influence BDM performance (e.g., the number of blindspots in a model, or features used to define the blindspot) and show that PlaneSpot is competitive with and in many cases outperforms existing BDMs. Importantly, we validate these findings by designing additional experiments that use real image data from MS-COCO, a large image benchmark dataset. Our findings suggest several promising directions for future work on BDM design and evaluation. Overall, we hope that the methodology and analyses presented in this work will help facilitate a more rigorous science of blindspot discovery.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "31929250",
                        "name": "Gregory Plumb"
                    },
                    {
                        "authorId": "2142541079",
                        "name": "Nari Johnson"
                    },
                    {
                        "authorId": "2128108979",
                        "name": "'Angel Alexander Cabrera"
                    },
                    {
                        "authorId": "145532827",
                        "name": "Ameet Talwalkar"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dddcce3c8be676a6e98613eb610b0b687c871a8a",
                "externalIds": {
                    "ArXiv": "2207.02842",
                    "DBLP": "journals/corr/abs-2207-02842",
                    "DOI": "10.48550/arXiv.2207.02842",
                    "CorpusId": 250311577
                },
                "corpusId": 250311577,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/dddcce3c8be676a6e98613eb610b0b687c871a8a",
                "title": "When does Bias Transfer in Transfer Learning?",
                "abstract": "Using transfer learning to adapt a pre-trained\"source model\"to a downstream\"target task\"can dramatically increase performance with seemingly no downside. In this work, we demonstrate that there can exist a downside after all: bias transfer, or the tendency for biases of the source model to persist even after adapting the model to the target class. Through a combination of synthetic and natural experiments, we show that bias transfer both (a) arises in realistic settings (such as when pre-training on ImageNet or other standard datasets) and (b) can occur even when the target dataset is explicitly de-biased. As transfer-learned models are increasingly deployed in the real world, our work highlights the importance of understanding the limitations of pre-trained source models. Code is available at https://github.com/MadryLab/bias-transfer",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40575781",
                        "name": "Hadi Salman"
                    },
                    {
                        "authorId": "82853009",
                        "name": "Saachi Jain"
                    },
                    {
                        "authorId": "2064782238",
                        "name": "Andrew Ilyas"
                    },
                    {
                        "authorId": "39468283",
                        "name": "Logan Engstrom"
                    },
                    {
                        "authorId": "51026953",
                        "name": "Eric Wong"
                    },
                    {
                        "authorId": "143826246",
                        "name": "A. Madry"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "86d69aa17809d5d7c9ac2b9657aad969ad7ed609",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-09191",
                    "ArXiv": "2206.09191",
                    "DOI": "10.48550/arXiv.2206.09191",
                    "CorpusId": 249888997
                },
                "corpusId": 249888997,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/86d69aa17809d5d7c9ac2b9657aad969ad7ed609",
                "title": "Gender Artifacts in Visual Datasets",
                "abstract": "Gender biases are known to exist within large-scale visual datasets and can be reflected or even amplified in downstream models. Many prior works have proposed methods for mitigating gender biases, often by attempting to remove gender expression information from images. To understand the feasibility and practicality of these approaches, we investigate what $\\textit{gender artifacts}$ exist within large-scale visual datasets. We define a $\\textit{gender artifact}$ as a visual cue that is correlated with gender, focusing specifically on those cues that are learnable by a modern image classifier and have an interpretable human corollary. Through our analyses, we find that gender artifacts are ubiquitous in the COCO and OpenImages datasets, occurring everywhere from low-level information (e.g., the mean value of the color channels) to the higher-level composition of the image (e.g., pose and location of people). Given the prevalence of gender artifacts, we claim that attempts to remove gender artifacts from such datasets are largely infeasible. Instead, the responsibility lies with researchers and practitioners to be aware that the distribution of images within datasets is highly gendered and hence develop methods which are robust to these distributional shifts across groups.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49947642",
                        "name": "Nicole Meister"
                    },
                    {
                        "authorId": "2116403497",
                        "name": "Dora Zhao"
                    },
                    {
                        "authorId": "46991154",
                        "name": "Angelina Wang"
                    },
                    {
                        "authorId": "2030978165",
                        "name": "V. V. Ramaswamy"
                    },
                    {
                        "authorId": "25576460",
                        "name": "Ruth C. Fong"
                    },
                    {
                        "authorId": "2192178",
                        "name": "Olga Russakovsky"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "257f9f3dbe2bac1ae242728827f8a861bd8469fd",
                "externalIds": {
                    "DBLP": "conf/nips/CheferSW22",
                    "ArXiv": "2206.01161",
                    "DOI": "10.48550/arXiv.2206.01161",
                    "CorpusId": 249282278
                },
                "corpusId": 249282278,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/257f9f3dbe2bac1ae242728827f8a861bd8469fd",
                "title": "Optimizing Relevance Maps of Vision Transformers Improves Robustness",
                "abstract": "It has been observed that visual classification models often rely mostly on the image background, neglecting the foreground, which hurts their robustness to distribution changes. To alleviate this shortcoming, we propose to monitor the model's relevancy signal and manipulate it such that the model is focused on the foreground object. This is done as a finetuning step, involving relatively few samples consisting of pairs of images and their associated foreground masks. Specifically, we encourage the model's relevancy map (i) to assign lower relevance to background regions, (ii) to consider as much information as possible from the foreground, and (iii) we encourage the decisions to have high confidence. When applied to Vision Transformer (ViT) models, a marked improvement in robustness to domain shifts is observed. Moreover, the foreground masks can be obtained automatically, from a self-supervised variant of the ViT model itself; therefore no additional supervision is required.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2038268012",
                        "name": "Hila Chefer"
                    },
                    {
                        "authorId": "38211837",
                        "name": "Idan Schwartz"
                    },
                    {
                        "authorId": "145128145",
                        "name": "Lior Wolf"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[37] focus on addressing context biases for visual classifier by explicitly learning a robust feature subspace of a category."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "95ea008993bc32cf3342aa2a65b0d8856bb86c81",
                "externalIds": {
                    "DBLP": "conf/cvpr/Xia0Z0T22",
                    "ArXiv": "2206.11493",
                    "DOI": "10.1109/CVPR52688.2022.01351",
                    "CorpusId": 249954078
                },
                "corpusId": 249954078,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/95ea008993bc32cf3342aa2a65b0d8856bb86c81",
                "title": "Learning to Refactor Action and Co-occurrence Features for Temporal Action Localization",
                "abstract": "The main challenge of Temporal Action Localization is to retrieve subtle human actions from various co-occurring ingredients, e.g., context and background, in an untrimmed video. While prior approaches have achieved substantial progress through devising advanced action detectors, they still suffer from these co-occurring ingredients which often dominate the actual action content in videos. In this paper, we explore two orthogonal but complementary aspects of a video snippet, i.e., the action features and the co-occurrence features. Especially, we develop a novel auxiliary task by decoupling these two types of features within a video snippet and recombining them to generate a new feature representation with more salient action information for accurate action localization. We term our method RefactorNet, which first explicitly factorizes the action content and regularizes its co-occurrence features, and then synthesizes a new action-dominated video representation. Extensive experimental results and ablation studies on THUMOS14 and ActivityNet v 1.3 demonstrate that our new representation, combined with a simple action detector, can significantly improve the action localization performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2065015906",
                        "name": "Kun Xia"
                    },
                    {
                        "authorId": "2108571702",
                        "name": "Le Wang"
                    },
                    {
                        "authorId": "3373601",
                        "name": "Sanping Zhou"
                    },
                    {
                        "authorId": "2144620206",
                        "name": "Nanning Zheng"
                    },
                    {
                        "authorId": "2021307128",
                        "name": "Wei Tang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Some of these studies (Alvi et al., 2018; Kim et al., 2019; McDuff et al., 2019; Singh et al., 2020; Li et al., 2018; Li & Vasconcelos, 2019), used bias labels for each sample to reduce the influence of the bias labels when classifying target labels.",
                "Furthermore, Singh\net al. (2020) proposed a new overlap loss defined by a class activation map (CAM).",
                "To reduce the dataset bias, initial studies (Kim et al., 2019; McDuff et al., 2019; Singh et al., 2020; Li & Vasconcelos, 2019) have frequently assumed a case where labels with bias attributes are provided, but these additional labels provided through human effort are expensive.",
                "\u201d To reduce the dataset bias, initial studies (Kim et al., 2019; McDuff et al., 2019; Singh et al., 2020; Li & Vasconcelos, 2019) have frequently assumed a case where labels with bias attributes are provided, but these additional labels provided through human effort are expensive.",
                "Various studies (Alvi et al., 2018; Kim et al., 2019; McDuff et al., 2019; Singh et al., 2020; Teney et al., 2021) have attempted to reduce dataset bias using explicit bias labels."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "35cd12a6fe30e8fc532c37396f581bc8dc0c2a09",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-15704",
                    "ArXiv": "2205.15704",
                    "DOI": "10.48550/arXiv.2205.15704",
                    "CorpusId": 249209690
                },
                "corpusId": 249209690,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/35cd12a6fe30e8fc532c37396f581bc8dc0c2a09",
                "title": "Mitigating Dataset Bias by Using Per-sample Gradient",
                "abstract": "The performance of deep neural networks is strongly influenced by the training dataset setup. In particular, when attributes having a strong correlation with the target attribute are present, the trained model can provide unintended prejudgments and show significant inference errors (i.e., the dataset bias problem). Various methods have been proposed to mitigate dataset bias, and their emphasis is on weakly correlated samples, called bias-conflicting samples. These methods are based on explicit bias labels involving human or empirical correlation metrics (e.g., training loss). However, such metrics require human costs or have insufficient theoretical explanation. In this study, we propose a debiasing algorithm, called PGD (Per-sample Gradient-based Debiasing), that comprises three steps: (1) training a model on uniform batch sampling, (2) setting the importance of each sample in proportion to the norm of the sample gradient, and (3) training the model using importance-batch sampling, whose probability is obtained in step (2). Compared with existing baselines for various synthetic and real-world datasets, the proposed method showed state-of-the-art accuracy for a the classification task. Furthermore, we describe theoretical understandings about how PGD can mitigate dataset bias.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40917250",
                        "name": "Sumyeong Ahn"
                    },
                    {
                        "authorId": "2109600198",
                        "name": "Seongyoon Kim"
                    },
                    {
                        "authorId": "70509252",
                        "name": "Se-Young Yun"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8545e249ab7a49f4a5abcfade395b90ffadb687a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-15480",
                    "ArXiv": "2205.15480",
                    "DOI": "10.48550/arXiv.2205.15480",
                    "CorpusId": 249209990
                },
                "corpusId": 249209990,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8545e249ab7a49f4a5abcfade395b90ffadb687a",
                "title": "Post-hoc Concept Bottleneck Models",
                "abstract": "Concept Bottleneck Models (CBMs) map the inputs onto a set of interpretable concepts (``the bottleneck'') and use the concepts to make predictions. A concept bottleneck enhances interpretability since it can be investigated to understand what concepts the model\"sees\"in an input and which of these concepts are deemed important. However, CBMs are restrictive in practice as they require dense concept annotations in the training data to learn the bottleneck. Moreover, CBMs often do not match the accuracy of an unrestricted neural network, reducing the incentive to deploy them in practice. In this work, we address these limitations of CBMs by introducing Post-hoc Concept Bottleneck models (PCBMs). We show that we can turn any neural network into a PCBM without sacrificing model performance while still retaining the interpretability benefits. When concept annotations are not available on the training data, we show that PCBM can transfer concepts from other datasets or from natural language descriptions of concepts via multimodal models. A key benefit of PCBM is that it enables users to quickly debug and update the model to reduce spurious correlations and improve generalization to new distributions. PCBM allows for global model edits, which can be more efficient than previous works on local interventions that fix a specific prediction. Through a model-editing user study, we show that editing PCBMs via concept-level feedback can provide significant performance gains without using data from the target domain or model retraining.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2186981598",
                        "name": "Mert Yuksekgonul"
                    },
                    {
                        "authorId": "2027032530",
                        "name": "Maggie Wang"
                    },
                    {
                        "authorId": "145085305",
                        "name": "James Y. Zou"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dfa2c99fecaf0f5a592af5e63a4e51b76eaee2c6",
                "externalIds": {
                    "ArXiv": "2205.06253",
                    "DBLP": "journals/corr/abs-2205-06253",
                    "DOI": "10.1109/CVPRW56347.2022.00520",
                    "CorpusId": 248721941
                },
                "corpusId": 248721941,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/dfa2c99fecaf0f5a592af5e63a4e51b76eaee2c6",
                "title": "What\u2019s in a Caption? Dataset-Specific Linguistic Diversity and Its Effect on Visual Description Models and Metrics",
                "abstract": "While there have been significant gains in the field of automated video description, the generalization performance of automated description models to novel domains remains a major barrier to using these systems in the real world. Most visual description methods are known to capture and exploit patterns in the training data leading to evaluation metric increases, but what are those patterns? In this work, we examine several popular visual description datasets, and capture, analyze, and understand the dataset-specific linguistic patterns that models exploit but do not generalize to new domains. At the token level, sample level, and dataset level, we find that caption diversity is a major driving factor behind the generation of generic and uninformative captions. We further show that state-of-the-art models even outperform held-out ground truth captions on modern metrics, and that this effect is an artifact of linguistic diversity in datasets. Understanding this linguistic diversity is key to building strong captioning models, we recommend several methods and approaches for maintaining diversity in the collection of new data, and dealing with the consequences of limited diversity when using current models and metrics.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152502885",
                        "name": "David Chan"
                    },
                    {
                        "authorId": "49588480",
                        "name": "Austin Myers"
                    },
                    {
                        "authorId": "2259154",
                        "name": "Sudheendra Vijayanarasimhan"
                    },
                    {
                        "authorId": "144711958",
                        "name": "David A. Ross"
                    },
                    {
                        "authorId": "2535887",
                        "name": "Bryan Seybold"
                    },
                    {
                        "authorId": "1729041",
                        "name": "J. Canny"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Besides, to have a robust decision, [9] uses mixup augmentation and [10] investigates how to disentangle object from its co-occurring context."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3cf8949576165e839e31720e7ec21a8dee7f4d1b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-02887",
                    "ArXiv": "2205.02887",
                    "DOI": "10.48550/arXiv.2205.02887",
                    "CorpusId": 248562516
                },
                "corpusId": 248562516,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3cf8949576165e839e31720e7ec21a8dee7f4d1b",
                "title": "Evaluating Context for Deep Object Detectors",
                "abstract": "Which object detector is suitable for your context sensitive task? Deep object detectors exploit scene context for recognition differently. In this paper, we group object detectors into 3 categories in terms of context use: no context by cropping the input (RCNN), partial context by cropping the featuremap (two-stage methods) and full context without any cropping (single-stage methods). We systematically evaluate the effect of context for each deep detector category. We create a fully controlled dataset for varying context and investigate the context for deep detectors. We also evaluate gradually removing the background context and the foreground object on MS COCO. We demonstrate that single-stage and two-stage object detectors can and will use the context by virtue of their large receptive field. Thus, choosing the best object detector may depend on the application context.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50311569",
                        "name": "O. Kayhan"
                    },
                    {
                        "authorId": "1738975",
                        "name": "J. V. Gemert"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[15] study the problem of contextual biases learned by deep models based on frequently co-occuring categories."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "66da1fafd46924f738d1ef24bf4224555be3fade",
                "externalIds": {
                    "DBLP": "conf/cvpr/PrabhuSHN22",
                    "ArXiv": "2204.11122",
                    "DOI": "10.1109/CVPRW56347.2022.00443",
                    "CorpusId": 248377225
                },
                "corpusId": 248377225,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/66da1fafd46924f738d1ef24bf4224555be3fade",
                "title": "Can domain adaptation make object recognition work for everyone?",
                "abstract": "Despite the rapid progress in deep visual recognition, modern computer vision datasets significantly overrepresent the developed world and models trained on such datasets underperform on images from unseen geographies. We investigate the effectiveness of unsupervised domain adaptation (UDA) of such models across geographies at closing this performance gap. To do so, we first curate two shifts from existing datasets to study the Geographical DA problem, and discover new challenges beyond data distribution shift: context shift, wherein object surroundings may change significantly across geographies, and subpopulation shift, wherein the intra-category distributions may shift. We demonstrate the inefficacy of standard DA methods at Geographical DA, highlighting the need for specialized geographical adaptation solutions to address the challenge of making object recognition work for everyone.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "39351028",
                        "name": "Viraj Prabhu"
                    },
                    {
                        "authorId": "35100058",
                        "name": "Ramprasaath R. Selvaraju"
                    },
                    {
                        "authorId": "50196944",
                        "name": "Judy Hoffman"
                    },
                    {
                        "authorId": "2047256670",
                        "name": "N. Naik"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[47] investigated the co-occurence between objects and their contexts for each category and attempted to decorrelate them to reduce classifiers\u2019 dependency on the contexts."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "13479233b69767d2e2cd1846f3f2805d6b33dc6e",
                "externalIds": {
                    "ArXiv": "2204.05899",
                    "DBLP": "journals/corr/abs-2204-05899",
                    "DOI": "10.1109/CVPR52688.2022.02081",
                    "CorpusId": 248118980
                },
                "corpusId": 248118980,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/13479233b69767d2e2cd1846f3f2805d6b33dc6e",
                "title": "VIsCUIT: Visual Auditor for Bias in CNN Image Classifier",
                "abstract": "CNN image classifiers are widely used, thanks to their efficiency and accuracy. However, they can suffer from biases that impede their practical applications. Most existing bias investigation techniques are either inapplicable to general image classification tasks or require significant user efforts in perusing all data subgroups to manually specify which data attributes to inspect. We present VIsCUIT, an interactive visualization system that reveals how and why a CNN classifier is biased. VIsCUIT visually summarizes the subgroups on which the classifier underperforms and helps users discover and characterize the cause of the underperformances by revealing image concepts responsible for activating neurons that contribute to misclassifications. VIsCUIT runs in modern browsers and is opensource, allowing people to easily access and extend the tool to other model architectures and datasets. VIsCUIT is available at the following public demo link: https://poloclub.github.io/VisCUIT. A video demo is available at https://youtu.be/eNDbSyM4R_4.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108642708",
                        "name": "Seongmin Lee"
                    },
                    {
                        "authorId": "1390877819",
                        "name": "Zijie J. Wang"
                    },
                    {
                        "authorId": "50196944",
                        "name": "Judy Hoffman"
                    },
                    {
                        "authorId": "1793506",
                        "name": "Duen Horng Chau"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "We hypothesize this would be especially useful for combating background and contextual biases [3, 63]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "630f95bf37c561a5c3b313bf0d84c3250835ecba",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-02426",
                    "ArXiv": "2204.02426",
                    "DOI": "10.48550/arXiv.2204.02426",
                    "CorpusId": 247996722
                },
                "corpusId": 247996722,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/630f95bf37c561a5c3b313bf0d84c3250835ecba",
                "title": "OccamNets: Mitigating Dataset Bias by Favoring Simpler Hypotheses",
                "abstract": "Dataset bias and spurious correlations can significantly impair generalization in deep neural networks. Many prior efforts have addressed this problem using either alternative loss functions or sampling strategies that focus on rare patterns. We propose a new direction: modifying the network architecture to impose inductive biases that make the network robust to dataset bias. Specifically, we propose OccamNets, which are biased to favor simpler solutions by design. OccamNets have two inductive biases. First, they are biased to use as little network depth as needed for an individual example. Second, they are biased toward using fewer image locations for prediction. While OccamNets are biased toward simpler hypotheses, they can learn more complex hypotheses if necessary. In experiments, OccamNets outperform or rival state-of-the-art methods run on architectures that do not incorporate these inductive biases. Furthermore, we demonstrate that when the state-of-the-art debiasing methods are combined with OccamNets results further improve.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "153677280",
                        "name": "Robik Shrestha"
                    },
                    {
                        "authorId": "33315685",
                        "name": "Kushal Kafle"
                    },
                    {
                        "authorId": "3290098",
                        "name": "Christopher Kanan"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Fairness through explanation is another bias mitigation technique [17,9,18], this technique requires fine-grained feature-level annotation as the domain knowledge to train the model to only focus on bias-unrelated features in the original input."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2714740aacc642cb273f7eb68e41f78d7fbfc1d5",
                "externalIds": {
                    "ArXiv": "2203.02110",
                    "DBLP": "conf/miccai/WuZXSH22",
                    "DOI": "10.48550/arXiv.2203.02110",
                    "CorpusId": 247244624
                },
                "corpusId": 247244624,
                "publicationVenue": {
                    "id": "61a709e3-2060-423c-8de5-ffd3885aa31c",
                    "name": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                    "type": "conference",
                    "alternate_names": [
                        "Medical Image Computing and Computer-Assisted Intervention",
                        "MICCAI",
                        "Med Image Comput Comput Interv",
                        "Int Conf Med Image Comput Comput Interv"
                    ],
                    "url": "http://www.miccai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/2714740aacc642cb273f7eb68e41f78d7fbfc1d5",
                "title": "FairPrune: Achieving Fairness Through Pruning for Dermatological Disease Diagnosis",
                "abstract": "Many works have shown that deep learning-based medical image classification models can exhibit bias toward certain demographic attributes like race, gender, and age. Existing bias mitigation methods primarily focus on learning debiased models, which may not necessarily guarantee all sensitive information can be removed and usually comes with considerable accuracy degradation on both privileged and unprivileged groups. To tackle this issue, we propose a method, FairPrune, that achieves fairness by pruning. Conventionally, pruning is used to reduce the model size for efficient inference. However, we show that pruning can also be a powerful tool to achieve fairness. Our observation is that during pruning, each parameter in the model has different importance for different groups' accuracy. By pruning the parameters based on this importance difference, we can reduce the accuracy difference between the privileged group and the unprivileged group to improve fairness without a large accuracy drop. To this end, we use the second derivative of the parameters of a pre-trained model to quantify the importance of each parameter with respect to the model accuracy for each group. Experiments on two skin lesion diagnosis datasets over multiple sensitive attributes demonstrate that our method can greatly improve fairness while keeping the average accuracy of both groups as high as possible.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2107887906",
                        "name": "Yawen Wu"
                    },
                    {
                        "authorId": "2051047670",
                        "name": "Dewen Zeng"
                    },
                    {
                        "authorId": "144838755",
                        "name": "Xiaowei Xu"
                    },
                    {
                        "authorId": "1702907",
                        "name": "Yiyu Shi"
                    },
                    {
                        "authorId": "2118517757",
                        "name": "Jingtong Hu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Others use slightly less expensive image-level annotations of the biased feature [1, 14, 38, 40, 43]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "657ec6e3d6332e41a2daa2352ecf2bda0bdb8038",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-08926",
                    "ArXiv": "2202.08926",
                    "DOI": "10.1109/CVPR52688.2022.01756",
                    "CorpusId": 246996797
                },
                "corpusId": 246996797,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/657ec6e3d6332e41a2daa2352ecf2bda0bdb8038",
                "title": "On Guiding Visual Attention with Language Specification",
                "abstract": "While real world challenges typically define visual categories with language words or phrases, most visual classification methods define categories with numerical indices. However, the language specification of the classes provides an especially useful prior for biased and noisy datasets, where it can help disambiguate what features are task-relevant. Recently, large-scale multimodal models have been shown to recognize a wide variety of high-level concepts from a language specification even without additional image training data, but they are often unable to distinguish classes for more fine-grained tasks. CNNs, in contrast, can extract subtle image features that are required for fine-grained discrimination, but will overfit to any bias or noise in datasets. Our insight is to use high-level language specification as advice for constraining the classification evidence to task-relevant features, instead of distractors. To do this, we ground task-relevant words or phrases with attention maps from a pretrained large-scale model. We then use this grounding to supervise a classifier's spatial attention away from distracting context. We show that supervising spatial attention in this way improves performance on classification tasks with biased and noisy data, including ~3 \u221215% worst-group accuracy improvements and ~41-45% relative improvements on fairness metrics.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "52013156",
                        "name": "Suzanne Petryk"
                    },
                    {
                        "authorId": "151088535",
                        "name": "Lisa Dunlap"
                    },
                    {
                        "authorId": "2114436811",
                        "name": "Keyan Nasseri"
                    },
                    {
                        "authorId": "49988044",
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "authorId": "1753210",
                        "name": "Trevor Darrell"
                    },
                    {
                        "authorId": "34721166",
                        "name": "Anna Rohrbach"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "On the contrary, the latter benefits from different augmentations and another pretext task and thereby avoid a potential pitfall of DINO: encouraging contextual bias [17], which occurs when the similarity between the representations of views depicting distinct tissue types is enforced.",
                "To ensure that the pretext task does not encourage contextual biases [17], we only employ augmentations that change the image pixels\u2019 values, but not their locations, such that the semantic content of the two augmented views is identical."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "af0bcb4e570aa21d8e0b3753b27b1bba9659a711",
                "externalIds": {
                    "DBLP": "conf/wacv/StegmullerBST23",
                    "ArXiv": "2202.07570",
                    "DOI": "10.1109/WACV56688.2023.00611",
                    "CorpusId": 246863585
                },
                "corpusId": 246863585,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/af0bcb4e570aa21d8e0b3753b27b1bba9659a711",
                "title": "ScoreNet: Learning Non-Uniform Attention and Augmentation for Transformer-Based Histopathological Image Classification",
                "abstract": "Progress in digital pathology is hindered by high-resolution images and the prohibitive cost of exhaustive localized annotations. The commonly used paradigm to categorize pathology images is patch-based processing, which often incorporates multiple instance learning (MIL) to aggregate local patch-level representations yielding image-level prediction. Nonetheless, diagnostically relevant regions may only take a small fraction of the whole tissue, and current MIL-based approaches often process images uniformly, discarding the inter-patches interactions. To alleviate these issues, we propose ScoreNet, a new efficient transformer that exploits a differentiable recommendation stage to extract discriminative image regions and dedicate computational resources accordingly. The proposed transformer leverages the local and global attention of a few dynamically recommended high-resolution regions at an efficient computational cost. We further introduce a novel mixing data-augmentation, namely ScoreMix, by leveraging the image\u2019s semantic distribution to guide the data mixing and produce coherent sample-label pairs. ScoreMix is embarrassingly simple and mitigates the pitfalls of previous augmentations, which assume a uniform semantic distribution and risk mislabeling the samples. Thorough experiments and ablation studies on three breast cancer histology datasets of Haematoxylin & Eosin (H&E) have validated the superiority of our approach over prior arts, including transformer-based models on tumour regions-of-interest (TRoIs) classification. ScoreNet equipped with proposed ScoreMix augmentation demonstrates better generalization capabilities and achieves new state-of-the-art (SOTA) results with only 50% of the data compared to other mixing augmentation variants. Finally, ScoreNet yields high efficacy and outperforms SOTA efficient transformers, namely TransPath [37] and SwinTransformer [20], with throughput around 3\u00d7 and 4\u00d7 higher than the aforementioned architectures, respectively. Our code is publicly available1.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2154951058",
                        "name": "Thomas Stegm\u00fcller"
                    },
                    {
                        "authorId": "48292467",
                        "name": "A. Spahr"
                    },
                    {
                        "authorId": "1697559",
                        "name": "B. Bozorgtabar"
                    },
                    {
                        "authorId": "1710257",
                        "name": "J. Thiran"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "These methods have also been used for regularizing CNNs to focus on task-relevant features and to be robust against spurious features [18, 3, 8].",
                "This problem frequently occurs in image recognition because the context may change after the models are deployed [3, 5], as illustrated in Fig.",
                "However, in the real world, this assumption may not be valid and the distributions easily shift because of changing contexts in images such as in the co-occurrences between backgrounds and objects [3, 4, 5].",
                "Thus, to train robust and reliable CNNs, it is important to focus on taskrelevant features that are originally related to the task and are invariant to the training and test distributions [3, 8].",
                "hoc explanation modules for generating attention maps [3, 8]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "84346e2c6a270499d455b8ddaf6032a6fccbc218",
                "externalIds": {
                    "DBLP": "conf/icmcs/AdachiY22",
                    "ArXiv": "2202.04237",
                    "DOI": "10.1109/ICME52920.2022.9859838",
                    "CorpusId": 246679877
                },
                "corpusId": 246679877,
                "publicationVenue": {
                    "id": "975e1529-a14b-436f-ad7e-a09d8eabc50d",
                    "name": "IEEE International Conference on Multimedia and Expo",
                    "type": "conference",
                    "alternate_names": [
                        "ICME",
                        "Int Conf Multimedia Expo",
                        "IEEE Int Conf Multimedia Expo",
                        "International Conference on Multimedia and Expo"
                    ],
                    "issn": "1945-7871",
                    "alternate_issns": [
                        "2330-7927"
                    ],
                    "url": "http://csdl2.computer.org/persagen/ProceedingByFilter.jsp?filter=I",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conferences.jsp",
                        "http://www.wikicfp.com/cfp/program?id=1418"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/84346e2c6a270499d455b8ddaf6032a6fccbc218",
                "title": "Learning Robust Convolutional Neural Networks with Relevant Feature Focusing Via Explanations",
                "abstract": "Existing image recognition techniques based on convolutional neural networks (CNNs) basically assume that the training and test datasets are sampled from i.i.d distributions. However, this assumption is easily broken in the real world because of the distribution shift that occurs when the co-occurrence relations between objects and backgrounds in input images change. Under this type of distribution shift, CNNs learn to focus on features that are not task-relevant, such as backgrounds from the training data, and degrade their accuracy on the test data. To tackle this problem, we propose relevant feature focusing (ReFF). ReFF detects task-relevant features and regularizes CNNs via explanation outputs (e.g., Grad-CAM). Since ReFF is composed of post-hoc explanation modules, it can be easily applied to off-the-shelf CNNs. Furthermore, ReFF requires no additional inference cost at test time because it is only used for regularization while training. We demonstrate that CNNs trained with ReFF focus on features relevant to the target task and that ReFF improves the test-time accuracy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2056089543",
                        "name": "Kazuki Adachi"
                    },
                    {
                        "authorId": "36351779",
                        "name": "Shin'ya Yamaguchi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "As a result, the training is prone to inherit bias through classimbalance [19, 20] or co-occurrence statistics [21] from the training data, and neglects synergies in the real world, where there exist semantic similarities between classes."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dc62cfe0ac33377bb67388c75bf8e0716a2c3d71",
                "externalIds": {
                    "ArXiv": "2112.11366",
                    "DBLP": "journals/corr/abs-2112-11366",
                    "CorpusId": 245353421
                },
                "corpusId": 245353421,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/dc62cfe0ac33377bb67388c75bf8e0716a2c3d71",
                "title": "Contrastive Object Detection Using Knowledge Graph Embeddings",
                "abstract": "Object recognition for the most part has been approached as a one-hot problem that treats classes to be discrete and unrelated. Each image region has to be assigned to one member of a set of objects, including a background class, disregarding any similarities in the object types. In this work, we compare the error statistics of the class embeddings learned from a one-hot approach with semantically structured embeddings from natural language processing or knowledge graphs that are widely applied in open world object detection. Extensive experimental results on multiple knowledge-embeddings as well as distance metrics indicate that knowledge-based class representations result in more semantically grounded misclassifications while performing on par compared to one-hot methods on the challenging COCO and Cityscapes object detection benchmarks. We generalize our findings to multiple object detection architectures by proposing a knowledge-embedded design for keypoint-based and transformer-based object detection architectures.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2146550450",
                        "name": "Christopher Lang"
                    },
                    {
                        "authorId": "2149358116",
                        "name": "Alexander Braun"
                    },
                    {
                        "authorId": "2609831",
                        "name": "Abhinav Valada"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "47058c6e2e8a606c3c061f0e2eaba41262e492ab",
                "externalIds": {
                    "DBLP": "conf/eccv/ZhouKSGLZLKG22",
                    "ArXiv": "2112.05892",
                    "DOI": "10.1007/978-3-031-19833-5_15",
                    "CorpusId": 247594352
                },
                "corpusId": 247594352,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/47058c6e2e8a606c3c061f0e2eaba41262e492ab",
                "title": "COMPOSER: Compositional Reasoning of Group Activity in Videos with Keypoint-Only Modality",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2157475030",
                        "name": "Honglu Zhou"
                    },
                    {
                        "authorId": "2293919",
                        "name": "Asim Kadav"
                    },
                    {
                        "authorId": "1587627172",
                        "name": "Aviv Shamsian"
                    },
                    {
                        "authorId": "1947101",
                        "name": "Shijie Geng"
                    },
                    {
                        "authorId": "1868193",
                        "name": "Farley Lai"
                    },
                    {
                        "authorId": "33860220",
                        "name": "Long Zhao"
                    },
                    {
                        "authorId": "2000259498",
                        "name": "Tingxi Liu"
                    },
                    {
                        "authorId": "143980997",
                        "name": "M. Kapadia"
                    },
                    {
                        "authorId": "1775043",
                        "name": "H. Graf"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6d6f2cfe938b05308cce5a18e1b1943592251b8d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-01901",
                    "ArXiv": "2112.01901",
                    "DOI": "10.1109/WACV56688.2023.00152",
                    "CorpusId": 244896551
                },
                "corpusId": 244896551,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/6d6f2cfe938b05308cce5a18e1b1943592251b8d",
                "title": "The Box Size Confidence Bias Harms Your Object Detector",
                "abstract": "Countless applications depend on accurate predictions with reliable confidence estimates from modern object detectors. However, it is well known that neural networks, including object detectors, produce miscalibrated confidence estimates. Recent work even suggests that detectors\u2019 confidence predictions are biased with respect to object size and position. In object detection, the issues of conditional biases, confidence calibration, and task performance are usually explored in isolation, but, as we aim to show, they are closely related. We formally prove that the conditional confidence bias harms the performance of object detectors and empirically validate these findings. Specifically, to quantify the performance impact of the confidence bias on object detectors, we modify the histogram binning calibration to avoid performance impairment and instead improve it through calibration conditioned on the bounding box size. We further find that the confidence bias is also present in detections generated on the training data of the detector, which can be leveraged to perform the de-biasing. Moreover, we show that Test Time Augmentation (TTA) confounds this bias, which results in even more significant performance impairments on the detectors. Finally, we use our proposed algorithm to analyze a diverse set of object detection architectures and show that the conditional confidence bias harms their performance by up to 0.6 mAP and 0.8 mAP50. Code available at https://github.com/Blueblue4/Object-Detection-Confidence-Bias.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2047398341",
                        "name": "Johannes Gilg"
                    },
                    {
                        "authorId": "2047398193",
                        "name": "Torben Teepe"
                    },
                    {
                        "authorId": "2064289432",
                        "name": "Fabian Herzog"
                    },
                    {
                        "authorId": "145512909",
                        "name": "G. Rigoll"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "\u2026types of biases due to factors such as background, color, racial (Gwilliam et al. (2021)), gender (Tang et al. (2021); Zhao et al. (2017)), contextual (Singh et al. (2020)), co-occurrence (Petsiuk et al. (2021)), spatial noise, dataset (Tommasi et al. (2017)) and object-size (Nguyen et al. (2020))."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "55cf57efc27fda3c6c252374f5891902fdcfcba7",
                "externalIds": {
                    "ArXiv": "2111.07370",
                    "DBLP": "journals/corr/abs-2111-07370",
                    "DOI": "10.1016/j.cviu.2022.103532",
                    "CorpusId": 244117524
                },
                "corpusId": 244117524,
                "publicationVenue": {
                    "id": "5fbb417b-d7a5-44e6-856d-993f0624ed9c",
                    "name": "Computer Vision and Image Understanding",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Vis Image Underst"
                    ],
                    "issn": "1077-3142",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/622809/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/10773142",
                        "http://www.idealibrary.com/links/toc/cviu",
                        "https://www.journals.elsevier.com/computer-vision-and-image-understanding"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/55cf57efc27fda3c6c252374f5891902fdcfcba7",
                "title": "Co-segmentation Inspired Attention Module for Video-based Computer Vision Tasks",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "7598442",
                        "name": "Arulkumar Subramaniam"
                    },
                    {
                        "authorId": "2140462210",
                        "name": "Jayesh Vaidya"
                    },
                    {
                        "authorId": "2140464360",
                        "name": "Muhammed Ameen"
                    },
                    {
                        "authorId": "3265714",
                        "name": "Athira M. Nambiar"
                    },
                    {
                        "authorId": "50853059",
                        "name": "Anurag Mittal"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[40] proposed the use of statistical information to identify biased categories."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2ad05ca1ff540dbf286b95f0dca308b17f226b3f",
                "externalIds": {
                    "ArXiv": "2110.15499",
                    "DBLP": "journals/corr/abs-2110-15499",
                    "CorpusId": 240288902
                },
                "corpusId": 240288902,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/2ad05ca1ff540dbf286b95f0dca308b17f226b3f",
                "title": "UDIS: Unsupervised Discovery of Bias in Deep Visual Recognition Models",
                "abstract": "Deep learning models have been shown to learn spurious correlations from data that sometimes lead to systematic failures for certain subpopulations. Prior work has typically diagnosed this by crowdsourcing annotations for various protected attributes and measuring performance, which is both expensive to acquire and difficult to scale. In this work, we propose UDIS, an unsupervised algorithm for surfacing and analyzing such failure modes. UDIS identifies subpopulations via hierarchical clustering of dataset embeddings and surfaces systematic failure modes by visualizing low performing clusters along with their gradient-weighted class-activation maps. We show the effectiveness of UDIS in identifying failure modes in models trained for image classification on the CelebA and MSCOCO datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2136115527",
                        "name": "Arvindkumar Krishnakumar"
                    },
                    {
                        "authorId": "39351028",
                        "name": "Viraj Prabhu"
                    },
                    {
                        "authorId": "2136114132",
                        "name": "Sruthi Sudhakar"
                    },
                    {
                        "authorId": "50196944",
                        "name": "Judy Hoffman"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "In a supervised setting, we show that our fair selection algorithm achieves cv value of almost 0, thus reducing the representational bias and improving model performance over the baselines techniques [46, 23, 25, 10, 36].",
                "Dataset repair approaches are often criticized for possible data reduction due to re-sampling [36].",
                "Recent works [36] have also identified co-occurring bias between a pair of classes, and using class activation",
                "In [36] authors identified 20 most biased pair of categories in COCO such as (Skateboard, Person) where Skateboard mostly co-occurs with Person than exclusively.",
                "80K COCO images such that the number of images of minority class, with and without biased co-occurring class, becomes balanced for all pairs given by [36].",
                "Recent works [36] have proposed algorithmic changes to handle co-occurring bias in the dataset, which restricts its scope to be re-integrated in every application."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "a5c7bd7ccd2e3db114dd467304eb4e6e928c0ef8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-10389",
                    "ArXiv": "2110.10389",
                    "DOI": "10.1109/WACV51458.2022.00395",
                    "CorpusId": 239049762
                },
                "corpusId": 239049762,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/a5c7bd7ccd2e3db114dd467304eb4e6e928c0ef8",
                "title": "Does Data Repair Lead to Fair Models? Curating Contextually Fair Data To Reduce Model Bias",
                "abstract": "Contextual information is a valuable cue for Deep Neural Networks (DNNs) to learn better representations and improve accuracy. However, co-occurrence bias in the training dataset may hamper a DNNmodel\u2019s generalizabil- ity to unseen scenarios in the real world. For example, in COCO [26], many object categories have a much higher cooccurrence with men compared to women, which can bias a DNN\u2019s prediction in favor of men. Recent works have focused on task-specific training strategies to handle bias in such scenarios, but fixing the available data is often ignored. In this paper, we propose a novel and more generic solution to address the contextual bias in the datasets by selecting a subset of the samples, which is fair in terms of the co-occurrence with various classes for a protected attribute. We introduce a data repair algorithm using the coefficient of variation( cv), which can curate fair and contextually balanced data for a protected class(es). This helps in training a fair model irrespective of the task, architecture or training methodology. Our proposed solution is simple, effective and can even be used in an active learning setting where the data labels are not present or being generated incrementally. We demonstrate the effectiveness of our algorithm for the task of object detection and multi-label image classification across different datasets. Through a series of experiments, we validate that curating contextually fair data helps make model predictions fair by balancing the true positive rate for the protected class across groups without compromising on the model\u2019s overall performance. Code: https://github.com/sumanyumuku98/contextual-bias",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2114357941",
                        "name": "Sharat Agarwal"
                    },
                    {
                        "authorId": "1443437912",
                        "name": "Sumanyu Muku"
                    },
                    {
                        "authorId": "144024614",
                        "name": "Saket Anand"
                    },
                    {
                        "authorId": "145676235",
                        "name": "Chetan Arora"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "that the network may have inadvertently learned to use to make its decision [33]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2b5018a5d2776d4a76d26b0c3822906061cfa89c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-00527",
                    "ArXiv": "2110.00527",
                    "DOI": "10.1109/CVPR52688.2022.00997",
                    "CorpusId": 238253348
                },
                "corpusId": 238253348,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2b5018a5d2776d4a76d26b0c3822906061cfa89c",
                "title": "Consistent Explanations by Contrastive Learning",
                "abstract": "Post-hoc explanation methods, e.g., Grad-CAM, enable humans to inspect the spatial regions responsible for a particular network decision. However, it is shown that such explanations are not always consistent with human priors, such as consistency across image transformations. Given an interpretation algorithm, e.g., Grad-CAM, we introduce a novel training method to train the model to produce more consistent explanations. Since obtaining the ground truth for a desired model interpretation is not a well-defined task, we adopt ideas from contrastive self-supervised learning, and apply them to the interpretations of the model rather than its embeddings. We show that our method, Contrastive Grad-CAM Consistency (CGC), results in Grad-CAM interpretation heatmaps that are more consistent with human annotations while still achieving comparable classification accuracy. Moreover, our method acts as a regularizer and improves the accuracy on limited-data, fine-grained classification settings. In addition, because our method does not rely on annotations, it allows for the incorporation of unlabeled data into training, which enables better generalization of the model. The code is available here: https://github.com/UCDvision/CGC",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "101577633",
                        "name": "Vipin Pillai"
                    },
                    {
                        "authorId": "2004045536",
                        "name": "Soroush Abbasi Koohpayegani"
                    },
                    {
                        "authorId": "2130498243",
                        "name": "Ashley Ouligian"
                    },
                    {
                        "authorId": "2130466039",
                        "name": "Dennis Fong"
                    },
                    {
                        "authorId": "2367683",
                        "name": "H. Pirsiavash"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[31] used CAM [41] and feature-slitting methods to decorrelate category and its co-occurring context but only applicable to fixed category pairs."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0d8768aab838ec5c1af063fc95d22796fac05acf",
                "externalIds": {
                    "ArXiv": "2110.07118",
                    "DBLP": "journals/corr/abs-2110-07118",
                    "DOI": "10.1109/ICCVW54120.2021.00179",
                    "CorpusId": 238857299
                },
                "corpusId": 238857299,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0d8768aab838ec5c1af063fc95d22796fac05acf",
                "title": "Nuisance-Label Supervision: Robustness Improvement by Free Labels",
                "abstract": "In this paper, we present a Nuisance-label Supervision (NLS) module, which can make models more robust to nuisance factor variations. Nuisance factors are those irrelevant to a task, and an ideal model should be invariant to them. For example, an activity recognition model should perform consistently regardless of the change of clothes and background. But our experiments show existing models are far from this capability. So we explicitly supervise a model with nuisance labels to make extracted features less dependent on nuisance factors. Although the values of nuisance factors are rarely annotated, we demonstrate that besides existing annotations, nuisance labels can be acquired freely from data augmentation and synthetic data. Experiments show consistent improvement in robustness towards image corruption and appearance change in action recognition.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1491337573",
                        "name": "Xinyue Wei"
                    },
                    {
                        "authorId": "3256056",
                        "name": "Weichao Qiu"
                    },
                    {
                        "authorId": "2153910406",
                        "name": "Yi Zhang"
                    },
                    {
                        "authorId": "9381483",
                        "name": "Zihao Xiao"
                    },
                    {
                        "authorId": "145081362",
                        "name": "A. Yuille"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Other works use expensive sampling or refinement steps to increase the spatial extent of CAMs [34, 41, 42]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cd6f089a25a858b7ae47c5576c8bd62adbd342bb",
                "externalIds": {
                    "DBLP": "conf/iccv/BiertimpelSBB21",
                    "DOI": "10.1109/ICCV48922.2021.00282",
                    "CorpusId": 247181345
                },
                "corpusId": 247181345,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/cd6f089a25a858b7ae47c5576c8bd62adbd342bb",
                "title": "Prior to Segment: Foreground Cues for Weakly Annotated Classes in Partially Supervised Instance Segmentation",
                "abstract": "Instance segmentation methods require large datasets with expensive and thus limited instance-level mask labels. Partially supervised instance segmentation aims to improve mask prediction with limited mask labels by utilizing the more abundant weak box labels. In this work, we show that a class agnostic mask head, commonly used in partially supervised instance segmentation, has difficulties learning a general concept of foreground for the weakly annotated classes using box supervision only. To resolve this problem, we introduce an object mask prior (OMP) that provides the mask head with the general concept of foreground implicitly learned by the box classification head under the supervision of all classes. This helps the class agnostic mask head to focus on the primary object in a region of interest (RoI) and improves generalization to the weakly annotated classes. We test our approach on the COCO dataset using different splits of strongly and weakly supervised classes. Our approach significantly improves over the Mask R-CNN baseline and obtains competitive performance with the state-of-the-art, while offering a much simpler architecture. 1",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1468624513",
                        "name": "David Biertimpel"
                    },
                    {
                        "authorId": "51208845",
                        "name": "Sindi Shkodrani"
                    },
                    {
                        "authorId": "31582751",
                        "name": "A. S. Baslamisli"
                    },
                    {
                        "authorId": "1820934",
                        "name": "N. Baka"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "However, there are challenging scenarios for which DNNs have difficulty regardless of pre-training or architectural changes, such as highly occluded scenes, or objects appearing out of their normal context (Singh et al., 2020).",
                "However, there are challenging scenarios for which DNNs have difficulty regardless of pre-training or architectural changes, such as highly occluded scenes, or objects appearing out of their normal context [20]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7b4e0062ac079c96ce632ea205c9cb61e88aae9e",
                "externalIds": {
                    "DBLP": "journals/ijcv/IslamKDB23",
                    "ArXiv": "2108.09929",
                    "DOI": "10.1007/s11263-022-01720-7",
                    "CorpusId": 237267098
                },
                "corpusId": 237267098,
                "publicationVenue": {
                    "id": "939ee07c-6009-43f8-b884-69238b40659e",
                    "name": "International Journal of Computer Vision",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Comput Vis"
                    ],
                    "issn": "0920-5691",
                    "url": "https://www.springer.com/computer/image+processing/journal/11263",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11263",
                        "http://link.springer.com/journal/11263"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7b4e0062ac079c96ce632ea205c9cb61e88aae9e",
                "title": "SegMix: Co-occurrence Driven Mixup for Semantic Segmentation and Adversarial Robustness",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3240989",
                        "name": "Md. Amirul Islam"
                    },
                    {
                        "authorId": "2065442325",
                        "name": "M. Kowal"
                    },
                    {
                        "authorId": "3150825",
                        "name": "K. Derpanis"
                    },
                    {
                        "authorId": "2866780",
                        "name": "Neil D. B. Bruce"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                ", contextual bias [22], or adjacent object and background, i.",
                "Specifically, the co-occurrence of different objects is called contextual bias [22], and that of object and background is called background bias [23]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "37de04f1055d97acdec3d3710a8db219ba8e0273",
                "externalIds": {
                    "ArXiv": "2108.00049",
                    "DBLP": "journals/corr/abs-2108-00049",
                    "CorpusId": 236772573
                },
                "corpusId": 236772573,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/37de04f1055d97acdec3d3710a8db219ba8e0273",
                "title": "Object-aware Contrastive Learning for Debiased Scene Representation",
                "abstract": "Contrastive self-supervised learning has shown impressive results in learning visual representations from unlabeled images by enforcing invariance against different data augmentations. However, the learned representations are often contextually biased to the spurious scene correlations of different objects or object and background, which may harm their generalization on the downstream tasks. To tackle the issue, we develop a novel object-aware contrastive learning framework that first (a) localizes objects in a self-supervised manner and then (b) debias scene correlations via appropriate data augmentations considering the inferred object locations. For (a), we propose the contrastive class activation map (ContraCAM), which finds the most discriminative regions (e.g., objects) in the image compared to the other images using the contrastively trained models. We further improve the ContraCAM to detect multiple objects and entire shapes via an iterative refinement procedure. For (b), we introduce two data augmentations based on ContraCAM, object-aware random crop and background mixup, which reduce contextual and background biases during contrastive self-supervised learning, respectively. Our experiments demonstrate the effectiveness of our representation learning framework, particularly when trained under multi-object images or evaluated under the background (and distribution) shifted images.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9962692",
                        "name": "Sangwoo Mo"
                    },
                    {
                        "authorId": "2115457459",
                        "name": "H. Kang"
                    },
                    {
                        "authorId": "1729571",
                        "name": "Kihyuk Sohn"
                    },
                    {
                        "authorId": "2116729195",
                        "name": "Chun-Liang Li"
                    },
                    {
                        "authorId": "143720148",
                        "name": "Jinwoo Shin"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d4699efa686c7e046dfaf5c7261e6a48de523127",
                "externalIds": {
                    "ArXiv": "2107.07919",
                    "DBLP": "journals/cviu/FabbrizziPNK22",
                    "DOI": "10.1016/j.cviu.2022.103552",
                    "CorpusId": 236033931
                },
                "corpusId": 236033931,
                "publicationVenue": {
                    "id": "5fbb417b-d7a5-44e6-856d-993f0624ed9c",
                    "name": "Computer Vision and Image Understanding",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Vis Image Underst"
                    ],
                    "issn": "1077-3142",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/622809/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/10773142",
                        "http://www.idealibrary.com/links/toc/cviu",
                        "https://www.journals.elsevier.com/computer-vision-and-image-understanding"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d4699efa686c7e046dfaf5c7261e6a48de523127",
                "title": "A Survey on Bias in Visual Datasets",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2119886436",
                        "name": "Simone Fabbrizzi"
                    },
                    {
                        "authorId": "144178604",
                        "name": "S. Papadopoulos"
                    },
                    {
                        "authorId": "1804618",
                        "name": "Eirini Ntoutsi"
                    },
                    {
                        "authorId": "1715604",
                        "name": "Y. Kompatsiaris"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Models taking shortcuts were widely observed from various tasks, such as object detection (Singh et al., 2020), NLI (Tu et al.",
                "Models taking shortcuts were widely observed from various tasks, such as object detection (Singh et al., 2020), NLI (Tu et al., 2020), and also for our target task of multihop QA (Min et al., 2019; Chen and Durrett, 2019; Trivedi et al., 2020), where models learn simple heuristic rules, answering\u2026"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a0c51191f7b3fa8207b4401903c10b87e2458129",
                "externalIds": {
                    "DBLP": "conf/acl/LeeHHL20",
                    "ACL": "2021.acl-long.476",
                    "ArXiv": "2107.03242",
                    "DOI": "10.18653/v1/2021.acl-long.476",
                    "CorpusId": 235755349
                },
                "corpusId": 235755349,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/a0c51191f7b3fa8207b4401903c10b87e2458129",
                "title": "Robustifying Multi-hop QA through Pseudo-Evidentiality Training",
                "abstract": "This paper studies the bias problem of multi-hop question answering models, of answering correctly without correct reasoning. One way to robustify these models is by supervising to not only answer right, but also with right reasoning chains. An existing direction is to annotate reasoning chains to train models, requiring expensive additional annotations. In contrast, we propose a new approach to learn evidentiality, deciding whether the answer prediction is supported by correct evidences, without such annotations. Instead, we compare counterfactual changes in answer confidence with and without evidence sentences, to generate \u201cpseudo-evidentiality\u201d annotations. We validate our proposed model on an original set and challenge set in HotpotQA, showing that our method is accurate and robust in multi-hop reasoning.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "79733119",
                        "name": "Kyungjae Lee"
                    },
                    {
                        "authorId": "1716415",
                        "name": "Seung-won Hwang"
                    },
                    {
                        "authorId": "2115652956",
                        "name": "Sanghyun Han"
                    },
                    {
                        "authorId": "2135607329",
                        "name": "Dohyeon Lee"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "In a similar spirit to our work, (Singh et al., 2020) aim to identify and mitigate contextual bias, however, they assume access to the whole training setup and data; which is a limiting factor in practical scenarios."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "febaed20d1c13c29dac2ea1e5d35b176022c5f8b",
                "externalIds": {
                    "ArXiv": "2106.12723",
                    "DBLP": "conf/icml/AbidY022",
                    "CorpusId": 249674673
                },
                "corpusId": 249674673,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/febaed20d1c13c29dac2ea1e5d35b176022c5f8b",
                "title": "Meaningfully debugging model mistakes using conceptual counterfactual explanations",
                "abstract": "Understanding and explaining the mistakes made by trained models is critical to many machine learning objectives, such as improving robustness, addressing concept drift, and mitigating biases. However, this is often an ad hoc process that involves manually looking at the model\u2019s mistakes on many test samples and guessing at the underlying reasons for those incorrect predictions. In this paper, we propose a systematic approach, conceptual counterfactual explanations (CCE), that explains why a classi\ufb01er makes a mistake on a particular test sample(s) in terms of human-understandable concepts (e.g. this zebra is misclassi\ufb01ed as a dog because of faint stripes ). We base CCE on two prior ideas: counterfactual explanations and concept activation vectors, and validate our approach on well-known pretrained models, showing that it explains the models\u2019 mistakes meaningfully. In addition, for new models trained on data with spurious correlations, CCE accurately identi\ufb01es the spurious correlation as the cause of model mistakes from a single misclassi\ufb01ed test sample. On two challenging medical applications, CCE generated useful insights, con\ufb01rmed by clin-icians, into biases and mistakes the model makes in real-world settings. The code for CCE is publicly available at https://github.com/ mertyg/debug-mistakes-cce .",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144948925",
                        "name": "Abubakar Abid"
                    },
                    {
                        "authorId": "2186981598",
                        "name": "Mert Yuksekgonul"
                    },
                    {
                        "authorId": "145085305",
                        "name": "James Y. Zou"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Another family of methods [13, 14, 15] implicitly learn debiased representations by incorporating explanation during model training to suppress it from paying high attention to biased features in the original input.",
                "The second representative family of mitigation methods is based on explainability [13, 14, 15, 24]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4c2e9b401e6fe3b3b81799ae0837048fccaef0a6",
                "externalIds": {
                    "ArXiv": "2106.12674",
                    "DBLP": "journals/corr/abs-2106-12674",
                    "CorpusId": 235623731
                },
                "corpusId": 235623731,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4c2e9b401e6fe3b3b81799ae0837048fccaef0a6",
                "title": "Fairness via Representation Neutralization",
                "abstract": "Existing bias mitigation methods for DNN models primarily work on learning debiased encoders. This process not only requires a lot of instance-level annotations for sensitive attributes, it also does not guarantee that all fairness sensitive information has been removed from the encoder. To address these limitations, we explore the following research question: Can we reduce the discrimination of DNN models by only debiasing the classification head, even with biased representations as inputs? To this end, we propose a new mitigation technique, namely, Representation Neutralization for Fairness (RNF) that achieves fairness by debiasing only the task-specific classification head of DNN models. To this end, we leverage samples with the same ground-truth label but different sensitive attributes, and use their neutralized representations to train the classification head of the DNN model. The key idea of RNF is to discourage the classification head from capturing spurious correlation between fairness sensitive information in encoder representations with specific class labels. To address low-resource settings with no access to sensitive attribute annotations, we leverage a bias-amplified model to generate proxy annotations for sensitive attributes. Experimental results over several benchmark datasets demonstrate our RNF framework to effectively reduce discrimination of DNN models with minimal degradation in task-specific performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3432460",
                        "name": "Mengnan Du"
                    },
                    {
                        "authorId": "2153292652",
                        "name": "Subhabrata Mukherjee"
                    },
                    {
                        "authorId": "32780441",
                        "name": "Guanchu Wang"
                    },
                    {
                        "authorId": "2057059798",
                        "name": "Ruixiang Tang"
                    },
                    {
                        "authorId": "2072795428",
                        "name": "A. Awadallah"
                    },
                    {
                        "authorId": "48539382",
                        "name": "Xia Hu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Object hallucination by deep detectors can be causes by sensitivity to the absolute position in the image [14, 15] while also affected by scene context [16, 17, 18, 19, 20]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5fa05d7701a4fd08507d6bd9f60499772d08e3c3",
                "externalIds": {
                    "DBLP": "conf/icip/KayhanVG21",
                    "ArXiv": "2106.02523",
                    "MAG": "3193613442",
                    "DOI": "10.1109/ICIP42928.2021.9506670",
                    "CorpusId": 235352929
                },
                "corpusId": 235352929,
                "publicationVenue": {
                    "id": "b6369c33-5d70-463c-8e82-95a54efa3cc8",
                    "name": "International Conference on Information Photonics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Image Process",
                        "ICIP",
                        "Int Conf Inf Photonics",
                        "International Conference on Image Processing"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5fa05d7701a4fd08507d6bd9f60499772d08e3c3",
                "title": "Hallucination In Object Detection \u2014 A Study In Visual Part VERIFICATION",
                "abstract": "We show that object detectors can hallucinate and detect missing objects; potentially even accurately localized at their expected, but non-existing, position. This is particularly problematic for applications that rely on visual part verification: detecting if an object part is present or absent. We show how popular object detectors hallucinate objects in a visual part verification task and introduce the first visual part verification dataset: DelftBikes 1, which has 10,000 bike photographs, with 22 densely annotated parts per image, where some parts may be missing. We explicitly annotated an extra object state label for each part to reflect if a part is missing or intact. We propose to evaluate visual part verification by relying on recall and compare popular object detectors on DelftBikes.1https://github.com/oskyhn/DelftBikes",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50311569",
                        "name": "O. Kayhan"
                    },
                    {
                        "authorId": "2351159",
                        "name": "Bart Vredebregt"
                    },
                    {
                        "authorId": "1738975",
                        "name": "J. V. Gemert"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "[6] follow a similar principle and look for object pairs such that the presence of one object increases the prediction probability of the other object.",
                ", tie-cat) while prior work [3, 5, 6] has only found positive SPs (e.",
                "These data augmentation or regularization based approaches represent SPIRE\u2019s most direct competition and, as a result, we compare against \u201cRight for the Right Reasons\u201d (RRR) [1], \u201cQuantifying and Controlling the Effects of Context\u201d (QCEC) [3], \u201cContextual Decomposition Explanation Penalization\u201d (CDEP) [4], \u201cGradient Supervision\u201d (GS) [5], and the \u201cFeature Splitting\u201d (FS) method from [6] because they all directly apply to image classification.",
                "We compare SPIRE to RRR [1], QCEC [3], CDEP [4], GS [5], and FS [6] (Section 2).",
                "These methods have been found to be less effective than methods that use data augmentation or regularization [4, 6, 13, 15]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "1e58f1e94a03ef6434ce5e3360781d546f8a2f5b",
                "externalIds": {
                    "DBLP": "journals/tmlr/PlumbRT22",
                    "ArXiv": "2106.02112",
                    "CorpusId": 235352578
                },
                "corpusId": 235352578,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1e58f1e94a03ef6434ce5e3360781d546f8a2f5b",
                "title": "Finding and Fixing Spurious Patterns with Explanations",
                "abstract": "Image classifiers often use spurious patterns, such as\"relying on the presence of a person to detect a tennis racket, which do not generalize. In this work, we present an end-to-end pipeline for identifying and mitigating spurious patterns for such models, under the assumption that we have access to pixel-wise object-annotations. We start by identifying patterns such as\"the model's prediction for tennis racket changes 63% of the time if we hide the people.\"Then, if a pattern is spurious, we mitigate it via a novel form of data augmentation. We demonstrate that our method identifies a diverse set of spurious patterns and that it mitigates them by producing a model that is both more accurate on a distribution where the spurious pattern is not helpful and more robust to distribution shift.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "31929250",
                        "name": "Gregory Plumb"
                    },
                    {
                        "authorId": "78846919",
                        "name": "Marco Tulio Ribeiro"
                    },
                    {
                        "authorId": "145532827",
                        "name": "Ameet Talwalkar"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "However, it is unwanted since we know that such biases may not exist in some final test images (Singh et al. 2020)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "13e5c87d143940a40ffcfa750470711c810e7d59",
                "externalIds": {
                    "MAG": "3130355594",
                    "DBLP": "conf/aaai/PillaiP21",
                    "DOI": "10.13016/M2ORPW-45BJ",
                    "CorpusId": 231401138
                },
                "corpusId": 231401138,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/13e5c87d143940a40ffcfa750470711c810e7d59",
                "title": "Explainable Models with Consistent Interpretations",
                "abstract": "Given the widespread deployment of black box deep neural networks in computer vision applications, the interpretability aspect of these black box systems has recently gained traction. Various methods have been proposed to explain the results of such deep neural networks. However, some recent works have shown that such explanation methods are biased and do not produce consistent interpretations. Hence, rather than introducing a novel explanation method, we learn models that are encouraged to be interpretable given an explanation method. We use Grad-CAM as the explanation algorithm and encourage the network to learn consistent interpretations along with maximizing the log-likelihood of the correct class. We show that our method outperforms the baseline on the pointing game evaluation on ImageNet and MS-COCO datasets respectively. We also introduce new evaluation metrics that penalize the saliency map if it lies outside the ground truth bounding box or segmentation mask, and show that our method outperforms the baseline on these metrics as well. Moreover, our model trained with interpretation consistency generalizes to other explanation algorithms on all the evaluation metrics. The code and models are publicly available.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "101577633",
                        "name": "Vipin Pillai"
                    },
                    {
                        "authorId": "2367683",
                        "name": "H. Pirsiavash"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "[9] point out the dangers of contextual bias in visual recognition datasets.",
                "[9] proposes two methods for mitigating such contextual biases and improving the robustness of the learnt feature representations."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "65eeacbd293fe896bf7dbbe018ff1362a7140d05",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-13582",
                    "ArXiv": "2104.13582",
                    "DOI": "10.5281/zenodo.4834352",
                    "CorpusId": 233423439
                },
                "corpusId": 233423439,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/65eeacbd293fe896bf7dbbe018ff1362a7140d05",
                "title": "[Re] Don't Judge an Object by Its Context: Learning to Overcome Contextual Bias",
                "abstract": "Singh et al. (2020) point out the dangers of contextual bias in visual recognition datasets. They propose two methods, CAM-based and feature-split, that better recognize an object or attribute in the absence of its typical context while maintaining competitive within-context accuracy. To verify their performance, we attempted to reproduce all 12 tables in the original paper, including those in the appendix. We also conducted additional experiments to better understand the proposed methods, including increasing the regularization in CAM-based and removing the weighted loss in feature-split. As the original code was not made available, we implemented the entire pipeline from scratch in PyTorch 1.7.0. Our implementation is based on the paper and email exchanges with the authors. We found that both proposed methods in the original paper help mitigate contextual bias, although for some methods, we could not completely replicate the quantitative results in the paper even after completing an extensive hyperparameter search. For example, on COCO-Stuff, DeepFashion, and UnRel, our feature-split model achieved an increase in accuracy on out-of-context images over the standard baseline, whereas on AwA, we saw a drop in performance. For the proposed CAM-based method, we were able to reproduce the original paper's results to within 0.5$\\%$ mAP. Our implementation can be found at https://github.com/princetonvisualai/ContextualBias.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109648781",
                        "name": "Sunnie S. Y. Kim"
                    },
                    {
                        "authorId": "2145443664",
                        "name": "Sharon Zhang"
                    },
                    {
                        "authorId": "49947642",
                        "name": "Nicole Meister"
                    },
                    {
                        "authorId": "2192178",
                        "name": "Olga Russakovsky"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "This is because the top-down approaches are strongly biased toward contextual cues from seen classes [35]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "57076b4306819203521d14dfad08693c2f5452f7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-04691",
                    "ArXiv": "2104.04691",
                    "DOI": "10.1109/ICCV48922.2021.01060",
                    "CorpusId": 233209798
                },
                "corpusId": 233209798,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/57076b4306819203521d14dfad08693c2f5452f7",
                "title": "Unidentified Video Objects: A Benchmark for Dense, Open-World Segmentation",
                "abstract": "Current state-of-the-art object detection and segmentation methods work well under the closed-world assumption. This closed-world setting assumes that the list of object categories is available during training and deployment. However, many real-world applications require detecting or segmenting novel objects, i.e., object categories never seen during training. In this paper, we present, UVO (Unidentified Video Objects), a new benchmark for openworld class-agnostic object segmentation in videos. Besides shifting the focus to the open-world setup, UVO is significantly larger, providing approximately 6 times more videos compared with DAVIS, and 7 times more mask (instance) annotations per video compared with YouTube-VO(I)S. UVO is also more challenging as it includes many videos with crowded scenes and complex background motions. We also demonstrated that UVO can be used for other applications, such as object tracking and super-voxel segmentation. We believe that UVO is a versatile testbed for researchers to develop novel approaches for open-world class-agnostic object segmentation, and inspires new research directions towards a more comprehensive video understanding beyond classification and detection.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "7634810",
                        "name": "Weiyao Wang"
                    },
                    {
                        "authorId": "3429328",
                        "name": "Matt Feiszli"
                    },
                    {
                        "authorId": "46506697",
                        "name": "Heng Wang"
                    },
                    {
                        "authorId": "1687325",
                        "name": "Du Tran"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "ImageNet [21] rely implicitly but strongly on context [15, 4, 31]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ec23ec36e724f87683f162b6f86e655973063787",
                "externalIds": {
                    "DBLP": "conf/iccv/BomatterZKMTK21",
                    "ArXiv": "2104.02215",
                    "DOI": "10.1109/ICCV48922.2021.00032",
                    "CorpusId": 233033825,
                    "PubMed": "36051852"
                },
                "corpusId": 233033825,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/ec23ec36e724f87683f162b6f86e655973063787",
                "title": "When Pigs Fly: Contextual Reasoning in Synthetic and Natural Scenes",
                "abstract": "Context is of fundamental importance to both human and machine vision; e.g., an object in the air is more likely to be an airplane than a pig. The rich notion of context incorporates several aspects including physics rules, statistical co-occurrences, and relative object sizes, among others. While previous work has focused on crowd-sourced out-of-context photographs from the web to study scene context, controlling the nature and extent of contextual violations has been a daunting task. Here we introduce a diverse, synthetic Out-of-Context Dataset (OCD) with fine-grained control over scene context. By leveraging a 3D simulation engine, we systematically control the gravity, object co-occurrences and relative sizes across 36 object categories in a virtual household environment. We conducted a series of experiments to gain insights into the impact of contextual cues on both human and machine vision using OCD. We conducted psychophysics experiments to establish a human benchmark for out-of-context recognition, and then compared it with state-of-the-art computer vision models to quantify the gap between the two. We propose a context-aware recognition transformer model, fusing object and contextual information via multi-head attention. Our model captures useful information for contextual reasoning, enabling human-level performance and better robustness in out-of-context conditions compared to baseline models across OCD and other out-of-context datasets. All source code and data are publicly available at https://github.com/kreimanlab/WhenPigsFlyContext",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1581864488",
                        "name": "Philipp Bomatter"
                    },
                    {
                        "authorId": "2418491",
                        "name": "Mengmi Zhang"
                    },
                    {
                        "authorId": "46523251",
                        "name": "D. Karev"
                    },
                    {
                        "authorId": "7232330",
                        "name": "Spandan Madan"
                    },
                    {
                        "authorId": "2052424999",
                        "name": "Claire Tseng"
                    },
                    {
                        "authorId": "2066787605",
                        "name": "Gabriel Kreiman"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1637b9e2b102dfeea37cf608fa5e85b21188778b",
                "externalIds": {
                    "PubMedCentral": "8521929",
                    "ArXiv": "2103.03048",
                    "DBLP": "journals/corr/abs-2103-03048",
                    "DOI": "10.3389/fdgth.2021.671015",
                    "CorpusId": 232110891,
                    "PubMed": "34713144"
                },
                "corpusId": 232110891,
                "publicationVenue": {
                    "id": "d325aacd-7bff-49a6-a4c1-f9c82f48af34",
                    "name": "Frontiers in Digital Health",
                    "type": "journal",
                    "alternate_names": [
                        "Front Digit Health"
                    ],
                    "issn": "2673-253X",
                    "url": "https://www.frontiersin.org/journals/digital-health#"
                },
                "url": "https://www.semanticscholar.org/paper/1637b9e2b102dfeea37cf608fa5e85b21188778b",
                "title": "Detecting Spurious Correlations With Sanity Tests for Artificial Intelligence Guided Radiology Systems",
                "abstract": "Artificial intelligence (AI) has been successful at solving numerous problems in machine perception. In radiology, AI systems are rapidly evolving and show progress in guiding treatment decisions, diagnosing, localizing disease on medical images, and improving radiologists' efficiency. A critical component to deploying AI in radiology is to gain confidence in a developed system's efficacy and safety. The current gold standard approach is to conduct an analytical validation of performance on a generalization dataset from one or more institutions, followed by a clinical validation study of the system's efficacy during deployment. Clinical validation studies are time-consuming, and best practices dictate limited re-use of analytical validation data, so it is ideal to know ahead of time if a system is likely to fail analytical or clinical validation. In this paper, we describe a series of sanity tests to identify when a system performs well on development data for the wrong reasons. We illustrate the sanity tests' value by designing a deep learning system to classify pancreatic cancer seen in computed tomography scans.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "10388629",
                        "name": "U. Mahmood"
                    },
                    {
                        "authorId": "153677280",
                        "name": "Robik Shrestha"
                    },
                    {
                        "authorId": "39704097",
                        "name": "D. Bates"
                    },
                    {
                        "authorId": "7704297",
                        "name": "L. Mannelli"
                    },
                    {
                        "authorId": "15334539",
                        "name": "G. Corrias"
                    },
                    {
                        "authorId": "3330252",
                        "name": "Y. Erdi"
                    },
                    {
                        "authorId": "3290098",
                        "name": "Christopher Kanan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Further mitigation techniques are outside of our scope, but we look to works like Singh et al. (2020); Wang et al. (2019); Agarwal et al. (2020).\nvery few situations in which predicting sensitive attributes makes sense (Scheuerman et al., 2020; Larson, 2017), so we should carefully consider if this\u2026"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "23532ba2fa468ee3cc0ba578c5536ebc1f3cceb7",
                "externalIds": {
                    "ArXiv": "2102.12594",
                    "DBLP": "conf/icml/WangR21",
                    "CorpusId": 232046102
                },
                "corpusId": 232046102,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/23532ba2fa468ee3cc0ba578c5536ebc1f3cceb7",
                "title": "Directional Bias Amplification",
                "abstract": "Mitigating bias in machine learning systems requires refining our understanding of bias propagation pathways: from societal structures to large-scale data to trained models to impact on society. In this work, we focus on one aspect of the problem, namely bias amplification: the tendency of models to amplify the biases present in the data they are trained on. A metric for measuring bias amplification was introduced in the seminal work by Zhao et al. (2017); however, as we demonstrate, this metric suffers from a number of shortcomings including conflating different types of bias amplification and failing to account for varying base rates of protected attributes. We introduce and analyze a new, decoupled metric for measuring bias amplification, $\\text{BiasAmp}_{\\rightarrow}$ (Directional Bias Amplification). We thoroughly analyze and discuss both the technical assumptions and normative implications of this metric. We provide suggestions about its measurement by cautioning against predicting sensitive attributes, encouraging the use of confidence intervals due to fluctuations in the fairness of models across runs, and discussing the limitations of what this metric captures. Throughout this paper, we work to provide an interrogative look at the technical measurement of bias amplification, guided by our normative ideas of what we want it to encompass. Code is located at https://github.com/princetonvisualai/directional-bias-amp",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "46991154",
                        "name": "Angelina Wang"
                    },
                    {
                        "authorId": "2192178",
                        "name": "Olga Russakovsky"
                    }
                ]
            }
        },
        {
            "intents": [
                "result"
            ],
            "contexts": [
                "Note that our approach is similar to previous work that addresses contextual bias [39]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "01b83275e3e71f8568271a14817f1dc40fe4b7fa",
                "externalIds": {
                    "MAG": "3105250724",
                    "ArXiv": "2011.05558",
                    "DBLP": "journals/corr/abs-2011-05558",
                    "DOI": "10.1109/CVPR46437.2021.01279",
                    "CorpusId": 226300123
                },
                "corpusId": 226300123,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/01b83275e3e71f8568271a14817f1dc40fe4b7fa",
                "title": "Intentonomy: a Dataset and Study towards Human Intent Understanding",
                "abstract": "An image is worth a thousand words, conveying information that goes beyond the mere visual content therein. In this paper, we study the intent behind social media images with an aim to analyze how visual information can facilitate recognition of human intent. Towards this goal, we introduce an intent dataset, Intentonomy, comprising 14K images covering a wide range of everyday scenes. These images are manually annotated with 28 intent categories derived from a social psychology taxonomy. We then systematically study whether, and to what extent, commonly used visual information, i.e., object and context, contribute to human motive understanding. Based on our findings, we conduct further study to quantify the effect of attending to object and context classes as well as textual information in the form of hashtags when training an intent classifier. Our results quantitatively and qualitatively shed light on how visual and textual information can produce observable effects when predicting intent.1",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51502783",
                        "name": "Menglin Jia"
                    },
                    {
                        "authorId": "3099139",
                        "name": "Zuxuan Wu"
                    },
                    {
                        "authorId": "3207112",
                        "name": "A. Reiter"
                    },
                    {
                        "authorId": "1748501",
                        "name": "Claire Cardie"
                    },
                    {
                        "authorId": "50172592",
                        "name": "Serge J. Belongie"
                    },
                    {
                        "authorId": "38760573",
                        "name": "Ser-Nam Lim"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Additional work has used multi-label annotations (Singh et al., 2020), pixel-level annotations (Hendricks et al.",
                "Additional work has used multi-label annotations (Singh et al., 2020), pixel-level annotations (Hendricks et al., 2018), or other annotated features (Kim et al., 2019) to help train debiased models, although these kinds of annotations will not always be available."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "55188532ea9ea4d4a4872ebe79fc3add14cd34dd",
                "externalIds": {
                    "DBLP": "conf/emnlp/ClarkYZ20",
                    "MAG": "3102259594",
                    "ArXiv": "2011.03856",
                    "ACL": "2020.findings-emnlp.272",
                    "DOI": "10.18653/v1/2020.findings-emnlp.272",
                    "CorpusId": 226282321
                },
                "corpusId": 226282321,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/55188532ea9ea4d4a4872ebe79fc3add14cd34dd",
                "title": "Learning to Model and Ignore Dataset Bias with Mixed Capacity Ensembles",
                "abstract": "Many datasets have been shown to contain incidental correlations created by idiosyncrasies in the data collection process. For example, sentence entailment datasets can have spurious word-class correlations if nearly all contradiction sentences contain the word \u201cnot\u201d, and image recognition datasets can have tell-tale object-background correlations if dogs are always indoors. In this paper, we propose a method that can automatically detect and ignore these kinds of dataset-specific patterns, which we call dataset biases. Our method trains a lower capacity model in an ensemble with a higher capacity model. During training, the lower capacity model learns to capture relatively shallow correlations, which we hypothesize are likely to reflect dataset bias. This frees the higher capacity model to focus on patterns that should generalize better. We ensure the models learn non-overlapping approaches by introducing a novel method to make them conditionally independent. Importantly, our approach does not require the bias to be known in advance. We evaluate performance on synthetic datasets, and four datasets built to penalize models that exploit known biases on textual entailment, visual question answering, and image recognition tasks. We show improvement in all settings, including a 10 point gain on the visual question answering dataset.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "143997772",
                        "name": "Christopher Clark"
                    },
                    {
                        "authorId": "2064210",
                        "name": "Mark Yatskar"
                    },
                    {
                        "authorId": "1982950",
                        "name": "Luke Zettlemoyer"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[12] also proposed a disentangling method to split the representation in half and selectively learns the feature to mitigate contextual bias issue in the object recognition task.",
                "On the other hand, disentangling methods [2], [12] are introduced to split the representation into sub-representations as target and protected attribute information."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2fdaa1dcf798fdbd73ff031a434cbe2a5fc48e42",
                "externalIds": {
                    "DBLP": "conf/ictc/0004PHKJB20",
                    "DOI": "10.1109/ICTC49870.2020.9289379",
                    "CorpusId": 229374484
                },
                "corpusId": 229374484,
                "publicationVenue": {
                    "id": "9aa57e01-2dc2-4422-93b7-2b628326f78c",
                    "name": "Information and Communication Technology Convergence",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Information and Communication Technology Convergence",
                        "Inf Commun Technol Converg",
                        "Int Conf Inf Commun Technol Converg",
                        "ICTC"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2fdaa1dcf798fdbd73ff031a434cbe2a5fc48e42",
                "title": "Resampling Strategy for Mitigating Unfairness in Face Attribute Classification",
                "abstract": "With the widespread success of artificial intelligence (AI) systems, various applications based on the systems are being applied to our daily life. However, AI systems also raise societal problems since it is highly dependent on training datasets with bias. Consequently, concerning about trustworthiness in AI systems becomes a popular research topic, and recent studies reveal unfairness in developed models. In this paper, we propose a new batch sampling strategy considering fairness among demographic groups. Unlike conventional batch sampling methods such as under-sampling or oversampling, we reflect the notion of fairness directly to estimate the batch sampling probability of data. We empirically demonstrate that our batch sampling method achieves fairer results compared to prior methods in image classification tasks on CelebA and UTKFace datasets.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "38332102",
                        "name": "D. Kim"
                    },
                    {
                        "authorId": "2108106364",
                        "name": "Sungho Park"
                    },
                    {
                        "authorId": "145864562",
                        "name": "Sunhee Hwang"
                    },
                    {
                        "authorId": "9559582",
                        "name": "Minsong Ki"
                    },
                    {
                        "authorId": "2041291264",
                        "name": "Seogkyu Jeon"
                    },
                    {
                        "authorId": "144036125",
                        "name": "H. Byun"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "97299dca8ded6f948fc0f3c3b9a502be72fba983",
                "externalIds": {
                    "DBLP": "conf/mlhc/BeerEGES20",
                    "ArXiv": "2008.10936",
                    "MAG": "3107080942",
                    "CorpusId": 221293355
                },
                "corpusId": 221293355,
                "publicationVenue": {
                    "id": "6171bcff-8306-41c7-af12-fa1d87117cf1",
                    "name": "Machine Learning in Health Care",
                    "type": "conference",
                    "alternate_names": [
                        "MLHC",
                        "Mach Learn Health Care"
                    ],
                    "url": "http://mucmd.org"
                },
                "url": "https://www.semanticscholar.org/paper/97299dca8ded6f948fc0f3c3b9a502be72fba983",
                "title": "Using Deep Networks for Scientific Discovery in Physiological Signals",
                "abstract": "Deep neural networks (DNN) have shown remarkable success in the classification of physiological signals. In this study we propose a method for examining to what extent does a DNN's performance rely on rediscovering existing features of the signals, as opposed to discovering genuinely new features. Moreover, we offer a novel method of \"removing\" a hand-engineered feature from the network's hypothesis space, thus forcing it to try and learn representations which are different from known ones, as a method of scientific exploration. We then build on existing work in the field of interpretability, specifically class activation maps, to try and infer what new features the network has learned. We demonstrate this approach using ECG and EEG signals. With respect to ECG signals we show that for the specific task of classifying atrial fibrillation, DNNs are likely rediscovering known features. We also show how our method could be used to discover new features, by selectively removing some ECG features and \"rediscovering\" them. We further examine how could our method be used as a tool for examining scientific hypotheses. We simulate this scenario by looking into the importance of eye movements in classifying sleep from EEG. We show that our tool can successfully focus a researcher's attention by bringing to light patterns in the data that would be hidden otherwise.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2066991335",
                        "name": "Tom Beer"
                    },
                    {
                        "authorId": "1905995870",
                        "name": "Bar Eini-Porat"
                    },
                    {
                        "authorId": "144890784",
                        "name": "S. Goodfellow"
                    },
                    {
                        "authorId": "1954568",
                        "name": "D. Eytan"
                    },
                    {
                        "authorId": "2304764",
                        "name": "Uri Shalit"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "However, there are challenging scenarios for which DNNs have difficulty on regardless of pre-training or architectural changes, such as highly occluded scenes, or objects appearing out of their normal context [35]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8b03d96695761c66153cd8be65e62d148b058f87",
                "externalIds": {
                    "DBLP": "conf/bmvc/IslamKDB20",
                    "ArXiv": "2008.05667",
                    "MAG": "3048534249",
                    "CorpusId": 221112708
                },
                "corpusId": 221112708,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/8b03d96695761c66153cd8be65e62d148b058f87",
                "title": "Feature Binding with Category-Dependant MixUp for Semantic Segmentation and Adversarial Robustness",
                "abstract": "In this paper, we present a strategy for training convolutional neural networks to effectively resolve interference arising from competing hypotheses relating to inter-categorical information throughout the network. The premise is based on the notion of feature binding, which is defined as the process by which activation's spread across space and layers in the network are successfully integrated to arrive at a correct inference decision. In our work, this is accomplished for the task of dense image labelling by blending images based on their class labels, and then training a feature binding network, which simultaneously segments and separates the blended images. Subsequent feature denoising to suppress noisy activations reveals additional desirable properties and high degrees of successful predictions. Through this process, we reveal a general mechanism, distinct from any prior methods, for boosting the performance of the base segmentation network while simultaneously increasing robustness to adversarial attacks.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3240989",
                        "name": "Md. Amirul Islam"
                    },
                    {
                        "authorId": "2065442325",
                        "name": "M. Kowal"
                    },
                    {
                        "authorId": "3150825",
                        "name": "K. Derpanis"
                    },
                    {
                        "authorId": "2866780",
                        "name": "Neil D. B. Bruce"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Previous studies define an attribute, which people should not be discriminated against by, as the protected attribute and propose various fairness methods [4, 5, 25, 34, 22, 10].",
                "protected attribute information since the protected attribute is correlated with other attributes in the real world dataset [26, 11, 25]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "679afb74a671f6aa5f3524e13f078137b6941296",
                "externalIds": {
                    "ArXiv": "2007.03775",
                    "DBLP": "journals/corr/abs-2007-03775",
                    "MAG": "3041619459",
                    "CorpusId": 220403349
                },
                "corpusId": 220403349,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/679afb74a671f6aa5f3524e13f078137b6941296",
                "title": "README: REpresentation learning by fairness-Aware Disentangling MEthod",
                "abstract": "Fair representation learning aims to encode invariant representation with respect to the protected attribute, such as gender or age. In this paper, we design Fairness-aware Disentangling Variational AutoEncoder (FD-VAE) for fair representation learning. This network disentangles latent space into three subspaces with a decorrelation loss that encourages each subspace to contain independent information: 1) target attribute information, 2) protected attribute information, 3) mutual attribute information. After the representation learning, this disentangled representation is leveraged for fairer downstream classification by excluding the subspace with the protected attribute information. We demonstrate the effectiveness of our model through extensive experiments on CelebA and UTK Face datasets. Our method outperforms the previous state-of-the-art method by large margins in terms of equal opportunity and equalized odds.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "48079221",
                        "name": "Sungho Park"
                    },
                    {
                        "authorId": "38332102",
                        "name": "D. Kim"
                    },
                    {
                        "authorId": "145864562",
                        "name": "Sunhee Hwang"
                    },
                    {
                        "authorId": "144036125",
                        "name": "H. Byun"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Similarly, this need may be found in analyzing scene graphs, object co-currency, and contextual information [20, 26, 35]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0e3243e43f3cb27461faa6d5499df9caadefbc2a",
                "externalIds": {
                    "ArXiv": "2006.08601",
                    "DBLP": "journals/corr/abs-2006-08601",
                    "MAG": "3035560525",
                    "DOI": "10.1109/ICCV48922.2021.00126",
                    "CorpusId": 219708751
                },
                "corpusId": 219708751,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/0e3243e43f3cb27461faa6d5499df9caadefbc2a",
                "title": "Explaining Local, Global, And Higher-Order Interactions In Deep Learning",
                "abstract": "We present a simple yet highly generalizable method for explaining interacting parts within a neural network\u2019s reasoning process. First, we design an algorithm based on cross derivatives for computing statistical interaction effects between individual features, which is generalized to both 2-way and higher-order (3-way or more) interactions. We present results side by side with a weight-based attribution technique, corroborating that cross derivatives are a superior metric for both 2-way and higher-order interaction detection. Moreover, we extend the use of cross derivatives as an explanatory device in neural networks to the computer vision setting by expanding Grad-CAM, a popular gradient-based explanatory tool for CNNs, to the higher order. While Grad-CAM can only explain the importance of individual objects in images, our method, which we call Taylor-CAM, can explain a neural network\u2019s relational reasoning across multiple objects. We show the success of our explanations both qualitatively and quantitatively, including with a user study. We will release all code as a tool package to facilitate explainable deep learning.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1751441600",
                        "name": "Samuel Lerman"
                    },
                    {
                        "authorId": "2026123",
                        "name": "Chenliang Xu"
                    },
                    {
                        "authorId": "80640270",
                        "name": "C. Venuto"
                    },
                    {
                        "authorId": "1690271",
                        "name": "Henry A. Kautz"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "\u2022 Image classifiers have been shown to be fragile when objects from one image are transplanted in another image [12], and can be biased by object context [13, 14].",
                "[14] Krishna Kumar Singh, Dhruv Mahajan, Kristen Grauman, Yong Jae Lee, Matt Feiszli, and Deepti Ghadiyaram."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c106a1a0adbec8194049cde8cd20fd06c80eabde",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2003-08907",
                    "MAG": "3012435359",
                    "ArXiv": "2003.08907",
                    "CorpusId": 213002712
                },
                "corpusId": 213002712,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c106a1a0adbec8194049cde8cd20fd06c80eabde",
                "title": "Overinterpretation reveals image classification model pathologies",
                "abstract": "Image classifiers are typically scored on their test set accuracy, but high accuracy can mask a subtle type of model failure. We find that high scoring convolutional neural networks (CNN) exhibit troubling pathologies that allow them to display high accuracy even in the absence of semantically salient features. When a model provides a high-confidence decision without salient supporting input features we say that the classifier has overinterpreted its input, finding too much class-evidence in patterns that appear nonsensical to humans. Here, we demonstrate that state of the art neural networks for CIFAR-10 and ImageNet suffer from overinterpretation, and find CIFAR-10 trained models make confident predictions even when 95% of an input image has been masked and humans are unable to discern salient features in the remaining pixel subset. Although these patterns portend potential model fragility in real-world deployment, they are in fact valid statistical patterns of the image classification benchmark that alone suffice to attain high test accuracy. We find that ensembling strategies can help mitigate model overinterpretation, and classifiers which rely on more semantically meaningful features can improve accuracy over both the test set and out-of-distribution images from a different source than the training data.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "46286987",
                        "name": "Brandon Carter"
                    },
                    {
                        "authorId": "1971171",
                        "name": "Siddhartha Jain"
                    },
                    {
                        "authorId": "153430733",
                        "name": "Jonas W. Mueller"
                    },
                    {
                        "authorId": "143816853",
                        "name": "David Gifford"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "What blindspots do we study? We consider blindspots discovered by prior works (Hohman et al., 2019; Sohoni et al., 2020; Singh et al., 2020; Plumb et al., 2022; Singla et al., 2021) as well as novel blindspots (Adebayo et al.",
                "\u2022 Access to metadata help define coherent subsets of the data (e.g., Kim et al., 2018; Buolamwini & Gebru, 2018; Singh et al., 2020).",
                "More broadly, finding systemic errors can help us detect algorithmic bias (Buolamwini & Gebru, 2018) or sensitivity to distribution shifts (Sagawa et al., 2020; Singh et al., 2020).",
                "We consider blindspots discovered by prior works (Hohman et al., 2019; Sohoni et al., 2020; Singh et al., 2020; Plumb et al., 2022; Singla et al., 2021) as well as novel blindspots (Adebayo et al., 2022).",
                "In Table 6 of their paper, Singh et al. (2020) organize the spurious patterns that they identify by their own measure of bias.",
                "Some common assumptions are:\n\u2022 Access to metadata help define coherent subsets of the data (e.g., Kim et al., 2018; Buolamwini & Gebru, 2018; Singh et al., 2020)."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "cc8a830290262fb00de2a72c8ed49ef700fb639f",
                "externalIds": {
                    "CorpusId": 258170555
                },
                "corpusId": 258170555,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cc8a830290262fb00de2a72c8ed49ef700fb639f",
                "title": "Towards a More Rigorous Science of Blindspot Discovery in Image Models",
                "abstract": "A growing body of work studies Blindspot Discovery Methods (BDMs): methods that use an image embedding to find semantically meaningful (i.e., united by a human-understandable concept) subsets of the data where an image classifier performs significantly worse. Motivated by observed gaps in prior work, we introduce a new framework for evaluating BDMs, SpotCheck, that uses synthetic image datasets to train models with known blindspots and a new BDM, PlaneSpot, that uses a 2D image representation. We use SpotCheck to run controlled experiments that identify factors that influence BDM performance (e.g., the number of blindspots in a model, or features used to define the blindspot) and show that PlaneSpot is competitive with and in many cases outperforms existing BDMs. Importantly, we validate these findings by designing additional experiments that use real image data from MS-COCO, a large image benchmark dataset. Our findings suggest several promising directions for future work on BDM design and evaluation. Overall, we hope that the methodology and analyses presented in this work will help facilitate a more rigorous science of blindspot discovery.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "31929250",
                        "name": "Gregory Plumb"
                    },
                    {
                        "authorId": "2142541079",
                        "name": "Nari Johnson"
                    },
                    {
                        "authorId": "102477227",
                        "name": "\u00c1ngel Alexander Cabrera"
                    },
                    {
                        "authorId": "145532827",
                        "name": "Ameet Talwalkar"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[58] regarded co-occurring objects in the image as context bias in the multilabel classification tasks."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "23c1273512df9eb5d60e7110a99f6798b09366e7",
                "externalIds": {
                    "DBLP": "journals/tmm/QinZHLS23",
                    "DOI": "10.1109/TMM.2021.3136717",
                    "CorpusId": 245358470
                },
                "corpusId": 245358470,
                "publicationVenue": {
                    "id": "10e76a35-58d6-443c-9683-fc16f2dd0a92",
                    "name": "IEEE transactions on multimedia",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Transactions on Multimedia",
                        "IEEE Trans Multimedia",
                        "IEEE trans multimedia"
                    ],
                    "issn": "1520-9210",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=6046"
                },
                "url": "https://www.semanticscholar.org/paper/23c1273512df9eb5d60e7110a99f6798b09366e7",
                "title": "Causal Interventional Training for Image Recognition",
                "abstract": "Deep learning models often fit undesired dataset bias in training. In this paper, we formulate the bias using causal inference, which helps us uncover the ever-elusive causalities among the key factors in training, and thus pursue the desired causal effect without the bias. We start from revisiting the process of building a visual recognition system, and then propose a structural causal model (SCM) for the key variables involved in dataset collection and recognition model: object, common sense, bias, context, and label prediction. Based on the SCM, one can observe that there are \u201cgood\u201d and \u201cbad\u201d biases. Intuitively, in the image where a car is driving on a high way in a desert, the \u201cgood\u201d bias denoting the common-sense context is the highway, and the \u201cbad\u201d bias accounting for the noisy context factor is the desert. We tackle this problem with a novel causal interventional training (CIT) approach, where we control the observed context in each object class. We offer theoretical justifications for CIT and validate it with extensive classification experiments on CIFAR-10, CIFAR-100 and ImageNet, e.g., surpassing the standard deep neural networks ResNet-34 and ResNet-50, respectively, by 0.95% and 0.70% accuracies on the ImageNet. Our code is open-sourced on the GitHub https://github.com/qinwei-hfut/CIT.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1720767577",
                        "name": "Wei Qin"
                    },
                    {
                        "authorId": "2119078220",
                        "name": "Hanwang Zhang"
                    },
                    {
                        "authorId": "2075339632",
                        "name": "Richang Hong"
                    },
                    {
                        "authorId": "2084632198",
                        "name": "E. Lim"
                    },
                    {
                        "authorId": "2138109020",
                        "name": "Qianru Sun"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "While much prior work in vision has investigated bias in image classification systems [24, 32, 48], object detection models differ in model architecture and performance metrics, necessitating their own study.",
                "contextual [32] and co-occurrence biases [49] across visually diverse driving scenarios."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4a108c25660319a102f6de04dfce40b70005a7d0",
                "externalIds": null,
                "corpusId": "259096366",
                "publicationVenue": null,
                "url": "http://www.semanticscholar.org/paper/4a108c25660319a102f6de04dfce40b70005a7d0",
                "title": "ICON 2 : Reliably Benchmarking Predictive Inequity in Object Detection by Identifying and Controlling for Confounders",
                "abstract": null,
                "year": 2023,
                "authors": []
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Deep networks trained on natural image datasets like ImageNet (Krizhevsky et al., 2012) have been demonstrated to rely strongly on context (Geirhos et al., 2018), (Brendel and Bethge, 2019), (Singh et al., 2020).",
                "Multiple studies have shown that context can be both beneficial and detrimental to the performance of neural networks (Rosenfeld et al., 2018) (Singh et al., 2020) (Zhang et al., 2020)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7b35b93ff459e97593df547f7a16949a69b69b0e",
                "externalIds": {
                    "CorpusId": 233378818
                },
                "corpusId": 233378818,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7b35b93ff459e97593df547f7a16949a69b69b0e",
                "title": "Context-Robust Object Recognition via Object Manipulations in a Synthetic 3D Environment",
                "abstract": "The remote control is a small object that does not fly in the air and is generally found on a table, not in the sink. Such contextual regularities are ingrained in our perception of the world and previous research suggests that they can even influence human and computational models object recognition ability. However, the exact effects of contextual information on object recognition are still unknown for both humans and machine learning models. Here, we introduce a novel way of studying the effects of different contextual cues in a qualitative and systematic way. We present a diverse synthetic dataset created via a 3D simulation engine that allows for complex object modifications. Our dataset consists of more than 15000 images across 36 object categories and it is designed specifically for studying the effects of gravity, object co-occurrence statistics, and relative size regularities. We conduct a series of psychophysics experiments to assess human performance and establish a benchmark for computational models on the dataset. Additionally, we test state-of-the-art deep learning models on the same dataset and study how contextual information influences their object recognition accuracy. Finally, we propose a context-aware recognition transformer network that integrates contextual and object information via multi-head attention mechanism. Our model captures useful contextual information that allows it to achieve human-level performance and significantly better robustness in out-of-context conditions compared to baseline models across our dataset and another existing out-of-context natural image dataset. Moreover, our model performs in a way that is consistent with human object recognition and shows similar recognition artefacts.",
                "year": 2021,
                "authors": []
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "4), and often results in poor generalization due to scene biases [14, 83]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5fd25f8454ff808b963e217068f77bfb66620b99",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-05892",
                    "CorpusId": 245123941
                },
                "corpusId": 245123941,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5fd25f8454ff808b963e217068f77bfb66620b99",
                "title": "COMPOSER: Compositional Learning of Group Activity in Videos",
                "abstract": ".",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2157475030",
                        "name": "Honglu Zhou"
                    },
                    {
                        "authorId": "2293919",
                        "name": "Asim Kadav"
                    },
                    {
                        "authorId": "1587627172",
                        "name": "Aviv Shamsian"
                    },
                    {
                        "authorId": "1947101",
                        "name": "Shijie Geng"
                    },
                    {
                        "authorId": "1868193",
                        "name": "Farley Lai"
                    },
                    {
                        "authorId": "33860220",
                        "name": "Long Zhao"
                    },
                    {
                        "authorId": "2115431213",
                        "name": "Ting Liu"
                    },
                    {
                        "authorId": "143980997",
                        "name": "M. Kapadia"
                    },
                    {
                        "authorId": "1775043",
                        "name": "H. Graf"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Because of high frequency of label co-occurrence with multilabel data set, deep neural networks learn biased feature representation of particular multi-label instances [5]\u2013[7] that may not be explicitly related to represent an individual class.",
                "[5] present a method of using class activation map (CAM) to reduce the CAM similarity of different objects for co-occurring samples."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c1b4cd5d522a8f456bea8b1cd25d0dd234d5118e",
                "externalIds": {
                    "DBLP": "journals/access/AdiyaL21",
                    "DOI": "10.1109/ACCESS.2021.3096822",
                    "CorpusId": 236184764
                },
                "corpusId": 236184764,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c1b4cd5d522a8f456bea8b1cd25d0dd234d5118e",
                "title": "Learning Individual Class Representation From Biased Multi-Label Data",
                "abstract": "Image recognition is a popular and important research field of computer vision. Recently with the development of deep learning technology, image recognition performance has been improved significantly. However with multi-label images, recognizing individual category is a challenging task. In order to address the problem, we propose a Feature Disintegrator (FD) that decomposes co-occurred instance features of multi-label into individual categories. In experimental evaluation, proposed method achieves the gains of mean average precision (mAP) up to 18.67% and 29.05% over baseline networks in ML-MNIST and ML-CIFAR, respectively. It shows 5.76% and 6.65% higher mAP than baseline on PASCAL VOC-2007 and MS-COCO data set respectively.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2120330386",
                        "name": "Tserendorj Adiya"
                    },
                    {
                        "authorId": "2108261094",
                        "name": "Seungkyu Lee"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6e22f3519583a5326b3fea0e592f87cb3ab62365",
                "externalIds": {
                    "CorpusId": 260439885
                },
                "corpusId": 260439885,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6e22f3519583a5326b3fea0e592f87cb3ab62365",
                "title": "Intentonomy: a Dataset and Study towards Human Intent Understanding Supplementary Material",
                "abstract": "The aim of our work is to investigate the complex psycho-emotional landscape hidden behind social media posts, and to lay the groundwork for the research in this domain. Such research can foster the development of systems to identify harmful posts and to reduce social media abuse and misinformation. In our work we proposed to explore human intent understanding by introducing a new image dataset along with a new annotation process. We conduct an extensive analysis on the relationship between content and intent. We also presented a framework with two complementary modules for the task. In the supplemental material, we provide the following items that shed further insight on these contributions:",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51502783",
                        "name": "Menglin Jia"
                    },
                    {
                        "authorId": "3099139",
                        "name": "Zuxuan Wu"
                    },
                    {
                        "authorId": "3207112",
                        "name": "A. Reiter"
                    },
                    {
                        "authorId": "2064285348",
                        "name": "Claire Cardie"
                    },
                    {
                        "authorId": "2067789287",
                        "name": "S. Belongie"
                    },
                    {
                        "authorId": "153317808",
                        "name": "S. Lim"
                    }
                ]
            }
        }
    ]
}