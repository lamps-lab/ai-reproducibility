{
    "offset": 0,
    "data": [
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "[62] Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji.",
                "\u2022 Perturbation-based: These methods [59, 30, 62, 18, 29, 43, 27, 6, 31, 1, 50] utilize perturbations of the input to identify important subgraphs that serve as factual or counterfactual explanations.",
                "However, GraphFrameX and GraphXAI collectively assess only GnnExplainer [59], PGExplainer [30], and SubgraphX [62].",
                "com/wanyu-lin/ICML2021-Gem/ SubgraphX [62] https://github.",
                "Gradient: SA [7] , Guided-BP [7] , Grad-CAM [33]; Decomposition: Excitation-BP [33], GNN-LRP [38], CAM [33]; Perturbation: GNNExplainer [59], PGExplainer [30], SubgraphX [62], GEM [27], TAGExplainer [51], CF(2) [43], RCExplainer [6],CF-GNNexplainer [29], CLEAR [31]; Surrogate: GraphLime [18], Relex [64], PGM-Explainer [47]; Global: XGNN [60], GLG-Explainer [5], Xuanyuan et al.",
                "SubgraphX [62] and GStarX [63] use cooperative game theoretic techniques.",
                "Explanations can be broadly classified into two categories: factual reasoning [59, 30, 40, 62, 18] and counterfactual reasoning [29, 43, 31, 6, 1, 50].",
                "\u2022 Instance-level: Instance-level or local explainers [59, 30, 40, 62, 18, 61, 29, 43, 27, 6, 1, 50] provide explanations for specific predictions made by a model.",
                "GNNExplainer [57] Continuous relaxation Mutual Information Size Yes GC+NC Transductive PGExplainer [30] Parameterized edge selection Mutual Information Size and/or connectivity No GC+NC Inductive TAGExplainer [51] Sampling Mutual Information Size, Entropy No GC+NC Inductive GEM [27] Granger Causality+Autoencoder Causal Contribution Size, Connectivity No GC+NC Inductive SubgraphX [62] Monte Carlo Tree Search Shapley Value Size, connectivity No GC Transductive GstarX [63] Monte Carlo sampling HN-value Size No GC Inductive"
            ],
            "citingPaper": {
                "paperId": "8733ec39a7c9d19fcc8fea902cae12b31268fafa",
                "externalIds": {
                    "ArXiv": "2310.01794",
                    "CorpusId": 263608316
                },
                "corpusId": 263608316,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8733ec39a7c9d19fcc8fea902cae12b31268fafa",
                "title": "GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking",
                "abstract": "Numerous explainability methods have been proposed to shed light on the inner workings of GNNs. Despite the inclusion of empirical evaluations in all the proposed algorithms, the interrogative aspects of these evaluations lack diversity. As a result, various facets of explainability pertaining to GNNs, such as a comparative analysis of counterfactual reasoners, their stability to variational factors such as different GNN architectures, noise, stochasticity in non-convex loss surfaces, feasibility amidst domain constraints, and so forth, have yet to be formally investigated. Motivated by this need, we present a benchmarking study on perturbation-based explainability methods for GNNs, aiming to systematically evaluate and compare a wide range of explainability techniques. Among the key findings of our study, we identify the Pareto-optimal methods that exhibit superior efficacy and stability in the presence of noise. Nonetheless, our study reveals that all algorithms are affected by stability issues when faced with noisy data. Furthermore, we have established that the current generation of counterfactual explainers often fails to provide feasible recourses due to violations of topological constraints encoded by domain-specific considerations. Overall, this benchmarking study empowers stakeholders in the field of GNNs with a comprehensive understanding of the state-of-the-art explainability methods, potential research problems for further enhancement, and the implications of their application in real-world scenarios.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2134769449",
                        "name": "Mert Kosan"
                    },
                    {
                        "authorId": "143665702",
                        "name": "S. Verma"
                    },
                    {
                        "authorId": "2219690090",
                        "name": "Burouj Armgaan"
                    },
                    {
                        "authorId": "1491635783",
                        "name": "Khushbu Pahwa"
                    },
                    {
                        "authorId": "2238534218",
                        "name": "Ambuj Singh"
                    },
                    {
                        "authorId": "3390598",
                        "name": "Sourav Medya"
                    },
                    {
                        "authorId": "2253455409",
                        "name": "Sayan Ranu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "To address this challenge, several techniques have been proposed to explain GNNs, most commonly focusing on identifying a subgraph that dominates the model\u2019s prediction (Ying et al., 2019; Luo et al., 2020; Yuan et al., 2021).",
                "lieu of this, Fidelity metrics, namely Fid+, Fid\u2212, and Fid\u2206, have become the prevailing standards to gauge the faithfulness of explanation subgraphs (Yuan et al., 2021; 2022; Azzolin et al., 2023a; Zhang et al., 2022b; Rong et al., 2023; Xie et al., 2022).",
                "Within these methodologies, these methods can be classified as post-hoc explanations (Ying et al., 2019; Luo et al., 2020; Yuan et al., 2021) and self-explainable GNNs (Baldassarre & Azizpour, 2019; Dai & Wang, 2021; Miao et al.",
                "Following routinely adopted settings (Ying et al., 2019; Luo et al., 2020; Yuan et al., 2021), we can safely assume that both models can correctly use the informative components(motifs) in the input graphs to make predictions.",
                "As analyzed in previous works (Yuan et al., 2021), fidelity measurements ignore the size of the explanation."
            ],
            "citingPaper": {
                "paperId": "cc92ae2ce3caf2142a92bed5957a8d5519215eb7",
                "externalIds": {
                    "ArXiv": "2310.01820",
                    "CorpusId": 263609186
                },
                "corpusId": 263609186,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cc92ae2ce3caf2142a92bed5957a8d5519215eb7",
                "title": "Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks",
                "abstract": "Graph Neural Networks (GNNs) are neural models that leverage the dependency structure in graphical data via message passing among the graph nodes. GNNs have emerged as pivotal architectures in analyzing graph-structured data, and their expansive application in sensitive domains requires a comprehensive understanding of their decision-making processes -- necessitating a framework for GNN explainability. An explanation function for GNNs takes a pre-trained GNN along with a graph as input, to produce a `sufficient statistic' subgraph with respect to the graph label. A main challenge in studying GNN explainability is to provide fidelity measures that evaluate the performance of these explanation functions. This paper studies this foundational challenge, spotlighting the inherent limitations of prevailing fidelity metrics, including $Fid_+$, $Fid_-$, and $Fid_\\Delta$. Specifically, a formal, information-theoretic definition of explainability is introduced and it is shown that existing metrics often fail to align with this definition across various statistical scenarios. The reason is due to potential distribution shifts when subgraphs are removed in computing these fidelity measures. Subsequently, a robust class of fidelity measures are introduced, and it is shown analytically that they are resilient to distribution shift issues and are applicable in a wide range of scenarios. Extensive empirical analysis on both synthetic and real datasets are provided to illustrate that the proposed metrics are more coherent with gold standard metrics.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2226672788",
                        "name": "Xu Zheng"
                    },
                    {
                        "authorId": "2253469454",
                        "name": "Farhad Shirani"
                    },
                    {
                        "authorId": "2253432775",
                        "name": "Tianchun Wang"
                    },
                    {
                        "authorId": "2249879747",
                        "name": "Wei Cheng"
                    },
                    {
                        "authorId": "2253867973",
                        "name": "Zhuomin Chen"
                    },
                    {
                        "authorId": "2256237444",
                        "name": "Haifeng Chen"
                    },
                    {
                        "authorId": "2253655246",
                        "name": "Hua Wei"
                    },
                    {
                        "authorId": "2226519756",
                        "name": "Dongsheng Luo"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "In our experiments, we employ the Fidelity+ and Fidelity\u2212 [28] to evaluate the fidelity of the explanations.",
                "These methods have provided some elucidation of GNNs; however, substantial work is still required in the following aspects: 1) Explanation scale (local or global explanation): whether the explanation is linked to a specific instance or whether it has captured the archetypal patterns shared by the same group; 2) Gen-eralizability : whether an explainer can be generalized to unseen graphs without retraining; 3) Fidelity : whether the explanations are real important subgraphs; 4) Versatility : whether an explainer is able to generate accurate explanations for different tasks such as node classification and graph classification."
            ],
            "citingPaper": {
                "paperId": "5bbd1b9f44d320f1c6fa84aec9a48add0b1a0294",
                "externalIds": {
                    "ArXiv": "2309.16918",
                    "DOI": "10.1145/3583780.3614772",
                    "CorpusId": 263310884
                },
                "corpusId": 263310884,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5bbd1b9f44d320f1c6fa84aec9a48add0b1a0294",
                "title": "ACGAN-GNNExplainer: Auxiliary Conditional Generative Explainer for Graph Neural Networks",
                "abstract": "Graph neural networks (GNNs) have proven their efficacy in a variety of real-world applications, but their underlying mechanisms remain a mystery. To address this challenge and enable reliable decision-making, many GNN explainers have been proposed in recent years. However, these methods often encounter limitations, including their dependence on specific instances, lack of generalizability to unseen graphs, producing potentially invalid explanations, and yielding inadequate fidelity. To overcome these limitations, we, in this paper, introduce the Auxiliary Classifier Generative Adversarial Network (ACGAN) into the field of GNN explanation and propose a new GNN explainer dubbed~\\emph{ACGAN-GNNExplainer}. Our approach leverages a generator to produce explanations for the original input graphs while incorporating a discriminator to oversee the generation process, ensuring explanation fidelity and improving accuracy. Experimental evaluations conducted on both synthetic and real-world graph datasets demonstrate the superiority of our proposed method compared to other existing GNN explainers.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2179302425",
                        "name": "Yiqiao Li"
                    },
                    {
                        "authorId": "51239629",
                        "name": "Jianlong Zhou"
                    },
                    {
                        "authorId": "2249599312",
                        "name": "Yifei Dong"
                    },
                    {
                        "authorId": "2716743",
                        "name": "N. Shafiabady"
                    },
                    {
                        "authorId": "2249757048",
                        "name": "Fang Chen"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                ", 2019), PGMExplainer (Vu & Thai, 2020) and SubgraphX (Yuan et al., 2021).",
                "We compare non-generative methods, including the heuristic Occlusion (Zeiler & Fergus, 2014), gradient-based meth-ods Saliency (Baldassarre & Azizpour, 2019), Integrated Gradient (Sundararajan et al., 2017), and Grad-CAM (Pope et al., 2019), and perturbation-based methods GNNExplainer (Ying et al., 2019), PGMExplainer (Vu & Thai, 2020) and SubgraphX (Yuan et al., 2021).",
                "While Truth, PGMExplainer, SubgraphX, and GraphCFE constrain their explanations to be sparse, the rest of the methods include most of the edges in the explanations, assigning a different importance weight to each edge."
            ],
            "citingPaper": {
                "paperId": "66ce2f0bf2123f9e4965d0b244d435d0140c933c",
                "externalIds": {
                    "ArXiv": "2309.16223",
                    "CorpusId": 263136272
                },
                "corpusId": 263136272,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/66ce2f0bf2123f9e4965d0b244d435d0140c933c",
                "title": "GInX-Eval: Towards In-Distribution Evaluation of Graph Neural Network Explanations",
                "abstract": "Diverse explainability methods of graph neural networks (GNN) have recently been developed to highlight the edges and nodes in the graph that contribute the most to the model predictions. However, it is not clear yet how to evaluate the correctness of those explanations, whether it is from a human or a model perspective. One unaddressed bottleneck in the current evaluation procedure is the problem of out-of-distribution explanations, whose distribution differs from those of the training data. This important issue affects existing evaluation metrics such as the popular faithfulness or fidelity score. In this paper, we show the limitations of faithfulness metrics. We propose GInX-Eval (Graph In-distribution eXplanation Evaluation), an evaluation procedure of graph explanations that overcomes the pitfalls of faithfulness and offers new insights on explainability methods. Using a retraining strategy, the GInX score measures how informative removed edges are for the model and the EdgeRank score evaluates if explanatory edges are correctly ordered by their importance. GInX-Eval verifies if ground-truth explanations are instructive to the GNN model. In addition, it shows that many popular methods, including gradient-based methods, produce explanations that are not better than a random designation of edges as important subgraphs, challenging the findings of current works in the area. Results with GInX-Eval are consistent across multiple datasets and align with human evaluation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2146257620",
                        "name": "Kenza Amara"
                    },
                    {
                        "authorId": "1401917601",
                        "name": "Mennatallah El-Assady"
                    },
                    {
                        "authorId": "2248206514",
                        "name": "Rex Ying"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Moreover, the explainability of graph-based learning can offer significant insights in the biomedical domain [2, 451, 464, 470]."
            ],
            "citingPaper": {
                "paperId": "98538552448434483168c56becdafd590c3b91ad",
                "externalIds": {
                    "ArXiv": "2309.08478",
                    "CorpusId": 261875647
                },
                "corpusId": 261875647,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/98538552448434483168c56becdafd590c3b91ad",
                "title": "Current and future directions in network biology",
                "abstract": "Network biology, an interdisciplinary field at the intersection of computational and biological sciences, is critical for deepening understanding of cellular functioning and disease. While the field has existed for about two decades now, it is still relatively young. There have been rapid changes to it and new computational challenges have arisen. This is caused by many factors, including increasing data complexity, such as multiple types of data becoming available at different levels of biological organization, as well as growing data size. This means that the research directions in the field need to evolve as well. Hence, a workshop on Future Directions in Network Biology was organized and held at the University of Notre Dame in 2022, which brought together active researchers in various computational and in particular algorithmic aspects of network biology to identify pressing challenges in this field. Topics that were discussed during the workshop include: inference and comparison of biological networks, multimodal data integration and heterogeneous networks, higher-order network analysis, machine learning on networks, and network-based personalized medicine. Video recordings of the workshop presentations are publicly available on YouTube. For even broader impact of the workshop, this paper, co-authored mostly by the workshop participants, summarizes the discussion from the workshop. As such, it is expected to help shape short- and long-term vision for future computational and algorithmic research in network biology.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2095762",
                        "name": "M. Zitnik"
                    },
                    {
                        "authorId": "2241286447",
                        "name": "Michelle M. Li"
                    },
                    {
                        "authorId": "2241203481",
                        "name": "Aydin Wells"
                    },
                    {
                        "authorId": "2241203118",
                        "name": "Kimberly Glass"
                    },
                    {
                        "authorId": "13732786",
                        "name": "D. Gysi"
                    },
                    {
                        "authorId": "143790218",
                        "name": "Arjun Krishnan"
                    },
                    {
                        "authorId": "2241202901",
                        "name": "T. M. Murali"
                    },
                    {
                        "authorId": "1693041",
                        "name": "P. Radivojac"
                    },
                    {
                        "authorId": "2241870366",
                        "name": "Sushmita Roy"
                    },
                    {
                        "authorId": "2241202988",
                        "name": "Anais Baudot"
                    },
                    {
                        "authorId": "2632371",
                        "name": "S. Bozdag"
                    },
                    {
                        "authorId": "2241321622",
                        "name": "Danny Z. Chen"
                    },
                    {
                        "authorId": "143983748",
                        "name": "L. Cowen"
                    },
                    {
                        "authorId": "1557388853",
                        "name": "K. Devkota"
                    },
                    {
                        "authorId": "2041407",
                        "name": "A. Gitter"
                    },
                    {
                        "authorId": "2241203467",
                        "name": "Sara Gosline"
                    },
                    {
                        "authorId": "1924922339",
                        "name": "Pengfei Gu"
                    },
                    {
                        "authorId": "1738664",
                        "name": "P. Guzzi"
                    },
                    {
                        "authorId": "2241290578",
                        "name": "Heng Huang"
                    },
                    {
                        "authorId": "2241220586",
                        "name": "Meng Jiang"
                    },
                    {
                        "authorId": "1753972910",
                        "name": "Ziynet Nesibe Kesimoglu"
                    },
                    {
                        "authorId": "152153226",
                        "name": "M. Koyuturk"
                    },
                    {
                        "authorId": "2146394692",
                        "name": "Jian Ma"
                    },
                    {
                        "authorId": "2064114238",
                        "name": "Alexander R. Pico"
                    },
                    {
                        "authorId": "2218897155",
                        "name": "Natavsa Prvzulj"
                    },
                    {
                        "authorId": "1939433",
                        "name": "T. Przytycka"
                    },
                    {
                        "authorId": "2242604153",
                        "name": "Benjamin J. Raphael"
                    },
                    {
                        "authorId": "40336357",
                        "name": "Anna M. Ritz"
                    },
                    {
                        "authorId": "1725953",
                        "name": "R. Sharan"
                    },
                    {
                        "authorId": "2241292714",
                        "name": "Yang Shen"
                    },
                    {
                        "authorId": "2240716079",
                        "name": "Mona Singh"
                    },
                    {
                        "authorId": "47177862",
                        "name": "D. Slonim"
                    },
                    {
                        "authorId": "2241203495",
                        "name": "Hanghang Tong"
                    },
                    {
                        "authorId": "2241427929",
                        "name": "Xinan Holly Yang"
                    },
                    {
                        "authorId": "2237992964",
                        "name": "Byung-Jun Yoon"
                    },
                    {
                        "authorId": "2241372819",
                        "name": "Haiyuan Yu"
                    },
                    {
                        "authorId": "2241202964",
                        "name": "Tijana Milenkovi'c"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "1a8de3ce09316fa190b949540a756943f02851f7",
                "externalIds": {
                    "DBLP": "conf/recsys/Mohammadi23",
                    "DOI": "10.1145/3604915.3608875",
                    "CorpusId": 261823754
                },
                "corpusId": 261823754,
                "publicationVenue": {
                    "id": "61275a16-1e0d-479f-ac4e-f295310761f0",
                    "name": "ACM Conference on Recommender Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Recomm Syst",
                        "RecSys",
                        "ACM Conf Recomm Syst",
                        "Conference on Recommender Systems"
                    ],
                    "url": "http://recsys.acm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/1a8de3ce09316fa190b949540a756943f02851f7",
                "title": "Explainable Graph Neural Network Recommenders; Challenges and Opportunities",
                "abstract": "Graph Neural Networks (GNNs) have demonstrated significant potential in recommendation tasks by effectively capturing intricate connections among users, items, and their associated features. Given the escalating demand for interpretability, current research endeavors in the domain of GNNs for Recommender Systems (RecSys) necessitate the development of explainer methodologies to elucidate the decision-making process underlying GNN-based recommendations. In this work, we aim to present our research focused on techniques to extend beyond the existing approaches for addressing interpretability in GNN-based RecSys.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2240537973",
                        "name": "Amir Reza Mohammadi"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "0d70c423440d5265472fc8187ec28d80421e8572",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-03648",
                    "ArXiv": "2309.03648",
                    "DOI": "10.48550/arXiv.2309.03648",
                    "CorpusId": 261582540
                },
                "corpusId": 261582540,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0d70c423440d5265472fc8187ec28d80421e8572",
                "title": "Promoting Fairness in GNNs: A Characterization of Stability",
                "abstract": "The Lipschitz bound, a technique from robust statistics, can limit the maximum changes in the output concerning the input, taking into account associated irrelevant biased factors. It is an efficient and provable method for examining the output stability of machine learning models without incurring additional computation costs. Recently, Graph Neural Networks (GNNs), which operate on non-Euclidean data, have gained significant attention. However, no previous research has investigated the GNN Lipschitz bounds to shed light on stabilizing model outputs, especially when working on non-Euclidean data with inherent biases. Given the inherent biases in common graph data used for GNN training, it poses a serious challenge to constraining the GNN output perturbations induced by input biases, thereby safeguarding fairness during training. Recently, despite the Lipschitz constant's use in controlling the stability of Euclideanneural networks, the calculation of the precise Lipschitz constant remains elusive for non-Euclidean neural networks like GNNs, especially within fairness contexts. To narrow this gap, we begin with the general GNNs operating on an attributed graph, and formulate a Lipschitz bound to limit the changes in the output regarding biases associated with the input. Additionally, we theoretically analyze how the Lipschitz constant of a GNN model could constrain the output perturbations induced by biases learned from data for fairness training. We experimentally validate the Lipschitz bound's effectiveness in limiting biases of the model output. Finally, from a training dynamics perspective, we demonstrate why the theoretical Lipschitz bound can effectively guide the GNN training to better trade-off between accuracy and fairness.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2238407279",
                        "name": "Yaning Jia"
                    },
                    {
                        "authorId": "2243530767",
                        "name": "Chunhui Zhang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "9e6ddeeac1924de902bec156969e76c701ac0cfa",
                "externalIds": {
                    "DOI": "10.1016/j.compchemeng.2023.108403",
                    "CorpusId": 261598084
                },
                "corpusId": 261598084,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9e6ddeeac1924de902bec156969e76c701ac0cfa",
                "title": "Graph neural networks with molecular segmentation for property prediction and structure\u2013property relationship discovery",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1430724399",
                        "name": "Zhudan Chen"
                    },
                    {
                        "authorId": "2093239",
                        "name": "Dazi Li"
                    },
                    {
                        "authorId": "2128166709",
                        "name": "Minghui Liu"
                    },
                    {
                        "authorId": "2157177153",
                        "name": "Jun Liu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "048d0d7cfed76aeb37726ddf5e205a4fdfdf460a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-04406",
                    "ArXiv": "2308.04406",
                    "DOI": "10.48550/arXiv.2308.04406",
                    "CorpusId": 260704408
                },
                "corpusId": 260704408,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/048d0d7cfed76aeb37726ddf5e205a4fdfdf460a",
                "title": "XGBD: Explanation-Guided Graph Backdoor Detection",
                "abstract": "Backdoor attacks pose a significant security risk to graph learning models. Backdoors can be embedded into the target model by inserting backdoor triggers into the training dataset, causing the model to make incorrect predictions when the trigger is present. To counter backdoor attacks, backdoor detection has been proposed. An emerging detection strategy in the vision and NLP domains is based on an intriguing phenomenon: when training models on a mixture of backdoor and clean samples, the loss on backdoor samples drops significantly faster than on clean samples, allowing backdoor samples to be easily detected by selecting samples with the lowest loss values. However, the ignorance of topological feature information on graph data limits its detection effectiveness when applied directly to the graph domain. To this end, we propose an explanation-guided backdoor detection method to take advantage of the topological information. Specifically, we train a helper model on the graph dataset, feed graph samples into the model, and then adopt explanation methods to attribute model prediction to an important subgraph. We observe that backdoor samples have distinct attribution distribution than clean samples, so the explanatory subgraph could serve as more discriminative features for detecting backdoor samples. Comprehensive experiments on multiple popular datasets and attack methods demonstrate the effectiveness and explainability of our method. Our code is available: https://github.com/GuanZihan/GNN_backdoor_detection.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "95405590",
                        "name": "Zihan Guan"
                    },
                    {
                        "authorId": "3432460",
                        "name": "Mengnan Du"
                    },
                    {
                        "authorId": "47717322",
                        "name": "Ninghao Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "To evaluate the performance of explanation quantitatively, we adopt the metrics of fidelity score and the sparsity score following previous works [39, 45, 46]."
            ],
            "citingPaper": {
                "paperId": "2150e336eee8a886e4a661169b60bfbccd323d51",
                "externalIds": {
                    "DBLP": "conf/kdd/WangLL0DDZ23",
                    "DOI": "10.1145/3580305.3599330",
                    "CorpusId": 260500183
                },
                "corpusId": 260500183,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/2150e336eee8a886e4a661169b60bfbccd323d51",
                "title": "Empower Post-hoc Graph Explanations with Information Bottleneck: A Pre-training and Fine-tuning Perspective",
                "abstract": "Researchers recently investigated to explain Graph Neural Networks (GNNs) on the access to a task-specific GNN, which may hinder their wide applications in practice. Specifically, task-specific explanation methods are incapable of explaining pretrained GNNs whose downstream tasks are usually inaccessible, not to mention giving explanations for the transferable knowledge in pretrained GNNs. Additionally, task-specific methods only consider target models' output in the label space, which are coarse-grained and insufficient to reflect the model's internal logic. To address these limitations, we consider a two-stage explanation strategy, i.e., explainers are first pretrained in a task-agnostic fashion in the representation space and then further fine-tuned in the task-specific label space and representation space jointly if downstream tasks are accessible. The two-stage explanation strategy endows post-hoc graph explanations with the applicability to pretrained GNNs where downstream tasks are inaccessible and the capacity to explain the transferable knowledge in the pretrained GNNs. Moreover, as the two-stage explanation strategy explains the GNNs in the representation space, the fine-grained information in the representation space also empowers the explanations. Furthermore, to achieve a trade-off between the fidelity and intelligibility of explanations, we propose an explanation framework based on the Information Bottleneck principle, named Explainable Graph Information Bottleneck (EGIB). EGIB subsumes the task-specific explanation and task-agnostic explanation into a unified framework. To optimize EGIB objective, we derive a tractable bound and adopt a simple yet effective explanation generation architecture. Based on the unified framework, we further theoretically prove that task-agnostic explanation is a relaxed sufficient condition of task-specific explanation, which indicates the transferability of task-agnostic explanations. Extensive experimental results demonstrate the effectiveness of our proposed explanation method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2109656535",
                        "name": "Jihong Wang"
                    },
                    {
                        "authorId": "3326677",
                        "name": "Minnan Luo"
                    },
                    {
                        "authorId": "1737121128",
                        "name": "Jundong Li"
                    },
                    {
                        "authorId": "47904366",
                        "name": "Yun Lin"
                    },
                    {
                        "authorId": "123918726",
                        "name": "Yushun Dong"
                    },
                    {
                        "authorId": "2152487387",
                        "name": "J. Dong"
                    },
                    {
                        "authorId": "2152099796",
                        "name": "Qinghua Zheng"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "34213c6ba759e2b7ac5ea0cdafbdf78448a0fdfd",
                "externalIds": {
                    "DBLP": "conf/kdd/0015MZ0Z023",
                    "DOI": "10.1145/3580305.3599289",
                    "CorpusId": 260499632
                },
                "corpusId": 260499632,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/34213c6ba759e2b7ac5ea0cdafbdf78448a0fdfd",
                "title": "Counterfactual Learning on Heterogeneous Graphs with Greedy Perturbation",
                "abstract": "Due to the growing importance of using graph neural networks in high-stakes applications, there is a pressing need to interpret the predicted results of these models. Existing methods for explanation have mainly focused on generating sub-graphs comprising important edges for a specific prediction. However, these methods face two issues. Firstly, they lack counterfactual validity as removing the subgraph may not affect the prediction, and generating plausible counterfactual examples has not been adequately explored. Secondly, they cannot be extended to heterogeneous graphs as the complex information involved in such graphs increases the difficulty of generating interpretations. This paper proposes a novel counterfactual learning method, named CF-HGExplainer, for heterogeneous graphs. The method incorporates a semantic-aware attentive pooling strategy for the heterogeneous graph classifier and designs a heterogeneous decision boundaries extraction module to find the common logic for similar graphs based on the extracted graph embeddings from the classifier. Additionally, we propose to greedily perturb nodes and edges based on the distribution of node features and edge plausibility to train a neural network for heterogeneous edge weight learning. Extensive experiments on two public academic datasets demonstrate the effectiveness of CF-HGExplainer compared to state-of-the-art methods on the graph classification task and graph interpretation task.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1998961496",
                        "name": "Qiang Yang"
                    },
                    {
                        "authorId": "2109467053",
                        "name": "Changsheng Ma"
                    },
                    {
                        "authorId": "119718473",
                        "name": "Qiannan Zhang"
                    },
                    {
                        "authorId": "2118502950",
                        "name": "Xin Gao"
                    },
                    {
                        "authorId": "2117879943",
                        "name": "Chuxu Zhang"
                    },
                    {
                        "authorId": "2928371",
                        "name": "Xiangliang Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Brought to GML, perturbation-based explainers learn masks that assign an importance to edges and/or features of the graph [24, 29, 48]."
            ],
            "citingPaper": {
                "paperId": "913e43a412cd411434d84772c1035e81cb29d383",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-01682",
                    "ArXiv": "2308.01682",
                    "DOI": "10.48550/arXiv.2308.01682",
                    "CorpusId": 260438795
                },
                "corpusId": 260438795,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/913e43a412cd411434d84772c1035e81cb29d383",
                "title": "Evaluating Link Prediction Explanations for Graph Neural Networks",
                "abstract": "Graph Machine Learning (GML) has numerous applications, such as node/graph classification and link prediction, in real-world domains. Providing human-understandable explanations for GML models is a challenging yet fundamental task to foster their adoption, but validating explanations for link prediction models has received little attention. In this paper, we provide quantitative metrics to assess the quality of link prediction explanations, with or without ground-truth. State-of-the-art explainability methods for Graph Neural Networks are evaluated using these metrics. We discuss how underlying assumptions and technical details specific to the link prediction task, such as the choice of distance between node embeddings, can influence the quality of the explanations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2709930",
                        "name": "Claudio Borile"
                    },
                    {
                        "authorId": "26582424",
                        "name": "A. Perotti"
                    },
                    {
                        "authorId": "2735649",
                        "name": "A. Panisson"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "Finally, the third strategy is called SubgraphX (Yuan et al. 2021)."
            ],
            "citingPaper": {
                "paperId": "6e3dd4ee2f28659b94972bf750151fe7cb4cff36",
                "externalIds": {
                    "PubMedCentral": "10421968",
                    "DOI": "10.1093/bioinformatics/btad482",
                    "CorpusId": 260400600,
                    "PubMed": "37531293"
                },
                "corpusId": 260400600,
                "publicationVenue": {
                    "id": "15d4205f-903b-403c-9a2e-906f02ce04d8",
                    "name": "Bioinformatics",
                    "type": "journal",
                    "alternate_names": [
                        "Int Conf Bioinform",
                        "International Conference on Bioinformatics",
                        "BIOINFORMATICS"
                    ],
                    "issn": "1367-4803",
                    "alternate_issns": [
                        "1367-4811"
                    ],
                    "url": "http://bioinformatics.oxfordjournals.org/"
                },
                "url": "https://www.semanticscholar.org/paper/6e3dd4ee2f28659b94972bf750151fe7cb4cff36",
                "title": "XGDAG: explainable gene\u2013disease associations via graph neural networks",
                "abstract": "Abstract Motivation Disease gene prioritization consists in identifying genes that are likely to be involved in the mechanisms of a given disease, providing a ranking of such genes. Recently, the research community has used computational methods to uncover unknown gene\u2013disease associations; these methods range from combinatorial to machine learning-based approaches. In particular, during the last years, approaches based on deep learning have provided superior results compared to more traditional ones. Yet, the problem with these is their inherent black-box structure, which prevents interpretability. Results We propose a new methodology for disease gene discovery, which leverages graph-structured data using graph neural networks (GNNs) along with an explainability phase for determining the ranking of candidate genes and understanding the model\u2019s output. Our approach is based on a positive\u2013unlabeled learning strategy, which outperforms existing gene discovery methods by exploiting GNNs in a non-black-box fashion. Our methodology is effective even in scenarios where a large number of associated genes need to be retrieved, in which gene prioritization methods often tend to lose their reliability. Availability and implementation The source code of XGDAG is available on GitHub at: https://github.com/GiDeCarlo/XGDAG. The data underlying this article are available at: https://www.disgenet.org/, https://thebiogrid.org/, https://doi.org/10.1371/journal.pcbi.1004120.s003, and https://doi.org/10.1371/journal.pcbi.1004120.s004.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2064006197",
                        "name": "A. Mastropietro"
                    },
                    {
                        "authorId": "2226571577",
                        "name": "Gianluca De Carlo"
                    },
                    {
                        "authorId": "9602006",
                        "name": "A. Anagnostopoulos"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Perturbation-based methods, such as GNNExplainer [66], PGExplainer [35], ZORRO [15], GraphMask [52], RC-Explainer [60], SubgraphX [70], measure the impact of the perturbation of the input features on the output of the classifier, to detect the most important features."
            ],
            "citingPaper": {
                "paperId": "6a5741e39a3c0b9ba1eaab2c890bbc471e40395e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-14849",
                    "ArXiv": "2307.14849",
                    "DOI": "10.48550/arXiv.2307.14849",
                    "CorpusId": 260203044
                },
                "corpusId": 260203044,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6a5741e39a3c0b9ba1eaab2c890bbc471e40395e",
                "title": "Counterfactual Explanations for Graph Classification Through the Lenses of Density",
                "abstract": "Counterfactual examples have emerged as an effective approach to produce simple and understandable post-hoc explanations. In the context of graph classification, previous work has focused on generating counterfactual explanations by manipulating the most elementary units of a graph, i.e., removing an existing edge, or adding a non-existing one. In this paper, we claim that such language of explanation might be too fine-grained, and turn our attention to some of the main characterizing features of real-world complex networks, such as the tendency to close triangles, the existence of recurring motifs, and the organization into dense modules. We thus define a general density-based counterfactual search framework to generate instance-level counterfactual explanations for graph classifiers, which can be instantiated with different notions of dense substructures. In particular, we show two specific instantiations of this general framework: a method that searches for counterfactual graphs by opening or closing triangles, and a method driven by maximal cliques. We also discuss how the general method can be instantiated to exploit any other notion of dense substructures, including, for instance, a given taxonomy of nodes. We evaluate the effectiveness of our approaches in 7 brain network datasets and compare the counterfactual statements generated according to several widely-used metrics. Results confirm that adopting a semantic-relevant unit of change like density is essential to define versatile and interpretable counterfactual explanation methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "89449460",
                        "name": "Carlo Abrate"
                    },
                    {
                        "authorId": "39046274",
                        "name": "Giulia Preti"
                    },
                    {
                        "authorId": "2179558887",
                        "name": "Francesco Bonchi"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Explainable Graph Neural Network NN) techniques that apply specifically to Graph Neural Networks are broadly classified into several classes: Gradients/features based Guided\nBack-propagation (BP) [155], Perturbation Based GNN Explainer [156], SubgraphX [157],\nDecomposition Based, Surrogates GraphLIME [158] and Generation [159].",
                "Explainable Graph Neural Network NN) techniques that apply specifically to Graph Neural Networks are broadly classified into several classes: Gradients/features based Guided Back-propagation (BP) [155], Perturbation Based GNN Explainer [156], SubgraphX [157], Decomposition Based, Surrogates GraphLIME [158] and Generation [159]."
            ],
            "citingPaper": {
                "paperId": "bdd769eb1bfa6bfca6d58538f5f2f18927f29726",
                "externalIds": {
                    "ArXiv": "2307.14032",
                    "DOI": "10.1007/s11467-023-1325-z",
                    "CorpusId": 260164579
                },
                "corpusId": 260164579,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bdd769eb1bfa6bfca6d58538f5f2f18927f29726",
                "title": "Advances of Machine Learning in Materials Science: Ideas and Techniques",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2163207071",
                        "name": "S. Chong"
                    },
                    {
                        "authorId": "2224891850",
                        "name": "Yi Sheng Ng"
                    },
                    {
                        "authorId": "120241405",
                        "name": "Hui\u2010Qiong Wang"
                    },
                    {
                        "authorId": "50021362",
                        "name": "Jin-Cheng Zheng"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "4936c5acfffcc6368e99cd8326fc7555c76a58f0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-07832",
                    "ArXiv": "2307.07832",
                    "DOI": "10.1145/3580305.3599435",
                    "CorpusId": 259937050
                },
                "corpusId": 259937050,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/4936c5acfffcc6368e99cd8326fc7555c76a58f0",
                "title": "MixupExplainer: Generalizing Explanations for Graph Neural Networks with Data Augmentation",
                "abstract": "Graph Neural Networks (GNNs) have received increasing attention due to their ability to learn from graph-structured data. However, their predictions are often not interpretable. Post-hoc instance-level explanation methods have been proposed to understand GNN predictions. These methods seek to discover substructures that explain the prediction behavior of a trained GNN. In this paper, we shed light on the existence of the distribution shifting issue in existing methods, which affects explanation quality, particularly in applications on real-life datasets with tight decision boundaries. To address this issue, we introduce a generalized Graph Information Bottleneck (GIB) form that includes a label-independent graph variable, which is equivalent to the vanilla GIB. Driven by the generalized GIB, we propose a graph mixup method, MixupExplainer, with a theoretical guarantee to resolve the distribution shifting issue. We conduct extensive experiments on both synthetic and real-world datasets to validate the effectiveness of our proposed mixup approach over existing approaches. We also provide a detailed analysis of how our proposed approach alleviates the distribution shifting issue.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144129760",
                        "name": "Jiaxing Zhang"
                    },
                    {
                        "authorId": "153640788",
                        "name": "Dongsheng Luo"
                    },
                    {
                        "authorId": "2181654032",
                        "name": "Huazhou Wei"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "GNN explainability: The explanation methods for GNN models could be categorized into two types based on their granularity: instance-level [25, 34, 53, 58] and model-level [56], where the former methods explain the prediction for each instance by identifying important sub-graphs, and the latter method aims to understand the global decision rules captured by the GNN.",
                "Recent studies [33, 58] have contributed to the development of these methods.",
                "could also be classified into two categories based on their methodology: self-explainable GNNs [1, 8] and post-hoc explanation methods [25, 53, 58], where the former methods provide both predictions and explanations, while the latter methods use an additional model or strategy to explain the target GNN."
            ],
            "citingPaper": {
                "paperId": "54019f5026c385da3f9389ee80525c973f0eb6e5",
                "externalIds": {
                    "ArXiv": "2307.07840",
                    "DBLP": "journals/corr/abs-2307-07840",
                    "DOI": "10.48550/arXiv.2307.07840",
                    "CorpusId": 259937463
                },
                "corpusId": 259937463,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/54019f5026c385da3f9389ee80525c973f0eb6e5",
                "title": "RegExplainer: Generating Explanations for Graph Neural Networks in Regression Task",
                "abstract": "Graph regression is a fundamental task and has received increasing attention in a wide range of graph learning tasks. However, the inference process is often not interpretable. Most existing explanation techniques are limited to understanding GNN behaviors in classification tasks. In this work, we seek an explanation to interpret the graph regression models (XAIG-R). We show that existing methods overlook the distribution shifting and continuously ordered decision boundary, which hinders them away from being applied in the regression tasks. To address these challenges, we propose a novel objective based on the information bottleneck theory and introduce a new mix-up framework, which could support various GNNs in a model-agnostic manner. We further present a contrastive learning strategy to tackle the continuously ordered labels in regression task. To empirically verify the effectiveness of the proposed method, we introduce three benchmark datasets and a real-life dataset for evaluation. Extensive experiments show the effectiveness of the proposed method in interpreting GNN models in regression tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144129760",
                        "name": "Jiaxing Zhang"
                    },
                    {
                        "authorId": "153267839",
                        "name": "Zhuomin Chen"
                    },
                    {
                        "authorId": "2176403267",
                        "name": "Hao Mei"
                    },
                    {
                        "authorId": "153640788",
                        "name": "Dongsheng Luo"
                    },
                    {
                        "authorId": "1474226770",
                        "name": "Hua Wei"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                ", 2020), PGMExplainer (Yuan et al., 2021), and SubgraphX (Vu & Thai, 2020), followed (Ying et al.",
                "The subsequent work, such as PGExplainer (Luo et al., 2020), PGMExplainer (Yuan et al., 2021), and SubgraphX (Vu & Thai, 2020), followed (Ying et al., 2019) and evaluated GNN explainers on the suggested synthetic datasets or their close variants."
            ],
            "citingPaper": {
                "paperId": "6cf743c7031ef8bb29ff192849f07c653262561f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-06963",
                    "ArXiv": "2307.06963",
                    "DOI": "10.48550/arXiv.2307.06963",
                    "CorpusId": 259924516
                },
                "corpusId": 259924516,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6cf743c7031ef8bb29ff192849f07c653262561f",
                "title": "Is Task-Agnostic Explainable AI a Myth?",
                "abstract": "Our work serves as a framework for unifying the challenges of contemporary explainable AI (XAI). We demonstrate that while XAI methods provide supplementary and potentially useful output for machine learning models, researchers and decision-makers should be mindful of their conceptual and technical limitations, which frequently result in these methods themselves becoming black boxes. We examine three XAI research avenues spanning image, textual, and graph data, covering saliency, attention, and graph-type explainers. Despite the varying contexts and timeframes of the mentioned cases, the same persistent roadblocks emerge, highlighting the need for a conceptual breakthrough in the field to address the challenge of compatibility between XAI methods and application tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2223551448",
                        "name": "Alicja Chaszczewicz"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "ac7ba5ac9f31a48ad37b23a98f29c8128d8b2c91",
                "externalIds": {
                    "ArXiv": "2307.07321",
                    "CorpusId": 263334627
                },
                "corpusId": 263334627,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ac7ba5ac9f31a48ad37b23a98f29c8128d8b2c91",
                "title": "NS4AR: A new, focused on sampling areas sampling method in graphical recommendation Systems",
                "abstract": "The effectiveness of graphical recommender system depends on the quantity and quality of negative sampling. This paper selects some typical recommender system models, as well as some latest negative sampling strategies on the models as baseline. Based on typical graphical recommender model, we divide sample region into assigned-n areas and use AdaSim to give different weight to these areas to form positive set and negative set. Because of the volume and significance of negative items, we also proposed a subset selection model to narrow the core negative samples.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2223442406",
                        "name": "Xiangqi Wang"
                    },
                    {
                        "authorId": "2249764276",
                        "name": "Dilinuer Aishan"
                    },
                    {
                        "authorId": "2249843506",
                        "name": "Qi Liu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "6bb9c86483f212a631324ba9b47c344d419a428a",
                "externalIds": {
                    "DBLP": "conf/issta/HuWLPWZ023",
                    "DOI": "10.1145/3597926.3598145",
                    "CorpusId": 259844945
                },
                "corpusId": 259844945,
                "publicationVenue": {
                    "id": "289bfdda-eab3-4c9a-97be-ef1e0f9ddfc0",
                    "name": "International Symposium on Software Testing and Analysis",
                    "type": "conference",
                    "alternate_names": [
                        "ISSTA",
                        "Int Symp Softw Test Anal"
                    ],
                    "url": "https://dl.acm.org/conference/issta"
                },
                "url": "https://www.semanticscholar.org/paper/6bb9c86483f212a631324ba9b47c344d419a428a",
                "title": "Interpreters for GNN-Based Vulnerability Detection: Are We There Yet?",
                "abstract": "Traditional vulnerability detection methods have limitations due to their need for extensive manual labor. Using automated means for vulnerability detection has attracted research interest, especially deep learning, which has achieved remarkable results. Since graphs can better convey the structural feature of code than text, graph neural network (GNN) based vulnerability detection is significantly better than text-based approaches. Therefore, GNN-based vulnerability detection approaches are becoming popular. However, GNN models are close to black boxes for security analysts, so the models cannot provide clear evidence to explain why a code sample is detected as vulnerable or secure. At this stage, many GNN interpreters have been proposed. However, the explanations provided by these interpretations for vulnerability detection models are highly inconsistent and unconvincing to security experts. To address the above issues, we propose principled guidelines to assess the quality of the interpretation approaches for GNN-based vulnerability detectors based on concerns in vulnerability detection, namely, stability, robustness, and effectiveness. We conduct extensive experiments to evaluate the interpretation performance of six famous interpreters (GNN-LRP, DeepLIFT, GradCAM, GNNExplainer, PGExplainer, and SubGraphX) on four vulnerability detectors (DeepWukong, Devign, IVDetect, and Reveal). The experimental results show that the target interpreters achieve poor performance in terms of effectiveness, stability, and robustness. For effectiveness, we find that the instance-independent methods outperform others due to their deep insight into the detection model. In terms of stability, the perturbation-based interpretation methods are more resilient to slight changes in model parameters as they are model-agnostic. For robustness, the instance-independent approaches provide more consistent interpretation results for similar vulnerabilities.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2182450357",
                        "name": "Yutao Hu"
                    },
                    {
                        "authorId": "2223247522",
                        "name": "Suyuan Wang"
                    },
                    {
                        "authorId": "2108801422",
                        "name": "Wenke Li"
                    },
                    {
                        "authorId": "2158336147",
                        "name": "Junru Peng"
                    },
                    {
                        "authorId": "2109036133",
                        "name": "Yueming Wu"
                    },
                    {
                        "authorId": "2068865",
                        "name": "Deqing Zou"
                    },
                    {
                        "authorId": "2152883080",
                        "name": "Hai Jin"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "For example, our dataset measure method can be used to select relatively complex chemical molecular samples for drug discovery [43]."
            ],
            "citingPaper": {
                "paperId": "904628a92f98f36977b37007a4c8fa03e474b11b",
                "externalIds": {
                    "DBLP": "conf/iscc/WangZM23",
                    "DOI": "10.1109/ISCC58397.2023.10218000",
                    "CorpusId": 261316300
                },
                "corpusId": 261316300,
                "publicationVenue": {
                    "id": "159aed30-148b-4b50-99a4-372b0af958d9",
                    "name": "International Symposium on Computers and Communications",
                    "type": "conference",
                    "alternate_names": [
                        "ISCC",
                        "Int Symp Comput Commun"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000156/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/904628a92f98f36977b37007a4c8fa03e474b11b",
                "title": "Which2learn: A Vulnerability Dataset Complexity Measurement Method for Data-Driven Detectors",
                "abstract": "The increasing number of software vulnerabilities on complex programs has posed potential threats to cyberspace security. Recently, many data-driven methods have been proposed to detect such a large number of vulnerabilities. However, most of these data-driven detectors mainly focus on developing different models to improve the classification performance, ignoring the important research question that prior knowledge is learned from vulnerability datasets when training models. In this work, we propose a novel method to determine which dataset is relatively high-complexity for a data-driven detector to learn prior knowledge. Our method is called Which21earn for short. Our dataset complexity measure method is based on the sample's Program Dependence Graph. Experiments show that our dataset measurement method can improve the state-of-the-art GNN-based model's F1-score by about 9.5% in popular memory-related vulnerability detection. Moreover, our dataset measurement method can be easily extended to select training samples in most graph embedding machine learning tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2130321731",
                        "name": "Huozhu Wang"
                    },
                    {
                        "authorId": "2491847",
                        "name": "Ziyuan Zhu"
                    },
                    {
                        "authorId": "2128305824",
                        "name": "Dan Meng"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Given the input data, instancelevel techniques [29, 31, 52, 54, 65, 68] have been present main stream of GNN explanation, which aim to acquire explanations for a target instance.",
                "The dataset is widely used in GNN explanation works [65, 66, 68] given its distinct structural feature with respect to the underlying domain (see Figure 1(b)).",
                "As shown in Case 1. of Figure 1(a) is a 6-carbon ring found by SubgraphX [68] as an explanation for the mutagenic class in the MUTAG dataset; however its prediction score from the GNN for the underlying label is only 0.0028, which means the model does not recognize this structure asmutagenic.",
                "of Figure 1(a) is a 6-carbon ring found by SubgraphX [68] as an explanation for the mutagenic class in the MUTAG dataset; however its prediction score from the GNN for the underlying label is only 0."
            ],
            "citingPaper": {
                "paperId": "09ebe119cc955c804e2a5f742a8483e725213eaf",
                "externalIds": {
                    "DOI": "10.14778/3611479.3611538",
                    "CorpusId": 261193294
                },
                "corpusId": 261193294,
                "publicationVenue": {
                    "id": "fcbcaf18-8ab1-43e1-a973-604bbc7e344e",
                    "name": "Proceedings of the VLDB Endowment",
                    "type": "journal",
                    "alternate_names": [
                        "Proceedings of The Vldb Endowment",
                        "Proc VLDB Endow",
                        "Proc Vldb Endow"
                    ],
                    "issn": "2150-8097",
                    "url": "http://dl.acm.org/toc.cfm?id=J1174",
                    "alternate_urls": [
                        "http://portal.acm.org/toc.cfm?CFID=21632689&CFTOKEN=99329904&WantType=Affiliated%20Organizations&coll=ACM&dl=ACM&id=J1174&idx=J1174&part=affil&title=VLDB%20Endowment&type=periodical"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/09ebe119cc955c804e2a5f742a8483e725213eaf",
                "title": "On Data-Aware Global Explainability of Graph Neural Networks",
                "abstract": "Graph Neural Networks (GNNs) have significantly boosted the performance of many graph-based applications, yet they serve as black-box models. To understand how GNNs make decisions, explainability techniques have been extensively studied. While the majority of existing methods focus on local explainability, we propose DAG-Explainer in this work aiming for global explainability. Specifically, we observe three properties of superior explanations for a pretrained GNN: they should be highly recognized by the model, compliant with the data distribution and discriminative among all the classes. The first property entails an explanation to be faithful to the model, as the other two require the explanation to be convincing regarding the data distribution. Guided by these properties, we design metrics to quantify the quality of each single explanation and formulate the problem of finding data-aware global explanations for a pretrained GNN as an optimizing problem. We prove that the problem is NP-hard and adopt a randomized greedy algorithm to find a near optimal solution. Furthermore, we derive an improved bound of the approximation algorithm in our problem over the state-of-the-art (SOTA) best. Experimental results show that DAG-Explainer can efficiently produce meaningful and trustworthy explanations while preserving comparable quantitative evaluation results to the SOTA methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2142661696",
                        "name": "Gengsi Lv"
                    },
                    {
                        "authorId": "143891665",
                        "name": "Lei Chen"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "75fda42967abb42a928fba395cb8e011d3beea60",
                "externalIds": {
                    "DBLP": "journals/eswa/YuLZZW23",
                    "DOI": "10.1016/j.eswa.2023.120978",
                    "CorpusId": 259890287
                },
                "corpusId": 259890287,
                "publicationVenue": {
                    "id": "987139ae-a65d-49bb-aaf6-fb764dc40b19",
                    "name": "Expert systems with applications",
                    "type": "journal",
                    "alternate_names": [
                        "Expert syst appl",
                        "Expert Systems With Applications",
                        "Expert Syst Appl"
                    ],
                    "issn": "0957-4174",
                    "url": "https://www.journals.elsevier.com/expert-systems-with-applications/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/expert-systems-with-applications",
                        "http://www.sciencedirect.com/science/journal/09574174"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/75fda42967abb42a928fba395cb8e011d3beea60",
                "title": "Code classification with graph neural networks: Have you ever struggled to make it work?",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "84205136",
                        "name": "Qingchen Yu"
                    },
                    {
                        "authorId": "2120099904",
                        "name": "Xin Liu"
                    },
                    {
                        "authorId": "2118411250",
                        "name": "Qingguo Zhou"
                    },
                    {
                        "authorId": "144279461",
                        "name": "Jianwei Zhuge"
                    },
                    {
                        "authorId": "2118839825",
                        "name": "Chunming Wu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "f96b44a72089e238fc53eda17b2243a37d5b5879",
                "externalIds": {
                    "DOI": "10.1016/j.knosys.2023.110772",
                    "CorpusId": 260173519
                },
                "corpusId": 260173519,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f96b44a72089e238fc53eda17b2243a37d5b5879",
                "title": "KE-X: Towards subgraph explanations of knowledge graph embedding based on knowledge information gain",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2221145188",
                        "name": "Dong Zhao"
                    },
                    {
                        "authorId": "1380547970",
                        "name": "Guojia Wan"
                    },
                    {
                        "authorId": "1895813",
                        "name": "Yibing Zhan"
                    },
                    {
                        "authorId": "2145656",
                        "name": "Zengmao Wang"
                    },
                    {
                        "authorId": "46573238",
                        "name": "Liang Ding"
                    },
                    {
                        "authorId": "2232383487",
                        "name": "Zhigao Zheng"
                    },
                    {
                        "authorId": "2142452296",
                        "name": "Bo Du"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "b27a27195e288f4594bad807e9dce1dbfc735d56",
                "externalIds": {
                    "DOI": "10.14778/3611479.3611503",
                    "CorpusId": 261197657
                },
                "corpusId": 261197657,
                "publicationVenue": {
                    "id": "fcbcaf18-8ab1-43e1-a973-604bbc7e344e",
                    "name": "Proceedings of the VLDB Endowment",
                    "type": "journal",
                    "alternate_names": [
                        "Proceedings of The Vldb Endowment",
                        "Proc VLDB Endow",
                        "Proc Vldb Endow"
                    ],
                    "issn": "2150-8097",
                    "url": "http://dl.acm.org/toc.cfm?id=J1174",
                    "alternate_urls": [
                        "http://portal.acm.org/toc.cfm?CFID=21632689&CFTOKEN=99329904&WantType=Affiliated%20Organizations&coll=ACM&dl=ACM&id=J1174&idx=J1174&part=affil&title=VLDB%20Endowment&type=periodical"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b27a27195e288f4594bad807e9dce1dbfc735d56",
                "title": "HENCE-X: Toward Heterogeneity-Agnostic Multi-Level Explainability for Deep Graph Networks",
                "abstract": "Deep graph networks (DGNs) have demonstrated their outstanding effectiveness on both heterogeneous and homogeneous graphs. However their black-box nature does not allow human users to understand their working mechanisms. Recently, extensive efforts have been devoted to explaining DGNs' prediction, yet heterogeneity-agnostic multi-level explainability is still less explored. Since the two types of graphs are both irreplaceable in real-life applications, having a more general and end-to-end explainer becomes a natural and inevitable choice. In the meantime, feature-level explanation is often ignored by existing techniques, while topological-level explanation alone can be incomplete and deceptive. Thus, we propose a heterogeneity-agnostic multi-level explainer in this paper, named HENCE-X, which is a causality-guided method that can capture the non-linear dependencies of model behavior on the input using conditional probabilities. We theoretically prove that HENCE-X is guaranteed to find the Markov blanket of the explained prediction, meaning that all information that the prediction is dependent on is identified. Experiments on three real-world datasets show that HENCE-X outperforms state-of-the-art (SOTA) methods in generating faithful factual and counterfactual explanations of DGNs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2142661696",
                        "name": "Gengsi Lv"
                    },
                    {
                        "authorId": "50445897",
                        "name": "C. Zhang"
                    },
                    {
                        "authorId": "143891665",
                        "name": "Lei Chen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Second, among the adaptations of four advanced explanation methods, SubgraphX is superior in all cases.",
                "These methods attribute model predictions to graph objects, such as nodes (Vu and Thai 2020), edges (Ying et al. 2019; Luo et al. 2020; Schlichtkrull, De Cao, and Titov 2020; Wang et al. 2021b; Lin, Lan, and Li 2021) and subgraphs (Yuan et al. 2021).",
                "On probability fidelity, xPath performs slightly worse than SubgraphX on DBLP.",
                "2021b; Lin, Lan, and Li 2021) and subgraphs (Yuan et al. 2021).",
                "(6) SubgraphX (Yuan et al. 2021) provides subgraph-level explanations.",
                "ReFine generally runs faster than PGM-Explainer and SubgraphX.",
                "The sampling process in PGM-Explainer and the Monte Carlo search in SubgraphX incur high time costs when the target node has a large neighborhood, e.g., SIM3 on DBLP. GEM is efficient in many cases but also suffers in explaining SIM3 due to large node neighborhood, because it calculates the probability of edges between pairwise nodes in the neighborhood.",
                "This is because the influence score is defined to be less sensitive to the change in probabilities and xPath pays more attention to avoiding label change, i.e., better accuracy fidelity than SubgraphX."
            ],
            "citingPaper": {
                "paperId": "80d07f8cf49d0fe88bedde8a578a51fc81d0ed23",
                "externalIds": {
                    "DBLP": "conf/aaai/LiDSQHC23",
                    "DOI": "10.1609/aaai.v37i7.26040",
                    "CorpusId": 259747987
                },
                "corpusId": 259747987,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/80d07f8cf49d0fe88bedde8a578a51fc81d0ed23",
                "title": "Towards Fine-Grained Explainability for Heterogeneous Graph Neural Network",
                "abstract": "Heterogeneous graph neural networks (HGNs) are prominent approaches to node classification tasks on heterogeneous graphs. Despite the superior performance, insights about the predictions made from HGNs are obscure to humans. Existing explainability techniques are mainly proposed for GNNs on homogeneous graphs. They focus on highlighting salient graph objects to the predictions whereas the problem of how these objects affect the predictions remains unsolved. Given heterogeneous graphs with complex structures and rich semantics, it is imperative that salient objects can be accompanied with their influence paths to the predictions, unveiling the reasoning process of HGNs. In this paper, we develop xPath, a new framework that provides fine-grained explanations for black-box HGNs specifying a cause node with its influence path to the target node. In xPath, we differentiate the influence of a node on the prediction w.r.t. every individual influence path, and measure the influence by perturbing graph structure via a novel graph rewiring algorithm. Furthermore, we introduce a greedy search algorithm to find the most influential fine-grained explanations efficiently. Empirical results on various HGNs and heterogeneous graphs show that xPath yields faithful explanations efficiently, outperforming the adaptations of advanced GNN explanation approaches.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "15762417",
                        "name": "Tong Li"
                    },
                    {
                        "authorId": "2028910513",
                        "name": "Jiale Deng"
                    },
                    {
                        "authorId": "2923152",
                        "name": "Yanyan Shen"
                    },
                    {
                        "authorId": "1659028311",
                        "name": "Luyu Qiu"
                    },
                    {
                        "authorId": "9164904",
                        "name": "Hu Yongxiang"
                    },
                    {
                        "authorId": "3151540",
                        "name": "Caleb Chen Cao"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "SubgraphX [75] explains its predictions by efficiently exploring different subgraphs with a Monte Carlo tree search.",
                "Others only apply to simple graphs, which cannot handle signed and weighted brain graphs [29, 75]."
            ],
            "citingPaper": {
                "paperId": "2ecf83bfda9de04821036651d989c96ac0bca780",
                "externalIds": {
                    "ArXiv": "2306.14375",
                    "DBLP": "journals/corr/abs-2306-14375",
                    "DOI": "10.1145/3580305.3599394",
                    "CorpusId": 259252081
                },
                "corpusId": 259252081,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/2ecf83bfda9de04821036651d989c96ac0bca780",
                "title": "Interpretable Sparsification of Brain Graphs: Better Practices and Effective Designs for Graph Neural Networks",
                "abstract": "Brain graphs, which model the structural and functional relationships between brain regions, are crucial in neuroscientific and clinical applications that can be formulated as graph classification tasks. However, dense brain graphs pose computational challenges such as large time and memory consumption and poor model interpretability. In this paper, we investigate effective designs in Graph Neural Networks (GNNs) to sparsify brain graphs by eliminating noisy edges. Many prior works select noisy edges based on explainability or task-irrelevant properties, but this does not guarantee performance improvement when using the sparsified graphs. Additionally, the selection of noisy edges is often tailored to each individual graph, making it challenging to sparsify multiple graphs collectively using the same approach. To address the issues above, we first introduce an iterative framework to analyze the effectiveness of different sparsification models. By utilizing this framework, we find that (i) methods that prioritize interpretability may not be suitable for graph sparsification, as the sparsified graphs may degenerate the performance of GNN models; (ii) it is beneficial to learn the edge selection during the training of the GNN, rather than after the GNN has converged; (iii) learning a joint edge selection shared across all graphs achieves higher performance than generating separate edge selection for each graph; and (iv) gradient information, which is task-relevant, helps with edge selection. Based on these insights, we propose a new model, Interpretable Graph Sparsification (IGS), which improves the graph classification performance by up to 5.1% with 55.0% fewer edges than the original graphs. The retained edges identified by IGS provide neuroscientific interpretations and are supported by well-established literature.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2155120549",
                        "name": "Gao Li"
                    },
                    {
                        "authorId": "2184493570",
                        "name": "M. Duda"
                    },
                    {
                        "authorId": "1771551",
                        "name": "X. Zhang"
                    },
                    {
                        "authorId": "2479152",
                        "name": "Danai Koutra"
                    },
                    {
                        "authorId": "7957569",
                        "name": "Yujun Yan"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "In this paper, we compare our methods with state-of-the-art GNN explanation methods, including GNNExplainer (Ying et al. 2019), PGEXplainer (Luo et al. 2020) SubgraphX (Yuan et al. 2021) and ReFine (Wang et al. 2021), and inherently interpretable GNN methods, including GAT and GSAT (Miao, Liu, and Li 2022).",
                "2020) SubgraphX (Yuan et al. 2021) and ReFine (Wang et al."
            ],
            "citingPaper": {
                "paperId": "d19060b433e298d126ec287b372d4bb6a14043da",
                "externalIds": {
                    "DBLP": "conf/aaai/Chai0DTMWS23",
                    "DOI": "10.1609/aaai.v37i12.26656",
                    "CorpusId": 259748838
                },
                "corpusId": 259748838,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/d19060b433e298d126ec287b372d4bb6a14043da",
                "title": "Towards Learning to Discover Money Laundering Sub-network in Massive Transaction Network",
                "abstract": "Anti-money laundering (AML) systems play a critical role in safeguarding global economy. As money laundering is considered as one of the top group crimes, there is a crucial need to discover money laundering sub-network behind a particular money laundering transaction for a robust AML system. However, existing rule-based methods for money laundering sub-network discovery is heavily based on domain knowledge and may lag behind the modus operandi of launderers. Therefore, in this work, we first address the money laundering sub-network discovery problem with a neural network based approach, and propose an AML framework AMAP equipped with an adaptive sub-network proposer. In particular, we design an adaptive sub-network proposer guided by a supervised contrastive loss to discriminate money laundering transactions from massive benign transactions. We conduct extensive experiments on real-word datasets in AliPay of Ant Group. The result demonstrates the effectiveness of our AMAP in both money laundering transaction detection and money laundering sub-network discovering. The learned framework which yields money laundering sub-network from massive transaction network leads to a more comprehensive risk coverage and a deeper insight to money laundering strategies.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2121930095",
                        "name": "Ziwei Chai"
                    },
                    {
                        "authorId": "2152918752",
                        "name": "Yang Yang"
                    },
                    {
                        "authorId": "151045932",
                        "name": "Jiawang Dan"
                    },
                    {
                        "authorId": "2671378",
                        "name": "Sheng Tian"
                    },
                    {
                        "authorId": "2114323322",
                        "name": "Changhua Meng"
                    },
                    {
                        "authorId": null,
                        "name": "Weiqiang Wang"
                    },
                    {
                        "authorId": "2125100601",
                        "name": "Yifei Sun"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "[24] propose SubgraphX to efficiently explain GNNs by identifying the important subgraphs.",
                "Perturbations-based methods monitor the change of prediction with respect to different input perturbations, including GNNExplainer [8], PGExplainer [9], GraphMask [10], SubgraphX [24], and so on.",
                "SubgraphX uses subgraphs as nodes in its search process and assigns a score to each subgraph based on its Shapley value.",
                "Yuan\u2019s experimental results show that SubgraphX achieves significantly improved explanations compared with PGExplainer [9] and GNNExplainer [8].",
                "SubgraphX uses Fidelity and Infidelity metrics to study predictive changes by retaining important input features and removing unimportant features, respectively.",
                "Yuan et al. [24] propose SubgraphX to efficiently explain GNNs by identifying the important subgraphs."
            ],
            "citingPaper": {
                "paperId": "5409798cf2d25e60bd5f2ff5cbbf8dbca2774b49",
                "externalIds": {
                    "DBLP": "conf/ijcnn/DingLYWX23",
                    "DOI": "10.1109/IJCNN54540.2023.10191684",
                    "CorpusId": 260387935
                },
                "corpusId": 260387935,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/5409798cf2d25e60bd5f2ff5cbbf8dbca2774b49",
                "title": "MEGA: Explaining Graph Neural Networks with Network Motifs",
                "abstract": "Graph Neural Networks (GNNs) are powerful tools for graph representation. However, GNNs have remained black boxes, leading to the lack of explainability. As a consequence, the application of GNNs has been severely limited. Existing methods focus on generating an explanation with important nodes and edges but pay less attention to high-order structures (e.g., network motif), which are more intuitive and important for graph data. The explanations generated by the explainer will thus ignore the high-order information contained in multi-node neighbours, resulting in a decrease in the human comprehensibility of the explanation subgraph. In this paper, we propose a motif-aware GNNs explainer (MEGA), wherein a motif-aware subgraph generation module and a counterfactual optimization layer are employed. MEGA can provide high-quality counterfactual explanations for GNNs while focusing on high-order features of graph data. We justify the effectiveness of the proposed MEGA on both synthetic and real-world datasets. Experimental results show that MEGA outperforms state-of-the-art baselines while keeping explanations at a smaller level.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2064425407",
                        "name": "Feng Ding"
                    },
                    {
                        "authorId": "2192325640",
                        "name": "Naiwen Luo"
                    },
                    {
                        "authorId": "50443544",
                        "name": "Shuo Yu"
                    },
                    {
                        "authorId": "2226804056",
                        "name": "Tingting Wang"
                    },
                    {
                        "authorId": "2143633281",
                        "name": "Feng Xia"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "67be1c41a7ae7e212f1365f9c90b363733f45579",
                "externalIds": {
                    "DBLP": "conf/grades/Mobaraki023",
                    "DOI": "10.1145/3594778.3594880",
                    "CorpusId": 259213245
                },
                "corpusId": 259213245,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/67be1c41a7ae7e212f1365f9c90b363733f45579",
                "title": "A Demonstration of Interpretability Methods for Graph Neural Networks",
                "abstract": "Graph neural networks (GNNs) are widely used in many downstream applications, such as graphs and nodes classification, entity resolution, link prediction, and question answering. Several interpretability methods for GNNs have been proposed recently. However, since they have not been thoroughly compared with each other, their trade-offs and efficiency in the context of underlying GNNs and downstream applications are unclear. To support more research in this domain, we develop an end-to-end interactive tool, named gInterpreter, by re-implementing 15 recent GNN interpretability methods in a common environment on top of a number of state-of-the-art GNNs employed for different downstream tasks. This paper demonstrates gInterpreter with an interactive performance profiling of 15 recent GNN inter-pretability methods, aiming to explain the complex deep learning pipelines over graph-structured data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220350635",
                        "name": "Ehsan Bonabi Mobaraki"
                    },
                    {
                        "authorId": "2108514592",
                        "name": "Arijit Khan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Similar to global explanations, there exist a variety of approaches, such as perturbation-based methods [36, 51, 60], surrogate methods [21], gradient-based methods [46], and additive methods [12, 64], each with their benefits and limitations.",
                "To this end, there exist a variety of explanation techniques and explanation types [12,21,36,46,51,60,62,64], the most popular of which are subgraph explanation techniques."
            ],
            "citingPaper": {
                "paperId": "2c76331ac1676ed7fdd51b8cd744765628e0a181",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-10066",
                    "ArXiv": "2306.10066",
                    "DOI": "10.48550/arXiv.2306.10066",
                    "CorpusId": 259203474
                },
                "corpusId": 259203474,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2c76331ac1676ed7fdd51b8cd744765628e0a181",
                "title": "On the Interplay of Subset Selection and Informed Graph Neural Networks",
                "abstract": "Machine learning techniques paired with the availability of massive datasets dramatically enhance our ability to explore the chemical compound space by providing fast and accurate predictions of molecular properties. However, learning on large datasets is strongly limited by the availability of computational resources and can be infeasible in some scenarios. Moreover, the instances in the datasets may not yet be labelled and generating the labels can be costly, as in the case of quantum chemistry computations. Thus, there is a need to select small training subsets from large pools of unlabelled data points and to develop reliable ML methods that can effectively learn from small training sets. This work focuses on predicting the molecules atomization energy in the QM9 dataset. We investigate the advantages of employing domain knowledge-based data sampling methods for an efficient training set selection combined with informed ML techniques. In particular, we show how maximizing molecular diversity in the training set selection process increases the robustness of linear and nonlinear regression techniques such as kernel methods and graph neural networks. We also check the reliability of the predictions made by the graph neural network with a model-agnostic explainer based on the rate distortion explanation framework.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2186116021",
                        "name": "Niklas Breustedt"
                    },
                    {
                        "authorId": "2131643917",
                        "name": "Paolo Climaco"
                    },
                    {
                        "authorId": "2279864",
                        "name": "J. Garcke"
                    },
                    {
                        "authorId": "22232249",
                        "name": "J. Hamaekers"
                    },
                    {
                        "authorId": "3125779",
                        "name": "Gitta Kutyniok"
                    },
                    {
                        "authorId": "34678892",
                        "name": "D. Lorenz"
                    },
                    {
                        "authorId": "2220304429",
                        "name": "Rick Oerder"
                    },
                    {
                        "authorId": "2192822479",
                        "name": "Chirag Varun Shukla"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "For instance, to obtain an explanation, GNNExplainer [36] requires learning a mask of input neighbors; ZORRO [12] and SubgraphX [37] utilize on greedy algorithms and Monte Carlo Tree Search to sample important subgraph that can maximize the",
                "For instance, to obtain an explanation, GNNExplainer [36] requires learning a mask of input neighbors; ZORRO [12] and SubgraphX [37] utilize on greedy algorithms and Monte Carlo Tree Search to sample important subgraph that can maximize the\nPreprint."
            ],
            "citingPaper": {
                "paperId": "a2284f5fbee3eb250fe69d84881680dc388de473",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-05760",
                    "ArXiv": "2306.05760",
                    "DOI": "10.48550/arXiv.2306.05760",
                    "CorpusId": 259129511
                },
                "corpusId": 259129511,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a2284f5fbee3eb250fe69d84881680dc388de473",
                "title": "Efficient GNN Explanation via Learning Removal-based Attribution",
                "abstract": "As Graph Neural Networks (GNNs) have been widely used in real-world applications, model explanations are required not only by users but also by legal regulations. However, simultaneously achieving high fidelity and low computational costs in generating explanations has been a challenge for current methods. In this work, we propose a framework of GNN explanation named LeArn Removal-based Attribution (LARA) to address this problem. Specifically, we introduce removal-based attribution and demonstrate its substantiated link to interpretability fidelity theoretically and experimentally. The explainer in LARA learns to generate removal-based attribution which enables providing explanations with high fidelity. A strategy of subgraph sampling is designed in LARA to improve the scalability of the training process. In the deployment, LARA can efficiently generate the explanation through a feed-forward pass. We benchmark our approach with other state-of-the-art GNN explanation methods on six datasets. Results highlight the effectiveness of our framework regarding both efficiency and fidelity. In particular, LARA is 3.5 times faster and achieves higher fidelity than the state-of-the-art method on the large dataset ogbn-arxiv (more than 160K nodes and 1M edges), showing its great potential in real-world applications. Our source code is available at https://anonymous.4open.science/r/LARA-10D8/README.md.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2057815422",
                        "name": "Yao Rong"
                    },
                    {
                        "authorId": "32780441",
                        "name": "Guanchu Wang"
                    },
                    {
                        "authorId": "2151233715",
                        "name": "Qizhang Feng"
                    },
                    {
                        "authorId": "47717322",
                        "name": "Ninghao Liu"
                    },
                    {
                        "authorId": "47781070",
                        "name": "Zirui Liu"
                    },
                    {
                        "authorId": "1884159",
                        "name": "Enkelejda Kasneci"
                    },
                    {
                        "authorId": "2123553641",
                        "name": "Xia Hu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "83a2f57677ea625a80bb438f6080a5649d14b3bb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-04835",
                    "ArXiv": "2306.04835",
                    "DOI": "10.48550/arXiv.2306.04835",
                    "CorpusId": 259108268
                },
                "corpusId": 259108268,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/83a2f57677ea625a80bb438f6080a5649d14b3bb",
                "title": "Empowering Counterfactual Reasoning over Graph Neural Networks through Inductivity",
                "abstract": "Graph neural networks (GNNs) have various practical applications, such as drug discovery, recommendation engines, and chip design. However, GNNs lack transparency as they cannot provide understandable explanations for their predictions. To address this issue, counterfactual reasoning is used. The main goal is to make minimal changes to the input graph of a GNN in order to alter its prediction. While several algorithms have been proposed for counterfactual explanations of GNNs, most of them have two main drawbacks. Firstly, they only consider edge deletions as perturbations. Secondly, the counterfactual explanation models are transductive, meaning they do not generalize to unseen data. In this study, we introduce an inductive algorithm called INDUCE, which overcomes these limitations. By conducting extensive experiments on several datasets, we demonstrate that incorporating edge additions leads to better counterfactual results compared to the existing methods. Moreover, the inductive modeling approach allows INDUCE to directly predict counterfactual perturbations without requiring instance-specific training. This results in significant computational speed improvements compared to baseline methods and enables scalable counterfactual analysis for GNNs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "143665702",
                        "name": "S. Verma"
                    },
                    {
                        "authorId": "2219690090",
                        "name": "Burouj Armgaan"
                    },
                    {
                        "authorId": "3390598",
                        "name": "Sourav Medya"
                    },
                    {
                        "authorId": "1699732",
                        "name": "Sayan Ranu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "617bff50277c442fa895611f8f2b97e6ab0d3f30",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-02081",
                    "ArXiv": "2306.02081",
                    "DOI": "10.48550/arXiv.2306.02081",
                    "CorpusId": 259075505
                },
                "corpusId": 259075505,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/617bff50277c442fa895611f8f2b97e6ab0d3f30",
                "title": "Message-passing selection: Towards interpretable GNNs for graph classification",
                "abstract": "In this paper, we strive to develop an interpretable GNNs' inference paradigm, termed MSInterpreter, which can serve as a plug-and-play scheme readily applicable to various GNNs' baselines. Unlike the most existing explanation methods, MSInterpreter provides a Message-passing Selection scheme(MSScheme) to select the critical paths for GNNs' message aggregations, which aims at reaching the self-explaination instead of post-hoc explanations. In detail, the elaborate MSScheme is designed to calculate weight factors of message aggregation paths by considering the vanilla structure and node embedding components, where the structure base aims at weight factors among node-induced substructures; on the other hand, the node embedding base focuses on weight factors via node embeddings obtained by one-layer GNN.Finally, we demonstrate the effectiveness of our approach on graph classification benchmarks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108635077",
                        "name": "Wen-Ding Li"
                    },
                    {
                        "authorId": "145937448",
                        "name": "Kaixuan Chen"
                    },
                    {
                        "authorId": "2128786021",
                        "name": "Shunyu Liu"
                    },
                    {
                        "authorId": null,
                        "name": "Wenjie Huang"
                    },
                    {
                        "authorId": "1739347431",
                        "name": "Haofei Zhang"
                    },
                    {
                        "authorId": "2218856462",
                        "name": "Yingjie Tian"
                    },
                    {
                        "authorId": "2218839057",
                        "name": "Yun Su"
                    },
                    {
                        "authorId": "2152127912",
                        "name": "Mingli Song"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "ea3ecb8b809e7d5ae1bbc267863c0c4e72401a68",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-01958",
                    "ArXiv": "2306.01958",
                    "DOI": "10.48550/arXiv.2306.01958",
                    "CorpusId": 259075297
                },
                "corpusId": 259075297,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ea3ecb8b809e7d5ae1bbc267863c0c4e72401a68",
                "title": "A Survey on Explainability of Graph Neural Networks",
                "abstract": "Graph neural networks (GNNs) are powerful graph-based deep-learning models that have gained significant attention and demonstrated remarkable performance in various domains, including natural language processing, drug discovery, and recommendation systems. However, combining feature information and combinatorial graph structures has led to complex non-linear GNN models. Consequently, this has increased the challenges of understanding the workings of GNNs and the underlying reasons behind their predictions. To address this, numerous explainability methods have been proposed to shed light on the inner mechanism of the GNNs. Explainable GNNs improve their security and enhance trust in their recommendations. This survey aims to provide a comprehensive overview of the existing explainability techniques for GNNs. We create a novel taxonomy and hierarchy to categorize these methods based on their objective and methodology. We also discuss the strengths, limitations, and application scenarios of each category. Furthermore, we highlight the key evaluation metrics and datasets commonly used to assess the explainability of GNNs. This survey aims to assist researchers and practitioners in understanding the existing landscape of explainability methods, identifying gaps, and fostering further advancements in interpretable graph-based machine learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2219548757",
                        "name": "Jaykumar Kakkad"
                    },
                    {
                        "authorId": "2219549243",
                        "name": "Jaspal Jannu"
                    },
                    {
                        "authorId": "1571168324",
                        "name": "Kartik Sharma"
                    },
                    {
                        "authorId": "1682418",
                        "name": "C. Aggarwal"
                    },
                    {
                        "authorId": "3390598",
                        "name": "Sourav Medya"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "To address the weakness of whole-graph aggregation washing out the influence of small malicious subgraph modifications, PROVEXPLAINER\u2019s DT explanations can be combined with other subgraph explanations techniques (e.g., SubgraphX) which were originally not suitable for security domain, but can be tied to problem space actions by PROVEXPLAINER\u2019s interpretable features.",
                "In future work, we plan on combining the strengths of PROVEXPLAINER with those of generic GNNbased explainers e.g., GNNExplainer and SubgraphX, we hope to contextualize the relative degrees of global trends and edge-case logic involved in a particular prediction.",
                "Black box methods, such as XGNN [55] and SubgraphX [38], only need to access the inputs and outputs of the GNN."
            ],
            "citingPaper": {
                "paperId": "f6861ce39bad377f05337653eae78102f2c0a510",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-00934",
                    "ArXiv": "2306.00934",
                    "DOI": "10.48550/arXiv.2306.00934",
                    "CorpusId": 258999422
                },
                "corpusId": 258999422,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f6861ce39bad377f05337653eae78102f2c0a510",
                "title": "Interpreting GNN-based IDS Detections Using Provenance Graph Structural Features",
                "abstract": "The black-box nature of complex Neural Network (NN)-based models has hindered their widespread adoption in security domains due to the lack of logical explanations and actionable follow-ups for their predictions. To enhance the transparency and accountability of Graph Neural Network (GNN) security models used in system provenance analysis, we propose PROVEXPLAINER, a framework for projecting abstract GNN decision boundaries onto interpretable feature spaces. We first replicate the decision-making process of GNNbased security models using simpler and explainable models such as Decision Trees (DTs). To maximize the accuracy and fidelity of the surrogate models, we propose novel graph structural features founded on classical graph theory and enhanced by extensive data study with security domain knowledge. Our graph structural features are closely tied to problem-space actions in the system provenance domain, which allows the detection results to be explained in descriptive, human language. PROVEXPLAINER allowed simple DT models to achieve 95% fidelity to the GNN on program classification tasks with general graph structural features, and 99% fidelity on malware detection tasks with a task-specific feature package tailored for direct interpretation. The explanations for malware classification are demonstrated with case studies of five real-world malware samples across three malware families.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2218792397",
                        "name": "Kunal Mukherjee"
                    },
                    {
                        "authorId": "2219236005",
                        "name": "Joshua Wiedemeier"
                    },
                    {
                        "authorId": "49980880",
                        "name": "Tianhao Wang"
                    },
                    {
                        "authorId": "2169156186",
                        "name": "Muhyun Kim"
                    },
                    {
                        "authorId": "2156361901",
                        "name": "Feng Chen"
                    },
                    {
                        "authorId": "1741044",
                        "name": "Murat Kantarcioglu"
                    },
                    {
                        "authorId": "3344254",
                        "name": "Kangkook Jee"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Subgraph explanation [93, 94] is one of the most persuasive ways to explain GNNs.",
                "[93] Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji."
            ],
            "citingPaper": {
                "paperId": "137a3d9e519d6aabd7a35500324913a2291688b2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-01103",
                    "ArXiv": "2306.01103",
                    "DOI": "10.48550/arXiv.2306.01103",
                    "CorpusId": 259063744
                },
                "corpusId": 259063744,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/137a3d9e519d6aabd7a35500324913a2291688b2",
                "title": "Joint Learning of Label and Environment Causal Independence for Graph Out-of-Distribution Generalization",
                "abstract": "We tackle the problem of graph out-of-distribution (OOD) generalization. Existing graph OOD algorithms either rely on restricted assumptions or fail to exploit environment information in training data. In this work, we propose to simultaneously incorporate label and environment causal independence (LECI) to fully make use of label and environment information, thereby addressing the challenges faced by prior methods on identifying causal and invariant subgraphs. We further develop an adversarial training strategy to jointly optimize these two properties for causal subgraph discovery with theoretical guarantees. Extensive experiments and analysis show that LECI significantly outperforms prior methods on both synthetic and real-world datasets, establishing LECI as a practical and effective solution for graph OOD generalization.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1914700964",
                        "name": "Shurui Gui"
                    },
                    {
                        "authorId": "38813990",
                        "name": "Meng Liu"
                    },
                    {
                        "authorId": "2118053386",
                        "name": "Xiner Li"
                    },
                    {
                        "authorId": "2004524780",
                        "name": "Youzhi Luo"
                    },
                    {
                        "authorId": "1743600",
                        "name": "Shuiwang Ji"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "the unique contributions of each feature to the final prediction, we calculated the SHapley Additive exPlanations (SHAP) values for each model [28].",
                "To examine the unique contributions of each feature to the final prediction, we calculated the SHapley Additive exPlanations (SHAP) values for each model [28]."
            ],
            "citingPaper": {
                "paperId": "28e9e1078972606f738ef17ac10a94cf7e1a9b31",
                "externalIds": {
                    "PubMedCentral": "10303631",
                    "DOI": "10.3390/jpm13061004",
                    "CorpusId": 259242014,
                    "PubMed": "37373993"
                },
                "corpusId": 259242014,
                "publicationVenue": {
                    "id": "721ed878-9136-408b-b2a3-dce27b9fa39e",
                    "name": "Journal of Personalized Medicine",
                    "type": "journal",
                    "alternate_names": [
                        "J Pers Med"
                    ],
                    "issn": "2075-4426",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-225205",
                    "alternate_urls": [
                        "https://www.mdpi.com/journal/jpm",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-225205"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/28e9e1078972606f738ef17ac10a94cf7e1a9b31",
                "title": "Dynomics: A Novel and Promising Approach for Improved Breast Cancer Prognosis Prediction",
                "abstract": "Traditional imaging techniques for breast cancer (BC) diagnosis and prediction, such as X-rays and magnetic resonance imaging (MRI), demonstrate varying sensitivity and specificity due to clinical and technological factors. Consequently, positron emission tomography (PET), capable of detecting abnormal metabolic activity, has emerged as a more effective tool, providing critical quantitative and qualitative tumor-related metabolic information. This study leverages a public clinical dataset of dynamic 18F-Fluorothymidine (FLT) PET scans from BC patients, extending conventional static radiomics methods to the time domain\u2014termed as \u2018Dynomics\u2019. Radiomic features were extracted from both static and dynamic PET images on lesion and reference tissue masks. The extracted features were used to train an XGBoost model for classifying tumor versus reference tissue and complete versus partial responders to neoadjuvant chemotherapy. The results underscored the superiority of dynamic and static radiomics over standard PET imaging, achieving accuracy of 94% in tumor tissue classification. Notably, in predicting BC prognosis, dynomics delivered the highest performance, achieving accuracy of 86%, thereby outperforming both static radiomics and standard PET data. This study illustrates the enhanced clinical utility of dynomics in yielding more precise and reliable information for BC diagnosis and prognosis, paving the way for improved treatment strategies.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47065413",
                        "name": "M. Inglese"
                    },
                    {
                        "authorId": "2107003923",
                        "name": "Matteo Ferrante"
                    },
                    {
                        "authorId": "2101384782",
                        "name": "Tommaso Boccato"
                    },
                    {
                        "authorId": "143914561",
                        "name": "A. Conti"
                    },
                    {
                        "authorId": "2144255134",
                        "name": "C. A. Pistolese"
                    },
                    {
                        "authorId": "144034283",
                        "name": "O. Buonomo"
                    },
                    {
                        "authorId": "1398808186",
                        "name": "R. D'Angelillo"
                    },
                    {
                        "authorId": "2958393",
                        "name": "N. Toschi"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "The potential causes can be found with the aid of existing GNN explanation tools such as PGExplainer [26], SubgraphX [50], FexIoT [41] which identify important subgraphs or nodes accounting for the GNN prediction."
            ],
            "citingPaper": {
                "paperId": "fdb8f4ab2e32e6dfd66e2bc422ecf22862fac088",
                "externalIds": {
                    "DBLP": "journals/pacmmod/WangICWNY23",
                    "DOI": "10.1145/3588956",
                    "CorpusId": 259077170
                },
                "corpusId": 259077170,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fdb8f4ab2e32e6dfd66e2bc422ecf22862fac088",
                "title": "Graph Learning for Interactive Threat Detection in Heterogeneous Smart Home Rule Data",
                "abstract": "The interactions among automation configuration rule data have led to undesired and insecure issues in smart homes, which are known as interactive threats. Most existing solutions use program analysis to identify interactive threats among automation rules, which is not suitable for closed-source platforms. Meanwhile, security policy-based solutions suffer from low detection accuracy because the pre-defined security policies in a single platform can hardly cover diverse interactive threat types across heterogeneous platforms. In this paper, we propose Glint, the first graph learning-based system for interactive threat detection in smart homes. We design a multi-scale graph representation learning model, called ITGNN, for both homogeneous and heterogeneous interaction graph pattern learning. To facilitate graph learning, we build large interaction graph training datasets by multi-domain data fusion from five different platforms. Moreover, Glint detects drifting samples with contrastive learning and improves the generalization ability with transfer learning across heterogeneous platforms. Our evaluation shows that Glint achieves 95.5% accuracy in detecting interactive threats across the five platforms. Besides, we examine a set of user-designed blueprints in the Home Assistant platform and reveal four new types of real-world interactive threats, called \"action block\", \"action ablation\", \"trigger intake\", and \"condition duplicate\", which are cross-platform interactive threats captured by Glint.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3474846",
                        "name": "Guangjing Wang"
                    },
                    {
                        "authorId": "2059895293",
                        "name": "Nikolay Ivanov"
                    },
                    {
                        "authorId": "47440030",
                        "name": "Bocheng Chen"
                    },
                    {
                        "authorId": "2151571370",
                        "name": "Qi Wang"
                    },
                    {
                        "authorId": "1810147",
                        "name": "Thanhvu Nguyen"
                    },
                    {
                        "authorId": "2480351",
                        "name": "Qiben Yan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Hence, recent studies focus on discovering core substructures that are highly related to the functionalities of the given graph [83, 84]."
            ],
            "citingPaper": {
                "paperId": "01e7326be04eb1f6e654d86a316cfebc2985beb4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-18451",
                    "ArXiv": "2305.18451",
                    "DOI": "10.1145/3580305.3599437",
                    "CorpusId": 258967445
                },
                "corpusId": 258967445,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/01e7326be04eb1f6e654d86a316cfebc2985beb4",
                "title": "Shift-Robust Molecular Relational Learning with Causal Substructure",
                "abstract": "Recently, molecular relational learning, whose goal is to predict the interaction behavior between molecular pairs, got a surge of interest in molecular sciences due to its wide range of applications. In this work, we propose CMRL that is robust to the distributional shift in molecular relational learning by detecting the core substructure that is causally related to chemical reactions. To do so, we first assume a causal relationship based on the domain knowledge of molecular sciences and construct a structural causal model (SCM) that reveals the relationship between variables. Based on the SCM, we introduce a novel conditional intervention framework whose intervention is conditioned on the paired molecule. With the conditional intervention framework, our model successfully learns from the causal substructure and alleviates the confounding effect of shortcut substructures that are spuriously correlated to chemical reactions. Extensive experiments on various tasks with real-world and synthetic datasets demonstrate the superiority of CMRL over state-of-the-art baseline models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2153998505",
                        "name": "Namkyeong Lee"
                    },
                    {
                        "authorId": "2182294722",
                        "name": "Kanghoon Yoon"
                    },
                    {
                        "authorId": "51113624",
                        "name": "Gyoung S. Na"
                    },
                    {
                        "authorId": "98057309",
                        "name": "Sein Kim"
                    },
                    {
                        "authorId": "2109120259",
                        "name": "Chanyoung Park"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Several post-hoc explainers have been proposed for explaining Graph Neural Networks\u2019 predictions using subgraphs [1, 2, 30, 3, 31, 29].",
                "We also used transductive explainers (GNNExplainer [1], SubgraphX [30], CF2 [31]) for a Planted Clique case study (Supp.",
                "We also used transductive explainers (GNNExplainer [1], SubgraphX [30], CF(2) [31]) for a Planted Clique case study (Supp."
            ],
            "citingPaper": {
                "paperId": "02bed25ef9cc83d25515216dae0d14df1892a502",
                "externalIds": {
                    "ArXiv": "2305.15745",
                    "DBLP": "journals/corr/abs-2305-15745",
                    "DOI": "10.48550/arXiv.2305.15745",
                    "CorpusId": 258888221
                },
                "corpusId": 258888221,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/02bed25ef9cc83d25515216dae0d14df1892a502",
                "title": "Robust Ante-hoc Graph Explainer using Bilevel Optimization",
                "abstract": "Explaining the decisions made by machine learning models for high-stakes applications is critical for increasing transparency and guiding improvements to these decisions. This is particularly true in the case of models for graphs, where decisions often depend on complex patterns combining rich structural and attribute data. While recent work has focused on designing so-called post-hoc explainers, the question of what constitutes a good explanation remains open. One intuitive property is that explanations should be sufficiently informative to enable humans to approximately reproduce the predictions given the data. However, we show that post-hoc explanations do not achieve this goal as their explanations are highly dependent on fixed model parameters (e.g., learned GNN weights). To address this challenge, this paper proposes RAGE (Robust Ante-hoc Graph Explainer), a novel and flexible ante-hoc explainer designed to discover explanations for a broad class of graph neural networks using bilevel optimization. RAGE is able to efficiently identify explanations that contain the full information needed for prediction while still enabling humans to rank these explanations based on their influence. Our experiments, based on graph classification and regression, show that RAGE explanations are more robust than existing post-hoc and ante-hoc approaches and often achieve similar or better accuracy than state-of-the-art models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2134769449",
                        "name": "Mert Kosan"
                    },
                    {
                        "authorId": "2110040391",
                        "name": "A. Silva"
                    },
                    {
                        "authorId": "1399890865",
                        "name": "Ambuj K. Singh"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "We randomly select a number of similar molecules and visualize the explanations generated by DEGREE and SubgraphX.",
                "We can find that DEGREE has competitive performance compared to GNN-LRP and SubgraphX.",
                "F.2 QUALITATIVE COMPARISON\nIn this section we make a qualitative comparison between DEGREE and SubgraphX.",
                "Figure 8 shows the ACC of DEGREE, GNN-LRP and SubgraphX under various sparsity.",
                "We can find that none of the subgraphs generated by SubgraphX include the \u2019N-",
                "F.1 QUANTITATIVE EVALUATION\nIn this section, we perform additional experiments comparing DEGREE with GNN-LRP Schnake et al. (2020) and SubgraphX Yuan et al. (2021)."
            ],
            "citingPaper": {
                "paperId": "53e41177e6d0deefc3d335e886b72d60e6e37cec",
                "externalIds": {
                    "ArXiv": "2305.12895",
                    "DBLP": "conf/iclr/FengLYTDH22",
                    "DOI": "10.48550/arXiv.2305.12895",
                    "CorpusId": 251649165
                },
                "corpusId": 251649165,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/53e41177e6d0deefc3d335e886b72d60e6e37cec",
                "title": "DEGREE: Decomposition Based Explanation for Graph Neural Networks",
                "abstract": "Graph Neural Networks (GNNs) are gaining extensive attention for their application in graph data. However, the black-box nature of GNNs prevents users from understanding and trusting the models, thus hampering their applicability. Whereas explaining GNNs remains a challenge, most existing methods fall into approximation based and perturbation based approaches with suffer from faithfulness problems and unnatural artifacts, respectively. To tackle these problems, we propose DEGREE \\degree to provide a faithful explanation for GNN predictions. By decomposing the information generation and aggregation mechanism of GNNs, DEGREE allows tracking the contributions of specific components of the input graph to the final prediction. Based on this, we further design a subgraph level interpretation algorithm to reveal complex interactions between graph nodes that are overlooked by previous methods. The efficiency of our algorithm can be further improved by utilizing GNN characteristics. Finally, we conduct quantitative and qualitative experiments on synthetic and real-world datasets to demonstrate the effectiveness of DEGREE on node classification and graph classification tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2151233715",
                        "name": "Qizhang Feng"
                    },
                    {
                        "authorId": "47717322",
                        "name": "Ninghao Liu"
                    },
                    {
                        "authorId": "47829900",
                        "name": "Fan Yang"
                    },
                    {
                        "authorId": "2057059798",
                        "name": "Ruixiang Tang"
                    },
                    {
                        "authorId": "3432460",
                        "name": "Mengnan Du"
                    },
                    {
                        "authorId": "2123553641",
                        "name": "Xia Hu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "d0477e6be78c2fb558488c2fcb301678af570904",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-12578",
                    "ArXiv": "2305.12578",
                    "DOI": "10.48550/arXiv.2305.12578",
                    "CorpusId": 258832424
                },
                "corpusId": 258832424,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d0477e6be78c2fb558488c2fcb301678af570904",
                "title": "Self-Explainable Graph Neural Networks for Link Prediction",
                "abstract": "Graph Neural Networks (GNNs) have achieved state-of-the-art performance for link prediction. However, GNNs suffer from poor interpretability, which limits their adoptions in critical scenarios that require knowing why certain links are predicted. Despite various methods proposed for the explainability of GNNs, most of them are post-hoc explainers developed for explaining node classification. Directly adopting existing post-hoc explainers for explaining link prediction is sub-optimal because: (i) post-hoc explainers usually adopt another strategy or model to explain a target model, which could misinterpret the target model; and (ii) GNN explainers for node classification identify crucial subgraphs around each node for the explanation; while for link prediction, one needs to explain the prediction for each pair of nodes based on graph structure and node attributes. Therefore, in this paper, we study a novel problem of self-explainable GNNs for link prediction, which can simultaneously give accurate predictions and explanations. Concretely, we propose a new framework and it can find various $K$ important neighbors of one node to learn pair-specific representations for links from this node to other nodes. These $K$ different neighbors represent important characteristics of the node and model various factors for links from it. Thus, $K$ neighbors can provide explanations for the existence of links. Experiments on both synthetic and real-world datasets verify the effectiveness of the proposed framework for link prediction and explanation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1643792176",
                        "name": "Huaisheng Zhu"
                    },
                    {
                        "authorId": "153640788",
                        "name": "Dongsheng Luo"
                    },
                    {
                        "authorId": "48784944",
                        "name": "Xianfeng Tang"
                    },
                    {
                        "authorId": "2150638259",
                        "name": "Junjie Xu"
                    },
                    {
                        "authorId": "2159247988",
                        "name": "Hui Liu"
                    },
                    {
                        "authorId": "2116430057",
                        "name": "Suhang Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Due to the intended objective to learn the CFG node importance, the model is capable of addressing adversarial evasion techniques such as XOR obfuscation, Sematic NOP obfuscation, code manipulation, etc. Experiments with YANCFG dataset [20] over three state-of-the-art models (GNNExplainer [30], SubgraphX [31], and PGExplainer [32]) justify the feasibility of the approach.",
                "Experiments with YANCFG dataset [20] over three state-of-the-art models (GNNExplainer [30], SubgraphX [31], and PGExplainer [32]) justify the feasibility of the approach."
            ],
            "citingPaper": {
                "paperId": "75411bb007f63decc7b34e637282e57ebfa81335",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-08993",
                    "ArXiv": "2305.08993",
                    "DOI": "10.48550/arXiv.2305.08993",
                    "CorpusId": 258714699
                },
                "corpusId": 258714699,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/75411bb007f63decc7b34e637282e57ebfa81335",
                "title": "Survey of Malware Analysis through Control Flow Graph using Machine Learning",
                "abstract": "Malware is a significant threat to the security of computer systems and networks which requires sophisticated techniques to analyze the behavior and functionality for detection. Traditional signature-based malware detection methods have become ineffective in detecting new and unknown malware due to their rapid evolution. One of the most promising techniques that can overcome the limitations of signature-based detection is to use control flow graphs (CFGs). CFGs leverage the structural information of a program to represent the possible paths of execution as a graph, where nodes represent instructions and edges represent control flow dependencies. Machine learning (ML) algorithms are being used to extract these features from CFGs and classify them as malicious or benign. In this survey, we aim to review some state-of-the-art methods for malware detection through CFGs using ML, focusing on the different ways of extracting, representing, and classifying. Specifically, we present a comprehensive overview of different types of CFG features that have been used as well as different ML algorithms that have been applied to CFG-based malware detection. We provide an in-depth analysis of the challenges and limitations of these approaches, as well as suggest potential solutions to address some open problems and promising future directions for research in this field.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2150699855",
                        "name": "Shaswata Mitra"
                    },
                    {
                        "authorId": "40402638",
                        "name": "Stephen Torri"
                    },
                    {
                        "authorId": "2736774",
                        "name": "Sudip Mittal"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "\u2026et al., 2018; Zhao & Akoglu, 2020; Oono & Suzuki, 2020a; Rong et al., 2020), interpretability (Ying et al., 2019; Luo et al., 2020; Vu & Thai, 2020; Yuan et al., 2020; 2021), expressiveness (Xu et al., 2019; Chen et al., 2019; Maron et al., 2019; Dehmamy et al., 2019; Feng et al., 2022), and\u2026"
            ],
            "citingPaper": {
                "paperId": "206ee73f4e3187b5a71c3569bb1d443b8cce9351",
                "externalIds": {
                    "ArXiv": "2305.08048",
                    "DBLP": "journals/corr/abs-2305-08048",
                    "DOI": "10.48550/arXiv.2305.08048",
                    "CorpusId": 258685824
                },
                "corpusId": 258685824,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/206ee73f4e3187b5a71c3569bb1d443b8cce9351",
                "title": "Towards Understanding the Generalization of Graph Neural Networks",
                "abstract": "Graph neural networks (GNNs) are the most widely adopted model in graph-structured data oriented learning and representation. Despite their extraordinary success in real-world applications, understanding their working mechanism by theory is still on primary stage. In this paper, we move towards this goal from the perspective of generalization. To be specific, we first establish high probability bounds of generalization gap and gradients in transductive learning with consideration of stochastic optimization. After that, we provide high probability bounds of generalization gap for popular GNNs. The theoretical results reveal the architecture specific factors affecting the generalization gap. Experimental results on benchmark datasets show the consistency between theoretical results and empirical evidence. Our results provide new insights in understanding the generalization of GNNs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2112701620",
                        "name": "Huayi Tang"
                    },
                    {
                        "authorId": "93006732",
                        "name": "Y. Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[52] Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji."
            ],
            "citingPaper": {
                "paperId": "2b333a51c829681c41f0879a3c0241ea8ff559a7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-13010",
                    "ArXiv": "2304.13010",
                    "DOI": "10.48550/arXiv.2304.13010",
                    "CorpusId": 258309271
                },
                "corpusId": 258309271,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2b333a51c829681c41f0879a3c0241ea8ff559a7",
                "title": "Unstructured and structured data: Can we have the best of both worlds with large language models?",
                "abstract": "This paper presents an opinion on the potential of using large language models to query on both unstructured and structured data. It also outlines some research challenges related to the topic of building question-answering systems for both types of data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "34582619",
                        "name": "W. Tan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recently, the explainability of graph neural networks (GNNs) [19, 20, 21, 22, 23, 24] has been explored [25, 26, 27, 28, 29, 30, 31], but these studies are limited to understanding supervised GNNs.",
                "For example, GNNExplainer [25], SubgraphX [28], CP-Explainer [48], and GraphMask [47] leverage mutual information, Shapley values, conformal prediction, and divergence, respectively, using both the original and affected label predictions, which are predicted on the candidate explanation subgraph."
            ],
            "citingPaper": {
                "paperId": "e433e6712ec2475f1c9ce8cb0a715edd96ed4fc1",
                "externalIds": {
                    "ArXiv": "2304.12036",
                    "DBLP": "journals/corr/abs-2304-12036",
                    "DOI": "10.1016/j.neunet.2023.04.029",
                    "CorpusId": 258298674,
                    "PubMed": "37210973"
                },
                "corpusId": 258298674,
                "publicationVenue": {
                    "id": "a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                    "name": "Neural Networks",
                    "type": "journal",
                    "alternate_names": [
                        "Neural Netw"
                    ],
                    "issn": "0893-6080",
                    "url": "http://www.elsevier.com/locate/neunet",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/841/description",
                        "http://www.sciencedirect.com/science/journal/08936080"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e433e6712ec2475f1c9ce8cb0a715edd96ed4fc1",
                "title": "Generating Post-hoc Explanations for Skip-gram-based Node Embeddings by Identifying Important Nodes with Bridgeness",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1845151",
                        "name": "Hogun Park"
                    },
                    {
                        "authorId": "144050371",
                        "name": "Jennifer Neville"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "A small subset of GNN explainers, especially local, perturbation-based methods (such as Yuan et al. (2021); Duval & Malliaros (2021); Luo et al. (2020); Ying et al. (2019)) can be applied to black-box models."
            ],
            "citingPaper": {
                "paperId": "a4ab2885279de2a34e7554aff01ce4c2a8512cf3",
                "externalIds": {
                    "DBLP": "conf/iclr/MasoomiHXHSCID22",
                    "ArXiv": "2304.07670",
                    "DOI": "10.48550/arXiv.2304.07670",
                    "CorpusId": 251648935
                },
                "corpusId": 251648935,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a4ab2885279de2a34e7554aff01ce4c2a8512cf3",
                "title": "Explanations of Black-Box Models based on Directional Feature Interactions",
                "abstract": "As machine learning algorithms are deployed ubiquitously to a variety of domains, it is imperative to make these often black-box models transparent. Several recent works explain black-box models by capturing the most influential features for prediction per instance; such explanation methods are univariate, as they characterize importance per feature. We extend univariate explanation to a higher-order; this enhances explainability, as bivariate methods can capture feature interactions in black-box models, represented as a directed graph. Analyzing this graph enables us to discover groups of features that are equally important (i.e., interchangeable), while the notion of directionality allows us to identify the most influential features. We apply our bivariate method on Shapley value explanations, and experimentally demonstrate the ability of directional explanations to discover feature interactions. We show the superiority of our method against state-of-the-art on CIFAR10, IMDB, Census, Divorce, Drug, and gene data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "146126409",
                        "name": "A. Masoomi"
                    },
                    {
                        "authorId": "2164482366",
                        "name": "Davin Hill"
                    },
                    {
                        "authorId": "50070300",
                        "name": "Zhonghui Xu"
                    },
                    {
                        "authorId": "3157841",
                        "name": "C. Hersh"
                    },
                    {
                        "authorId": "9875892",
                        "name": "E. Silverman"
                    },
                    {
                        "authorId": "35190471",
                        "name": "P. Castaldi"
                    },
                    {
                        "authorId": "1776006",
                        "name": "Stratis Ioannidis"
                    },
                    {
                        "authorId": "144729179",
                        "name": "Jennifer G. Dy"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "79c11f2bce801395f76f6ac9cf1ff9b0acb73612",
                "externalIds": {
                    "DBLP": "journals/fi/FabreASBE23",
                    "DOI": "10.3390/fi15040147",
                    "CorpusId": 258093718
                },
                "corpusId": 258093718,
                "publicationVenue": {
                    "id": "c3e5f1c8-9ba7-47e5-acde-53063a69d483",
                    "name": "Future Internet",
                    "type": "journal",
                    "issn": "1999-5903",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-156830",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-156830",
                        "https://www.mdpi.com/journal/futureinternet"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/79c11f2bce801395f76f6ac9cf1ff9b0acb73612",
                "title": "A Multiverse Graph to Help Scientific Reasoning from Web Usage: Interpretable Patterns of Assessor Shifts in GRAPHYP",
                "abstract": "The digital support for scientific reasoning presents contrasting results. Bibliometric services are improving, but not academic assessment; no service for scholars relies on logs of web usage to base query strategies for relevance judgments (or assessor shifts). Our Scientific Knowledge Graph GRAPHYP innovates with interpretable patterns of web usage, providing scientific reasoning with conceptual fingerprints and helping identify eligible hypotheses. In a previous article, we showed how usage log data, in the form of \u2018documentary tracks\u2019, help determine distinct cognitive communities (called adversarial cliques) within sub-graphs. A typology of these documentary tracks through a triplet of measurements from logs (intensity, variety and attention) describes the potential approaches to a (research) question. GRAPHYP assists interpretation as a classifier, with possibilistic graphical modeling. This paper (Paper 2) shows what this approach can bring to scientific reasoning; it involves visualizing complete interpretable pathways, in a multi-hop assessor shift, which users can then explore toward the \u2018best possible solution\u2019\u2014the one that is most consistent with their hypotheses. Applying the Leibnizian paradigm of scientific reasoning, GRAPHYP highlights infinitesimal learning pathways, as a \u2018multiverse\u2019 geometric graph in modeling possible search strategies answering research questions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "48295441",
                        "name": "Renaud Fabre"
                    },
                    {
                        "authorId": "40866415",
                        "name": "Otmane Azeroual"
                    },
                    {
                        "authorId": "2164645",
                        "name": "Joachim Sch\u00f6pfel"
                    },
                    {
                        "authorId": "1794810",
                        "name": "P. Bellot"
                    },
                    {
                        "authorId": "1829136",
                        "name": "D. Egret"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "9eee6d56a2815e19a63a32dddda7c3383514d1e8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-02277",
                    "ArXiv": "2304.02277",
                    "DOI": "10.1109/IJCNN54540.2023.10191949",
                    "CorpusId": 257952370
                },
                "corpusId": 257952370,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/9eee6d56a2815e19a63a32dddda7c3383514d1e8",
                "title": "Rethinking the Trigger-injecting Position in Graph Backdoor Attack",
                "abstract": "Backdoor attacks have been demonstrated as a security threat for machine learning models. Traditional backdoor attacks intend to inject backdoor functionality into the model such that the backdoored model will perform abnormally on inputs with predefined backdoor triggers and still retain state-of-the-art performance on the clean inputs. While there are already some works on backdoor attacks on Graph Neural Networks (GNNs), the backdoor trigger in the graph domain is mostly injected into random positions of the sample. There is no work analyzing and explaining the backdoor attack performance when injecting triggers into the most important or least important area in the sample, which we refer to as trigger-injecting strategies MIAS and LIAS, respectively. Our results show that, generally, LIAS performs better, and the differences between the LIAS and MIAS performance can be significant. Furthermore, we explain these two strategies\u2019 similar (better) attack performance through explanation techniques, which results in a further understanding of backdoor attacks in GNNs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2155954578",
                        "name": "Jing Xu"
                    },
                    {
                        "authorId": "2144883147",
                        "name": "Gorka Abad"
                    },
                    {
                        "authorId": "1686538",
                        "name": "S. Picek"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Some present a new version of SHAP adapted to a particular type of input data\u2014e.g., text (Chen et al. 2020) and graphs (Yuan et al. 2021)\u2014and to specific models, e.g., random forests (Lundberg et al. 2018)."
            ],
            "citingPaper": {
                "paperId": "109fbe4cee816ba9e0ffcc08a36094ad92e4c2a5",
                "externalIds": {
                    "DOI": "10.3390/jrfm16040221",
                    "CorpusId": 257947440
                },
                "corpusId": 257947440,
                "publicationVenue": {
                    "id": "5bccd387-3836-42f2-9b60-9c190333ae01",
                    "name": "Journal of Risk and Financial Management",
                    "type": "journal",
                    "alternate_names": [
                        "J Risk Financial Manag"
                    ],
                    "issn": "1911-8066",
                    "url": "https://www.mdpi.com/journal/jrfm",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-318032"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/109fbe4cee816ba9e0ffcc08a36094ad92e4c2a5",
                "title": "Explaining Deep Learning Models for Credit Scoring with SHAP: A Case Study Using Open Banking Data",
                "abstract": "Predicting creditworthiness is an important task in the banking industry, as it allows banks to make informed lending decisions and manage risk. In this paper, we investigate the performance of two different deep learning credit scoring models developed on the textual descriptions of customer transactions available from open banking APIs. The first model is a deep learning model trained from scratch, while the second model uses transfer learning with a multilingual BERT model. We evaluate the predictive performance of these models using the area under the receiver operating characteristic curve (AUC) and Brier score. We find that a deep learning model trained from scratch outperforms a BERT transformer model finetuned on the same data. Furthermore, we find that SHAP can be used to explain such models both on a global level and for explaining rejections of actual applications.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2132663904",
                        "name": "Lars Ole Hjelkrem"
                    },
                    {
                        "authorId": "2649274",
                        "name": "P. E. Lange"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "5ba51a86d48bbfb82d7493375708cc3484450fa9",
                "externalIds": {
                    "DBLP": "conf/icde/WangGLLY23",
                    "DOI": "10.1109/ICDE55515.2023.00120",
                    "CorpusId": 260171471
                },
                "corpusId": 260171471,
                "publicationVenue": {
                    "id": "764e3630-ddac-4c21-af4b-9d32ffef082e",
                    "name": "IEEE International Conference on Data Engineering",
                    "type": "conference",
                    "alternate_names": [
                        "ICDE",
                        "Int Conf Data Eng",
                        "IEEE Int Conf Data Eng",
                        "International Conference on Data Engineering"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1331"
                },
                "url": "https://www.semanticscholar.org/paper/5ba51a86d48bbfb82d7493375708cc3484450fa9",
                "title": "Federated IoT Interaction Vulnerability Analysis",
                "abstract": "IoT devices provide users with great convenience in smart homes. However, the interdependent behaviors across devices may yield unexpected interactions. To analyze the potential IoT interaction vulnerabilities, in this paper, we propose a federated and explicable IoT interaction data management system FexIoT. To address the lack of information in the closed-source platforms, FexIoT captures causality information by fusing multi-domain data, including the descriptions of apps and real-time event logs, into interaction graphs. The interaction graph representation is encoded by graph neural networks (GNNs). To collaboratively train the GNN model without sharing the raw data, we design a layer-wise clustering-based federated GNN framework for learning intrinsic clustering relationships among GNN model weights, which copes with the statistical heterogeneity and the concept drift problem of graph data. In addition, we propose the Monte Carlo beam search with the SHAP method to search and measure the risk of subgraphs, in order to explain the potential vulnerability causes. We evaluate our prototype on datasets collected from five IoT automation platforms. The results show that FexIoT achieves more than 90% average accuracy for interaction vulnerability detection, outperforming the existing methods. Moreover, FexIoT offers an explainable result for the detected vulnerabilities.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3474846",
                        "name": "Guangjing Wang"
                    },
                    {
                        "authorId": "7014630",
                        "name": "Hanqing Guo"
                    },
                    {
                        "authorId": "2112838196",
                        "name": "Anran Li"
                    },
                    {
                        "authorId": "2225046620",
                        "name": "Xiaorui Liu"
                    },
                    {
                        "authorId": "2480351",
                        "name": "Qiben Yan"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Concerning this particular task, CFGExplainer outperforms other explainability frameworks like GNNExplainer [139], SubgraphX [140] and PGExplainer [141]."
            ],
            "citingPaper": {
                "paperId": "cea911f3a903a03b6333f2211cbde1c4b9ff82e8",
                "externalIds": {
                    "ArXiv": "2303.16004",
                    "DBLP": "journals/corr/abs-2303-16004",
                    "DOI": "10.48550/arXiv.2303.16004",
                    "CorpusId": 257771694
                },
                "corpusId": 257771694,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cea911f3a903a03b6333f2211cbde1c4b9ff82e8",
                "title": "A Survey on Malware Detection with Graph Representation Learning",
                "abstract": "Malware detection has become a major concern due to the increasing number and complexity of malware. Traditional detection methods based on signatures and heuristics are used for malware detection, but unfortunately, they suffer from poor generalization to unknown attacks and can be easily circumvented using obfuscation techniques. In recent years, Machine Learning (ML) and notably Deep Learning (DL) achieved impressive results in malware detection by learning useful representations from data and have become a solution preferred over traditional methods. More recently, the application of such techniques on graph-structured data has achieved state-of-the-art performance in various domains and demonstrates promising results in learning more robust representations from malware. Yet, no literature review focusing on graph-based deep learning for malware detection exists. In this survey, we provide an in-depth literature review to summarize and unify existing works under the common approaches and architectures. We notably demonstrate that Graph Neural Networks (GNNs) reach competitive results in learning robust embeddings from malware represented as expressive graph structures, leading to an efficient detection by downstream classifiers. This paper also reviews adversarial attacks that are utilized to fool graph-based detection methods. Challenges and future research directions are discussed at the end of the paper.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2176440737",
                        "name": "Tristan Bilot"
                    },
                    {
                        "authorId": "2046785",
                        "name": "Nour El Madhoun"
                    },
                    {
                        "authorId": "1682056",
                        "name": "K. A. Agha"
                    },
                    {
                        "authorId": "2727096",
                        "name": "Anis Zouaoui"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "1db1bde653658ec9b30858ae14650b8f9c9d438b",
                "externalIds": {
                    "PubMedCentral": "10134429",
                    "DOI": "10.1021/acs.jctc.2c01235",
                    "CorpusId": 257786462,
                    "PubMed": "36972469"
                },
                "corpusId": 257786462,
                "publicationVenue": {
                    "id": "05f4640f-1261-4186-8153-0cb50b1169b3",
                    "name": "Journal of Chemical Theory and Computation",
                    "type": "journal",
                    "alternate_names": [
                        "J Chem Theory Comput"
                    ],
                    "issn": "1549-9618",
                    "url": "http://pubs.acs.org/jctc",
                    "alternate_urls": [
                        "https://pubs.acs.org/journal/jctcce",
                        "http://pubs.acs.org/journals/jctcce/index.html",
                        "http://pubs.acs.org/journal/jctcce"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1db1bde653658ec9b30858ae14650b8f9c9d438b",
                "title": "A Perspective on Explanations of Molecular Prediction Models",
                "abstract": "Chemists can be skeptical in using deep learning (DL) in decision making, due to the lack of interpretability in \u201cblack-box\u201d models. Explainable artificial intelligence (XAI) is a branch of artificial intelligence (AI) which addresses this drawback by providing tools to interpret DL models and their predictions. We review the principles of XAI in the domain of chemistry and emerging methods for creating and evaluating explanations. Then, we focus on methods developed by our group and their applications in predicting solubility, blood\u2013brain barrier permeability, and the scent of molecules. We show that XAI methods like chemical counterfactuals and descriptor explanations can explain DL predictions while giving insight into structure\u2013property relationships. Finally, we discuss how a two-step process of developing a black-box model and explaining predictions can uncover structure\u2013property relationships.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1805407482",
                        "name": "Geemi P Wellawatte"
                    },
                    {
                        "authorId": "95666896",
                        "name": "Heta A. Gandhi"
                    },
                    {
                        "authorId": "1481244794",
                        "name": "Aditi Seshadri"
                    },
                    {
                        "authorId": "2257535",
                        "name": "A. White"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Q3: how to analyze and interpret dynamic node representations or model predictions on large scale graphs over time? Existing works on interpretable GRL often analyze the importance of neighboring nodes\u2019 features to a query node (mostly indicated by the learned attention weights [16] and the gradient-based weights [48]) as well as the importance of the (sub)graph topologies based on the Shapley value [67].",
                "In particular, it enables model-level interpretation instead of instance-level interpretation as done in most existing explainable graph learning works [16], [48], [66], [67].",
                ", in terms of the importance of node attributes indicated by the learned attention coefficients [16] and (sub)graph topologies based on the Shapley value [67]."
            ],
            "citingPaper": {
                "paperId": "d4ab0c94cc2c35265fb14c664ac9a471911a6e0d",
                "externalIds": {
                    "ArXiv": "2303.12341",
                    "DBLP": "journals/corr/abs-2303-12341",
                    "DOI": "10.48550/arXiv.2303.12341",
                    "CorpusId": 257663926
                },
                "corpusId": 257663926,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d4ab0c94cc2c35265fb14c664ac9a471911a6e0d",
                "title": "EasyDGL: Encode, Train and Interpret for Continuous-time Dynamic Graph Learning",
                "abstract": "Dynamic graphs arise in various real-world applications, and it is often welcomed to model the dynamics directly in continuous time domain for its flexibility. This paper aims to design an easy-to-use pipeline (termed as EasyDGL which is also due to its implementation by DGL toolkit) composed of three key modules with both strong fitting ability and interpretability. Specifically the proposed pipeline which involves encoding, training and interpreting: i) a temporal point process (TPP) modulated attention architecture to endow the continuous-time resolution with the coupled spatiotemporal dynamics of the observed graph with edge-addition events; ii) a principled loss composed of task-agnostic TPP posterior maximization based on observed events on the graph, and a task-aware loss with a masking strategy over dynamic graph, where the covered tasks include dynamic link prediction, dynamic node classification and node traffic forecasting; iii) interpretation of the model outputs (e.g., representations and predictions) with scalable perturbation-based quantitative analysis in the graph Fourier domain, which could more comprehensively reflect the behavior of the learned model. Extensive experimental results on public benchmarks show the superior performance of our EasyDGL for time-conditioned predictive tasks, and in particular demonstrate that EasyDGL can effectively quantify the predictive power of frequency content that a model learn from the evolving graph data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2145763884",
                        "name": "Chao Chen"
                    },
                    {
                        "authorId": "2075484158",
                        "name": "Haoyu Geng"
                    },
                    {
                        "authorId": "2119236740",
                        "name": "Nianzu Yang"
                    },
                    {
                        "authorId": "2159107948",
                        "name": "Xiaokang Yang"
                    },
                    {
                        "authorId": "3063894",
                        "name": "Junchi Yan"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "For this explainability problem, we could use GNNExplainer [51], which is designed to derive insights from the hidden layers of GNNs, and SubgraphX [53]."
            ],
            "citingPaper": {
                "paperId": "a1827a3e4a2cbd95c2f241b09a48e1612f89a8dc",
                "externalIds": {
                    "ArXiv": "2303.12812",
                    "DBLP": "journals/corr/abs-2303-12812",
                    "DOI": "10.48550/arXiv.2303.12812",
                    "CorpusId": 257687533
                },
                "corpusId": 257687533,
                "publicationVenue": {
                    "id": "1bb8d145-c7a8-4d44-871b-eaa6acfce304",
                    "name": "Journal of Computer Virology and Hacking Techniques",
                    "type": "journal",
                    "alternate_names": [
                        "Journal in Computer Virology and Hacking Techniques",
                        "J Comput Virol Hacking Tech"
                    ],
                    "issn": "2274-2042",
                    "alternate_issns": [
                        "2263-8733"
                    ],
                    "url": "http://angles.saesfrance.org/",
                    "alternate_urls": [
                        "http://link.springer.com/journal/11416",
                        "https://link.springer.com/journal/volumesAndIssues/11416",
                        "https://web.archive.org/web/*/https:/link.springer.com/journal/volumesAndIssues/11416"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a1827a3e4a2cbd95c2f241b09a48e1612f89a8dc",
                "title": "A Comparison of Graph Neural Networks for Malware Classification",
                "abstract": "Managing the threat posed by malware requires accurate detection and classification techniques. Traditional detection strategies, such as signature scanning, rely on manual analysis of malware to extract relevant features, which is labor intensive and requires expert knowledge. Function call graphs consist of a set of program functions and their inter-procedural calls, providing a rich source of information that can be leveraged to classify malware without the labor intensive feature extraction step of traditional techniques. In this research, we treat malware classification as a graph classification problem. Based on Local Degree Profile features, we train a wide range of Graph Neural Network (GNN) architectures to generate embeddings which we then classify. We find that our best GNN models outperform previous comparable research involving the well-known MalNet-Tiny Android malware dataset. In addition, our GNN models do not suffer from the overfitting issues that commonly afflict non-GNN techniques, although GNN models require longer training times.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "37175498",
                        "name": "Vrinda Malhotra"
                    },
                    {
                        "authorId": "1702192",
                        "name": "Katerina Potika"
                    },
                    {
                        "authorId": "1722355",
                        "name": "M. Stamp"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Although GNN explanations can come in different flavors (Ying et al., 2019; Yuan et al., 2021; Wang et al., 2021; Lucic et al., 2022; Yuan et al., 2020), they usually take the form of (minimal) substructures of input graphs that are highly influential to the prediction we want to explain."
            ],
            "citingPaper": {
                "paperId": "f945b6788d4042c950e57e6032c0ad122566661e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-10139",
                    "ArXiv": "2303.10139",
                    "DOI": "10.48550/arXiv.2303.10139",
                    "CorpusId": 257622939
                },
                "corpusId": 257622939,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f945b6788d4042c950e57e6032c0ad122566661e",
                "title": "Distill n' Explain: explaining graph neural networks using simple surrogates",
                "abstract": "Explaining node predictions in graph neural networks (GNNs) often boils down to finding graph substructures that preserve predictions. Finding these structures usually implies back-propagating through the GNN, bonding the complexity (e.g., number of layers) of the GNN to the cost of explaining it. This naturally begs the question: Can we break this bond by explaining a simpler surrogate GNN? To answer the question, we propose Distill n' Explain (DnX). First, DnX learns a surrogate GNN via knowledge distillation. Then, DnX extracts node or edge-level explanations by solving a simple convex program. We also propose FastDnX, a faster version of DnX that leverages the linear decomposition of our surrogate model. Experiments show that DnX and FastDnX often outperform state-of-the-art GNN explainers while being orders of magnitude faster. Additionally, we support our empirical findings with theoretical results linking the quality of the surrogate model (i.e., distillation error) to the faithfulness of explanations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2061161328",
                        "name": "Tamara A. Pereira"
                    },
                    {
                        "authorId": "2211967383",
                        "name": "Erik Nasciment"
                    },
                    {
                        "authorId": "2164014168",
                        "name": "Lucas Emanuel Resck"
                    },
                    {
                        "authorId": "144128644",
                        "name": "Diego Mesquita"
                    },
                    {
                        "authorId": "3383481",
                        "name": "A. Souza"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Various modifications of SHAP have been developed to explain different machine learning models and tools [46, 47, 48, 49, 50, 51, 52]."
            ],
            "citingPaper": {
                "paperId": "bf39229350661d8a156d5de544967a9e1017f54a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-08625",
                    "ArXiv": "2303.08625",
                    "DOI": "10.1007/s00521-023-08929-8",
                    "CorpusId": 257532578
                },
                "corpusId": 257532578,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bf39229350661d8a156d5de544967a9e1017f54a",
                "title": "Interpretable ensembles of hyper-rectangles as base models",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "50585467",
                        "name": "A. Konstantinov"
                    },
                    {
                        "authorId": "145528703",
                        "name": "L. Utkin"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "GNNExplainer [206], GraphMask [207], PGMExplainer [208], SubgraphX [209], and XGNN [210] are among some methods that attempt to explain the decision-making process of GNNs."
            ],
            "citingPaper": {
                "paperId": "656e8c5f8bced540425c12d854b2911dddefff14",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-06471",
                    "ArXiv": "2303.06471",
                    "DOI": "10.48550/arXiv.2303.06471",
                    "CorpusId": 257496741
                },
                "corpusId": 257496741,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/656e8c5f8bced540425c12d854b2911dddefff14",
                "title": "Multimodal Data Integration for Oncology in the Era of Deep Neural Networks: A Review",
                "abstract": "Cancer has relational information residing at varying scales, modalities, and resolutions of the acquired data, such as radiology, pathology, genomics, proteomics, and clinical records. Integrating diverse data types can improve the accuracy and reliability of cancer diagnosis and treatment. There can be disease-related information that is too subtle for humans or existing technological tools to discern visually. Traditional methods typically focus on partial or unimodal information about biological systems at individual scales and fail to encapsulate the complete spectrum of the heterogeneous nature of data. Deep neural networks have facilitated the development of sophisticated multimodal data fusion approaches that can extract and integrate relevant information from multiple sources. Recent deep learning frameworks such as Graph Neural Networks (GNNs) and Transformers have shown remarkable success in multimodal learning. This review article provides an in-depth analysis of the state-of-the-art in GNNs and Transformers for multimodal data fusion in oncology settings, highlighting notable research studies and their findings. We also discuss the foundations of multimodal learning, inherent challenges, and opportunities for integrative learning in oncology. By examining the current state and potential future developments of multimodal data integration in oncology, we aim to demonstrate the promising role that multimodal neural networks can play in cancer prevention, early detection, and treatment through informed oncology practices in personalized settings.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2053769148",
                        "name": "Asim Waqas"
                    },
                    {
                        "authorId": "2164093525",
                        "name": "Aakash Tripathi"
                    },
                    {
                        "authorId": "2093344107",
                        "name": "Ravichandran Ramachandran"
                    },
                    {
                        "authorId": "2070243140",
                        "name": "P. Stewart"
                    },
                    {
                        "authorId": "145129130",
                        "name": "G. Rasool"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Even though PGExplainer Luo et al. (2020) and GraphMask Schlichtkrull et al. (2020) could provide some global insights, they require a reparameterization trick and could not guarantee that the outputs of the subgraph are connected, which lacks explanations for the message passing scheme in GNNs. Shapley-value based approaches SubgraphX Yuan et al. (2021) and GraphSVX Duval & Malliaros (2021) are computationally expensive especially for exploring different subgraphs with the MCTS algorithm.",
                "\u2026the outputs of the subgraph are connected, which lacks explanations for the message passing scheme in GNNs. Shapley-value based approaches SubgraphX Yuan et al. (2021) and GraphSVX Duval & Malliaros (2021) are computationally expensive especially for exploring different subgraphs with the MCTS\u2026",
                "In this section, we compare GFlowExplainer with a shapley-value based approache SubgraphX Yuan et al. (2021) and DEGREE on accuracy.",
                "As for the accuracy calculation, we follow similar setting in SubgraphX and DEGREE for fair comparison."
            ],
            "citingPaper": {
                "paperId": "2fd0e6aaccee819a880a55d9190700c6b754d32d",
                "externalIds": {
                    "DBLP": "conf/iclr/LiLLHP23",
                    "ArXiv": "2303.02448",
                    "DOI": "10.48550/arXiv.2303.02448",
                    "CorpusId": 257365860
                },
                "corpusId": 257365860,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2fd0e6aaccee819a880a55d9190700c6b754d32d",
                "title": "DAG Matters! GFlowNets Enhanced Explainer For Graph Neural Networks",
                "abstract": "Uncovering rationales behind predictions of graph neural networks (GNNs) has received increasing attention over the years. Existing literature mainly focus on selecting a subgraph, through combinatorial optimization, to provide faithful explanations. However, the exponential size of candidate subgraphs limits the applicability of state-of-the-art methods to large-scale GNNs. We enhance on this through a different approach: by proposing a generative structure -- GFlowNets-based GNN Explainer (GFlowExplainer), we turn the optimization problem into a step-by-step generative problem. Our GFlowExplainer aims to learn a policy that generates a distribution of subgraphs for which the probability of a subgraph is proportional to its' reward. The proposed approach eliminates the influence of node sequence and thus does not need any pre-training strategies. We also propose a new cut vertex matrix to efficiently explore parent states for GFlowNets structure, thus making our approach applicable in a large-scale setting. We conduct extensive experiments on both synthetic and real datasets, and both qualitative and quantitative results show the superiority of our GFlowExplainer.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2128179285",
                        "name": "Wenqian Li"
                    },
                    {
                        "authorId": "47002988",
                        "name": "Yinchuan Li"
                    },
                    {
                        "authorId": "46947185",
                        "name": "Zhigang Li"
                    },
                    {
                        "authorId": "2069718816",
                        "name": "Jianye Hao"
                    },
                    {
                        "authorId": "2057013801",
                        "name": "Yan Pang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "As the determinant subgraphs are expected to be connected, connective constraints [17, 37] are used to allocate more selective probabilities to the edges, which connect with the part selected already.",
                "SubgraphX [37] employs Monte Carlo Tree to search different subgraphs and leverages Shapley value to evaluate their importance.",
                "Perturbation-based line [15, 33, 37] studies the output variations in response to different input perturbations."
            ],
            "citingPaper": {
                "paperId": "52c36f7f3554c497d680aa8ccb38aef6b96e26f0",
                "externalIds": {
                    "DBLP": "conf/wsdm/Fang00L0C23",
                    "DOI": "10.1145/3539597.3570378",
                    "CorpusId": 257079786
                },
                "corpusId": 257079786,
                "publicationVenue": {
                    "id": "ea38228f-6ed3-4222-a3ce-d963d8cc9516",
                    "name": "Web Search and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Web Search Data Min",
                        "WSDM"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=3158"
                },
                "url": "https://www.semanticscholar.org/paper/52c36f7f3554c497d680aa8ccb38aef6b96e26f0",
                "title": "Cooperative Explanations of Graph Neural Networks",
                "abstract": "With the growing success of graph neural networks (GNNs), the explainability of GNN is attracting considerable attention. Current explainers mostly leverage feature attribution and selection to explain a prediction. By tracing the importance of input features, they select the salient subgraph as the explanation. However, their explainability is at the granularity of input features only, and cannot reveal the usefulness of hidden neurons. This inherent limitation makes the explainers fail to scrutinize the model behavior thoroughly, resulting in unfaithful explanations. In this work, we explore the explainability of GNNs at the granularity of both input features and hidden neurons. To this end, we propose an explainer-agnostic framework, Cooperative GNN Explanation (CGE) to generate the explanatory subgraph and subnetwork simultaneously, which jointly explain how the GNN model arrived at its prediction. Specifically, it first initializes the importance scores of input features and hidden neurons with masking networks. Then it iteratively retrains the importance scores, refining the salient subgraph and subnetwork by discarding low-scored features and neurons in each iteration. Through such cooperative learning, CGE not only generates faithful and concise explanations, but also exhibits how the salient information flows by activating and deactivating neurons. We conduct extensive experiments on both synthetic and real-world datasets, validating the superiority of CGE over state-of-the-art approaches. Code is available at https://github.com/MangoKiller/CGE_demo.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2159830802",
                        "name": "Junfeng Fang"
                    },
                    {
                        "authorId": "98285513",
                        "name": "Xiang Wang"
                    },
                    {
                        "authorId": "2153659066",
                        "name": "An Zhang"
                    },
                    {
                        "authorId": "2163529904",
                        "name": "Zemin Liu"
                    },
                    {
                        "authorId": "7792071",
                        "name": "Xiangnan He"
                    },
                    {
                        "authorId": "143779329",
                        "name": "Tat-Seng Chua"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "result"
            ],
            "isInfluential": true,
            "contexts": [
                "Given a data instance, most methods generate an explanation by learning a mask to select an edge-induced subgraph [25, 43] or searching over the space of subgraphs [46].",
                "SubgraphX [48] uses the Shapley value [34] and performs Monte Carlo Tree Search (MCTS) on subgraphs.",
                "SubgraphX has a much higher time complexity exponential in |V\ud835\udc50 |, so a size budget of \ud835\udc35\ud835\udc5b\ud835\udc5c\ud835\udc51\ud835\udc52 nodes is forced to replace |V\ud835\udc50 |, and ?",
                "To get the importance of a path, we first use a mean-field approximation for the joint probability by multiplying \ud835\udc43 (\ud835\udc52) together, and we normalize each\nAlgorithm 1 PaGE-Link\nGNNExp [45] PGExp [25] SubgraphX [48] PaGE-Link (ours) \ud835\udc42 ( |E\ud835\udc50 |\ud835\udc47 ) \ud835\udc42 ( |E |\ud835\udc47 ) / \ud835\udc42 ( |E\ud835\udc50",
                "SubgraphX [46] uses the Shapley value [32] and performs Monte Carlo Tree Search (MCTS) on subgraphs.",
                "For example, SubgraphX finds all connected subgraphs with at most \ud835\udc35\ud835\udc5b\ud835\udc5c\ud835\udc51\ud835\udc52 nodes, which has complexity \u0398( |V\ud835\udc50 |?",
                "We do not compare to other search-based explainers like SubgraphX [48]\nbecause of their high computational complexity (see Section 5.4).",
                "We do not compare to other search-based explainers like SubgraphX [46] because they have high computational complexity, as discussed in Section 5.",
                "GNNExp [43] PGExp [25] SubgraphX [46] PaGE-Link (ours)"
            ],
            "citingPaper": {
                "paperId": "4008f607e29cfe6c0cce0b5ae119827380b99031",
                "externalIds": {
                    "DBLP": "conf/www/Zhang0SAZFS23",
                    "ArXiv": "2302.12465",
                    "DOI": "10.1145/3543507.3583511",
                    "CorpusId": 257205930
                },
                "corpusId": 257205930,
                "publicationVenue": {
                    "id": "e07422f9-c065-40c3-a37b-75e98dce79fe",
                    "name": "The Web Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Web Conf",
                        "WWW"
                    ],
                    "url": "http://www.iw3c2.org/"
                },
                "url": "https://www.semanticscholar.org/paper/4008f607e29cfe6c0cce0b5ae119827380b99031",
                "title": "PaGE-Link: Path-based Graph Neural Network Explanation for Heterogeneous Link Prediction",
                "abstract": "Transparency and accountability have become major concerns for black-box machine learning (ML) models. Proper explanations for the model behavior increase model transparency and help researchers develop more accountable models. Graph neural networks (GNN) have recently shown superior performance in many graph ML problems than traditional methods, and explaining them has attracted increased interest. However, GNN explanation for link prediction (LP) is lacking in the literature. LP is an essential GNN task and corresponds to web applications like recommendation and sponsored search on web. Given existing GNN explanation methods only address node/graph-level tasks, we propose Path-based GNN Explanation for heterogeneous Link prediction (PaGE-Link) that generates explanations with connection interpretability, enjoys model scalability, and handles graph heterogeneity. Qualitatively, PaGE-Link can generate explanations as paths connecting a node pair, which naturally captures connections between the two nodes and easily transfer to human-interpretable explanations. Quantitatively, explanations generated by PaGE-Link improve AUC for recommendation on citation and user-item graphs by 9 - 35% and are chosen as better by 78.79% of responses in human evaluation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2145408511",
                        "name": "Shichang Zhang"
                    },
                    {
                        "authorId": "73329314",
                        "name": "Jiani Zhang"
                    },
                    {
                        "authorId": "2118943843",
                        "name": "Xiang Song"
                    },
                    {
                        "authorId": "2121390172",
                        "name": "Soji Adeshina"
                    },
                    {
                        "authorId": "122579067",
                        "name": "Da Zheng"
                    },
                    {
                        "authorId": "1702392",
                        "name": "C. Faloutsos"
                    },
                    {
                        "authorId": "2109461904",
                        "name": "Yizhou Sun"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "SubgraphX [5] efficiently explores subgraphs using a Monte Carlo tree search to identify structures that play an important role in the prediction.",
                "The proposed method outperformed GNNExplainer and PGExplainer, whereas SubgraphX displayed the best performance.",
                "As HSIC lasso solvers,\nscikit-learn [21], SPAMS [22, 23], and CVXPY [24, 25] were used, and the DIG library [26] was adapted for use in PGExplainer and SubgraphX.",
                "Table 3 demonstrates that the proposed methods could detect the bridge better than baselines other than SubgraphX in Top-5 Acc.",
                "For SubgraphX, the maximum number of nodes in subgraph was set to the output of the aforementioned values.",
                "Following [5], the sparsity and fidelity scores were adapted to make quantitative evaluations.",
                "graphX [5] efficiently explores subgraphs using a Monte Carlo tree search to identify structures that play an important role in the prediction.",
                "Although SubgraphX displayed a higher ability in identifying either hub node than other methods, the ability to identify both was inferior to our method and equivalent to GNNExplainer and PGExplainer.",
                "This is because SubgraphX always outputs a connected graph and is difficult to detect with a small number of candidates when the critical nodes are isolated.",
                "SubgraphX[5] finds significant subgraphs for GNNs by using the Monte Carlo tree search algorithm and the Shapley values from game theory.",
                "Table 2 demonstrates that our method and SubgraphX can detect the hub nodes more accurately than the other methods.",
                "A random explainer (which places a random score on each node), GNNExplainer [3], PGExpaliner [4], and SubgraphX [5] were adopted as the baselines for graph classification.",
                "Although SubgraphX always generates a connected graph as a human-friendly explanation, it cannot simultaneously detect separate important nodes."
            ],
            "citingPaper": {
                "paperId": "997f3147417b286293f999a781b8f5aa8be140e8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-02139",
                    "ArXiv": "2302.02139",
                    "DOI": "10.48550/arXiv.2302.02139",
                    "CorpusId": 256616269
                },
                "corpusId": 256616269,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/997f3147417b286293f999a781b8f5aa8be140e8",
                "title": "Structural Explanations for Graph Neural Networks using HSIC",
                "abstract": "Graph neural networks (GNNs) are a type of neural model that tackle graphical tasks in an end-to-end manner. Recently, GNNs have been receiving increased attention in machine learning and data mining communities because of the higher performance they achieve in various tasks, including graph classification, link prediction, and recommendation. However, the complicated dynamics of GNNs make it difficult to understand which parts of the graph features contribute more strongly to the predictions. To handle the interpretability issues, recently, various GNN explanation methods have been proposed. In this study, a flexible model agnostic explanation method is proposed to detect significant structures in graphs using the Hilbert-Schmidt independence criterion (HSIC), which captures the nonlinear dependency between two variables through kernels. More specifically, we extend the GraphLIME method for node explanation with a group lasso and a fused lasso-based node explanation method. The group and fused regularization with GraphLIME enables the interpretation of GNNs in substructure units. Then, we show that the proposed approach can be used for the explanation of sequential graph classification tasks. Through experiments, it is demonstrated that our method can identify crucial structures in a target graph in various settings.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2051714212",
                        "name": "Ayato Toyokuni"
                    },
                    {
                        "authorId": "143979662",
                        "name": "Makoto Yamada"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "tion and interpretability performance [36]."
            ],
            "citingPaper": {
                "paperId": "b4e2967b73fe0908e59bda6fc0f686dcddceea4e",
                "externalIds": {
                    "DBLP": "journals/tetci/LiSZWZ23",
                    "DOI": "10.1109/TETCI.2022.3183679",
                    "CorpusId": 250295117
                },
                "corpusId": 250295117,
                "publicationVenue": {
                    "id": "544cddb9-1149-469a-8377-d8c34f08d8b1",
                    "name": "IEEE Transactions on Emerging Topics in Computational Intelligence",
                    "alternate_names": [
                        "IEEE Trans Emerg Top Comput Intell"
                    ],
                    "issn": "2471-285X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=7433297",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7433297"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b4e2967b73fe0908e59bda6fc0f686dcddceea4e",
                "title": "FSNet: Dual Interpretable Graph Convolutional Network for Alzheimer\u2019s Disease Analysis",
                "abstract": "Graph Convolutional Networks (GCNs) are widely used in medical images diagnostic research, because they can automatically learn powerful and robust feature representations. However, their performance might be significantly deteriorated by trivial or corrupted medical features and samples. Moreover, existing methods cannot simultaneously interpret the significant features and samples. To overcome these limitations, in this paper, we propose a novel dual interpretable graph convolutional network, namely FSNet, to simultaneously select significant features and samples, so as to boost model performance for medical diagnosis and interpretation. Specifically, the proposed network consists of three modules, two of which leverage one simple yet effective sparse mechanism to obtain feature and sample weight matrices for interpreting features and samples, respectively, and the third one is utilized for medical diagnosis. Extensive experiments on the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) datasets demonstrate the superior classification performance and interpretability over the recent state-of-the-art methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2044342723",
                        "name": "Hengxin Li"
                    },
                    {
                        "authorId": "2766473",
                        "name": "Xiaoshuang Shi"
                    },
                    {
                        "authorId": "46875503",
                        "name": "Xiao-lan Zhu"
                    },
                    {
                        "authorId": "7488474",
                        "name": "Shuihua Wang"
                    },
                    {
                        "authorId": "2148907608",
                        "name": "Zheng Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "Adopting a simple numeric operation for all node embeddings is a common graph pooling method [81, 107], since it is easy to use and obeys the permutation invariant.",
                "The intimate connection between GL-GNNs and 1-WL is exploited in the Graph Isomorphism Network (GIN) [52].",
                "Further, GIN also allows us to analyze the efficacy of different functions: summation, maximization, and mean functions.",
                "Inspired by GIN, Principal Neighbourhood Aggregation (PNA) [108] employs all three of these functions to pool the node embeddings, while TextING [109] includes both mean and maximization pooling to capture the label distribution and strengthen the keyword features.",
                "Similarly, GIN [107] shows us that the injective relabeling function in the WL algorithm can be replaced with a simple numeric operation."
            ],
            "citingPaper": {
                "paperId": "6c7796ba7f04c77d888bf6b65d3498abfacfe71a",
                "externalIds": {
                    "ArXiv": "2301.05860",
                    "CorpusId": 258865620
                },
                "corpusId": 258865620,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6c7796ba7f04c77d888bf6b65d3498abfacfe71a",
                "title": "State of the Art and Potentialities of Graph-level Learning",
                "abstract": "Graphs have a superior ability to represent relational data, like chemical compounds, proteins, and social networks. Hence, graph-level learning, which takes a set of graphs as input, has been applied to many tasks including comparison, regression, classification, and more. Traditional approaches to learning a set of graphs heavily rely on hand-crafted features, such as substructures. But while these methods benefit from good interpretability, they often suffer from computational bottlenecks as they cannot skirt the graph isomorphism problem. Conversely, deep learning has helped graph-level learning adapt to the growing scale of graphs by extracting features automatically and encoding graphs into low-dimensional representations. As a result, these deep graph learning methods have been responsible for many successes. Yet, there is no comprehensive survey that reviews graph-level learning starting with traditional learning and moving through to the deep learning approaches. This article fills this gap and frames the representative algorithms into a systematic taxonomy covering traditional learning, graph-level deep neural networks, graph-level graph neural networks, and graph pooling. To ensure a thoroughly comprehensive survey, the evolutions, interactions, and communications between methods from four different branches of development are also examined. This is followed by a brief review of the benchmark data sets, evaluation metrics, and common downstream applications. The survey concludes with a broad overview of 12 current and future directions in this booming field.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2149233069",
                        "name": "Zhenyu Yang"
                    },
                    {
                        "authorId": "2151251543",
                        "name": "Ge Zhang"
                    },
                    {
                        "authorId": "2142734769",
                        "name": "Jia Wu"
                    },
                    {
                        "authorId": "2118801701",
                        "name": "Jian Yang"
                    },
                    {
                        "authorId": "120607997",
                        "name": "Quan.Z Sheng"
                    },
                    {
                        "authorId": "2057237074",
                        "name": "Shan Xue"
                    },
                    {
                        "authorId": "1857210",
                        "name": "Chuan Zhou"
                    },
                    {
                        "authorId": "1682418",
                        "name": "C. Aggarwal"
                    },
                    {
                        "authorId": "2138443697",
                        "name": "Hao Peng"
                    },
                    {
                        "authorId": "2146226874",
                        "name": "Wenbin Hu"
                    },
                    {
                        "authorId": "2064408469",
                        "name": "Edwin R. Hancock"
                    },
                    {
                        "authorId": "2075355155",
                        "name": "Pietro Lio'"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "tation learning [65], graph adversarial attack [66], relational reasoning [67], GNN explainer [68] and combination optimization [69]."
            ],
            "citingPaper": {
                "paperId": "5deddacff83d61971afb41a8dde8ec0ae0038363",
                "externalIds": {
                    "DBLP": "journals/pami/LiSPYWY23",
                    "DOI": "10.1109/TPAMI.2023.3235931",
                    "CorpusId": 255642415,
                    "PubMed": "37018637"
                },
                "corpusId": 255642415,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5deddacff83d61971afb41a8dde8ec0ae0038363",
                "title": "Adaptive Subgraph Neural Network With Reinforced Critical Structure Mining",
                "abstract": "While graph representation learning methods have shown success in various graph mining tasks, what knowledge is exploited for predictions is less discussed. This paper proposes a novel Adaptive Subgraph Neural Network named AdaSNN to find critical structures in graph data, i.e., subgraphs that are dominant to the prediction results. To detect critical subgraphs of arbitrary size and shape in the absence of explicit subgraph-level annotations, AdaSNN designs a Reinforced Subgraph Detection Module to search subgraphs adaptively without heuristic assumptions or predefined rules. To encourage the subgraph to be predictive at the global scale, we design a Bi-Level Mutual Information Enhancement Mechanism including both global-aware and label-aware mutual information maximization to further enhance the subgraph representations in the perspective of information theory. By mining critical subgraphs that reflect the intrinsic property of a graph, AdaSNN can provide sufficient interpretability to the learned results. Comprehensive experimental results on seven typical graph datasets demonstrate that AdaSNN has a significant and consistent performance improvement and provides insightful results.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47785906",
                        "name": "Jianxin Li"
                    },
                    {
                        "authorId": "3422968",
                        "name": "Qingyun Sun"
                    },
                    {
                        "authorId": "2138443697",
                        "name": "Hao Peng"
                    },
                    {
                        "authorId": "2218711319",
                        "name": "Beining Yang"
                    },
                    {
                        "authorId": "2144139161",
                        "name": "Jia Wu"
                    },
                    {
                        "authorId": "2200045681",
                        "name": "Phillp S. Yu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                ", 2019), PGM-Explainer (Vu & Thai, 2020) and SubgraphX (Yuan et al., 2021).",
                "\u2026signals (Baldassarre & Azizpour, 2019; Pope et al., 2019; Schnake et al., 2020), mask or attention scores (Ying et al., 2019; Luo et al., 2020), or prediction changes on perturbed features (Schwab & Karlen, 2019; Yuan et al., 2021), and then choose a salient substructure as the explanation.",
                "Notably, instead of merely optimizing the information hidden in GS , another line of research (Yuan et al., 2021) seeks to reduce the mutual information between the remaining subgraph G \u2212 GS and the original one G as:",
                ", 2020), or prediction changes on perturbed features (Schwab & Karlen, 2019; Yuan et al., 2021), and then choose a salient substructure as the explanation.",
                "Notably, instead of merely optimizing the information hidden in GS , another line of research (Yuan et al., 2021) seeks to reduce the mutual information between the remaining subgraph G \u2212 GS and the original one G as:\nmin GS\u2282G,|GS |\u2264K\nI(G \u2212 GS , Th).",
                "Some explainer models are optimized toward local fidelity (Chen et al., 2018), such as GNNExplainer (Ying et al., 2019), PGM-Explainer (Vu & Thai, 2020) and SubgraphX (Yuan et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "175b99cd341c162b9f78fdfc4ccec83d58c3b87c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-02780",
                    "ArXiv": "2301.02780",
                    "DOI": "10.48550/arXiv.2301.02780",
                    "CorpusId": 255545775
                },
                "corpusId": 255545775,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/175b99cd341c162b9f78fdfc4ccec83d58c3b87c",
                "title": "Explaining Graph Neural Networks via Non-parametric Subgraph Matching",
                "abstract": "The great success in graph neural networks (GNNs) provokes the question about explainability: \u201cWhich fraction of the input graph is the most determinant to the prediction?\u201d Particularly, parametric explainers prevail in existing approaches because of their stronger capability to decipher the black-box (i.e., the target GNN). In this paper, based on the observation that graphs typically share some joint motif patterns, we propose a novel non-parametric subgraph matching framework, dubbed MatchExplainer, to explore explanatory subgraphs. It couples the target graph with other counterpart instances and identi\ufb01es the most crucial joint substructure by minimizing the node corresponding-based distance. Moreover, we note that present graph sampling or node-dropping methods usually suffer from the false positive sampling problem. To ameliorate that issue, we design a new augmentation paradigm named MatchDrop. It takes advantage of MatchExplainer to \ufb01x the most informative portion of the graph and merely operates graph augmentations on the rest less informative part. We conduct extensive experiments on both synthetic and real-world datasets and show the effectiveness of our MatchExplainer by outperforming all parametric baselines with signi\ufb01cant margins. Additional results also demonstrate that our MatchDrop is a general scheme to be equipped with GNNs for enhanced performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152167768",
                        "name": "Fang Wu"
                    },
                    {
                        "authorId": "2118155623",
                        "name": "Siyuan Li"
                    },
                    {
                        "authorId": "47767812",
                        "name": "Lirong Wu"
                    },
                    {
                        "authorId": "9215251",
                        "name": "Dragomir R. Radev"
                    },
                    {
                        "authorId": "2146420304",
                        "name": "Yinghui Jiang"
                    },
                    {
                        "authorId": "2153674069",
                        "name": "Xurui Jin"
                    },
                    {
                        "authorId": "152135528",
                        "name": "Z. Niu"
                    },
                    {
                        "authorId": "1390908654",
                        "name": "Stan Z. Li"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "a69ca65d10b15716fc5da5dc830e04101b419e9e",
                "externalIds": {
                    "ArXiv": "2301.02791",
                    "DBLP": "journals/corr/abs-2301-02791",
                    "DOI": "10.1145/3616542",
                    "CorpusId": 255545933
                },
                "corpusId": 255545933,
                "publicationVenue": {
                    "id": "0d993d4a-09ba-4df8-90a4-7dfe25f0cb9e",
                    "name": "ACM Transactions on Intelligent Systems and Technology",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Intell Syst Technol"
                    ],
                    "issn": "2157-6904",
                    "url": "http://portal.acm.org/tist",
                    "alternate_urls": [
                        "https://tist.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a69ca65d10b15716fc5da5dc830e04101b419e9e",
                "title": "Faithful and Consistent Graph Neural Network Explanations with Rationale Alignment",
                "abstract": "Uncovering rationales behind predictions of graph neural networks (GNNs) has received increasing attention over recent years. Instance-level GNN explanation aims to discover critical input elements, like nodes or edges, that the target GNN relies upon for making predictions. Though various algorithms are proposed, most of them formalize this task by searching the minimal subgraph which can preserve original predictions. However, an inductive bias is deep-rooted in this framework: several subgraphs can result in the same or similar outputs as the original graphs. Consequently, they have the danger of providing spurious explanations and failing to provide consistent explanations. Applying them to explain weakly-performed GNNs would further amplify these issues. To address this problem, we theoretically examine the predictions of GNNs from the causality perspective. Two typical reasons for spurious explanations are identified: confounding effect of latent variables like distribution shift, and causal factors distinct from the original input. Observing that both confounding effects and diverse causal rationales are encoded in internal representations, we propose a new explanation framework with an auxiliary alignment loss, which is theoretically proven to be optimizing a more faithful explanation objective intrinsically. Concretely for this alignment loss, a set of different perspectives are explored: anchor-based alignment, distributional alignment based on Gaussian mixture models, mutual-information-based alignment, etc. A comprehensive study is conducted both on the effectiveness of this new framework in terms of explanation faithfulness/consistency and on the advantages of these variants. For our codes, please refer to the following URL link: https://github.com/TianxiangZhao/GraphNNExplanation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1999191869",
                        "name": "Tianxiang Zhao"
                    },
                    {
                        "authorId": "153640788",
                        "name": "Dongsheng Luo"
                    },
                    {
                        "authorId": "48505793",
                        "name": "Xiang Zhang"
                    },
                    {
                        "authorId": "2116430057",
                        "name": "Suhang Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "These black-box models cannot be fully trusted and do not meet the demands of fairness, security, and robustness Yuan et al. (2020, 2021); Zeng et al. (2022), which severely hinders their real-world applications, particularly in medical diagnosis in which a transparent decision-making process is a\u2026"
            ],
            "citingPaper": {
                "paperId": "108c960f7d8dafdbc4c41ab7e23610a5959ae2fc",
                "externalIds": {
                    "ArXiv": "2301.01642",
                    "DBLP": "journals/corr/abs-2301-01642",
                    "DOI": "10.48550/arXiv.2301.01642",
                    "CorpusId": 255415863
                },
                "corpusId": 255415863,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/108c960f7d8dafdbc4c41ab7e23610a5959ae2fc",
                "title": "CI-GNN: A Granger Causality-Inspired Graph Neural Network for Interpretable Brain Network-Based Psychiatric Diagnosis",
                "abstract": "There is a recent trend to leverage the power of graph neural networks (GNNs) for brain-network based psychiatric diagnosis, which,in turn, also motivates an urgent need for psychiatrists to fully understand the decision behavior of the used GNNs. However, most of the existing GNN explainers are either post-hoc in which another interpretive model needs to be created to explain a well-trained GNN, or do not consider the causal relationship between the extracted explanation and the decision, such that the explanation itself contains spurious correlations and suffers from weak faithfulness. In this work, we propose a granger causality-inspired graph neural network (CI-GNN), a built-in interpretable model that is able to identify the most influential subgraph (i.e., functional connectivity within brain regions) that is causally related to the decision (e.g., major depressive disorder patients or healthy controls), without the training of an auxillary interpretive network. CI-GNN learns disentangled subgraph-level representations {\\alpha} and \\b{eta} that encode, respectively, the causal and noncausal aspects of original graph under a graph variational autoencoder framework, regularized by a conditional mutual information (CMI) constraint. We theoretically justify the validity of the CMI regulation in capturing the causal relationship. We also empirically evaluate the performance of CI-GNN against three baseline GNNs and four state-of-the-art GNN explainers on synthetic data and three large-scale brain disease datasets. We observe that CI-GNN achieves the best performance in a wide range of metrics and provides more reliable and concise explanations which have clinical evidence.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "152804328",
                        "name": "Kaizhong Zheng"
                    },
                    {
                        "authorId": "2462771",
                        "name": "Shujian Yu"
                    },
                    {
                        "authorId": "2199175116",
                        "name": "Ba-dong Chen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", 2022; Vu & Thai, 2020), Shapley values (Lundberg & Lee, 2017; Chen et al., 2019a; Liu et al., 2020; Yuan et al., 2021; Ancona et al., 2019), and causality (Pearl, 2018; Chattopadhyay et al.",
                "\u2026et al., 2017), surrogate model based (Ribeiro et al., 2016; Huang et al., 2022; Vu & Thai, 2020), Shapley values (Lundberg & Lee, 2017; Chen et al., 2019a; Liu et al., 2020; Yuan et al., 2021; Ancona et al., 2019), and causality (Pearl, 2018; Chattopadhyay et al., 2019; Parafita & Vitria\u0300, 2019)."
            ],
            "citingPaper": {
                "paperId": "c01a5bc04294be3562854d2586f973eaf2878f68",
                "externalIds": {
                    "ArXiv": "2212.14106",
                    "CorpusId": 259502039
                },
                "corpusId": 259502039,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c01a5bc04294be3562854d2586f973eaf2878f68",
                "title": "Provable Robust Saliency-based Explanations",
                "abstract": "Robust explanations of machine learning models are critical to establishing human trust in the models. The top-$k$ intersection is widely used to evaluate the robustness of explanations. However, most existing attacking and defense strategies are based on $\\ell_p$ norms, thus creating a mismatch between the evaluation and optimization objectives. To this end, we define explanation thickness for measuring top-$k$ salient features ranking stability, and design the \\textit{R2ET} algorithm based on a novel tractable surrogate to maximize the thickness and stabilize the top salient features efficiently. Theoretically, we prove a connection between R2ET and adversarial training; using a novel multi-objective optimization formulation and a generalization error bound, we further prove that the surrogate objective can improve both the numerical and statistical stability of the explanations. Experiments with a wide spectrum of network architectures and data modalities demonstrate that R2ET attains higher explanation robustness under stealthy attacks while retaining model accuracy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2145762275",
                        "name": "Chao Chen"
                    },
                    {
                        "authorId": "2199043865",
                        "name": "Chenghua Guo"
                    },
                    {
                        "authorId": "2068988549",
                        "name": "Guixiang Ma"
                    },
                    {
                        "authorId": "2181315370",
                        "name": "Ming Zeng"
                    },
                    {
                        "authorId": "47957054",
                        "name": "Xi Zhang"
                    },
                    {
                        "authorId": "3131378",
                        "name": "Sihong Xie"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "a03f43a8987a148b7f3b599e8aa70ab9feeb8b64",
                "externalIds": {
                    "DBLP": "conf/bigdataconf/LiuZX22",
                    "DOI": "10.1109/BigData55660.2022.10020318",
                    "CorpusId": 256312270
                },
                "corpusId": 256312270,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a03f43a8987a148b7f3b599e8aa70ab9feeb8b64",
                "title": "Trade less Accuracy for Fairness and Trade-off Explanation for GNN",
                "abstract": "Graphs are widely found in social network analysis and e-commerce, where Graph Neural Networks (GNNs) are the state-of the-art model. GNNs can be biased due to sensitive attributes and network topology. With existing work that learns a fair node representation or adjacency matrix, achieving a strong guarantee of group fairness while preserving prediction accuracy is still challenging, with the fairness-accuracy trade-off remaining obscure to human decision-makers. We first define and analyze a novel upper bound of group fairness to optimize the adjacency matrix for fairness without significantly h arming prediction accuracy. To understand the nuance of fairness-accuracy tradeoff, we further propose macroscopic and microscopic explanation methods to reveal the trade-offs and the space that one can exploit. The macroscopic explanation method is based on stratified sampling and linear programming to deterministically explain the dynamics of the group fairness and prediction accuracy. Driving down to the microscopic level, we propose a path-based explanation that reveals how network topology leads to the tradeoff. On seven graph datasets, we demonstrate the novel upper bound can achieve more efficient fairness-accuracy trade-offs and the intuitiveness of the explanation methods can clearly pinpoint where the trade-off is improved.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2144399347",
                        "name": "Yazheng Liu"
                    },
                    {
                        "authorId": "2108286275",
                        "name": "Xi Zhang"
                    },
                    {
                        "authorId": "3131378",
                        "name": "Sihong Xie"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "SubgraphX [10] proposes to use shapley values as the importance scoring function.",
                "SubgraphX [10] proposed to use subgraphs to interpret the model and use shapley values [11] to score its importance."
            ],
            "citingPaper": {
                "paperId": "25b1fa9960dadba581dc4b342c61ede99a486f45",
                "externalIds": {
                    "DBLP": "conf/globecom/LiLLCY22",
                    "DOI": "10.1109/GLOBECOM48099.2022.10001460",
                    "CorpusId": 255597121
                },
                "corpusId": 255597121,
                "publicationVenue": {
                    "id": "b189dec0-41d0-4cea-a906-7c5186895904",
                    "name": "Global Communications Conference",
                    "type": "conference",
                    "alternate_names": [
                        "GLOBECOM",
                        "Glob Commun Conf"
                    ],
                    "url": "http://www.ieee-globecom.org/"
                },
                "url": "https://www.semanticscholar.org/paper/25b1fa9960dadba581dc4b342c61ede99a486f45",
                "title": "Shapley Explainer - An Interpretation Method for GNNs Used in SDN",
                "abstract": "Graph neural networks (GNNs) have been widely applied in software-defined network (SDN) for better network modeling and performance prediction. However, the black-box characteristic of deep learning makes the GNNs hard to interpret, such interpretability issue hinders the wide use of GNNs. In this paper, we propose Shapley Explainer, that provides fair importance scores to the input nodes of a GNN within an appropriate computation cost, thereby providing a valid and reasonable interpretation of graph neural network on software defined network. The proposed method derives the importance ranking of topological nodes by combining shapley values with a soft discrete mask matrix. We apply Shapley Explainer to RouteNet model, a GNN model that provides intelligent predictions of SDN network performance metrics. The experimental results show that Shapley Explainer can provide effective interpretations for RouteNet. It also verifies that the RouteNet model can correctly learn the relationship between features, which can provide a better understanding of the prediction process of RouteNet, promoting the application of GNN-based SDN systems in engineering practice.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1838103",
                        "name": "Chuanhuang Li"
                    },
                    {
                        "authorId": "2199949540",
                        "name": "Jiali Lou"
                    },
                    {
                        "authorId": "2172052034",
                        "name": "Shiyuan Liu"
                    },
                    {
                        "authorId": "49865500",
                        "name": "Zebin Chen"
                    },
                    {
                        "authorId": "152162529",
                        "name": "Xiaoyong Yuan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "While several categories of GNN explanation methods have been proposed: gradient-based [5, 10, 14], perturbation-based [8, 9, 11, 13, 15], and surrogatebased [7, 12], their utility is limited to generating post hoc node- and edge-level explanations for a given pre-trained GNN model."
            ],
            "citingPaper": {
                "paperId": "242e3249bc4a22d39ad939c4445e73a0aee1eca9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-16731",
                    "ArXiv": "2211.16731",
                    "DOI": "10.48550/arXiv.2211.16731",
                    "CorpusId": 254096047
                },
                "corpusId": 254096047,
                "publicationVenue": {
                    "id": "50534c12-f4ba-4c64-806b-01647d1baacf",
                    "name": "LOG IN",
                    "type": "journal",
                    "alternate_names": [
                        "Log in",
                        "Log",
                        "LOG"
                    ],
                    "issn": "0720-8642",
                    "alternate_issns": [
                        "1547-4690",
                        "0024-5798"
                    ],
                    "url": "https://www.log-in-verlag.de/informatische_bildung/",
                    "alternate_urls": [
                        "https://www.anycorp.com/log/about",
                        "https://www.jstor.org/journal/log",
                        "https://www.anycorp.com/",
                        "http://www.jstor.org/action/showPublication?journalCode=log",
                        "http://www.anycorp.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/242e3249bc4a22d39ad939c4445e73a0aee1eca9",
                "title": "Towards Training GNNs using Explanation Directed Message Passing",
                "abstract": "With the increasing use of Graph Neural Networks (GNNs) in critical real-world applications, several post hoc explanation methods have been proposed to understand their predictions. However, there has been no work in generating explanations on the fly during model training and utilizing them to improve the expressive power of the underlying GNN models. In this work, we introduce a novel explanation-directed neural message passing framework for GNNs, EXPASS (EXplainable message PASSing), which aggregates only embeddings from nodes and edges identified as important by a GNN explanation method. EXPASS can be used with any existing GNN architecture and subgraph-optimizing explainer to learn accurate graph embeddings. We theoretically show that EXPASS alleviates the oversmoothing problem in GNNs by slowing the layer wise loss of Dirichlet energy and that the embedding difference between the vanilla message passing and EXPASS framework can be upper bounded by the difference of their respective model weights. Our empirical results show that graph embeddings learned using EXPASS improve the predictive performance and alleviate the oversmoothing problems of GNNs, opening up new frontiers in graph machine learning to develop explanation-based training frameworks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2156850569",
                        "name": "V. Giunchiglia"
                    },
                    {
                        "authorId": "2192822479",
                        "name": "Chirag Varun Shukla"
                    },
                    {
                        "authorId": "2159543073",
                        "name": "Guadalupe Gonzalez"
                    },
                    {
                        "authorId": "40228633",
                        "name": "Chirag Agarwal"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "84849eb18a103aac184668d72e469fa5545802e7",
                "externalIds": {
                    "ArXiv": "2211.14997",
                    "CorpusId": 258547330
                },
                "corpusId": 258547330,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/84849eb18a103aac184668d72e469fa5545802e7",
                "title": "A Comprehensive Survey on Enterprise Financial Risk Analysis from Big Data Perspective",
                "abstract": "Enterprise financial risk analysis aims at predicting the future financial risk of enterprises. Due to its wide and significant application, enterprise financial risk analysis has always been the core research topic in the fields of Finance and Management. Based on advanced computer science and artificial intelligence technologies, enterprise risk analysis research is experiencing rapid developments and making significant progress. Therefore, it is both necessary and challenging to comprehensively review the relevant studies. Although there are already some valuable and impressive surveys on enterprise risk analysis from the perspective of Finance and Management, these surveys introduce approaches in a relatively isolated way and lack recent advances in enterprise financial risk analysis. In contrast, this paper attempts to provide a systematic literature survey of enterprise risk analysis approaches from Big Data perspective, which reviews more than 250 representative articles in the past almost 50 years (from 1968 to 2023). To the best of our knowledge, this is the first and only survey work on enterprise financial risk from Big Data perspective. Specifically, this survey connects and systematizes the existing enterprise financial risk studies, i.e. to summarize and interpret the problems, methods, and spotlights in a comprehensive way. In particular, we first introduce the issues of enterprise financial risks in terms of their types,granularity, intelligence, and evaluation metrics, and summarize the corresponding representative works. Then, we compare the analysis methods used to learn enterprise financial risk, and finally summarize the spotlights of the most representative works. Our goal is to clarify current cutting-edge research and its possible future directions to model enterprise risk, aiming to fully understand the mechanisms of enterprise risk generation and contagion.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "97522134",
                        "name": "Yu Zhao"
                    },
                    {
                        "authorId": "94613261",
                        "name": "Huaming Du"
                    },
                    {
                        "authorId": "2117895423",
                        "name": "Qing Li"
                    },
                    {
                        "authorId": "2162961864",
                        "name": "Fuzhen Zhuang"
                    },
                    {
                        "authorId": "2155375528",
                        "name": "Ji Liu"
                    },
                    {
                        "authorId": "2147326459",
                        "name": "Gang Kou"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "63d2f88b6d3d912d5898b18e8c796c19779d4823",
                "externalIds": {
                    "PubMedCentral": "9669545",
                    "DBLP": "journals/air/AskrEEEGH23",
                    "DOI": "10.1007/s10462-022-10306-1",
                    "CorpusId": 253662441,
                    "PubMed": "36415536"
                },
                "corpusId": 253662441,
                "publicationVenue": {
                    "id": "ea8553fe-2467-4367-afee-c4deb3754820",
                    "name": "Artificial Intelligence Review",
                    "type": "journal",
                    "alternate_names": [
                        "Artif Intell Rev"
                    ],
                    "issn": "0269-2821",
                    "url": "https://link.springer.com/journal/10462"
                },
                "url": "https://www.semanticscholar.org/paper/63d2f88b6d3d912d5898b18e8c796c19779d4823",
                "title": "Deep learning in drug discovery: an integrative review and future challenges",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1641821967",
                        "name": "Heba Askr"
                    },
                    {
                        "authorId": "118275048",
                        "name": "Enas Elgeldawi"
                    },
                    {
                        "authorId": "2191345430",
                        "name": "Heba Aboul Ella"
                    },
                    {
                        "authorId": "11911513",
                        "name": "Y. Elshaier"
                    },
                    {
                        "authorId": "41229815",
                        "name": "M. Gomaa"
                    },
                    {
                        "authorId": "1697259",
                        "name": "A. Hassanien"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Disturbance based methods [9], [18], [25], [26] monitor the changes of predicted values under different input disturbances, to learn the importance score of input characteristics."
            ],
            "citingPaper": {
                "paperId": "e0639a141a5bca7c2100b042226a859e69370d07",
                "externalIds": {
                    "DBLP": "conf/iccpr/KangLL22",
                    "DOI": "10.1145/3581807.3581850",
                    "CorpusId": 258834549
                },
                "corpusId": 258834549,
                "publicationVenue": {
                    "id": "73e015d4-4386-4edd-b9c2-f563f870bfa2",
                    "name": "International Conferences on Computing and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "ICCPR",
                        "Int Conf Comput Pattern Recognit"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e0639a141a5bca7c2100b042226a859e69370d07",
                "title": "GANExplainer: Explainability Method for Graph Neural Network with Generative Adversarial Nets",
                "abstract": "In recent years, graph neural networks (GNNs) have achieved encouraging performance in the processing of graph data generated in non-Euclidean space. GNNs learn node features by aggregating and combining neighbor information, which is applied to many graphics tasks. However, the complex deep learning structure is still regarded as a black box, which is difficult to obtain the full trust of human beings. Due to the lack of interpretability, the application of graph neural network is greatly limited. Therefore, we propose an interpretable method, called GANExplainer, to explain GNNs at the model level. Our method can implicitly generate the characteristic subgraph of the graph without relying on specific input examples as the interpretation of the model to the data. GANExplainer relies on the framework of generative-adversarial method to train the generator and discriminator at the same time. More importantly, when constructing the discriminator, the corresponding graph rules are added to ensure the effectiveness of the generated characteristic subgraph. We carried out experiments on synthetic dataset and chemical molecules dataset and verified the effect of our method on model level interpreter from three aspects: accuracy, fidelity and sparsity.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2172124591",
                        "name": "Xinrui Kang"
                    },
                    {
                        "authorId": "2176030959",
                        "name": "Dong Liang"
                    },
                    {
                        "authorId": "2108049317",
                        "name": "Qinfeng Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "For example, selecting and extracting subgraphs for graph pooling [34], identifying important subgraphs to explain graph neural network [51], fusing subgraph hierarchical features for link prediction [25], classifying subgraphs for link prediction [23], and incorporating subgraphs for reasoning [35] and link prediction [19] on knowledge graphs."
            ],
            "citingPaper": {
                "paperId": "7dc596e7b9d9df2ed54a9852dd07eeb60028191d",
                "externalIds": {
                    "DBLP": "conf/wsdm/LiuYXL023",
                    "ArXiv": "2211.00572",
                    "DOI": "10.1145/3539597.3570429",
                    "CorpusId": 253244624
                },
                "corpusId": 253244624,
                "publicationVenue": {
                    "id": "ea38228f-6ed3-4222-a3ce-d963d8cc9516",
                    "name": "Web Search and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Web Search Data Min",
                        "WSDM"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=3158"
                },
                "url": "https://www.semanticscholar.org/paper/7dc596e7b9d9df2ed54a9852dd07eeb60028191d",
                "title": "Position-Aware Subgraph Neural Networks with Data-Efficient Learning",
                "abstract": "Data-efficient learning on graphs (GEL) is essential in real-world applications. Existing GEL methods focus on learning useful representations for nodes, edges, or entire graphs with \"small\" labeled data. But the problem of data-efficient learning for subgraph prediction has not been explored. The challenges of this problem lie in the following aspects: 1) It is crucial for subgraphs to learn positional features to acquire structural information in the base graph in which they exist. Although the existing subgraph neural network method is capable of learning disentangled position encodings, the overall computational complexity is very high. 2) Prevailing graph augmentation methods for GEL, including rule-based, sample-based, adaptive, and automated methods, are not suitable for augmenting subgraphs because a subgraph contains fewer nodes but richer information such as position, neighbor, and structure. Subgraph augmentation is more susceptible to undesirable perturbations. 3) Only a small number of nodes in the base graph are contained in subgraphs, which leads to a potential \"bias\" problem that the subgraph representation learning is dominated by these \"hot\" nodes. By contrast, the remaining nodes fail to be fully learned, which reduces the generalization ability of subgraph representation learning. In this paper, we aim to address the challenges above and propose a Position-Aware Data-Efficient Learning framework for subgraph neural networks called PADEL. Specifically, we propose a novel node position encoding method that is anchor-free, and design a new generative subgraph augmentation method based on a diffused variational subgraph autoencoder, and we propose exploratory and exploitable views for subgraph contrastive learning. Extensive experiment results on three real-world datasets show the superiority of our proposed method over state-of-the-art baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118484039",
                        "name": "Chang Liu"
                    },
                    {
                        "authorId": "2108734262",
                        "name": "Yuwen Yang"
                    },
                    {
                        "authorId": "152306954",
                        "name": "Zhe Xie"
                    },
                    {
                        "authorId": "2115863606",
                        "name": "Hongtao Lu"
                    },
                    {
                        "authorId": "2142355138",
                        "name": "Yue Ding"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In [8], SubgraphX identified the most relevant subgraph explaining the model prediction via Monte Carlo tree search using Shapley values as a measure of subgraph importance.",
                ") input graph that contributed most towards the underlying GNN model\u2019s prediction [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15]."
            ],
            "citingPaper": {
                "paperId": "67c0a7c64b62c2b6370c3788c685d00d610963d6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-17159",
                    "ArXiv": "2210.17159",
                    "DOI": "10.48550/arXiv.2210.17159",
                    "CorpusId": 253237095
                },
                "corpusId": 253237095,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/67c0a7c64b62c2b6370c3788c685d00d610963d6",
                "title": "PAGE: Prototype-Based Model-Level Explanations for Graph Neural Networks",
                "abstract": "\u2014Aside from graph neural networks (GNNs) catching signi\ufb01cant attention as a powerful framework revolutionizing graph representation learning, there has been an increasing demand for explaining GNN models. Although various explanation methods for GNNs have been developed, most studies have focused on instance-level explanations, which produce explanations tailored to a given graph instance. In our study, we propose Prototype-bAsed GNN-Explainer (PAGE) , a novel model-level GNN explanation method that explains what the underlying GNN model has learned for graph classi\ufb01cation by discovering human-interpretable prototype graphs . Our method produces explanations for a given class , thus being capable of offering more concise and comprehensive explanations than those of instance-level explanations. First, PAGE selects embeddings of class-discriminative input graphs on the graph-level embedding space after clustering them. Then, PAGE discovers a common subgraph pattern by iteratively searching for high matching node tuples using node-level embeddings via a prototype scoring function, thereby yielding a prototype graph as our explanation. Using \ufb01ve graph classi\ufb01cation datasets, we demonstrate that PAGE qualitatively and quantitatively outperforms the state-of-the-art model-level explanation method. We also carry out experimental studies systematically by showing the relationship between PAGE and instance-level explanation methods, the robustness of PAGE to input data scarce environments, and the computational ef\ufb01ciency of the proposed prototype scoring function in PAGE.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2087244325",
                        "name": "Yong-Min Shin"
                    },
                    {
                        "authorId": "2129906247",
                        "name": "Sun-Woo Kim"
                    },
                    {
                        "authorId": "2164501868",
                        "name": "Won-Yong Shin"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Shapley values have been extensively applied to machine learning model explanations (Lundberg & Lee, 2017; Lundberg et al., 2018; \u0160trumbelj & Kononenko, 2014; Sundararajan & Najmi, 2020; Wang et al., 2021a; Zhang et al., 2021; Frye et al., 2020; Yuan et al., 2021) and feature importance (Covert et al.",
                "\u2026have been extensively applied to machine learning model explanations (Lundberg & Lee, 2017; Lundberg et al., 2018; S\u030ctrumbelj & Kononenko, 2014; Sundararajan & Najmi, 2020; Wang et al., 2021a; Zhang et al., 2021; Frye et al., 2020; Yuan et al., 2021) and feature importance (Covert et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "ac983aa2f778cdb017df3e13430ec17802a54cf3",
                "externalIds": {
                    "ArXiv": "2210.17426",
                    "CorpusId": 258967747
                },
                "corpusId": 258967747,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ac983aa2f778cdb017df3e13430ec17802a54cf3",
                "title": "Trade-off Between Efficiency and Consistency for Removal-based Explanations",
                "abstract": "In the current landscape of explanation methodologies, most predominant approaches, such as SHAP and LIME, employ removal-based techniques to evaluate the impact of individual features by simulating various scenarios with specific features omitted. Nonetheless, these methods primarily emphasize efficiency in the original context, often resulting in general inconsistencies. In this paper, we demonstrate that such inconsistency is an inherent aspect of these approaches by establishing the Impossible Trinity Theorem, which posits that interpretability, efficiency and consistency cannot hold simultaneously. Recognizing that the attainment of an ideal explanation remains elusive, we propose the utilization of interpretation error as a metric to gauge inconsistencies and inefficiencies. To this end, we present two novel algorithms founded on the standard polynomial basis, aimed at minimizing interpretation error. Our empirical findings indicate that the proposed methods achieve a substantial reduction in interpretation error, up to 31.8 times lower when compared to alternative techniques.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108464109",
                        "name": "Yifan Zhang"
                    },
                    {
                        "authorId": "2110436433",
                        "name": "Haowei He"
                    },
                    {
                        "authorId": "144459366",
                        "name": "Zhiyuan Tan"
                    },
                    {
                        "authorId": "2116944866",
                        "name": "Yang Yuan"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Some methods to interpret graph neural networks can be applied to geometric data (Ying et al., 2019; Luo et al., 2020; Schlichtkrull et al., 2021; Yuan et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "18ffe561eefe204c116d7f871a25c0500f5098e0",
                "externalIds": {
                    "DBLP": "conf/iclr/0001LLL23",
                    "ArXiv": "2210.16966",
                    "DOI": "10.48550/arXiv.2210.16966",
                    "CorpusId": 253238010
                },
                "corpusId": 253238010,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/18ffe561eefe204c116d7f871a25c0500f5098e0",
                "title": "Interpretable Geometric Deep Learning via Learnable Randomness Injection",
                "abstract": "Point cloud data is ubiquitous in scientific fields. Recently, geometric deep learning (GDL) has been widely applied to solve prediction tasks with such data. However, GDL models are often complicated and hardly interpretable, which poses concerns to scientists who are to deploy these models in scientific analysis and experiments. This work proposes a general mechanism, learnable randomness injection (LRI), which allows building inherently interpretable models based on general GDL backbones. LRI-induced models, once trained, can detect the points in the point cloud data that carry information indicative of the prediction label. We also propose four datasets from real scientific applications that cover the domains of high-energy physics and biochemistry to evaluate the LRI mechanism. Compared with previous post-hoc interpretation methods, the points detected by LRI align much better and stabler with the ground-truth patterns that have actual scientific meanings. LRI is grounded by the information bottleneck principle, and thus LRI-induced models are also more robust to distribution shifts between training and test scenarios. Our code and datasets are available at \\url{https://github.com/Graph-COM/LRI}.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2151793768",
                        "name": "Siqi Miao"
                    },
                    {
                        "authorId": "6426643",
                        "name": "Yunan Luo"
                    },
                    {
                        "authorId": "2156102035",
                        "name": "Miaoyuan Liu"
                    },
                    {
                        "authorId": "2112519768",
                        "name": "Pan Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Independently from this categorization, a further fundamental distinction is among explainers providing explanations in terms of edge [91, 53, 69, 96] or node masks [79, 80, 57, 6, 57, 70].",
                "[95], and choose to investigate instance-based explainers[105, 74, 78, 80, 79, 84, 91, 6, 71, 38, 65, 96, 54, 75, 52, 101, 70, 24], i.",
                "[91], is a widely used dataset for benchmarking GNN explainers [91, 53, 75, 95, 96, 104].",
                "Despite the existence of other works proposing explainers, which occasionally fall outside the aforementioned categorization [96, 54, 75, 52, 38, 101, 70], we limited our analysis on a subset."
            ],
            "citingPaper": {
                "paperId": "bfafae6c6add7f06bd212911de27f75e416c015a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-15304",
                    "ArXiv": "2210.15304",
                    "DOI": "10.48550/arXiv.2210.15304",
                    "CorpusId": 253157826
                },
                "corpusId": 253157826,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bfafae6c6add7f06bd212911de27f75e416c015a",
                "title": "Explaining the Explainers in Graph Neural Networks: a Comparative Study",
                "abstract": "Following a fast initial breakthrough in graph based learning, Graph Neural Networks (GNNs) have reached a widespread application in many science and engineering fields, prompting the need for methods to understand their decision process. GNN explainers have started to emerge in recent years, with a multitude of methods both novel or adapted from other domains. To sort out this plethora of alternative approaches, several studies have benchmarked the performance of different explainers in terms of various explainability metrics. However, these earlier works make no attempts at providing insights into why different GNN architectures are more or less explainable, or which explainer should be preferred in a given setting. In this survey, we fill these gaps by devising a systematic experimental study, which tests ten explainers on eight representative architectures trained on six carefully designed graph and node classification datasets. With our results we provide key insights on the choice and applicability of GNN explainers, we isolate key components that make them usable and successful and provide recommendations on how to avoid common interpretation pitfalls. We conclude by highlighting open questions and directions of possible future research.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2130615549",
                        "name": "Antonio Longa"
                    },
                    {
                        "authorId": "2165224662",
                        "name": "Steve Azzolin"
                    },
                    {
                        "authorId": "2042269425",
                        "name": "G. Santin"
                    },
                    {
                        "authorId": "50139333",
                        "name": "G. Cencetti"
                    },
                    {
                        "authorId": "2075355155",
                        "name": "Pietro Lio'"
                    },
                    {
                        "authorId": "49305855",
                        "name": "B. Lepri"
                    },
                    {
                        "authorId": "1702610",
                        "name": "Andrea Passerini"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "explainability in GNNs (for example, via subgraph exploration [31]) to Multiplex GNNs to automatically highlight patterns relevant to downstream prediction."
            ],
            "citingPaper": {
                "paperId": "0feca99382cdbf8e4a4b007b9b6a767893bacab8",
                "externalIds": {
                    "ArXiv": "2210.14377",
                    "DBLP": "conf/miccai/DSouzaWGFBBS22",
                    "DOI": "10.1007/978-3-031-16449-1_28",
                    "CorpusId": 252408449
                },
                "corpusId": 252408449,
                "publicationVenue": {
                    "id": "61a709e3-2060-423c-8de5-ffd3885aa31c",
                    "name": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                    "type": "conference",
                    "alternate_names": [
                        "Medical Image Computing and Computer-Assisted Intervention",
                        "MICCAI",
                        "Med Image Comput Comput Interv",
                        "Int Conf Med Image Comput Comput Interv"
                    ],
                    "url": "http://www.miccai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/0feca99382cdbf8e4a4b007b9b6a767893bacab8",
                "title": "Fusing Modalities by Multiplexed Graph Neural Networks for Outcome Prediction in Tuberculosis",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1409286673",
                        "name": "N. S. D'Souza"
                    },
                    {
                        "authorId": "2145338633",
                        "name": "Hongzhi Wang"
                    },
                    {
                        "authorId": "2063980268",
                        "name": "Andrea Giovannini"
                    },
                    {
                        "authorId": "1403825113",
                        "name": "A. Foncubierta-Rodr\u00edguez"
                    },
                    {
                        "authorId": "1380315035",
                        "name": "Kristen L. Beck"
                    },
                    {
                        "authorId": "2109885",
                        "name": "O. Boyko"
                    },
                    {
                        "authorId": "1387878041",
                        "name": "T. Syeda-Mahmood"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "601e18e5158d03d4db62c9bd85519f40f8e32fe4",
                "externalIds": {
                    "ArXiv": "2210.12089",
                    "DBLP": "journals/corr/abs-2210-12089",
                    "DOI": "10.1145/3618105",
                    "CorpusId": 253080529
                },
                "corpusId": 253080529,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/601e18e5158d03d4db62c9bd85519f40f8e32fe4",
                "title": "A Survey on Graph Counterfactual Explanations: Definitions, Methods, Evaluation, and Research Challenges",
                "abstract": "Graph Neural Networks (GNNs) perform well in community detection and molecule classification. Counterfactual Explanations (CE) provide counter-examples to overcome the transparency limitations of black-box models. Due to the growing attention in graph learning, we focus on the concepts of CE for GNNs. We analysed the SoA to provide a taxonomy, a uniform notation, and the benchmarking datasets and evaluation metrics. We discuss fourteen methods, their evaluation protocols, twenty-two datasets, and nineteen metrics. We integrated the majority of methods into the GRETEL library to conduct an empirical evaluation to understand their strengths and pitfalls. We highlight open challenges and future work.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1390033678",
                        "name": "Mario Alfonso Prado-Romero"
                    },
                    {
                        "authorId": "32208207",
                        "name": "Bardh Prenkaj"
                    },
                    {
                        "authorId": "1765155",
                        "name": "G. Stilo"
                    },
                    {
                        "authorId": "1685102",
                        "name": "F. Giannotti"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[7], is an active research area with several following research papers [8, 22, 23]."
            ],
            "citingPaper": {
                "paperId": "9be4ad61622a522dd044a0696afcfdb27378e8f0",
                "externalIds": {
                    "ArXiv": "2210.11094",
                    "DBLP": "journals/corr/abs-2210-11094",
                    "DOI": "10.48550/arXiv.2210.11094",
                    "CorpusId": 253018959
                },
                "corpusId": 253018959,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9be4ad61622a522dd044a0696afcfdb27378e8f0",
                "title": "Toward Multiple Specialty Learners for Explaining GNNs via Online Knowledge Distillation",
                "abstract": "Graph Neural Networks (GNNs) have become increasingly ubiquitous in numerous applications and systems, necessitating explanations of their predictions, especially when making critical decisions. However, explaining GNNs is challenging due to the complexity of graph data and model execution. Despite additional computational costs, post-hoc explanation approaches have been widely adopted due to the generality of their architectures. Intrinsically interpretable models provide instant explanations but are usually model-specific, which can only explain particular GNNs. Therefore, we propose a novel GNN explanation framework named SCALE, which is general and fast for explaining predictions. SCALE trains multiple specialty learners to explain GNNs since constructing one powerful explainer to examine attributions of interactions in input graphs is complicated. In training, a black-box GNN model guides learners based on an online knowledge distillation paradigm. In the explanation phase, explanations of predictions are provided by multiple explainers corresponding to trained learners. Specifically, edge masking and random walk with restart procedures are executed to provide structural explanations for graph-level and node-level predictions, respectively. A feature attribution module provides overall summaries and instance-level feature contributions. We compare SCALE with state-of-the-art baselines via quantitative and qualitative experiments to prove its explanation correctness and execution performance. We also conduct a series of ablation studies to understand the strengths and weaknesses of the proposed framework.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "41022579",
                        "name": "Tien-Cuong Bui"
                    },
                    {
                        "authorId": "2055470540",
                        "name": "Van-Duc Le"
                    },
                    {
                        "authorId": "2108718185",
                        "name": "Wen-Syan Li"
                    },
                    {
                        "authorId": "2237996",
                        "name": "S. Cha"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Furthermore, most explanation methods for GNNs\u2019 focus on providing factual explanations [9, 14, 26]."
            ],
            "citingPaper": {
                "paperId": "0923ba40016c1a93e3642a7a5cd43acfa834dc0a",
                "externalIds": {
                    "DBLP": "conf/cikm/Prado-RomeroS22",
                    "DOI": "10.1145/3511808.3557608",
                    "CorpusId": 252904717
                },
                "corpusId": 252904717,
                "publicationVenue": {
                    "id": "7431ff67-91dc-41fa-b322-1b1ca657025f",
                    "name": "International Conference on Information and Knowledge Management",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Information and Knowledge Management",
                        "Conf Inf Knowl Manag",
                        "Int Conf Inf Knowl Manag",
                        "CIKM"
                    ],
                    "url": "http://www.cikm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/0923ba40016c1a93e3642a7a5cd43acfa834dc0a",
                "title": "GRETEL: Graph Counterfactual Explanation Evaluation Framework",
                "abstract": "Machine Learning (ML) systems are a building part of the modern tools which impact our daily life in several application domains. Due to their black-box nature, those systems are hardly adopted in application domains (e.g. health, finance) where understanding the decision process is of paramount importance. Explanation methods were developed to explain how the ML model has taken a specific decision for a given case/instance. Graph Counterfactual Explanations (GCE) is one of the explanation techniques adopted in the Graph Learning domain. The existing works on Graph Counterfactual Explanations diverge mostly in the problem definition, application domain, test data, and evaluation metrics, and most existing works do not compare exhaustively against other counterfactual explanation techniques present in the literature. We present GRETEL, a unified framework to develop and test GCE methods in several settings. GRETEL is a highly extensible evaluation framework which promotes Open Science and the reproducibility of the evaluation by providing a set of well-defined mechanisms to integrate and manage easily: both real and synthetic datasets, ML models, state-of-the-art explanation techniques, and evaluation measures. Lastly, we also show the experiments conducted to integrate and test several existing scenarios (datasets, measures, explainers).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1390033678",
                        "name": "Mario Alfonso Prado-Romero"
                    },
                    {
                        "authorId": "1765155",
                        "name": "G. Stilo"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "A second class of methods [6],[9],[15],[13] perturbs input features by learning masks to investigate the change in class prediction due to these perturbations."
            ],
            "citingPaper": {
                "paperId": "38f513a6cefe6148e1e479312e56f3ed1deaa3a5",
                "externalIds": {
                    "DBLP": "conf/cikm/SahaDB22",
                    "DOI": "10.1145/3511808.3557535",
                    "CorpusId": 252905038
                },
                "corpusId": 252905038,
                "publicationVenue": {
                    "id": "7431ff67-91dc-41fa-b322-1b1ca657025f",
                    "name": "International Conference on Information and Knowledge Management",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Information and Knowledge Management",
                        "Conf Inf Knowl Manag",
                        "Int Conf Inf Knowl Manag",
                        "CIKM"
                    ],
                    "url": "http://www.cikm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/38f513a6cefe6148e1e479312e56f3ed1deaa3a5",
                "title": "A Model-Centric Explainer for Graph Neural Network based Node Classification",
                "abstract": "Graph Neural Networks (GNNs) learn node representations by aggregating a node's feature vector with its neighbors. They perform well across a variety of graph tasks. However, to enhance the reliability and trustworthiness of these models during use in critical scenarios, it is of essence to look into the decision making mechanisms of these models rather than treating them as black boxes. Our model-centric method gives insight into the kind of information learnt by GNNs about node neighborhoods during the task of node classification. We propose a neighborhood generator as an explainer that generates optimal neighborhoods to maximize a particular class prediction of the trained GNN model. We formulate neighborhood generation as a reinforcement learning problem and use a policy gradient method to train our generator using feedback from the trained GNN-based node classifier. Our method provides intelligible explanations of learning mechanisms of GNN models on synthetic as well as real-world datasets and even highlights certain shortcomings of these models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145760267",
                        "name": "Sayan Saha"
                    },
                    {
                        "authorId": "39544273",
                        "name": "Monidipa Das"
                    },
                    {
                        "authorId": "82752795",
                        "name": "S. Bandyopadhyay"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "a489f6377247d7c98102e644a48de0e640c7f279",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-09475",
                    "ArXiv": "2210.09475",
                    "DOI": "10.48550/arXiv.2210.09475",
                    "CorpusId": 252968370
                },
                "corpusId": 252968370,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a489f6377247d7c98102e644a48de0e640c7f279",
                "title": "AMPNet: Attention as Message Passing for Graph Neural Networks",
                "abstract": "Feature-level interactions between nodes can carry crucial information for under-standing complex interactions in graph-structured data. Current interpretability techniques, however, are limited in their ability to capture feature-level interactions between different nodes. In this work, we propose AMPNet, a general Graph Neural Network (GNN) architecture for uncovering feature-level interactions between different nodes in a graph. Our framework applies a multiheaded attention operation during message-passing to contextualize messages based on the feature interactions between different nodes. We utilize subgraph sampling and node feature downsampling in our experiments to improve the scalability of our architecture to large networks. We evaluate AMPNet on several benchmark and real-world datasets, and develop a synthetic benchmark based on cyclic cellular automata to test the ability of our framework to recover the underlying generation rules of the cellular automata based on feature-interactions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49669190",
                        "name": "S. A. Rizvi"
                    },
                    {
                        "authorId": "120729272",
                        "name": "N. Nguyen"
                    },
                    {
                        "authorId": "8544727",
                        "name": "H. Lyu"
                    },
                    {
                        "authorId": "2188058342",
                        "name": "B. Christensen"
                    },
                    {
                        "authorId": "16217712",
                        "name": "J. O. Caro"
                    },
                    {
                        "authorId": "103284094",
                        "name": "E. Zappala"
                    },
                    {
                        "authorId": "2117751468",
                        "name": "M. Brbi\u0107"
                    },
                    {
                        "authorId": "2180409627",
                        "name": "R. M. Dhodapkar"
                    },
                    {
                        "authorId": "7385683",
                        "name": "D. V. Dijk"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "More specifically, we fall short of understanding the influence of the input graph elements on both the changes in model parameters and the generalizability of a trained model [Ying et al., 2019, Huang et al., 2022, Yuan et al., 2021, Xu et al., 2019b, Zheng et al., 2021].\nar X\niv :2\n21 0.",
                "Explanation models for graphs [Ying et al., 2019, Huang et al., 2022, Yuan et al., 2021] provide an accessible relationship between the model\u2019s predictions and corresponding elements in graphs."
            ],
            "citingPaper": {
                "paperId": "78a8e8986ea2234a24df34b62e4cca095325aea3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-07441",
                    "ArXiv": "2210.07441",
                    "DOI": "10.48550/arXiv.2210.07441",
                    "CorpusId": 252907833
                },
                "corpusId": 252907833,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/78a8e8986ea2234a24df34b62e4cca095325aea3",
                "title": "Characterizing the Influence of Graph Elements",
                "abstract": "Influence function, a method from robust statistics, measures the changes of model parameters or some functions about model parameters concerning the removal or modification of training instances. It is an efficient and useful post-hoc method for studying the interpretability of machine learning models without the need for expensive model re-training. Recently, graph convolution networks (GCNs), which operate on graph data, have attracted a great deal of attention. However, there is no preceding research on the influence functions of GCNs to shed light on the effects of removing training nodes/edges from an input graph. Since the nodes/edges in a graph are interdependent in GCNs, it is challenging to derive influence functions for GCNs. To fill this gap, we started with the simple graph convolution (SGC) model that operates on an attributed graph and formulated an influence function to approximate the changes in model parameters when a node or an edge is removed from an attributed graph. Moreover, we theoretically analyzed the error bound of the estimated influence of removing an edge. We experimentally validated the accuracy and effectiveness of our influence estimation function. In addition, we showed that the influence function of an SGC model could be used to estimate the impact of removing training nodes/edges on the test performance of the SGC without re-training the model. Finally, we demonstrated how to use influence functions to guide the adversarial attacks on GCNs effectively.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2154502802",
                        "name": "Zizhang Chen"
                    },
                    {
                        "authorId": "48981982",
                        "name": "Peizhao Li"
                    },
                    {
                        "authorId": "2144318307",
                        "name": "Hongfu Liu"
                    },
                    {
                        "authorId": "145595134",
                        "name": "Pengyu Hong"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "We believe that threshold selection for Local Explainers is a fundamental problem to make local explainers actionable, but it is often left behind in favor of top-k selections where k is chosen based on the ground-truth motif.",
                "In the process, we use one of the available Local Explainers [35,21,38,30,26,24] to obtain a local explanation for each sample in the dataset.",
                "We leave to [37,18] a detailed overview about Local Explainers, who recently proposed a taxonomy to categorize the heterogeneity of those.",
                "In this work, we will broadly refer to all of those whose output can be mapped to a subgraph of the input graph (Ying et al., 2019; Luo et al., 2020; Yuan et al., 2021; Vu & Thai, 2020; Shan et al., 2021; Pope et al., 2019).",
                "Over the last years, many works proposed Local Explainers [35,21,38,30,26,24] to explain the decision process of a GNN in terms of factual explanations, often represented as subgraphs for each sample in the dataset.",
                "Over the last years, many works proposed Local Explainers (Ying et al., 2019; Luo et al., 2020; Yuan et al., 2021; Vu & Thai, 2020; Shan et al., 2021; Pope et al., 2019; Magister et al., 2021) to explain the decision process of a GNN in terms of factual explanations, often represented as subgraphs for each sample in the dataset.",
                "In principle, every Local Explainer whose output can be mapped to a subgraph of the input sample is compatible with our pipeline (Ying et al., 2019; Luo et al., 2020; Yuan et al., 2021; Vu & Thai, 2020; Shan et al., 2021; Pope et al., 2019).",
                "Many works recently proposed Local Explainers to explain the behaviour of a GNN [37].",
                "Overall, Local Explainers shed light over why the network predicted a certain value for a specific input sample."
            ],
            "citingPaper": {
                "paperId": "5bce30f98caa5decac63ef6cf58f5580b4c17883",
                "externalIds": {
                    "ArXiv": "2210.07147",
                    "DBLP": "journals/corr/abs-2210-07147",
                    "DOI": "10.48550/arXiv.2210.07147",
                    "CorpusId": 252873057
                },
                "corpusId": 252873057,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5bce30f98caa5decac63ef6cf58f5580b4c17883",
                "title": "Global Explainability of GNNs via Logic Combination of Learned Concepts",
                "abstract": "While instance-level explanation of GNN is a well-studied problem with plenty of approaches being developed, providing a global explanation for the behaviour of a GNN is much less explored, despite its potential in interpretability and debugging. Existing solutions either simply list local explanations for a given class, or generate a synthetic prototypical graph with maximal score for a given class, completely missing any combinatorial aspect that the GNN could have learned. In this work, we propose GLGExplainer (Global Logic-based GNN Explainer), the first Global Explainer capable of generating explanations as arbitrary Boolean combinations of learned graphical concepts. GLGExplainer is a fully differentiable architecture that takes local explanations as inputs and combines them into a logic formula over graphical concepts, represented as clusters of local explanations. Contrary to existing solutions, GLGExplainer provides accurate and human-interpretable global explanations that are perfectly aligned with ground-truth explanations (on synthetic data) or match existing domain knowledge (on real-world data). Extracted formulas are faithful to the model predictions, to the point of providing insights into some occasionally incorrect rules learned by the model, making GLGExplainer a promising diagnostic tool for learned GNNs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2165224662",
                        "name": "Steve Azzolin"
                    },
                    {
                        "authorId": "2130615549",
                        "name": "Antonio Longa"
                    },
                    {
                        "authorId": "2123005765",
                        "name": "Pietro Barbiero"
                    },
                    {
                        "authorId": "2075355155",
                        "name": "Pietro Lio'"
                    },
                    {
                        "authorId": "1702610",
                        "name": "Andrea Passerini"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "5aeea559f7f6ea5747507f55b9d5e5b5787e617b",
                "externalIds": {
                    "DBLP": "journals/tvcg/WangHCZG23",
                    "DOI": "10.1109/TVCG.2022.3209435",
                    "CorpusId": 252845550,
                    "PubMed": "36223348"
                },
                "corpusId": 252845550,
                "publicationVenue": {
                    "id": "5e1f6444-5d03-48c7-b202-7f47d492aeae",
                    "name": "IEEE Transactions on Visualization and Computer Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Vis Comput Graph"
                    ],
                    "issn": "1077-2626",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=2945"
                },
                "url": "https://www.semanticscholar.org/paper/5aeea559f7f6ea5747507f55b9d5e5b5787e617b",
                "title": "Extending the Nested Model for User-Centric XAI: A Design Study on GNN-based Drug Repurposing",
                "abstract": "Whether AI explanations can help users achieve specific tasks efficiently (i.e., usable explanations) is significantly influenced by their visual presentation. While many techniques exist to generate explanations, it remains unclear how to select and visually present AI explanations based on the characteristics of domain users. This paper aims to understand this question through a multidisciplinary design study for a specific problem: explaining graph neural network (GNN) predictions to domain experts in drug repurposing, i.e., reuse of existing drugs for new diseases. Building on the nested design model of visualization, we incorporate XAI design considerations from a literature review and from our collaborators' feedback into the design process. Specifically, we discuss XAI-related design considerations for usable visual explanations at each design layer: target user, usage context, domain explanation, and XAI goal at the domain layer; format, granularity, and operation of explanations at the abstraction layer; encodings and interactions at the visualization layer; and XAI and rendering algorithm at the algorithm layer. We present how the extended nested model motivates and informs the design of DrugExplorer, an XAI tool for drug repurposing. Based on our domain characterization, DrugExplorer provides path-based explanations and presents them both as individual paths and meta-paths for two key XAI operations, why and what else. DrugExplorer offers a novel visualization design called MetaMatrix with a set of interactions to help domain users organize and compare explanation paths at different levels of granularity to generate domain-meaningful insights. We demonstrate the effectiveness of the selected visual presentation and DrugExplorer as a whole via a usage scenario, a user study, and expert interviews. From these evaluations, we derive insightful observations and reflections that can inform the design of XAI visualizations for other scientific applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49110486",
                        "name": "Qianwen Wang"
                    },
                    {
                        "authorId": "49454094",
                        "name": "Kexin Huang"
                    },
                    {
                        "authorId": "89356020",
                        "name": "P. Chandak"
                    },
                    {
                        "authorId": "2095762",
                        "name": "M. Zitnik"
                    },
                    {
                        "authorId": "1896974",
                        "name": "N. Gehlenborg"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "65152afe85ce1c510ec20e70494f390b8bfcf4fd",
                "externalIds": {
                    "ArXiv": "2210.05713",
                    "PubMedCentral": "10089104",
                    "DBLP": "journals/corr/abs-2210-05713",
                    "DOI": "10.1002/hbm.26255",
                    "CorpusId": 252846247,
                    "PubMed": "36852610"
                },
                "corpusId": 252846247,
                "publicationVenue": {
                    "id": "7f501ba9-201f-4020-9843-a6fa8299c43e",
                    "name": "Human Brain Mapping",
                    "type": "journal",
                    "alternate_names": [
                        "Hum Brain Mapp"
                    ],
                    "issn": "1065-9471",
                    "url": "http://www3.interscience.wiley.com/cgi-bin/jhome/38751",
                    "alternate_urls": [
                        "https://onlinelibrary.wiley.com/journal/10970193",
                        "http://onlinelibrary.wiley.com/journal/10.1002/(ISSN)1097-0193"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/65152afe85ce1c510ec20e70494f390b8bfcf4fd",
                "title": "Explainable fMRI\u2010based brain decoding via spatial temporal\u2010pyramid graph convolutional network",
                "abstract": "Brain decoding, aiming to identify the brain states using neural activity, is important for cognitive neuroscience and neural engineering. However, existing machine learning methods for fMRI\u2010based brain decoding either suffer from low classification performance or poor explainability. Here, we address this issue by proposing a biologically inspired architecture, Spatial Temporal\u2010pyramid Graph Convolutional Network (STpGCN), to capture the spatial\u2013temporal graph representation of functional brain activities. By designing multi\u2010scale spatial\u2013temporal pathways and bottom\u2010up pathways that mimic the information process and temporal integration in the brain, STpGCN is capable of explicitly utilizing the multi\u2010scale temporal dependency of brain activities via graph, thereby achieving high brain decoding performance. Additionally, we propose a sensitivity analysis method called BrainNetX to better explain the decoding results by automatically annotating task\u2010related brain regions from the brain\u2010network standpoint. We conduct extensive experiments on fMRI data under 23 cognitive tasks from Human Connectome Project (HCP) S1200. The results show that STpGCN significantly improves brain\u2010decoding performance compared to competing baseline models; BrainNetX successfully annotates task\u2010relevant brain regions. Post hoc analysis based on these regions further validates that the hierarchical structure in STpGCN significantly contributes to the explainability, robustness and generalization of the model. Our methods not only provide insights into information representation in the brain under multiple cognitive tasks but also indicate a bright future for fMRI\u2010based brain decoding.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2114134255",
                        "name": "Ziyuan Ye"
                    },
                    {
                        "authorId": "2093923999",
                        "name": "Youzhi Qu"
                    },
                    {
                        "authorId": "2087124536",
                        "name": "Zhichao Liang"
                    },
                    {
                        "authorId": "2187668838",
                        "name": "Mo Wang"
                    },
                    {
                        "authorId": "1909685",
                        "name": "Quanying Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Other methods analyze the output of the model on the perturbation of the input [33] or determine contribution of a given feature to a prediction [44]."
            ],
            "citingPaper": {
                "paperId": "be13e66f6ec9550740ee0f6b1d8ffcfcc7727493",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-03745",
                    "ArXiv": "2210.03745",
                    "DOI": "10.48550/arXiv.2210.03745",
                    "CorpusId": 252780609
                },
                "corpusId": 252780609,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/be13e66f6ec9550740ee0f6b1d8ffcfcc7727493",
                "title": "ProGReST: Prototypical Graph Regression Soft Trees for Molecular Property Prediction",
                "abstract": "In this work, we propose the novel Prototypical Graph Regression Self-explainable Trees (ProGReST) model, which combines prototype learning, soft decision trees, and Graph Neural Networks. In contrast to other works, our model can be used to address various challenging tasks, including compound property prediction. In ProGReST, the rationale is obtained along with prediction due to the model's built-in interpretability. Additionally, we introduce a new graph prototype projection to accelerate model training. Finally, we evaluate PRoGReST on a wide range of chemical datasets for molecular property prediction and perform in-depth analysis with chemical experts to evaluate obtained interpretations. Our method achieves competitive results against state-of-the-art methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "146543800",
                        "name": "Dawid Rymarczyk"
                    },
                    {
                        "authorId": "2006371411",
                        "name": "D. Dobrowolski"
                    },
                    {
                        "authorId": "1388006351",
                        "name": "Tomasz Danel"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recently, some initial efforts [14, 22, 38, 41, 43] have been taken to address the explainability issue of GNNs."
            ],
            "citingPaper": {
                "paperId": "a0e0b032c29d71507b132caa0da77749c9b0ba01",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01974",
                    "ArXiv": "2210.01974",
                    "DOI": "10.48550/arXiv.2210.01974",
                    "CorpusId": 252715869
                },
                "corpusId": 252715869,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a0e0b032c29d71507b132caa0da77749c9b0ba01",
                "title": "Towards Prototype-Based Self-Explainable Graph Neural Network",
                "abstract": "Graph Neural Networks (GNNs) have shown great ability in modeling graph-structured data for various domains. However, GNNs are known as black-box models that lack interpretability. Without understanding their inner working, we cannot fully trust them, which largely limits their adoption in high-stake scenarios. Though some initial efforts have been taken to interpret the predictions of GNNs, they mainly focus on providing post-hoc explanations using an additional explainer, which could misrepresent the true inner working mechanism of the target GNN. The works on self-explainable GNNs are rather limited. Therefore, we study a novel problem of learning prototype-based self-explainable GNNs that can simultaneously give accurate predictions and prototype-based explanations on predictions. We design a framework which can learn prototype graphs that capture representative patterns of each class as class-level explanations. The learned prototypes are also used to simultaneously make prediction for for a test instance and provide instance-level explanation. Extensive experiments on real-world and synthetic datasets show the effectiveness of the proposed framework for both prediction accuracy and explanation quality.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152961073",
                        "name": "Enyan Dai"
                    },
                    {
                        "authorId": "2116430057",
                        "name": "Suhang Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "24b2aed0f130e5278325b5055711de44d247460e",
                "externalIds": {
                    "DBLP": "conf/nips/Fan0MST22",
                    "ArXiv": "2209.14107",
                    "DOI": "10.48550/arXiv.2209.14107",
                    "CorpusId": 252567836
                },
                "corpusId": 252567836,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/24b2aed0f130e5278325b5055711de44d247460e",
                "title": "Debiasing Graph Neural Networks via Learning Disentangled Causal Substructure",
                "abstract": "Most Graph Neural Networks (GNNs) predict the labels of unseen graphs by learning the correlation between the input graphs and labels. However, by presenting a graph classification investigation on the training graphs with severe bias, surprisingly, we discover that GNNs always tend to explore the spurious correlations to make decision, even if the causal correlation always exists. This implies that existing GNNs trained on such biased datasets will suffer from poor generalization capability. By analyzing this problem in a causal view, we find that disentangling and decorrelating the causal and bias latent variables from the biased graphs are both crucial for debiasing. Inspiring by this, we propose a general disentangled GNN framework to learn the causal substructure and bias substructure, respectively. Particularly, we design a parameterized edge mask generator to explicitly split the input graph into causal and bias subgraphs. Then two GNN modules supervised by causal/bias-aware loss functions respectively are trained to encode causal and bias subgraphs into their corresponding representations. With the disentangled representations, we synthesize the counterfactual unbiased training samples to further decorrelate causal and bias variables. Moreover, to better benchmark the severe bias problem, we construct three new graph datasets, which have controllable bias degrees and are easier to visualize and explain. Experimental results well demonstrate that our approach achieves superior generalization performance over existing baselines. Furthermore, owing to the learned edge mask, the proposed model has appealing interpretability and transferability. Code and data are available at: https://github.com/googlebaba/DisC.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48635390",
                        "name": "Shaohua Fan"
                    },
                    {
                        "authorId": "2118449003",
                        "name": "Xiao Wang"
                    },
                    {
                        "authorId": "2186301319",
                        "name": "Yanhu Mo"
                    },
                    {
                        "authorId": "2151458697",
                        "name": "Chuan Shi"
                    },
                    {
                        "authorId": "152226504",
                        "name": "Jian Tang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "02a6474812c0847def08d838570dcad3ef9ffdbd",
                "externalIds": {
                    "ArXiv": "2209.14402",
                    "CorpusId": 258833737
                },
                "corpusId": 258833737,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/02a6474812c0847def08d838570dcad3ef9ffdbd",
                "title": "L2XGNN: Learning to Explain Graph Neural Networks",
                "abstract": "Graph Neural Networks (GNNs) are a popular class of machine learning models. Inspired by the learning to explain (L2X) paradigm, we propose L2XGNN, a framework for explainable GNNs which provides faithful explanations by design. L2XGNN learns a mechanism for selecting explanatory subgraphs (motifs) which are exclusively used in the GNNs message-passing operations. L2XGNN is able to select, for each input graph, a subgraph with specific properties such as being sparse and connected. Imposing such constraints on the motifs often leads to more interpretable and effective explanations. Experiments on several datasets suggest that L2XGNN achieves the same classification accuracy as baseline methods using the entire input graph while ensuring that only the provided explanations are used to make predictions. Moreover, we show that L2XGNN is able to identify motifs responsible for the graph's properties it is intended to predict.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2163190285",
                        "name": "Giuseppe Serra"
                    },
                    {
                        "authorId": "2780262",
                        "name": "Mathias Niepert"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", 2022), most instance-level explanation methods can be classified into six categories: gradient-based methods(Baldassarre & Azizpour, 2019), perturbation-based methods (Yuan et al., 2021), decomposition methods (Schwarzenberg et al."
            ],
            "citingPaper": {
                "paperId": "192067b0d238d54480d72d751cbd005e2ad2d2e4",
                "externalIds": {
                    "ArXiv": "2209.07924",
                    "DBLP": "conf/iclr/WangS23",
                    "DOI": "10.48550/arXiv.2209.07924",
                    "CorpusId": 252355289
                },
                "corpusId": 252355289,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/192067b0d238d54480d72d751cbd005e2ad2d2e4",
                "title": "GNNInterpreter: A Probabilistic Generative Model-Level Explanation for Graph Neural Networks",
                "abstract": "Recently, Graph Neural Networks (GNNs) have significantly advanced the performance of machine learning tasks on graphs. However, this technological breakthrough makes people wonder: how does a GNN make such decisions, and can we trust its prediction with high confidence? When it comes to some critical fields, such as biomedicine, where making wrong decisions can have severe consequences, it is crucial to interpret the inner working mechanisms of GNNs before applying them. In this paper, we propose a model-agnostic model-level explanation method for different GNNs that follow the message passing scheme, GNNInterpreter, to explain the high-level decision-making process of the GNN model. More specifically, GNNInterpreter learns a probabilistic generative graph distribution that produces the most discriminative graph pattern the GNN tries to detect when making a certain prediction by optimizing a novel objective function specifically designed for the model-level explanation for GNNs. Compared to existing works, GNNInterpreter is more flexible and computationally efficient in generating explanation graphs with different types of node and edge features, without introducing another blackbox or requiring manually specified domain-specific rules. In addition, the experimental studies conducted on four different datasets demonstrate that the explanation graphs generated by GNNInterpreter match the desired graph pattern if the model is ideal; otherwise, potential model pitfalls can be revealed by the explanation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2144685915",
                        "name": "Xiaoqi Wang"
                    },
                    {
                        "authorId": "2110771216",
                        "name": "Hang Shen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "These explanations can be given with respect to node attributes Ma \u2208 R, nodes Mn \u2208 R , or edges Me \u2208 RN\u00d7N , depending on specific GNN explainer, such as GNNExplainer [14], PGExplainer [10], and SubgraphX [31].",
                "The visualization tools in GraphXAI allow users to compare the explanations of different GNN explainers, such as gradient-based methods (Gradient and Grad-CAM) and perturbation-based methods (GNNExplainer and SubgraphX).",
                "These explanations can be given with respect to node attributes Ma d\u2208 , nodes \u2208 Mn N , or edges \u2208 \u00d7Me\nN N , depending on specific GNN explainer, such as GNNExplainer14, PGExplainer10, and SubgraphX31.",
                "Results in Tables\u00a01\u20135 show that, while no explanation method performs well across all properties, across different ShapeGGen node classification datasets (Table\u00a06), SubgraphX outperforms other methods on average.",
                "We incorporate eight GNN explainability methods, including gradient-based: Grad29, GradCAM11, GuidedBP6, Integrated Gradients30; perturbation-based: GNNExplainer14, PGExplainer10, SubgraphX31; and surrogate-based methods: PGMExplainer13.",
                "Still, this faithfulness is relatively weak, only 0.001 better than\nMethod GEA (\u2191) GEF (\u2193) GES (\u2193) GECF (\u2193) GEGF (\u2193)\nRandom 0.148 \u00b1 0.002 0.579 \u00b1 0.007 0.920 \u00b1 0.002 0.763 \u00b1 0.003 0.023 \u00b1 0.002\nGrad 0.193 \u00b1 0.002 0.392 \u00b1 0.006 0.806 \u00b1 0.004 0.159 \u00b1 0.004 0.039 \u00b1 0.003\nGradCAM 0.222 \u00b1 0.002 0.452 \u00b1 0.006 0.263 \u00b1 0.004 0.010 \u00b1 0.001 0.020 \u00b1 0.002\nGuidedBP 0.194 \u00b1 0.001 0.557 \u00b1 0.007 0.432 \u00b1 0.004 0.067 \u00b1 0.002 0.021 \u00b1 0.002\nIG 0.142 \u00b1 0.002 0.545 \u00b1 0.007 0.727 \u00b1 0.005 0.110 \u00b1 0.003 0.021 \u00b1 0.002\nGNNExplainer 0.102 \u00b1 0.003 0.534 \u00b1 0.007 0.431 \u00b1 0.008 0.233 \u00b1 0.006 0.027 \u00b1 0.002\nPGMExplainer 0.133 \u00b1 0.002 0.541 \u00b1 0.007 0.984 \u00b1 0.001 0.791 \u00b1 0.003 0.096 \u00b1 0.004\nPGExplainer 0.194 \u00b1 0.002 0.557 \u00b1 0.007 0.217 \u00b1 0.004 0.009 \u00b1 0.000 0.029 \u00b1 0.002\nSubgraphX 0.324 \u00b1 0.004 0.254 \u00b1 0.006 0.745 \u00b1 0.005 0.241 \u00b1 0.006 0.035 \u00b1 0.003\nDataset Method GEA (\u2191) GEF (\u2193)\nMutag\nRandom 0.044 \u00b1 0.007 0.590 \u00b1 0.031\nGrad 0.022 \u00b1 0.006 0.598 \u00b1 0.030\nGradCAM 0.085 \u00b1 0.012 0.672 \u00b1 0.029\nGuidedBP 0.036 \u00b1 0.007 0.649 \u00b1 0.030\nIntegrated Grad (IG) 0.049 \u00b1 0.010 0.443 \u00b1 0.031\nGNNExplainer 0.031 \u00b1 0.005 0.618 \u00b1 0.030\nPGMExplainer 0.042 \u00b1 0.007 0.503 \u00b1 0.031\nPGExplainer 0.046 \u00b1 0.007 0.504 \u00b1 0.031\nSubgraphX 0.039 \u00b1 0.007 0.611 \u00b1 0.030\nBenzene\nRandom 0.108 \u00b1 0.003 0.513 \u00b1 0.012\nGrad 0.122 \u00b1 0.007 0.262 \u00b1 0.011\nGradCAM 0.291 \u00b1 0.007 0.551 \u00b1 0.012\nGuidedBP 0.205 \u00b1 0.007 0.438 \u00b1 0.012\nIntegrated Grad (IG) 0.044 \u00b1 0.003 0.182 \u00b1 0.010\nGNNExplainer 0.129 \u00b1 0.005 0.444 \u00b1 0.012\nPGMExplainer 0.154 \u00b1 0.006 0.433 \u00b1 0.012\nPGExplainer 0.169 \u00b1 0.007 0.375 \u00b1 0.012\nSubgraphX 0.371 \u00b1 0.009 0.513 \u00b1 0.012\nFl-Carbonyl\nRandom 0.087 \u00b1 0.007 0.440 \u00b1 0.26\nGrad 0.132 \u00b1 0.010 0.210 \u00b1 0.021\nGradCAM 0.005 \u00b1 0.007 0.500 \u00b1 0.026\nGuidedBP 0.089 \u00b1 0.010 0.315 \u00b1 0.024\nIntegrated Grad (IG) 0.091 \u00b1 0.007 0.174 \u00b1 0.019\nGNNExplainer 0.094 \u00b1 0.009 0.423 \u00b1 0.026\nPGMExplainer 0.078 \u00b1 0.008 0.426 \u00b1 0.026\nPGExplainer 0.079 \u00b1 0.009 0.372 \u00b1 0.025\nSubgraphX 0.008 \u00b1 0.002 0.466 \u00b1 0.026\n9Scientific Data | (2023) 10:144 | https://doi.org/10.1038/s41597-023-01974-x\nrandom explanation.",
                "In particular, SubgraphX generates 145.95% more accurate and 64.80% less unfaithful explanations than other GNN explanation methods.",
                "We incorporate eight GNN explainability methods, including gradient-based: Grad [29], GradCAM [11], GuidedBP [6], Integrated Gradients [30]; perturbation-based: GNNExplainer [14], PGExplainer [10], SubgraphX [31]; and surrogate-based methods: PGMExplainer [13].",
                "Some explainers can capture portions of the ground-truth explanation, such as SubgraphX and GNNExplainer, but others attribute no importance to the ground-truth shape, such as CAM and Gradient."
            ],
            "citingPaper": {
                "paperId": "f50c92916832fba9e0e56fa781b0a03b3e07f3d4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-09339",
                    "PubMedCentral": "10024712",
                    "ArXiv": "2208.09339",
                    "DOI": "10.1038/s41597-023-01974-x",
                    "CorpusId": 251710449,
                    "PubMed": "36934095"
                },
                "corpusId": 251710449,
                "publicationVenue": {
                    "id": "62924b2a-8fb8-4b93-92c6-735516b49af0",
                    "name": "Scientific Data",
                    "type": "journal",
                    "alternate_names": [
                        "Sci Data"
                    ],
                    "issn": "2052-4463",
                    "url": "http://www.nature.com/sdata/"
                },
                "url": "https://www.semanticscholar.org/paper/f50c92916832fba9e0e56fa781b0a03b3e07f3d4",
                "title": "Evaluating explainability for graph neural networks",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40228633",
                        "name": "Chirag Agarwal"
                    },
                    {
                        "authorId": "2149932294",
                        "name": "Owen Queen"
                    },
                    {
                        "authorId": "1892673",
                        "name": "Himabindu Lakkaraju"
                    },
                    {
                        "authorId": "2095762",
                        "name": "M. Zitnik"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "(Ying et al. 2019; Luo et al. 2020; Yuan et al. 2021) propose to adopt an explanation method to figure out the causal relationship between the model\u2019s inputs and outputs."
            ],
            "citingPaper": {
                "paperId": "8a4c8b331abc0d5522fc5262595ff7d597c8a93b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-08584",
                    "ArXiv": "2208.08584",
                    "DOI": "10.48550/arXiv.2208.08584",
                    "CorpusId": 251643624
                },
                "corpusId": 251643624,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/8a4c8b331abc0d5522fc5262595ff7d597c8a93b",
                "title": "Robust Causal Graph Representation Learning against Confounding Effects",
                "abstract": "The prevailing graph neural network models have achieved significant progress in graph representation learning. However, in this paper, we uncover an ever-overlooked phenomenon: the pre-trained graph representation learning model tested with full graphs underperforms the model tested with well-pruned graphs. This observation reveals that there exist confounders in graphs, which may interfere with the model learning semantic information, and current graph representation learning methods have not eliminated their influence. To tackle this issue, we propose Robust Causal Graph Representation Learning (RCGRL) to learn robust graph representations against confounding effects. RCGRL introduces an active approach to generate instrumental variables under unconditional moment restrictions, which empowers the graph representation learning model to eliminate confounders, thereby capturing discriminative information that is causally related to downstream predictions. We offer theorems and proofs to guarantee the theoretical effectiveness of the proposed approach. Empirically, we conduct extensive experiments on a synthetic dataset and multiple benchmark datasets. Experimental results demonstrate the effectiveness and generalization ability of RCGRL. Our codes are available at https://github.com/hang53/RCGRL.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108469098",
                        "name": "Hang Gao"
                    },
                    {
                        "authorId": "2118506408",
                        "name": "Jiangmeng Li"
                    },
                    {
                        "authorId": "2059455684",
                        "name": "Wenwen Qiang"
                    },
                    {
                        "authorId": "2114860376",
                        "name": "Lingyu Si"
                    },
                    {
                        "authorId": "2113743531",
                        "name": "Bing Xu"
                    },
                    {
                        "authorId": "2153619515",
                        "name": "Changwen Zheng"
                    },
                    {
                        "authorId": "2323566",
                        "name": "Fuchun Sun"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2013 Interpretability [14] \u2013 Robustness [17] \u2013 Accountability [6] \u2022 Part V: Future Trends In this part, we elucidate open challenges and future directions from the following perspectives."
            ],
            "citingPaper": {
                "paperId": "1edb1ac1f931121de094f39e2515a159cbc39621",
                "externalIds": {
                    "DBLP": "conf/kdd/KangT22",
                    "DOI": "10.1145/3534678.3542599",
                    "CorpusId": 251518303
                },
                "corpusId": 251518303,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/1edb1ac1f931121de094f39e2515a159cbc39621",
                "title": "Algorithmic Fairness on Graphs: Methods and Trends",
                "abstract": "Graph is a ubiquitous type of data that appears in many real-world applications, including social network analysis, recommendations and financial security. Important as it is, decades of research have developed plentiful computational models to mine graphs. Despite its prosperity, concerns with respect to the potential algorithmic discrimination have been grown recently. Algorithmic fairness on graphs, which aims to mitigate bias introduced or amplified during the graph mining process, is an attractive yet challenging research topic. The first challenge corresponds to the theoretical challenge, where the non-IID nature of graph data may not only invalidate the basic assumption behind many existing studies in fair machine learning, but also introduce new fairness definition(s) based on the inter-correlation between nodes rather than the existing fairness definition(s) in fair machine learning. The second challenge regarding its algorithmic aspect aims to understand how to balance the trade-off between model accuracy and fairness. This tutorial aims to (1) comprehensively review the state-of-the-art techniques to enforce algorithmic fairness on graphs and (2) enlighten the open challenges and future directions. We believe this tutorial could benefit researchers and practitioners from the areas of data mining, artificial intelligence and social science.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2111625448",
                        "name": "Jian Kang"
                    },
                    {
                        "authorId": "2058143613",
                        "name": "H. Tong"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "In this part, we firstly introduce several explanation methods like SubgraphX [13] and XGNN [11]."
            ],
            "citingPaper": {
                "paperId": "21f3d4b1129ee498cee9aad8adad3df9ee253348",
                "externalIds": {
                    "DBLP": "conf/kdd/JiLLLWXXY22",
                    "DOI": "10.1145/3534678.3542624",
                    "CorpusId": 251518269
                },
                "corpusId": 251518269,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/21f3d4b1129ee498cee9aad8adad3df9ee253348",
                "title": "Frontiers of Graph Neural Networks with DIG",
                "abstract": "This tutorial is proposed based upon the recently released open-source library Dive into Graphs (DIG) along with hands-on code examples. DIG is a turnkey library that considers four frontiers in graph deep learning, including self-supervised learning of GNNs, 3D GNNs, explainability of GNNs, and graph generation. It provides data interfaces, common algorithms, and evaluation metrics for each direction. It has 255,000+ visitors, 11,000+ installations, and 1,100+ stars within a year and is becoming a robust and dominant ecosystem for graph neural network research. In this tutorial, we will review representative methodologies for these four directions and show hands-on code examples to demonstrate how to effortlessly implement benchmarks using DIG. This tutorial targets a broad audience working on or interested in various research themes. To encourage audience participation, we will promote our tutorial in advance on social media, reading groups, and library contribution community. We anticipate this tutorial would attract more researchers to these interesting and promising topics, leading to a more active community, eventually generating both scientific values and real-world impacts.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1743600",
                        "name": "Shuiwang Ji"
                    },
                    {
                        "authorId": null,
                        "name": "Meng Liu"
                    },
                    {
                        "authorId": "2153630672",
                        "name": "Yi Liu"
                    },
                    {
                        "authorId": "2004524780",
                        "name": "Youzhi Luo"
                    },
                    {
                        "authorId": "2109120459",
                        "name": "Limei Wang"
                    },
                    {
                        "authorId": "14629242",
                        "name": "Yaochen Xie"
                    },
                    {
                        "authorId": "2115510017",
                        "name": "Zhao Xu"
                    },
                    {
                        "authorId": "2119316118",
                        "name": "Haiyang Yu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "In GNN prediction interpretation, SubgraphX [43] is the first work that utilizes the Shapley value.",
                "While several GNN explanation methods have been proposed lately, they mainly focus on finding essential subgraphs via edge selection processes [19, 26, 41, 42] or Monte Carlo tree search [43].",
                "Following GNNExplainer [41], multiple perturbation methods [19, 26, 43] have been proposed to explain GNNs by extracting essential subgraphs."
            ],
            "citingPaper": {
                "paperId": "5c9bbfa5e8fa084595c14b503cf9d1410ddfb22c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-03075",
                    "ArXiv": "2208.03075",
                    "DOI": "10.48550/arXiv.2208.03075",
                    "CorpusId": 251371662
                },
                "corpusId": 251371662,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5c9bbfa5e8fa084595c14b503cf9d1410ddfb22c",
                "title": "PGX: A Multi-level GNN Explanation Framework Based on Separate Knowledge Distillation Processes",
                "abstract": "Graph Neural Networks (GNNs) are widely adopted in advanced AI systems due to their capability of representation learning on graph data. Even though GNN explanation is crucial to increase user trust in the systems, it is challenging due to the complexity of GNN execution. Lately, many works have been proposed to address some of the issues in GNN explanation. However, they lack generalization capability or suffer from computational burden when the size of graphs is enormous. To address these challenges, we propose a multi-level GNN explanation framework based on an observation that GNN is a multimodal learning process of multiple components in graph data. The complexity of the original problem is relaxed by breaking into multiple sub-parts represented as a hierarchical structure. The top-level explanation aims at specifying the contribution of each component to the model execution and predictions, while fine-grained levels focus on feature attribution and graph structure attribution analysis based on knowledge distillation. Student models are trained in standalone modes and are responsible for capturing different teacher behaviors, later used for particular component interpretation. Besides, we also aim for personalized explanations as the framework can generate different results based on user preferences. Finally, extensive experiments demonstrate the effectiveness and fidelity of our proposed approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "41022579",
                        "name": "Tien-Cuong Bui"
                    },
                    {
                        "authorId": "2108718185",
                        "name": "Wen-Syan Li"
                    },
                    {
                        "authorId": "2237996",
                        "name": "S. Cha"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", via subgraph explorations [54] or counterfactual explanations [24].",
                "The majority of existing methods provide a factual explanation in the form of a subgraph of the original graph that is deemed to be important for the prediction [3,9,22,27,30,36,46,51,54]."
            ],
            "citingPaper": {
                "paperId": "edbdec562cb1525eafe148b249b75480a284dc59",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-04222",
                    "ArXiv": "2208.04222",
                    "DOI": "10.48550/arXiv.2208.04222",
                    "CorpusId": 251402395
                },
                "corpusId": 251402395,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/edbdec562cb1525eafe148b249b75480a284dc59",
                "title": "GREASE: Generate Factual and Counterfactual Explanations for GNN-based Recommendations",
                "abstract": "Recently, graph neural networks (GNNs) have been widely used to develop successful recommender systems. Although powerful, it is very difficult for a GNN-based recommender system to attach tangible explanations of why a specific item ends up in the list of suggestions for a given user. Indeed, explaining GNN-based recommendations is unique, and existing GNN explanation methods are inappropriate for two reasons. First, traditional GNN explanation methods are designed for node, edge, or graph classification tasks rather than ranking, as in recommender systems. Second, standard machine learning explanations are usually intended to support skilled decision-makers. Instead, recommendations are designed for any end-user, and thus their explanations should be provided in user-understandable ways. In this work, we propose GREASE, a novel method for explaining the suggestions provided by any black-box GNN-based recommender system. Specifically, GREASE first trains a surrogate model on a target user-item pair and its $l$-hop neighborhood. Then, it generates both factual and counterfactual explanations by finding optimal adjacency matrix perturbations to capture the sufficient and necessary conditions for an item to be recommended, respectively. Experimental results conducted on real-world datasets demonstrate that GREASE can generate concise and effective explanations for popular GNN-based recommender models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2157281262",
                        "name": "Ziheng Chen"
                    },
                    {
                        "authorId": "144925193",
                        "name": "F. Silvestri"
                    },
                    {
                        "authorId": "2144547216",
                        "name": "Jia Wang"
                    },
                    {
                        "authorId": "1591136873",
                        "name": "Yongfeng Zhang"
                    },
                    {
                        "authorId": null,
                        "name": "Zhenhua Huang"
                    },
                    {
                        "authorId": "34609799",
                        "name": "H. Ahn"
                    },
                    {
                        "authorId": "2651748",
                        "name": "Gabriele Tolomei"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "d6c8a17df1ffbdb705618d148f515b5f4847d40a",
                "externalIds": {
                    "DBLP": "journals/sensors/KyamakyaTMAM22",
                    "PubMedCentral": "9414192",
                    "DOI": "10.3390/s22166279",
                    "CorpusId": 251749465,
                    "PubMed": "36016040"
                },
                "corpusId": 251749465,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d6c8a17df1ffbdb705618d148f515b5f4847d40a",
                "title": "A Comprehensive \u201cReal-World Constraints\u201d-Aware Requirements Engineering Related Assessment and a Critical State-of-the-Art Review of the Monitoring of Humans in Bed",
                "abstract": "Currently, abnormality detection and/or prediction is a very hot topic. In this paper, we addressed it in the frame of activity monitoring of a human in bed. This paper presents a comprehensive formulation of a requirements engineering dossier for a monitoring system of a \u201chuman in bed\u201d for abnormal behavior detection and forecasting. Hereby, practical and real-world constraints and concerns were identified and taken into consideration in the requirements dossier. A comprehensive and holistic discussion of the anomaly concept was extensively conducted and contributed to laying the ground for a realistic specifications book of the anomaly detection system. Some systems engineering relevant issues were also briefly addressed, e.g., verification and validation. A structured critical review of the relevant literature led to identifying four major approaches of interest. These four approaches were evaluated from the perspective of the requirements dossier. It was thereby clearly demonstrated that the approach integrating graph networks and advanced deep-learning schemes (Graph-DL) is the one capable of fully fulfilling the challenging issues expressed in the real-world conditions aware specification book. Nevertheless, to meet immediate market needs, systems based on advanced statistical methods, after a series of adaptations, already ensure and satisfy the important requirements related to, e.g., low cost, solid data security and a fully embedded and self-sufficient implementation. To conclude, some recommendations regarding system architecture and overall systems engineering were formulated.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2051304",
                        "name": "K. Kyamakya"
                    },
                    {
                        "authorId": "2000683174",
                        "name": "V. Tavakkoli"
                    },
                    {
                        "authorId": "2182472694",
                        "name": "Simon McClatchie"
                    },
                    {
                        "authorId": "46807300",
                        "name": "M. Arbeiter"
                    },
                    {
                        "authorId": "93480196",
                        "name": "Bart G. Scholte van Mast"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "e3e081e97a6d3b4c12b8129e4c53966ddcf62c0c",
                "externalIds": {
                    "PubMedCentral": "9483788",
                    "DOI": "10.1016/j.isci.2022.105043",
                    "CorpusId": 251970917,
                    "PubMed": "36134335"
                },
                "corpusId": 251970917,
                "publicationVenue": {
                    "id": "60a698a8-aea0-41b9-86bd-e979ded8bc8d",
                    "name": "iScience",
                    "type": "journal",
                    "issn": "2589-0042",
                    "url": "https://www.cell.com/iscience/home",
                    "alternate_urls": [
                        "http://www.cell.com/iscience/home",
                        "https://www.sciencedirect.com/journal/iscience"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e3e081e97a6d3b4c12b8129e4c53966ddcf62c0c",
                "title": "EdgeSHAPer: Bond-centric Shapley value-based explanation method for graph neural networks",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2064006197",
                        "name": "A. Mastropietro"
                    },
                    {
                        "authorId": "1699347664",
                        "name": "G. Pasculli"
                    },
                    {
                        "authorId": "2064359012",
                        "name": "Christian Feldmann"
                    },
                    {
                        "authorId": "1411913888",
                        "name": "Raquel Rodr\u00edguez-P\u00e9rez"
                    },
                    {
                        "authorId": "143639187",
                        "name": "J. Bajorath"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "[37] proposed SubgraphX to explain GNNs by identifying important subgraphs.",
                "Fidelity+ = 1\nN N\u2211 i=1 (f(Gi)yi \u2212 f(G 1\u2212mi i )yi) (4)\nFidelity\u2212 = 1\nN N\u2211 i=1 (f(Gi)yi \u2212 f(G mi i )yi) (5)\nWhere N is the total number of samples, and yi is the class label. f(Gi)yi and f(G1\u2212mii )yi are the prediction probabilities of yi when using the original graph Gi and the occluded graph G 1\u2212mi i , which is gained by occluding important features found by explainers from the original graph.",
                "Explainability Evaluation\nFidelity.",
                "Thus a lower Fidelity\u2212 (\u2193) is desired.",
                "Specifically, Fidelity+ and\nFidelity\u2212 are used to quantify the necessity and sufficiency of the explanations, respectively.",
                "On the contrary, the lower Fidelity\u2212, the more sufficient the explanation.",
                "The characterization score is the weighted harmonic mean of Fidelity+ and Fidelity- as defined below:\nCharact = 2\u00d7 Fidelity+ \u00d7 (1\u2212 Fidelity\u2212) Fidelity+ + (1\u2212 Fidelity\u2212)\n(6)\nSparsity.",
                "The Fidelity+ [36,37] metric indicates the difference in predicted probability between the original predictions and the new prediction after removing important input features.",
                "The higher Fidelity+, the more necessary the explanation.",
                "Thus, a higher Fidelity+ (\u2191) is desired. f(Gmii )yi is the prediction probabilities of yi when using the explanation graph Gmii , which is obtained by important structures found by explainable methods.",
                "In contrast, the metric Fidelity\u2212 [36] represents prediction changes by keeping important input features and removing unimportant structures.",
                "It measures the fraction of features selected as important by explanation methods [20,37], which is defined in Eq.",
                "The Fidelity [36,37] metric indicates the difference in predicted probability between the original predictions and the new prediction after removing important input features."
            ],
            "citingPaper": {
                "paperId": "8f13afe3ce7391873ce92807fe3938851bafd079",
                "externalIds": {
                    "ArXiv": "2207.12599",
                    "DBLP": "journals/corr/abs-2207-12599",
                    "DOI": "10.48550/arXiv.2207.12599",
                    "CorpusId": 251067111
                },
                "corpusId": 251067111,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8f13afe3ce7391873ce92807fe3938851bafd079",
                "title": "A Survey of Explainable Graph Neural Networks: Taxonomy and Evaluation Metrics",
                "abstract": "Graph neural networks (GNNs) have demonstrated a significant boost in prediction performance on graph data. At the same time, the predictions made by these models are often hard to interpret. In that regard, many efforts have been made to explain the prediction mechanisms of these models from perspectives such as GNNExplainer, XGNN and PGExplainer. Although such works present systematic frameworks to interpret GNNs, a holistic review for explainable GNNs is unavailable. In this survey, we present a comprehensive review of explainability techniques developed for GNNs. We focus on explainable graph neural networks and categorize them based on the use of explainable methods. We further provide the common performance metrics for GNNs explanations and point out several future research directions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2179302425",
                        "name": "Yiqiao Li"
                    },
                    {
                        "authorId": "51239629",
                        "name": "Jianlong Zhou"
                    },
                    {
                        "authorId": "3455244",
                        "name": "Sunny Verma"
                    },
                    {
                        "authorId": "145093625",
                        "name": "Fang Chen"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "221ee346c96d60895eb0d446870aedcc5b0c9e81",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-12748",
                    "ArXiv": "2207.12748",
                    "DOI": "10.48550/arXiv.2207.12748",
                    "CorpusId": 251066831
                },
                "corpusId": 251066831,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/221ee346c96d60895eb0d446870aedcc5b0c9e81",
                "title": "ScoreCAM GNN: une explication optimale des r\u00e9seaux profonds sur graphes",
                "abstract": "The explainability of deep networks is becoming a central issue in the deep learning community. It is the same for learning on graphs, a data structure present in many real world problems. In this paper, we propose a method that is more optimal, lighter, consistent and better exploits the topology of the evaluated graph than the state-of-the-art methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2179189861",
                        "name": "Adrien Raison"
                    },
                    {
                        "authorId": "2066193155",
                        "name": "Pascal Bourdon"
                    },
                    {
                        "authorId": "48552959",
                        "name": "David Helbert"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2022 Sparsity: Sparsity measures the fraction of nodes that are selected for an explanation [8], [25]."
            ],
            "citingPaper": {
                "paperId": "07cec14c0f9d0e05caf770802b6d7346085f1449",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-11175",
                    "ArXiv": "2207.11175",
                    "DOI": "10.48550/arXiv.2207.11175",
                    "CorpusId": 251018570
                },
                "corpusId": 251018570,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/07cec14c0f9d0e05caf770802b6d7346085f1449",
                "title": "Explaining Dynamic Graph Neural Networks via Relevance Back-propagation",
                "abstract": "Graph Neural Networks (GNNs) have shown remarkable effectiveness in capturing abundant information in graph-structured data. However, the black-box nature of GNNs hinders users from understanding and trusting the models, thus leading to difficulties in their applications. While recent years witness the prosperity of the studies on explaining GNNs, most of them focus on static graphs, leaving the explanation of dynamic GNNs nearly unexplored. It is challenging to explain dynamic GNNs, due to their unique characteristic of time-varying graph structures. Directly using existing models designed for static graphs on dynamic graphs is not feasible because they ignore temporal dependencies among the snapshots. In this work, we propose DGExplainer to provide reliable explanation on dynamic GNNs. DGExplainer redistributes the output activation score of a dynamic GNN to the relevances of the neurons of its previous layer, which iterates until the relevance scores of the input neuron are obtained. We conduct quantitative and qualitative experiments on real-world datasets to demonstrate the effectiveness of the proposed framework for identifying important nodes for link prediction and node regression for dynamic GNNs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "95102259",
                        "name": "Jiaxuan Xie"
                    },
                    {
                        "authorId": "9720172",
                        "name": "Yezi Liu"
                    },
                    {
                        "authorId": "1798830",
                        "name": "Yanning Shen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "SubgraphX [27] adopts Monte Carlo tree search to obtain the important subgraph of the GNN\u2019s prediction."
            ],
            "citingPaper": {
                "paperId": "f8c25a4728b4d39ff8475358c36bc3a8c6944320",
                "externalIds": {
                    "DBLP": "conf/ijcnn/LiYPS22",
                    "DOI": "10.1109/IJCNN55064.2022.9892241",
                    "CorpusId": 252625108
                },
                "corpusId": 252625108,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/f8c25a4728b4d39ff8475358c36bc3a8c6944320",
                "title": "CoGNet: Cooperative Graph Neural Networks",
                "abstract": "Graph representation learning has received increasing attention in recent years for many real-world applications. A major challenge in graph representation learning is the lack of labeled data. To address this challenge, Graph Neural Networks (GNNs) use message passing frameworks to combine information from unlabeled data with labeled data. However, the use of unlabeled data under the message passing framework is indirect in the training process where unlabeled data does not supervise the training process. To fully exploit the potential of unlabeled data, we propose a novel dual-view cooperative training framework for graph data where unlabeled data is involved in the training process for supervision. Specifically, we regard different views as the reasoning processes of two GNN models with which the models make predictions, integrating the understanding of different models on the underlying graph. To exchange information between models, we design a pseudo-label-based approach, where the two models mutually provide pseudo labels to each other iteratively. Moreover, to ensure the quality of pseudo labels, we propose an entropy-based pseudo-labels selection procedure and we adopt GNNExplainer to visualize different views in our framework. Our comprehensive experimental evaluation shows that our methods can boost the performance of state-of-the-art models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2159314356",
                        "name": "Peibo Li"
                    },
                    {
                        "authorId": "2108586036",
                        "name": "Yixing Yang"
                    },
                    {
                        "authorId": "1783801",
                        "name": "M. Pagnucco"
                    },
                    {
                        "authorId": "2157995570",
                        "name": "Yang Song"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "The perturbation-based approaches (6; 38; 20; 41; 27) learns the important features and structural information by observing the predictive power of the model when noise is added to the input."
            ],
            "citingPaper": {
                "paperId": "0bff78ef88c6f1bbbf2fb77de4439f7d2168d8e9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-13983",
                    "ArXiv": "2206.13983",
                    "DOI": "10.48550/arXiv.2206.13983",
                    "CorpusId": 250089361
                },
                "corpusId": 250089361,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0bff78ef88c6f1bbbf2fb77de4439f7d2168d8e9",
                "title": "BAGEL: A Benchmark for Assessing Graph Neural Network Explanations",
                "abstract": "The problem of interpreting the decisions of machine learning is a well-researched and important. We are interested in a specific type of machine learning model that deals with graph data called graph neural networks. Evaluating interpretability approaches for graph neural networks (GNN) specifically are known to be challenging due to the lack of a commonly accepted benchmark. Given a GNN model, several interpretability approaches exist to explain GNN models with diverse (sometimes conflicting) evaluation methodologies. In this paper, we propose a benchmark for evaluating the explainability approaches for GNNs called Bagel. In Bagel, we firstly propose four diverse GNN explanation evaluation regimes -- 1) faithfulness, 2) sparsity, 3) correctness. and 4) plausibility. We reconcile multiple evaluation metrics in the existing literature and cover diverse notions for a holistic evaluation. Our graph datasets range from citation networks, document graphs, to graphs from molecules and proteins. We conduct an extensive empirical study on four GNN models and nine post-hoc explanation approaches for node and graph classification tasks. We open both the benchmarks and reference implementations and make them available at https://github.com/Mandeep-Rathee/Bagel-benchmark.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50447319",
                        "name": "M. Rathee"
                    },
                    {
                        "authorId": "143923185",
                        "name": "Thorben Funke"
                    },
                    {
                        "authorId": "39775488",
                        "name": "Avishek Anand"
                    },
                    {
                        "authorId": "35070805",
                        "name": "Megha Khosla"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": true,
            "contexts": [
                "Thus, different methods have been proposed to explain the predictions of GNNs, such as GraphLime [30], GNNExplainer [27], PGExplainer [28], PGMExplainer [31], SubgraphX [29], TAGE [32], XGNN [26], and GraphSVX [33].",
                "Recently, several techniques have been proposed to explain GNNs, such as XGNN [26], GNNExplainer [27], PGExplainer [28], and SubgraphX [29], etc.",
                "Another recent study proposes SubgraphX [29], which employs a search algorithm to explore and identify subgraphs with high Shapley scores.",
                "With the trained graph models, we quantitatively and qualitatively compare our FlowX with eight baselines, including GradCAM [34], DeepLIFT [43], GNNExplainer [27], PGExplainer [28], PGMExplainer [31], SubgraphX [29], GNNGI [37], GNN-LRP [37].",
                "Specially, for subgraph-based method SubgraphX [29], we pick the explainable subgraph out, then assign the edges in this subgraph instead of nodes as the explanation.",
                "Next, the recent study SubgraphX [29] proposes to explain GNNs via subgraphs.",
                "The metrics we apply are Fidelity [29], [42], Accuracy [27], and Sparsity [42]."
            ],
            "citingPaper": {
                "paperId": "661986252dc2457a3ce1ed8a208ddf1dd6aa1ece",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-12987",
                    "ArXiv": "2206.12987",
                    "DOI": "10.48550/arXiv.2206.12987",
                    "CorpusId": 250072908
                },
                "corpusId": 250072908,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/661986252dc2457a3ce1ed8a208ddf1dd6aa1ece",
                "title": "FlowX: Towards Explainable Graph Neural Networks via Message Flows",
                "abstract": "We investigate the explainability of graph neural networks (GNNs) as a step towards elucidating their working mechanisms. While most current methods focus on explaining graph nodes, edges, or features, we argue that, as the inherent functional mechanism of GNNs, message flows are more natural for performing explainability. To this end, we propose a novel method here, known as FlowX, to explain GNNs by identifying important message flows. To quantify the importance of flows, we propose to follow the philosophy of Shapley values from cooperative game theory. To tackle the complexity of computing all coalitions' marginal contributions, we propose an approximation scheme to compute Shapley-like values as initial assessments of further redistribution training. We then propose a learning algorithm to train flow scores and improve explainability. Experimental studies on both synthetic and real-world datasets demonstrate that our proposed FlowX leads to improved explainability of GNNs. The code is available at https://github.com/divelab/DIG.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1914700964",
                        "name": "Shurui Gui"
                    },
                    {
                        "authorId": "1498527026",
                        "name": "Hao Yuan"
                    },
                    {
                        "authorId": "2146041754",
                        "name": "Jie Wang"
                    },
                    {
                        "authorId": "40522277",
                        "name": "Qicheng Lao"
                    },
                    {
                        "authorId": "2154305557",
                        "name": "Kang Li"
                    },
                    {
                        "authorId": "1743600",
                        "name": "Shuiwang Ji"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "19e83a60ed705144729d312c854cd91086303243",
                "externalIds": {
                    "ArXiv": "2206.12104",
                    "DBLP": "journals/corr/abs-2206-12104",
                    "DOI": "10.1145/3534678.3539319",
                    "CorpusId": 250048541
                },
                "corpusId": 250048541,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/19e83a60ed705144729d312c854cd91086303243",
                "title": "On Structural Explanation of Bias in Graph Neural Networks",
                "abstract": "Graph Neural Networks (GNNs) have shown satisfying performance in various graph analytical problems. Hence, they have become the de facto solution in a variety of decision-making scenarios. However, GNNs could yield biased results against certain demographic subgroups. Some recent works have empirically shown that the biased structure of the input network is a significant source of bias for GNNs. Nevertheless, no studies have systematically scrutinized which part of the input network structure leads to biased predictions for any given node. The low transparency on how the structure of the input network influences the bias in GNN outcome largely limits the safe adoption of GNNs in various decision-critical scenarios. In this paper, we study a novel research problem of structural explanation of bias in GNNs. Specifically, we propose a novel post-hoc explanation framework to identify two edge sets that can maximally account for the exhibited bias and maximally contribute to the fairness level of the GNN prediction for any given node, respectively. Such explanations not only provide a comprehensive understanding of bias/fairness of GNN predictions but also have practical significance in building an effective yet fair GNN model. Extensive experiments on real-world datasets validate the effectiveness of the proposed framework towards delivering effective structural explanations for the bias of GNNs. Open-source code can be found at https://github.com/yushundong/REFEREE.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "123918726",
                        "name": "Yushun Dong"
                    },
                    {
                        "authorId": "2117075272",
                        "name": "Song Wang"
                    },
                    {
                        "authorId": "2153607948",
                        "name": "Yu Wang"
                    },
                    {
                        "authorId": "12524628",
                        "name": "Tyler Derr"
                    },
                    {
                        "authorId": "1737121128",
                        "name": "Jundong Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In our experiments, we compare the following methods: Random gives every edge and node feature a random value between 0 and 1; Distance assigns higher importance to edges that have lower distance to the target node; PageRank measures the importance of edges following the personalized PageRank strategy with automatic restart on the target node [9, 40]; Saliency (SA) measures node importance as the weight on every node after computing the gradient of the output with respect to node features [6]; Integrated Gradient (IG) avoids the saturation problem of the gradient-based method Saliency by accumulating gradients over the path from a baseline input (zero-vector) and the input at hand [36]; Grad-CAM is a generalization of class activation maps (CAM) [33]; Occlusion attributes the importance of an edge as the difference of the model initial prediction prediction on the graph after removing this edge [10]; GNNExplainer computes the importance of graph entities (node/edge/node feature) using the mutual information [46]; PGExplainer is very similar to GNNExplainer, but generates explanations only for the graph structure (nodes/edges) using the reparameterization trick to overcome computation intractability [22]; PGM-Explainer perturbs the input and uses probabilistic graphical models to find the dependencies between the nodes and the output [38]; and SubgraphX explores possible explanatory subgraphs with Monte Carlo Tree Search and assigns them a score using the Shapley value [49].",
                "To explain the decisions made by the GNNs, we adopt different classes of explainers including structure-based methods such as Distance and personalized PageRank [8, 36], gradient/featurebased methods such as SA [5], IG [5] and Grad-CAM [30], and perturbation-based methods such as GNNExplainer [41], PGM-Explainer [34], SubgraphX [44], PGExplainer [20] and Occlusion to generate the explanatory subgraphs.",
                "To make them comparable, most papers propose to fix a sparsity level to apply to all explanations and compare the same-sized explanations [4, 22, 44]."
            ],
            "citingPaper": {
                "paperId": "c559dc62c0d64295f9c0dfb5f322b3f30ddf44eb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-09677",
                    "ArXiv": "2206.09677",
                    "DOI": "10.48550/arXiv.2206.09677",
                    "CorpusId": 249889598
                },
                "corpusId": 249889598,
                "publicationVenue": {
                    "id": "50534c12-f4ba-4c64-806b-01647d1baacf",
                    "name": "LOG IN",
                    "type": "journal",
                    "alternate_names": [
                        "Log in",
                        "Log",
                        "LOG"
                    ],
                    "issn": "0720-8642",
                    "alternate_issns": [
                        "1547-4690",
                        "0024-5798"
                    ],
                    "url": "https://www.log-in-verlag.de/informatische_bildung/",
                    "alternate_urls": [
                        "https://www.anycorp.com/log/about",
                        "https://www.jstor.org/journal/log",
                        "https://www.anycorp.com/",
                        "http://www.jstor.org/action/showPublication?journalCode=log",
                        "http://www.anycorp.com/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c559dc62c0d64295f9c0dfb5f322b3f30ddf44eb",
                "title": "GraphFramEx: Towards Systematic Evaluation of Explainability Methods for Graph Neural Networks",
                "abstract": "As one of the most popular machine learning models today, graph neural networks (GNNs) have attracted intense interest recently, and so does their explainability. Users are increasingly interested in a better understanding of GNN models and their outcomes. Unfortunately, today's evaluation frameworks for GNN explainability often rely on few inadequate synthetic datasets, leading to conclusions of limited scope due to a lack of complexity in the problem instances. As GNN models are deployed to more mission-critical applications, we are in dire need for a common evaluation protocol of explainability methods of GNNs. In this paper, we propose, to our best knowledge, the first systematic evaluation framework for GNN explainability, considering explainability on three different\"user needs\". We propose a unique metric that combines the fidelity measures and classifies explanations based on their quality of being sufficient or necessary. We scope ourselves to node classification tasks and compare the most representative techniques in the field of input-level explainability for GNNs. For the inadequate but widely used synthetic benchmarks, surprisingly shallow techniques such as personalized PageRank have the best performance for a minimum computation time. But when the graph structure is more complex and nodes have meaningful features, gradient-based methods are the best according to our evaluation criteria. However, none dominates the others on all evaluation dimensions and there is always a trade-off. We further apply our evaluation protocol in a case study for frauds explanation on eBay transaction graphs to reflect the production environment.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146257620",
                        "name": "Kenza Amara"
                    },
                    {
                        "authorId": "83539859",
                        "name": "Rex Ying"
                    },
                    {
                        "authorId": "1445089663",
                        "name": "Zitao Zhang"
                    },
                    {
                        "authorId": "2171816624",
                        "name": "Zhihao Han"
                    },
                    {
                        "authorId": "2412958",
                        "name": "Yinan Shan"
                    },
                    {
                        "authorId": "1689559",
                        "name": "U. Brandes"
                    },
                    {
                        "authorId": "50323214",
                        "name": "S. Schemm"
                    },
                    {
                        "authorId": "1776014",
                        "name": "Ce Zhang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "Furthermore, most explanation methods for GNNs\u2019 focus on providing factual explanations [16,29,11]."
            ],
            "citingPaper": {
                "paperId": "28362783e390c02fe0ac129f0268e8fdfdd8293d",
                "externalIds": {
                    "ArXiv": "2206.02957",
                    "DBLP": "journals/corr/abs-2206-02957",
                    "DOI": "10.1145/3511808.3557608",
                    "CorpusId": 249431911
                },
                "corpusId": 249431911,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/28362783e390c02fe0ac129f0268e8fdfdd8293d",
                "title": "GRETEL: A unified framework for Graph Counterfactual Explanation Evaluation",
                "abstract": ". Nowadays, Machine Learning (ML) systems are a fundamental part of those tools with an impact on our daily life in several application domains. Unfortunately those systems, due to their black-box nature, are hardly adopted in those application domains (e.g. health, \ufb01nance) where having an understanding of the decision process is of paramount importance. For this reason, explanation methods were developed to give insight into how the ML model has taken a speci\ufb01c decision for a given case/instance. In particular, Graph Counterfactual Explanations (GCE) is one of the possible explanation techniques in the Graph Learning domain. Those techniques can be useful to discover, for example: i) molecular compounds similar in terms of speci\ufb01c desired properties, or ii) new insights into the interplay of di\ufb00erent brain regions for certain diseases. Unfortunately, the existing works of Graph Counterfactual Explanations diverge mostly in the problem de\ufb01nition, application domain, test data, and evaluation metrics, and most existing works do not compare against other counterfactual explanation techniques present in the literature. For these reasons, we present GRETEL , a uni\ufb01ed framework to develop and test GCEs\u2019. Our framework provides a set of well-de\ufb01ned mechanisms to easily integrate and manage: both real and synthetic datasets, ML models, state-of-the-art explanation techniques, and a set of evaluation measures. GRETEL is a well-organized and highly extensible platform, which promotes the Open Science and experiments reproducibility thus it can be adopted e\ufb00ortlessly by future researchers who want to create and test their new explanation methods by comparing them to existing techniques across several application domains, data and evaluation measures. To present GRETEL , we show the experiments conducted to integrate and test several synthetic and real datasets with several existing explanation techniques and base ML models. University of L\u2019Aquila.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1390033678",
                        "name": "Mario Alfonso Prado-Romero"
                    },
                    {
                        "authorId": "1765155",
                        "name": "G. Stilo"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "SubgraphX [20] derived connected subgraph sequences from the input graph, to overcome the relevant information flow breaks that may arise in PGExplainer or GNNExplainer."
            ],
            "citingPaper": {
                "paperId": "249f6d16f87e509629c7944e39d7b6e8ec2d81da",
                "externalIds": {
                    "ArXiv": "2206.03491",
                    "DBLP": "journals/corr/abs-2206-03491",
                    "DOI": "10.48550/arXiv.2206.03491",
                    "CorpusId": 249461902
                },
                "corpusId": 249461902,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/249f6d16f87e509629c7944e39d7b6e8ec2d81da",
                "title": "EiX-GNN : Concept-level eigencentrality explainer for graph neural networks",
                "abstract": "Nowadays, deep prediction models, especially graph neural networks, have a majorplace in critical applications. In such context, those models need to be highlyinterpretable or being explainable by humans, and at the societal scope, this understandingmay also be feasible for humans that do not have a strong prior knowledgein models and contexts that need to be explained. In the literature, explainingis a human knowledge transfer process regarding a phenomenon between an explainerand an explainee. We propose EiX-GNN (Eigencentrality eXplainer forGraph Neural Networks) a new powerful method for explaining graph neural networksthat encodes computationally this social explainer-to-explainee dependenceunderlying in the explanation process. To handle this dependency, we introducethe notion of explainee concept assimibility which allows explainer to adapt itsexplanation to explainee background or expectation. We lead a qualitative studyto illustrate our explainee concept assimibility notion on real-world data as wellas a qualitative study that compares, according to objective metrics established inthe literature, fairness and compactness of our method with respect to performingstate-of-the-art methods. It turns out that our method achieves strong results inboth aspects.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50792035",
                        "name": "P. Bourdon"
                    },
                    {
                        "authorId": "2013308",
                        "name": "D. Helbert"
                    },
                    {
                        "authorId": "1631289689",
                        "name": "A. Raison"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "On the other hand, several GNN explanation methods are proposed recently [23], [24], [25], [26], [27].",
                "SubgraphX [24] explains graph in node-assembled subgraph level by Monte Carlo tree search with Shapley value\nas the scoring function.",
                "SubgraphX [24] explains graph in node-assembled subgraph level by Monte Carlo tree search with Shapley value as the scoring function.",
                "In particular, PGMExplainer [23] and SubgraphX [24] apply a node-centric strategy to identify the important nodes as the explanation result."
            ],
            "citingPaper": {
                "paperId": "26647ac28bfd1bed6cf2d81d1cdd01dc1e852b69",
                "externalIds": {
                    "ArXiv": "2303.14836",
                    "DBLP": "conf/eurosp/HeJH22",
                    "DOI": "10.1109/EuroSP53844.2022.00013",
                    "CorpusId": 249997059
                },
                "corpusId": 249997059,
                "publicationVenue": {
                    "id": "4c2b8cb8-e51c-4ece-9122-89595989b56f",
                    "name": "European Symposium on Security and Privacy",
                    "type": "conference",
                    "alternate_names": [
                        "EuroS&P",
                        "IEEE European Symposium on Security and Privacy",
                        "Eur Symp Secur Priv",
                        "IEEE Eur Symp Secur Priv",
                        "EUROS&P"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/26647ac28bfd1bed6cf2d81d1cdd01dc1e852b69",
                "title": "Illuminati: Towards Explaining Graph Neural Networks for Cybersecurity Analysis",
                "abstract": "Graph neural networks (GNNs) have been utilized to create multi-layer graph models for a number of cybersecurity applications from fraud detection to software vulnerability analysis. Unfortunately, like traditional neural networks, GNNs also suffer from a lack of transparency, that is, it is challenging to interpret the model predictions. Prior works focused on specific factor explanations for a GNN model. In this work, we have designed and implemented Illuminati, a comprehensive and accurate explanation framework for cybersecurity applications using GNN models. Given a graph and a pre-trained GNN model, Illuminati is able to identify the important nodes, edges, and attributes that are contributing to the prediction while requiring no prior knowledge of GNN models. We evaluate Illuminati in two cybersecurity applications, i.e., code vulnerability detection and smart contract vulnerability detection. The experiments show that Illuminati achieves more accurate explanation results than state-of-the-art methods, specifically, 87.6% of subgraphs identified by Illuminati are able to retain their original prediction, an improvement of 10.3% over others at 77.3%. Furthermore, the explanation of Illuminati can be easily understood by the domain experts, suggesting the significant usefulness for the development of cybersecurity applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "11270586",
                        "name": "Haoyu He"
                    },
                    {
                        "authorId": "3028841",
                        "name": "Yuede Ji"
                    },
                    {
                        "authorId": "2107466791",
                        "name": "H. H. Huang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "81724e067cbd9c5f62e86074924a1b6f3e2bfa53",
                "externalIds": {
                    "DBLP": "conf/dsn/HerathWYY22",
                    "DOI": "10.1109/dsn53405.2022.00028",
                    "CorpusId": 250122258
                },
                "corpusId": 250122258,
                "publicationVenue": {
                    "id": "7f03ba38-4bb4-41a3-a6d7-914c28f84272",
                    "name": "Dependable Systems and Networks",
                    "type": "conference",
                    "alternate_names": [
                        "DSN",
                        "Dependable Syst Netw"
                    ],
                    "url": "http://www.dsn.org/"
                },
                "url": "https://www.semanticscholar.org/paper/81724e067cbd9c5f62e86074924a1b6f3e2bfa53",
                "title": "CFGExplainer: Explaining Graph Neural Network-Based Malware Classification from Control Flow Graphs",
                "abstract": "With the ever increasing threat of malware, extensive research effort has been put on applying Deep Learning for malware classification tasks. Graph Neural Networks (GNNs) that process malware as Control Flow Graphs (CFGs) have shown great promise for malware classification. However, these models are viewed as black-boxes, which makes it hard to validate and identify malicious patterns. To that end, we propose CFG-Explainer, a deep learning based model for interpreting GNN-oriented malware classification results. CFGExplainer identifies a subgraph of the malware CFG that contributes most towards classification and provides insight into importance of the nodes (i.e., basic blocks) within it. To the best of our knowledge, CFGExplainer is the first work that explains GNN-based mal-ware classification. We compared CFGExplainer against three explainers, namely GNNExplainer, SubgraphX and PGExplainer, and showed that CFGExplainer is able to identify top equisized subgraphs with higher classification accuracy than the other three models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145021872",
                        "name": "J. D. Herath"
                    },
                    {
                        "authorId": "50847225",
                        "name": "Priti Wakodikar"
                    },
                    {
                        "authorId": "93329376",
                        "name": "Pin Yang"
                    },
                    {
                        "authorId": "144456144",
                        "name": "Guanhua Yan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "25fcf6c8325a408c072f18f84c21c8b946408df3",
                "externalIds": {
                    "ArXiv": "2205.13733",
                    "DBLP": "conf/wsdm/ZhaoLZW23",
                    "DOI": "10.1145/3539597.3570421",
                    "CorpusId": 254854393
                },
                "corpusId": 254854393,
                "publicationVenue": {
                    "id": "ea38228f-6ed3-4222-a3ce-d963d8cc9516",
                    "name": "Web Search and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Web Search Data Min",
                        "WSDM"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=3158"
                },
                "url": "https://www.semanticscholar.org/paper/25fcf6c8325a408c072f18f84c21c8b946408df3",
                "title": "Towards Faithful and Consistent Explanations for Graph Neural Networks",
                "abstract": "Uncovering rationales behind predictions of graph neural networks (GNNs) has received increasing attention over recent years. Instance-level GNN explanation aims to discover critical input elements, like nodes or edges, that the target GNN relies upon for making predictions. Though various algorithms are proposed, most of them formalize this task by searching the minimal subgraph which can preserve original predictions. However, an inductive bias is deep-rooted in this framework: several subgraphs can result in the same or similar outputs as the original graphs. Consequently, they have the danger of providing spurious explanations and fail to provide consistent explanations. Applying them to explain weakly-performed GNNs would further amplify these issues. To address this problem, we theoretically examine the predictions of GNNs from the causality perspective. Two typical reasons of spurious explanations are identified: confounding effect of latent variables like distribution shift, and causal factors distinct from the original input. Observing that both confounding effects and diverse causal rationales are encoded in internal representations, we propose a simple yet effective countermeasure by aligning embeddings. Concretely, concerning potential shifts in the high-dimensional space, we design a distribution-aware alignment algorithm based on anchors. This new objective is easy to compute and can be incorporated into existing techniques with no or little effort. Theoretical analysis shows that it is in effect optimizing a more faithful explanation objective in design, which further justifies the proposed approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1999191869",
                        "name": "Tianxiang Zhao"
                    },
                    {
                        "authorId": "153640788",
                        "name": "Dongsheng Luo"
                    },
                    {
                        "authorId": "48505793",
                        "name": "Xiang Zhang"
                    },
                    {
                        "authorId": "2116430057",
                        "name": "Suhang Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "There are several works [11; 31; 54; 55; 56] that use the Mutagenicity dataset but call it MUTAG.",
                "Previous explanation methods[11; 14; 54; 56] use the existence of NO2 subgraphs as correct explanations [10].",
                "[56] search the space of all subgraphs as possible explanations."
            ],
            "citingPaper": {
                "paperId": "50164ce3a7126f815d33a2b004b83d122cd6f0cd",
                "externalIds": {
                    "ArXiv": "2205.13234",
                    "DBLP": "journals/corr/abs-2205-13234",
                    "DOI": "10.48550/arXiv.2205.13234",
                    "CorpusId": 249097802
                },
                "corpusId": 249097802,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/50164ce3a7126f815d33a2b004b83d122cd6f0cd",
                "title": "DT+GNN: A Fully Explainable Graph Neural Network using Decision Trees",
                "abstract": "We propose the fully explainable Decision Tree Graph Neural Network (DT+GNN) architecture. In contrast to existing black-box GNNs and post-hoc explanation methods, the reasoning of DT+GNN can be inspected at every step. To achieve this, we first construct a differentiable GNN layer, which uses a categorical state space for nodes and messages. This allows us to convert the trained MLPs in the GNN into decision trees. These trees are pruned using our newly proposed method to ensure they are small and easy to interpret. We can also use the decision trees to compute traditional explanations. We demonstrate on both real-world datasets and synthetic GNN explainability benchmarks that this architecture works as well as traditional GNNs. Furthermore, we leverage the explainability of DT+GNNs to find interesting insights into many of these datasets, with some surprising results. We also provide an interactive web tool to inspect DT+GNN's decision making.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146980777",
                        "name": "Peter M\u00fcller"
                    },
                    {
                        "authorId": "36352356",
                        "name": "Lukas Faber"
                    },
                    {
                        "authorId": "1995092493",
                        "name": "Karolis Martinkus"
                    },
                    {
                        "authorId": "2075356250",
                        "name": "Roger Wattenhofer"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "We can also observe a limitation of USIB that it considers edges independently but ignores the substructures of graphs whose importance is emphasized in previous work [41]."
            ],
            "citingPaper": {
                "paperId": "1c13e3ce072f6eeb9f33244c4866a52f7b584ed0",
                "externalIds": {
                    "ArXiv": "2205.09934",
                    "DBLP": "journals/corr/abs-2205-09934",
                    "DOI": "10.48550/arXiv.2205.09934",
                    "CorpusId": 248965040
                },
                "corpusId": 248965040,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1c13e3ce072f6eeb9f33244c4866a52f7b584ed0",
                "title": "Towards Explanation for Unsupervised Graph-Level Representation Learning",
                "abstract": "Due to the superior performance of Graph Neural Networks (GNNs) in various domains, there is an increasing interest in the GNN explanation problem \" which fraction of the input graph is the most crucial to decide the model\u2019s decision? \" Existing explanation methods focus on the supervised settings, e.g. , node classi\ufb01cation and graph classi\ufb01cation, while the explanation for unsupervised graph-level representation learning is still unexplored. The opaqueness of the graph representations may lead to unexpected risks when deployed for high-stake decision-making scenarios. In this paper, we advance the Information Bottleneck principle (IB) to tackle the proposed explanation problem for unsupervised graph representations, which leads to a novel principle, Unsupervised Subgraph Information Bottleneck (USIB). We also theoretically analyze the connection between graph representations and explanatory subgraphs on the label space, which reveals that the expressiveness and robustness of representations bene\ufb01t the \ufb01delity of explanatory subgraphs. Experimental results on both synthetic and real-world datasets demonstrate the superiority of our developed explainer and the validity of our theoretical analysis. ABSTRACT Data artifacts incentivize machine learning models to learn non-transferable generalizations by taking advantage of shortcuts in the data, and there is growing evidence that data artifacts play a role for the strong results that deep learning models achieve in recent natural language processing benchmarks. In this paper, we focus on task-oriented dialogue and investigate whether popular datasets such as MultiWOZ contain such data artifacts. We found that by only keeping frequent phrases in the training examples, state-of-the-art models perform similarly compared to the variant trained with full data, suggesting they exploit these spurious correlations to solve the task. Motivated by this, we propose a contrastive learning based framework to encourage the model to ignore these cues and focus on learning generalisable patterns. We also experiment with adversarial filtering to remove \u201ceasy\u201d training instances so that the model would focus on learning from the \u201charder\u201d instances. We conduct a number of generalization experiments \u2014 e.g., cross-domain/dataset and adversarial tests \u2014 to assess the robustness of our approach and found that it works exceptionally well.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152099796",
                        "name": "Qinghua Zheng"
                    },
                    {
                        "authorId": "2109656535",
                        "name": "Jihong Wang"
                    },
                    {
                        "authorId": "3326677",
                        "name": "Minnan Luo"
                    },
                    {
                        "authorId": "40508553",
                        "name": "Yaoliang Yu"
                    },
                    {
                        "authorId": "1737121128",
                        "name": "Jundong Li"
                    },
                    {
                        "authorId": "145095579",
                        "name": "L. Yao"
                    },
                    {
                        "authorId": "2840330",
                        "name": "Xiao Chang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Generally, they highlight the important patterns of the input graphs such as nodes [52, 96], edges [145, 79] and sub-graphs [148, 153] which are crucial for the model predictions.",
                "SubgraphX [153] explains the trained GNNs by generating subgraphs that are highly correlated with model predictions."
            ],
            "citingPaper": {
                "paperId": "d22efa7a35464ab9b40f8a4c926bbdcb91b84699",
                "externalIds": {
                    "ArXiv": "2205.10014",
                    "DBLP": "journals/corr/abs-2205-10014",
                    "DOI": "10.48550/arXiv.2205.10014",
                    "CorpusId": 248965359
                },
                "corpusId": 248965359,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d22efa7a35464ab9b40f8a4c926bbdcb91b84699",
                "title": "A Survey of Trustworthy Graph Learning: Reliability, Explainability, and Privacy Protection",
                "abstract": "Deep graph learning has achieved remarkable progresses in both business and scienti\ufb01c areas ranging from \ufb01nance and e-commerce, to drug and advanced material discovery. Despite these progresses, how to ensure various deep graph learning algorithms behave in a socially responsible manner and meet regulatory compliance requirements becomes an emerging problem, especially in risk-sensitive domains. Trustworthy graph learning (TwGL) aims to solve the above problems from a technical viewpoint. In contrast to conventional graph learning research which mainly cares about model performance, TwGL considers various reliability and safety aspects of the graph learning framework including but not limited to robustness, explainability, and privacy. In this survey, we provide a comprehensive review of recent leading approaches in the TwGL \ufb01eld from three dimensions, namely, reliability, explainability, and privacy protection. We give a general categorization for existing work and review typical work for each category. To give further insights for TwGL research, we provide a uni\ufb01ed view to inspect previous works and build the connection between them. We also point out some important open problems remaining to be solved in the future developments of TwGL.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "27055880",
                        "name": "Bingzhe Wu"
                    },
                    {
                        "authorId": "2115953679",
                        "name": "Jintang Li"
                    },
                    {
                        "authorId": "28822585",
                        "name": "Junchi Yu"
                    },
                    {
                        "authorId": "2419616",
                        "name": "Yatao Bian"
                    },
                    {
                        "authorId": "7214272",
                        "name": "Hengtong Zhang"
                    },
                    {
                        "authorId": "2145762399",
                        "name": "Chaochao Chen"
                    },
                    {
                        "authorId": "144549366",
                        "name": "Chengbin Hou"
                    },
                    {
                        "authorId": null,
                        "name": "Guoji Fu"
                    },
                    {
                        "authorId": "1853048147",
                        "name": "Liang Chen"
                    },
                    {
                        "authorId": "1754673",
                        "name": "Tingyang Xu"
                    },
                    {
                        "authorId": "48537464",
                        "name": "Yu Rong"
                    },
                    {
                        "authorId": "1687974",
                        "name": "Xiaolin Zheng"
                    },
                    {
                        "authorId": "1768190",
                        "name": "Junzhou Huang"
                    },
                    {
                        "authorId": "2053865709",
                        "name": "Ran He"
                    },
                    {
                        "authorId": "143905981",
                        "name": "Baoyuan Wu"
                    },
                    {
                        "authorId": "2113638448",
                        "name": "Guangyu Sun"
                    },
                    {
                        "authorId": "2153522384",
                        "name": "Peng Cui"
                    },
                    {
                        "authorId": "144291579",
                        "name": "Zibin Zheng"
                    },
                    {
                        "authorId": "47781621",
                        "name": "Zhe Liu"
                    },
                    {
                        "authorId": "144259957",
                        "name": "P. Zhao"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "GNNExplainer [22] Explainability Perturbation-based Grey-box Instance/Group NC/GC Edge/Feature PGExplainer [56] Explainability Perturbation-based Grey-box Instance NC/GC Edge ZORRO [159] Explainability Perturbation-based Grey-box Instance NC Node/Feature Causal Screening [149] Explainability Perturbation-based Grey-box Instance GC Edge GraphMask [160] Explainability Perturbation-based White-box Instance SRL/MQA Edge SubgraphX [161] Explainability Perturbation-based Black-box Instance NC/GC Subgraph CF-GNNExplainer [162] Explainability Perturbation-based Grey-box Instance NC Edge RCExplainer [136] Explainability Perturbation-based Grey-box Instance NC/GC Edge ReFine [163] Explainability Perturbation-based Grey-box Instance GC Edge CF2 [164] Explainability Perturbation-based Grey-box Instance NC/GC Edge/Feature"
            ],
            "citingPaper": {
                "paperId": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
                "externalIds": {
                    "ArXiv": "2205.07424",
                    "DBLP": "journals/corr/abs-2205-07424",
                    "DOI": "10.48550/arXiv.2205.07424",
                    "CorpusId": 248811191
                },
                "corpusId": 248811191,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/21913eb287f8fc33db8f6274fd2a07072c4e11eb",
                "title": "Trustworthy Graph Neural Networks: Aspects, Methods and Trends",
                "abstract": "Graph neural networks (GNNs) have emerged as a series of competent graph learning methods for diverse real-world scenarios, ranging from daily applications like recommendation systems and question answering to cutting-edge technologies such as drug discovery in life sciences and n-body simulation in astrophysics. However, task performance is not the only requirement for GNNs. Performance-oriented GNNs have exhibited potential adverse effects like vulnerability to adversarial attacks, unexplainable discrimination against disadvantaged groups, or excessive resource consumption in edge computing environments. To avoid these unintentional harms, it is necessary to build competent GNNs characterised by trustworthiness. To this end, we propose a comprehensive roadmap to build trustworthy GNNs from the view of the various computing technologies involved. In this survey, we introduce basic concepts and comprehensively summarise existing efforts for trustworthy GNNs from six aspects, including robustness, explainability, privacy, fairness, accountability, and environmental well-being. Additionally, we highlight the intricate cross-aspect relations between the above six aspects of trustworthy GNNs. Finally, we present a thorough overview of trending directions for facilitating the research and industrialisation of trustworthy GNNs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2156713249",
                        "name": "He Zhang"
                    },
                    {
                        "authorId": "2115265646",
                        "name": "Bang Wu"
                    },
                    {
                        "authorId": "3032058",
                        "name": "Xingliang Yuan"
                    },
                    {
                        "authorId": "2153326034",
                        "name": "Shirui Pan"
                    },
                    {
                        "authorId": "8163721",
                        "name": "Hanghang Tong"
                    },
                    {
                        "authorId": "2112496348",
                        "name": "Jian Pei"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "1b15d1e2cf7d879c64553dddb24901902cb7e5a0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-03612",
                    "ArXiv": "2205.03612",
                    "DOI": "10.48550/arXiv.2205.03612",
                    "CorpusId": 248572361
                },
                "corpusId": 248572361,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1b15d1e2cf7d879c64553dddb24901902cb7e5a0",
                "title": "BrainIB: Interpretable Brain Network-based Psychiatric Diagnosis with Graph Information Bottleneck",
                "abstract": "Developing a new diagnostic models based on the underlying biological mechanisms rather than subjective symptoms for psychiatric disorders is an emerging consensus. Recently, machine learning-based classifiers using functional connectivity (FC) for psychiatric disorders and healthy controls are developed to identify brain markers. However, existing machine learningbased diagnostic models are prone to over-fitting (due to insufficient training samples) and perform poorly in new test environment. Furthermore, it is difficult to obtain explainable and reliable brain biomarkers elucidating the underlying diagnostic decisions. These issues hinder their possible clinical applications. In this work, we propose BrainIB, a new graph neural network (GNN) framework to analyze functional magnetic resonance images (fMRI), by leveraging the famed Information Bottleneck (IB) principle. BrainIB is able to identify the most informative edges in the brain (i.e., subgraph) and generalizes well to unseen data. We evaluate the performance of BrainIB against 8 popular brain network classification methods on two multi-site, largescale datasets and observe that our BrainIB always achieves the highest diagnosis accuracy. It also discovers the subgraph biomarkers which are consistent to clinical and neuroimaging findings.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152804328",
                        "name": "Kaizhong Zheng"
                    },
                    {
                        "authorId": "2462771",
                        "name": "Shujian Yu"
                    },
                    {
                        "authorId": "7480500",
                        "name": "Baojuan Li"
                    },
                    {
                        "authorId": "1747567",
                        "name": "R. Jenssen"
                    },
                    {
                        "authorId": "2108424611",
                        "name": "Badong Chen"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "044c866a4ab50a096864a3507da4d893fbd6fa28",
                "externalIds": {
                    "DBLP": "conf/infocom/ZhuZZGLL22",
                    "DOI": "10.1109/infocomwkshps54753.2022.9798287",
                    "CorpusId": 249900451
                },
                "corpusId": 249900451,
                "publicationVenue": {
                    "id": "be267cb9-6411-4126-8b64-4847025171ee",
                    "name": "Conference on Computer Communications Workshops",
                    "type": "conference",
                    "alternate_names": [
                        "INFOCOM WKSHPS",
                        "Conf Comput Commun Work"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/044c866a4ab50a096864a3507da4d893fbd6fa28",
                "title": "Interpretability Evaluation of Botnet Detection Model based on Graph Neural Network",
                "abstract": "Due to the conspicuous ability to capture topology characteristics, graph neural networks (GNN) have been widely used in botnet detection and proven efficient. However, the blackbox nature of GNN models creates an obstacle for users to trust these classified instruments. In addition to high accuracy, stakeholders also hope that these models are consistent with human cognition. To cope with this problem, we propose a method to evaluate the trustworthiness of GNN-based botnet detection models, called BD-GNNExplainer. Concretely, BD-GNNExplainer extracts the data that contribute the most to GNN\u2019s decision by reducing the loss between the classification results generated by the selected subgraph as the GNN model\u2019s input and the results generated by the entire graph as input. We calculate the relevance between the model-relied data and the informative data to quantify a score expressing interpretability. For different-structure GNN models, these scores will tell us which one is more trustworthy and ultimately become an essential basis for model optimization. To the best of our knowledge, our work is the first time to discuss the interpretability of botnet detection systems and will provide a guideline for making the botnet detection methodology more understandable.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2116876486",
                        "name": "Xiaoli Zhu"
                    },
                    {
                        "authorId": "2144291097",
                        "name": "Yong Zhang"
                    },
                    {
                        "authorId": "2156120015",
                        "name": "Zhao Zhang"
                    },
                    {
                        "authorId": "2169303888",
                        "name": "Da Guo"
                    },
                    {
                        "authorId": "2118912886",
                        "name": "Qi Li"
                    },
                    {
                        "authorId": null,
                        "name": "Zhao Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "ture; (2) masks or attention scores of edges [16], [17], [18], which are derived from the masking functions or attention networks to approximate the target prediction via the fractional (masked or attentive) graph; or (3) prediction changes on perturbed edges [4], [19], [20], which are fetched by per-",
                "Alongside performance, explainability becomes central to the practical impact of GNNs, especially in real-world applications on fairness, security, and robustness [3], [4], [5].",
                "More recently, SubgraphX [4] employs the Monte Carlo tree search algorithm to explore differ-",
                "Prediction changes on structure perturbations [4], [19], [20], [35]: To get the node attributions, PGMExplainer [19] applies random perturbations on nodes and learns a Bayesian network upon the per-",
                "More recently, SubgraphX [4] employs the Monte Carlo tree search algorithm to explore different subgraphs and uses Shapley values to measure each subgraph\u2019s importance."
            ],
            "citingPaper": {
                "paperId": "6958980a4c4946d6575ee0dcbdf4cf38af59294e",
                "externalIds": {
                    "ArXiv": "2204.11028",
                    "DBLP": "journals/corr/abs-2204-11028",
                    "DOI": "10.1109/TPAMI.2022.3170302",
                    "CorpusId": 248376956,
                    "PubMed": "35471869"
                },
                "corpusId": 248376956,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6958980a4c4946d6575ee0dcbdf4cf38af59294e",
                "title": "Reinforced Causal Explainer for Graph Neural Networks",
                "abstract": "Explainability is crucial for probing graph neural networks (GNNs), answering questions like \u201cWhy the GNN model makes a certain prediction?\u201d. Feature attribution is a prevalent technique of highlighting the explanatory subgraph in the input graph, which plausibly leads the GNN model to make its prediction. Various attribution methods have been proposed to exploit gradient-like or attention scores as the attributions of edges, then select the salient edges with top attribution scores as the explanation. However, most of these works make an untenable assumption \u2014 the selected edges are linearly independent \u2014 thus leaving the dependencies among edges largely unexplored, especially their coalition effect. We demonstrate unambiguous drawbacks of this assumption \u2014 making the explanatory subgraph unfaithful and verbose. To address this challenge, we propose a reinforcement learning agent, Reinforced Causal Explainer (RC-Explainer). It frames the explanation task as a sequential decision process \u2014 an explanatory subgraph is successively constructed by adding a salient edge to connect the previously selected subgraph. Technically, its policy network predicts the action of edge addition, and gets a reward that quantifies the action\u2019s causal effect on the prediction. Such reward accounts for the dependency of the newly-added edge and the previously-added edges, thus reflecting whether they collaborate together and form a coalition to pursue better explanations. It is trained via policy gradient to optimize the reward stream of edge sequences. As such, RC-Explainer is able to generate faithful and concise explanations, and has a better generalization power to unseen graphs. When explaining different GNNs on three graph classification datasets, RC-Explainer achieves better or comparable performance to state-of-the-art approaches w.r.t. two quantitative metrics: predictive accuracy, contrastivity, and safely passes sanity checks and visual inspections. Codes and datasets are available at https://github.com/xiangwang1223/reinforced_causal_explainer.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "98285513",
                        "name": "Xiang Wang"
                    },
                    {
                        "authorId": "40507402",
                        "name": "Y. Wu"
                    },
                    {
                        "authorId": "2153659066",
                        "name": "An Zhang"
                    },
                    {
                        "authorId": "2163400298",
                        "name": "Fuli Feng"
                    },
                    {
                        "authorId": "7792071",
                        "name": "Xiangnan He"
                    },
                    {
                        "authorId": "143779329",
                        "name": "Tat-Seng Chua"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "However, given extrinsic drug and gene features and a graph neural network trained on these modern explanation techniques could allow the creation of such post-hoc explanations of the drug-drug interaction predictions [28, 37, 45, 46]."
            ],
            "citingPaper": {
                "paperId": "86860f06aa78ea05d46adbbf93e2da11035108b9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-08206",
                    "ArXiv": "2204.08206",
                    "DOI": "10.48550/arXiv.2204.08206",
                    "CorpusId": 248227584
                },
                "corpusId": 248227584,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/86860f06aa78ea05d46adbbf93e2da11035108b9",
                "title": "TigerLily: Finding drug interactions in silico with the Graph",
                "abstract": "Tigerlily is a TigerGraph based system designed to solve the drug interaction prediction task. In this machine learning task, we want to predict whether two drugs have an adverse interaction. Our framework allows us to solve this highly relevant real-world problem using graph mining techniques in these steps: (a) Using PyTigergraph we create a heterogeneous biological graph of drugs and proteins. (b) We calculate the personalized PageRank scores of drug nodes in the TigerGraph Cloud. (c) We embed the nodes using sparse non-negative matrix factorization of the personalized PageRank matrix. (d) Using the node embeddings we train a gradient boosting based drug interaction predictor.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "35806328",
                        "name": "Benedek Rozemberczki"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "There are several surveys of GNNs in robustness [87, 171, 197, 243] and explainability [221]."
            ],
            "citingPaper": {
                "paperId": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-08570",
                    "ArXiv": "2204.08570",
                    "DOI": "10.48550/arXiv.2204.08570",
                    "CorpusId": 248239981
                },
                "corpusId": 248239981,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
                "title": "A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability",
                "abstract": "Graph Neural Networks (GNNs) have made rapid developments in the recent years. Due to their great ability in modeling graph-structured data, GNNs are vastly used in various applications, including high-stakes scenarios such as financial analysis, traffic predictions, and drug discovery. Despite their great potential in benefiting humans in the real world, recent study shows that GNNs can leak private information, are vulnerable to adversarial attacks, can inherit and magnify societal bias from training data and lack interpretability, which have risk of causing unintentional harm to the users and society. For example, existing works demonstrate that attackers can fool the GNNs to give the outcome they desire with unnoticeable perturbation on training graph. GNNs trained on social networks may embed the discrimination in their decision process, strengthening the undesirable societal bias. Consequently, trustworthy GNNs in various aspects are emerging to prevent the harm from GNN models and increase the users' trust in GNNs. In this paper, we give a comprehensive survey of GNNs in the computational aspects of privacy, robustness, fairness, and explainability. For each aspect, we give the taxonomy of the related methods and formulate the general frameworks for the multiple categories of trustworthy GNNs. We also discuss the future research directions of each aspect and connections between these aspects to help achieve trustworthiness.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152961073",
                        "name": "Enyan Dai"
                    },
                    {
                        "authorId": "1999191869",
                        "name": "Tianxiang Zhao"
                    },
                    {
                        "authorId": "1643792176",
                        "name": "Huaisheng Zhu"
                    },
                    {
                        "authorId": "2150636336",
                        "name": "Jun Xu"
                    },
                    {
                        "authorId": "2149465392",
                        "name": "Zhimeng Guo"
                    },
                    {
                        "authorId": "2146672392",
                        "name": "Hui Liu"
                    },
                    {
                        "authorId": "1736632",
                        "name": "Jiliang Tang"
                    },
                    {
                        "authorId": "2893721",
                        "name": "Suhang Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "e74e1902e19b36f4724f322a2d5f2d3e6ce94891",
                "externalIds": {
                    "ACL": "2022.ltedi-1.12",
                    "DBLP": "journals/corr/abs-2204-08105",
                    "ArXiv": "2204.08105",
                    "DOI": "10.48550/arXiv.2204.08105",
                    "CorpusId": 248227755
                },
                "corpusId": 248227755,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e74e1902e19b36f4724f322a2d5f2d3e6ce94891",
                "title": "Monte Carlo Tree Search for Interpreting Stress in Natural Language",
                "abstract": "Natural language processing can facilitate the analysis of a person\u2019s mental state from text they have written. Previous studies have developed models that can predict whether a person is experiencing a mental health condition from social media posts with high accuracy. Yet, these models cannot explain why the person is experiencing a particular mental state. In this work, we present a new method for explaining a person\u2019s mental state from text using Monte Carlo tree search (MCTS). Our MCTS algorithm employs trained classification models to guide the search for key phrases that explain the writer\u2019s mental state in a concise, interpretable manner. Furthermore, our algorithm can find both explanations that depend on the particular context of the text (e.g., a recent breakup) and those that are context-independent. Using a dataset of Reddit posts that exhibit stress, we demonstrate the ability of our MCTS algorithm to identify interpretable explanations for a person\u2019s feeling of stress in both a context-dependent and context-independent manner.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145564250",
                        "name": "Kyle Swanson"
                    },
                    {
                        "authorId": "2072741114",
                        "name": "Joy Hsu"
                    },
                    {
                        "authorId": "51903517",
                        "name": "Mirac Suzgun"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "9064845595d2fe7dd860c612050e4818a191ff62",
                "externalIds": {
                    "ArXiv": "2204.06127",
                    "DBLP": "journals/tetci/NieCW23",
                    "DOI": "10.1109/TETCI.2022.3222545",
                    "CorpusId": 248405972
                },
                "corpusId": 248405972,
                "publicationVenue": {
                    "id": "544cddb9-1149-469a-8377-d8c34f08d8b1",
                    "name": "IEEE Transactions on Emerging Topics in Computational Intelligence",
                    "alternate_names": [
                        "IEEE Trans Emerg Top Comput Intell"
                    ],
                    "issn": "2471-285X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=7433297",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7433297"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9064845595d2fe7dd860c612050e4818a191ff62",
                "title": "Reinforcement Learning on Graphs: A Survey",
                "abstract": "Graph mining tasks arise from many different application domains, including social networks, biological networks, transportation, and E-commerce, which have been receiving great attention from the theoretical and algorithmic design communities in recent years, and there has been some pioneering work employing the research-rich Reinforcement Learning (RL) techniques to address graph mining tasks. However, these fusion works are dispersed in different research domains, which makes them difficult to compare. In this survey, we provide a comprehensive overview of these fusion works and generalize these works to Graph Reinforcement Learning (GRL) as a unified formulation. We further discuss the applications of GRL methods across various domains, and simultaneously propose the key challenges and advantages of integrating graph mining and RL methods. Furthermore, we propose important directions and challenges to be solved in the future. To our knowledge, this is the latest work on a comprehensive survey of GRL, this work provides a global view and a learning resource for scholars. Based on our review, we create a collection of papers for both interested scholars who want to enter this rapidly developing domain and experts who would like to compare GRL methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2153114137",
                        "name": "Mingshuo Nie"
                    },
                    {
                        "authorId": "3252139",
                        "name": "Dongming Chen"
                    },
                    {
                        "authorId": "2111215516",
                        "name": "Dongqi Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "SubgraphX explores the subgraphs with Monte Carlo tree search and evaluates the importance of the subgraphs with Shapley values [35].",
                "Other prior works, including GNNExplainer [32], PGExplainer [14], PGM-Explainer [25], SubgraphX [35], GraphMask [23], XGNN [34] and others [21] are provided in Appendix A.",
                "Other prior works, including GNNExplainer [32], PGExplainer [14], PGM-Explainer [25], SubgraphX [35], GraphMask [23], XGNN [34] and others [21] are provided in Appendix A.4."
            ],
            "citingPaper": {
                "paperId": "cfd1ba68e0dee2bbf86dd202e79587599939539c",
                "externalIds": {
                    "DBLP": "conf/cvpr/LinL0022",
                    "ArXiv": "2203.15209",
                    "DOI": "10.1109/CVPR52688.2022.01336",
                    "CorpusId": 247778655
                },
                "corpusId": 247778655,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/cfd1ba68e0dee2bbf86dd202e79587599939539c",
                "title": "OrphicX: A Causality-Inspired Latent Variable Model for Interpreting Graph Neural Networks",
                "abstract": "This paper proposes a new eXplanation framework, called OrphicX, for generating causal explanations for any graph neural networks (GNNs) based on learned latent causal factors. Specifically, we construct a distinct generative model and design an objective function that encourages the generative model to produce causal, compact, and faithful explanations. This is achieved by isolating the causal factors in the latent space of graphs by maximizing the information flow measurements. We theoretically analyze the cause-effect relationships in the proposed causal graph, identify node attributes as confounders between graphs and GNN predictions, and circumvent such confounder effect by leveraging the backdoor adjustment formula. Our framework is compatible with any GNNs, and it does not require access to the process by which the target GNN produces its predictions. In addition, it does not rely on the linear-independence assumption of the explained features, nor require prior knowledge on the graph learning tasks. We show a proof-of-concept of OrphicX on canonical classification problems on graph data. In particular, we analyze the explanatory subgraphs obtained from explanations for molecular graphs (i.e., Mutag) and quantitatively evaluate the explanation performance with frequently occurring subgraph patterns. Empirically, we show that OrphicX can effectively identify the causal semantics for generating causal explanations, significantly outperforming its alternatives11This project is supported by the Internal Research Fund at The Hong Kong Polytechnic University P0035763. HW is partially supported by NSF Grant IIS-2127918 and an Amazon Faculty Research Award..",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2526241",
                        "name": "Wanyu Lin"
                    },
                    {
                        "authorId": "2105547066",
                        "name": "Hao Lan"
                    },
                    {
                        "authorId": "39483391",
                        "name": "Hao Wang"
                    },
                    {
                        "authorId": "2145520273",
                        "name": "Baochun Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "In addition, SubgraphX [Yuan et al., 2021] adopts Monte Carlo Tree Search [Silver et al., 2017] to identify important subgraphs.",
                "The drawback of SubgraphX is similar to PGM-Explainer: it can only provide instance-level explanations and cannot be applied to large graphs as the size of search trees increase exponentially.",
                "While some of the explainability methods just output the subgraphs [Vu and Thai, 2020; Yuan et al., 2021; Yuan et al., 2020a], most of them assign an importance score to every edge of the graph; and, to get the important subgraph, thresholding based on these importance scores is needed [Ying et\u2026",
                "In addition, SubgraphX [Yuan et al., 2021] adopts Monte Carlo Tree Search [Silver et al.",
                "Another way is based on perturbation [Luo et al., 2020; Schlichtkrull et al., 2021; Vu and Thai, 2020; Ying et al., 2019; Yuan et al., 2021; Yuan et al., 2020a].",
                "Another way is based on\nperturbation [Luo et al., 2020; Schlichtkrull et al., 2021; Vu and Thai, 2020; Ying et al., 2019; Yuan et al., 2021; Yuan et al., 2020a].",
                "To evaluate the importance of subgraphs, SubgraphX forms a cooperative game with generated subgraphs as players and uses the Shapley value [Kuhn and Tucker, 1953] as the scoring function.",
                "PGM-Explainer and SubgraphX do not scale to big graphs and XGNN can not be applied to the node classification task.",
                "While some of the explainability methods just output the subgraphs [Vu and Thai, 2020; Yuan et al., 2021; Yuan et al., 2020a], most of them assign an importance score to every edge of the graph; and, to get the important subgraph, thresholding based on these importance scores is needed [Ying et al.",
                "SubgraphX [Yuan et al., 2021] explains GNNs on a subgraph level by efficiently exploring different subgraphs with Monte Carlo Tree Search (MCTS).",
                "To be more specific, SubgraphX builds the MCTS by setting the input graph as the root node and each of the other nodes corresponds to a connected subgraph."
            ],
            "citingPaper": {
                "paperId": "394d251832bfd841dccabc449c25ef562845dc53",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-09258",
                    "ArXiv": "2203.09258",
                    "DOI": "10.48550/arXiv.2203.09258",
                    "CorpusId": 247518963
                },
                "corpusId": 247518963,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/394d251832bfd841dccabc449c25ef562845dc53",
                "title": "Explainability in Graph Neural Networks: An Experimental Survey",
                "abstract": "Graph neural networks (GNNs) have been extensively developed for graph representation learning in various application domains. However, similar to all other neural networks models, GNNs suffer from the black-box problem as people cannot understand the mechanism underlying them. To solve this problem, several GNN explainability methods have been proposed to explain the decisions made by GNNs. In this survey, we give an overview of the state-of-the-art GNN explainability methods and how they are evaluated. Furthermore, we propose a new evaluation metric and conduct thorough experiments to compare GNN explainability methods on real world datasets. We also suggest future directions for GNN explainability.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2159314356",
                        "name": "Peibo Li"
                    },
                    {
                        "authorId": "2108586036",
                        "name": "Yixing Yang"
                    },
                    {
                        "authorId": "1783801",
                        "name": "M. Pagnucco"
                    },
                    {
                        "authorId": "2157995570",
                        "name": "Yang Song"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Our method is related to graph explainability in that the predicted transformation probabilities from our augmentation model g is similar to explainability scores of some graph explainability methods (Maruhashi et al., 2018; Yuan et al., 2020; 2021)."
            ],
            "citingPaper": {
                "paperId": "6c76ae1dee7f76f83549a37f59cb4237fa1cac2d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-13248",
                    "ArXiv": "2202.13248",
                    "CorpusId": 247158749
                },
                "corpusId": 247158749,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/6c76ae1dee7f76f83549a37f59cb4237fa1cac2d",
                "title": "Automated Data Augmentations for Graph Classification",
                "abstract": "Data augmentations are effective in improving the invariance of learning machines. We argue that the core challenge of data augmentations lies in designing data transformations that preserve labels. This is relatively straightforward for images, but much more challenging for graphs. In this work, we propose GraphAug, a novel automated data augmentation method aiming at computing label-invariant augmentations for graph classification. Instead of using uniform transformations as in existing studies, GraphAug uses an automated augmentation model to avoid compromising critical label-related information of the graph, thereby producing label-invariant augmentations at most times. To ensure label-invariance, we develop a training method based on reinforcement learning to maximize an estimated label-invariance probability. Experiments show that GraphAug outperforms previous graph augmentation methods on various graph classification tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2004524780",
                        "name": "Youzhi Luo"
                    },
                    {
                        "authorId": "2909646",
                        "name": "Michael McThrow"
                    },
                    {
                        "authorId": "121768478",
                        "name": "Wing Yee Au"
                    },
                    {
                        "authorId": "2057096289",
                        "name": "Tao Komikado"
                    },
                    {
                        "authorId": "2156583777",
                        "name": "Kanji Uchino"
                    },
                    {
                        "authorId": "2197887",
                        "name": "Koji Maruhashi"
                    },
                    {
                        "authorId": "1743600",
                        "name": "Shuiwang Ji"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "However, there are key differences between our approach and SubgraphX: (i) in SubgraphX the explanation corresponds to a set of nodes and its induced subgraph, while GRAPHSHAP computes attribution scores, and therefore relative numerical importance, for an arbitrary set of motifs, that can be mined or user-defined; (ii) our approach is deterministic, while SubgraphX is based on MonteCarlo tree search; and (iii) GRAPHSHAP can assign an importance score to any subgraph, spanning from a single edge to complex patterns, while SubgraphX is limited to the unique subgraph induced by a set of nodes.",
                "In order to correlate this explanation with our motif-based expected explanation scored, for each graph we computed the six Jaccard similarities between the single SubgraphX-produced motif and the six injected motifs.",
                "based [23]\u2013[25], perturbation-based [26]\u2013[29], and surrogate methods [30]\u2013[32].",
                "Similarly to our proposal, SubgraphX [29] considers a connected subgraph (or motif) as more comprehensible explanation than isolated edges/nodes.",
                "SubgraphX return a single set of nodes, so that the produced explanation is the subgraph induced by that set of nodes.",
                "sion of the differences with two relatively similar explainers, GNNExplainer [27] and SubgraphX [29]; we use the same experimental settings of the previous section, results are shown in Figure 3.",
                "However, for the sake of comparison, we include a brief discussion of the differences with two relatively similar explainers, GNNExplainer [27] and SubgraphX [29]; we use the same experimental settings of the previous section, results are shown in Figure 3."
            ],
            "citingPaper": {
                "paperId": "72e41c76bfb8b2c1e42b4a5297a84e7e259debbc",
                "externalIds": {
                    "ArXiv": "2202.08815",
                    "DBLP": "conf/ijcnn/PerottiBBP23",
                    "DOI": "10.1109/IJCNN54540.2023.10191053",
                    "CorpusId": 259375972
                },
                "corpusId": 259375972,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/72e41c76bfb8b2c1e42b4a5297a84e7e259debbc",
                "title": "Explaining Identity-aware Graph Classifiers through the Language of Motifs",
                "abstract": "Most methods for explaining black-box classifiers (e.g., on tabular data, images, or time series) rely on measuring the impact that removing/perturbing features has on the model output. This forces the explanation language to match the classifier's feature space. However, when dealing with graph data, in which the basic features correspond to the edges describing the graph structure, this matching between features space and explanation language might not be appropriate. Decoupling the feature space (edges) from a desired high-lever explanation language (such as motifs) is thus a major challenge towards developing actionable explanations for graph classification tasks. In this paper we introduce Graphshap, a Shapley-based approach able to provide motif-based explanations for identityaware graph classifiers, assuming no knowledge whatsoever about the model or its training data: the only requirement is that the classifier can be queried as a black-box at will. For the sake of computational efficiency we explore a progressive approximation strategy and show how a simple kernel can efficiently approximate explanation scores, thus allowing Graphshap to scale on scenarios with a large explanation space (i.e., large number of motifs). We showcase Graphshap on a real-world brain-network dataset consisting of patients affected by Autism Spectrum Disorder and a control group. Our experiments highlight how the classification provided by a black-box model can be effectively explained by few connectomics patterns.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "26582424",
                        "name": "A. Perotti"
                    },
                    {
                        "authorId": "46726748",
                        "name": "P. Bajardi"
                    },
                    {
                        "authorId": "2179558887",
                        "name": "Francesco Bonchi"
                    },
                    {
                        "authorId": "2735649",
                        "name": "A. Panisson"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "e76317b5191b88d83758be322c026af77492de44",
                "externalIds": {
                    "ArXiv": "2202.08335",
                    "DBLP": "conf/nips/XieKTH0SJ22",
                    "CorpusId": 246904603
                },
                "corpusId": 246904603,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e76317b5191b88d83758be322c026af77492de44",
                "title": "Task-Agnostic Graph Explanations",
                "abstract": "Graph Neural Networks (GNNs) have emerged as powerful tools to encode graph-structured data. Due to their broad applications, there is an increasing need to develop tools to explain how GNNs make decisions given graph-structured data. Existing learning-based GNN explanation approaches are task-specific in training and hence suffer from crucial drawbacks. Specifically, they are incapable of producing explanations for a multitask prediction model with a single explainer. They are also unable to provide explanations in cases where the GNN is trained in a self-supervised manner, and the resulting representations are used in future downstream tasks. To address these limitations, we propose a Task-Agnostic GNN Explainer (TAGE) that is independent of downstream models and trained under self-supervision with no knowledge of downstream tasks. TAGE enables the explanation of GNN embedding models with unseen downstream tasks and allows efficient explanation of multitask models. Our extensive experiments show that TAGE can significantly speed up the explanation efficiency by using the same model to explain predictions for multiple downstream tasks while achieving explanation quality as good as or even better than current state-of-the-art GNN explanation approaches. Our code is pubicly available as part of the DIG library at https://github.com/divelab/DIG/tree/main/dig/xgraph/TAGE/.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "14629242",
                        "name": "Yaochen Xie"
                    },
                    {
                        "authorId": "47617256",
                        "name": "S. Katariya"
                    },
                    {
                        "authorId": "48784944",
                        "name": "Xianfeng Tang"
                    },
                    {
                        "authorId": "2057479333",
                        "name": "E-Wen Huang"
                    },
                    {
                        "authorId": "145850291",
                        "name": "Nikhil S. Rao"
                    },
                    {
                        "authorId": "2691095",
                        "name": "Karthik Subbian"
                    },
                    {
                        "authorId": "1743600",
                        "name": "Shuiwang Ji"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "09c72d9d46f6750e487afdb5f7cae7693ffccc10",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-05594",
                    "ArXiv": "2202.05594",
                    "DOI": "10.24963/ijcai.2022/778",
                    "CorpusId": 246822765
                },
                "corpusId": 246822765,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/09c72d9d46f6750e487afdb5f7cae7693ffccc10",
                "title": "The Shapley Value in Machine Learning",
                "abstract": "Over the last few years, the Shapley value, a solution concept from cooperative game theory, has found numerous applications in machine learning. In this paper, we first discuss fundamental concepts of cooperative game theory and axiomatic properties of the Shapley value. Then we give an overview of the most important applications of the Shapley value in machine learning: feature selection, explainability, multi-agent reinforcement learning, ensemble pruning, and data valuation. We examine the most crucial limitations of the Shapley value and point out directions for future research.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "35806328",
                        "name": "Benedek Rozemberczki"
                    },
                    {
                        "authorId": "2058604454",
                        "name": "Lauren Watson"
                    },
                    {
                        "authorId": "2056355686",
                        "name": "P\u00e9ter Bayer"
                    },
                    {
                        "authorId": "2820299",
                        "name": "Hao-Tsung Yang"
                    },
                    {
                        "authorId": "104031520",
                        "name": "Oliver Kiss"
                    },
                    {
                        "authorId": "2136371907",
                        "name": "Sebastian Nilsson"
                    },
                    {
                        "authorId": "2056781762",
                        "name": "Rik Sarkar"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "SubgraphX produces subgraphs as explanations that are neither motifs nor human-understandable.",
                "The only method that considers subgraphs is SubgraphX [10], which searches all possible subgraphs and identifies the most significant one.",
                "SubgraphX [10] proposed to employ subgraphs for GNN explanation.",
                "However, the subgraphs identified by SubgraphX may not be recurrent or statistically important.",
                "In addition, subgraph-based explainers like SubgraphX need to handle a large searching space, which leads to efficiency issues when generating explanations for dense or large scale graphs.",
                "SubgraphX [10] employs the Monte Carlo Tree Search algorithm to search possible subgraphs and uses the Shapley value to measure the importance of subgraphs and choose a subgraph as the explanation.",
                "Following previous works [7], [8], [9], [10], [12], we focus on instance-level methods with explanations using graph sub-structures.",
                "We compare our MotifExplainer model with several state-of-the-art baselines: GNNExplainer, Sub-\ngraphX, PGExplainer, and ReFine.",
                "Table 10 shows the comparison results with four state-of-the-art GNN explanation models: MotifExplainer, SubgraphX, PGExplainer, GNNExplainer, and ReFine.",
                "Another limitation of SubgraphX is that it needs to pre-determine a maximum number of nodes for its searching space.",
                "SubgraphX [10] is the first work that proposed a method to explain GNN models by generating the most significant subgraph for an input graph."
            ],
            "citingPaper": {
                "paperId": "2d23a740ade86345d53bceda3326952be8659f28",
                "externalIds": {
                    "ArXiv": "2202.00519",
                    "DBLP": "journals/corr/abs-2202-00519",
                    "CorpusId": 246442194
                },
                "corpusId": 246442194,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2d23a740ade86345d53bceda3326952be8659f28",
                "title": "MotifExplainer: a Motif-based Graph Neural Network Explainer",
                "abstract": "We consider the explanation problem of Graph Neural Networks (GNNs). Most existing GNN explanation methods identify the most important edges or nodes but fail to consider substructures, which are more important for graph data. The only method that considers subgraphs tries to search all possible subgraphs and identify the most significant subgraphs. However, the subgraphs identified may not be recurrent or statistically important. In this work, we propose a novel method, known as MotifExplainer, to explain GNNs by identifying important motifs, recurrent and statistically significant patterns in graphs. Our proposed motif-based methods can provide better human-understandable explanations than methods based on nodes, edges, and regular subgraphs. Given an input graph and a pre-trained GNN model, our method first extracts motifs in the graph using well-designed motif extraction rules. Then we generate motif embedding by feeding motifs into the pre-trained GNN. Finally, we employ an attention-based method to identify the most influential motifs as explanations for the final prediction results. The empirical studies on both synthetic and real-world datasets demonstrate the effectiveness of our method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8873470",
                        "name": "Zhaoning Yu"
                    },
                    {
                        "authorId": "3920758",
                        "name": "Hongyang Gao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Some GNN explainers also use motif knowledge to generate subgraphs to explain GNNs (Ying et al., 2019; Yuan et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "b713093906ce80530f1937fc4ab1998a72ce90a6",
                "externalIds": {
                    "ArXiv": "2202.00529",
                    "DBLP": "conf/icml/YuG22",
                    "CorpusId": 249494537
                },
                "corpusId": 249494537,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b713093906ce80530f1937fc4ab1998a72ce90a6",
                "title": "Molecular Representation Learning via Heterogeneous Motif Graph Neural Networks",
                "abstract": "We consider feature representation learning problem of molecular graphs. Graph Neural Networks have been widely used in feature representation learning of molecular graphs. However, most existing methods deal with molecular graphs individually while neglecting their connections, such as motif-level relationships. We propose a novel molecular graph representation learning method by constructing a heterogeneous motif graph to address this issue. In particular, we build a heterogeneous motif graph that contains motif nodes and molecular nodes. Each motif node corresponds to a motif extracted from molecules. Then, we propose a Heterogeneous Motif Graph Neural Network (HM-GNN) to learn feature representations for each node in the heterogeneous motif graph. Our heterogeneous motif graph also enables effective multi-task learning, especially for small molecular datasets. To address the potential efficiency issue, we propose to use an edge sampler, which can significantly reduce computational resources usage. The experimental results show that our model consistently outperforms previous state-of-the-art models. Under multi-task settings, the promising performances of our methods on combined datasets shed light on a new learning paradigm for small molecular datasets. Finally, we show that our model achieves similar performances with significantly less computational resources by using our edge sampler.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8873470",
                        "name": "Zhaoning Yu"
                    },
                    {
                        "authorId": "3920758",
                        "name": "Hongyang Gao"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", 2020a) or Monte Carlo tree search (Yuan et al., 2021).",
                "They adopt either reinforcement learning (Yuan et al., 2020a) or Monte Carlo tree search (Yuan et al., 2021).",
                "Many works have been recently proposed to extract critical data patterns for the prediction by interpreting GNNs in post-hoc ways (Ying et al., 2019; Yuan et al., 2020a; Vu & Thai, 2020; Luo et al., 2020; Schlichtkrull et al., 2021; Yuan et al., 2021; Lin et al., 2021; Henderson et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "68c25a2dcb4df9632996fdcb078ff3bed8300a9c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-12987",
                    "ArXiv": "2201.12987",
                    "CorpusId": 246430773
                },
                "corpusId": 246430773,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/68c25a2dcb4df9632996fdcb078ff3bed8300a9c",
                "title": "Interpretable and Generalizable Graph Learning via Stochastic Attention Mechanism",
                "abstract": "Interpretable graph learning is in need as many scientific applications depend on learning models to collect insights from graph-structured data. Previous works mostly focused on using post-hoc approaches to interpret pre-trained models (graph neural networks in particular). They argue against inherently interpretable models because the good interpretability of these models is often at the cost of their prediction accuracy. However, those post-hoc methods often fail to provide stable interpretation and may extract features that are spuriously correlated with the task. In this work, we address these issues by proposing Graph Stochastic Attention (GSAT). Derived from the information bottleneck principle, GSAT injects stochasticity to the attention weights to block the information from task-irrelevant graph components while learning stochasticity-reduced attention to select task-relevant subgraphs for interpretation. The selected subgraphs provably do not contain patterns that are spuriously correlated with the task under some assumptions. Extensive experiments on eight datasets show that GSAT outperforms the state-of-the-art methods by up to 20%$\\uparrow$ in interpretation AUC and 5%$\\uparrow$ in prediction accuracy. Our code is available at https://github.com/Graph-COM/GSAT.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2151793768",
                        "name": "Siqi Miao"
                    },
                    {
                        "authorId": "2156102035",
                        "name": "Miaoyuan Liu"
                    },
                    {
                        "authorId": "1561672016",
                        "name": "Pan Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Two research lines of rationalization have recently emerged in GNNs. Post-hoc explainability (Ying et al., 2019; Luo et al., 2020; Yuan et al., 2021; Wang et al., 2021c) attributes a model\u2019s prediction to the input graph with a separate explanation method, while intrinsic interpretability\u2026"
            ],
            "citingPaper": {
                "paperId": "bd15a322c20f891f38e247bd5ed6e9d2f0b637eb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-12872",
                    "ArXiv": "2201.12872",
                    "CorpusId": 246431036
                },
                "corpusId": 246431036,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/bd15a322c20f891f38e247bd5ed6e9d2f0b637eb",
                "title": "Discovering Invariant Rationales for Graph Neural Networks",
                "abstract": "Intrinsic interpretability of graph neural networks (GNNs) is to find a small subset of the input graph's features -- rationale -- which guides the model prediction. Unfortunately, the leading rationalization models often rely on data biases, especially shortcut features, to compose rationales and make predictions without probing the critical and causal patterns. Moreover, such data biases easily change outside the training distribution. As a result, these models suffer from a huge drop in interpretability and predictive performance on out-of-distribution data. In this work, we propose a new strategy of discovering invariant rationale (DIR) to construct intrinsically interpretable GNNs. It conducts interventions on the training distribution to create multiple interventional distributions. Then it approaches the causal rationales that are invariant across different distributions while filtering out the spurious patterns that are unstable. Experiments on both synthetic and real-world datasets validate the superiority of our DIR in terms of interpretability and generalization ability on graph classification over the leading baselines. Code and datasets are available at https://github.com/Wuyxin/DIR-GNN.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "10593442",
                        "name": "Yingmin Wu"
                    },
                    {
                        "authorId": "98285513",
                        "name": "Xiang Wang"
                    },
                    {
                        "authorId": "2153659066",
                        "name": "An Zhang"
                    },
                    {
                        "authorId": "7792071",
                        "name": "Xiangnan He"
                    },
                    {
                        "authorId": "143779329",
                        "name": "Tat-Seng Chua"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "result",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Among baselines, SubgraphX gives more reasonable results.",
                "However, it cannot cover two groups of important nodes with a limited budget because it can only select a connected subgraph as the explanation; e.g.\nto cover the negative word \u201clameness\u201d in the lower sentence, SubgraphX needs at least three more nodes along the way, which will significantly decrease Sparsity while including undesirable, neutral words.",
                "While SubgraphX and GraphSVX were shown to perform better than prior alternatives, as we show in Section 3, the Shapley value they try to approximate is non-ideal as it is non-structure-aware.",
                "3, we further evaluate on GIN [40] and GAT [36] on certain datasets following [44].",
                "In fact, GNNExplainer, PGExplainer, and SubgraphX can never generate explanations including only disconnected -",
                "Moreover, SubgraphX is the only baseline that has better H-Fidelity than the \u201cground truth\u201d, but it\n3Carbon rings were also claimed as mutagenic by human experts, but we found it is not discriminative as they exist in both mutagenic and non-mutagenic molecules in MUTAG.\ncan only capture one -NO2 because its search algorithm requires the explanation to be connected, so its Inv-Fidelity is not optimal.",
                "Although SubgraphX and GraphSVX use L-hop subgraphs and thus technically they use the graph structure, such structure usage are very limited in achieving structure-awareness as we show in Appendix G.",
                "The Shapley value has recently been extended to explain GNNs on graphs through feature importance scoring as above, where features are nodes [9] or supernodes [44].",
                "In particular, SubgraphX and GraphSVX use Shapley-value-based scoring functions.",
                "We compare with 5 strong baselines representing the SOTA methods for GNN explanation: GNNExplainer [41], PGExplainer [25], SubgraphX [44], GraphSVX [9], and OrphicX [21].",
                "We follow [44, 43] to employ Fidelity, Inverse Fidelity (Inv-Fidelity), and Sparsity as our evaluation metrics.",
                "SubgraphX [44] uses the Shapley value as its scoring function on subgraphs\n4As some baselines take over 24 hours on full GraphSST2, we randomly select 30 graphs for this analysis.\nselected by Monte Carlo Tree Search (MCTS), and GraphSVX [9] uses a least-square approximation to the Shapley value to score nodes and their features.",
                "SubgraphX [44] uses the Shapley value as its scoring function on subgraphs (4)As some baselines take over 24 hours on full GraphSST2, we randomly select 30 graphs for this analysis.",
                "Following [44], we study the empirical efficiency of GStarX by explaining 50 randomly selected graphs from BBBP.",
                "Our results for the baselines are similar to [44].",
                "We also follow [44] to show the Fidelity vs.",
                "GStarX is not the fastest method, but it is more than two times faster than SubgraphX.",
                "We follow [44] to train GIN on MUTAG and GAT on GraphSST24, and show results in Table 2."
            ],
            "citingPaper": {
                "paperId": "bb608cf0cfef9103f14f7d326be36e999dc88af5",
                "externalIds": {
                    "DBLP": "conf/nips/ZhangLSS22",
                    "ArXiv": "2201.12380",
                    "CorpusId": 248987580
                },
                "corpusId": 248987580,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/bb608cf0cfef9103f14f7d326be36e999dc88af5",
                "title": "GStarX: Explaining Graph Neural Networks with Structure-Aware Cooperative Games",
                "abstract": "Explaining machine learning models is an important and increasingly popular area of research interest. The Shapley value from game theory has been proposed as a prime approach to compute feature importance towards model predictions on images, text, tabular data, and recently graph neural networks (GNNs) on graphs. In this work, we revisit the appropriateness of the Shapley value for GNN explanation, where the task is to identify the most important subgraph and constituent nodes for GNN predictions. We claim that the Shapley value is a non-ideal choice for graph data because it is by definition not structure-aware. We propose a Graph Structure-aware eXplanation (GStarX) method to leverage the critical graph structure information to improve the explanation. Specifically, we define a scoring function based on a new structure-aware value from the cooperative game theory proposed by Hamiache and Navarro (HN). When used to score node importance, the HN value utilizes graph structures to attribute cooperation surplus between neighbor nodes, resembling message passing in GNNs, so that node importance scores reflect not only the node feature importance, but also the node structural roles. We demonstrate that GStarX produces qualitatively more intuitive explanations, and quantitatively improves explanation fidelity over strong baselines on chemical graph property prediction and text graph sentiment classification.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2145408511",
                        "name": "Shichang Zhang"
                    },
                    {
                        "authorId": "152891495",
                        "name": "Yozen Liu"
                    },
                    {
                        "authorId": "145474474",
                        "name": "Neil Shah"
                    },
                    {
                        "authorId": "2109461904",
                        "name": "Yizhou Sun"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "Lately, SubgraphX (Yuan et al., 2021) explores different subgraphs with Monte-Carlo tree search and evaluates subgraphs with the Shapley value (Kuhn & Tucker, 1953)."
            ],
            "citingPaper": {
                "paperId": "d1be97e8d37dc9e1c1a386c6d2d2a7d5b069e28b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-08802",
                    "ArXiv": "2201.08802",
                    "CorpusId": 246210075
                },
                "corpusId": 246210075,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d1be97e8d37dc9e1c1a386c6d2d2a7d5b069e28b",
                "title": "Deconfounding to Explanation Evaluation in Graph Neural Networks",
                "abstract": "Explainability of graph neural networks (GNNs) aims to answer\"Why the GNN made a certain prediction?\", which is crucial to interpret the model prediction. The feature attribution framework distributes a GNN's prediction to its input features (e.g., edges), identifying an influential subgraph as the explanation. When evaluating the explanation (i.e., subgraph importance), a standard way is to audit the model prediction based on the subgraph solely. However, we argue that a distribution shift exists between the full graph and the subgraph, causing the out-of-distribution problem. Furthermore, with an in-depth causal analysis, we find the OOD effect acts as the confounder, which brings spurious associations between the subgraph importance and model prediction, making the evaluation less reliable. In this work, we propose Deconfounded Subgraph Evaluation (DSE) which assesses the causal effect of an explanatory subgraph on the model prediction. While the distribution shift is generally intractable, we employ the front-door adjustment and introduce a surrogate variable of the subgraphs. Specifically, we devise a generative model to generate the plausible surrogates that conform to the data distribution, thus approaching the unbiased estimation of subgraph importance. Empirical results demonstrate the effectiveness of DSE in terms of explanation fidelity.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "10593442",
                        "name": "Yingmin Wu"
                    },
                    {
                        "authorId": "98285513",
                        "name": "Xiang Wang"
                    },
                    {
                        "authorId": "2153659066",
                        "name": "An Zhang"
                    },
                    {
                        "authorId": "2148950326",
                        "name": "Xia Hu"
                    },
                    {
                        "authorId": "2163400298",
                        "name": "Fuli Feng"
                    },
                    {
                        "authorId": "7792071",
                        "name": "Xiangnan He"
                    },
                    {
                        "authorId": "143779329",
                        "name": "Tat-Seng Chua"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "Furthermore, it is worth exploring explainable graph neural network algorithms, such as SubgraphX [48], as it can help researchers analyze and explain the working process of GNNs to detect malware by highlighting suspicious function call paths for automatic malware forensics."
            ],
            "citingPaper": {
                "paperId": "40af04e10af65b9579f77fd27532df6cd7df4e50",
                "externalIds": {
                    "DBLP": "conf/desec/LoLSGP22",
                    "ArXiv": "2201.07537",
                    "DOI": "10.1109/DSC54232.2022.9888878",
                    "CorpusId": 246077274
                },
                "corpusId": 246077274,
                "publicationVenue": {
                    "id": "74f72915-6d78-48b3-85dc-5f79881ab60b",
                    "name": "International Conference on Data Science in Cyberspace",
                    "type": "conference",
                    "alternate_names": [
                        "DSC",
                        "Int Conf Data Sci Sp",
                        "IEEE Conference Dependable and Secure Computing",
                        "IEEE Conference on Dependable and Secure Computing",
                        "IEEE Conf Dependable Secur Comput",
                        "IEEE International Conference on Data Science in Cyberspace",
                        "IEEE Int Conf Data Sci Sp"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/40af04e10af65b9579f77fd27532df6cd7df4e50",
                "title": "Graph Neural Network-based Android Malware Classification with Jumping Knowledge",
                "abstract": "This paper presents a new Android malware de-tection method based on Graph Neural Networks (GNNs) with Jumping-Knowledge (JK). Android function call graphs (FCGs) consist of a set of program functions and their inter-procedural calls. Thus, this paper proposes a GNN-based method for Android malware detection by capturing meaningful intra-procedural call path patterns. In addition, a Jumping-Knowledge technique is applied to minimize the effect of the over-smoothing problem, which is common in GNNs. The proposed method has been extensively evaluated using two benchmark datasets. The results demonstrate the superiority of our approach compared to state-of-the-art approaches in terms of key classification metrics, which demonstrates the potential of GNNs in Android malware detection and classification.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "151253960",
                        "name": "Wai Weng Lo"
                    },
                    {
                        "authorId": "3242014",
                        "name": "S. Layeghy"
                    },
                    {
                        "authorId": "2026368688",
                        "name": "Mohanad Sarhan"
                    },
                    {
                        "authorId": "2150528482",
                        "name": "Marcus Gallagher"
                    },
                    {
                        "authorId": "2105726298",
                        "name": "Marius Portmann"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "We employ the fidelity score to evaluate how the explanation is faithful to the GCN model [55].",
                "SubgraphX [55] explains the prediction of GCN with a subgraph found by Monte Carlo Tree Search."
            ],
            "citingPaper": {
                "paperId": "afe13acf0a5a5c126d0394e09a5a55616d581128",
                "externalIds": {
                    "ArXiv": "2112.09899",
                    "DBLP": "journals/corr/abs-2112-09899",
                    "DOI": "10.1109/CVPR52688.2022.01879",
                    "CorpusId": 245335028
                },
                "corpusId": 245335028,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/afe13acf0a5a5c126d0394e09a5a55616d581128",
                "title": "Improving Subgraph Recognition with Variational Graph Information Bottleneck",
                "abstract": "Subgraph recognition aims at discovering a compressed substructure of a graph that is most informative to the graph property. It can be formulated by optimizing Graph Information Bottleneck (GIB) with a mutual information estimator. However, GIB suffers from training instability and degenerated results due to its intrinsic optimization process. To tackle these issues, we reformulate the subgraph recognition problem into two steps: graph perturbation and subgraph selection, leading to a novel Variational Graph Information Bottleneck (VGIB) framework. VGIB first employs the noise injection to modulate the information flow from the input graph to the perturbed graph. Then, the perturbed graph is encouraged to be informative to the graph property. VGIB further obtains the desired subgraph by filtering out the noise in the perturbed graph. With the customized noise prior for each input, the VGIB objective is endowed with a tractable variational upper bound, leading to a superior empirical performance as well as theoretical properties. Extensive experiments on graph interpretation, explainability of Graph Neural Networks, and graph classification show that VGIB finds better subgraphs than existing methods11Code is avaliable on https://github.com/Samyu0304/VGIB.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "28822585",
                        "name": "Junchi Yu"
                    },
                    {
                        "authorId": "2109811424",
                        "name": "Jie Cao"
                    },
                    {
                        "authorId": "2053866626",
                        "name": "Ran He"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Fidelity: The fidelity scores evaluate how the explanations are faithful to the GNN model [39].",
                "The discrete and combinatorial nature of graph-structured data hinders the development of GNNs\u2019 explanation models [39, 36].",
                "Different from the practice in GNNEXPLAINER [35], we do not mask node features since we only highlight the critical topology of the biological graph to GNN\u2019s prediction, which is more intuitive and human-intelligible [39].",
                "The explainers such as SubgraphX [39] and XGNN [38] formulate the generation of explanations as a reinforcement learning task."
            ],
            "citingPaper": {
                "paperId": "170ce0eebe1c6e65ecf70f2ded7864f6d4428f1f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-09895",
                    "ArXiv": "2112.09895",
                    "CorpusId": 245334653
                },
                "corpusId": 245334653,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/170ce0eebe1c6e65ecf70f2ded7864f6d4428f1f",
                "title": "Towards the Explanation of Graph Neural Networks in Digital Pathology with Information Flows",
                "abstract": "As Graph Neural Networks (GNNs) are widely adopted in digital pathology, there is increasing attention to developing explanation models (explainers) of GNNs for improved transparency in clinical decisions. Existing explainers discover an explanatory subgraph relevant to the prediction. However, such a subgraph is insufficient to reveal all the critical biological substructures for the prediction because the prediction will remain unchanged after removing that subgraph. Hence, an explanatory subgraph should be not only necessary for prediction, but also sufficient to uncover the most predictive regions for the explanation. Such explanation requires a measurement of information transferred from different input subgraphs to the predictive output, which we define as information flow. In this work, we address these key challenges and propose IFEXPLAINER, which generates a necessary and sufficient explanation for GNNs. To evaluate the information flow within GNN's prediction, we first propose a novel notion of predictiveness, named $f$-information, which is directional and incorporates the realistic capacity of the GNN model. Based on it, IFEXPLAINER generates the explanatory subgraph with maximal information flow to the prediction. Meanwhile, it minimizes the information flow from the input to the predictive result after removing the explanation. Thus, the produced explanation is necessarily important to the prediction and sufficient to reveal the most crucial substructures. We evaluate IFEXPLAINER to interpret GNN's predictions on breast cancer subtyping. Experimental results on the BRACS dataset show the superior performance of the proposed method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "28822585",
                        "name": "Junchi Yu"
                    },
                    {
                        "authorId": "1754673",
                        "name": "Tingyang Xu"
                    },
                    {
                        "authorId": "2053865709",
                        "name": "Ran He"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026gradients/features-based methods (Baldassarre and Azizpour 2019; Pope et al. 2019), perturbationbased methods (Ying et al. 2019; Luo et al. 2020; Yuan et al. 2021; Schlichtkrull, De Cao, and Titov 2020), decomposition methods (Schwarzenberg et al. 2019; Schnake et al. 2020), and surrogate\u2026",
                "2019), perturbationbased methods (Ying et al. 2019; Luo et al. 2020; Yuan et al. 2021; Schlichtkrull, De Cao, and Titov 2020), decomposition methods (Schwarzenberg et al."
            ],
            "citingPaper": {
                "paperId": "7de413da6e0a00e14270cfaed2a31666e7c28747",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-00911",
                    "ArXiv": "2112.00911",
                    "DOI": "10.1609/aaai.v36i8.20898",
                    "CorpusId": 244798623
                },
                "corpusId": 244798623,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/7de413da6e0a00e14270cfaed2a31666e7c28747",
                "title": "ProtGNN: Towards Self-Explaining Graph Neural Networks",
                "abstract": "Despite the recent progress in Graph Neural Networks (GNNs), it remains challenging to explain the predictions\n made by GNNs. Existing explanation methods mainly focus on post-hoc explanations where another explanatory model is employed to provide explanations for a trained GNN. The fact that post-hoc methods fail to reveal the original reasoning process of GNNs raises the need of building GNNs with built-in interpretability. In this work, we propose Prototype Graph Neural Network (ProtGNN), which combines prototype learning with GNNs and provides a new perspective on the explanations of GNNs. In ProtGNN, the explanations are naturally derived from the case-based reasoning process and are actually used during classification. The prediction of ProtGNN is obtained by comparing the inputs to a few learned prototypes in the latent space.\n Furthermore, for better interpretability and higher efficiency, a novel conditional subgraph sampling module is incorporated to indicate which part of the input graph is most similar to each prototype in ProtGNN+. Finally, we evaluate our method on a wide range of datasets and perform concrete case studies. Extensive results show that ProtGNN and ProtGNN+ can provide inherent interpretability while achieving accuracy on par with the non-interpretable counterparts.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2129455190",
                        "name": "Zaixin Zhang"
                    },
                    {
                        "authorId": "2144831836",
                        "name": "Qi Liu"
                    },
                    {
                        "authorId": "2144219662",
                        "name": "Hao Wang"
                    },
                    {
                        "authorId": "46655401",
                        "name": "Chengqiang Lu"
                    },
                    {
                        "authorId": "153897134",
                        "name": "Chee-Kong Lee"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In [54], Monte Carlo search is used for exploration.",
                "Stochastic explaining subgraph search have been proposed [53], [46], [54] using reinforcement learning and hill-climbing."
            ],
            "citingPaper": {
                "paperId": "50cfdcfc5b2cdf21d4e7ca9cdd9b74a426fa4671",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-14651",
                    "ArXiv": "2111.14651",
                    "DOI": "10.1109/ICDM51629.2021.00052",
                    "CorpusId": 244714677
                },
                "corpusId": 244714677,
                "publicationVenue": {
                    "id": "67d15a94-d523-4b5f-be58-03fe2ef9dcfb",
                    "name": "Industrial Conference on Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Ind Conf Data Min",
                        "ICDM"
                    ],
                    "url": "http://www.data-mining-forum.de/"
                },
                "url": "https://www.semanticscholar.org/paper/50cfdcfc5b2cdf21d4e7ca9cdd9b74a426fa4671",
                "title": "Multi-objective Explanations of GNN Predictions",
                "abstract": "Graph Neural Network (GNN) has achieved state-of-the-art performance in various high-stake prediction tasks, but multiple layers of aggregations on graphs with irregular structures make GNN a less interpretable model. Prior methods use simpler subgraphs to simulate the full model, or counterfactuals to identify the causes of a prediction. The two families of approaches aim at two distinct objectives, \u201csimulatability\u201d and \u201ccounterfactual relevance\u201d, but it is not clear how the objectives can jointly influence the human understanding of an explanation. We design a user-study to investigate such joint effects, and use the findings to design a multi-objective optimization (MOO) algorithm to find Pareto optimal explanations that are well-balanced in simulatability and counterfactual. Since the target model can be of any GNN variants and may not be accessible due to privacy concerns, we design a search algorithm using zero-th order information without accessing the architecture and parameters of the target model. Quantitative experiments on nine graphs from four applications demonstrate that the Pareto efficient explanations dominate single-objective baselines that use first-order continuous optimization or discrete combinatorial search. The explanations are further evaluated in robustness and sensitivity to show their capability of revealing convincing causes, while being cautious about the possible confounders. The diverse dominating counterfactuals can certify the feasibility of algorithmic recourse, that can potentially promote algorithmic fairness where humans are participating in the decision-making using GNN.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108101960",
                        "name": "Yifei Liu"
                    },
                    {
                        "authorId": "2145762275",
                        "name": "Chao Chen"
                    },
                    {
                        "authorId": "2144399347",
                        "name": "Yazheng Liu"
                    },
                    {
                        "authorId": "47957054",
                        "name": "Xi Zhang"
                    },
                    {
                        "authorId": "3131378",
                        "name": "Sihong Xie"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "a63fa4b6d85c5bb1037666a5877befb340b0591d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-12984",
                    "ArXiv": "2111.12984",
                    "DOI": "10.1007/978-3-030-93736-2_6",
                    "CorpusId": 244708879
                },
                "corpusId": 244708879,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a63fa4b6d85c5bb1037666a5877befb340b0591d",
                "title": "Demystifying Graph Neural Network Explanations",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1992914162",
                        "name": "Anna Himmelhuber"
                    },
                    {
                        "authorId": "2020545",
                        "name": "Mitchell Joblin"
                    },
                    {
                        "authorId": "2599096",
                        "name": "Martin Ringsquandl"
                    },
                    {
                        "authorId": "1727058",
                        "name": "T. Runkler"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Techniques also exist to extract explainable substructures of graphs that are associated with certain outcomes (Yuan et al. 2021)."
            ],
            "citingPaper": {
                "paperId": "38d69761538105bd1ef4e8f51b19aa96d7194f4a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-09231",
                    "ArXiv": "2110.09231",
                    "CorpusId": 239016116
                },
                "corpusId": 239016116,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/38d69761538105bd1ef4e8f51b19aa96d7194f4a",
                "title": "Machine Learning Featurizations for AI Hacking of Political Systems",
                "abstract": "What would the inputs be to a machine whose output is the destabilization of a robust democracy, or whose emanations could disrupt the political power of nations? In the recent essay\"The Coming AI Hackers,\"Schneier (2021) proposed a future application of artificial intelligences to discover, manipulate, and exploit vulnerabilities of social, economic, and political systems at speeds far greater than humans' ability to recognize and respond to such threats. This work advances the concept by applying to it theory from machine learning, hypothesizing some possible\"featurization\"(input specification and transformation) frameworks for AI hacking. Focusing on the political domain, we develop graph and sequence data representations that would enable the application of a range of deep learning models to predict attributes and outcomes of political, particularly legislative, systems. We explore possible data models, datasets, predictive tasks, and actionable applications associated with each framework. We speculate about the likely practical impact and feasibility of such models, and conclude by discussing their ethical implications.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2058314353",
                        "name": "Nathan Sanders"
                    },
                    {
                        "authorId": "1696985",
                        "name": "B. Schneier"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[109] show how GNNs\u2019 decisions can be explained by (often large) subgraphs, further motivating our use of graph reconstruction as a powerful inductive bias for GRL."
            ],
            "citingPaper": {
                "paperId": "e3bf755eefb9c21b3ebe1a57dcbc3cf9d5041e17",
                "externalIds": {
                    "ArXiv": "2110.00577",
                    "DBLP": "journals/corr/abs-2110-00577",
                    "CorpusId": 238253248
                },
                "corpusId": 238253248,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e3bf755eefb9c21b3ebe1a57dcbc3cf9d5041e17",
                "title": "Reconstruction for Powerful Graph Representations",
                "abstract": "Graph neural networks (GNNs) have limited expressive power, failing to represent many graph classes correctly. While more expressive graph representation learning (GRL) alternatives can distinguish some of these classes, they are significantly harder to implement, may not scale well, and have not been shown to outperform well-tuned GNNs in real-world tasks. Thus, devising simple, scalable, and expressive GRL architectures that also achieve real-world improvements remains an open challenge. In this work, we show the extent to which graph reconstruction -- reconstructing a graph from its subgraphs -- can mitigate the theoretical and practical problems currently faced by GRL architectures. First, we leverage graph reconstruction to build two new classes of expressive graph representations. Secondly, we show how graph reconstruction boosts the expressive power of any GNN architecture while being a (provably) powerful inductive bias for invariances to vertex removals. Empirically, we show how reconstruction can boost GNN's expressive power -- while maintaining its invariance to permutations of the vertices -- by solving seven graph property tasks not solvable by the original GNN. Further, we demonstrate how it boosts state-of-the-art GNN's performance across nine real-world benchmark datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "113902542",
                        "name": "Leonardo Cotta"
                    },
                    {
                        "authorId": "143622465",
                        "name": "Christopher Morris"
                    },
                    {
                        "authorId": "145617731",
                        "name": "Bruno Ribeiro"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                ", GNNExplainer [25], PGExplainer [17] and SubgraphX [28].",
                "Explanation methods can be broadly categorized as model-level explainers [17], [20], [26], which try to extract global explanatory patterns from the trained model, and instance-level algorithms [5], [12], [16], [25], [28], which try to explain individual predictions performed by the model.",
                "SubgraphX [28] explains its predictions by exploring different subgraphs with Monte Carlo tree search.",
                "Concerning the explainers we use GNNExplainer [25], PGExplainer [17] and SubgraphX [28].",
                "For SubgraphX we used the hyperparameters of the original implementation [28]."
            ],
            "citingPaper": {
                "paperId": "13a2ee8292908a8064abc2f43835a8dd5b11d695",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-09426",
                    "ArXiv": "2109.09426",
                    "DOI": "10.1109/TNNLS.2022.3171398",
                    "CorpusId": 237571970,
                    "PubMed": "35544494"
                },
                "corpusId": 237571970,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/13a2ee8292908a8064abc2f43835a8dd5b11d695",
                "title": "A Meta-Learning Approach for Training Explainable Graph Neural Networks",
                "abstract": "In this article, we investigate the degree of explainability of graph neural networks (GNNs). The existing explainers work by finding global/local subgraphs to explain a prediction, but they are applied after a GNN has already been trained. Here, we propose a meta-explainer for improving the level of explainability of a GNN directly at training time, by steering the optimization procedure toward minima that allow post hoc explainers to achieve better results, without sacrificing the overall accuracy of GNN. Our framework (called MATE, MetA-Train to Explain) jointly trains a model to solve the original task, e.g., node classification, and to provide easily processable outputs for downstream algorithms that explain the model's decisions in a human-friendly way. In particular, we meta-train the model's parameters to quickly minimize the error of an instance-level GNNExplainer trained on-the-fly on randomly sampled nodes. The final internal representation relies on a set of features that can be ``better'' understood by an explanation algorithm, e.g., another instance of GNNExplainer. Our model-agnostic approach can improve the explanations produced for different GNN architectures and use any instance-based explainer to drive this process. Experiments on synthetic and real-world datasets for node and graph classification show that we can produce models that are consistently easier to explain by different algorithms. Furthermore, this increase in explainability comes at no cost to the accuracy of the model.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "115566972",
                        "name": "Indro Spinelli"
                    },
                    {
                        "authorId": "1752983",
                        "name": "Simone Scardapane"
                    },
                    {
                        "authorId": "1737292",
                        "name": "A. Uncini"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "e92fde4731f996d69abab06c9a2078513e4e11d0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-03388",
                    "ArXiv": "2108.03388",
                    "DOI": "10.1109/ICDE55515.2023.00056",
                    "CorpusId": 236956812
                },
                "corpusId": 236956812,
                "publicationVenue": {
                    "id": "764e3630-ddac-4c21-af4b-9d32ffef082e",
                    "name": "IEEE International Conference on Data Engineering",
                    "type": "conference",
                    "alternate_names": [
                        "ICDE",
                        "Int Conf Data Eng",
                        "IEEE Int Conf Data Eng",
                        "International Conference on Data Engineering"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1331"
                },
                "url": "https://www.semanticscholar.org/paper/e92fde4731f996d69abab06c9a2078513e4e11d0",
                "title": "Jointly Attacking Graph Neural Network and its Explanations",
                "abstract": "Graph Neural Networks (GNNs) have boosted the performance for many graph-related tasks. Despite the great success, recent studies have shown that GNNs are still vulnerable to adversarial attacks, where adversaries can mislead the GNNs' prediction by modifying graphs. On the other hand, the explanation of GNNs (GnnExplainer for short) provides a better understanding of a trained GNN model by generating a small subgraph and features that are most influential for its prediction. In this paper, we first perform empirical studies to validate that GnnExplainer can act as an inspection tool and have the potential to detect the adversarial perturbations for graphs. This finding motivates us to further investigate a new problem: Whether a graph neural network and its explanations can be jointly attacked by modifying graphs with malicious desires? It is challenging to answer this question since the goals of adversarial attack and bypassing the GnnExplainer essentially contradict with each other. In this work, we give a confirmative answer for this question by proposing a novel attack framework (GEAttack) for graphs, which can attack both a GNN model and its explanations by exploiting their vulnerabilities simultaneously. To the best of our knowledge, this is the very first effort to attack both GNNs and explanations on graph-structured data for the trustworthiness of GNNs. Comprehensive experiments on various real-world datasets demonstrate the effectiveness of the proposed method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "41031455",
                        "name": "Wenqi Fan"
                    },
                    {
                        "authorId": "144767914",
                        "name": "Wei Jin"
                    },
                    {
                        "authorId": "2124928119",
                        "name": "Xiaorui Liu"
                    },
                    {
                        "authorId": "2018756699",
                        "name": "Han Xu"
                    },
                    {
                        "authorId": "48784944",
                        "name": "Xianfeng Tang"
                    },
                    {
                        "authorId": "2116430057",
                        "name": "Suhang Wang"
                    },
                    {
                        "authorId": "2117897052",
                        "name": "Qing Li"
                    },
                    {
                        "authorId": "1736632",
                        "name": "Jiliang Tang"
                    },
                    {
                        "authorId": "2110325165",
                        "name": "Jianping Wang"
                    },
                    {
                        "authorId": "1682418",
                        "name": "C. Aggarwal"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "f7231aee1e18428d6c0b314b5e1e65d6707e8747",
                "externalIds": {
                    "ArXiv": "2107.04086",
                    "DBLP": "journals/corr/abs-2107-04086",
                    "CorpusId": 235790538
                },
                "corpusId": 235790538,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f7231aee1e18428d6c0b314b5e1e65d6707e8747",
                "title": "Robust Counterfactual Explanations on Graph Neural Networks",
                "abstract": "Massive deployment of Graph Neural Networks (GNNs) in high-stake applications generates a strong demand for explanations that are robust to noise and align well with human intuition. Most existing methods generate explanations by identifying a subgraph of an input graph that has a strong correlation with the prediction. These explanations are not robust to noise because independently optimizing the correlation for a single input can easily overfit noise. Moreover, they do not align well with human intuition because removing an identified subgraph from an input graph does not necessarily change the prediction result. In this paper, we propose a novel method to generate robust counterfactual explanations on GNNs by explicitly modelling the common decision logic of GNNs on similar input graphs. Our explanations are naturally robust to noise because they are produced from the common decision boundaries of a GNN that govern the predictions of many similar input graphs. The explanations also align well with human intuition because removing the set of edges identified by an explanation from the input graph changes the prediction significantly. Exhaustive experiments on many public datasets demonstrate the superior performance of our method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2065738416",
                        "name": "Mohit Bajaj"
                    },
                    {
                        "authorId": "2074100",
                        "name": "Lingyang Chu"
                    },
                    {
                        "authorId": "2060445410",
                        "name": "Zihui Xue"
                    },
                    {
                        "authorId": "145525190",
                        "name": "J. Pei"
                    },
                    {
                        "authorId": "49680751",
                        "name": "Lanjun Wang"
                    },
                    {
                        "authorId": "23033976",
                        "name": "P. C. Lam"
                    },
                    {
                        "authorId": "2144288655",
                        "name": "Yong Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Other methods such as SubgraphX [151] provide subgraph-level explanations which may be more intuitive and human-intelligible for digital pathology.",
                "Other methods such as SubgraphX [119] provide subgraph-level explanations which could be more intuitive and human-intelligible for digital pathology."
            ],
            "citingPaper": {
                "paperId": "70727c82f1ab97b3f64ee3e81e6e209c40fa0a02",
                "externalIds": {
                    "ArXiv": "2107.00272",
                    "DBLP": "journals/corr/abs-2107-00272",
                    "DOI": "10.1016/j.compmedimag.2021.102027",
                    "CorpusId": 235694722,
                    "PubMed": "34959100"
                },
                "corpusId": 235694722,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/70727c82f1ab97b3f64ee3e81e6e209c40fa0a02",
                "title": "A Survey on Graph-Based Deep Learning for Computational Histopathology",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1404441879",
                        "name": "David Ahmedt-Aristizabal"
                    },
                    {
                        "authorId": "2179032",
                        "name": "M. Armin"
                    },
                    {
                        "authorId": "1980700",
                        "name": "S. Denman"
                    },
                    {
                        "authorId": "3140440",
                        "name": "C. Fookes"
                    },
                    {
                        "authorId": "47773335",
                        "name": "L. Petersson"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                ", 2020), (Vu & Thai, 2020), (Yuan et al., 2021), (Schlichtkrull et al.",
                "Similar visualizations can be obtained from other GNN explainability techniques like (Ying et al., 2019), (Yuan et al., 2020), (Vu & Thai, 2020), (Yuan et al., 2021), (Schlichtkrull et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "1102867bbe6c16592b90ab76871346647435920a",
                "externalIds": {
                    "ArXiv": "2106.12665",
                    "DBLP": "journals/corr/abs-2106-12665",
                    "CorpusId": 235623767
                },
                "corpusId": 235623767,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1102867bbe6c16592b90ab76871346647435920a",
                "title": "Reimagining GNN Explanations with ideas from Tabular Data",
                "abstract": "Explainability techniques for Graph Neural Networks still have a long way to go compared to explanations available for both neural and decision decision tree-based models trained on tabular data. Using a task that straddles both graphs and tabular data, namely Entity Matching, we comment on key aspects of explainability that are missing in GNN model explanations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2120295059",
                        "name": "Anjali Singh"
                    },
                    {
                        "authorId": "1713772257",
                        "name": "K. ShamanthRNayak"
                    },
                    {
                        "authorId": "27526892",
                        "name": "Balaji Ganesan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                ", 2020), (Vu & Thai, 2020), (Yuan et al., 2021), (Schlichtkrull et al.",
                "Recent works in GNN explanations include (Ying et al., 2019), (Yuan et al., 2020), (Vu & Thai, 2020), (Yuan et al., 2021), (Schlichtkrull et al., 2020).",
                "Although SubgraphX assures to offer connected subgraphs as explanation for every single input, this approach is highly sensitive to actions being considered while looking for a path using Monte-Carlo Tree Search (MCTS) being utilized for SubgraphX method.",
                "The most recent subgraph-based approach is SubgraphX (Yuan et al., 2021), which aims to explore different subgraphs and identify only those that mostly impact the final predictions.",
                "This is because different actions cause SubgraphX to output different explanations."
            ],
            "citingPaper": {
                "paperId": "89f054ea7277f2bc4605f4907b9b8d40b2d8f0e3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-11864",
                    "ArXiv": "2106.11864",
                    "CorpusId": 235593071
                },
                "corpusId": 235593071,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/89f054ea7277f2bc4605f4907b9b8d40b2d8f0e3",
                "title": "Towards Automated Evaluation of Explanations in Graph Neural Networks",
                "abstract": "Explaining Graph Neural Networks predictions to end users of AI applications in easily understandable terms remains an unsolved problem. In particular, we do not have well developed methods for automatically evaluating explanations, in ways that are closer to how users consume those explanations. Based on recent application trends and our own experiences in real world problems, we propose automatic evaluation approaches for GNN Explanations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2114429785",
                        "name": "Bannihati Kumar Vanya"
                    },
                    {
                        "authorId": "27526892",
                        "name": "Balaji Ganesan"
                    },
                    {
                        "authorId": "2114800291",
                        "name": "Aniket Saxena"
                    },
                    {
                        "authorId": "2118804097",
                        "name": "Devbrat Sharma"
                    },
                    {
                        "authorId": "2078528713",
                        "name": "Arvind Agarwal"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "eb5e05ea3d92ac8b5d52ae7ed50c7406d0e6ced1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-09111",
                    "ArXiv": "2106.09111",
                    "CorpusId": 235458379
                },
                "corpusId": 235458379,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/eb5e05ea3d92ac8b5d52ae7ed50c7406d0e6ced1",
                "title": "An Imprecise SHAP as a Tool for Explaining the Class Probability Distributions under Limited Training Data",
                "abstract": "One of the most popular methods of the machine learning prediction explanation is the SHapley Additive exPlanations method (SHAP). An imprecise SHAP as a modification of the original SHAP is proposed for cases when the class probability distributions are imprecise and represented by sets of distributions. The first idea behind the imprecise SHAP is a new approach for computing the marginal contribution of a feature, which fulfils the important efficiency property of Shapley values. The second idea is an attempt to consider a general approach to calculating and reducing interval-valued Shapley values, which is similar to the idea of reachable probability intervals in the imprecise probability theory. A simple special implementation of the general approach in the form of linear optimization problems is proposed, which is based on using the Kolmogorov-Smirnov distance and imprecise contamination models. Numerical examples with synthetic and real data illustrate the imprecise SHAP.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145528703",
                        "name": "L. Utkin"
                    },
                    {
                        "authorId": "50585467",
                        "name": "A. Konstantinov"
                    },
                    {
                        "authorId": "2113246513",
                        "name": "Kirill Vishniakov"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "SubgraphX retrieves explanations similar to Grad, but has the drawback of very high runtime, see Appendix B, available in the online supplemental material.",
                "We could only run SubgraphX for the small synthetic dataset, due to its long runtime (see Appendix B), available in the online supplementalmaterial.",
                "Explainability approaches for explaining node level decisions include soft-masking approaches [11], [22], [24], [31], [32], [44], Shapely based approaches [8], [48], surrogate",
                "For a comprehensive quantitative evaluation we chose our baselines from the three different categories of post-hoc explanations models consisting of (i) soft-masking approaches like GNNExplainer, which returns a continuous feature and edge mask and PGE [24] learns soft masks over edges in the graph (ii) surrogate model based hard-masking approach, PGM [40], which returns a binary node mask, iii) Shapely based hard masking approach SubgraphX [48], which returns a subgraph as an explanation, and (iv) gradient-basedmethodsGrad&GradInput [34] which utilize gradients to compute feature attribu-",
                "SubgraphX [48] optimizes for Shapely values based on a Monte Carlo tree search.",
                "Others adopt existing explanations approaches such as Shapely [8], [48], layer-wise relevance propagation [32], causal effects [22] or LIME [13], [17], to graph data.",
                "SubgraphX [48] optimizes for Shapely values based on",
                "For a comprehensive quantitative evaluation we chose our baselines from the three different categories of post-hoc explanations models consisting of (i) soft-masking approaches like GNNExplainer, which returns a continuous feature and edge mask and PGE [24] learns soft masks over edges in the graph (ii) surrogate model based hard-masking approach, PGM [40], which returns a binary node mask, iii) Shapely based hard masking approach SubgraphX [48], which returns a subgraph as an explanation, and (iv) gradient-basedmethodsGrad&GradInput [34] which utilize gradients to compute feature attributions."
            ],
            "citingPaper": {
                "paperId": "7c85ad5f11ef9afb4f568c33304d86105da956ce",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-08621",
                    "ArXiv": "2105.08621",
                    "DOI": "10.1109/TKDE.2022.3201170",
                    "CorpusId": 234762791
                },
                "corpusId": 234762791,
                "publicationVenue": {
                    "id": "c6840156-ee10-4d78-8832-7f8909811576",
                    "name": "IEEE Transactions on Knowledge and Data Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Knowl Data Eng"
                    ],
                    "issn": "1041-4347",
                    "url": "https://www.computer.org/web/tkde",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=69"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7c85ad5f11ef9afb4f568c33304d86105da956ce",
                "title": "Zorro: Valid, Sparse, and Stable Explanations in Graph Neural Networks",
                "abstract": "With the ever-increasing popularity and applications of graph neural networks, several proposals have been made to explain and understand the decisions of a graph neural network. Explanations for graph neural networks differ in principle from other input settings. It is important to attribute the decision to input features and other related instances connected by the graph structure. We find that the previous explanation generation approaches that maximize the mutual information between the label distribution produced by the model and the explanation to be restrictive. Specifically, existing approaches do not enforce explanations to be valid, sparse, or robust to input perturbations. In this paper, we lay down some of the fundamental principles that an explanation method for graph neural networks should follow and introduce a metric RDT-Fidelity as a measure of the explanation's effectiveness. We propose a novel approach Zorro based on the principles from rate-distortion theory that uses a simple combinatorial procedure to optimize for RDT-Fidelity. Extensive experiments on real and synthetic datasets reveal that Zorro produces sparser, stable, and more faithful explanations than existing graph neural network explanation approaches.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "143923185",
                        "name": "Thorben Funke"
                    },
                    {
                        "authorId": "35070805",
                        "name": "Megha Khosla"
                    },
                    {
                        "authorId": "39775488",
                        "name": "Avishek Anand"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "323ca85c5c1ebd0ec9bf74897d8c8e8fbf203ae6",
                "externalIds": {
                    "ArXiv": "2103.12608",
                    "DBLP": "journals/jmlr/LiuLWXYGYXZLYLF21",
                    "CorpusId": 232320529
                },
                "corpusId": 232320529,
                "publicationVenue": {
                    "id": "c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                    "name": "Journal of machine learning research",
                    "type": "journal",
                    "alternate_names": [
                        "Journal of Machine Learning Research",
                        "J mach learn res",
                        "J Mach Learn Res"
                    ],
                    "issn": "1532-4435",
                    "alternate_issns": [
                        "1533-7928"
                    ],
                    "url": "http://www.ai.mit.edu/projects/jmlr/",
                    "alternate_urls": [
                        "http://jmlr.csail.mit.edu/",
                        "http://www.jmlr.org/",
                        "http://portal.acm.org/affiliated/jmlr"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/323ca85c5c1ebd0ec9bf74897d8c8e8fbf203ae6",
                "title": "DIG: A Turnkey Library for Diving into Graph Deep Learning Research",
                "abstract": "Although there exist several libraries for deep learning on graphs, they are aiming at implementing basic operations for graph deep learning. In the research community, implementing and benchmarking various advanced tasks are still painful and time-consuming with existing libraries. To facilitate graph deep learning research, we introduce DIG: Dive into Graphs, a turnkey library that provides a unified testbed for higher level, research-oriented graph deep learning tasks. Currently, we consider graph generation, self-supervised learning on graphs, explainability of graph neural networks, and deep learning on 3D graphs. For each direction, we provide unified implementations of data interfaces, common algorithms, and evaluation metrics. Altogether, DIG is an extensible, open-source, and turnkey library for researchers to develop new methods and effortlessly compare with common baselines using widely used datasets and evaluation metrics. Source code is available at https://github.com/divelab/DIG.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "38813990",
                        "name": "Meng Liu"
                    },
                    {
                        "authorId": "2004524780",
                        "name": "Youzhi Luo"
                    },
                    {
                        "authorId": "2109120459",
                        "name": "Limei Wang"
                    },
                    {
                        "authorId": "14629242",
                        "name": "Yaochen Xie"
                    },
                    {
                        "authorId": "1491238705",
                        "name": "Haonan Yuan"
                    },
                    {
                        "authorId": "1914700964",
                        "name": "Shurui Gui"
                    },
                    {
                        "authorId": "2115510017",
                        "name": "Zhao Xu"
                    },
                    {
                        "authorId": "2119316118",
                        "name": "Haiyang Yu"
                    },
                    {
                        "authorId": "2108134764",
                        "name": "Jingtun Zhang"
                    },
                    {
                        "authorId": "2153630672",
                        "name": "Yi Liu"
                    },
                    {
                        "authorId": "1879114760",
                        "name": "Keqiang Yan"
                    },
                    {
                        "authorId": "1734808354",
                        "name": "Bora Oztekin"
                    },
                    {
                        "authorId": "2143857491",
                        "name": "Haoran Liu"
                    },
                    {
                        "authorId": "2108232316",
                        "name": "Xuan Zhang"
                    },
                    {
                        "authorId": "2084647086",
                        "name": "Cong Fu"
                    },
                    {
                        "authorId": "1743600",
                        "name": "Shuiwang Ji"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "3b43c8b69d1c206149a60800695fb469ef923651",
                "externalIds": {
                    "ArXiv": "2103.03302",
                    "DBLP": "journals/corr/abs-2103-03302",
                    "DOI": "10.3390/a15110431",
                    "CorpusId": 232135020
                },
                "corpusId": 232135020,
                "publicationVenue": {
                    "id": "e95c8d18-09be-464f-a3cf-5b2637f0eff6",
                    "name": "Algorithms",
                    "type": "journal",
                    "issn": "1999-4893",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-150910",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-150910",
                        "http://www.mdpi.com/journal/algorithms",
                        "http://www.mdpi.com/journal/algorithms/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3b43c8b69d1c206149a60800695fb469ef923651",
                "title": "Ensembles of Random SHAPs",
                "abstract": "The ensemble-based modifications of the well-known SHapley Additive exPlanations (SHAP) method for the local explanation of a black-box model are proposed. The modifications aim to simplify the SHAP which is computationally expensive when there is a large number of features. The main idea behind the proposed modifications is to approximate the SHAP by an ensemble of SHAPs with a smaller number of features. According to the first modification, called the ER-SHAP, several features are randomly selected many times from the feature set, and the Shapley values for the features are computed by means of \u201csmall\u201d SHAPs. The explanation results are averaged to obtain the final Shapley values. According to the second modification, called the ERW-SHAP, several points are generated around the explained instance for diversity purposes, and the results of their explanation are combined with weights depending on the distances between the points and the explained instance. The third modification, called the ER-SHAP-RF, uses the random forest for a preliminary explanation of the instances and determines a feature probability distribution which is applied to the selection of the features in the ensemble-based procedure of the ER-SHAP. Many numerical experiments illustrating the proposed modifications demonstrate their efficiency and properties for a local explanation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145528703",
                        "name": "L. Utkin"
                    },
                    {
                        "authorId": "50585467",
                        "name": "A. Konstantinov"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "11b9f4729c8e355dec7122993076f6e2788c03c4",
                "externalIds": {
                    "DBLP": "conf/aistats/LucicHTRS22",
                    "ArXiv": "2102.03322",
                    "CorpusId": 231839528
                },
                "corpusId": 231839528,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/11b9f4729c8e355dec7122993076f6e2788c03c4",
                "title": "CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks",
                "abstract": "Given the increasing promise of graph neural networks (GNNs) in real-world applications, several methods have been developed for explaining their predictions. Existing methods for interpreting predictions from GNNs have primarily focused on generating subgraphs that are especially relevant for a particular prediction. However, such methods are not counterfactual (CF) in nature: given a prediction, we want to understand how the prediction can be changed in order to achieve an alternative outcome. In this work, we propose a method for generating CF explanations for GNNs: the minimal perturbation to the input (graph) data such that the prediction changes. Using only edge deletions, we find that our method, CF-GNNExplainer, can generate CF explanations for the majority of instances across three widely used datasets for GNN explanations, while removing less than 3 edges on average, with at least 94\\% accuracy. This indicates that CF-GNNExplainer primarily removes edges that are crucial for the original predictions, resulting in minimal CF explanations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "38702106",
                        "name": "Ana Lucic"
                    },
                    {
                        "authorId": "41096186",
                        "name": "Maartje ter Hoeve"
                    },
                    {
                        "authorId": "2651748",
                        "name": "Gabriele Tolomei"
                    },
                    {
                        "authorId": "1696030",
                        "name": "M. de Rijke"
                    },
                    {
                        "authorId": "144925193",
                        "name": "F. Silvestri"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "PGExplainer [47], and SubgraphX [48], etc.",
                "SubgraphX [48] explores subgraph-level explanations for deep graph models.",
                "SA [54], [55] Instance-level \u2717 GC/NC N/E/NF \u2717 Backward \u2717 Guided BP [54] Instance-level \u2717 GC/NC N/E/NF \u2717 Backward \u2717 CAM [55] Instance-level \u2717 GC N \u2717 Backward \u2717 Grad-CAM [55] Instance-level \u2717 GC N \u2717 Backward \u2717 GNNExplainer [46] Instance-level \u2713 GC/NC E/NF \u2713 Forward \u2713 PGExplainer [47] Instance-level \u2713 GC/NC E \u2717 Forward \u2713 GraphMask [57] Instance-level \u2713 GC/NC E \u2717 Forward \u2713 ZORRO [56] Instance-level \u2717 GC/NC N/NF \u2713 Forward \u2713 Causal Screening [58] Instance-level \u2717 GC/NC E \u2713 Forward \u2713 SubgraphX [48] Instance-level \u2713 GC/NC Subgraph \u2713 Forward \u2713 LRP [54], [59] Instance-level \u2717 GC/NC N \u2717 Backward \u2717 Excitation BP [55] Instance-level \u2717 GC/NC N \u2717 Backward \u2717 GNN-LRP [60] Instance-level \u2717 GC/NC Walk \u2717 Backward \u2713 GraphLime [61] Instance-level \u2713 NC NF \u2713 Forward \u2717 RelEx [62] Instance-level \u2713 NC N/E \u2713 Forward \u2713 PGM-Explainer [63] Instance-level \u2713 GC/NC N \u2713 Forward \u2713 XGNN [45] Model-level \u2713 GC Subgraph \u2713 Forward \u2713",
                "For the instance-level methods, the gradients/features-based methods include SA [54], Guided BP [54], CAM [55], and GradCAM [55]; the perturbation-based methods are GNNExplainer [46], PGExplainer [47], ZORRO [56], GraphMask [57], Causal Screening [58], and SubgraphX [48]; the decomposition methods contains LRP [54], [59], Excitation BP [55] and GNN-LRP [60]; the surrogate methods include GraphLime [61], RelEx [62], and PGM-Explainer [63].",
                "To explain deep graph models, several perturbationbased methods are proposed, including GNNExplainer [46], PGExplainer [47], ZORRO [56], GraphMask [57], Causal Screening [58], and SubgraphX [48]."
            ],
            "citingPaper": {
                "paperId": "14f0ee2594c550de7fb5e590b322bcb1bcec8061",
                "externalIds": {
                    "ArXiv": "2012.15445",
                    "DBLP": "journals/corr/abs-2012-15445",
                    "DOI": "10.1109/TPAMI.2022.3204236",
                    "CorpusId": 229923402,
                    "PubMed": "36063508"
                },
                "corpusId": 229923402,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/14f0ee2594c550de7fb5e590b322bcb1bcec8061",
                "title": "Explainability in Graph Neural Networks: A Taxonomic Survey",
                "abstract": "Deep learning methods are achieving ever-increasing performance on many artificial intelligence tasks. A major limitation of deep models is that they are not amenable to interpretability. This limitation can be circumvented by developing post hoc techniques to explain predictions, giving rise to the area of explainability. Recently, explainability of deep models on images and texts has achieved significant progress. In the area of graph data, graph neural networks (GNNs) and their explainability are experiencing rapid developments. However, there is neither a unified treatment of GNN explainability methods, nor a standard benchmark and testbed for evaluations. In this survey, we provide a unified and taxonomic view of current GNN explainability methods. Our unified and taxonomic treatments of this subject shed lights on the commonalities and differences of existing methods and set the stage for further methodological developments. To facilitate evaluations, we provide a testbed for GNN explainability, including datasets, common algorithms and evaluation metrics. Furthermore, we conduct comprehensive experiments to compare and analyze the performance of many techniques. Altogether, this work provides a unified methodological treatment of GNN explainability and a standardized testbed for evaluations.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1498527026",
                        "name": "Hao Yuan"
                    },
                    {
                        "authorId": "2119316118",
                        "name": "Haiyang Yu"
                    },
                    {
                        "authorId": "1914700964",
                        "name": "Shurui Gui"
                    },
                    {
                        "authorId": "1743600",
                        "name": "Shuiwang Ji"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "9d6039c022cdab1ea78a562aedd3b5e6a7b67eb6",
                "externalIds": {
                    "ArXiv": "2012.03476",
                    "DOI": "10.1109/TNNLS.2022.3179306",
                    "CorpusId": 249544281,
                    "PubMed": "35679381"
                },
                "corpusId": 249544281,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9d6039c022cdab1ea78a562aedd3b5e6a7b67eb6",
                "title": "NCGNN: Node-Level Capsule Graph Neural Network for Semisupervised Classification.",
                "abstract": "Message passing has evolved as an effective tool for designing graph neural networks (GNNs). However, most existing methods for message passing simply sum or average all the neighboring features to update node representations. They are restricted by two problems: 1) lack of interpretability to identify node features significant to the prediction of GNNs and 2) feature overmixing that leads to the oversmoothing issue in capturing long-range dependencies and inability to handle graphs under heterophily or low homophily. In this article, we propose a node-level capsule graph neural network (NCGNN) to address these problems with an improved message passing scheme. Specifically, NCGNN represents nodes as groups of node-level capsules, in which each capsule extracts distinctive features of its corresponding node. For each node-level capsule, a novel dynamic routing procedure is developed to adaptively select appropriate capsules for aggregation from a subgraph identified by the designed graph filter. NCGNN aggregates only the advantageous capsules and restrains irrelevant messages to avoid overmixing features of interacting nodes. Therefore, it can relieve the oversmoothing issue and learn effective node representations over graphs with homophily or heterophily. Furthermore, our proposed message passing scheme is inherently interpretable and exempt from complex post hoc explanations, as the graph filter and the dynamic routing procedure identify a subset of node features that are most significant to the model prediction from the extracted subgraph. Extensive experiments on synthetic as well as real-world graphs demonstrate that NCGNN can well address the oversmoothing issue and produce better node representations for semisupervised node classification. It outperforms the state of the arts under both homophily and heterophily.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2115430809",
                        "name": "Rui Yang"
                    },
                    {
                        "authorId": "3207464",
                        "name": "Wenrui Dai"
                    },
                    {
                        "authorId": "144535686",
                        "name": "Chenglin Li"
                    },
                    {
                        "authorId": "38871632",
                        "name": "Junni Zou"
                    },
                    {
                        "authorId": "144045763",
                        "name": "H. Xiong"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Themethods can be divided into two categories: the instance-levelmethods provide example-specific explanations by identifying important input features for its prediction [3, 123, 194]; the model-level methods provide high-level interpretations and a generic understanding of how deep graph models work [192]."
            ],
            "citingPaper": {
                "paperId": "3443efc855cebd17d1512d1a703b6e9ee2e4da8b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2011-02260",
                    "MAG": "3097300053",
                    "ArXiv": "2011.02260",
                    "DOI": "10.1145/3535101",
                    "CorpusId": 226246289
                },
                "corpusId": 226246289,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3443efc855cebd17d1512d1a703b6e9ee2e4da8b",
                "title": "Graph Neural Networks in Recommender Systems: A Survey",
                "abstract": "With the explosive growth of online information, recommender systems play a key role to alleviate such information overload. Due to the important application value of recommender systems, there have always been emerging works in this field. In recommender systems, the main challenge is to learn the effective user/item representations from their interactions and side information (if any). Recently, graph neural network (GNN) techniques have been widely utilized in recommender systems since most of the information in recommender systems essentially has graph structure and GNN has superiority in graph representation learning. This article aims to provide a comprehensive review of recent research efforts on GNN-based recommender systems. Specifically, we provide a taxonomy of GNN-based recommendation models according to the types of information used and recommendation tasks. Moreover, we systematically analyze the challenges of applying GNN on different types of data and discuss how existing works in this field address these challenges. Furthermore, we state new perspectives pertaining to the development of this field. We collect the representative papers along with their open-source implementations in https://github.com/wusw14/GNN-in-RS.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2112379399",
                        "name": "Shiwen Wu"
                    },
                    {
                        "authorId": "2108232566",
                        "name": "Wentao Zhang"
                    },
                    {
                        "authorId": "2113629792",
                        "name": "Fei Sun"
                    },
                    {
                        "authorId": "2068228300",
                        "name": "Bin Cui"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Some explanation techniques such as GNNExplainer [26], PGExplainer [27] and SubgraphX [32] are based solely on evaluating the function or its gradient multiple times.",
                "The method SubgraphX [32] attributes the prediction to subgraphs of the input graph and is closely related to our subgraph selection technique presented in Section 4.1.",
                "[26], PGExplainer [27] and SubgraphX [32] are based solely",
                "[32] proposes a Monte-Carlo tree search to find relevant sub-",
                "SubgraphX uses a Monte-Carlo optimization algorithm to find the most relevant subgraph, whereas we use either a local best guess or a random sampling approach (cf. Appendix D of the Supplement, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org/ 10.1109/TPAMI.2021.3115452).",
                "SubgraphX uses Shapley values for the subgraph scoring, whereas we use a backward propagation pass.",
                "The method SubgraphX [32] attributes the prediction to subgraphs of the input graph and is closely related to our subgraph selection technique presented in Section 4.",
                "The method SubgraphX [32] proposes a Monte-Carlo tree search to find relevant subgraphs in the input graph and uses Shapley values as an attribution function for subgraphs."
            ],
            "citingPaper": {
                "paperId": "9e707dd89bba25a3dd22c96f43bd72b9b3ab94bb",
                "externalIds": {
                    "ArXiv": "2006.03589",
                    "DBLP": "journals/pami/SchnakeELNSMM22",
                    "MAG": "3108823960",
                    "DOI": "10.1109/TPAMI.2021.3115452",
                    "CorpusId": 227225626,
                    "PubMed": "34559639"
                },
                "corpusId": 227225626,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9e707dd89bba25a3dd22c96f43bd72b9b3ab94bb",
                "title": "Higher-Order Explanations of Graph Neural Networks via Relevant Walks",
                "abstract": "Graph Neural Networks (GNNs) are a popular approach for predicting graph structured data. As GNNs tightly entangle the input graph into the neural network structure, common explainable AI approaches are not applicable. To a large extent, GNNs have remained black-boxes for the user so far. In this paper, we show that GNNs can in fact be naturally explained using higher-order expansions, i.e., by identifying groups of edges that jointly contribute to the prediction. Practically, we find that such explanations can be extracted using a nested attribution scheme, where existing techniques such as layer-wise relevance propagation (LRP) can be applied at each step. The output is a collection of walks into the input graph that are relevant for the prediction. Our novel explanation method, which we denote by GNN-LRP, is applicable to a broad range of graph neural networks and lets us extract practically relevant insights on sentiment analysis of text data, structure-property relationships in quantum chemistry, and image classification.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "90387439",
                        "name": "Thomas Schnake"
                    },
                    {
                        "authorId": "1557932201",
                        "name": "Oliver Eberle"
                    },
                    {
                        "authorId": "95930534",
                        "name": "Jonas Lederer"
                    },
                    {
                        "authorId": "3187484",
                        "name": "Shinichi Nakajima"
                    },
                    {
                        "authorId": "51257580",
                        "name": "K. T. Schutt"
                    },
                    {
                        "authorId": "116099820",
                        "name": "Klaus-Robert Muller"
                    },
                    {
                        "authorId": "144535526",
                        "name": "G. Montavon"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "For example, [9] was proposed to utilize mutual information to find a subgraph with associated features for interpreting GNN models; PGExplainer [16] learns a parameterized model to predict whether an edge is important; SubgraphX [17] explains GNNs by exploring and identifying important subgraphs; GraphSVX [18] utilizes decomposition technique to explain GNNs based on the Shapley Values from game theory."
            ],
            "citingPaper": {
                "paperId": "760ab37ab4d5a68b53035208d2e179494d879322",
                "externalIds": {
                    "DBLP": "journals/tkde/HuangYTSC23",
                    "MAG": "3000120900",
                    "ArXiv": "2001.06216",
                    "DOI": "10.1109/TKDE.2022.3187455",
                    "CorpusId": 210714016
                },
                "corpusId": 210714016,
                "publicationVenue": {
                    "id": "c6840156-ee10-4d78-8832-7f8909811576",
                    "name": "IEEE Transactions on Knowledge and Data Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Knowl Data Eng"
                    ],
                    "issn": "1041-4347",
                    "url": "https://www.computer.org/web/tkde",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=69"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/760ab37ab4d5a68b53035208d2e179494d879322",
                "title": "GraphLIME: Local Interpretable Model Explanations for Graph Neural Networks",
                "abstract": "Recently, graph neural networks (GNN) were shown to be successful in effectively representing graph structured data because of their good performance and generalization ability. However, explaining the effectiveness of GNN models is a challenging task because of the complex nonlinear transformations made over the iterations. In this paper, we propose GraphLIME, a local interpretable model explanation for graphs using the Hilbert-Schmidt Independence Criterion (HSIC) Lasso, which is a nonlinear feature selection method. GraphLIME is a generic GNN-model explanation framework that learns a nonlinear interpretable model locally in the subgraph of the node being explained. Through experiments on two real-world datasets, the explanations of GraphLIME are found to be of extraordinary degree and more descriptive in comparison to the existing explanation methods.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "144938775",
                        "name": "Q. Huang"
                    },
                    {
                        "authorId": "50142992",
                        "name": "M. Yamada"
                    },
                    {
                        "authorId": "2152948229",
                        "name": "Yuan Tian"
                    },
                    {
                        "authorId": "2112755810",
                        "name": "Dinesh Singh"
                    },
                    {
                        "authorId": "50559722",
                        "name": "Dawei Yin"
                    },
                    {
                        "authorId": "2118858577",
                        "name": "Yi Chang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "GNNExplainer [15], SubgraphX [18], and GraphMask [38] leveragemutual information, Shapley values, and divergence, respectively, using both the original and affected label predictions, which are predicted on the candidate explanation subgraph.",
                "Recently, although the explainability of graph neural networks (GNNs) [9], [10], [11], [12], [13], [14] has been explored as in [15], [16], [17], [18], [19], [20], and [21], they are limited to understanding",
                "The Fidelity score [18], [35], [62] is used to determine the impact of the found local explanations in downstream tasks.",
                "For example, GNNExplainer [15], SubgraphX [18], and GraphMask [38] leveragemutual information, Shapley values, and divergence, respectively, using both the original and affected label predictions, which are predicted on the candidate explanation subgraph."
            ],
            "citingPaper": {
                "paperId": "17328001f78d936c45ba7cb883107664fb27d2b4",
                "externalIds": {
                    "DBLP": "journals/access/Park23",
                    "DOI": "10.1109/ACCESS.2022.3233036",
                    "CorpusId": 255282566
                },
                "corpusId": 255282566,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/17328001f78d936c45ba7cb883107664fb27d2b4",
                "title": "Providing Post-Hoc Explanation for Node Representation Learning Models Through Inductive Conformal Predictions",
                "abstract": "Learning with graph-structured data, such as social, biological, and financial networks, requires effective low-dimensional representations to handle their large and complex interactions. Recently, with the advances of neural networks and embedding algorithms, many unsupervised approaches have been proposed for many downstream tasks with promising results; however, there has been limited research on interpreting the unsupervised representations and, specifically, on understanding which parts of the neighboring nodes contribute to the representation of a node. To mitigate this problem, we propose a statistical framework to interpret the learned representations. Many of the existing works, which are designed for supervised node presentation models, compute the difference in prediction scores after perturbing the edges of a candidate explanation node; however, our proposed framework leverages a conformal prediction (CP)-based statistical test to verify the importance of the candidate node in each node representation. In our evaluation, our proposed framework was verified in many experimental settings and presented promising results compared to those of the recent baseline methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1845151",
                        "name": "Hogun Park"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                ", 2020) and SubgraphX (Yuan et al., 2021)) for static GNNs are the most related.",
                "Previous works expand all possible children for any selected node (Yuan et al., 2021).",
                "Besides, search-based methods (Yuan et al. (2021); Wang et al. (2021a)) utilize heuristic search algorithms with a score function (e.g., defined by Shapley value or causality) to find an important input subset.",
                "While currently there are no methods for explaining temporal graph models, some recent explanation methods (e.g., GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020) and SubgraphX (Yuan et al., 2021)) for static GNNs are the most related."
            ],
            "citingPaper": {
                "paperId": "7d6aa3d0a9113501e658ff939dda4b01e0f6a785",
                "externalIds": {
                    "DBLP": "conf/iclr/XiaLS0D0023",
                    "CorpusId": 259298210
                },
                "corpusId": 259298210,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7d6aa3d0a9113501e658ff939dda4b01e0f6a785",
                "title": "Explaining Temporal Graph Models through an Explorer-Navigator Framework",
                "abstract": "While Graph Neural Network (GNN) explanation has recently received significant attention, existing works are generally designed for static graphs. Due to the prevalence of temporal graphs, many temporal graph models have been proposed, but explaining their predictions still remains to be explored. To bridge the gap, in this paper, we propose a Temporal GNN Explainer (T-GNNExplainer) method. Specifically, we regard a temporal graph as a sequence of temporal events between nodes. Given a temporal prediction of a model, our task is to find a subset of historical events that lead to the prediction. To handle this combinatorial optimization problem, T-GNNExplainer includes an explorer to find the event subsets with Monte Carlo Tree Search (MCTS), and a navigator that learns the correlations between events and helps reduce the search space. In particular, the navigator is trained in advance and then integrated with the explorer to speed up searching and achieve better results. To the best of our knowledge, T-GNNExplainer is the first explainer tailored for temporal graph models. We conduct extensive experiments to evaluate the performance of T-GNNExplainer. Experimental results demonstrate that T-GNNExplainer can achieve superior performance with up to \u21e050% improvement in Area under Fidelity-Sparsity Curve.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "102410799",
                        "name": "Wenwen Xia"
                    },
                    {
                        "authorId": "3075031",
                        "name": "Mincai Lai"
                    },
                    {
                        "authorId": "145663545",
                        "name": "Caihua Shan"
                    },
                    {
                        "authorId": "2195028945",
                        "name": "Yaofang Zhang"
                    },
                    {
                        "authorId": "104993629",
                        "name": "Xinnan Dai"
                    },
                    {
                        "authorId": "47875796",
                        "name": "Xiang Li"
                    },
                    {
                        "authorId": "2119081394",
                        "name": "Dongsheng Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Alternatively, SubgraphX [139] samples a group of nodes\u2019 neighborhoods as subgraphs."
            ],
            "citingPaper": {
                "paperId": "5ff8c0c4c8de76e0e925fe716d0411d3e74653fc",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-05860",
                    "DOI": "10.48550/arXiv.2301.05860",
                    "CorpusId": 255942689
                },
                "corpusId": 255942689,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5ff8c0c4c8de76e0e925fe716d0411d3e74653fc",
                "title": "A Comprehensive Survey of Graph-level Learning",
                "abstract": "\u2014Graphs have a superior ability to represent re- lational data, like chemical compounds, proteins, and social networks. Hence, graph-level learning, which takes a set of graphs as input, has been applied to many tasks including comparison, regression, classi\ufb01cation, and more. Traditional approaches to learning a set of graphs tend to rely on hand-crafted features, such as substructures. But while these methods bene\ufb01t from good interpretability, they often suffer from computational bottlenecks as they cannot skirt the graph isomorphism problem. Conversely, deep learning has helped graph-level learning adapt to the growing scale of graphs by extracting features automatically and decoding graphs into low-dimensional representations. As a result, these deep graph learning methods have been responsible for many successes. Yet, there is no comprehensive survey that reviews graph-level learning starting with traditional learning and moving through to the deep learning approaches. This article \ufb01lls this gap and frames the representative algorithms into a systematic taxonomy covering traditional learning, graph-level deep neural networks, graph-level graph neural networks, and graph pooling. To ensure a thoroughly comprehensive survey, the evolutions, interactions, and communications between methods from four different branches of development are also examined. This is followed by a brief review of the benchmark data sets, evaluation metrics, and common downstream applications. The survey concludes with 13 future directions of necessary research that will help to overcome the challenges facing this booming \ufb01eld.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "152747612",
                        "name": "Zhenyu Yang"
                    },
                    {
                        "authorId": "2151251543",
                        "name": "Ge Zhang"
                    },
                    {
                        "authorId": "2142734769",
                        "name": "Jia Wu"
                    },
                    {
                        "authorId": "2118801701",
                        "name": "Jian Yang"
                    },
                    {
                        "authorId": "120607997",
                        "name": "Quan.Z Sheng"
                    },
                    {
                        "authorId": "2057237074",
                        "name": "Shan Xue"
                    },
                    {
                        "authorId": "2110713858",
                        "name": "Chuan Zhou"
                    },
                    {
                        "authorId": "1682418",
                        "name": "C. Aggarwal"
                    },
                    {
                        "authorId": "2138443697",
                        "name": "Hao Peng"
                    },
                    {
                        "authorId": "2146226874",
                        "name": "Wenbin Hu"
                    },
                    {
                        "authorId": "2064408469",
                        "name": "Edwin R. Hancock"
                    },
                    {
                        "authorId": "2075355155",
                        "name": "Pietro Lio'"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "research area with several subsequent papers [8], [23], [24] exploring the topic."
            ],
            "citingPaper": {
                "paperId": "bc0802aa3f2b80c5273774f57a1490f2ef37fdb7",
                "externalIds": {
                    "DBLP": "journals/access/BuiLL23",
                    "DOI": "10.1109/ACCESS.2023.3270385",
                    "CorpusId": 258350116
                },
                "corpusId": 258350116,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bc0802aa3f2b80c5273774f57a1490f2ef37fdb7",
                "title": "Generating Real-Time Explanations for GNNs via Multiple Specialty Learners and Online Knowledge Distillation",
                "abstract": "Graph Neural Networks have become increasingly ubiquitous in numerous applications, necessitating explanations of their predictions. However, explaining GNNs is challenging due to the complexity of graph data and model execution. Post-hoc explanation approaches have gained popularity due to their versatility, despite their additional computational costs. Although intrinsically interpretable models can provide instant explanations, they are usually model-specific and can only explain particular GNNs. To address these challenges, we propose a novel, general, and fast GNN explanation framework named SCALE. SCALE trains multiple specialty learners to explain GNNs, as creating a single powerful explainer for examining the attributions of interactions in input graphs is complicated. In training, a black-box GNN model guides learners based on an online knowledge distillation paradigm. During the explanation phase, explanations of predictions are generated by multiple explainers corresponding to trained learners. Edge masking and random walk with restart procedures are implemented to provide structural explanations for graph-level and node-level predictions. A feature attribution module provides overall summaries and instance-level feature contributions. We compare SCALE with state-of-the-art baselines through extensive experiments to demonstrate its explanation correctness and execution performance. Furthermore, we conduct a user study and a series of ablation studies to understand its strengths and weaknesses.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "41022579",
                        "name": "Tien-Cuong Bui"
                    },
                    {
                        "authorId": "2055470540",
                        "name": "Van-Duc Le"
                    },
                    {
                        "authorId": "2108718185",
                        "name": "Wen-Syan Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Perturbation-based methods [16], [17], [18], [19], [20], [21] measure importance scores by masking the"
            ],
            "citingPaper": {
                "paperId": "8ce5298c1dce9d6f422ab0b7b0f4b3eb8a0ff09b",
                "externalIds": {
                    "CorpusId": 259281125
                },
                "corpusId": 259281125,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8ce5298c1dce9d6f422ab0b7b0f4b3eb8a0ff09b",
                "title": "Towards Semantic Interpretation and Validation of Graph Attention-based Explanations",
                "abstract": "\u2014In this work, we investigate the use of semantic attention to explain the performance of a Graph Neural Network (GNN)-based pose estimation model. To validate our approach, we apply semantically-informed perturbations to the input data and correlate the predicted feature importance weights with the model\u2019s accuracy. Graph Deep Learning (GDL) is an emerging field of machine learning for tasks like scene interpretation, as it exploits flexible graph structures to describe complex features and relationships in a very concise format. However, due to the unconventional structure of the graphs, traditional explainability methods used in eXplainable AI (XAI) require further adaptation and thus, graph-specific methods are introduced. Attention is a powerful tool, introduced to estimate the importance of input features in deep learning models. It has been previously used to provide feature-based explanations on the predictions of GNN models. In our proposed work, we exploit graph attention to identify key semantic classes for lidar pointcloud pose estimation. We extend the current attention-based graph explainability methods by investigating the use of attention weights as importance indicators of semantically sorted feature sets by analysing the correlation between attention weights distribution and model accuracy. Our method has shown promising results for post-hoc semantic explanation of graph-based pose estimation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220785800",
                        "name": "Efimia Panagiotaki"
                    },
                    {
                        "authorId": "7764753",
                        "name": "D. Martini"
                    },
                    {
                        "authorId": "51177608",
                        "name": "Lars Kunze"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "Yuan et al. (2021) consider each subgraph as possible explanation."
            ],
            "citingPaper": {
                "paperId": "57a9eb4f3d9f614d5507f1f79b20a91a08edfffc",
                "externalIds": {
                    "CorpusId": 259926240
                },
                "corpusId": 259926240,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/57a9eb4f3d9f614d5507f1f79b20a91a08edfffc",
                "title": "GraphChef: Learning the Recipe of Your Dataset",
                "abstract": "We propose a new graph model, GraphChef, that enables us to understand graph datasets as a whole. Given a dataset, GraphChef returns a set of rules (a recipe) that describes each class in the dataset. Existing GNNs and explanation methods reason on individual graphs not on the entire dataset. GraphChef uses decision trees to build recipes that are understandable by humans. We show how to compute decision trees in the message passing framework in order to create GraphChef. We also present a new pruning method to produce small and easy to digest trees. In the experiments, we present and analyze GraphChef\u2019s recipes for Reddit-Binary, MUTAG, BA-2Motifs, BA-Shapes, Tree-Cycle, and Tree-Grid. We verify the correctness of the discovered recipes against the datasets\u2019 ground truth.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2223560094",
                        "name": "Peter M\u00fcller"
                    },
                    {
                        "authorId": "36352356",
                        "name": "Lukas Faber"
                    },
                    {
                        "authorId": "1995092493",
                        "name": "Karolis Martinkus"
                    },
                    {
                        "authorId": "2075356250",
                        "name": "Roger Wattenhofer"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026neural networks (GNNs) attract increasing attentions due to their remarkable performance (Gao et al., 2021; Gao & Ji, 2019; Liu et al., 2021a;b; Yuan et al., 2021) in many applications, such as knowledge graphs (Hamaguchi et al., 2017), molecular property prediction (Liu et al., 2022; 2020;\u2026",
                "Recently, graph neural networks (GNNs) attract increasing attentions due to their remarkable performance (Gao et al., 2021; Gao & Ji, 2019; Liu et al., 2021a;b; Yuan et al., 2021) in many applications, such as knowledge graphs (Hamaguchi et al."
            ],
            "citingPaper": {
                "paperId": "10b0313fac132e2fa2395257c83271c9d2d596b2",
                "externalIds": {
                    "CorpusId": 259935204
                },
                "corpusId": 259935204,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/10b0313fac132e2fa2395257c83271c9d2d596b2",
                "title": "AUTOMATED DATA AUGMENTATIONS",
                "abstract": "We consider fair graph representation learning via data augmentations. While this direction has been explored previously, existing methods invariably rely on certain assumptions on the properties of fair graph data in order to design fixed strategies on data augmentations. Nevertheless, the exact properties of fair graph data may vary significantly in different scenarios. Hence, heuristically designed augmentations may not always generate fair graph data in different application scenarios. In this work, we propose a method, known as Graphair, to learn fair representations based on automated graph data augmentations. Such fairness-aware augmentations are themselves learned from data. Our Graphair is designed to automatically discover fairness-aware augmentations from input graphs in order to circumvent sensitive information while preserving other useful information. Experimental results demonstrate that our Graphair consistently outperforms many baselines on multiple node classification datasets in terms of fairness-accuracy trade-off performance. In addition, results indicate that Graphair can automatically learn to generate fair graph data without prior knowledge on fairness-relevant graph properties. Our code is publicly available as part of the DIG package (https://github.com/divelab/DIG).",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47653902",
                        "name": "Zhimeng Jiang"
                    },
                    {
                        "authorId": "2004524780",
                        "name": "Youzhi Luo"
                    },
                    {
                        "authorId": "1743600",
                        "name": "Shuiwang Ji"
                    },
                    {
                        "authorId": "49648991",
                        "name": "Na Zou"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "SubgraphX [14] uses the Shapley value and onbtain the most important subgraphs with Monte Carlo Tree Search (MCTS)."
            ],
            "citingPaper": {
                "paperId": "d8b9bfe6485870e3076cd72a905a2715746d4422",
                "externalIds": {
                    "CorpusId": 260604556
                },
                "corpusId": 260604556,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d8b9bfe6485870e3076cd72a905a2715746d4422",
                "title": "Graph Model Explainer Tool",
                "abstract": "Graph Neural Networks (GNNs) have gained popularity in various fields, such as recommendation systems, social network analysis and fraud detection. However, despite their effectiveness, the topological nature of GNNs makes it challenging for users to understand the model predictions. To address this challenge, we built a user-friendly UI to visualize the most important relationships for both homogeneous and heterogeneous static graphs models, which a post-hoc explanation technique called GNNExplainer is implemented. This UI can be applied to a wide range of applications that use graph models. It offers an intuitive and interpretable way for users to understand the complex relationships within a graph and how they influence the model\u2019s predictions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2226180643",
                        "name": "Yudi Zhang"
                    },
                    {
                        "authorId": "2228929658",
                        "name": "Naveed Janvekar"
                    },
                    {
                        "authorId": "2228930523",
                        "name": "Phanindra Reddy Madduru"
                    },
                    {
                        "authorId": "2227971277",
                        "name": "Nitika Bhaskar"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Numerous explanation methods have been extensively studied for GNNs, including gradient-based attribution methods [32, 6, 36], perturbation-based methods [48, 41, 52, 34, 16], etc.",
                "Other methods utilize more advanced frameworks such as mask optimization [48], surrogate model [39], and Monte Carlo Tree Search [52] to search the explanation subgraphs for each individual instance.",
                "Explainability methods We compare non-generative methods: Saliency [6], Integrated Gradient [36], Occlusion [53], Grad-CAM [32], GNNExplainer [48], PGMExplainer [39], and SubgraphX [52], with generative ones: PGExplainer [27], GSAT [29], GraphCFE (CLEAR) [28], D4Explainer and RCExplainer [42]."
            ],
            "citingPaper": {
                "paperId": "9c6f0e3e89e73303ec3c76d31e5371d7e59635dc",
                "externalIds": {
                    "CorpusId": 260744309
                },
                "corpusId": 260744309,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9c6f0e3e89e73303ec3c76d31e5371d7e59635dc",
                "title": "Generative Explanation for Graph Neural Network: Methods and Evaluation",
                "abstract": "Graph Neural Networks (GNNs) achieve state-of-the-art performance in various graph-related tasks. However the black-box nature often limits their interpretability and trustworthiness. Numerous explanation methods have been proposed to uncover the decision-making logic of GNNs, by generating underlying explanatory substructures. In this paper, we conduct a comprehensive review of the existing explanation methods for GNNs from the perspective of graph generation. Specifically, we propose a unified optimization objective for current generative explanation methods, comprising two sub-objectives: Attribution and Information constraints. We further demonstrate their specific manifestations in different generative model architectures and explanation scenarios. With the unified objective of the explanation problem, we reveal the shared characteristics and distinctions among current methods, laying the foundation for future methodological advancements. Empirical results demonstrate the advantages and limitations of different approaches in terms of explanation performance, efficiency, and generalizability.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "83539859",
                        "name": "Rex Ying"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "SubgraphX (Yuan et al., 2021) searches the most relevant subgraph using MonteCarlo Tree Search (MCTS) with Shapley value (Lundberg & Lee, 2017), and applied approximation methods in computing Shapley values, which is otherwise too computationintensive."
            ],
            "citingPaper": {
                "paperId": "7a3c3084a2c109d7497f4981661df9f58bda3d65",
                "externalIds": {
                    "DBLP": "conf/icml/XiongSGMMN23",
                    "CorpusId": 260879711
                },
                "corpusId": 260879711,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7a3c3084a2c109d7497f4981661df9f58bda3d65",
                "title": "Relevant Walk Search for Explaining Graph Neural Networks",
                "abstract": "Graph Neural Networks (GNNs) have become important machine learning tools for graph analysis, and its explainability is crucial for safety, fairness, and robustness. Layer-wise relevance propagation for GNNs (GNN-LRP) evaluates the relevance of walks to reveal important information flows in the network, and provides higher-order explanations, which have been shown to be superior to the lower-order, i.e., node-/edge-level, explanations. However, identifying relevant walks by GNN-LRP requires exponential computational complexity with respect to the network depth, which we will remedy in this paper. Specifically, we propose polynomial-time algorithms for finding top-K relevant walks, which drastically reduces the computation and thus increases the applicability of GNN-LRP to large-scale problems. Our proposed algorithms are based on the max-product algorithm\u2014a common tool for finding the maximum likelihood configurations in probabilistic graphical models\u2014and can find the most relevant walks exactly at the neuron level and approximately at the node level. Our experiments demonstrate the performance of our algorithms at scale and their utility across application domains, i.e., on epidemiology, molecular, and natural language benchmarks. We provide our codes under",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2062775158",
                        "name": "Pin Xiong"
                    },
                    {
                        "authorId": "90387439",
                        "name": "Thomas Schnake"
                    },
                    {
                        "authorId": "5742764",
                        "name": "M. Gastegger"
                    },
                    {
                        "authorId": "144535526",
                        "name": "G. Montavon"
                    },
                    {
                        "authorId": "2113612432",
                        "name": "Klaus-Robert M\u00fcller"
                    },
                    {
                        "authorId": "2055678647",
                        "name": "S. Nakajima"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Independently from this categorization, a further fundamental distinction is among explainers providing explanations in terms of edge [87, 52, 66, 92] or node masks [75, 76, 55, 6, 55, 67].",
                "[91], and choose to investigate instance-based explainers[101, 71, 74, 76, 75, 80, 87, 6, 68, 36, 62, 92, 53, 72, 51, 97, 67, 23], i.",
                "Despite the existence of other works proposing explainers, which occasionally fall outside the aforementioned categorization [92, 53, 72, 51, 36, 97, 67], we limited our analysis on a subset."
            ],
            "citingPaper": {
                "paperId": "b8e00ad63de8c6963bb8c18994ac65874ec89abb",
                "externalIds": {
                    "CorpusId": 262698822
                },
                "corpusId": 262698822,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b8e00ad63de8c6963bb8c18994ac65874ec89abb",
                "title": "Understanding how explainers work in graph neural networks",
                "abstract": ",",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2130615549",
                        "name": "Antonio Longa"
                    },
                    {
                        "authorId": "2165224662",
                        "name": "Steve Azzolin"
                    },
                    {
                        "authorId": "2042269425",
                        "name": "G. Santin"
                    },
                    {
                        "authorId": "50139333",
                        "name": "G. Cencetti"
                    },
                    {
                        "authorId": "2075355155",
                        "name": "Pietro Lio'"
                    },
                    {
                        "authorId": "1776476",
                        "name": "B. Lepri"
                    },
                    {
                        "authorId": "1702610",
                        "name": "Andrea Passerini"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": true,
            "contexts": [
                "In fact, GNNExplainer, PGExplainer, and SubgraphX can never generate explanations including only disconnected oxygen atoms but not nitrogen atoms like GStarX, because the former two solve the explanation problem by optimizing edges (as opposed to Equation 3), and the latter requires connectedness.",
                "GStarX is not as fast as GNNExplainer, PGExplainer, and GraphSVX, but it is about more than two times faster than SubgraphX.",
                ", 2021) or a supernode covering a subgraph (Yuan et al., 2021).",
                "SubgraphX (Yuan et al., 2021) uses the Shapley value as scoring function on subgraphs selected by Monte Carlo Tree Search (MCTS), while GraphSVX (Duval & Malliaros, 2021) uses a least-square approximation of the Shapley value to score nodes and node features.",
                "SubgraphX gives reasonable results, but because it can only select a connected subgraph as the explanation, it cannot cover two groups of important nodes with limited budget; e.g. to cover the negative word \u201clameness\u201d in the second sentence, SubgraphX needs at least 3 more nodes along the way, which will significantly decrease Sparsity while including undesirable, neutral words.",
                "In particular, both SubgraphX and GraphSVX use Shapley-value-based scoring functions.",
                "While SubgraphX and GraphSVX were shown to perform better than prior alternatives, as we show in Section 4, the Shapley value they try to approximate is unideal due to not having structure-awareness.",
                ", 2020), SubgraphX (Yuan et al., 2021), and GraphSVX (Duval & Malliaros, 2021).",
                "The idea is to do feature importance scoring as above, where the features are nodes (Yuan et al., 2021) or a supernode covering a subgraph (Yuan et al.",
                "Although SubgraphX and GraphSVX use L-hop subgraphs and thus technically they use the graph structure, we discuss why this approach has limitations in achieving structure-awareness in Appendix F.",
                "We compare with 4 strong baselines representing the state-of-the-art methods for GNN explanation: GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), SubgraphX (Yuan et al., 2021), and GraphSVX (Duval & Malliaros, 2021).",
                "SubgraphX shows the best HFidelity among these baselines."
            ],
            "citingPaper": {
                "paperId": "699a946c7e519e4f288baae422bae8920070bff3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-12380",
                    "CorpusId": 246430826
                },
                "corpusId": 246430826,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/699a946c7e519e4f288baae422bae8920070bff3",
                "title": "Explaining Graph-level Predictions with Communication Structure-Aware Cooperative Games",
                "abstract": "Explaining predictions made by machine learning models is important and have attracted an increased interest. The Shapley value from cooperative game theory has been proposed as a prime approach to compute feature importances towards predictions, especially for images, text, tabular data, and recently graph neural networks (GNNs) on graphs. In this work, we revisit the appropriate-ness of the Shapley value for graph explanation, where the task is to identify the most important subgraph and constituent nodes for graph-level predictions. We purport that the Shapley value is a no-ideal choice for graph data because it is by def-inition not structure-aware. We propose a Graph Structure-aware eXplanation (GStarX) method to leverage the critical graph structure information to improve the explanation. Speci\ufb01cally, we propose a scoring function based on a new structure-aware value from the cooperative game theory called the HN value. When used to score node importance, the HN value utilizes graph structures to attribute cooperation surplus between neighbor nodes, re-sembling message passing in GNNs, so that node importance scores re\ufb02ect not only the node feature importance, but also the structural roles. We demonstrate that GstarX produces qualitatively more intuitive explanations, and quantitatively improves over strong baselines on chemical graph property prediction and text graph sentiment classi\ufb01cation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2145408511",
                        "name": "Shichang Zhang"
                    },
                    {
                        "authorId": "145474474",
                        "name": "Neil Shah"
                    },
                    {
                        "authorId": "152891495",
                        "name": "Yozen Liu"
                    },
                    {
                        "authorId": "2109461904",
                        "name": "Yizhou Sun"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "[35] propose to employ the Monte Carlo tree search method [137] to explore the critical subgraphs and thus explain the prediction problem of GNNs from subgraph-level.",
                "An illustration of the SubgraphX architecture [35].",
                "A Summary of Open-source Implementations\nModel ear Framework Link Sourc\ne\nAGILE 2022 PyTorch https://github.com/clvrai/agile [30] GTA-RL 2022 PyTorch https://github.com/udeshmg/GTA-RL [34] SubgraphX 2021 PyTorch https://github.com/divelab/DIG [35] SUGAR 2021 Tensorflow https://github.com/RingBDStack/SUGAR [15]\nCORL 2021 PyTorch https://github.com/huawei-\nnoah/trustworthyAI/tree/master/gcastle\n[124]\nRioGNN 2021 PyTorch https://github.com/safe-graph/RioGNN [73] IG-RL 2021 PyTorch https://github.com/FXDevailly/IG-RL [142] TITer 2021 Python https://github.com/JHL-HUST/TITer/ [128] SparRL 2021 PyTorch https://github.com/rwickman/SparRL-PyTorch [37] PAAR 2021 PyTorch https://github.com/seukgcode/PAAR [87] Policy-GNN 2020 PyTorch https://github.com/lhenry15/Policy-GNN [41] CARE-GNN 2020 PyTorch https://github.com/YingtongDou/CARE-GNN [36]\nRL-BIC 2020 Tensorflow https://github.com/huawei-\nnoah/trustworthyAI/tree/master/Causal_Structure_Learning\n/Causal_Discovery_RL\n[123]\nRL-based\nGraph2Seq\n2020 PyTorch https://github.com/hugochan/RL-based-Graph2Seq-for-\nNQG\n[126]\nDGN 2020 Tensorflow https://github.com/PKU-AI-Edge/DGN/ [27]\nDeepGraphM\nolGen\n2020 PyTorch https://github.com/dbkgroup/prop_gen [154]\nKG-A2C 2020 PyTorch https://github.com/rajammanabrolu/KG-A2C [171]\nGPA 2020 PyTorch https://github.com/ShengdingHu/GraphPolicyNetworkActi\nveLearning\n[14]\nGAEA 2020 Tensorflow https://github.com/salesforce/GAEA [172] CompNet 2019 PyTorch https://github.com/WOW5678/CompNet [7]\nGRPI 2019 Python https://github.com/LASP-UCL/Graph-RL [111]\nDRL+GNN 2019 PyTorch https://github.com/knowledgedefinednetworking/DRL-\nGNN\n[145]\nGPN 2019 PyTorch https://github.com/qiang-ma/graph-pointer-network [161] PGPR 2019 PyTorch https://github.com/orcax/PGPR [173] DGN 2018 PyTorch https://github.com/PKU-AI-Edge/DGN [27] GCPN 2018 Python https://github.com/bowenliu16/rl_graph_generation [10] KG-DQN 2018 PyTorch https://github.com/rajammanabrolu/KG-DQN [174] ASNets 2018 Tensorflow https://github.com/qxcv/asnets [150]\nS2V-DQN 2017 C+Python https://github.com/Hanjun-Dai/graph_comb_opt [148] DeepPath 2017 Tensorflow https://github.com/xwhan/DeepPath [3] MINERVA 2017 Tensorflow https://github.com/shehzaadzd/MINERVA [92] KBGAN 2017 PyTorch https://github.com/cai-lw/KBGAN [95]",
                "(Redrawn from [35]) In addition, the scholars believe that explaining GNNs from the model level could enhance human trust for certain application domains.",
                "3 mining for co-learning are divided into two main categories: (1) Solving RL problems by exploiting graph structures [26-32], (2) Solving graph mining tasks with RL methods [15, 22, 33-35].",
                "95 [103] SubgraphX[35] Graph Classification",
                "Source Citation Task\nMUTAG 188 17.93 19.79 [97] SUGAR[15], SubgraphX[35], XGNN[18],\nGraphAug[98]\nGraph Classification\nPTC 344 14.29 14.69 [99] SUGAR[15] Graph Classification\nPROTEINS 1113 39.06 72.82 [100] SUGAR[15], GraphAug[98] Graph Classification\nD&D 1178 284.32 715.66 [101] SUGAR[15] Graph Classification\nNCI1 4110 29.87 32.30 [102] SUGAR[15], GraphAug[98] Graph Classification\nNCI109 4127 29.68 32.13 [102] SUGAR[15], GraphAug[98] Graph Classification\nBBBP 2039 24.06 25.95 [103] SubgraphX[35] Graph Classification\nGRAPH-SST2 70042 10.19 9.20 [104] SubgraphX[35] Graph Classification\nTable .",
                "79 [97] SUGAR[15], SubgraphX[35], XGNN[18], GraphAug[98] Graph Classification",
                "20 [104] SubgraphX[35] Graph Classification"
            ],
            "citingPaper": {
                "paperId": "b45ea295f8bab57f48680df4610909131695fe64",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-06127",
                    "DOI": "10.48550/arXiv.2204.06127",
                    "CorpusId": 248157202
                },
                "corpusId": 248157202,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b45ea295f8bab57f48680df4610909131695fe64",
                "title": "Reinforcement learning on graph: A survey",
                "abstract": "Graph mining tasks arise from many different application domains, ranging from social networks, transportation, E-commerce, etc., which have been receiving great attention from the theoretical and algorithm design communities in recent years, and there has been some pioneering work using the hotly researched Reinforcement Learning (RL) techniques to address graph data mining tasks. However, these graph mining algorithms and RL models are dispersed in different research areas, which makes it hard to compare different algorithms with each other. In this survey, we provide a comprehensive overview of RL models and graph mining and generalize these algorithms to Graph Reinforcement Learning (GRL) as a unified formulation. We further discuss the applications of GRL methods across various domains and summarize the method description, open-source codes, and benchmark datasets of GRL methods. Finally, we propose possible important directions and challenges to be solved in the future. This is the latest work on a comprehensive survey of GRL literature, and this work provides a global view for scholars as well as a learning resource for scholars outside the domain. In addition, we create an online open-source for both interested scholars who want to enter this rapidly developing domain and experts who would like to compare GRL methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2153114137",
                        "name": "Mingshuo Nie"
                    },
                    {
                        "authorId": "3252139",
                        "name": "Dongming Chen"
                    },
                    {
                        "authorId": "2111215516",
                        "name": "Dongqi Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                ", 2021)\u2014any of the previous methods can be combined with SubGraphX (Yuan et al., 2021) to also produce explanations for the graph component of the input.",
                ", 2021) features and produce more realistic explanations R SPVIM (C4) (C5) Global variable importance measure using an efficient Not Relevant (Williamson and Feng, 2020) regression-based Shapley value estimator Python and R SubgraphX (C1) (C2) Explain GNNs by identifying important subgraphs Not Relevant (Yuan et al., 2021) (C5) using Shapley values as importance measures PyTorch SurrogateSHAP (C5) An XGBoost tree model is trained as a surrogate model Potentially Applicable (Messalas et al.",
                "Some present a new ver-034 sion of SHAP tailored to a certain type of input035 data\u2014e.g. graphs (Yuan et al., 2021) and text036 (Chen et al., 2020)\u2014or to specific models such037 as random forests (Lundberg et al., 2018).",
                "291\nFor models trained on graph data, especially 292 graph DNNs, Yuan et al. (2021) proposed to ex- 293 plain predictions by using Shapley values as a 294 measure of subgraph importance.",
                "For use cases involving graphs as480 part of multi-modal inputs\u2014e.g. modeling a social481 network (Wich et al., 2021)\u2014any of the previous482 methods can be combined with SubGraphX (Yuan483 et al., 2021) to also produce explanations for the484 graph component of the input.485\nWhen it comes to more sequence-to-sequence486 tasks such as question answering or machine trans-487 lation, SHAP-based methods seem in general not488 suitable as they are particularly tailored to classifi-489 cation settings."
            ],
            "citingPaper": {
                "paperId": "f72053903270d9a7f41108461ad04d5aa075218d",
                "externalIds": {
                    "ACL": "2022.coling-1.406",
                    "DBLP": "conf/coling/MoscaSTGG22",
                    "CorpusId": 252571112
                },
                "corpusId": 252571112,
                "publicationVenue": {
                    "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
                    "name": "International Conference on Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Linguistics",
                        "COLING"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/coling/"
                },
                "url": "https://www.semanticscholar.org/paper/f72053903270d9a7f41108461ad04d5aa075218d",
                "title": "SHAP-Based Explanation Methods: A Review for NLP Interpretability",
                "abstract": "Model explanations are crucial for the transparent, safe, and trustworthy deployment of machine learning models. The SHapley Additive exPlanations (SHAP) framework is considered by many to be a gold standard for local explanations thanks to its solid theoretical background and general applicability. In the years following its publication, several variants appeared in the literature\u2014presenting adaptations in the core assumptions and target applications. In this work, we review all relevant SHAP-based interpretability approaches available to date and provide instructive examples as well as recommendations regarding their applicability to NLP use cases.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1786389",
                        "name": "E. Mosca"
                    },
                    {
                        "authorId": "2178446",
                        "name": "F. Szigeti"
                    },
                    {
                        "authorId": "2187454523",
                        "name": "Stella Tragianni"
                    },
                    {
                        "authorId": "2187454784",
                        "name": "Daniel Gallagher"
                    },
                    {
                        "authorId": "146800020",
                        "name": "G. Groh"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                ", based on signals from gradients [19, 3], perturbed predictions [33, 16, 36, 23], decomposition [3, 21], etc.",
                "SubgraphX [36] employs Monte Carlo Tree Search (MCTS) to find connected subgraphs that preserve predictions as explanation.",
                "[36] constraints explanations as connected sub-graphs and conducts Monte Carlo tree search.",
                "For each group, existing methods can be further categorized as (1) self-explainable GNNs [38, 6], where the GNN can simultaneously give prediction and explanations on the prediction; and (2) post-hoc explanations [33, 16, 36], which adopt another model or strategy to provide explanations of a target GNN."
            ],
            "citingPaper": {
                "paperId": "533181183d2d76e752913f273a6d5ab3344829e0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-13733",
                    "DOI": "10.48550/arXiv.2205.13733",
                    "CorpusId": 249151859
                },
                "corpusId": 249151859,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/533181183d2d76e752913f273a6d5ab3344829e0",
                "title": "On Consistency in Graph Neural Network Interpretation",
                "abstract": "Uncovering rationales behind predictions of graph neural networks (GNNs) has received increasing attention over recent years. Instance-level GNN explanation aims to discover critical input elements, like nodes or edges, that the target GNN relies upon for making predictions. These identi\ufb01ed sub-structures can provide interpretations of GNN\u2019s behavior. Though various algorithms are proposed, most of them formalize this task by searching the minimal subgraph which can preserve original predictions. An inductive bias is deep-rooted in this framework: the same output cannot guarantee that two inputs are processed under the same rationale. Consequently, they have the danger of providing spurious explanations and fail to provide consistent explanations. Applying them to explain weakly-performed GNNs would further amplify these issues. To address the issues, we propose to obtain more faithful and consistent explanations of GNNs. After a close examination on predictions of GNNs from the causality perspective, we attribute spurious explanations to two typical reasons: confounding effect of latent variables like distribution shift, and causal factors distinct from the original input. Motivated by the observation that both confounding effects and diverse causal rationales are encoded in internal representations, we propose a simple yet effective countermeasure by aligning embeddings. This new objective can be incorporated into existing GNN explanation algorithms with no effort. We implement both a simpli\ufb01ed version based on absolute distance and a distribution-aware version based on anchors. Experiments on 5 datasets validate its effectiveness, and theoretical analysis shows that it is in effect optimizing a more faithful explanation objective in design, which further justi\ufb01es the proposed approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1999191869",
                        "name": "Tianxiang Zhao"
                    },
                    {
                        "authorId": "153640788",
                        "name": "Dongsheng Luo"
                    },
                    {
                        "authorId": "48505793",
                        "name": "Xiang Zhang"
                    },
                    {
                        "authorId": "2116430057",
                        "name": "Suhang Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Furthermore, it is worth exploring explainable graph neural network algorithms, such as SubgraphX [42], as it can help",
                "Furthermore, it is worth exploring explainable graph neural network algorithms, such as SubgraphX [42], as it can help\nresearchers analyze and explain the working process of GNNs to detect the malware by highlighting specious function call paths for automatic malware forensics."
            ],
            "citingPaper": {
                "paperId": "0914f2f1e9ad57a93ec713016bfeccdddbd351cd",
                "externalIds": {
                    "CorpusId": 246035884
                },
                "corpusId": 246035884,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0914f2f1e9ad57a93ec713016bfeccdddbd351cd",
                "title": "GNN-based Android Malware Detection with Jumping Knowledge",
                "abstract": "This paper presents a new Android malware detection method based on Graph Neural Networks (GNNs) with Jumping-Knowledge (JK). Android function call graphs (FCGs) consist of a set of program functions and their interprocedural calls. Thus, this paper proposes a GNN-based method for Android malware detection by capturing meaningful intraprocedural call path patterns. In addition, a Jumping-Knowledge technique is applied to minimize the effect of the over-smoothing problem, which is common in GNNs. The proposed method has been extensively evaluated using two benchmark datasets. The results demonstrate the superiority of our approach compared to baseline methods in terms of key classification metrics, which demonstrates the potential of GNNs in Android malware detection.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "151253960",
                        "name": "Wai Weng Lo"
                    },
                    {
                        "authorId": "3242014",
                        "name": "S. Layeghy"
                    },
                    {
                        "authorId": "2026368688",
                        "name": "Mohanad Sarhan"
                    },
                    {
                        "authorId": "2150528482",
                        "name": "Marcus Gallagher"
                    },
                    {
                        "authorId": "2105726298",
                        "name": "Marius Portmann"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Some GNN explainers also use motif knowledge to generate subgraphs to explain GNNs (Ying et al., 2019; Yuan et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "1f81d28bbea73dfb6738fb43d09da4cec8bafff2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-00529",
                    "CorpusId": 246442123
                },
                "corpusId": 246442123,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1f81d28bbea73dfb6738fb43d09da4cec8bafff2",
                "title": "Molecular Graph Representation Learning via Heterogeneous Motif Graph Construction",
                "abstract": "We consider feature representation learning prob-lem of molecular graphs. Graph Neural Networks have been widely used in feature representation learning of molecular graphs. However, most existing methods deal with molecular graphs indi-vidually while neglecting their connections, such as motif-level relationships. We propose a novel molecular graph representation learning method by constructing a heterogeneous motif graph to address this issue. In particular, we build a heterogeneous motif graph that contains motif nodes and molecular nodes. Each motif node corresponds to a motif extracted from molecules. Then, we propose a Heterogeneous Motif Graph Neural Network (HM-GNN) to learn feature representations for each node in the heterogeneous motif graph. Our heterogeneous motif graph also enables effective multi-task learning, especially for small molecular datasets. To address the potential ef\ufb01ciency issue, we propose to use an edge sampler, which can signi\ufb01cantly reduce computational resources usage. The experimental results show that our model consistently outperforms previous state-of-the-art models. Under multi-task settings, the promising performances of our methods on combined datasets shed light on a new learning paradigm for small molecular datasets. Finally, we show that our model achieves similar performances with signi\ufb01cantly less computational resources by using our edge sampler.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8873470",
                        "name": "Zhaoning Yu"
                    },
                    {
                        "authorId": "3920758",
                        "name": "Hongyang Gao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In this context, the most important features are those that lead to similar predictions once retained [24,52,33,54]."
            ],
            "citingPaper": {
                "paperId": "33037ce7a4024973a0203e7ac718c14068b18241",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-08815",
                    "CorpusId": 246904782
                },
                "corpusId": 246904782,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/33037ce7a4024973a0203e7ac718c14068b18241",
                "title": "GRAPHSHAP: Motif-based Explanations for Black-box Graph Classifiers",
                "abstract": "Most methods for explaining black-box classifiers (e.g., on tabular data, images, or time series) rely on measuring the impact that the removal/perturbation of features has on the model output. This forces the explanation language to match the classifier features space. However, when dealing with graph data, in which the basic features correspond essentially to the adjacency information describing the graph structure (i.e., the edges), this matching between features space and explanation language might not be appropriate. In this regard, we argue that (i) a good explanation method for graph classification should be fully agnostic with respect to the internal representation used by the black-box; and (ii) a good explanation language for graph classification tasks should be represented by higher-order structures, such as motifs. The need to decouple the feature space (edges) from the explanation space (motifs) is thus a major challenge towards developing actionable explanations for graph classification tasks. In this paper we introduce GRAPHSHAP, a Shapley-based approach able to provide motif-based explanations for black-box graph classifiers, assuming no knowledge whatsoever about the model or its training data: the only requirement is that the black-box can be queried at will. For the sake of computational efficiency we explore a progressive approximation strategy and show how a simple kernel can efficiently approximate explanation scores, thus allowing GRAPHSHAP to scale on scenarios with a large explanation space (i.e., large number of motifs). We devise a synthetic dataset generator with artificially injected motifs in order to empirically compare different masking approaches and to demonstrate that the proposed kernel is able to approximate the exact Shapley values with a computational complexity that is linear with respect to the number of explained features. Furthermore, we introduce additional auxiliary components such as a custom graph convolutional layer and algorithms for motif mining and ranking. Finally, we test GRAPHSHAP on a real-world brain-network dataset consisting of patients affected by Autism Spectrum Disorder and a control group. Our experiments highlight how the classification provided by a black-box model can be effectively explained by few connectomics patterns.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "26582424",
                        "name": "A. Perotti"
                    },
                    {
                        "authorId": "46726748",
                        "name": "P. Bajardi"
                    },
                    {
                        "authorId": "1705764",
                        "name": "F. Bonchi"
                    },
                    {
                        "authorId": "2735649",
                        "name": "A. Panisson"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "17f42d0b5b2390ce6e6980d766abced054a31aa9",
                "externalIds": {
                    "DOI": "10.1016/j.cej.2022.136669",
                    "CorpusId": 248459060
                },
                "corpusId": 248459060,
                "publicationVenue": {
                    "id": "b815d078-dda4-4d10-b862-9277eb4f0d4e",
                    "name": "Chemical Engineering Journal",
                    "type": "journal",
                    "alternate_names": [
                        "Chem Eng J",
                        "The Chemical Engineering Journal"
                    ],
                    "issn": "1385-8947",
                    "alternate_issns": [
                        "0300-9467"
                    ],
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/601273/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/chemical-engineering-journal",
                        "http://www.sciencedirect.com/science/journal/03009467",
                        "http://www.sciencedirect.com/science/journal/13858947"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/17f42d0b5b2390ce6e6980d766abced054a31aa9",
                "title": "Deep learning to catalyze inverse molecular design",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1704330636",
                        "name": "A. Alshehri"
                    },
                    {
                        "authorId": "2647121",
                        "name": "F. You"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026be developed in Section 7 belongs to the explainablility category, where the objective is to discover qualitative justifications on the learning outcome of a GCN; see (Luo et al., 2020; Lin et al., 2021; Schlichtkrull et al., 2021; Ying et al., 2019; Yuan et al., 2021) and the references therein.",
                "The statistic model to be developed in Section 7 belongs to the explainablility category, where the objective is to discover qualitative justifications on the learning outcome of a GCN; see (Luo et al., 2020; Lin et al., 2021; Schlichtkrull et al., 2021; Ying et al., 2019; Yuan et al., 2021) and the references therein."
            ],
            "citingPaper": {
                "paperId": "14b610eeacb97d2ad5b40b8def1fcaadc52ee217",
                "externalIds": {
                    "DBLP": "conf/icml/TaoWD22",
                    "CorpusId": 250242203
                },
                "corpusId": 250242203,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/14b610eeacb97d2ad5b40b8def1fcaadc52ee217",
                "title": "Cross-Space Active Learning on Graph Convolutional Networks",
                "abstract": "This paper formalizes cross-space active learning on a graph convolutional network (GCN). The objective is to attain the most accurate hypothesis available in any of the instance spaces generated by the GCN. Subject to the objective, the chal-lenge is to minimize the label cost , measured in the number of vertices whose labels are requested. Our study covers both budget algorithms which terminate after a designated number of label requests, and verifiable algorithms which terminate only after having found an accurate hypothesis. A new separation in label complexity between the two algorithm types is established. The separation is unique to GCNs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144779196",
                        "name": "Yufei Tao"
                    },
                    {
                        "authorId": "1664776313",
                        "name": "Hao Wu"
                    },
                    {
                        "authorId": "148032393",
                        "name": "Shiyuan Deng"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": null,
                "externalIds": null,
                "corpusId": "251732960",
                "publicationVenue": null,
                "url": null,
                "title": "DIRECTIONAL FEATURE INTERACTIONS",
                "abstract": null,
                "year": 2022,
                "authors": []
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "More generally, as also reported in Yuan et al. (2021), studying connected subgraphs results in more natural motifs compared to the motifs obtained without the connectedness constraint.",
                "We compare the proposed method with popular post-hoc explanation techniques including the GNN-Explainer (Ying et al. 2019), PGEExplainer (Luo et al. 2020), GradCAM (Pope et al. 2019) and SubgraphX (Yuan et al. 2021)2.",
                "(Ying et al. 2019; Luo et al. 2020; Funke, Khosla, and Anand 2021; Loveland et al. 2021;\nSchlichtkrull, Cao, and Titov 2021; Yuan et al. 2021; Perotti et al. 2022)."
            ],
            "citingPaper": {
                "paperId": "1067830966ce4f3c3e3b039adff6c8bc9be988cd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-14402",
                    "DOI": "10.48550/arXiv.2209.14402",
                    "CorpusId": 252596276
                },
                "corpusId": 252596276,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1067830966ce4f3c3e3b039adff6c8bc9be988cd",
                "title": "Learning to Explain Graph Neural Networks",
                "abstract": "Graph Neural Networks (GNNs) are a popular class of ma- chine learning models. Inspired by the learning to explain (L2X) paradigm, we propose L2 X G NN , a framework for ex- plainable GNNs which provides faithful explanations by design. L2 X G NN learns a mechanism for selecting explanatory subgraphs (motifs) which are exclusively used in the GNNs message-passing operations. L2 X G NN is able to select, for each input graph, a subgraph with speci\ufb01c properties such as being sparse and connected. Imposing such constraints on the motifs often leads to more interpretable and effective explanations. Experiments on several datasets suggest that L2 X G NN achieves the same classi\ufb01cation accuracy as baseline methods using the entire input graph while ensuring that only the provided explanations are used to make predictions. Moreover, we show that L2 X G NN is able to identify motifs responsible for the graph\u2019s properties it is intended to predict.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2275344",
                        "name": "G. Serra"
                    },
                    {
                        "authorId": "2780262",
                        "name": "Mathias Niepert"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Recently, it has been adopted for explanations of machine learning models (Lundberg & Lee, 2017; S\u030ctrumbelj & Kononenko, 2014; Sundararajan & Najmi, 2020; Wang et al., 2021a; Zhang et al., 2021; Frye et al., 2020; Yuan et al., 2021) or feature importance (Covert et al., 2020).",
                "Recently, it has been adopted for explanations of machine learning models (Lundberg & Lee, 2017; \u0160trumbelj & Kononenko, 2014; Sundararajan & Najmi, 2020; Wang et al., 2021a; Zhang et al., 2021; Frye et al., 2020; Yuan et al., 2021) or feature importance (Covert et al."
            ],
            "citingPaper": {
                "paperId": "89756b84e3d6537d66083ce38d107864fac6750e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-17426",
                    "DOI": "10.48550/arXiv.2210.17426",
                    "CorpusId": 253237014
                },
                "corpusId": 253237014,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/89756b84e3d6537d66083ce38d107864fac6750e",
                "title": "Consistent and Truthful Interpretation with Fourier Analysis",
                "abstract": "For many interdisciplinary \ufb01elds, ML interpretations need to be consistent with what-if scenarios related to the current case, i.e., if one factor changes, how does the model react? Although the attribution methods are supported by the elegant axiomatic systems, they mainly focus on individual inputs, and are generally inconsistent. To support what-if scenarios, we introduce a new notion called truthful interpretation, and apply Fourier analysis of Boolean functions to get rigorous guarantees. Experimental results show that for neighborhoods with various radii, our method achieves 2x - 50x lower interpretation error compared with the other methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108464546",
                        "name": "Yifan Zhang"
                    },
                    {
                        "authorId": "2110436433",
                        "name": "Haowei He"
                    },
                    {
                        "authorId": "1752381",
                        "name": "Yang Yuan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "bd0769c931e27ba638cc58b20f2dcfa9d16473d9",
                "externalIds": {
                    "CorpusId": 253456746
                },
                "corpusId": 253456746,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bd0769c931e27ba638cc58b20f2dcfa9d16473d9",
                "title": "Efficient Higher-order Subgraph Attribution via Message Passing",
                "abstract": "Explaining graph neural networks (GNNs) has become more and more important recently. Higher-order interpretation schemes, such as GNN-LRP (layer-wise relevance propagation for GNN), emerged as powerful tools for unraveling how different features interact thereby contributing to explaining GNNs. GNN-LRP gives a relevance attribution of walks between nodes at each layer, and the subgraph attribution is expressed as a sum over exponentially many such walks. In this work, we demonstrate that such exponential complexity can be avoided. In particular, we propose novel algorithms that enable to attribute subgraphs with GNN-LRP in linear-time (w.r.t. the network depth). Our algorithms are derived via message passing techniques that make use of the distributive property, thereby directly computing quantities for higher-order explanations. We further adapt our efficient algorithms to compute a generalization of subgraph attributions that also takes into account the neighboring graph features. Experimental results show the significant acceler-ation of the proposed algorithms and demonstrate the high usefulness and scalability of our novel generalized subgraph attribution method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2062775158",
                        "name": "Pin Xiong"
                    },
                    {
                        "authorId": "90387439",
                        "name": "Thomas Schnake"
                    },
                    {
                        "authorId": "144535526",
                        "name": "G. Montavon"
                    },
                    {
                        "authorId": "2113612432",
                        "name": "Klaus-Robert M\u00fcller"
                    },
                    {
                        "authorId": "2055678647",
                        "name": "S. Nakajima"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Several approaches are proposed to explain the predictions of GNNs, such as SubgraphX [293], RC-Explainer [294], SE-GNN [295] and ProtGNN [296], etc."
            ],
            "citingPaper": {
                "paperId": "1688452a7f88530d4679b2d233e059507c630348",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-14997",
                    "DOI": "10.48550/arXiv.2211.14997",
                    "CorpusId": 254044156
                },
                "corpusId": 254044156,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1688452a7f88530d4679b2d233e059507c630348",
                "title": "A Comprehensive Survey on Enterprise Financial Risk Analysis: Problems, Methods, Spotlights and Applications",
                "abstract": "\u2014Enterprise \ufb01nancial risk analysis aims at predicting the future \ufb01nancial risk of enterprises. Due to its wide and signi\ufb01cant application, enterprise \ufb01nancial risk analysis has always been the core research topic in the \ufb01elds of Finance and Management. Although there are already some valuable and impressive surveys on enterprise risk analysis from the perspective of Finance and Management, these surveys introduce approaches in a relatively isolated way and lack recent advances in enterprise \ufb01nancial risk analysis. Due to the rapid expansion of the enterprise risk research area, especially from the Computer Science and Big Data perspective, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing enterprise \ufb01nancial risk studies, i.e. to summarize and interpret the problems, methodologies, spotlights and applications of enterprise \ufb01nancial risk analysis in a comprehensive way, which may help readers to have a better understanding of the current research status and ideas. Unlike previous surveys, this paper attempts to provide a systematic literature survey of enterprise risk analysis approaches from the \ufb01elds of Finance, Management, as well as Computer Science, which reviews more than 300 representative articles in the past almost 50 years (from 1968 to 2022). In particular, we \ufb01rst introduce the problems of the types, granu- larity, intelligence and evaluation metrics of enterprise \ufb01nancial risk, and summarize the representative works in terms of them perspectively. Then, we compare the analysis methods used to learn enterprise \ufb01nancial risk, and summarize the spotlights of the most representative works. Finally, the applications of enterprise risk analysis are also brie\ufb02y introduced. Our goal is to clarify current cutting-edge research and its possible future directions to model enterprise risk, aiming to fully understand the mechanisms of enterprise risk generation and contagion and its application on corporate governance, \ufb01nancial institution and government regulation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2110262691",
                        "name": "Yu Zhao"
                    },
                    {
                        "authorId": "94613261",
                        "name": "Huaming Du"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Researchers are still working on opening the \u2019black-box\u2019 of embedding-based methods [9, 10]."
            ],
            "citingPaper": {
                "paperId": "608f22c68fd62a99fb656844cc9c9db2b684ff15",
                "externalIds": {
                    "CorpusId": 263726151
                },
                "corpusId": 263726151,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/608f22c68fd62a99fb656844cc9c9db2b684ff15",
                "title": "From competition to collaboration: Ensembling similarity-based heuristics for supervised link prediction in biological graphs",
                "abstract": ". Link prediction is a fundamental problem in the field of graph mining. The aim of link prediction is to infer/discover unobserved links in graphs. Link prediction in biological graphs is highly challenging. There exist many similarity-based methods in the literature for link prediction. These methods compete for victory in graphs from various domains. Unfortunately, they are efficient only in some specific graphs, and no one wins in all graphs. In this paper, we study some well-known similarity-based methods and consider them as independent features to define a feature set. The feature set is then used to train traditional supervised learning methods for link prediction in biological graphs. We evaluate the methods on ten biological graphs from different organisms. Experimental results show that the similarity-based methods collaboratively improve prediction performance, and are even comparable to high-performing embedding-based methods in some biological graphs. We compute the importance score of similarity-based features in order to explain the leading features in a graph.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2256126144",
                        "name": "Md Kamrul Islam"
                    },
                    {
                        "authorId": "3172643",
                        "name": "Sabeur Aridhi"
                    },
                    {
                        "authorId": "1405255202",
                        "name": "Malika Smail-Tabbone"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                ", 2020), PGMExplainer (Vu & Thai, 2020), SubgraphX (Yuan et al., 2021), XGNN (Yuan et al.",
                "Specially, for subgraph-based method SubgraphX (Yuan et al., 2021), we pick the explainable subgraph out, then assign the edges in this subgraph instead of nodes as the explanation.",
                "Another recent study proposes SubgraphX (Yuan et al., 2021), which employs a search algorithm to explore and identify subgraphs with high Shapley scores.",
                "Next, the recent study SubgraphX (Yuan et al., 2021) proposes to explain GNNs via subgraphs.",
                "\u2026our FlowX with eight baselines, including GradCAM (Pope et al., 2019), DeepLIFT (Shrikumar et al., 2017), GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), PGMExplainer (Vu & Thai, 2020), SubgraphX (Yuan et al., 2021), GNN-GI (Schnake et al., 2020), GNN-LRP (Schnake et al., 2020).",
                "\u2026methods have been proposed to explain the predictions of GNNs, such as GraphLime (Huang et al., 2020), GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), PGMExplainer (Vu & Thai, 2020), SubgraphX (Yuan et al., 2021), XGNN (Yuan et al., 2020b), and GraphSVX (Duval & Malliaros, 2021).",
                "Recently, several techniques have been proposed to explain GNNs, such as XGNN (Yuan et al., 2020b), GNNExplainer (Ying et al., 2019), PGExplainer (Luo et al., 2020), and SubgraphX (Yuan et al., 2021), etc.",
                ", 2020), PGMExplainer (Vu & Thai, 2020), SubgraphX (Yuan et al., 2021), GNN-GI (Schnake et al."
            ],
            "citingPaper": {
                "paperId": "421b495c2379182b8874ce91af5ab1121d356834",
                "externalIds": {
                    "CorpusId": 248384864
                },
                "corpusId": 248384864,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/421b495c2379182b8874ce91af5ab1121d356834",
                "title": "F LOW X: T OWARDS E XPLAINABLE G RAPH N EURAL N ETWORKS VIA M ESSAGE F LOWS",
                "abstract": "We investigate the explainability of graph neural networks (GNNs) as a step towards elucidating their working mechanisms. While most current methods focus on explaining graph nodes, edges, or features, we argue that, as the inherent functional mechanism of GNNs, message \ufb02ows are more natural for performing explainability. To this end, we propose a novel method here, known as FlowX, to explain GNNs by identifying important message \ufb02ows. To quantify the importance of \ufb02ows, we propose to follow the philosophy of Shapley values from cooperative game theory. To tackle the complexity of computing all coalitions\u2019 marginal contributions, we propose an approximation scheme to compute Shapley-like values as initial assessments of further redistribution training. We then propose a learning algorithm to train \ufb02ow scores and improve explainability. Experimental studies on both synthetic and real-world datasets demonstrate that our proposed FlowX leads to improved explainability of GNNs.",
                "year": 2021,
                "authors": []
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "SubgraphX [37] uses Monte Carlo tree search and Shapley value as a score function to find the best connected subgraphs as explanations for GNNs. Causal Screening [31] is another search-based method, but it uses greedy search and causality measure to generate the explanations.",
                "Some recent works, such as SubgraphX [37] and Causal Screening [31], design the search criteria and use search-based methods to solve the optimization problem.",
                "To enhance the interpretability of GNNs, a line of works [34, 17, 30, 37, 31] focused on developing GNN explainers.",
                "SubgraphX [37] uses Monte Carlo tree search and Shapley value as a score function to find the best connected subgraphs as explanations for GNNs."
            ],
            "citingPaper": {
                "paperId": "9d557d85c206ddbf1b9fb1ad0c848a64e0973360",
                "externalIds": {
                    "DBLP": "conf/nips/ShanSZLL21",
                    "CorpusId": 245117010
                },
                "corpusId": 245117010,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9d557d85c206ddbf1b9fb1ad0c848a64e0973360",
                "title": "Reinforcement Learning Enhanced Explainer for Graph Neural Networks",
                "abstract": "Graph neural networks (GNNs) have recently emerged as revolutionary technolo-gies for machine learning tasks on graphs. In GNNs, the graph structure is generally incorporated with node representation via the message passing scheme, making the explanation much more challenging. Given a trained GNN model, a GNN explainer aims to identify a most in\ufb02uential subgraph to interpret the prediction of an instance (e.g., a node or a graph), which is essentially a combinatorial optimization problem over graph. The existing works solve this problem by continuous relaxation or search-based heuristics. But they suffer from key issues such as violation of message passing and hand-crafted heuristics, leading to inferior interpretability. To address these issues, we propose a RL-enhanced GNN explainer, RG-Explainer , which consists of three main components: starting point selection, iterative graph generation and stopping criteria learning. RG-Explainer could construct a connected explanatory subgraph by sequentially adding nodes from the boundary of the current generated graph, which is consistent with the message passing scheme. Further, we design an effective seed locator to select the starting point, and learn stopping criteria to generate superior explanations. Extensive experiments on both synthetic and real datasets show that RG-Explainer outperforms state-of-the-art GNN explainers. Moreover, RG-Explainer can be applied in the inductive setting, demonstrating its better generalization ability.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145663545",
                        "name": "Caihua Shan"
                    },
                    {
                        "authorId": "2115383310",
                        "name": "Yifei Shen"
                    },
                    {
                        "authorId": "2118390430",
                        "name": "Yao Zhang"
                    },
                    {
                        "authorId": "47875796",
                        "name": "Xiang Li"
                    },
                    {
                        "authorId": "2119081394",
                        "name": "Dongsheng Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "(2) Parametric explanation methods [6, 19, 51, 52] additionally train a parametrized explainer model to generate the saliency maps or explanatory subgraphs for individual instances."
            ],
            "citingPaper": {
                "paperId": "8a8f853c9604bbe068c686e7d38b2c7560c36a1b",
                "externalIds": {
                    "DBLP": "conf/nips/WangWZHC21",
                    "CorpusId": 247405839
                },
                "corpusId": 247405839,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8a8f853c9604bbe068c686e7d38b2c7560c36a1b",
                "title": "Towards Multi-Grained Explainability for Graph Neural Networks",
                "abstract": "When a graph neural network (GNN) made a prediction, one raises question about explainability: \u201cWhich fraction of the input graph is most influential to the model\u2019s decision?\u201d Producing an answer requires understanding the model\u2019s inner workings in general and emphasizing the insights on the decision for the instance at hand. Nonetheless, most of current approaches focus only on one aspect: (1) local explainability, which explains each instance independently, thus hardly exhibits the class-wise patterns; and (2) global explainability, which systematizes the globally important patterns, but might be trivial in the local context. This dichotomy limits the flexibility and effectiveness of explainers greatly. A performant paradigm towards multi-grained explainability is until-now lacking and thus a focus of our work. In this work, we exploit the pre-training and fine-tuning idea to develop our explainer and generate multi-grained explanations. Specifically, the pre-training phase accounts for the contrastivity among different classes, so as to highlight the class-wise characteristics from a global view; afterwards, the fine-tuning phase adapts the explanations in the local context. Experiments on both synthetic and real-world datasets show the superiority of our explainer, in terms of AUC on explaining graph classification over the leading baselines. Our codes and datasets are available at https://github.com/Wuyxin/ReFine.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "98285513",
                        "name": "Xiang Wang"
                    },
                    {
                        "authorId": "10593442",
                        "name": "Yingmin Wu"
                    },
                    {
                        "authorId": "2153659066",
                        "name": "An Zhang"
                    },
                    {
                        "authorId": "7792071",
                        "name": "Xiangnan He"
                    },
                    {
                        "authorId": "143779329",
                        "name": "Tat-Seng Chua"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "SubgraphX uses Monte Carlo tree search to select the most important subgraph with Shapley value-based formulation.",
                "Several works [9], [21], [23], [24], [26], [39], [51]\u2013 [53] theoretically and empirically compared GNN-based interpretability methods.",
                "We shall discuss and compare 19 recent GNN interpretability methods from the aforementioned categories: GNNExplainer [36], PGExplainer [37], GraphMask [38], SubgraphX [39], PGMExplainer [40], RelEx [41], GraphLime [42], RCExplainer [43], DnX [44], GCFExplainer [45], CF2 [46], SA [26], GuidedBP [26], CAM [21], Grad-CAM [47], LRP [48], GNNLRP [49], ExcitationBP [21], and XGNN [50].",
                "We shall discuss and compare 19 recent GNN interpretability methods from the aforementioned categories: GNNExplainer [36], PGExplainer [37], GraphMask [38], SubgraphX [39], PGMExplainer [40], RelEx [41], GraphLime [42], RCExplainer [43], DnX [44], GCFExplainer [45], CF(2) [46], SA [26], GuidedBP [26], CAM [21], Grad-CAM [47], LRP [48], GNNLRP [49], ExcitationBP [21], and XGNN [50]."
            ],
            "citingPaper": {
                "paperId": "a4fadb1331a3faee62fdd90c226417a593e35b51",
                "externalIds": {
                    "CorpusId": 260890101
                },
                "corpusId": 260890101,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a4fadb1331a3faee62fdd90c226417a593e35b51",
                "title": "Interpretability Methods for Graph Neural Networks",
                "abstract": "\u2014The emerging graph neural network models (GNNs) have demonstrated great potential and success for downstream graph machine learning tasks, such as graph and node classification, link prediction, entity resolution, and question answering. However, neural networks are \u201cblack-box\u201d \u2013 it is difficult to understand which aspects of the input data and the model guide the decisions of the network. Recently, several interpretability methods for GNNs have been developed, aiming at improving the model\u2019s transparency and fairness, thus making them trustworthy in decision-critical applications, leading to democratization of deep learning approaches and easing their adoptions. The tutorial is designed to offer an overview of the state-of-the-art interpretability techniques for graph neural networks, including their taxonomy, evaluation metrics, benchmarking study, and ground truth. In addition, the tutorial discusses open problems and important research directions.",
                "year": null,
                "authors": [
                    {
                        "authorId": "2108514592",
                        "name": "Arijit Khan"
                    },
                    {
                        "authorId": "2231550366",
                        "name": "Ehsan B. Mobaraki"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": null,
                "externalIds": null,
                "corpusId": "259933538",
                "publicationVenue": null,
                "url": null,
                "title": "L EARNING F AIR G RAPH R EPRESENTATIONS VIA A UTOMATED D ATA A UGMENTATIONS",
                "abstract": null,
                "year": null,
                "authors": []
            }
        }
    ]
}