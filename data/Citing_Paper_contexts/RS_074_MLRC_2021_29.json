{
    "offset": 0,
    "data": [
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Starting with explicit intermediate shape representations, such as voxels [38, 19] and meshes [42], which lack photorealism and are memory-inefficient, researchers have recently shifted towards using implicit functions [44, 36, 9] along with physical rendering processes [60, 37] as intrinsic 3D inductive biases.",
                "Inspired by the success of Generative Adversarial Networks (GAN)[16] in generating photorealistic images[24, 5, 26], researchers have been making efforts towards 3D-aware generation [38, 19, 42]."
            ],
            "citingPaper": {
                "paperId": "49f4422eeae7cab13bf0603bb0a42f6f9eec4fa2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-04410",
                    "ArXiv": "2309.04410",
                    "DOI": "10.48550/arXiv.2309.04410",
                    "CorpusId": 261660418
                },
                "corpusId": 261660418,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/49f4422eeae7cab13bf0603bb0a42f6f9eec4fa2",
                "title": "DeformToon3D: Deformable 3D Toonification from Neural Radiance Fields",
                "abstract": "In this paper, we address the challenging problem of 3D toonification, which involves transferring the style of an artistic domain onto a target 3D face with stylized geometry and texture. Although fine-tuning a pre-trained 3D GAN on the artistic domain can produce reasonable performance, this strategy has limitations in the 3D domain. In particular, fine-tuning can deteriorate the original GAN latent space, which affects subsequent semantic editing, and requires independent optimization and storage for each new style, limiting flexibility and efficient deployment. To overcome these challenges, we propose DeformToon3D, an effective toonification framework tailored for hierarchical 3D GAN. Our approach decomposes 3D toonification into subproblems of geometry and texture stylization to better preserve the original latent space. Specifically, we devise a novel StyleField that predicts conditional 3D deformation to align a real-space NeRF to the style space for geometry stylization. Thanks to the StyleField formulation, which already handles geometry stylization well, texture stylization can be achieved conveniently via adaptive style mixing that injects information of the artistic domain into the decoder of the pre-trained 3D GAN. Due to the unique design, our method enables flexible style degree control and shape-texture-specific style swap. Furthermore, we achieve efficient training without any real-world 2D-3D training pairs but proxy samples synthesized from off-the-shelf 2D toonification models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2107967578",
                        "name": "Junzhe Zhang"
                    },
                    {
                        "authorId": "2055985023",
                        "name": "Yushi Lan"
                    },
                    {
                        "authorId": "2159711748",
                        "name": "Shuai Yang"
                    },
                    {
                        "authorId": "1568986485",
                        "name": "Fangzhou Hong"
                    },
                    {
                        "authorId": "2238891543",
                        "name": "Quan Wang"
                    },
                    {
                        "authorId": "1751452",
                        "name": "C. Yeo"
                    },
                    {
                        "authorId": "2244737559",
                        "name": "Ziwei Liu"
                    },
                    {
                        "authorId": "1717179",
                        "name": "Chen Change Loy"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Later approaches [10, 36, 58, 48, 68, 37, 9] successfully extract the 3D representations by the 3D generator and refine the output using image-based CNN networks."
            ],
            "citingPaper": {
                "paperId": "1641b7023e945a87d43b4ab6895549821d809fb2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-02434",
                    "ArXiv": "2309.02434",
                    "DOI": "10.48550/arXiv.2309.02434",
                    "CorpusId": 261556746
                },
                "corpusId": 261556746,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1641b7023e945a87d43b4ab6895549821d809fb2",
                "title": "ReliTalk: Relightable Talking Portrait Generation from a Single Video",
                "abstract": "Recent years have witnessed great progress in creating vivid audio-driven portraits from monocular videos. However, how to seamlessly adapt the created video avatars to other scenarios with different backgrounds and lighting conditions remains unsolved. On the other hand, existing relighting studies mostly rely on dynamically lighted or multi-view data, which are too expensive for creating video portraits. To bridge this gap, we propose ReliTalk, a novel framework for relightable audio-driven talking portrait generation from monocular videos. Our key insight is to decompose the portrait's reflectance from implicitly learned audio-driven facial normals and images. Specifically, we involve 3D facial priors derived from audio features to predict delicate normal maps through implicit functions. These initially predicted normals then take a crucial part in reflectance decomposition by dynamically estimating the lighting condition of the given video. Moreover, the stereoscopic face representation is refined using the identity-consistent loss under simulated multiple lighting conditions, addressing the ill-posed problem caused by limited views available from a single monocular video. Extensive experiments validate the superiority of our proposed framework on both real and synthetic datasets. Our code is released in https://github.com/arthur-qiu/ReliTalk.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2237987570",
                        "name": "Haonan Qiu"
                    },
                    {
                        "authorId": "2111607225",
                        "name": "Zhaoxi Chen"
                    },
                    {
                        "authorId": "2127773416",
                        "name": "Yuming Jiang"
                    },
                    {
                        "authorId": "145798292",
                        "name": "Hang Zhou"
                    },
                    {
                        "authorId": "2163862484",
                        "name": "Xiangyu Fan"
                    },
                    {
                        "authorId": "2165477990",
                        "name": "Lei Yang"
                    },
                    {
                        "authorId": "2238148650",
                        "name": "Wayne Wu"
                    },
                    {
                        "authorId": "2237482886",
                        "name": "Ziwei Liu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "8aa2bddbea68bcdcf08f2f0ffb3ec829e27bddd8",
                "externalIds": {
                    "ArXiv": "2308.14078",
                    "DBLP": "journals/corr/abs-2308-14078",
                    "DOI": "10.48550/arXiv.2308.14078",
                    "CorpusId": 261244080
                },
                "corpusId": 261244080,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8aa2bddbea68bcdcf08f2f0ffb3ec829e27bddd8",
                "title": "Sparse3D: Distilling Multiview-Consistent Diffusion for Object Reconstruction from Sparse Views",
                "abstract": "Reconstructing 3D objects from extremely sparse views is a long-standing and challenging problem. While recent techniques employ image diffusion models for generating plausible images at novel viewpoints or for distilling pre-trained diffusion priors into 3D representations using score distillation sampling (SDS), these methods often struggle to simultaneously achieve high-quality, consistent, and detailed results for both novel-view synthesis (NVS) and geometry. In this work, we present Sparse3D, a novel 3D reconstruction method tailored for sparse view inputs. Our approach distills robust priors from a multiview-consistent diffusion model to refine a neural radiance field. Specifically, we employ a controller that harnesses epipolar features from input views, guiding a pre-trained diffusion model, such as Stable Diffusion, to produce novel-view images that maintain 3D consistency with the input. By tapping into 2D priors from powerful image diffusion models, our integrated model consistently delivers high-quality results, even when faced with open-world objects. To address the blurriness introduced by conventional SDS, we introduce the category-score distillation sampling (C-SDS) to enhance detail. We conduct experiments on CO3DV2 which is a multi-view dataset of real-world objects. Both quantitative and qualitative evaluations demonstrate that our approach outperforms previous state-of-the-art works on the metrics regarding NVS and geometry reconstruction.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "100837133",
                        "name": "Zi-Xin Zou"
                    },
                    {
                        "authorId": "10405156",
                        "name": "Weihao Cheng"
                    },
                    {
                        "authorId": "2157429990",
                        "name": "Yan-Pei Cao"
                    },
                    {
                        "authorId": "2550389",
                        "name": "Shi-Sheng Huang"
                    },
                    {
                        "authorId": "2187307826",
                        "name": "Ying Shan"
                    },
                    {
                        "authorId": "1390851418",
                        "name": "Songiie Zhang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Prior approaches utilize GANs [19, 33, 55] to synthesize data for tasks including classification [2, 8, 43, 72, 75], 3D vision [21, 52, 64, 86], 2D segmentation [44, 74, 87], dense visual alignment [54]; or diffusion models [27, 59] for data augmentation [73] or as synthetic data for few-shot learning [25]."
            ],
            "citingPaper": {
                "paperId": "1254f59b09b5a881ac09e49392f79af0dabff95f",
                "externalIds": {
                    "ArXiv": "2308.12288",
                    "DBLP": "journals/corr/abs-2308-12288",
                    "DOI": "10.48550/arXiv.2308.12288",
                    "CorpusId": 261076274
                },
                "corpusId": 261076274,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1254f59b09b5a881ac09e49392f79af0dabff95f",
                "title": "CHORUS: Learning Canonicalized 3D Human-Object Spatial Relations from Unbounded Synthesized Images",
                "abstract": "We present a method for teaching machines to understand and model the underlying spatial common sense of diverse human-object interactions in 3D in a self-supervised way. This is a challenging task, as there exist specific manifolds of the interactions that can be considered human-like and natural, but the human pose and the geometry of objects can vary even for similar interactions. Such diversity makes the annotating task of 3D interactions difficult and hard to scale, which limits the potential to reason about that in a supervised way. One way of learning the 3D spatial relationship between humans and objects during interaction is by showing multiple 2D images captured from different viewpoints when humans interact with the same type of objects. The core idea of our method is to leverage a generative model that produces high-quality 2D images from an arbitrary text prompt input as an\"unbounded\"data generator with effective controllability and view diversity. Despite its imperfection of the image quality over real images, we demonstrate that the synthesized images are sufficient to learn the 3D human-object spatial relations. We present multiple strategies to leverage the synthesized images, including (1) the first method to leverage a generative image model for 3D human-object spatial relation learning; (2) a framework to reason about the 3D spatial relations from inconsistent 2D cues in a self-supervised manner via 3D occupancy reasoning with pose canonicalization; (3) semantic clustering to disambiguate different types of interactions with the same object types; and (4) a novel metric to assess the quality of 3D spatial learning of interaction.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2218014742",
                        "name": "Sookwan Han"
                    },
                    {
                        "authorId": "7996087",
                        "name": "H. Joo"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Some works go further [10, 17] and reconstruct some simple 3D models from a single image."
            ],
            "citingPaper": {
                "paperId": "e8a8572130683c74f2637446ac9d3a0668ebe711",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-11356",
                    "ArXiv": "2308.11356",
                    "DOI": "10.48550/arXiv.2308.11356",
                    "CorpusId": 261065099
                },
                "corpusId": 261065099,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e8a8572130683c74f2637446ac9d3a0668ebe711",
                "title": "Semantic RGB-D Image Synthesis",
                "abstract": "Collecting diverse sets of training images for RGB-D semantic image segmentation is not always possible. In particular, when robots need to operate in privacy-sensitive areas like homes, the collection is often limited to a small set of locations. As a consequence, the annotated images lack diversity in appearance and approaches for RGB-D semantic image segmentation tend to overfit the training data. In this paper, we thus introduce semantic RGB-D image synthesis to address this problem. It requires synthesising a realistic-looking RGB-D image for a given semantic label map. Current approaches, however, are uni-modal and cannot cope with multi-modal data. Indeed, we show that extending uni-modal approaches to multi-modal data does not perform well. In this paper, we therefore propose a generator for multi-modal data that separates modal-independent information of the semantic layout from the modal-dependent information that is needed to generate an RGB and a depth image, respectively. Furthermore, we propose a discriminator that ensures semantic consistency between the label maps and the generated images and perceptual similarity between the real and generated images. Our comprehensive experiments demonstrate that the proposed method outperforms previous uni-modal methods by a large margin and that the accuracy of an approach for RGB-D semantic segmentation can be significantly improved by mixing real and generated images during training.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2118159309",
                        "name": "Shijie Li"
                    },
                    {
                        "authorId": "2119920623",
                        "name": "Rongqing Li"
                    },
                    {
                        "authorId": "145689714",
                        "name": "Juergen Gall"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Conversely, other approaches [36, 58] use a single channel for illumination, which only enforces white illumination."
            ],
            "citingPaper": {
                "paperId": "b3e68977640d53b3a8b8d9441b92c15d4a5e5d86",
                "externalIds": {
                    "DBLP": "journals/vc/HaCSY23",
                    "DOI": "10.1007/s00371-023-02948-1",
                    "CorpusId": 260034144
                },
                "corpusId": 260034144,
                "publicationVenue": {
                    "id": "9a037417-d032-481a-bb54-de987ec2138b",
                    "name": "The Visual Computer",
                    "type": "journal",
                    "alternate_names": [
                        "Vis Comput"
                    ],
                    "issn": "0178-2789",
                    "url": "https://link.springer.com/journal/371"
                },
                "url": "https://www.semanticscholar.org/paper/b3e68977640d53b3a8b8d9441b92c15d4a5e5d86",
                "title": "Learning to disentangle latent physical factors of deformable faces",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2224248161",
                        "name": "Inwoo Ha"
                    },
                    {
                        "authorId": "1947373",
                        "name": "Hyun Sung Chang"
                    },
                    {
                        "authorId": "1706682",
                        "name": "Minjung Son"
                    },
                    {
                        "authorId": "144182454",
                        "name": "Sung-eui Yoon"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "570189c0389347e5962a313e3568809d592e30d2",
                "externalIds": {
                    "DOI": "10.1109/ICECCME57830.2023.10252913",
                    "CorpusId": 262131301
                },
                "corpusId": 262131301,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/570189c0389347e5962a313e3568809d592e30d2",
                "title": "Exploring the Role of Extracted Features in Deep Learning-based 3D Face Reconstruction from Single 2D Images",
                "abstract": "In 3D face reconstruction, features extracted from a single image are crucial as they provide extra information beyond the image\u2019s size and quality. These features, including facial landmarks and face parsing, texture and UV maps, depth maps, shading information, and albedo maps, can be used to compensate for the lack of prior knowledge provided by a single 2D image and to overcome the dimensional differences between 2D and 3D. By using these features, 3D face reconstruction neural networks can create more detailed and accurate 3D models of faces, even when the input image is of low quality or has extreme poses or occlusions. The paper investigates the definition, importance, extraction, and various applications of each feature, including their potential use in 3D face reconstruction and facial recognition. The paper will provide an analysis of the current state of the art in this field and discuss the potential for future research and development.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2244014599",
                        "name": "Mahfoudh Batarfi"
                    },
                    {
                        "authorId": "52004037",
                        "name": "M. Mareboyana"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "The recovered 3D shapes allow high-quality image editing such as relighting and object rotation [148]."
            ],
            "citingPaper": {
                "paperId": "bbd734cf6458cc0e5e4d98b0605dfe10fe9e2f90",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-10275",
                    "ArXiv": "2307.10275",
                    "DOI": "10.48550/arXiv.2307.10275",
                    "CorpusId": 259991513
                },
                "corpusId": 259991513,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bbd734cf6458cc0e5e4d98b0605dfe10fe9e2f90",
                "title": "Survey on Controlable Image Synthesis with Deep Learning",
                "abstract": "Image synthesis has attracted emerging research interests in academic and industry communities. Deep learning technologies especially the generative models greatly inspired controllable image synthesis approaches and applications, which aim to generate particular visual contents with latent prompts. In order to further investigate low-level controllable image synthesis problem which is crucial for fine image rendering and editing tasks, we present a survey of some recent works on 3D controllable image synthesis using deep learning. We first introduce the datasets and evaluation indicators for 3D controllable image synthesis. Then, we review the state-of-the-art research for geometrically controllable image synthesis in two aspects: 1) Viewpoint/pose-controllable image synthesis; 2) Structure/shape-controllable image synthesis. Furthermore, the photometrically controllable image synthesis approaches are also reviewed for 3D re-lighting researches. While the emphasis is on 3D controllable image synthesis algorithms, the related applications, products and resources are also briefly summarized for practitioners.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2048620850",
                        "name": "Shixiong Zhang"
                    },
                    {
                        "authorId": "1492112282",
                        "name": "Jiao Li"
                    },
                    {
                        "authorId": "2109069513",
                        "name": "Lu Yang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "08c37e164e514e897d938e41dc67e8fec8893995",
                "externalIds": {
                    "DOI": "10.1109/BDAI59165.2023.10256925",
                    "CorpusId": 262948758
                },
                "corpusId": 262948758,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/08c37e164e514e897d938e41dc67e8fec8893995",
                "title": "A Realistic AI Avatar Generation Method Combining GAN, Unity and Kinect Techniques",
                "abstract": "With the comprehensive progress of science and technology, virtual characters have begun to enter people\u2019s vision. There is still a gap between virtual characters and real people. Hence, the study examined, digital virtual humans through GAN2Shape and Kinect 2.0 to obtain high-quality three-dimensional (3D) images of human faces. First, we captured and recognized facial expressions through Kinect. Second, we generated high-quality 3D images of the facial expressions through GAN2Shape and transferred the images to Unity 3D for front and back-end development to generate a facial model. Third, we recognized and captured motions through Kinect. Finally, we combined the facial model with the motions to form the virtual human model. Experimental results show that the virtual human model generated through this method has a clear face and a good effect in motion capture.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108005467",
                        "name": "Weijia Zhang"
                    },
                    {
                        "authorId": "2233474709",
                        "name": "Yuan Zheng"
                    },
                    {
                        "authorId": "2157323705",
                        "name": "Feiyu Chen"
                    },
                    {
                        "authorId": "2233384294",
                        "name": "Yulin Li"
                    },
                    {
                        "authorId": "2233315615",
                        "name": "Yucheng Tian"
                    },
                    {
                        "authorId": "1455125091",
                        "name": "Xu Cao"
                    },
                    {
                        "authorId": "2233356301",
                        "name": "Jin Xie"
                    },
                    {
                        "authorId": "2220325116",
                        "name": "Haiping Ma"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "However, one common property among Unsup3d-based frameworks [5]\u2013[8] is the absence of skip connections [9] giving networks the advantage of picking up useful details for tasks.",
                "GAN2Shape [8] trained reconstruction networks with the help of 3D geometry clues mined from the pre-trained GAN."
            ],
            "citingPaper": {
                "paperId": "9d28743bfd20936b298f0f172e4070a3c2ae5346",
                "externalIds": {
                    "DBLP": "conf/icmcs/DongZL23",
                    "DOI": "10.1109/ICME55011.2023.00297",
                    "CorpusId": 261126946
                },
                "corpusId": 261126946,
                "publicationVenue": {
                    "id": "975e1529-a14b-436f-ad7e-a09d8eabc50d",
                    "name": "IEEE International Conference on Multimedia and Expo",
                    "type": "conference",
                    "alternate_names": [
                        "ICME",
                        "Int Conf Multimedia Expo",
                        "IEEE Int Conf Multimedia Expo",
                        "International Conference on Multimedia and Expo"
                    ],
                    "issn": "1945-7871",
                    "alternate_issns": [
                        "2330-7927"
                    ],
                    "url": "http://csdl2.computer.org/persagen/ProceedingByFilter.jsp?filter=I",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conferences.jsp",
                        "http://www.wikicfp.com/cfp/program?id=1418"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9d28743bfd20936b298f0f172e4070a3c2ae5346",
                "title": "Unsupervised 3D Face Reconstruction with Reprogramming Skip Connections",
                "abstract": "In unsupervised 3D face reconstruction, existing methods modeling the canonical face typically exclude the skip connections between encoder-decoder pairs. Consequently, they have difficulty capturing appearance details necessary for the task. However, directly applying original skip connections only induces these methods to degrade to a trivial 2D texture reconstruction algorithm. In this paper, we propose novel Reprogramming Skip Connections (RSCs), which escape from bringing about degradation and improve the 3D face reconstruction quality. Specifically, the proposed method filters out inappropriate information causing degradation by aggregating the features from the encoder in spatial dimensions into several prototypes. These prototypes preserving beneficial information are subsequently combined with the corresponding decoder features with the help of expansion masks. Further, we design the masks reconstruction consistency loss to improve the quality of the expansion masks. Our experiments verify the superiority of our method compared to other competitors.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2233964258",
                        "name": "Zhuoming Dong"
                    },
                    {
                        "authorId": "46544637",
                        "name": "Huajun Zhou"
                    },
                    {
                        "authorId": "2151605931",
                        "name": "Jianhuang Lai"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "2f8f9b1217cf7b854ac4de4c8192153138e190d5",
                "externalIds": {
                    "DBLP": "conf/icmcs/ZhangWYWL23",
                    "DOI": "10.1109/ICME55011.2023.00164",
                    "CorpusId": 261127524
                },
                "corpusId": 261127524,
                "publicationVenue": {
                    "id": "975e1529-a14b-436f-ad7e-a09d8eabc50d",
                    "name": "IEEE International Conference on Multimedia and Expo",
                    "type": "conference",
                    "alternate_names": [
                        "ICME",
                        "Int Conf Multimedia Expo",
                        "IEEE Int Conf Multimedia Expo",
                        "International Conference on Multimedia and Expo"
                    ],
                    "issn": "1945-7871",
                    "alternate_issns": [
                        "2330-7927"
                    ],
                    "url": "http://csdl2.computer.org/persagen/ProceedingByFilter.jsp?filter=I",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conferences.jsp",
                        "http://www.wikicfp.com/cfp/program?id=1418"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2f8f9b1217cf7b854ac4de4c8192153138e190d5",
                "title": "A Lightweight Grouped Low-rank Tensor Approximation Network for 3D Mesh Reconstruction From Videos",
                "abstract": "Existing methods for 3D mesh reconstruction from videos suffer from increasingly large parameter counts and model sizes due to encoders such as multi-hidden state recurrence. Therefore many models become complex and more difficult to be applied in practice. Based on this problem, we propose a lightweight grouped low-rank tensor approximation network for 3D mesh reconstruction from videos. Specifically, firstly we propose a generalized grouped low-rank tensor approximation algorithm, which decomposes the original high-rank tensor into multiple weighted groups with different low-rank tensors to maximize the approximation of the high-rank tensor. Then we also design a lightweight selection rearranging strategy to reduce feature redundancy and focus on local fragment features. Notably, our method can be flexibly plugged into other 3D reconstruction tasks. Experiments show that our method not only improves the performance but also reduces the parameters of the entire network by about 90% compared with the existing SOTA method. Our grouped low-rank tensor approximation method reduces the parameters by about 99.8% in a single GRU. We demonstrate the outstanding generalization of our method in other 3D reconstruction tasks, eg. 3D face reconstruction and multi-view stereo.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2141897317",
                        "name": "Bo Zhang"
                    },
                    {
                        "authorId": "2946907",
                        "name": "Suping Wu"
                    },
                    {
                        "authorId": "2233549328",
                        "name": "Leyang Yang"
                    },
                    {
                        "authorId": "2256857071",
                        "name": "Bin Wang"
                    },
                    {
                        "authorId": "2233778426",
                        "name": "Wenlong Lu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "86b89c788d749ea3eea0267c8f4d84ed9cff0c9f",
                "externalIds": {
                    "DBLP": "conf/ijcnn/LiWYG23",
                    "DOI": "10.1109/IJCNN54540.2023.10191817",
                    "CorpusId": 260387643
                },
                "corpusId": 260387643,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/86b89c788d749ea3eea0267c8f4d84ed9cff0c9f",
                "title": "Incorporating Least-Effort Loss to Stabilize Training of Wasserstein GAN",
                "abstract": "In order to further improve the convergence properties of generative adversarial networks, in this paper, we analyze how the stability can be affected by the so-called best-effort manner of the discriminator in the minimax game. We point out that this manner can cause the multistate problem and the optimization entangling problem. To alleviate these, we proposed an alternative least-effort loss to regularize the training behaviors of the discriminator. With this loss, the discriminator only updates when it is unable to distinguish distributions. To evaluate the effectiveness of the least-effort loss, we introduce it into Wasserstein GAN. Experiments on Dirac delta distribution and image datasets demonstrate that the least-effort loss can effectively improve the convergence properties and generation quality of WGAN. Furthermore, the behaviors of the discriminator and generator during the training show that, with the least-effort loss, the state space of the discriminator shrinks, and the optimization of the discriminator and the generator disentangles in some way.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2146329569",
                        "name": "Fanqi Li"
                    },
                    {
                        "authorId": "46660076",
                        "name": "L. Wang"
                    },
                    {
                        "authorId": "2119658606",
                        "name": "Bo Yang"
                    },
                    {
                        "authorId": "2181685915",
                        "name": "Pengwei Guan"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "What is known about what StyleGAN knows: Various papers have investigated what StyleGAN knows, starting with good evidence that StyleGAN \u201cknows\u201d 3D information about faces [38, 65], enough to support editing [40, 39, 52]."
            ],
            "citingPaper": {
                "paperId": "238dcca980eff213607495b5dd820ac5654d467b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-00987",
                    "ArXiv": "2306.00987",
                    "DOI": "10.48550/arXiv.2306.00987",
                    "CorpusId": 258999598
                },
                "corpusId": 258999598,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/238dcca980eff213607495b5dd820ac5654d467b",
                "title": "StyleGAN knows Normal, Depth, Albedo, and More",
                "abstract": "Intrinsic images, in the original sense, are image-like maps of scene properties like depth, normal, albedo or shading. This paper demonstrates that StyleGAN can easily be induced to produce intrinsic images. The procedure is straightforward. We show that, if StyleGAN produces $G({w})$ from latents ${w}$, then for each type of intrinsic image, there is a fixed offset ${d}_c$ so that $G({w}+{d}_c)$ is that type of intrinsic image for $G({w})$. Here ${d}_c$ is {\\em independent of ${w}$}. The StyleGAN we used was pretrained by others, so this property is not some accident of our training regime. We show that there are image transformations StyleGAN will {\\em not} produce in this fashion, so StyleGAN is not a generic image regression engine. It is conceptually exciting that an image generator should ``know'' and represent intrinsic images. There may also be practical advantages to using a generative model to produce intrinsic images. The intrinsic images obtained from StyleGAN compare well both qualitatively and quantitatively with those obtained by using SOTA image regression techniques; but StyleGAN's intrinsic images are robust to relighting effects, unlike SOTA methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "35986726",
                        "name": "Anand Bhattad"
                    },
                    {
                        "authorId": "49001466",
                        "name": "Daniel McKee"
                    },
                    {
                        "authorId": "2433269",
                        "name": "Derek Hoiem"
                    },
                    {
                        "authorId": "144016256",
                        "name": "D. Forsyth"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "aa296e10312e730173f116faad62ad937a2842cd",
                "externalIds": {
                    "DBLP": "conf/cvpr/0005CCTW23",
                    "DOI": "10.1109/CVPR52729.2023.00045",
                    "CorpusId": 261081253
                },
                "corpusId": 261081253,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/aa296e10312e730173f116faad62ad937a2842cd",
                "title": "Learning Neural Proto-Face Field for Disentangled 3D Face Modeling in the Wild",
                "abstract": "Generative models show good potential for recovering 3D faces beyond limited shape assumptions. While plausible details and resolutions are achieved, these models easily fail under extreme conditions of pose, shadow or appearance, due to the entangled fitting or lack of multi-view priors. To address this problem, this paper presents a novel Neural Proto-face Field (NPF) for unsupervised robust 3D face modeling. Instead of using constrained images as Neural Radiance Field (NeRF), NPF disentangles the common/specific facial cues, i.e., ID, expression and scene-specific details from in-the-wild photo collections. Specifically, NPF learns a face prototype to aggregate 3D-consistent identity via uncertainty modeling, extracting multi-image priors from a photo collection. NPF then learns to deform the prototype with the appropriate facial expressions, constrained by a loss of expression consistency and personal idiosyncrasies. Finally, NPF is optimized to fit a target image in the collection, recovering specific details of appearance and geometry. In this way, the generative model benefits from multi-image priors and meaningful facial structures. Extensive experiments on benchmarks show that NPF recovers superior or competitive facial shapes and textures, compared to state-of-the-art methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2192943489",
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "authorId": "1625895990",
                        "name": "Renwang Chen"
                    },
                    {
                        "authorId": "2075437879",
                        "name": "Weijian Cao"
                    },
                    {
                        "authorId": "144970872",
                        "name": "Ying Tai"
                    },
                    {
                        "authorId": "1978245",
                        "name": "Chengjie Wang"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "methodology",
                "result"
            ],
            "contexts": [
                "Our experimental results show comparable performances to GAN2Shape [12] in terms of 2D level.",
                "Method 3DMMCNN [5]\n3DMM GCN [18] Nonlinear 3DMM [7]\nUnsup3D [8] GAN2Shape [12]\nOurs 3DMM+CNN [5]\n3DMM+GCN [18] Nonlinear 3DMM [7]\nUnsup3D [8] GAN2Shape [12]\nOurs\nDataset Ce1ebA Ce1ebA CelebA Ce1ebA Ce1ebA Ce1ebA\nBosphorus Bosphorus Bosphorus Bosphorus Bosphorus Bosphorus PSNR(dB)t 26.58 29.69 27.36 29.51 29.92 29.87 26.37 29.24 26.91 29.06 29.58 29.54 SSIMt 0.826 0.894 0.857 0.883 0.898 0.903 0.778 0.861 0.813 0.876 0.867 0.872 N-CD+ 0.0260 0.0145 0.0203 0.0194 0.0139 0.0136",
                "Unsup3d [8], GAN2Shape [12]) on datasets(CelebA, Bosphorus) in Table I.",
                "We compare our method with five state-of-the-art methods (3DMM CNN [5], 3DMM GCN [18], Nonlinear 3DMM [7], Unsup3d [8], GAN2Shape [12]) on datasets(CelebA, Bosphorus) in Table I.",
                "Method 3DMMCNN [5] 3DMM GCN [18] Nonlinear 3DMM [7] Unsup3D [8] GAN2Shape [12] Ours 3DMM+CNN [5] 3DMM+GCN [18] Nonlinear 3DMM [7] Unsup3D [8] GAN2Shape [12] Ours Dataset Ce1ebA Ce1ebA CelebA Ce1ebA Ce1ebA Ce1ebA Bosphorus Bosphorus Bosphorus Bosphorus Bosphorus Bosphorus PSNR(dB)t 26.",
                "[12] proposed a multi-stage 3D face reconstruction framework, which utilized GAN in the second stage of training to infer the viewpoint and illumination changes of images to obtain accurate 3D face shapes."
            ],
            "citingPaper": {
                "paperId": "5ed0e888519eb6ab52c449ed81ee34b4325d475e",
                "externalIds": {
                    "DBLP": "conf/iscas/WangHWS23",
                    "DOI": "10.1109/ISCAS46773.2023.10181689",
                    "CorpusId": 260004359
                },
                "corpusId": 260004359,
                "publicationVenue": {
                    "id": "9bc219ae-a4dc-4241-8e1a-0552f9ee9ef7",
                    "name": "International Symposium on Circuits and Systems",
                    "type": "conference",
                    "alternate_names": [
                        "ISCAS",
                        "Int Symp Circuit Syst"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5ed0e888519eb6ab52c449ed81ee34b4325d475e",
                "title": "Texture-Shape Optimized GAT for 3D Face Reconstruction",
                "abstract": "3D face reconstruction is widely used in face recognition research, online makeup, etc. However, texture and shape distortion regions usually exist in the reconstruction results. This paper proposes a novel framework named Texture-Shape optimized GAT for 3D face Reconstruction (TSGAT-3D), including data preprocessing and training phases. In the data preprocessing phase, we adopt the styleGAN2 to convert single-view images to multi-view images set. In the training phase, we design a novel Texture-Shape optimized Graph Attention Network, which can learn the facial prior knowledge from the multi-view images set, to improve the details of the initially reconstructed faces based on the auto-encoder module. This network can aggregate the features of face vertices according to the correlation of vertices, thereby improving the accuracy of the 3D reconstructed faces. Furthermore, we present a view loss function for this framework to constrain the shape and texture of the reconstructed face. Extensive experiments conducted on CelebA and Bosphorus show that the reconstruction results of our proposed method are closer to the real 3D faces.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Chen Wang"
                    },
                    {
                        "authorId": "2224098640",
                        "name": "Chao Hao"
                    },
                    {
                        "authorId": "2585506",
                        "name": "Guijin Wang"
                    },
                    {
                        "authorId": "2054603240",
                        "name": "Nan Su"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[2] was adopted in the experiment, which may be summed up as follows: 1."
            ],
            "citingPaper": {
                "paperId": "0a9d84758b2e5e319058bf4e400a406d0354245e",
                "externalIds": {
                    "DOI": "10.1109/ISNE56211.2023.10221572",
                    "CorpusId": 261106998
                },
                "corpusId": 261106998,
                "publicationVenue": {
                    "id": "5f65a3c9-a0dc-49f6-95d2-78260e49dd6f",
                    "name": "International Symposium on Next-Generation Electronics",
                    "type": "conference",
                    "alternate_names": [
                        "ISNE",
                        "Int Symp Next-generation Electron"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0a9d84758b2e5e319058bf4e400a406d0354245e",
                "title": "Creation of Digital Virtual Human Based on GAN2Shape and Kinect 2.0",
                "abstract": "This study aimed at the problem of the low recognition rate of facial expressions cause by complex environments. Hence, the study examined, digital virtual humans through GAN2Shape and Kinect 2.0 to obtain high-quality three-dimensional (3D) images of human faces. First, we captured and recognized facial expressions through Kinect. Second, we generated high-quality 3D images of the facial expressions through GAN2Shape and transferred the images to Unity 3D for front and back-end development to generate a facial model. Third, we recognized and captured motions through Kinect. Finally, we combined the facial model with the motions to form the virtual human model. Experimental results show that the virtual human model generated through this method has a clear face and a good effect in motion capture.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108005467",
                        "name": "Weijia Zhang"
                    },
                    {
                        "authorId": "2233474709",
                        "name": "Yuan Zheng"
                    },
                    {
                        "authorId": "2157323705",
                        "name": "Feiyu Chen"
                    },
                    {
                        "authorId": "2233384294",
                        "name": "Yulin Li"
                    },
                    {
                        "authorId": "2233315615",
                        "name": "Yucheng Tian"
                    },
                    {
                        "authorId": "1455125091",
                        "name": "Xu Cao"
                    },
                    {
                        "authorId": "2233337066",
                        "name": "Dan Qiu"
                    },
                    {
                        "authorId": "2233432753",
                        "name": "Jiaju Li"
                    },
                    {
                        "authorId": "2233356301",
                        "name": "Jin Xie"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "dad8852fec4b4eaba8bf3c2652b13c34a0a04742",
                "externalIds": {
                    "DBLP": "journals/tog/TrevithickCSCLYKCRN23",
                    "ArXiv": "2305.02310",
                    "DOI": "10.1145/3592460",
                    "CorpusId": 258461577
                },
                "corpusId": 258461577,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/dad8852fec4b4eaba8bf3c2652b13c34a0a04742",
                "title": "Real-Time Radiance Fields for Single-Image Portrait View Synthesis",
                "abstract": "We present a one-shot method to infer and render a photorealistic 3D representation from a single unposed image (e.g., face portrait) in real-time. Given a single RGB input, our image encoder directly predicts a canonical triplane representation of a neural radiance field for 3D-aware novel view synthesis via volume rendering. Our method is fast (24 fps) on consumer hardware, and produces higher quality results than strong GAN-inversion baselines that require test-time optimization. To train our triplane encoder pipeline, we use only synthetic data, showing how to distill the knowledge from a pretrained 3D GAN into a feedforward encoder. Technical contributions include a Vision Transformer-based triplane encoder, a camera data augmentation strategy, and a well-designed loss function for synthetic data training. We benchmark against the state-of-the-art methods, demonstrating significant improvements in robustness and image quality in challenging real-world settings. We showcase our results on portraits of faces (FFHQ) and cats (AFHQ), but our algorithm can also be applied in the future to other categories with a 3D-aware image generator.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1993540993",
                        "name": "Alex Trevithick"
                    },
                    {
                        "authorId": "2147382797",
                        "name": "Matthew Chan"
                    },
                    {
                        "authorId": "2267017",
                        "name": "Michael Stengel"
                    },
                    {
                        "authorId": "121028414",
                        "name": "Eric Chan"
                    },
                    {
                        "authorId": "2152505971",
                        "name": "Chao Liu"
                    },
                    {
                        "authorId": "1751019",
                        "name": "Zhiding Yu"
                    },
                    {
                        "authorId": "2121982",
                        "name": "S. Khamis"
                    },
                    {
                        "authorId": "2099305",
                        "name": "Manmohan Chandraker"
                    },
                    {
                        "authorId": "1752236",
                        "name": "R. Ramamoorthi"
                    },
                    {
                        "authorId": "1897417",
                        "name": "Koki Nagano"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "4d5b8dc7fb0a2eab18aa152c9e3c14473d54cd68",
                "externalIds": {
                    "DOI": "10.1109/ICCECT57938.2023.10141185",
                    "CorpusId": 259102658
                },
                "corpusId": 259102658,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4d5b8dc7fb0a2eab18aa152c9e3c14473d54cd68",
                "title": "3D Building Reconstruction Using 2D GANs",
                "abstract": "There are many historical sites across the globe that hold a huge importance to mankind as it contains the history and culture of the past. But as buildings wither and lose their beauty over the course of time, we lose a precious piece of the heritage of mankind forever. With the growing emergence of new technologies and the internet, we are instead able to preserve these heritage sites through their reconstruction as 3D models using convolutional neural networks and simple 2D vision. We base our work mainly on a previous implementation of 3D shape reconstruction using 2D image GANs by Xingang Pan et al, where we mine for cues such as albedo, viewpoint, lighting and depth such that we can infer a 3D image from these factors. We believe our work is different from other works in that we use purely an 2D image to reconstruct its 3D form, unlike other related works such as Pixel2Mesh, which uses 3D Mesh Models for ground truths. Our work infers a 3D shape and model from simply one 2D image, which cuts costs and is much easier to implement as it does not require any other input other than the singular 2D image.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2219565307",
                        "name": "Christopher Siyuan Tao"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "8863a4de2a231ef0dec92efe4672fc228a2488df",
                "externalIds": {
                    "DBLP": "conf/ccoms/Liu23",
                    "DOI": "10.1109/ICCCS57501.2023.10151086",
                    "CorpusId": 259280905
                },
                "corpusId": 259280905,
                "publicationVenue": {
                    "id": "da18ef0b-933e-4678-b2fa-4290baf6970a",
                    "name": "International Conference on Communication, Computing & Security",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Commun Comput  Secur",
                        "International Conference Communication and Computing Systems",
                        "ICCCS",
                        "International Conference on Computer and Communication Systems",
                        "Int Conf Comput Commun Syst",
                        "Int Conf Commun Comput Syst"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8863a4de2a231ef0dec92efe4672fc228a2488df",
                "title": "Image Segmentation Based on Tibetan Architecture",
                "abstract": "Tibetan-style architecture represents the history and development of the Tibetan people. Since many Tibetan-style buildings have been damaged today, effective division of Tibetan-style buildings has great academic research significance and development prospects. The segmentation technology of Tibetan buildings is the content of artificial intelligence research. Accurate and efficient segmentation helps to provide cheaper data and accurate information for the 3D reconstruction of buildings. In this article, we propose an image segmentation method for Tibetan architecture, which enhances the attention information points of image segmentation through the NAM attention mechanism, and uses the modified residual ASPP to obtain context information, thereby accurately obtaining edge information.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220802642",
                        "name": "Lulian Liu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "In [105], the authors answer the question, \u201dIs it possible to reconstruct the 3D shape of a single 2D image by exploiting the 3D-alike image manipulation effects produced by GANs?\u201d with a yes using their unsupervised approach, GAN2Shape, by showing that when existing 2D GANs are trained only on images, they could accurately reconstruct its 3D shape for objects belonging to several categories including human faces, cars, buildings among others without the need for a 2D keypoint or 3D annotations."
            ],
            "citingPaper": {
                "paperId": "5f2380116499291db0f2e9e80fd19107a120828e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-03932",
                    "ArXiv": "2304.03932",
                    "DOI": "10.48550/arXiv.2304.03932",
                    "CorpusId": 258048474
                },
                "corpusId": 258048474,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5f2380116499291db0f2e9e80fd19107a120828e",
                "title": "3D GANs and Latent Space: A comprehensive survey",
                "abstract": "Generative Adversarial Networks (GANs) have emerged as a significant player in generative modeling by mapping lower-dimensional random noise to higher-dimensional spaces. These networks have been used to generate high-resolution images and 3D objects. The efficient modeling of 3D objects and human faces is crucial in the development process of 3D graphical environments such as games or simulations. 3D GANs are a new type of generative model used for 3D reconstruction, point cloud reconstruction, and 3D semantic scene completion. The choice of distribution for noise is critical as it represents the latent space. Understanding a GAN's latent space is essential for fine-tuning the generated samples, as demonstrated by the morphing of semantically meaningful parts of images. In this work, we explore the latent space and 3D GANs, examine several GAN variants and training methods to gain insights into improving 3D GAN training, and suggest potential future directions for further research.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2067133241",
                        "name": "S. Tata"
                    },
                    {
                        "authorId": "2199255897",
                        "name": "Subhankar Mishra"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "1a6444fc0dd3c42b5058d635a502b7c869395159",
                "externalIds": {
                    "DBLP": "journals/chinaf/LiuWWZZ23",
                    "DOI": "10.1007/s11432-022-3679-0",
                    "CorpusId": 250918946
                },
                "corpusId": 250918946,
                "publicationVenue": {
                    "id": "0534c8a0-1226-4f5b-bcf6-a13a8dd1825e",
                    "name": "Science China Information Sciences",
                    "alternate_names": [
                        "Sci China Inf Sci"
                    ],
                    "issn": "1869-1919",
                    "url": "http://info.scichina.com/"
                },
                "url": "https://www.semanticscholar.org/paper/1a6444fc0dd3c42b5058d635a502b7c869395159",
                "title": "Survey on leveraging pre-trained generative adversarial networks for image editing and restoration",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "144474429",
                        "name": "Ming Liu"
                    },
                    {
                        "authorId": "2156252416",
                        "name": "Yuxiang Wei"
                    },
                    {
                        "authorId": "39637222",
                        "name": "Xiaohe Wu"
                    },
                    {
                        "authorId": "1724520",
                        "name": "W. Zuo"
                    },
                    {
                        "authorId": "1720539",
                        "name": "L. Zhang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "[30] attempts to reconstruct the 3D shape using pretrained 2D GANs."
            ],
            "citingPaper": {
                "paperId": "d84616f108ccbd958735fef7622e58d148b32139",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-14184",
                    "ArXiv": "2303.14184",
                    "DOI": "10.48550/arXiv.2303.14184",
                    "CorpusId": 257757320
                },
                "corpusId": 257757320,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d84616f108ccbd958735fef7622e58d148b32139",
                "title": "Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior",
                "abstract": "In this work, we investigate the problem of creating high-fidelity 3D content from only a single image. This is inherently challenging: it essentially involves estimating the underlying 3D geometry while simultaneously hallucinating unseen textures. To address this challenge, we leverage prior knowledge from a well-trained 2D diffusion model to act as 3D-aware supervision for 3D creation. Our approach, Make-It-3D, employs a two-stage optimization pipeline: the first stage optimizes a neural radiance field by incorporating constraints from the reference image at the frontal view and diffusion prior at novel views; the second stage transforms the coarse model into textured point clouds and further elevates the realism with diffusion prior while leveraging the high-quality textures from the reference image. Extensive experiments demonstrate that our method outperforms prior works by a large margin, resulting in faithful reconstructions and impressive visual quality. Our method presents the first attempt to achieve high-quality 3D creation from a single image for general objects and enables various applications such as text-to-3D creation and texture editing.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "152949334",
                        "name": "Junshu Tang"
                    },
                    {
                        "authorId": "2116582092",
                        "name": "Tengfei Wang"
                    },
                    {
                        "authorId": "145803569",
                        "name": "Bo Zhang"
                    },
                    {
                        "authorId": "2146320438",
                        "name": "Ting Zhang"
                    },
                    {
                        "authorId": "2073168604",
                        "name": "Ran Yi"
                    },
                    {
                        "authorId": "2149343311",
                        "name": "Lizhuang Ma"
                    },
                    {
                        "authorId": "47514557",
                        "name": "Dong Chen"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "03209edc230adfcf4ffc056f5b6e2cc7d0f946a4",
                "externalIds": {
                    "ArXiv": "2303.12865",
                    "DBLP": "journals/corr/abs-2303-12865",
                    "DOI": "10.48550/arXiv.2303.12865",
                    "CorpusId": 257687857
                },
                "corpusId": 257687857,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/03209edc230adfcf4ffc056f5b6e2cc7d0f946a4",
                "title": "NeRF-GAN Distillation for Efficient 3D-Aware Generation with Convolutions",
                "abstract": "Pose-conditioned convolutional generative models struggle with high-quality 3D-consistent image generation from single-view datasets, due to their lack of sufficient 3D priors. Recently, the integration of Neural Radiance Fields (NeRFs) and generative models, such as Generative Adversarial Networks (GANs), has transformed 3D-aware generation from single-view images. NeRF-GANs exploit the strong inductive bias of neural 3D representations and volumetric rendering at the cost of higher computational complexity. This study aims at revisiting pose-conditioned 2D GANs for efficient 3D-aware generation at inference time by distilling 3D knowledge from pretrained NeRF-GANs. We propose a simple and effective method, based on re-using the well-disentangled latent space of a pre-trained NeRF-GAN in a pose-conditioned convolutional network to directly generate 3D-consistent images corresponding to the underlying 3D representations. Experiments on several datasets demonstrate that the proposed method obtains results comparable with volumetric rendering in terms of quality and 3D consistency while benefiting from the computational advantage of convolutional networks. The code will be available at: https://github.com/mshahbazi72/NeRF-GAN-Distillation",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "73774192",
                        "name": "Mohamad Shahbazi"
                    },
                    {
                        "authorId": "1628458093",
                        "name": "Evangelos Ntavelis"
                    },
                    {
                        "authorId": "20406113",
                        "name": "A. Tonioni"
                    },
                    {
                        "authorId": "33942393",
                        "name": "Edo Collins"
                    },
                    {
                        "authorId": "35268081",
                        "name": "D. Paudel"
                    },
                    {
                        "authorId": "2129520569",
                        "name": "Martin Danelljan"
                    },
                    {
                        "authorId": "1681236",
                        "name": "L. Gool"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "7eb2afee6823eebdf37f379a7eb1a2d1fb0a98a5",
                "externalIds": {
                    "ArXiv": "2303.09036",
                    "DBLP": "journals/corr/abs-2303-09036",
                    "DOI": "10.48550/arXiv.2303.09036",
                    "CorpusId": 257557618
                },
                "corpusId": 257557618,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7eb2afee6823eebdf37f379a7eb1a2d1fb0a98a5",
                "title": "Mimic3D: Thriving 3D-Aware GANs via 3D-to-2D Imitation",
                "abstract": "Generating images with both photorealism and multiview 3D consistency is crucial for 3D-aware GANs, yet existing methods struggle to achieve them simultaneously. Improving the photorealism via CNN-based 2D super-resolution can break the strict 3D consistency, while keeping the 3D consistency by learning high-resolution 3D representations for direct rendering often compromises image quality. In this paper, we propose a novel learning strategy, namely 3D-to-2D imitation, which enables a 3D-aware GAN to generate high-quality images while maintaining their strict 3D consistency, by letting the images synthesized by the generator's 3D rendering branch to mimic those generated by its 2D super-resolution branch. We also introduce 3D-aware convolutions into the generator for better 3D representation learning, which further improves the image generation quality. With the above strategies, our method reaches FID scores of 5.4 and 4.3 on FFHQ and AFHQ-v2 Cats, respectively, at 512x512 resolution, largely outperforming existing 3D-aware GANs using direct 3D rendering and coming very close to the previous state-of-the-art method that leverages 2D super-resolution. Project website: https://seanchenxy.github.io/Mimic3DWeb.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2143792314",
                        "name": "Xingyu Chen"
                    },
                    {
                        "authorId": "152710186",
                        "name": "Yu Deng"
                    },
                    {
                        "authorId": "2450889",
                        "name": "Baoyuan Wang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Iterative methods [50, 30, 7] based on StyleGAN optimization can produce the most realistic face UV images.",
                "To this end, we use the pre-trained model of Unsup3D [45] since it has been widely applied in unsupervised face reconstruction [50, 30, 35, 49] in recent years."
            ],
            "citingPaper": {
                "paperId": "676005e85a0e3956bf0778a01ee41b887f301fad",
                "externalIds": {
                    "ArXiv": "2303.07709",
                    "DBLP": "journals/corr/abs-2303-07709",
                    "DOI": "10.48550/arXiv.2303.07709",
                    "CorpusId": 257505090
                },
                "corpusId": 257505090,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/676005e85a0e3956bf0778a01ee41b887f301fad",
                "title": "3D Face Arbitrary Style Transfer",
                "abstract": "Style transfer of 3D faces has gained more and more attention. However, previous methods mainly use images of artistic faces for style transfer while ignoring arbitrary style images such as abstract paintings. To solve this problem, we propose a novel method, namely Face-guided Dual Style Transfer (FDST). To begin with, FDST employs a 3D decoupling module to separate facial geometry and texture. Then we propose a style fusion strategy for facial geometry. Subsequently, we design an optimization-based DDSG mechanism for textures that can guide the style transfer by two style images. Besides the normal style image input, DDSG can utilize the original face input as another style input as the face prior. By this means, high-quality face arbitrary style transfer results can be obtained. Furthermore, FDST can be applied in many downstream tasks, including region-controllable style transfer, high-fidelity face texture reconstruction, large-pose face reconstruction, and artistic face reconstruction. Comprehensive quantitative and qualitative results show that our method can achieve comparable performance. All source codes and pre-trained weights will be released to the public.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2087027693",
                        "name": "Xiangwen Deng"
                    },
                    {
                        "authorId": "2113958213",
                        "name": "Ying Zou"
                    },
                    {
                        "authorId": null,
                        "name": "Yuanhao Cai"
                    },
                    {
                        "authorId": "2152588546",
                        "name": "Chendong Zhao"
                    },
                    {
                        "authorId": "1614039034",
                        "name": "Yang Liu"
                    },
                    {
                        "authorId": "2203016405",
                        "name": "Zhifang Liu"
                    },
                    {
                        "authorId": "2211723905",
                        "name": "Yuxiao Liu"
                    },
                    {
                        "authorId": "2112249764",
                        "name": "Jia-wei Zhou"
                    },
                    {
                        "authorId": "2143410777",
                        "name": "Haoqian Wang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "c565c2d89f7297b19128635e9e6254a505ca474e",
                "externalIds": {
                    "DBLP": "journals/air/SamavatiS23",
                    "DOI": "10.1007/s10462-023-10399-2",
                    "CorpusId": 256380970
                },
                "corpusId": 256380970,
                "publicationVenue": {
                    "id": "ea8553fe-2467-4367-afee-c4deb3754820",
                    "name": "Artificial Intelligence Review",
                    "type": "journal",
                    "alternate_names": [
                        "Artif Intell Rev"
                    ],
                    "issn": "0269-2821",
                    "url": "https://link.springer.com/journal/10462"
                },
                "url": "https://www.semanticscholar.org/paper/c565c2d89f7297b19128635e9e6254a505ca474e",
                "title": "Deep learning-based 3D reconstruction: a survey",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2099433428",
                        "name": "Taha Samavati"
                    },
                    {
                        "authorId": "2522837",
                        "name": "M. Soryani"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Previously, some approaches attempt to extract 3D structure from pretrained 2D-GANs [44, 54]."
            ],
            "citingPaper": {
                "paperId": "12f99b597fd65c9eb730cfef498b47f3fb3a5ec8",
                "externalIds": {
                    "DBLP": "conf/cvpr/AbdalL0CSWT23",
                    "ArXiv": "2301.02700",
                    "DOI": "10.1109/CVPR52729.2023.00442",
                    "CorpusId": 255546292
                },
                "corpusId": 255546292,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/12f99b597fd65c9eb730cfef498b47f3fb3a5ec8",
                "title": "3DAvatarGAN: Bridging Domains for Personalized Editable Avatars",
                "abstract": "Modern 3D-GANs synthesize geometry and texture by training on large-scale datasets with a consistent structure. Training such models on stylized, artistic data, with often unknown, highly variable geometry, and camera information has not yet been shown possible. Can we train a 3D GAN on such artistic data, while maintaining multi-view consistency and texture quality? To this end, we propose an adaptation framework, where the source domain is a pre-trained 3D-GAN, while the target domain is a 2D-GAN trained on artistic datasets. We, then, distill the knowledge from a 2D generator to the source 3D generator. To do that, we first propose an optimization-based method to align the distributions of camera parameters across domains. Second, we propose regularizations necessary to learn high-quality texture, while avoiding degenerate geometric solutions, such as flat shapes. Third, we show a deformation-based technique for modeling exaggerated geometry of artistic domains, enabling-as a byproduct- personalized geometric editing. Finally, we propose a novel inversion method for 3D-GANs linking the latent spaces of the source and the target domains. Our contributions-for the first time-allow for the generation, editing, and animation of personalized artistic 3D avatars on artistic datasets. Project Page: https:/rameenabdal.github.io/3DAvatarGAN",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "94395014",
                        "name": "Rameen Abdal"
                    },
                    {
                        "authorId": "49923155",
                        "name": "Hsin-Ying Lee"
                    },
                    {
                        "authorId": "100516590",
                        "name": "Peihao Zhu"
                    },
                    {
                        "authorId": "1752091",
                        "name": "Menglei Chai"
                    },
                    {
                        "authorId": "10753214",
                        "name": "Aliaksandr Siarohin"
                    },
                    {
                        "authorId": "1798011",
                        "name": "Peter Wonka"
                    },
                    {
                        "authorId": "145582202",
                        "name": "S. Tulyakov"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "a786b4faf741c9c24c10e0a99e2d43c7cd067744",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-09735",
                    "ArXiv": "2212.09735",
                    "DOI": "10.48550/arXiv.2212.09735",
                    "CorpusId": 254854608
                },
                "corpusId": 254854608,
                "publicationVenue": {
                    "id": "939ee07c-6009-43f8-b884-69238b40659e",
                    "name": "International Journal of Computer Vision",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Comput Vis"
                    ],
                    "issn": "0920-5691",
                    "url": "https://www.springer.com/computer/image+processing/journal/11263",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11263",
                        "http://link.springer.com/journal/11263"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a786b4faf741c9c24c10e0a99e2d43c7cd067744",
                "title": "Correspondence Distillation from NeRF-based GAN",
                "abstract": "The neural radiance field (NeRF) has shown promising results in preserving the fine details of objects and scenes. However, unlike mesh-based representations, it remains an open problem to build dense correspondences across different NeRFs of the same category, which is essential in many downstream tasks. The main difficulties of this problem lie in the implicit nature of NeRF and the lack of ground-truth correspondence annotations. In this paper, we show it is possible to bypass these challenges by leveraging the rich semantics and structural priors encapsulated in a pre-trained NeRF-based GAN. Specifically, we exploit such priors from three aspects, namely 1) a dual deformation field that takes latent codes as global structural indicators, 2) a learning objective that regards generator features as geometric-aware local descriptors, and 3) a source of infinite object-specific NeRF samples. Our experiments demonstrate that such priors lead to 3D dense correspondence that is accurate, smooth, and robust. We also show that established dense correspondence across NeRFs can effectively enable many NeRF-based downstream applications such as texture transfer.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2055985023",
                        "name": "Yushi Lan"
                    },
                    {
                        "authorId": "1717179",
                        "name": "Chen Change Loy"
                    },
                    {
                        "authorId": "144445937",
                        "name": "Bo Dai"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                ", voxels [15,31] and meshes [34] as the intermediate shape models, which lacks photorealism and is memory-inefficient.",
                "Generative Adversarial Network [13] has shown promising results in generating photorealistic images [5, 21, 22] and inspired researchers to put efforts on 3D aware generation [15, 31, 34]."
            ],
            "citingPaper": {
                "paperId": "94cda50f5e311975d00ba6e960f6e325b44d4b85",
                "externalIds": {
                    "ArXiv": "2212.07409",
                    "DBLP": "journals/corr/abs-2212-07409",
                    "DOI": "10.1109/CVPR52729.2023.02006",
                    "CorpusId": 254636076
                },
                "corpusId": 254636076,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/94cda50f5e311975d00ba6e960f6e325b44d4b85",
                "title": "Self-Supervised Geometry-Aware Encoder for Style-Based 3D GAN Inversion",
                "abstract": "StyleGAN has achieved great progress in 2D face reconstruction and semantic editing via image inversion and latent editing. While studies over extending 2D StyleGAN to 3D faces have emerged, a corresponding generic 3D GAN inversion framework is still missing, limiting the applications of 3D face reconstruction and semantic editing. In this paper, we study the challenging problem of 3D GAN inversion where a latent code is predicted given a single face image to faithfully recover its 3D shapes and detailed textures. The problem is ill-posed: innumerable compositions of shape and texture could be rendered to the current image. Furthermore, with the limited capacity of a global latent code, 2D inversion methods cannot preserve faithful shape and texture at the same time when applied to 3D models. To solve this problem, we devise an effective self-training scheme to constrain the learning of inversion. The learning is done efficiently without any real-world 2D-3D training pairs but proxy samples generated from a 3D GAN. In addition, apart from a global latent code that captures the coarse shape and texture information, we augment the generation network with a local branch, where pixel-aligned features are added to faithfully reconstruct face details. We further consider a new pipeline to perform 3D view-consistent editing. Extensive experiments show that our method outperforms state-of-the-art inversion methods in both shape and texture reconstruction quality.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2055985023",
                        "name": "Yushi Lan"
                    },
                    {
                        "authorId": "2196065255",
                        "name": "Xuyi Meng"
                    },
                    {
                        "authorId": "2159711748",
                        "name": "Shuai Yang"
                    },
                    {
                        "authorId": "1717179",
                        "name": "Chen Change Loy"
                    },
                    {
                        "authorId": "144445937",
                        "name": "Bo Dai"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "2021] and physics-based decomposition methods [Pan et al. 2021; Wu et al. 2020], refer to the Supplemental Document.",
                "Another research direction is to reconstruct the 3D geometry of an input image based on the physics-based priors of light transport, where Unsup3d [Wu et al. 2020], GAN2Shape [Pan et al. 2021], and StyleGANRender [Zhang et al. 2021] show promising results.",
                "2020], GAN2Shape [Pan et al. 2021], and StyleGANRender [Zhang et al.",
                "For the results of parametric fitting [Feng et al. 2021] and physics-based decomposition methods [Pan et al. 2021; Wu et al. 2020], refer to the Supplemental Document."
            ],
            "citingPaper": {
                "paperId": "d8f7207feac88d7ca59f816f7c8a35c1875aee7f",
                "externalIds": {
                    "DBLP": "conf/siggrapha/JinRKBC22",
                    "ArXiv": "2211.16798",
                    "DOI": "10.1145/3550469.3555422",
                    "CorpusId": 254070613
                },
                "corpusId": 254070613,
                "publicationVenue": {
                    "id": "51fbe23f-0058-4484-b71c-186477a032db",
                    "name": "ACM SIGGRAPH Conference and Exhibition on Computer Graphics and Interactive Techniques in Asia",
                    "type": "conference",
                    "alternate_names": [
                        "ACM GR Conf Exhib Comput Graph Interact Tech Asia",
                        "GR Conf Exhib Comput Graph Interact Tech Asia",
                        "SIGGRAPH Asia",
                        "SIGGRAPH Conference and Exhibition on Computer Graphics and Interactive Techniques in Asia"
                    ],
                    "url": "http://www.siggraph.org/"
                },
                "url": "https://www.semanticscholar.org/paper/d8f7207feac88d7ca59f816f7c8a35c1875aee7f",
                "title": "Dr.3D: Adapting 3D GANs to Artistic Drawings",
                "abstract": "While 3D GANs have recently demonstrated the high-quality synthesis of multi-view consistent images and 3D shapes, they are mainly restricted to photo-realistic human portraits. This paper aims to extend 3D GANs to a different, but meaningful visual form: artistic portrait drawings. However, extending existing 3D GANs to drawings is challenging due to the inevitable geometric ambiguity present in drawings. To tackle this, we present Dr.3D, a novel adaptation approach that adapts an existing 3D GAN to artistic drawings. Dr.3D is equipped with three novel components to handle the geometric ambiguity: a deformation-aware 3D synthesis network, an alternating adaptation of pose estimation and image synthesis, and geometric priors. Experiments show that our approach can successfully adapt 3D GANs to drawings and enable multi-view consistent semantic editing of drawings.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2192734801",
                        "name": "Wonjoon Jin"
                    },
                    {
                        "authorId": "2192712191",
                        "name": "Nuri Ryu"
                    },
                    {
                        "authorId": "2157689053",
                        "name": "Geon-Yeong Kim"
                    },
                    {
                        "authorId": "2192608318",
                        "name": "Seung-Hwan Baek"
                    },
                    {
                        "authorId": "2149156222",
                        "name": "Sunghyun Cho"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "408248c1614254cd8a6967e7faaabf18754fead6",
                "externalIds": {
                    "ArXiv": "2211.13901",
                    "DBLP": "journals/corr/abs-2211-13901",
                    "DOI": "10.1109/CVPR52729.2023.00430",
                    "CorpusId": 254017882
                },
                "corpusId": 254017882,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/408248c1614254cd8a6967e7faaabf18754fead6",
                "title": "Learning Detailed Radiance Manifolds for High-Fidelity and 3D-Consistent Portrait Synthesis from Monocular Image",
                "abstract": "A key challenge for novel view synthesis of monocular portrait images is 3D consistency under continuous pose variations. Most existing methods rely on 2D generative models which often leads to obvious 3D inconsistency artifacts. We present a 3D-consistent novel view synthesis approach for monocular portrait images based on a recent proposed 3D-aware GAN, namely Generative Radiance Manifolds (GRAM) [13], which has shown strong 3D consistency at multiview image generation of virtual subjects via the radiance manifolds representation. However, simply learning an encoder to map a real image into the latent space of GRAM can only reconstruct coarse radiance manifolds without faithful fine details, while improving the reconstruction fidelity via instance-specific optimization is time-consuming. We introduce a novel detail manifolds reconstructor to learn 3D-consistent fine details on the radiance manifolds from monocular images, and combine them with the coarse radiance manifolds for high-fidelity reconstruction. The 3D priors derived from the coarse radiance manifolds are used to regulate the learned details to ensure reasonable synthesized results at novel views. Trained on in-the-wild 2D images, our method achieves high-fidelity and 3D-consistent portrait synthesis largely outperforming the prior art. Project page: https://yudeng.github.io/GRAMInverter/",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152710186",
                        "name": "Yu Deng"
                    },
                    {
                        "authorId": "2450889",
                        "name": "Baoyuan Wang"
                    },
                    {
                        "authorId": "93596028",
                        "name": "H. Shum"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Generative Adversarial Networks (GANs) [10] have shown promise in learning unsupervised representations of 2D data, even learning information about the underlying 3D geometry [28].",
                "Using the knowledge that 2D GANs can learn 3D geometrical information [28], we train a StyleGAN2 [17] network to generate rendered feet from StyleGAN2 latent codes."
            ],
            "citingPaper": {
                "paperId": "e8e7576b4044f403fddf1a5b487927d3dcab6a65",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-12241",
                    "ArXiv": "2210.12241",
                    "DOI": "10.48550/arXiv.2210.12241",
                    "CorpusId": 253098448
                },
                "corpusId": 253098448,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/e8e7576b4044f403fddf1a5b487927d3dcab6a65",
                "title": "FIND: An Unsupervised Implicit 3D Model of Articulated Human Feet",
                "abstract": "In this paper we present a high fidelity and articulated 3D human foot model. The model is parameterised by a disentangled latent code in terms of shape, texture and articulated pose. While high fidelity models are typically created with strong supervision such as 3D keypoint correspondences or pre-registration, we focus on the difficult case of little to no annotation. To this end, we make the following contributions: (i) we develop a Foot Implicit Neural Deformation field model, named FIND, capable of tailoring explicit meshes at any resolution i.e. for low or high powered devices; (ii) an approach for training our model in various modes of weak supervision with progressively better disentanglement as more labels, such as pose categories, are provided; (iii) a novel unsupervised part-based loss for fitting our model to 2D images which is better than traditional photometric or silhouette losses; (iv) finally, we release a new dataset of high resolution 3D human foot scans, Foot3D. On this dataset, we show our model outperforms a strong PCA implementation trained on the same data in terms of shape quality and part correspondences, and that our novel unsupervised part-based loss improves inference on images.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1825766207",
                        "name": "Oliver Boyne"
                    },
                    {
                        "authorId": "2055745260",
                        "name": "James Charles"
                    },
                    {
                        "authorId": "1745672",
                        "name": "R. Cipolla"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "627881be799bf762b682d3fdc3f54c692d5b53bd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-04888",
                    "ArXiv": "2210.04888",
                    "DOI": "10.48550/arXiv.2210.04888",
                    "CorpusId": 252780848
                },
                "corpusId": 252780848,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/627881be799bf762b682d3fdc3f54c692d5b53bd",
                "title": "EVA3D: Compositional 3D Human Generation from 2D Image Collections",
                "abstract": "Inverse graphics aims to recover 3D models from 2D observations. Utilizing differentiable rendering, recent 3D-aware generative models have shown impressive results of rigid object generation using 2D images. However, it remains challenging to generate articulated objects, like human bodies, due to their complexity and diversity in poses and appearances. In this work, we propose, EVA3D, an unconditional 3D human generative model learned from 2D image collections only. EVA3D can sample 3D humans with detailed geometry and render high-quality images (up to 512x256) without bells and whistles (e.g. super resolution). At the core of EVA3D is a compositional human NeRF representation, which divides the human body into local parts. Each part is represented by an individual volume. This compositional representation enables 1) inherent human priors, 2) adaptive allocation of network parameters, 3) efficient training and rendering. Moreover, to accommodate for the characteristics of sparse 2D human image collections (e.g. imbalanced pose distribution), we propose a pose-guided sampling strategy for better GAN learning. Extensive experiments validate that EVA3D achieves state-of-the-art 3D human generation performance regarding both geometry and texture quality. Notably, EVA3D demonstrates great potential and scalability to\"inverse-graphics\"diverse human bodies with a clean framework.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1568986485",
                        "name": "Fangzhou Hong"
                    },
                    {
                        "authorId": "2111607225",
                        "name": "Zhaoxi Chen"
                    },
                    {
                        "authorId": "2055985023",
                        "name": "Yushi Lan"
                    },
                    {
                        "authorId": "50379842",
                        "name": "Liang Pan"
                    },
                    {
                        "authorId": "2164276341",
                        "name": "Ziwei Liu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Recent work of [38, 57] leverage photo-geometric autoencoding and neural rendering [49] to reconstruct depth map, while estimating albedo map, viewpoint and lighting condition from a single image.",
                "Compared to the SFS-based approaches [27, 38, 57], ours"
            ],
            "citingPaper": {
                "paperId": "ee38c3296bd64bbc47d3bbeced00c598f4859d02",
                "externalIds": {
                    "DBLP": "conf/mm/ZhaoCSLJ22",
                    "DOI": "10.1145/3503161.3548285",
                    "CorpusId": 252783101
                },
                "corpusId": 252783101,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ee38c3296bd64bbc47d3bbeced00c598f4859d02",
                "title": "Uncertainty-Aware Semi-Supervised Learning of 3D Face Rigging from Single Image",
                "abstract": "We present a method to rig 3D faces via Action Units (AUs), viewpoint and light direction, from single input image. Existing 3D methods for face synthesis and animation rely heavily on 3D morphable model (3DMM), which was built on 3D data and cannot provide intuitive expression parameters, while AU-driven 2D methods cannot handle head pose and lighting effect. We bridge the gap by integrating a recent 3D reconstruction method with 2D AU-driven method in a semi-supervised fashion. Built upon the auto-encoding 3D face reconstruction model that decouples depth, albedo, viewpoint and light without any supervision, we further decouple expression from identity for depth and albedo with a novel conditional feature translation module and pretrained critics for AU intensity estimation and image classification. Novel objective functions are designed using unlabeled in-the-wild images and in-door images with AU labels. We also leverage uncertainty losses to model the probably changing AU region of images as input noise for synthesis, and model the noisy AU intensity labels for intensity estimation of the AU critic. Experiments with face editing and animation on four datasets show that, compared with six state-of-the-art methods, our proposed method is superior and effective on expression consistency, identity similarity and pose similarity.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2151270112",
                        "name": "Yong Zhao"
                    },
                    {
                        "authorId": "2118441776",
                        "name": "Haifeng Chen"
                    },
                    {
                        "authorId": "48077408",
                        "name": "H. Sahli"
                    },
                    {
                        "authorId": "2153781760",
                        "name": "Ke Lu"
                    },
                    {
                        "authorId": "48219791",
                        "name": "D. Jiang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Other methods focus on wild images and reconstruct 3D faces in supervised [15, 66] or unsupervised ways [33, 49]."
            ],
            "citingPaper": {
                "paperId": "186e4a7ab8676d1bd95682cd7617a6e65675104c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-05300",
                    "DOI": "10.1145/3503161.3547791",
                    "CorpusId": 250533889
                },
                "corpusId": 250533889,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/186e4a7ab8676d1bd95682cd7617a6e65675104c",
                "title": "SD-GAN: Semantic Decomposition for Face Image Synthesis with Discrete Attribute",
                "abstract": "Manipulating latent code in generative adversarial networks (GANs) for facial image synthesis mainly focuses on continuous attribute synthesis (e.g., age, pose and emotion), while discrete attribute synthesis (like face mask and eyeglasses) receives less attention. Directly applying existing works to facial discrete attributes may cause inaccurate results. In this work, we propose an innovative framework to tackle challenging facial discrete attribute synthesis via semantic decomposing, dubbed SD-GAN. To be concrete, we explicitly decompose the discrete attribute representation into two components, i.e. the semantic prior basis and offset latent representation. The semantic prior basis shows an initializing direction for manipulating face representation in the latent space. The offset latent presentation obtained by 3D-aware semantic fusion network is proposed to adjust prior basis. In addition, the fusion network integrates 3D embedding for better identity preservation and discrete attribute synthesis. The combination of prior basis and offset latent representation enable our method to synthesize photo-realistic face images with discrete attributes. Notably, we construct a large and valuable dataset MEGN (Face Mask and Eyeglasses images crawled from Google and Naver) for completing the lack of discrete attributes in the existing dataset. Extensive qualitative and quantitative experiments demonstrate the state-of-the-art performance of our method. Our code is available at an anonymous website: https://github.com/MontaEllis/SD-GAN.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1491232168",
                        "name": "Kangneng Zhou"
                    },
                    {
                        "authorId": "2159182559",
                        "name": "Xiaobin Zhu"
                    },
                    {
                        "authorId": "1380181436",
                        "name": "Daiheng Gao"
                    },
                    {
                        "authorId": "2176278333",
                        "name": "Kai Lee"
                    },
                    {
                        "authorId": "2108191762",
                        "name": "Xinjie Li"
                    },
                    {
                        "authorId": "1682664",
                        "name": "Xu-Cheng Yin"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "While aforementioned 2D GANs [15], [17], [22], [23] allow explicit head pose control to some extent, they fail to guarantee appearance consistency, leading to inconsistent identity or facial attributes when viewed from vastly different angles."
            ],
            "citingPaper": {
                "paperId": "4d0b528bc9f7e7e373dbba62bea8d9df43cffcca",
                "externalIds": {
                    "ArXiv": "2209.05434",
                    "CorpusId": 252917593
                },
                "corpusId": 252917593,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4d0b528bc9f7e7e373dbba62bea8d9df43cffcca",
                "title": "3DFaceShop: Explicitly Controllable 3D-Aware Portrait Generation",
                "abstract": "In contrast to the traditional avatar creation pipeline which is a costly process, contemporary generative approaches directly learn the data distribution from photographs. While plenty of works extend unconditional generative models and achieve some levels of controllability, it is still challenging to ensure multi-view consistency, especially in large poses. In this work, we propose a network that generates 3D-aware portraits while being controllable according to semantic parameters regarding pose, identity, expression and illumination. Our network uses neural scene representation to model 3D-aware portraits, whose generation is guided by a parametric face model that supports explicit control. While the latent disentanglement can be further enhanced by contrasting images with partially different attributes, there still exists noticeable inconsistency in non-face areas, e.g., hair and background, when animating expressions. Wesolve this by proposing a volume blending strategy in which we form a composite output by blending dynamic and static areas, with two parts segmented from the jointly learned semantic field. Our method outperforms prior arts in extensive experiments, producing realistic portraits with vivid expression in natural lighting when viewed from free viewpoints. It also demonstrates generalization ability to real images as well as out-of-domain data, showing great promise in real applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152949334",
                        "name": "Junshu Tang"
                    },
                    {
                        "authorId": "145803569",
                        "name": "Bo Zhang"
                    },
                    {
                        "authorId": "2157857267",
                        "name": "Binxin Yang"
                    },
                    {
                        "authorId": "2146320438",
                        "name": "Ting Zhang"
                    },
                    {
                        "authorId": "47514557",
                        "name": "Dong Chen"
                    },
                    {
                        "authorId": "2149343311",
                        "name": "Lizhuang Ma"
                    },
                    {
                        "authorId": "1716835",
                        "name": "Fang Wen"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Training one cycle of the above three steps is not enough to reconstruct the 3D shape with fine details, hence we repeat these three steps four times to refine the 3D reconstructed results [20].",
                "[20] adopts an ellipsoid shape as the shape prior, giving better estimations on the face depth.",
                "However, the discovered semantic directions by these methods are still coupled [20] with other semantic concepts.",
                "Since the 3D shape reconstruction requires images of consistent multiple views and lighting, recent works [6], [7], [20], [33], [34] attempt to uncover extra cues to guide the learning process.",
                "Note that here we have the latent codes w for the original input images as discussed in Section III-C, hence we predict the offset \u2206w with an encoder E(\u00b7) to ease the training difficulty [20], whose optimization process can be denoted as",
                "[20], [33], [35] aim to manipulate the latent codes of StyleGAN [8], [9] to generate synthetic data for 3D shape learning.",
                "We take the iterative learning scheme of [20], and initialize the shape prior with an ellipsoid shape.",
                "However, the rendered images have unnatural distortions, we follow [20] to project the rendered images back to the latent space W of StyleGAN, which gives strong regularization on the projected images, such that they can have better quality.",
                "To reconstruct the 3D avatar shape from a single image, we follow the method proposed by [20], [34], where we use the manipulated images through StyleGAN to give the various viewpoint and lighting information.",
                "Hence we follow [20] to reconstruct the 3D cartoon shapes."
            ],
            "citingPaper": {
                "paperId": "161c7f7c41e73513ecb1a99bfc00a560a1b3c625",
                "externalIds": {
                    "ArXiv": "2207.14425",
                    "DBLP": "journals/corr/abs-2207-14425",
                    "DOI": "10.48550/arXiv.2207.14425",
                    "CorpusId": 251196845
                },
                "corpusId": 251196845,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/161c7f7c41e73513ecb1a99bfc00a560a1b3c625",
                "title": "3D Cartoon Face Generation with Controllable Expressions from a Single GAN Image",
                "abstract": "In this paper, we investigate an open research task of generating 3D cartoon face shapes from single 2D GAN generated human faces and without 3D supervision, where we can also manipulate the facial expressions of the 3D shapes. To this end, we discover the semantic meanings of StyleGAN latent space, such that we are able to produce face images of various expressions, poses, and lighting by controlling the latent codes. Specifically, we first finetune the pretrained StyleGAN face model on the cartoon datasets. By feeding the same latent codes to face and cartoon generation models, we aim to realize the translation from 2D human face images to cartoon styled avatars. We then discover semantic directions of the GAN latent space, in an attempt to change the facial expressions while preserving the original identity. As we do not have any 3D annotations for cartoon faces, we manipulate the latent codes to generate images with different poses and lighting, such that we can reconstruct the 3D cartoon face shapes. We validate the efficacy of our method on three cartoon datasets qualitatively and quantitatively.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2359832",
                        "name": "Hongya Wang"
                    },
                    {
                        "authorId": "2604251",
                        "name": "Guosheng Lin"
                    },
                    {
                        "authorId": "1741126",
                        "name": "S. Hoi"
                    },
                    {
                        "authorId": "2158509654",
                        "name": "Chun Miao"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Recently, a few methods [57,47] have incorporated a pre-trained StyleGAN with a differentiable renderer, but they struggle with photorealism, high-resolution [47] and real image editing [57].",
                "In addition, a few unsupervised approaches have been proposed by adopting implicit 3D feature [42,43] or differentiable renderer [57,47] in generation."
            ],
            "citingPaper": {
                "paperId": "1a47926f35211ba5aeeaff0e5d373d1f1e44f6d0",
                "externalIds": {
                    "DBLP": "conf/eccv/KwakLYKHK22",
                    "ArXiv": "2207.10257",
                    "DOI": "10.48550/arXiv.2207.10257",
                    "CorpusId": 250921216
                },
                "corpusId": 250921216,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/1a47926f35211ba5aeeaff0e5d373d1f1e44f6d0",
                "title": "Injecting 3D Perception of Controllable NeRF-GAN into StyleGAN for Editable Portrait Image Synthesis",
                "abstract": "Over the years, 2D GANs have achieved great successes in photorealistic portrait generation. However, they lack 3D understanding in the generation process, thus they suffer from multi-view inconsistency problem. To alleviate the issue, many 3D-aware GANs have been proposed and shown notable results, but 3D GANs struggle with editing semantic attributes. The controllability and interpretability of 3D GANs have not been much explored. In this work, we propose two solutions to overcome these weaknesses of 2D GANs and 3D-aware GANs. We first introduce a novel 3D-aware GAN, SURF-GAN, which is capable of discovering semantic attributes during training and controlling them in an unsupervised manner. After that, we inject the prior of SURF-GAN into StyleGAN to obtain a high-fidelity 3D-controllable generator. Unlike existing latent-based methods allowing implicit pose control, the proposed 3D-controllable StyleGAN enables explicit pose control over portrait generation. This distillation allows direct compatibility between 3D control and many StyleGAN-based techniques (e.g., inversion and stylization), and also brings an advantage in terms of computational resources. Our codes are available at https://github.com/jgkwak95/SURF-GAN.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2016481042",
                        "name": "Jeong-gi Kwak"
                    },
                    {
                        "authorId": "2111164052",
                        "name": "Yuanming Li"
                    },
                    {
                        "authorId": "2146861723",
                        "name": "Dongsik Yoon"
                    },
                    {
                        "authorId": "2145183568",
                        "name": "Donghyeon Kim"
                    },
                    {
                        "authorId": "2757702",
                        "name": "D. Han"
                    },
                    {
                        "authorId": "144878703",
                        "name": "Hanseok Ko"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "caf4dc36923e7447ec974ce647e3ff83fc7300e6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-10309",
                    "ArXiv": "2207.10309",
                    "DOI": "10.48550/arXiv.2207.10309",
                    "CorpusId": 263787709
                },
                "corpusId": 263787709,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/caf4dc36923e7447ec974ce647e3ff83fc7300e6",
                "title": "A Survey on Leveraging Pre-trained Generative Adversarial Networks for Image Editing and Restoration",
                "abstract": "Generative adversarial networks (GANs) have drawn enormous attention due to the simple yet effective training mechanism and superior image generation quality. With the ability to generate photo-realistic high-resolution (e.g., $1024\\times1024$) images, recent GAN models have greatly narrowed the gaps between the generated images and the real ones. Therefore, many recent works show emerging interest to take advantage of pre-trained GAN models by exploiting the well-disentangled latent space and the learned GAN priors. In this paper, we briefly review recent progress on leveraging pre-trained large-scale GAN models from three aspects, i.e., 1) the training of large-scale generative adversarial networks, 2) exploring and understanding the pre-trained GAN models, and 3) leveraging these models for subsequent tasks like image restoration and editing. More information about relevant methods and repositories can be found at https://github.com/csmliu/pretrained-GANs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Ming Liu"
                    },
                    {
                        "authorId": "2156252416",
                        "name": "Yuxiang Wei"
                    },
                    {
                        "authorId": "39637222",
                        "name": "Xiaohe Wu"
                    },
                    {
                        "authorId": "2243334363",
                        "name": "Wangmeng Zuo"
                    },
                    {
                        "authorId": "2256831865",
                        "name": "Lei Zhang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "[38] recover the geometric cues from pre-trained 2D GANs and achieve exceptional reconstruction results, but the reconstructed shapes are limited to 2."
            ],
            "citingPaper": {
                "paperId": "3bcf42780f142e0c5d7efc6741018a34ed161c3c",
                "externalIds": {
                    "ArXiv": "2207.10061",
                    "DBLP": "conf/eccv/ZhangRCYDL22",
                    "DOI": "10.48550/arXiv.2207.10061",
                    "CorpusId": 250698796
                },
                "corpusId": 250698796,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/3bcf42780f142e0c5d7efc6741018a34ed161c3c",
                "title": "Monocular 3D Object Reconstruction with GAN Inversion",
                "abstract": "Recovering a textured 3D mesh from a monocular image is highly challenging, particularly for in-the-wild objects that lack 3D ground truths. In this work, we present MeshInversion, a novel framework to improve the reconstruction by exploiting the generative prior of a 3D GAN pre-trained for 3D textured mesh synthesis. Reconstruction is achieved by searching for a latent space in the 3D GAN that best resembles the target mesh in accordance with the single view observation. Since the pre-trained GAN encapsulates rich 3D semantics in terms of mesh geometry and texture, searching within the GAN manifold thus naturally regularizes the realness and fidelity of the reconstruction. Importantly, such regularization is directly applied in the 3D space, providing crucial guidance of mesh parts that are unobserved in the 2D space. Experiments on standard benchmarks show that our framework obtains faithful 3D reconstructions with consistent geometry and texture across both observed and unobserved parts. Moreover, it generalizes well to meshes that are less commonly seen, such as the extended articulation of deformable objects. Code is released at https://github.com/junzhezhang/mesh-inversion",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2107967578",
                        "name": "Junzhe Zhang"
                    },
                    {
                        "authorId": "1781272237",
                        "name": "Daxuan Ren"
                    },
                    {
                        "authorId": "66562436",
                        "name": "Zhongang Cai"
                    },
                    {
                        "authorId": "1751452",
                        "name": "C. Yeo"
                    },
                    {
                        "authorId": "144445937",
                        "name": "Bo Dai"
                    },
                    {
                        "authorId": "1717179",
                        "name": "Chen Change Loy"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "5D, depth \u2717 \u2717 GAN models, pre-trained \u2713 GAN2Shape [44] 2.",
                "Similarly, GAN2Shape [44] produces an unsupervised decomposition by using a GAN model as supervision.",
                "Early attempts [44,52,63] are made to mine 3D geometric cues from the pretrained 2D GAN models in an unsupervised manner."
            ],
            "citingPaper": {
                "paperId": "762d9b045fd2e6fe521720de57d8fc441fff7746",
                "externalIds": {
                    "ArXiv": "2207.10183",
                    "DBLP": "journals/corr/abs-2207-10183",
                    "DOI": "10.48550/arXiv.2207.10183",
                    "CorpusId": 250920692
                },
                "corpusId": 250920692,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/762d9b045fd2e6fe521720de57d8fc441fff7746",
                "title": "2D GANs Meet Unsupervised Single-view 3D Reconstruction",
                "abstract": "Recent research has shown that controllable image generation based on pre-trained GANs can benefit a wide range of computer vision tasks. However, less attention has been devoted to 3D vision tasks. In light of this, we propose a novel image-conditioned neural implicit field, which can leverage 2D supervisions from GAN-generated multi-view images and perform the single-view reconstruction of generic objects. Firstly, a novel offline StyleGAN-based generator is presented to generate plausible pseudo images with full control over the viewpoint. Then, we propose to utilize a neural implicit function, along with a differentiable renderer to learn 3D geometry from pseudo images with object masks and rough pose initializations. To further detect the unreliable supervisions, we introduce a novel uncertainty module to predict uncertainty maps, which remedy the negative effect of uncertain regions in pseudo images, leading to a better reconstruction performance. The effectiveness of our approach is demonstrated through superior single-view 3D reconstruction results of generic objects.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152943473",
                        "name": "Feng Liu"
                    },
                    {
                        "authorId": "2111119747",
                        "name": "Xiaoming Liu"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "contexts": [
                "Additionally, we add a depth smoothness loss Lds used in [3, 27] to reduce inference error of surface normals.",
                "Differently, the semantics-embedding networks of GAN2Shape use the rendered images as a intermediary, while LiftedGAN does not and achieves high-fidelity rotation results in a large angle range.",
                "As GAN2Shape has to do GAN inversion to produce multi-view images, it will have more model parameters and cost much more time to retrain its networks for each input image.",
                "Besides, GAN2Shape, while produces realistic qualitative results, is a resource-consuming method that should additionally\nre-train StyleGAN2.",
                "GAN2Shape [3] and [16] uses randomly sampled view to render more versions of input for learning, while SMR uses interpolated attributes to avoid unreasonable sampling values.",
                "7, are not vivid enough comparing to GAN2shape [3].",
                "GAN2Shape [3] and LiftedGAN [24] use neural networks and a differentiable renderer [25] for reasoning the mapping process by semantics-embedding-semantics selfmapping.",
                "Experiments show that ASRMM outperforms the stateof-the-art unsupervised shape learning methods [3] on BFM [8] dataset in some reconstruction metric.",
                "To further improve the reconstruction quality, some methods consider to sample and generate novel-view images to enrich the diversity of the training data, while this idea may require silhouette annotations [1, 2] or models of inferring novel-view images [3].",
                "In contrast, our ASRMM focuses on a light way to improve the reconstruction accuracy, without relying on heavy prior models for view changing or relighting, but our ASRMM is also inspired by [3, 22, 24] that style-transferred images can improve the diversity of the input, and we transfer image style by making the material monotonous.",
                "7 Limitations of our proposed model comparing to GAN2Shape [3].",
                "Although GAN2Shape [3] introduces a strong image generator StyleGAN2 [20] to produce vivid novel-view images, ASRMM still outperforms it in SIDE and reach close performance in MAD.",
                "In Table 4, we also compare ASRMM with Unsup3d and GAN2Shape in terms of model complexity and time consuming.",
                "Unsupervised Shape Reconstruction We quantitatively compare our ASRMM with a fully-supervised baseline and two unsupervised methods [3, 8] in Table 4."
            ],
            "citingPaper": {
                "paperId": "bd7497c11a8c2e41bdcf910e8145ede3bdd7f07c",
                "externalIds": {
                    "DBLP": "journals/apin/FangX23",
                    "DOI": "10.1007/s10489-022-03724-9",
                    "CorpusId": 250545945
                },
                "corpusId": 250545945,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bd7497c11a8c2e41bdcf910e8145ede3bdd7f07c",
                "title": "Self-supervised reflectance-guided 3d shape reconstruction from single-view images",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2176301850",
                        "name": "Binbin Fang"
                    },
                    {
                        "authorId": "1713125",
                        "name": "N. Xiao"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "9edc32aeb60b2c38450bc1eb29da829d1bc5f72c",
                "externalIds": {
                    "ArXiv": "2207.05300",
                    "CorpusId": 250451057
                },
                "corpusId": 250451057,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9edc32aeb60b2c38450bc1eb29da829d1bc5f72c",
                "title": "SD-GAN: Semantic Decomposition for Face Image Synthesis with Discrete Attribute",
                "abstract": "Manipulating latent code in generative adversarial networks (GANs) for facial image synthesis mainly focuses on continuous attribute synthesis (e.g., age, pose and emotion), while discrete attribute synthesis (like face mask and eyeglasses) receives less attention. Directly applying existing works to facial discrete attributes may cause inaccurate results. In this work, we propose an innovative framework to tackle challenging facial discrete attribute synthesis via semantic decomposing, dubbed SD-GAN. To be concrete, we explicitly decompose the discrete attribute representation into two components, i.e. the semantic prior basis and offset latent representation. The semantic prior basis shows an initializing direction for manipulating face representation in the latent space. The offset latent presentation obtained by 3D-aware semantic fusion network is proposed to adjust prior basis. In addition, the fusion network integrates 3D embedding for better identity preservation and discrete attribute synthesis. The combination of prior basis and offset latent representation enable our method to synthesize photo-realistic face images with discrete attributes. Notably, we construct a large and valuable dataset MEGN (Face Mask and Eyeglasses images crawled from Google and Naver) for completing the lack of discrete attributes in the existing dataset. Extensive qualitative and quantitative experiments demonstrate the state-of-the-art performance of our method. Our code is available at: https://github.com/MontaEllis/SD-GAN.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1491232168",
                        "name": "Kangneng Zhou"
                    },
                    {
                        "authorId": "2159182559",
                        "name": "Xiaobin Zhu"
                    },
                    {
                        "authorId": "1380181436",
                        "name": "Daiheng Gao"
                    },
                    {
                        "authorId": "2175782177",
                        "name": "Lee Kai"
                    },
                    {
                        "authorId": "2108191762",
                        "name": "Xinjie Li"
                    },
                    {
                        "authorId": "2146267053",
                        "name": "Xu-Cheng Yin"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "An efficient unsupervised approach for reconstructing 3D shapes has been discussed in paper [2]."
            ],
            "citingPaper": {
                "paperId": "a39cc2fc284ff3ba23a4550c71aba4b4af09cb67",
                "externalIds": {
                    "DOI": "10.1109/CONIT55038.2022.9848353",
                    "CorpusId": 251762717
                },
                "corpusId": 251762717,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a39cc2fc284ff3ba23a4550c71aba4b4af09cb67",
                "title": "3DVAEReCNN: Region-based Convolutional Neural Network for Volumetric Rendering of Indoor Scenes",
                "abstract": "3D Reconstructions are being appreciated across various fields as a more informative means of visualisation that offers great insight about the qualitative characteristics of the objects or scene under consideration. Hence more research is being carried out in this area as 3D are proving to be of great help to fields such as medicine, for improving the diagnostic accuracy and surgical precision of the medical procedure. It has also found applications in fields such as intelligent robot navigation by reproduction of the depth map of a scene, object recognition and so on. We present a unique proposition to synthesize volumetric reconstructions from a singular or multiple positions of view, based on RGB images/videos. The project aims at rapidly generating all the different parts of the indoor environment, without having to actually observe them in reality.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2114839341",
                        "name": "Karan Gala"
                    },
                    {
                        "authorId": "2138524647",
                        "name": "Pravesh Ganwani"
                    },
                    {
                        "authorId": "2072230005",
                        "name": "R. Kulkarni"
                    },
                    {
                        "authorId": "143643904",
                        "name": "R. Pawar"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "contexts": [
                "Modern GANs are a lot of engineering and it often takes a lot of futile experiments to get to a point where the obtained performance is acceptable.",
                "We believe that the future of 3D GANs is a combination of efficient volumetric representations, regularized 2D upsamplers, and patch-wise training.",
                "In contrast to classical NeRF [38], we do not utilize view direction conditioning since it worsens multi-view consistency [7] in GANs which are trained on RGB datasets with a single view per instance.",
                "Note that this high training efficiency is achieved without the use of an upsampler, which initially enabled high-resolution synthesis of 3D-aware GANs.",
                "Also, in contrast to upsampler-based 3D GANs, our generator can naturally incorporate the techniques from the traditional NeRF literature.",
                "NeRF-based GANs.",
                "Compared to upsampler-based 3D GANs [15, 43, 72, 79, 6, 78], we use a pure NeRF [38] as our generator G and utilize the tri-plane representation [6, 8] as the backbone.",
                "Recently, there appeared works which train from single-view RGB only, including mesh-generation methods [19, 73, 53] and methods that extract 3D structure from pretrained 2D GANs [58, 48].",
                "Apart from that, we also compare to pi-GAN [7] and GRAM [12], which are non-upsampler-based GANs.",
                "Finally, 3D GANs generating faces and humans may have negative societal impact as discussed in Appx G.",
                "Patch-wise training of NeRF-based GANs was originally proposed by GRAF [56] and got largely neglected by the community since then.",
                "But for NeRF-based GANs, it becomes prohibitively expensive for high resolutions since convolutional discriminators operate on dense full-size images.",
                "Training NeRF-based GANs is computationally expensive, because rendering each pixel via volumetric rendering requires many evaluations (e.g., in our case, 96) of the underlying MLP.",
                "People address these scaling issues of NeRF-based GANs in different ways, but the dominating approach is to train a separate 2D decoder to produce a high-resolution image from a low-resolution image or feature grid rendered from a NeRF backbone [43]."
            ],
            "citingPaper": {
                "paperId": "89a856845a3640f414eea896e088c9bf92466c75",
                "externalIds": {
                    "DBLP": "conf/nips/SkorokhodovT0W22",
                    "ArXiv": "2206.10535",
                    "DOI": "10.48550/arXiv.2206.10535",
                    "CorpusId": 249889658
                },
                "corpusId": 249889658,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/89a856845a3640f414eea896e088c9bf92466c75",
                "title": "EpiGRAF: Rethinking training of 3D GANs",
                "abstract": "A very recent trend in generative modeling is building 3D-aware generators from 2D image collections. To induce the 3D bias, such models typically rely on volumetric rendering, which is expensive to employ at high resolutions. During the past months, there appeared more than 10 works that address this scaling issue by training a separate 2D decoder to upsample a low-resolution image (or a feature tensor) produced from a pure 3D generator. But this solution comes at a cost: not only does it break multi-view consistency (i.e. shape and texture change when the camera moves), but it also learns the geometry in a low fidelity. In this work, we show that it is possible to obtain a high-resolution 3D generator with SotA image quality by following a completely different route of simply training the model patch-wise. We revisit and improve this optimization scheme in two ways. First, we design a location- and scale-aware discriminator to work on patches of different proportions and spatial positions. Second, we modify the patch sampling strategy based on an annealed beta distribution to stabilize training and accelerate the convergence. The resulted model, named EpiGRAF, is an efficient, high-resolution, pure 3D generator, and we test it on four datasets (two introduced in this work) at $256^2$ and $512^2$ resolutions. It obtains state-of-the-art image quality, high-fidelity geometry and trains ${\\approx} 2.5 \\times$ faster than the upsampler-based counterparts. Project website: https://universome.github.io/epigraf.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51118864",
                        "name": "Ivan Skorokhodov"
                    },
                    {
                        "authorId": "145582202",
                        "name": "S. Tulyakov"
                    },
                    {
                        "authorId": "2155345360",
                        "name": "Yiqun Wang"
                    },
                    {
                        "authorId": "1798011",
                        "name": "Peter Wonka"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Our method partially uses [32] as a tool, but we go beyond it to further explore if GANs can also be used to disentangle material properties.",
                "We first initialize Fs to produce an ellipsoid as a convex shape prior following [32].",
                "Different from [32], in this work the inverse rendering is based on the new non-Lambertian neural representation and rendering equation introduced before.",
                "4 shows the qualitative comparison between our approach and two unsupervised inverse rendering baselines Unsup3d [52] and GAN2Shape [32] on the CelebA dataset.",
                "GAN2Shape [32] assumes Lambertian reflectance and does not recover high-quality albedo, while [62] does\nnot disentangle albedo and illumination.",
                "GAN2Shape [32] assumes Lambertian reflectance and does not recover high-quality albedo, while [62] does not disentangle albedo and illumination.",
                "Similar to us, [32] and [62] also exploit GANs to reconstruct 3D shapes.",
                "Apart from Unsup3d and GAN2Shape, we also compare with pi-GAN [9] and ShadeGAN [33] that can perform unsupervised 3D reconstruction via GAN inversion.",
                "Inspired by [32], we adopt an exploration-and-exploitation algorithm to generate pseudo multi-view and multi-lighting images from a pretrained GAN.",
                "Having this dependence allows us to further extend our method for joint training on multiple instances as done in [32], which improves generalization.",
                "It is shown in the recent Shape-from-GAN works [32, 62] that the pseudo paired data generated by GAN can be used to reconstruct 3D shapes.",
                "In order to generate a number of approximated paired images of various viewpoint and lighting conditions using the GAN, we adopt an exploration and exploitation algorithm following [32]."
            ],
            "citingPaper": {
                "paperId": "41ff790ff81eab621fc82bc1f27d0ea1625ea353",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-09244",
                    "ArXiv": "2206.09244",
                    "DOI": "10.1109/3DV57658.2022.00081",
                    "CorpusId": 249889447
                },
                "corpusId": 249889447,
                "publicationVenue": {
                    "id": "4b02e809-1c26-4203-b9ba-311a418f664b",
                    "name": "International Conference on 3D Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf 3D Vis",
                        "3DV"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/41ff790ff81eab621fc82bc1f27d0ea1625ea353",
                "title": "GAN2X: Non-Lambertian Inverse Rendering of Image GANs",
                "abstract": "2D images are observations of the 3D physical world depicted with the geometry, material, and illumination components. Recovering these underlying intrinsic components from 2D images, also known as inverse rendering, usually requires a supervised setting with paired images collected from multiple viewpoints and lighting conditions, which is resource-demanding. In this work, we present GAN2X, a new method for unsupervised inverse rendering that only uses unpaired images for training. Unlike previous Shape-from-GAN approaches that mainly focus on 3D shapes, we take the first attempt to also recover non-Lambertian material properties by exploiting the pseudo paired data generated by a GAN. To achieve precise inverse rendering, we devise a specularity-aware neural surface representation that continuously models the geometry and material properties. A shading-based refinement technique is adopted to further distill information in the target image and recover more fine details. Experiments demonstrate that GAN2X can accurately decompose 2D images to 3D shape, albedo, and specular properties for different object categories, and achieves state-of-the-art performance for unsupervised single-view 3D face reconstruction. We also show its applications in downstream tasks including real image editing and lifting 2D GANs to decomposed 3D GANs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "14214933",
                        "name": "Xingang Pan"
                    },
                    {
                        "authorId": "9102722",
                        "name": "A. Tewari"
                    },
                    {
                        "authorId": "46458089",
                        "name": "Lingjie Liu"
                    },
                    {
                        "authorId": "1680185",
                        "name": "C. Theobalt"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "GAN2Shape [42] avoids such symmetric constraint but brings heavy per-image optimization.",
                "More recent works [42, 58] disentangle a face into intrinsic factors and accomplishes canonical reconstruction in an unsupervised manner via render-"
            ],
            "citingPaper": {
                "paperId": "1ba27595f9272490cb7bcea360978658b38bf7d2",
                "externalIds": {
                    "DBLP": "conf/cvpr/0005GTHWTHX22",
                    "DOI": "10.1109/CVPR52688.2022.00420",
                    "CorpusId": 250551746
                },
                "corpusId": 250551746,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1ba27595f9272490cb7bcea360978658b38bf7d2",
                "title": "Learning to Restore 3D Face from In-the-Wild Degraded Images",
                "abstract": "In-the-wild 3D face modelling is a challenging problem as the predicted facial geometry and texture suffer from a lack of reliable clues or priors, when the input images are degraded. To address such a problem, in this paper we propose a novel Learning to Restore (L2R) 3D face framework for unsupervised high-quality face reconstruction from low-resolution images. Rather than directly refining 2D image appearance, L2R learns to recover fine-grained 3D details on the proxy against degradation via extracting generative facial priors. Concretely, L2R proposes a novel albedo restoration network to model high-quality 3D facial texture, in which the diverse guidance from the pre-trained Generative Adversarial Networks (GANs) is leveraged to complement the lack of input facial clues. With the finer details of the restored 3D texture, L2R then learns displacement maps from scratch to enhance the significant facial structure and geometry. Both of the procedures are mutually optimized with a novel 3D-aware adversarial loss, which further improves the modelling performance and suppresses the potential uncertainty. Extensive experiments on benchmarks show that L2R outperforms state-of-the-art methods under the condition of low-quality inputs, and obtains superior performances than 2D pre-processed modelling approaches with limited 3D proxy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2144399891",
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "authorId": "83103152",
                        "name": "Yanhao Ge"
                    },
                    {
                        "authorId": "144970872",
                        "name": "Ying Tai"
                    },
                    {
                        "authorId": "2124765851",
                        "name": "Xiao-Ying Huang"
                    },
                    {
                        "authorId": "1978245",
                        "name": "Chengjie Wang"
                    },
                    {
                        "authorId": "2112388851",
                        "name": "H. Tang"
                    },
                    {
                        "authorId": "3034852",
                        "name": "Dongjin Huang"
                    },
                    {
                        "authorId": "1724454",
                        "name": "Zhifeng Xie"
                    },
                    {
                        "authorId": "2176289497",
                        "name": "Tencent Youtu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Gan2Shape [40] and LiftedGAN [51] try to distill knowledge from 2D GANs for 3D reconstruction."
            ],
            "citingPaper": {
                "paperId": "9db416c089d917e7ab489a4fb5829928432c5d6f",
                "externalIds": {
                    "DBLP": "conf/cvpr/0005GTCCLTHWXH22",
                    "DOI": "10.1109/CVPR52688.2022.01971",
                    "CorpusId": 250647716
                },
                "corpusId": 250647716,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9db416c089d917e7ab489a4fb5829928432c5d6f",
                "title": "Physically-guided Disentangled Implicit Rendering for 3D Face Modeling",
                "abstract": "This paper presents a novel Physically-guided Disentangled Implicit Rendering (PhyDIR) framework for highfidelity 3D face modeling. The motivation comes from two observations: Widely-used graphics renderers yield excessive approximations against photo-realistic imaging, while neural rendering methods produce superior appearances but are highly entangled to perceive 3D-aware operations. Hence, we learn to disentangle the implicit rendering via explicit physical guidance, while guaranteeing the properties of: (1) 3D-aware comprehension and (2) high-reality image formation. For the former one, PhyDIR explicitly adopts 3D shading and rasterizing modules to control the renderer, which disentangles the light, facial shape, and viewpoint from neural reasoning. Specifically, PhyDIR proposes a novel multi-image shading strategy to compensate for the monocular limitation, so that the lighting variations are accessible to the neural renderer. For the latter, PhyDIR learns the face-collection implicit texture to avoid ill-posed intrinsic factorization, then leverages a series of consistency losses to constrain the rendering robustness. With the disentangled method, we make 3D face modeling benefit from both kinds of rendering strategies. Extensive experiments on benchmarks show that PhyDIR obtains superior performance than state-of-the-art explicit/implicit methods on geometry/texture modeling.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144470702",
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "authorId": "83103152",
                        "name": "Yanhao Ge"
                    },
                    {
                        "authorId": "144970872",
                        "name": "Ying Tai"
                    },
                    {
                        "authorId": "2075437879",
                        "name": "Weijian Cao"
                    },
                    {
                        "authorId": "1625895990",
                        "name": "Renwang Chen"
                    },
                    {
                        "authorId": "2118800472",
                        "name": "Kunlin Liu"
                    },
                    {
                        "authorId": "2178852343",
                        "name": "Hao Tang"
                    },
                    {
                        "authorId": "2124765851",
                        "name": "Xiao-Ying Huang"
                    },
                    {
                        "authorId": "1978245",
                        "name": "Chengjie Wang"
                    },
                    {
                        "authorId": "1724454",
                        "name": "Zhifeng Xie"
                    },
                    {
                        "authorId": "3034852",
                        "name": "Dongjin Huang"
                    },
                    {
                        "authorId": "2176289497",
                        "name": "Tencent Youtu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "ized by certain latent space in unsupervised GANs, contains rich 3D geometric clues [11], such that walking along certain paths on the manifold could lead to meaningful geometric transformation of an underlying object (e."
            ],
            "citingPaper": {
                "paperId": "c29e2f7ae6be0e90687d149cc9bb7c524dae256d",
                "externalIds": {
                    "DBLP": "journals/tvcg/ChenFZZ23",
                    "DOI": "10.1109/TVCG.2022.3166159",
                    "CorpusId": 248100175,
                    "PubMed": "35404818"
                },
                "corpusId": 248100175,
                "publicationVenue": {
                    "id": "5e1f6444-5d03-48c7-b202-7f47d492aeae",
                    "name": "IEEE Transactions on Visualization and Computer Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Vis Comput Graph"
                    ],
                    "issn": "1077-2626",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=2945"
                },
                "url": "https://www.semanticscholar.org/paper/c29e2f7ae6be0e90687d149cc9bb7c524dae256d",
                "title": "OrthoAligner: Image-Based Teeth Alignment Prediction via Latent Style Manipulation",
                "abstract": "In this article, we present OrthoAligner, a novel method to predict the visual outcome of orthodontic treatment in a portrait image. Unlike the state-of-the-art method, which relies on a 3D teeth model obtained from dental scanning, our method generates realistic alignment effects in images without requiring additional 3D information as input and thus making our system readily available to average users. The key of our approach is to employ the 3D geometric information encoded in an unsupervised generative model, i.e., StyleGAN in this article. Instead of directly conducting translation in the image space, we embed the teeth region extracted from a given portrait to the latent space of the StyleGAN generator and propose a novel latent editing method to discover a geometrically meaningful editing path that yields the alignment process in the image space. To blend the edited mouth region with the original portrait image, we further introduce a BlendingNet to remove boundary artifacts and correct color inconsistency. We also extend our method to short video clips by propagating the alignment effects across neighboring frames. We evaluate our method in various orthodontic cases, compare it to the state-of-the-art and competitive baselines, and validate the effectiveness of each component.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "32769975",
                        "name": "Beijia Chen"
                    },
                    {
                        "authorId": "3169698",
                        "name": "Hongbo Fu"
                    },
                    {
                        "authorId": "1423651904",
                        "name": "Kun Zhou"
                    },
                    {
                        "authorId": "3049304",
                        "name": "Youyi Zheng"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Template-based methods assume that all target shapes can be represented by deforming a template mesh, usually a sphere [5, 10, 22, 27, 35, 43] or a plane [33, 46]."
            ],
            "citingPaper": {
                "paperId": "52b7cfc383b754b176f780aa873c99a83e717676",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-03105",
                    "ArXiv": "2204.03105",
                    "DOI": "10.1109/CVPR52688.2022.00152",
                    "CorpusId": 248006380
                },
                "corpusId": 248006380,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/52b7cfc383b754b176f780aa873c99a83e717676",
                "title": "AUV-Net: Learning Aligned UV Maps for Texture Transfer and Synthesis",
                "abstract": "In this paper, we address the problem of texture representation for 3D shapes for the challenging and under-explored tasks of texture transfer and synthesis. Previous works either apply spherical texture maps which may lead to large distortions, or use continuous texture fields that yield smooth outputs lacking details. We argue that the traditional way of representing textures with images and linking them to a 3D mesh via UV mapping is more desirable, since synthesizing 2D images is a well-studied problem. We propose AUV-Net which learns to embed 3D surfaces into a 2D aligned UV space, by mapping the corresponding semantic parts of different 3D shapes to the same location in the UV space. As a result, textures are aligned across objects, and can thus be easily synthesized by generative models of images. Texture alignment is learned in an unsupervised manner by a simple yet effective texture alignment module, taking inspiration from traditional works on linear subspace learning. The learned UV mapping and aligned texture representations enable a variety of applications including texture transfer, texture synthesis, and textured single view 3D reconstruction. We conduct experiments on multiple datasets to demonstrate the effectiveness of our method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2406577",
                        "name": "Zhiqin Chen"
                    },
                    {
                        "authorId": "2042641",
                        "name": "K. Yin"
                    },
                    {
                        "authorId": "37895334",
                        "name": "S. Fidler"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "c0f9feb4a6dae31388d9c0cb9e24ea6fad104a96",
                "externalIds": {
                    "DOI": "10.1088/1742-6596/2258/1/012051",
                    "CorpusId": 248466974
                },
                "corpusId": 248466974,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c0f9feb4a6dae31388d9c0cb9e24ea6fad104a96",
                "title": "Feature Sharing Attention 3D Face Reconstruction with Unsupervised Learning from In-the-Wild Photo Collection",
                "abstract": "3D face reconstruction is an active research topic in the field of computer vision research. It has a wide range of application scenarios. The number of 3D face annotation samples is small and the accuracy of the 3D reconstruction algorithm is insufficient. In this paper, we propose a shared attention module and a feature point constraint mechanism, and proposes a 3D face reconstruction algorithm that takes a single face image as input and uses a encoder-decoder network to predict the 3D face. The prediction view and light network introduces the attention mechanism module, which can improve the feature extraction ability of the network. The albedo and depth introduces shared features to help extract richer facial feature information. At the same time, the self-monitoring method of single image input avoids the high requirements of traditional methods for datasets. We conducted comparative experiments and ablation experiments on the BFM, and CelebA face datasets. The experimental results show that compared with Unsup3d and other representative 3D Face reconstruction algorithms, it has a certain improvement in the two evaluation indexes of Scale Invariant Depth Error (SIDE) and Mean Angle Deviation (MAD).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2112078192",
                        "name": "Xiaoxiao Yang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "More relevant topics to 3D-aware deformation would be (1) human pose transfer [6, 29, 33], which works only for human bodies, (2) novel view synthesis [13, 37, 51], which is limited to altering a viewpoint of an image, and (3) 3D modelbased manipulation [26], which requires the exact 3D model of the object in an image."
            ],
            "citingPaper": {
                "paperId": "edc11fe17afc3379929386df1c2fd4645814dcf1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-15235",
                    "ArXiv": "2203.15235",
                    "DOI": "10.1109/CVPR52688.2022.01798",
                    "CorpusId": 247778849
                },
                "corpusId": 247778849,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/edc11fe17afc3379929386df1c2fd4645814dcf1",
                "title": "Pop-Out Motion: 3D-Aware Image Deformation via Learning the Shape Laplacian",
                "abstract": "We propose a framework that can deform an object in a 2D image as it exists in 3D space. Most existing methods for 3D-aware image manipulation are limited to (1) only changing the global scene information or depth, or (2) manipulating an object of specific categories. In this paper, we present a 3D-aware image deformation method with minimal restrictions on shape category and deformation type. While our framework leverages 2D-to-3D reconstruction, we argue that reconstruction is not sufficient for realistic deformations due to the vulnerability to topological errors. Thus, we propose to take a supervised learning-based approach to predict the shape Laplacian of the underlying volume of a 3D reconstruction represented as a point cloud. Given the deformation energy calculated using the predicted shape Laplacian and user-defined deformation handles (e.g., keypoints), we obtain bounded biharmonic weights to model plausible handle-based image deformation. In the experiments, we present our results of deforming 2D character and clothed human images. We also quantitatively show that our approach can produce more accurate deformation weights compared to alternative methods (i.e., mesh reconstruction and point cloud Laplacian methods).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108586991",
                        "name": "Jihyun Lee"
                    },
                    {
                        "authorId": "46461051",
                        "name": "Minhyuk Sung"
                    },
                    {
                        "authorId": "2155095065",
                        "name": "Hyun-jung Kim"
                    },
                    {
                        "authorId": "143617697",
                        "name": "Tae-Kyun Kim"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                ", not deviate too far from mean face [58,26,27,4,44])."
            ],
            "citingPaper": {
                "paperId": "85490463b81b6682d394171d5d544870bf14f14e",
                "externalIds": {
                    "DBLP": "conf/eccv/ChaiZRKXZYB22",
                    "ArXiv": "2203.09729",
                    "DOI": "10.48550/arXiv.2203.09729",
                    "CorpusId": 247594925
                },
                "corpusId": 247594925,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/85490463b81b6682d394171d5d544870bf14f14e",
                "title": "REALY: Rethinking the Evaluation of 3D Face Reconstruction",
                "abstract": "The evaluation of 3D face reconstruction results typically relies on a rigid shape alignment between the estimated 3D model and the ground-truth scan. We observe that aligning two shapes with different reference points can largely affect the evaluation results. This poses difficulties for precisely diagnosing and improving a 3D face reconstruction method. In this paper, we propose a novel evaluation approach with a new benchmark REALY, consists of 100 globally aligned face scans with accurate facial keypoints, high-quality region masks, and topology-consistent meshes. Our approach performs region-wise shape alignment and leads to more accurate, bidirectional correspondences during computing the shape errors. The fine-grained, region-wise evaluation results provide us detailed understandings about the performance of state-of-the-art 3D face reconstruction methods. For example, our experiments on single-image based reconstruction methods reveal that DECA performs the best on nose regions, while GANFit performs better on cheek regions. Besides, a new and high-quality 3DMM basis, HIFI3D++, is further derived using the same procedure as we construct REALY to align and retopologize several 3D face datasets. We will release REALY, HIFI3D++, and our new evaluation pipeline at https://realy3dface.com.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "150355978",
                        "name": "Zenghao Chai"
                    },
                    {
                        "authorId": "2143781255",
                        "name": "Haoxian Zhang"
                    },
                    {
                        "authorId": "1439352749",
                        "name": "Jing Ren"
                    },
                    {
                        "authorId": "2151879170",
                        "name": "Di Kang"
                    },
                    {
                        "authorId": "1390847590",
                        "name": "Zhengzhuo Xu"
                    },
                    {
                        "authorId": "9621748",
                        "name": "Xuefei Zhe"
                    },
                    {
                        "authorId": "2117728946",
                        "name": "Chun Yuan"
                    },
                    {
                        "authorId": "2780029",
                        "name": "Linchao Bao"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "contexts": [
                "\u2026Steenkiste, Locatello, Schmidhuber, and Bachem], [Yue et al.(2021)Yue, Wang, Sun, Hua, and Zhang], discovery of physical concepts [Iten et al.(2020)Iten, Metger, Wilming, Del Rio, and Renner] and enabling 3D shape reconstruction from 2D images [Pan et al.(2020)Pan, Dai, Liu, Loy, and Luo].",
                "(2020)Iten, Metger, Wilming, Del Rio, and Renner] and enabling 3D shape reconstruction from 2D images [Pan et al.(2020)Pan, Dai, Liu, Loy, and Luo].",
                "[Pan et al.(2020)Pan, Dai, Liu, Loy, and Luo] Xingang Pan, Bo Dai, Ziwei Liu, Chen Change Loy, and Ping Luo."
            ],
            "citingPaper": {
                "paperId": "99ff65391e6de5ced413ba04dc3919adbd92d75c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-09926",
                    "ArXiv": "2202.09926",
                    "CorpusId": 247011191
                },
                "corpusId": 247011191,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/99ff65391e6de5ced413ba04dc3919adbd92d75c",
                "title": "Disentangling Autoencoders (DAE)",
                "abstract": "Noting the importance of factorizing (or disentangling) the latent space, we propose a novel, non-probabilistic disentangling framework for autoencoders, based on the principles of symmetry transformations in group-theory. To the best of our knowledge, this is the first deterministic model that is aiming to achieve disentanglement based on autoencoders without regularizers. The proposed model is compared to seven state-of-the-art generative models based on autoencoders and evaluated based on five supervised disentanglement metrics. The experimental results show that the proposed model can have better disentanglement when variances of each features are different. We believe that this model leads to a new field for disentanglement learning based on autoencoders without regularizers.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2155622802",
                        "name": "Jaehoon Cha"
                    },
                    {
                        "authorId": "2155482801",
                        "name": "Jeyan Thiyagalingam"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "5c9cb718ae91ebdcf07e05a026db469dc46aaed4",
                "externalIds": {
                    "DBLP": "journals/csur/BansalSK22",
                    "DOI": "10.1145/3502287",
                    "CorpusId": 245772675
                },
                "corpusId": 245772675,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5c9cb718ae91ebdcf07e05a026db469dc46aaed4",
                "title": "A Systematic Review on Data Scarcity Problem in Deep Learning: Solution and Applications",
                "abstract": "Recent advancements in deep learning architecture have increased its utility in real-life applications. Deep learning models require a large amount of data to train the model. In many application domains, there is a limited set of data available for training neural networks as collecting new data is either not feasible or requires more resources such as in marketing, computer vision, and medical science. These models require a large amount of data to avoid the problem of overfitting. One of the data space solutions to the problem of limited data is data augmentation. The purpose of this study focuses on various data augmentation techniques that can be used to further improve the accuracy of a neural network. This saves the cost and time consumption required to collect new data for the training of deep neural networks by augmenting available data. This also regularizes the model and improves its capability of generalization. The need for large datasets in different fields such as computer vision, natural language processing, security, and healthcare is also covered in this survey paper. The goal of this paper is to provide a comprehensive survey of recent advancements in data augmentation techniques and their application in various domains.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2148970351",
                        "name": "Ms. Aayushi Bansal"
                    },
                    {
                        "authorId": "2149481737",
                        "name": "Dr. Rewa Sharma"
                    },
                    {
                        "authorId": "2148968778",
                        "name": "Dr. Mamta Kathuria"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "[65] Xingang Pan, Bo Dai, Ziwei Liu, Chen Change Loy, and Ping Luo.",
                "Prior work has explored the use of GANs [27, 68] in vision tasks such as classification [10, 12, 55, 75, 85], segmentation [57, 80, 83, 91] and representation learning [7, 20, 21, 23, 36], as well as 3D vision and graphics tasks [28, 65, 73, 90]."
            ],
            "citingPaper": {
                "paperId": "03871045478e9a5062c336b16230e4a79d488052",
                "externalIds": {
                    "DBLP": "conf/cvpr/PeeblesZ00ES22",
                    "ArXiv": "2112.05143",
                    "DOI": "10.1109/CVPR52688.2022.01311",
                    "CorpusId": 245005830
                },
                "corpusId": 245005830,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/03871045478e9a5062c336b16230e4a79d488052",
                "title": "GAN-Supervised Dense Visual Alignment",
                "abstract": "We propose GAN-Supervised Learning, a framework for learning discriminative models and their GAN-generated training data jointly end-to-end. We apply our framework to the dense visual alignment problem. Inspired by the classic Congealing method, our GAN gealing algorithm trains a Spatial Transformer to map random samples from a GAN trained on unaligned data to a common, jointly-learned target mode. We show results on eight datasets, all of which demonstrate our method successfully aligns complex data and discovers dense correspondences. GANgealing significantly outperforms past self-supervised correspondence algorithms and performs on-par with (and sometimes exceeds) state-of-the-art supervised correspondence algorithms on several datasets-without making use of any correspondence supervision or data augmentation and despite being trained exclusively on GAN-generated data. For precise correspondence, we improve upon state-of-the-art supervised methods by as much as 3 \u00d7. We show applications of our method for augmented reality, image editing and automated pre-processing of image datasets for downstream GAN training.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35235273",
                        "name": "William S. Peebles"
                    },
                    {
                        "authorId": "1922024303",
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "authorId": "2109976035",
                        "name": "Richard Zhang"
                    },
                    {
                        "authorId": "143805211",
                        "name": "A. Torralba"
                    },
                    {
                        "authorId": "1763086",
                        "name": "Alexei A. Efros"
                    },
                    {
                        "authorId": "2177801",
                        "name": "Eli Shechtman"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "2020) and recover explicit 3D shapes from images (Pan et al. 2020; Zhang et al. 2020).",
                "In another line of work, the 3D scene information is extracted from 2D GANs such as StyleGAN2 to manipulate 2D images in 3D (Shen and Zhou 2020; Ha\u0308rko\u0308nen et al. 2020) and recover explicit 3D shapes from images (Pan et al. 2020; Zhang et al. 2020)."
            ],
            "citingPaper": {
                "paperId": "e3e133722951392bb3c26b397e723f892ea98a6e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-01048",
                    "ArXiv": "2111.01048",
                    "DOI": "10.1609/aaai.v36i2.20091",
                    "CorpusId": 240353691
                },
                "corpusId": 240353691,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/e3e133722951392bb3c26b397e723f892ea98a6e",
                "title": "MOST-GAN: 3D Morphable StyleGAN for Disentangled Face Image Manipulation",
                "abstract": "Recent advances in generative adversarial networks (GANs) have led to remarkable achievements in face image synthesis. While methods that use style-based GANs can generate strikingly photorealistic face images, it is often difficult to control the characteristics of the generated faces in a meaningful and disentangled way. Prior approaches aim to achieve such semantic control and disentanglement within the latent space of a previously trained GAN. In contrast, we propose a framework that a priori models physical attributes of the face such as 3D shape, albedo, pose, and lighting explicitly, thus providing disentanglement by design. Our method, MOST-GAN, integrates the expressive power and photorealism of style-based GANs with the physical disentanglement and flexibility of nonlinear 3D morphable models, which we couple with a state-of-the-art 2D hair manipulation network. MOST-GAN achieves photorealistic manipulation of portrait images with fully disentangled 3D control over their physical attributes, enabling extreme manipulation of lighting, facial expression, and pose variations up to full profile view.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51291503",
                        "name": "Safa C. Medin"
                    },
                    {
                        "authorId": "2053695764",
                        "name": "B. Egger"
                    },
                    {
                        "authorId": "2691929",
                        "name": "A. Cherian"
                    },
                    {
                        "authorId": "2115737963",
                        "name": "Ye Wang"
                    },
                    {
                        "authorId": "1763295",
                        "name": "J. Tenenbaum"
                    },
                    {
                        "authorId": "2111119747",
                        "name": "Xiaoming Liu"
                    },
                    {
                        "authorId": "34749896",
                        "name": "Tim K. Marks"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "0755a30a4da86f60534e38507d786fc4fcae6540",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-00969",
                    "ArXiv": "2111.00969",
                    "CorpusId": 240354276
                },
                "corpusId": 240354276,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0755a30a4da86f60534e38507d786fc4fcae6540",
                "title": "Generative Occupancy Fields for 3D Surface-Aware Image Synthesis",
                "abstract": "The advent of generative radiance fields has significantly promoted the development of 3D-aware image synthesis. The cumulative rendering process in radiance fields makes training these generative models much easier since gradients are distributed over the entire volume, but leads to diffused object surfaces. In the meantime, compared to radiance fields occupancy representations could inherently ensure deterministic surfaces. However, if we directly apply occupancy representations to generative models, during training they will only receive sparse gradients located on object surfaces and eventually suffer from the convergence problem. In this paper, we propose Generative Occupancy Fields (GOF), a novel model based on generative radiance fields that can learn compact object surfaces without impeding its training convergence. The key insight of GOF is a dedicated transition from the cumulative rendering in radiance fields to rendering with only the surface points as the learned surface gets more and more accurate. In this way, GOF combines the merits of two representations in a unified framework. In practice, the training-time transition of start from radiance fields and march to occupancy representations is achieved in GOF by gradually shrinking the sampling region in its rendering process from the entire volume to a minimal neighboring region around the surface. Through comprehensive experiments on multiple datasets, we demonstrate that GOF can synthesize high-quality images with 3D consistency and simultaneously learn compact and smooth object surfaces. Code, models, and demo videos are available at https://sheldontsui.github.io/projects/GOF",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2112692444",
                        "name": "Xudong Xu"
                    },
                    {
                        "authorId": "14214933",
                        "name": "Xingang Pan"
                    },
                    {
                        "authorId": "1807606",
                        "name": "Dahua Lin"
                    },
                    {
                        "authorId": "144445937",
                        "name": "Bo Dai"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Recently, GAN2Shape [41] shows that it is possible to recover 3D shapes for images generated by 2D GANs.",
                "Besides, ShadeGAN also outperforms other advanced unsupervised 3D shape learning approaches including Unsup3d [39] and GAN2Shape [41], demonstrating its large potential in unsupervised 3D shapes learning.",
                "In experiments, we demonstrate superior performance over recent state-of-the-art approaches Unsup3d [39] and GAN2Shape [41]."
            ],
            "citingPaper": {
                "paperId": "7f03aa91b5bfdfc2b5c1a177262ca5da21dfca04",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-15678",
                    "ArXiv": "2110.15678",
                    "CorpusId": 240288458
                },
                "corpusId": 240288458,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7f03aa91b5bfdfc2b5c1a177262ca5da21dfca04",
                "title": "A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image Synthesis",
                "abstract": "The advancement of generative radiance fields has pushed the boundary of 3D-aware image synthesis. Motivated by the observation that a 3D object should look realistic from multiple viewpoints, these methods introduce a multi-view constraint as regularization to learn valid 3D radiance fields from 2D images. Despite the progress, they often fall short of capturing accurate 3D shapes due to the shape-color ambiguity, limiting their applicability in downstream tasks. In this work, we address this ambiguity by proposing a novel shading-guided generative implicit model that is able to learn a starkly improved shape representation. Our key insight is that an accurate 3D shape should also yield a realistic rendering under different lighting conditions. This multi-lighting constraint is realized by modeling illumination explicitly and performing shading with various lighting conditions. Gradients are derived by feeding the synthesized images to a discriminator. To compensate for the additional computational burden of calculating surface normals, we further devise an efficient volume rendering strategy via surface tracking, reducing the training and inference time by 24% and 48%, respectively. Our experiments on multiple datasets show that the proposed approach achieves photorealistic 3D-aware image synthesis while capturing accurate underlying 3D shapes. We demonstrate improved performance of our approach on 3D shape reconstruction against existing methods, and show its applicability on image relighting. Our code will be released at https://github.com/XingangPan/ShadeGAN.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "14214933",
                        "name": "Xingang Pan"
                    },
                    {
                        "authorId": "2112692444",
                        "name": "Xudong Xu"
                    },
                    {
                        "authorId": "1717179",
                        "name": "Chen Change Loy"
                    },
                    {
                        "authorId": "1680185",
                        "name": "C. Theobalt"
                    },
                    {
                        "authorId": "144445937",
                        "name": "Bo Dai"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Note that the symmetry is a double-edged sword, and it facilitates the fitting of symmetrical objects [44,58] such as human faces, cat faces, cars, etc."
            ],
            "citingPaper": {
                "paperId": "6971b691bf5514b309822b9deda8f89fd13944ad",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-09788",
                    "ArXiv": "2110.09788",
                    "CorpusId": 239024427
                },
                "corpusId": 239024427,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6971b691bf5514b309822b9deda8f89fd13944ad",
                "title": "CIPS-3D: A 3D-Aware Generator of GANs Based on Conditionally-Independent Pixel Synthesis",
                "abstract": "The style-based GAN (StyleGAN) architecture achieved state-of-the-art results for generating high-quality images, but it lacks explicit and precise control over camera poses. The recently proposed NeRF-based GANs made great progress towards 3D-aware generators, but they are unable to generate high-quality images yet. This paper presents CIPS-3D, a style-based, 3D-aware generator that is composed of a shallow NeRF network and a deep implicit neural representation (INR) network. The generator synthesizes each pixel value independently without any spatial convolution or upsampling operation. In addition, we diagnose the problem of mirror symmetry that implies a suboptimal solution and solve it by introducing an auxiliary discriminator. Trained on raw, single-view images, CIPS-3D sets new records for 3D-aware image synthesis with an impressive FID of 6.97 for images at the $256\\times256$ resolution on FFHQ. We also demonstrate several interesting directions for CIPS-3D such as transfer learning and 3D-aware face stylization. The synthesis results are best viewed as videos, so we recommend the readers to check our github project at https://github.com/PeterouZh/CIPS-3D",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2113325955",
                        "name": "Peng Zhou"
                    },
                    {
                        "authorId": "3041937",
                        "name": "Lingxi Xie"
                    },
                    {
                        "authorId": "5796401",
                        "name": "Bingbing Ni"
                    },
                    {
                        "authorId": "2056267867",
                        "name": "Qi Tian"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [
                "scratch [11], [12], [13], [14], [15], [16], [17]."
            ],
            "citingPaper": {
                "paperId": "5cebb7310d74f8d7cc37d992b35121a6e410650c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-12492",
                    "ArXiv": "2109.12492",
                    "DOI": "10.1109/TMM.2022.3159115",
                    "CorpusId": 237940383
                },
                "corpusId": 237940383,
                "publicationVenue": {
                    "id": "10e76a35-58d6-443c-9683-fc16f2dd0a92",
                    "name": "IEEE transactions on multimedia",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Transactions on Multimedia",
                        "IEEE Trans Multimedia",
                        "IEEE trans multimedia"
                    ],
                    "issn": "1520-9210",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=6046"
                },
                "url": "https://www.semanticscholar.org/paper/5cebb7310d74f8d7cc37d992b35121a6e410650c",
                "title": "ISF-GAN: An Implicit Style Function for High-Resolution Image-to-Image Translation",
                "abstract": "Recently, there has been an increasing interest in image editing methods that employ pre-trained unconditional image generators (e.g., StyleGAN). However, applying these methods to translate images to multiple visual domains remains challenging. Existing works do not often preserve the domain-invariant part of the image (e.g., the identity in human face translations), or they do not usually handle multiple domains or allow for multi-modal translations. This work proposes an implicit style function (ISF) to straightforwardly achieve multi-modal and multi-domain image-to-image translation from pre-trained unconditional generators. The ISF manipulates the semantics of a latent code to ensure that the image generated from the manipulated code lies in the desired visual domain. Our human faces and animal image manipulations show significantly improved results over the baselines. Our model enables cost-effective multi-modal unsupervised image-to-image translations at high resolution using pre-trained unconditional GANs. The code and data are available at: https://github.com/yhlleo/stylegan-mmuit.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1646872838",
                        "name": "Yahui Liu"
                    },
                    {
                        "authorId": "2109270851",
                        "name": "Yajing Chen"
                    },
                    {
                        "authorId": "2780029",
                        "name": "Linchao Bao"
                    },
                    {
                        "authorId": "1703601",
                        "name": "N. Sebe"
                    },
                    {
                        "authorId": "49305855",
                        "name": "B. Lepri"
                    },
                    {
                        "authorId": "7405787",
                        "name": "Marco De Nadai"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "8bf5ede037959c3bec9857288753c945e6f55143",
                "externalIds": {
                    "ArXiv": "2109.09378",
                    "DBLP": "journals/corr/abs-2109-09378",
                    "DOI": "10.1145/3478513.3480538",
                    "CorpusId": 237510393
                },
                "corpusId": 237510393,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8bf5ede037959c3bec9857288753c945e6f55143",
                "title": "FreeStyleGAN",
                "abstract": "Current Generative Adversarial Networks (GANs) produce photorealistic renderings of portrait images. Embedding real images into the latent space of such models enables high-level image editing. While recent methods provide considerable semantic control over the (re-)generated images, they can only generate a limited set of viewpoints and cannot explicitly control the camera. Such 3D camera control is required for 3D virtual and mixed reality applications. In our solution, we use a few images of a face to perform 3D reconstruction, and we introduce the notion of the GAN camera manifold, the key element allowing us to precisely define the range of images that the GAN can reproduce in a stable manner. We train a small face-specific neural implicit representation network to map a captured face to this manifold and complement it with a warping scheme to obtain free-viewpoint novel-view synthesis. We show how our approach - due to its precise camera control - enables the integration of a pre-trained StyleGAN into standard 3D rendering pipelines, allowing e.g., stereo rendering or consistent insertion of faces in synthetic 3D environments. Our solution proposes the first truly free-viewpoint rendering of realistic faces at interactive rates, using only a small number of casual photos as input, while simultaneously allowing semantic editing capabilities, such as facial expression or lighting changes.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2422386",
                        "name": "Thomas Leimk\u00fchler"
                    },
                    {
                        "authorId": "1721779",
                        "name": "G. Drettakis"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Semantic Field for Facial Editing Given an input image I \u2208 R3\u00d7H\u00d7W and a pretrained GAN generator G, similar to previous latent space based manipulation methods [40, 41, 59, 35], we need to firstly inverse the corresponding latent code z \u2208 R such that I = G(z), and then find the certain vector fz \u2208 R which can change the attribute degree."
            ],
            "citingPaper": {
                "paperId": "842ad35b67d410b40432dbb6d5349103f3f33e53",
                "externalIds": {
                    "DBLP": "conf/iccv/0003HPL021",
                    "ArXiv": "2109.04425",
                    "DOI": "10.1109/ICCV48922.2021.01354",
                    "CorpusId": 237453495
                },
                "corpusId": 237453495,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/842ad35b67d410b40432dbb6d5349103f3f33e53",
                "title": "Talk-to-Edit: Fine-Grained Facial Editing via Dialog",
                "abstract": "Facial editing is an important task in vision and graphics with numerous applications. However, existing works are incapable to deliver a continuous and fine-grained editing mode (e.g., editing a slightly smiling face to a big laughing one) with natural interactions with users. In this work, we propose Talk-to-Edit, an interactive facial editing framework that performs fine-grained attribute manipulation through dialog between the user and the system. Our key insight is to model a continual \"semantic field\" in the GAN latent space. 1) Unlike previous works that regard the editing as traversing straight lines in the latent space, here the fine-grained editing is formulated as finding a curving trajectory that respects fine-grained attribute landscape on the semantic field. 2) The curvature at each step is location-specific and determined by the input image as well as the users\u2019 language requests. 3) To engage the users in a meaningful dialog, our system generates language feedback by considering both the user request and the current state of the semantic field.We also contribute CelebA-Dialog, a visual-language facial editing dataset to facilitate large-scale study. Specifically, each image has manually annotated fine-grained attribute annotations as well as template-based textual descriptions in natural language. Extensive quantitative and qualitative experiments demonstrate the superiority of our framework in terms of 1) the smoothness of fine-grained editing, 2) the identity/attribute preservation, and 3) the visual photorealism and dialog fluency. Notably, user study validates that our overall system is consistently favored by around 80% of the participants. Our project page is https://www.mmlab-ntu.com/project/talkedit/.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2127773416",
                        "name": "Yuming Jiang"
                    },
                    {
                        "authorId": "3150911",
                        "name": "Ziqi Huang"
                    },
                    {
                        "authorId": "14214933",
                        "name": "Xingang Pan"
                    },
                    {
                        "authorId": "1717179",
                        "name": "Chen Change Loy"
                    },
                    {
                        "authorId": "2117940996",
                        "name": "Ziwei Liu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "GAN-based approaches [18, 7] for 3D reconstruction demonstrate high quality outputs and have recently been extended to allow control over the output."
            ],
            "citingPaper": {
                "paperId": "b3831283a2f12d81105820fa7d29e610cd27621d",
                "externalIds": {
                    "ArXiv": "2108.08477",
                    "DBLP": "journals/corr/abs-2108-08477",
                    "CorpusId": 237213507
                },
                "corpusId": 237213507,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b3831283a2f12d81105820fa7d29e610cd27621d",
                "title": "Image2Lego: Customized LEGO Set Generation from Images",
                "abstract": "Although LEGO sets have entertained generations of children and adults, the challenge of designing customized builds matching the complexity of real-world or imagined scenes remains too great for the average enthusiast. In order to make this feat possible, we implement a system that generates a LEGO brick model from 2D images. We design a novel solution to this problem that uses an octree-structured autoencoder trained on 3D voxelized models to obtain a feasible latent representation for model reconstruction, and a separate network trained to predict this latent representation from 2D images. LEGO models are obtained by algorithmic conversion of the 3D voxelized model to bricks. We demonstrate first-of-its-kind conversion of photographs to 3D LEGO models. An octree architecture enables the flexibility to produce multiple resolutions to best fit a user's creative vision or design needs. In order to demonstrate the broad applicability of our system, we generate step-by-step building instructions and animations for LEGO models of objects and human faces. Finally, we test these automatically generated LEGO sets by constructing physical builds using real LEGO bricks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "6764629",
                        "name": "Kyle Lennon"
                    },
                    {
                        "authorId": "120372933",
                        "name": "Katharina A Fransen"
                    },
                    {
                        "authorId": "2124000694",
                        "name": "Alexander O'Brien"
                    },
                    {
                        "authorId": "2146177662",
                        "name": "Yumeng Cao"
                    },
                    {
                        "authorId": "119641639",
                        "name": "Matthew Beveridge"
                    },
                    {
                        "authorId": "40954743",
                        "name": "Yamin Arefeen"
                    },
                    {
                        "authorId": "2110097630",
                        "name": "Nikhil Singh"
                    },
                    {
                        "authorId": "2861627",
                        "name": "Iddo Drori"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Adversarial learning has also been explored to replace the need of multi-views for training (Kudo et al., 2018; Chen et al., 2019; Henzler et al., 2019; Nguyen-Phuoc et al., 2019, 2020; Ye et al., 2021; Schwarz et al., 2020; Niemeyer & Geiger, 2021; Zhang et al., 2021; Chan et al., 2021; Pan et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "fb877c091d87a75aeb28a01002c34761b473ab64",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-10844",
                    "ArXiv": "2107.10844",
                    "DOI": "10.1007/s11263-023-01819-5",
                    "CorpusId": 236170851
                },
                "corpusId": 236170851,
                "publicationVenue": {
                    "id": "939ee07c-6009-43f8-b884-69238b40659e",
                    "name": "International Journal of Computer Vision",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Comput Vis"
                    ],
                    "issn": "0920-5691",
                    "url": "https://www.springer.com/computer/image+processing/journal/11263",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11263",
                        "http://link.springer.com/journal/11263"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fb877c091d87a75aeb28a01002c34761b473ab64",
                "title": "DOVE: Learning Deformable 3D Objects by Watching Videos",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2112538311",
                        "name": "Shangzhe Wu"
                    },
                    {
                        "authorId": "144834818",
                        "name": "Tomas Jakab"
                    },
                    {
                        "authorId": "49359942",
                        "name": "C. Rupprecht"
                    },
                    {
                        "authorId": "1687524",
                        "name": "A. Vedaldi"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "To alleviate the problem, random augmentation strategies were used to generate novel attributes in [27, 19, 30].",
                "Reconstruction on BFM: In this experiment, we combine our proposed IC and LC with the unsupervised reconstruction methods [41, 30] on the BFM Face reconstruction dataset.",
                "Compared with the original [10, 18] or randomly augmented attribute in [30], our interpolated attributes can render images with more viewpoints, geometrical structures, and appearances, thus is more efficient to promote the learning process of the target attribute encoder.",
                "The main difference between [30] and our IC is that the former randomly augments attributes."
            ],
            "citingPaper": {
                "paperId": "f472dd701efb952f650d2c25383d7281d3d8566f",
                "externalIds": {
                    "DBLP": "conf/cvpr/HuWX0J21",
                    "DOI": "10.1109/CVPR46437.2021.00594",
                    "CorpusId": 235497418
                },
                "corpusId": 235497418,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f472dd701efb952f650d2c25383d7281d3d8566f",
                "title": "Self-Supervised 3D Mesh Reconstruction from Single Images",
                "abstract": "Recent single-view 3D reconstruction methods reconstruct object\u2019s shape and texture from a single image with only 2D image-level annotation. However, without explicit 3D attribute-level supervision, it is still difficult to achieve satisfying reconstruction accuracy. In this paper, we propose a Self-supervised Mesh Reconstruction (SMR) approach to enhance 3D mesh attribute learning process. Our approach is motivated by observations that (1) 3D attributes from interpolation and prediction should be consistent, and (2) feature representation of landmarks from all images should be consistent. By only requiring silhouette mask annotation, our SMR can be trained in an end-to- end manner and generalizes to reconstruct natural objects of birds, cows, motorbikes, etc. Experiments demonstrate that our approach improves both 2D supervised and unsupervised 3D mesh reconstruction on multiple datasets. We also show that our model can be adapted to other image synthesis tasks, e.g., novel view generation, shape transfer, and texture transfer, with promising results. Our code is publicly available at https://github.com/Jia-Research-Lab.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144436761",
                        "name": "T. Hu"
                    },
                    {
                        "authorId": "2130339621",
                        "name": "Liwei Wang"
                    },
                    {
                        "authorId": "2027354611",
                        "name": "Xiaogang Xu"
                    },
                    {
                        "authorId": "25059098",
                        "name": "Shu Liu"
                    },
                    {
                        "authorId": "1729056",
                        "name": "Jiaya Jia"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Other works have taken this approach in the context of extracting 3D structure from 2D images [33]; Inverse Graphics GAN [29] uses a neural renderer to recover 3D (voxelbased) representations of scenes, and GAN2Shape [38] ex-",
                "Other works have taken this approach in the context of extracting 3D structure from 2D images [33]; Inverse Graphics GAN [29] uses a neural renderer to recover 3D (voxelbased) representations of scenes, and GAN2Shape [38] ex-\nploits viewpoint and lighting variations in generated images to recover 3D shapes."
            ],
            "citingPaper": {
                "paperId": "6470d56e2b96542d191067a258261f92aa8ed82c",
                "externalIds": {
                    "DBLP": "conf/iclr/Melas-Kyriazi0L22",
                    "ArXiv": "2105.08127",
                    "CorpusId": 234762955
                },
                "corpusId": 234762955,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/6470d56e2b96542d191067a258261f92aa8ed82c",
                "title": "Finding an Unsupervised Image Segmenter in Each of Your Deep Generative Models",
                "abstract": "Recent research has shown that numerous human-interpretable directions exist in the latent space of GANs. In this paper, we develop an automatic procedure for finding directions that lead to foreground-background image separation, and we use these directions to train an image segmentation model without human supervision. Our method is generator-agnostic, producing strong segmentation results with a wide range of different GAN architectures. Furthermore, by leveraging GANs pretrained on large datasets such as ImageNet, we are able to segment images from a range of domains without further training or finetuning. Evaluating our method on image segmentation benchmarks, we compare favorably to prior work while using neither human supervision nor access to the training data. Broadly, our results demonstrate that automatically extracting foreground-background structure from pretrained deep generative models can serve as a remarkably effective substitute for human supervision.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1411260673",
                        "name": "Luke Melas-Kyriazi"
                    },
                    {
                        "authorId": "49359942",
                        "name": "C. Rupprecht"
                    },
                    {
                        "authorId": "3422200",
                        "name": "Iro Laina"
                    },
                    {
                        "authorId": "1687524",
                        "name": "A. Vedaldi"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Apart from linear edit directions, recent works [67, 36] learn latent editing operations that control the 3D appearance of generated images."
            ],
            "citingPaper": {
                "paperId": "8f9d1de9e1bd60783eb75fd80c42bf99ec67363a",
                "externalIds": {
                    "DBLP": "conf/cvpr/ChaiZSI021",
                    "ArXiv": "2104.14551",
                    "DOI": "10.1109/CVPR46437.2021.01475",
                    "CorpusId": 233444321
                },
                "corpusId": 233444321,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8f9d1de9e1bd60783eb75fd80c42bf99ec67363a",
                "title": "Ensembling with Deep Generative Views",
                "abstract": "Recent generative models can synthesize \"views\" of artificial images that mimic real-world variations, such as changes in color or pose, simply by learning from unlabeled image collections. Here, we investigate whether such views can be applied to real images to benefit downstream analysis tasks such as image classification. Using a pre-trained generator, we first find the latent code corresponding to a given real input image. Applying perturbations to the code creates natural variations of the image, which can then be ensembled together at test-time. We use StyleGAN2 as the source of generative augmentations and investigate this setup on classification tasks involving facial attributes, cat faces, and cars. Critically, we find that several design decisions are required towards making this process work; the perturbation procedure, weighting between the augmentations and original image, and training the classifier on synthesized images can all impact the result. Currently, we find that while test-time ensembling with GAN-based augmentations can offer some small improvements, the remaining bottlenecks are the efficiency and accuracy of the GAN reconstructions, coupled with classifier sensitivities to artifacts in GAN-generated images.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51322829",
                        "name": "Lucy Chai"
                    },
                    {
                        "authorId": "1922024303",
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "authorId": "2177801",
                        "name": "Eli Shechtman"
                    },
                    {
                        "authorId": "2094770",
                        "name": "Phillip Isola"
                    },
                    {
                        "authorId": "2109976035",
                        "name": "Richard Zhang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "To generate 3D shapes directly from text or sound, we can easily integrate our method with a concurrent shape reconstruction method [74] for the reason that we share the same latent space of a pretrained GAN model."
            ],
            "citingPaper": {
                "paperId": "b5bfa6807237a112bea88b558e8a5038c17514ce",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-08910",
                    "ArXiv": "2104.08910",
                    "CorpusId": 233296879
                },
                "corpusId": 233296879,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b5bfa6807237a112bea88b558e8a5038c17514ce",
                "title": "Towards Open-World Text-Guided Face Image Generation and Manipulation",
                "abstract": "The existing text-guided image synthesis methods can only produce limited quality results with at most \\mbox{$\\text{256}^2$} resolution and the textual instructions are constrained in a small Corpus. In this work, we propose a unified framework for both face image generation and manipulation that produces diverse and high-quality images with an unprecedented resolution at 1024 from multimodal inputs. More importantly, our method supports open-world scenarios, including both image and text, without any re-training, fine-tuning, or post-processing. To be specific, we propose a brand new paradigm of text-guided image generation and manipulation based on the superior characteristics of a pretrained GAN model. Our proposed paradigm includes two novel strategies. The first strategy is to train a text encoder to obtain latent codes that align with the hierarchically semantic of the aforementioned pretrained GAN model. The second strategy is to directly optimize the latent codes in the latent space of the pretrained GAN model with guidance from a pretrained language model. The latent codes can be randomly sampled from a prior distribution or inverted from a given image, which provides inherent supports for both image generation and manipulation from multi-modal inputs, such as sketches or semantic labels, with textual guidance. To facilitate text-guided multi-modal synthesis, we propose the Multi-Modal CelebA-HQ, a large-scale dataset consisting of real face images and corresponding semantic segmentation map, sketch, and textual descriptions. Extensive experiments on the introduced dataset demonstrate the superior performance of our proposed method. Code and data are available at https://github.com/weihaox/TediGAN.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50875615",
                        "name": "Weihao Xia"
                    },
                    {
                        "authorId": "2108585311",
                        "name": "Yujiu Yang"
                    },
                    {
                        "authorId": "1891766",
                        "name": "Jing-Hao Xue"
                    },
                    {
                        "authorId": "143905981",
                        "name": "Baoyuan Wu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "[48] Xingang Pan, Bo Dai, Ziwei Liu, Chen Change Loy, and Ping Luo."
            ],
            "citingPaper": {
                "paperId": "18a9bfa6f6d733c9faf095b6e6f7867f3dba4100",
                "externalIds": {
                    "ArXiv": "2104.07659",
                    "DBLP": "conf/iccv/HaoMB021",
                    "DOI": "10.1109/ICCV48922.2021.01381",
                    "CorpusId": 233240846
                },
                "corpusId": 233240846,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/18a9bfa6f6d733c9faf095b6e6f7867f3dba4100",
                "title": "GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds",
                "abstract": "We present GANcraft, an unsupervised neural rendering framework for generating photorealistic images of large 3D block worlds such as those created in Minecraft. Our method takes a semantic block world as input, where each block is assigned a semantic label such as dirt, grass, or water. We represent the world as a continuous volumetric function and train our model to render view-consistent photorealistic images for a user-controlled camera. In the absence of paired ground truth real images for the block world, we devise a training technique based on pseudo-ground truth and adversarial training. This stands in contrast to prior work on neural rendering for view synthesis, which requires ground truth images to estimate scene geometry and view-dependent appearance. In addition to camera trajectory, GANcraft allows user control over both scene semantics and output style. Experimental results with comparison to strong baselines show the effectiveness of GANcraft on this novel task of photorealistic 3D block world synthesis. The project website is available at https://nvlabs.github.io/GANcraft/.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "19235216",
                        "name": "Zekun Hao"
                    },
                    {
                        "authorId": "36508529",
                        "name": "Arun Mallya"
                    },
                    {
                        "authorId": "2067789287",
                        "name": "S. Belongie"
                    },
                    {
                        "authorId": "39793900",
                        "name": "Ming-Yu Liu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Examples include various image and video editing tasks, image inpainting [13, 47, 46, 41], local image editing [42, 2], low bit-rate video conferencing [40], image super resolution [28, 12], image colorization [4, 27], and extracting 3D models [30]."
            ],
            "citingPaper": {
                "paperId": "b68788a9ade569ba3b5a036e29ccbfc6fc7aba2d",
                "externalIds": {
                    "ArXiv": "2103.14968",
                    "DBLP": "conf/iccv/AbdalZMW21",
                    "DOI": "10.1109/ICCV48922.2021.01371",
                    "CorpusId": 232404685
                },
                "corpusId": 232404685,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/b68788a9ade569ba3b5a036e29ccbfc6fc7aba2d",
                "title": "Labels4Free: Unsupervised Segmentation using StyleGAN",
                "abstract": "We propose an unsupervised segmentation framework for StyleGAN generated objects. We build on two main observations. First, the features generated by StyleGAN hold valuable information that can be utilized towards training segmentation networks. Second, the foreground and background can often be treated to be largely independent and be swapped across images to produce plausible composited images. For our solution, we propose to augment the StyleGAN2 generator architecture with a segmentation branch and to split the generator into a foreground and background network. This enables us to generate soft segmentation masks for the foreground object in an unsupervised fashion. On multiple object classes, we report comparable results against state-of-the-art supervised segmentation networks, while against the best unsupervised segmentation approach we demonstrate a clear improvement, both in qualitative and quantitative metricsProject Page : https:/rameenabdal.github.io/Labels4Free",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "94395014",
                        "name": "Rameen Abdal"
                    },
                    {
                        "authorId": "100516590",
                        "name": "Peihao Zhu"
                    },
                    {
                        "authorId": "1710455",
                        "name": "N. Mitra"
                    },
                    {
                        "authorId": "1798011",
                        "name": "Peter Wonka"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "common image editing applications, in the last few months, GAN inversion techniques have been widely introduced to many other tasks, such as 3D reconstruction [132], [133],",
                "[132] first render a number of unnatural images with various randomly sampled viewpoints and"
            ],
            "citingPaper": {
                "paperId": "a3ef5a321876738a6b257de5e1eebc4a8aa5b907",
                "externalIds": {
                    "DBLP": "journals/pami/XiaZYXZY23",
                    "ArXiv": "2101.05278",
                    "DOI": "10.1109/TPAMI.2022.3181070",
                    "CorpusId": 231603119,
                    "PubMed": "37022469"
                },
                "corpusId": 231603119,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a3ef5a321876738a6b257de5e1eebc4a8aa5b907",
                "title": "GAN Inversion: A Survey",
                "abstract": "GAN inversion aims to invert a given image back into the latent space of a pretrained GAN model so that the image can be faithfully reconstructed from the inverted code by the generator. As an emerging technique to bridge the real and fake image domains, GAN inversion plays an essential role in enabling pretrained GAN models, such as StyleGAN and BigGAN, for applications of real image editing. Moreover, GAN inversion interprets GAN's latent space and examines how realistic images can be generated. In this paper, we provide a survey of GAN inversion with a focus on its representative algorithms and its applications in image restoration and image manipulation. We further discuss the trends and challenges for future research. A curated list of GAN inversion methods, datasets, and other related information can be found at https://github.com/weihaox/awesome-gan-inversion.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50875615",
                        "name": "Weihao Xia"
                    },
                    {
                        "authorId": "2129519081",
                        "name": "Yulun Zhang"
                    },
                    {
                        "authorId": "3001727",
                        "name": "Yujiu Yang"
                    },
                    {
                        "authorId": "1891766",
                        "name": "Jing-Hao Xue"
                    },
                    {
                        "authorId": "145291669",
                        "name": "Bolei Zhou"
                    },
                    {
                        "authorId": "1715634",
                        "name": "Ming-Hsuan Yang"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "[18] propose to iteratively synthesize data and train the reconstruction network.",
                "[18], which use StyleGAN2 to create synthetic training data.",
                "[18], we implement Equation (3) as two parts.",
                "[18] have utilized StyleGAN to generate multiview synthetic data for 3D reconstruction tasks.",
                "However, different from them, we do not need any manual annotation [31] nor iterative training [18]."
            ],
            "citingPaper": {
                "paperId": "4a37f3705d9adba0debea2636d2229282dec94cc",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2011-13126",
                    "MAG": "3108745726",
                    "ArXiv": "2011.13126",
                    "DOI": "10.1109/CVPR46437.2021.00619",
                    "CorpusId": 227209060
                },
                "corpusId": 227209060,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4a37f3705d9adba0debea2636d2229282dec94cc",
                "title": "Lifting 2D StyleGAN for 3D-Aware Face Generation",
                "abstract": "We propose a framework, called LiftedGAN, that disentangles and lifts a pre-trained StyleGAN2 for 3D-aware face generation. Our model is \"3D-aware\" in the sense that it is able to (1) disentangle the latent space of StyleGAN2 into texture, shape, viewpoint, lighting and (2) generate 3D components for rendering synthetic images. Unlike most previous methods, our method is completely self-supervised, i.e. it neither requires any manual annotation nor 3DMM model for training. Instead, it learns to generate images as well as their 3D components by distilling the prior knowledge in StyleGAN2 with a differentiable renderer. The proposed model is able to output both the 3D shape and texture, allowing explicit pose and lighting control over generated images. Qualitative and quantitative results show the superiority of our approach over existing methods on 3D-controllable GANs in content controllability while generating realistic high quality images.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "9644181",
                        "name": "Yichun Shi"
                    },
                    {
                        "authorId": "2140707056",
                        "name": "Divyansh Aggarwal"
                    },
                    {
                        "authorId": "145295484",
                        "name": "Anil K. Jain"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "manipulation [15], [39], [40], [41], [42], [43], but is restricted to synthetic images of the GAN itself or real images of limited complexity, e."
            ],
            "citingPaper": {
                "paperId": "7101bc1c316740d99cd87185586829291a983a1d",
                "externalIds": {
                    "ArXiv": "2003.13659",
                    "DBLP": "conf/eccv/PanZDLLL20",
                    "MAG": "3107096356",
                    "DOI": "10.1109/TPAMI.2021.3115428",
                    "CorpusId": 214713474,
                    "PubMed": "34559638"
                },
                "corpusId": 214713474,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7101bc1c316740d99cd87185586829291a983a1d",
                "title": "Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation",
                "abstract": "Learning a good image prior is a long-term goal for image restoration and manipulation. While existing methods like deep image prior (DIP) capture low-level image statistics, there are still gaps toward an image prior that captures rich image semantics including color, spatial coherence, textures, and high-level concepts. This work presents an effective way to exploit the image prior captured by a generative adversarial network (GAN) trained on large-scale natural images. As shown in Fig. 1, the deep generative prior (DGP) provides compelling results to restore missing semantics, e.g., color, patch, resolution, of various degraded images. It also enables diverse image manipulation including random jittering, image morphing, and category transfer. Such highly flexible restoration and manipulation are made possible through relaxing the assumption of existing GAN inversion methods, which tend to fix the generator. Notably, we allow the generator to be fine-tuned on-the-fly in a progressive manner regularized by feature distance obtained by the discriminator in GAN. We show that these easy-to-implement and practical changes help preserve the reconstruction to remain in the manifold of nature images, and thus lead to more precise and faithful reconstruction for real images. Code is available at https://github.com/XingangPan/deep-generative-prior.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "14214933",
                        "name": "Xingang Pan"
                    },
                    {
                        "authorId": "31818765",
                        "name": "Xiaohang Zhan"
                    },
                    {
                        "authorId": "144445937",
                        "name": "Bo Dai"
                    },
                    {
                        "authorId": "1807606",
                        "name": "Dahua Lin"
                    },
                    {
                        "authorId": "1717179",
                        "name": "Chen Change Loy"
                    },
                    {
                        "authorId": "144389940",
                        "name": "P. Luo"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                ", and can be applied to face rendering [18], [19], [20].",
                "We recompose the photo-geometric autoencoding model in [20] by combining GANs and renderer, and adjust the output of the GANs to view, light, albedo, and normal (as shown in Fig."
            ],
            "citingPaper": {
                "paperId": "9be96438421a40eead7f4fe3b29b10a10319b7f1",
                "externalIds": {
                    "DBLP": "journals/spl/LiuFGZL23",
                    "DOI": "10.1109/LSP.2023.3238908",
                    "CorpusId": 256218778
                },
                "corpusId": 256218778,
                "publicationVenue": {
                    "id": "d5da7004-7b61-450a-9c7d-a39500de7acf",
                    "name": "IEEE Signal Processing Letters",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Signal Process Lett"
                    ],
                    "issn": "1070-9908",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=97"
                },
                "url": "https://www.semanticscholar.org/paper/9be96438421a40eead7f4fe3b29b10a10319b7f1",
                "title": "Fine-Scale Face Fitting and Texture Fusion With Inverse Renderer",
                "abstract": "3D face reconstruction from a single image still suffers from low accuracy and inability to recover textures in invisible regions. In this paper, we propose a method for generating a 3D portrait with complete texture. The coarse face-and-head model and texture parameters are obtained using 3D Morphable Model fitting. We design an image-geometric inverse renderer that acquires normal, albedo, and light to jointly reconstruct the facial details. Then, we use a texture fusion network to extract the valid texture from rendered faces employing different viewpoints. Specifically, this fused texture recovers the invisible region of the input face, which illustrates the realistic surface of our 3D geometric model. Our approach faithfully reconstructs the original face details, including the profiles and the head region. Extensive experiments are performed to demonstrate that our method outperforms state-of-the-art techniques in various challenging scenarios.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152802984",
                        "name": "Yang Liu"
                    },
                    {
                        "authorId": "3041045",
                        "name": "Yangyu Fan"
                    },
                    {
                        "authorId": "49694865",
                        "name": "Zhe Guo"
                    },
                    {
                        "authorId": "2136214301",
                        "name": "Anam Zaman"
                    },
                    {
                        "authorId": "1819906908",
                        "name": "Shiya Liu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "a8d6467e4126d0ada4e824b30a6098056b843242",
                "externalIds": {
                    "CorpusId": 259114449
                },
                "corpusId": 259114449,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a8d6467e4126d0ada4e824b30a6098056b843242",
                "title": "Generative Models of Images and Neural Networks",
                "abstract": "Generative Models of Images and Neural Networks",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2219729481",
                        "name": "Bill Peebles"
                    },
                    {
                        "authorId": "35235273",
                        "name": "William S. Peebles"
                    },
                    {
                        "authorId": "1763086",
                        "name": "Alexei A. Efros"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "As a result, later methods moved away from fully convolutional GANs by incorporating 3D inductive biases in the architecture and training pipeline, such as 3D neural representations and differentiable rendering methods [34, 35, 47, 38].",
                "As a result, later works aimed at unsupervised methods by introducing 3D inductive biases in GANs, including 3D neural representations and differentiable rendering [34, 38, 47, 35] These methods, although promising, lag far behind 2D GANs in terms of image quality or struggle with high-resolution generation due to the additional computational complexity compared to the convolutional generators."
            ],
            "citingPaper": {
                "paperId": "ad4ca90be642c80e48865adb1fba84718d250217",
                "externalIds": {
                    "CorpusId": 259138647
                },
                "corpusId": 259138647,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ad4ca90be642c80e48865adb1fba84718d250217",
                "title": "NeRF-GAN Distillation for Memory-Efficient 3D-Aware Generation with Convolutions",
                "abstract": "Pose-conditioned convolutional generative models struggle with high-quality 3D-consistent image generation from single-view datasets, due to their lack of sufficient 3D priors. Recently, the integration of Neural Radiance Fields (NeRFs) and generative models, such as Generative Adversarial Networks (GANs), has transformed 3D-aware generation from single-view images. NeRF-GANs exploit the strong inductive bias of 3D neural representations and volumetric rendering at the cost of higher computational complexity. This study aims at revisiting pose-conditioned 2D GANs for memory-efficient 3D-aware generation at inference time by distilling 3D knowledge from pretrained NeRF-GANS. We propose a simple and effective method, based on re-using the well-disentangled latent space of a pre-trained NeRF-GAN in a pose-conditioned convolutional network to directly generate 3D-consistent images corresponding to the underlying 3D representations. Experiments on several datasets demonstrate that the proposed method obtains results comparable with volumetric rendering in terms of quality and 3D consistency while benefiting from the superior computational advantage of convolutional networks. The code will be available at:",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "73774192",
                        "name": "Mohamad Shahbazi"
                    },
                    {
                        "authorId": "2219926024",
                        "name": "Evangelos Ntavelis"
                    },
                    {
                        "authorId": "20406113",
                        "name": "A. Tonioni"
                    },
                    {
                        "authorId": "33942393",
                        "name": "Edo Collins"
                    },
                    {
                        "authorId": "35268081",
                        "name": "D. Paudel"
                    },
                    {
                        "authorId": "2129520569",
                        "name": "Martin Danelljan"
                    },
                    {
                        "authorId": "1681236",
                        "name": "L. Gool"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "[14] proposed a GAN2Shape model to recover 3-D shapes from a single RGB image in an unsupervised manner which mines only some 3-D geometric cues from 2-D images generated by generative adversarial network (GAN), and depth information of the RGB image cannot be provided."
            ],
            "citingPaper": {
                "paperId": "52ca53d9233d6a85d8ed7d6277c0dba7e816852d",
                "externalIds": {
                    "DBLP": "journals/lgrs/ZhaoCWWPC22",
                    "DOI": "10.1109/LGRS.2022.3198850",
                    "CorpusId": 251609355
                },
                "corpusId": 251609355,
                "publicationVenue": {
                    "id": "290335d6-cddc-465d-87f1-807e86d8efee",
                    "name": "IEEE Geoscience and Remote Sensing Letters",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Geosci Remote Sens Lett"
                    ],
                    "issn": "1545-598X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=8859",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8859"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/52ca53d9233d6a85d8ed7d6277c0dba7e816852d",
                "title": "Phenotypic Parameters Estimation of Plants Using Deep Learning-Based 3-D Reconstruction From Single RGB Image",
                "abstract": "Monitoring crop growth is of great significance to obtain crop growth status information for development of smart agriculture. The traditional way to measure the phenotypic parameters of crops is labor-intensive and encounters inconvenient operations. In this study, we propose to obtain the phenotypic parameters of crops from 3-D reconstruction of plants from single RGB images using a data-driven plant phenotypic parameters estimation network (P3ES-Net) deep neural network, which enables to estimate the depth shift and camera focal length used for depth estimation and reconstruction of the 3-D model of plants. Based on the principles of the monocular ranging and pinhole imaging model, crop phenotypic parameters such as height, canopy size, and trunk diameter can then be calculated from the 3-D model. Experiments with four practical plants present that our method is able to achieve acceptable evaluation of the growth status of plants. Of more significance, it achieves particular superior depth estimation performance over a commercial depth camera, which is a very new on-sale depth camera using stereo vision and deep learning network. This potential performance throws light on the low-cost measurement of crop phenotypic parameters using RGB camera in monitoring crop growth.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2668111",
                        "name": "Genping Zhao"
                    },
                    {
                        "authorId": "2181720184",
                        "name": "Weitao Cai"
                    },
                    {
                        "authorId": "2108369912",
                        "name": "Zhuowei Wang"
                    },
                    {
                        "authorId": "2163050306",
                        "name": "Heng Wu"
                    },
                    {
                        "authorId": "123831638",
                        "name": "Yeping Peng"
                    },
                    {
                        "authorId": "39681786",
                        "name": "Lianglun Cheng"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "While aforementioned 2D GANs [15], [17], [22], [23] allow explicit head pose control to some extent, they fail to guarantee appearance consistency, leading to inconsistent identity or facial attributes when viewed from vastly different angles."
            ],
            "citingPaper": {
                "paperId": "25c6c02f2db2e90e5660a92fcd134cd33addc8ce",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-05434",
                    "DOI": "10.48550/arXiv.2209.05434",
                    "CorpusId": 252199614
                },
                "corpusId": 252199614,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/25c6c02f2db2e90e5660a92fcd134cd33addc8ce",
                "title": "Explicitly Controllable 3D-Aware Portrait Generation",
                "abstract": "\u2014In contrast to the traditional avatar creation pipeline which is a costly process, contemporary generative approaches directly learn the data distribution from photographs. While plenty of works extend unconditional generative models and achieve some levels of controllability, it is still challenging to ensure multi-view consistency, especially in large poses. In this work, we propose a network that generates 3D-aware portraits while being controllable according to semantic parameters regarding pose, identity, expression and illumination. Our network uses neural scene representation to model 3D-aware portraits, whose generation is guided by a parametric face model that supports explicit control. While the latent disentanglement can be further enhanced by contrasting images with partially different attributes, there still exists noticeable inconsistency in non-face areas, e.g. , hair and background, when animating expressions. We solve this by proposing a volume blending strategy in which we form a composite output by blending dynamic and static areas, with two parts segmented from the jointly learned semantic \ufb01eld. Our method outperforms prior arts in extensive experiments, producing realistic portraits with vivid expression in natural lighting when viewed from free viewpoints. It also demonstrates generalization ability to real images as well as out-of-domain data, showing great promise in real applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152949334",
                        "name": "Junshu Tang"
                    },
                    {
                        "authorId": "145803569",
                        "name": "Bo Zhang"
                    },
                    {
                        "authorId": "2157857267",
                        "name": "Binxin Yang"
                    },
                    {
                        "authorId": "2146320438",
                        "name": "Ting Zhang"
                    },
                    {
                        "authorId": "47514557",
                        "name": "Dong Chen"
                    },
                    {
                        "authorId": "2149343311",
                        "name": "Lizhuang Ma"
                    },
                    {
                        "authorId": "1716835",
                        "name": "Fang Wen"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[18] first used a neural renderer to generate pseudo samples with various poses and lightings, then used these samples to guide the images generated by GANs toward the corresponding sampled poses and lighting conditions."
            ],
            "citingPaper": {
                "paperId": "eabaf0003948824e3817b076bb75a3cc5eb0efa5",
                "externalIds": {
                    "DBLP": "conf/bmvc/ChungZSWC22",
                    "CorpusId": 253736279
                },
                "corpusId": 253736279,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/eabaf0003948824e3817b076bb75a3cc5eb0efa5",
                "title": "StyleFaceUV: a 3D Face UV Map Generator for View-Consistent Face Image Synthesis",
                "abstract": "Recent deep image generation models, such as StyleGAN2, face challenges to produce high-quality 2D face images with multi-view consistency. We address this issue by proposing an approach for generating detailed 3D faces using a pre-trained StyleGAN2 model. Our method estimates the 3D Morphable Model (3DMM) coefficients directly from the StyleGAN2\u2019s stylecode. To add more details to the produced 3D face models, we train a generator to produce two UV maps: a diffuse map to give the model a more faithful appearance and a generalized displacement map to add geometric details to the model. To achieve multi-view consistency, we also add a symmetric view image to recover information regarding the invisible side of a single image. The generated detailed 3D face models allow for consistent changes in viewing angles, expressions, and lighting conditions. Experimental results indicate that our method outperforms previous approaches both qualitatively and quantitatively.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2191617799",
                        "name": "Wei-Chieh Chung"
                    },
                    {
                        "authorId": "5415498",
                        "name": "Yu-Ting Wu"
                    },
                    {
                        "authorId": "143708263",
                        "name": "Yung-Yu Chuang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "3 Transcoder It has been observed in prior works that altering viewpoint by directly manipulating the style code is possible [28, 33].",
                "For instance, StyleGAN [13] representations have been shown to disentangle pose, shape and fine detail naturally, a property which has been used to help lift objects to 3D [12, 17, 28, 33, 39], these methods are 3D aware, but lack multi-view consistency."
            ],
            "citingPaper": {
                "paperId": "c8706740814131eeee90b5c279baa6a40de7ca63",
                "externalIds": {
                    "DBLP": "conf/bmvc/CharlesARC22",
                    "CorpusId": 256902308
                },
                "corpusId": 256902308,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/c8706740814131eeee90b5c279baa6a40de7ca63",
                "title": "Style2NeRF: An Unsupervised One-Shot NeRF for Semantic 3D Reconstruction",
                "abstract": "We present Style2NeRF, an unsupervised model for one-shot recovery of 3D pose, shape and appearance of symmetric objects. Style2NeRF contains a transcoder which disentangles 2D representations from pretrained StyleGANs, then maps them to a semantically editable 3D NeRF generator. As such, the generative NeRF inherits Style-GAN\u2019s expressiveness and image editing properties, translating them to 3D. We make four key contributions: (i) We provide a novel model to accurately estimate an object\u2019s 3D pose, shape and appearance without any human supervision during training; (ii) We show how to map between semantically meaningful 2D and 3D representations using a novel disentangled generative NeRF; (iii) we introduce the pose and viewpoint ambiguity problem (suffered by existing 3D GAN methods) and propose a solution improving pose estimation accuracy; (iv) Finally, via transfer learning, we show our model can be trained on real car images where the pose distribution is unknown. Style2NeRF outperforms the state-of-the-art on the CARLA cars dataset as well as a fully supervised model for the task of car pose estimation on ShapeNet-cars and a new dataset of real car images.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2055745260",
                        "name": "James Charles"
                    },
                    {
                        "authorId": "2461829",
                        "name": "Wim Abbeloos"
                    },
                    {
                        "authorId": "113231027",
                        "name": "Daniel Olmeda Reino"
                    },
                    {
                        "authorId": "1745672",
                        "name": "R. Cipolla"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "2188f4c932a9a7ddfab8a819f0257ce3cb4fc2eb",
                "externalIds": {
                    "DBLP": "conf/nips/YangJNK22",
                    "CorpusId": 258509658
                },
                "corpusId": 258509658,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2188f4c932a9a7ddfab8a819f0257ce3cb4fc2eb",
                "title": "Dense Interspecies Face Embedding",
                "abstract": "Dense Interspecies Face Embedding (DIFE) is a new direction for understanding faces of various animals by extracting common features among animal faces including human face. There are three main obstacles for interspecies face understanding: (1) lack of animal data compared to human, (2) ambiguous connection between faces of various animals, and (3) extreme shape and style variance. To cope with the lack of data, we utilize multi-teacher knowledge distillation of CSE and StyleGAN2 requiring no additional data or label. Then we synthesize pseudo pair images through the latent space exploration of StyleGAN2 to \ufb01nd implicit associations between different animal faces. Finally, we introduce the semantic matching loss to overcome the problem of extreme shape differences between species. To quantitatively evaluate our method over possible previous methodologies like unsupervised keypoint detection, we perform interspecies facial keypoint transfer on MAFL and AP-10K. Furthermore, the results of other applications like interspecies face image manipulation and dense keypoint transfer are provided. The code is available at https://github.com/kingsj0405/dife.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1579938286",
                        "name": "Sejong Yang"
                    },
                    {
                        "authorId": "79372499",
                        "name": "Subin Jeon"
                    },
                    {
                        "authorId": "7532506",
                        "name": "Seonghyeon Nam"
                    },
                    {
                        "authorId": "2248551752",
                        "name": "Seon Joo Kim"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "5730fadae797678927d74c1d03fe56939f3f06b8",
                "externalIds": {
                    "CorpusId": 240286167
                },
                "corpusId": 240286167,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5730fadae797678927d74c1d03fe56939f3f06b8",
                "title": "GAN-based 3D Manipulation of Car Models",
                "abstract": "Figure 1: Top row: We will build on the results obtained in FreeStyleGAN [1] where multi-view images a) are used to provide a coarse geometric proxy b). We can render the closest view with StyleGAN to generate a very realistic image c). This allows insertion into 3D virtual scenes d) and semantic manipulation e) or stereo viewing f). In this internship we will extend these ideas to models of cars captured with multiple images (left), allowing free-viewpoint rendering (center and right)",
                "year": 2021,
                "authors": []
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "736d4c1fd88d042a8a14d1bf816db73d3c604626",
                "externalIds": {
                    "CorpusId": 244129260
                },
                "corpusId": 244129260,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/736d4c1fd88d042a8a14d1bf816db73d3c604626",
                "title": "2021-04216-GAN-based 3D Manipulation of Car Models",
                "abstract": "The Inria Sophia Antipolis M\u00e9diterran\u00e9e center counts 34 research teams as well as 7 support departments. The center's staff (about 500 people including 320 Inria employees) is made up of scientists of different nationalities (250 foreigners of 50 nationalities), engineers, technicians and administrative staff. 1/3 of the staff are civil servants, the others are contractual agents. The majority of the center\u2019s research teams are located in Sophia Antipolis and Nice in the Alpes-Maritimes. Four teams are based in Montpellier and two teams are hosted in Bologna in Italy and Athens. The Center is a founding member of Universit\u00e9 C\u00f4te d'Azur and partner of the I-site MUSE supported by the University of Montpellier.",
                "year": 2021,
                "authors": []
            }
        }
    ]
}