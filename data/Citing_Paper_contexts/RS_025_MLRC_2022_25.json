{
    "offset": 0,
    "data": [
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "568e4260b47873a557d30a2eb0699d1046235057",
                "externalIds": {
                    "DBLP": "journals/kbs/ShimizuNG23",
                    "DOI": "10.1016/j.knosys.2023.110791",
                    "CorpusId": 259970336
                },
                "corpusId": 259970336,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/568e4260b47873a557d30a2eb0699d1046235057",
                "title": "Partial visual-semantic embedding: Fine-grained outfit image representation with massive volumes of tags via angular-based contrastive learning",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2053965325",
                        "name": "Ryotaro Shimizu"
                    },
                    {
                        "authorId": "2116472793",
                        "name": "Takuma Nakamura"
                    },
                    {
                        "authorId": "144092361",
                        "name": "M. Goto"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, many VSE models use triplet loss [2\u20138,12] and N -pair loss [1,13], and it is necessary to clarify the accuracy of other types of loss functions to demonstrate the validity of N -pair angular loss."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a1407da6239d4e31d388830b1566693f97342986",
                "externalIds": {
                    "DBLP": "conf/cvpr/ShimizuNG23",
                    "DOI": "10.1109/CVPRW59228.2023.00353",
                    "CorpusId": 260915828
                },
                "corpusId": 260915828,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a1407da6239d4e31d388830b1566693f97342986",
                "title": "Fashion-Specific Ambiguous Expression Interpretation with Partial Visual-Semantic Embedding",
                "abstract": "A novel technology named fashion intelligence system has been proposed to quantify ambiguous expressions unique to fashion, such as \"casual,\" \"adult-casual,\" and \"office-casual,\" and to support users\u2019 understanding of fashion. However, the existing visual-semantic embedding (VSE) model, which is the basis of its system, does not support situations in which images are composed of multiple parts such as hair, tops, pants, skirts, and shoes. We propose partial VSE, which enables sensitive learning for each part of the fashion outfits. This enables five types of practical functionalities, particularly image-retrieval tasks in which changes are made only to the specified parts and image-reordering tasks that focus on the specified parts by the single model. Based on both the multiple unique qualitative and quantitative evaluation experiments, we show the effectiveness of the proposed model.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2053965325",
                        "name": "Ryotaro Shimizu"
                    },
                    {
                        "authorId": "2116472793",
                        "name": "Takuma Nakamura"
                    },
                    {
                        "authorId": "144092361",
                        "name": "M. Goto"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5f2ffecd734106f061dec51f6600dcec5b9a404f",
                "externalIds": {
                    "ArXiv": "2304.07408",
                    "DBLP": "journals/corr/abs-2304-07408",
                    "DOI": "10.48550/arXiv.2304.07408",
                    "CorpusId": 258180440
                },
                "corpusId": 258180440,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5f2ffecd734106f061dec51f6600dcec5b9a404f",
                "title": "Fairness in Visual Clustering: A Novel Transformer Clustering Approach",
                "abstract": "Promoting fairness for deep clustering models in unsupervised clustering settings to reduce demographic bias is a challenging goal. This is because of the limitation of large-scale balanced data with well-annotated labels for sensitive or protected attributes. In this paper, we first evaluate demographic bias in deep clustering models from the perspective of cluster purity, which is measured by the ratio of positive samples within a cluster to their correlation degree. This measurement is adopted as an indication of demographic bias. Then, a novel loss function is introduced to encourage a purity consistency for all clusters to maintain the fairness aspect of the learned clustering model. Moreover, we present a novel attention mechanism, Cross-attention, to measure correlations between multiple clusters, strengthening faraway positive samples and improving the purity of clusters during the learning process. Experimental results on a large-scale dataset with numerous attribute settings have demonstrated the effectiveness of the proposed approach on both clustering accuracy and fairness enhancement on several sensitive attributes.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1959025244",
                        "name": "Xuan-Bac Nguyen"
                    },
                    {
                        "authorId": "1876581",
                        "name": "C. Duong"
                    },
                    {
                        "authorId": "1794486",
                        "name": "M. Savvides"
                    },
                    {
                        "authorId": "2091913080",
                        "name": "Kaushik Roy"
                    },
                    {
                        "authorId": "1769788",
                        "name": "Khoa Luu"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2021), training classifiers in the latent space (Jain et al., 2022; Yang et al., 2022), and embedding inputs with joint vision-language representations to find the error slices with a mixture model (Eyuboglu et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "38a2c434ad10f26e2b1f970d944d862da74f9e71",
                "externalIds": {
                    "ArXiv": "2304.03916",
                    "DBLP": "conf/icml/0007NPM23",
                    "DOI": "10.48550/arXiv.2304.03916",
                    "CorpusId": 258048716
                },
                "corpusId": 258048716,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/38a2c434ad10f26e2b1f970d944d862da74f9e71",
                "title": "Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning",
                "abstract": "Spurious correlations that degrade model generalization or lead the model to be right for the wrong reasons are one of the main robustness concerns for real-world deployments. However, mitigating these correlations during pre-training for large-scale models can be costly and impractical, particularly for those without access to high-performance computing resources. This paper proposes a novel approach to address spurious correlations during fine-tuning for a given domain of interest. With a focus on multi-modal models (e.g., CLIP), the proposed method leverages different modalities in these models to detect and explicitly set apart spurious attributes from the affected class, achieved through a multi-modal contrastive loss function that expresses spurious relationships through language. Our experimental results and in-depth visualizations on CLIP show that such an intervention can effectively i) improve the model's accuracy when spurious attributes are not present, and ii) directs the model's activation maps towards the actual class rather than the spurious attribute when present. In particular, on the Waterbirds dataset, our algorithm achieved a worst-group accuracy 23% higher than ERM on CLIP with a ResNet-50 backbone, and 32% higher on CLIP with a ViT backbone, while maintaining the same average accuracy as ERM.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2116468252",
                        "name": "Yu Yang"
                    },
                    {
                        "authorId": "2571049",
                        "name": "Besmira Nushi"
                    },
                    {
                        "authorId": "2542427",
                        "name": "H. Palangi"
                    },
                    {
                        "authorId": "2389094",
                        "name": "Baharan Mirzasoleiman"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a647ece46df6c6dc7837ce2b69774ae324cfbb6b",
                "externalIds": {
                    "ArXiv": "2303.14369",
                    "DBLP": "journals/corr/abs-2303-14369",
                    "DOI": "10.1109/CVPR52729.2023.00244",
                    "CorpusId": 257766265
                },
                "corpusId": 257766265,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a647ece46df6c6dc7837ce2b69774ae324cfbb6b",
                "title": "Video-Text as Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning",
                "abstract": "Contrastive learning-based video-language representation learning approaches, e.g., CLIP, have achieved outstanding performance, which pursue semantic interaction upon pre-defined video-text pairs. To clarify this coarse-grained global interaction and move a step further, we have to encounter challenging shell-breaking interactions for fine-grained cross-modal learning. In this paper, we creatively model video-text as game players with multivariate cooperative game theory to wisely handle the uncertainty during fine-grained semantic interaction with diverse granularity, flexible combination, and vague intensity. Concretely, we propose Hierarchical Banzhaf Interaction (HBI) to value possible correspondence between video frames and text words for sensitive and explainable cross-modal contrast. To efficiently realize the cooperative game of multiple video frames and multiple text words, the proposed method clusters the original video frames (text words) and computes the Banzhaf Interaction between the merged tokens. By stacking token merge modules, we achieve cooperative games at different semantic levels. Extensive experiments on commonly used text-video retrieval and video-question answering bench-marks with superior performances justify the efficacy of our HBI. More encouragingly, it can also serve as a visualization tool to promote the understanding of cross-modal interaction, which have a far-reaching impact on the community. Project page is available at https://jpthu17.github.io/HBI/.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2185571736",
                        "name": "Peng Jin"
                    },
                    {
                        "authorId": "2000011573",
                        "name": "Jinfa Huang"
                    },
                    {
                        "authorId": "40448951",
                        "name": "Pengfei Xiong"
                    },
                    {
                        "authorId": "37749726",
                        "name": "Shangxuan Tian"
                    },
                    {
                        "authorId": "2144545106",
                        "name": "Chang Liu"
                    },
                    {
                        "authorId": "2117709282",
                        "name": "Xiang Ji"
                    },
                    {
                        "authorId": "2087091278",
                        "name": "Li-ming Yuan"
                    },
                    {
                        "authorId": "2155102170",
                        "name": "Jie Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, many VSE models use triplet loss [19, 1, 52, 20, 13, 15, 4, 3] and n-pair loss [12, 51], and",
                "VSE models have been widely researched for image caption and image description retrieval [3, 4, 5], visual questionanswering [6, 7], image-to-description and description-toimage generation [8, 9, 10], hashing tasks [11, 12], person re-identification [13], and video similarity evaluation [14]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "da94711611548a4e0cedb77640938ae5c49c8d69",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-06688",
                    "ArXiv": "2211.06688",
                    "DOI": "10.48550/arXiv.2211.06688",
                    "CorpusId": 253511281
                },
                "corpusId": 253511281,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/da94711611548a4e0cedb77640938ae5c49c8d69",
                "title": "Partial Visual-Semantic Embedding: Fashion Intelligence System with Sensitive Part-by-Part Learning",
                "abstract": "In this study, we propose a technology called the Fashion Intelligence System based on the visual-semantic embedding (VSE) model to quantify abstract and complex expressions unique to fashion, such as ''casual,'' ''adult-casual,'' and ''office-casual,'' and to support users' understanding of fashion. However, the existing VSE model does not support the situations in which the image is composed of multiple parts such as hair, tops, pants, skirts, and shoes. We propose partial VSE, which enables sensitive learning for each part of the fashion coordinates. The proposed model partially learns embedded representations. This helps retain the various existing practical functionalities and enables image-retrieval tasks in which changes are made only to the specified parts and image reordering tasks that focus on the specified parts. This was not possible with conventional models. Based on both the qualitative and quantitative evaluation experiments, we show that the proposed model is superior to conventional models without increasing the computational complexity.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2053965325",
                        "name": "Ryotaro Shimizu"
                    },
                    {
                        "authorId": "2116472793",
                        "name": "Takuma Nakamura"
                    },
                    {
                        "authorId": "144092361",
                        "name": "M. Goto"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Prior works have found that a face dataset dominated by theWhite race produces a poor performance for other races, while a face dataset with balanced group distribution, from either real or synthesized data, can enhance fairness [15, 17, 18,30,52,58]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6bbff0e9f83d308804c10cbb6c659d5a413ef203",
                "externalIds": {
                    "DBLP": "conf/eccv/LinKJ22",
                    "ArXiv": "2207.10888",
                    "DOI": "10.48550/arXiv.2207.10888",
                    "CorpusId": 251018716
                },
                "corpusId": 251018716,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/6bbff0e9f83d308804c10cbb6c659d5a413ef203",
                "title": "FairGRAPE: Fairness-aware GRAdient Pruning mEthod for Face Attribute Classification",
                "abstract": "Existing pruning techniques preserve deep neural networks' overall ability to make correct predictions but may also amplify hidden biases during the compression process. We propose a novel pruning method, Fairness-aware GRAdient Pruning mEthod (FairGRAPE), that minimizes the disproportionate impacts of pruning on different sub-groups. Our method calculates the per-group importance of each model weight and selects a subset of weights that maintain the relative between-group total importance in pruning. The proposed method then prunes network edges with small importance values and repeats the procedure by updating importance values. We demonstrate the effectiveness of our method on four different datasets, FairFace, UTKFace, CelebA, and ImageNet, for the tasks of face attribute classification where our method reduces the disparity in performance degradation by up to 90% compared to the state-of-the-art pruning algorithms. Our method is substantially more effective in a setting with a high pruning rate (99%). The code and dataset used in the experiments are available at https://github.com/Bernardo1998/FairGRAPE",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2174655688",
                        "name": "Xiao-Ze Lin"
                    },
                    {
                        "authorId": "2294567",
                        "name": "Seungbae Kim"
                    },
                    {
                        "authorId": "1834047",
                        "name": "Jungseock Joo"
                    }
                ]
            }
        }
    ]
}