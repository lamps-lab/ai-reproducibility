{
    "offset": 0,
    "data": [
        {
            "contexts": [
                "Micaelli et al. [5] exploited adversarial distillation o transfer the knowledge (i.e., ZSKT) from teacher to student by\nullback-Leibler (KL) divergence and spatial attention.",
                "DAFL [4] ZSKT [5] DFAD [25] ADI [6] DFQ [26] CMI [7] Ours",
                "[5] exploited adversarial distillation o transfer the knowledge (i.",
                "As exhibited in Table 2 , we compare the proposed learning aradigm with the other representative approaches for data-free nowledge distillation, including DAFL [4] , ZSKT [5] , DFQ [26] , FAD [25] , ADI [6] , and CMI [7] .",
                "Through dversarial training, GANs can synthesize fake images to support istillation between teacher and student [4,5] .",
                "Comparison of DFKD methods\nAs exhibited in Table 2 , we compare the proposed learning aradigm with the other representative approaches for data-free nowledge distillation, including DAFL [4] , ZSKT [5] , DFQ [26] , FAD [25] , ADI [6] , and CMI [7] .",
                "[5] mainly exploited adersarial distillation with Kullback-Leibler (KL) divergence between he outputs of the teacher and student.",
                "DFKD methods incline to exploit a generator to synthesize masive samples to support the knowledge distillation learning beween teacher and student [4,5,25] .",
                "Besides, in the DFAD framework, the generator is more prone o model collapse due to inadequate optimization [4,5,25] .",
                "(1) DAFL, ZSKT, and DFAD mainly imitate the teacher by educing the discrepancy in the output layer."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "5918684f43a4571a9276c894647155bd4dcb927f",
                "externalIds": {
                    "DBLP": "journals/pr/ShaoZW23",
                    "DOI": "10.1016/j.patcog.2023.109781",
                    "CorpusId": 259689113
                },
                "corpusId": 259689113,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5918684f43a4571a9276c894647155bd4dcb927f",
                "title": "Conditional pseudo-supervised contrast for data-Free knowledge distillation",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2222550656",
                        "name": "Renrong Shao"
                    },
                    {
                        "authorId": "49039407",
                        "name": "Wei Zhang"
                    },
                    {
                        "authorId": "66063792",
                        "name": "J. Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "ZSKT [33] and DFAD [12] train an adversarial generator to search for images where the prediction of the student poorly matches the teacher\u2019s prediction."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b2385ec964b26cb560e3cc7bfb8f4471285b5749",
                "externalIds": {
                    "ArXiv": "2310.03661",
                    "CorpusId": 263671881
                },
                "corpusId": 263671881,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b2385ec964b26cb560e3cc7bfb8f4471285b5749",
                "title": "Robustness-Guided Image Synthesis for Data-Free Quantization",
                "abstract": "Quantization has emerged as a promising direction for model compression. Recently, data-free quantization has been widely studied as a promising method to avoid privacy concerns, which synthesizes images as an alternative to real training data. Existing methods use classification loss to ensure the reliability of the synthesized images. Unfortunately, even if these images are well-classified by the pre-trained model, they still suffer from low semantics and homogenization issues. Intuitively, these low-semantic images are sensitive to perturbations, and the pre-trained model tends to have inconsistent output when the generator synthesizes an image with poor semantics. To this end, we propose Robustness-Guided Image Synthesis (RIS), a simple but effective method to enrich the semantics of synthetic images and improve image diversity, further boosting the performance of downstream data-free compression tasks. Concretely, we first introduce perturbations on input and model weight, then define the inconsistency metrics at feature and prediction levels before and after perturbations. On the basis of inconsistency on two levels, we design a robustness optimization objective to enhance the semantics of synthetic images. Moreover, we also make our approach diversity-aware by forcing the generator to synthesize images with small correlations in the label space. With RIS, we achieve state-of-the-art performance for various settings on data-free quantization and can be extended to other data-free compression tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2155929084",
                        "name": "Jianhong Bai"
                    },
                    {
                        "authorId": "2254276639",
                        "name": "Yuchen Yang"
                    },
                    {
                        "authorId": "2147290128",
                        "name": "Huanpeng Chu"
                    },
                    {
                        "authorId": "2155981033",
                        "name": "Hualiang Wang"
                    },
                    {
                        "authorId": "2254306186",
                        "name": "Zuozhu Liu"
                    },
                    {
                        "authorId": "2255346632",
                        "name": "Ruizhe Chen"
                    },
                    {
                        "authorId": "2255343037",
                        "name": "Xiaoxuan He"
                    },
                    {
                        "authorId": "2253396330",
                        "name": "Lianrui Mu"
                    },
                    {
                        "authorId": "2254277102",
                        "name": "Chengfei Cai"
                    },
                    {
                        "authorId": "2254179601",
                        "name": "Haoji Hu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "03979a332c58617597b7e4e4fa7816e4d94693ca",
                "externalIds": {
                    "ArXiv": "2310.00258",
                    "CorpusId": 263334159
                },
                "corpusId": 263334159,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/03979a332c58617597b7e4e4fa7816e4d94693ca",
                "title": "Unleash Data Generation for Efficient and Effective Data-free Knowledge Distillation",
                "abstract": "Data-Free Knowledge Distillation (DFKD) has recently made remarkable advancements with its core principle of transferring knowledge from a teacher neural network to a student neural network without requiring access to the original data. Nonetheless, existing approaches encounter a significant challenge when attempting to generate samples from random noise inputs, which inherently lack meaningful information. Consequently, these models struggle to effectively map this noise to the ground-truth sample distribution, resulting in the production of low-quality data and imposing substantial time requirements for training the generator. In this paper, we propose a novel Noisy Layer Generation method (NAYER) which relocates the randomness source from the input to a noisy layer and utilizes the meaningful label-text embedding (LTE) as the input. The significance of LTE lies in its ability to contain substantial meaningful inter-class information, enabling the generation of high-quality samples with only a few training steps. Simultaneously, the noisy layer plays a key role in addressing the issue of diversity in sample generation by preventing the model from overemphasizing the constrained label information. By reinitializing the noisy layer in each iteration, we aim to facilitate the generation of diverse samples while still retaining the method's efficiency, thanks to the ease of learning provided by LTE. Experiments carried out on multiple datasets demonstrate that our NAYER not only outperforms the state-of-the-art methods but also achieves speeds 5 to 15 times faster than previous approaches.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2249550142",
                        "name": "Minh-Tuan Tran"
                    },
                    {
                        "authorId": "2249909946",
                        "name": "Trung Le"
                    },
                    {
                        "authorId": "2249761604",
                        "name": "Xuan-May Le"
                    },
                    {
                        "authorId": "23911916",
                        "name": "Mehrtash Harandi"
                    },
                    {
                        "authorId": "2249761516",
                        "name": "Quan Hung Tran"
                    },
                    {
                        "authorId": "1400659302",
                        "name": "Dinh Q. Phung"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Given the aforementioned context, we present a pipeline that extracts the functionality of a black-box classification model (named teacher) into a locally created copymodel (called student) via knowledge distillation [1, 7, 27, 29, 53] and self-paced active learning, as shown in Figure 1."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "32b28fee4daecef301dfe0e37482d17ab6f0e042",
                "externalIds": {
                    "ArXiv": "2310.00096",
                    "CorpusId": 263334500
                },
                "corpusId": 263334500,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/32b28fee4daecef301dfe0e37482d17ab6f0e042",
                "title": "Towards Few-Call Model Stealing via Active Self-Paced Knowledge Distillation and Diffusion-Based Image Generation",
                "abstract": "Diffusion models showcased strong capabilities in image synthesis, being used in many computer vision tasks with great success. To this end, we propose to explore a new use case, namely to copy black-box classification models without having access to the original training data, the architecture, and the weights of the model, \\ie~the model is only exposed through an inference API. More specifically, we can only observe the (soft or hard) labels for some image samples passed as input to the model. Furthermore, we consider an additional constraint limiting the number of model calls, mostly focusing our research on few-call model stealing. In order to solve the model extraction task given the applied restrictions, we propose the following framework. As training data, we create a synthetic data set (called proxy data set) by leveraging the ability of diffusion models to generate realistic and diverse images. Given a maximum number of allowed API calls, we pass the respective number of samples through the black-box model to collect labels. Finally, we distill the knowledge of the black-box teacher (attacked model) into a student model (copy of the attacked model), harnessing both labeled and unlabeled data generated by the diffusion model. We employ a novel active self-paced learning framework to make the most of the proxy data during distillation. Our empirical results on two data sets confirm the superiority of our framework over two state-of-the-art methods in the few-call model extraction scenario.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "123480571",
                        "name": "Vlad Hondru"
                    },
                    {
                        "authorId": "2249763264",
                        "name": "Radu Tudor Ionescu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "abc846a7cca07efeee8a866e0a8a4b548dba50d1",
                "externalIds": {
                    "ArXiv": "2309.13546",
                    "DBLP": "journals/corr/abs-2309-13546",
                    "DOI": "10.48550/arXiv.2309.13546",
                    "CorpusId": 262465831
                },
                "corpusId": 262465831,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/abc846a7cca07efeee8a866e0a8a4b548dba50d1",
                "title": "DFRD: Data-Free Robustness Distillation for Heterogeneous Federated Learning",
                "abstract": "Federated Learning (FL) is a privacy-constrained decentralized machine learning paradigm in which clients enable collaborative training without compromising private data. However, how to learn a robust global model in the data-heterogeneous and model-heterogeneous FL scenarios is challenging. To address it, we resort to data-free knowledge distillation to propose a new FL method (namely DFRD). DFRD equips a conditional generator on the server to approximate the training space of the local models uploaded by clients, and systematically investigates its training in terms of fidelity, transferability} and diversity. To overcome the catastrophic forgetting of the global model caused by the distribution shifts of the generator across communication rounds, we maintain an exponential moving average copy of the generator on the server. Additionally, we propose dynamic weighting and label sampling to accurately extract knowledge from local models. Finally, our extensive experiments on various image classification tasks illustrate that DFRD achieves significant performance gains compared to SOTA baselines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "103690594",
                        "name": "Kangyang Luo"
                    },
                    {
                        "authorId": "2155449067",
                        "name": "Shuai Wang"
                    },
                    {
                        "authorId": "2218317321",
                        "name": "Y. Fu"
                    },
                    {
                        "authorId": "2954787",
                        "name": "Xiang Li"
                    },
                    {
                        "authorId": "3458560",
                        "name": "Yunshi Lan"
                    },
                    {
                        "authorId": "2210492520",
                        "name": "Minghui Gao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, there have been developments in Zero-Shot Knowledge Distillation frameworks [37] that can distill knowledge from a teacher to a student model, even without training data."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "db4e40fd1d2caab22d1ad94ab0496a1ab155b1e2",
                "externalIds": {
                    "ArXiv": "2309.05132",
                    "DBLP": "journals/corr/abs-2309-05132",
                    "DOI": "10.48550/arXiv.2309.05132",
                    "CorpusId": 261682350
                },
                "corpusId": 261682350,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/db4e40fd1d2caab22d1ad94ab0496a1ab155b1e2",
                "title": "DAD++: Improved Data-free Test Time Adversarial Defense",
                "abstract": "With the increasing deployment of deep neural networks in safety-critical applications such as self-driving cars, medical imaging, anomaly detection, etc., adversarial robustness has become a crucial concern in the reliability of these networks in real-world scenarios. A plethora of works based on adversarial training and regularization-based techniques have been proposed to make these deep networks robust against adversarial attacks. However, these methods require either retraining models or training them from scratch, making them infeasible to defend pre-trained models when access to training data is restricted. To address this problem, we propose a test time Data-free Adversarial Defense (DAD) containing detection and correction frameworks. Moreover, to further improve the efficacy of the correction framework in cases when the detector is under-confident, we propose a soft-detection scheme (dubbed as\"DAD++\"). We conduct a wide range of experiments and ablations on several datasets and network architectures to show the efficacy of our proposed approach. Furthermore, we demonstrate the applicability of our approach in imparting adversarial defense at test time under data-free (or data-efficient) applications/setups, such as Data-free Knowledge Distillation and Source-free Unsupervised Domain Adaptation, as well as Semi-supervised classification frameworks. We observe that in all the experiments and applications, our DAD++ gives an impressive performance against various adversarial attacks with a minimal drop in clean accuracy. The source code is available at: https://github.com/vcl-iisc/Improved-Data-free-Test-Time-Adversarial-Defense",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "143747407",
                        "name": "Gaurav Kumar Nayak"
                    },
                    {
                        "authorId": "2044355793",
                        "name": "Inder Khatri"
                    },
                    {
                        "authorId": "2189541527",
                        "name": "Shubham Randive"
                    },
                    {
                        "authorId": "1658305348",
                        "name": "Ruchit Rawal"
                    },
                    {
                        "authorId": "2238953541",
                        "name": "Anirban Chakraborty"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9197e87ee8f00e7b3a3c3f7694e0e4f85b0b8824",
                "externalIds": {
                    "DOI": "10.1016/j.displa.2023.102543",
                    "CorpusId": 262095212
                },
                "corpusId": 262095212,
                "publicationVenue": {
                    "id": "09439d99-578a-4161-86ee-e22a190b0d4d",
                    "name": "Displays (Guildford)",
                    "type": "journal",
                    "alternate_names": [
                        "Displays",
                        "Disp (guildford"
                    ],
                    "issn": "0141-9382",
                    "url": "https://www.journals.elsevier.com/displays"
                },
                "url": "https://www.semanticscholar.org/paper/9197e87ee8f00e7b3a3c3f7694e0e4f85b0b8824",
                "title": "Trained Teacher: who is good at teaching",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2243551802",
                        "name": "Xingzhu Liang"
                    },
                    {
                        "authorId": "2225727995",
                        "name": "Feilong Bi"
                    },
                    {
                        "authorId": "2243688073",
                        "name": "Wen Liu"
                    },
                    {
                        "authorId": "2243564800",
                        "name": "Xinyun Yan"
                    },
                    {
                        "authorId": "2243530709",
                        "name": "Chunjiong Zhang"
                    },
                    {
                        "authorId": "2243427461",
                        "name": "Chenxing Xia"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some works (Nayak et al. 2019; Micaelli and Storkey 2019) have explored distillation through pseudodata generation from the weights of the teacher model or through a generator adversarially trained with the student model, particularly when real data are unavailable for training."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5e03815dacc2da0941dbd2f5e2439f806294ecbd",
                "externalIds": {
                    "ArXiv": "2308.13662",
                    "DBLP": "journals/corr/abs-2308-13662",
                    "DOI": "10.48550/arXiv.2308.13662",
                    "CorpusId": 261245277
                },
                "corpusId": 261245277,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5e03815dacc2da0941dbd2f5e2439f806294ecbd",
                "title": "Resource-Efficient Federated Learning for Heterogenous and Resource-Constrained Environments",
                "abstract": "Federated Learning (FL) is a privacy-enforcing sub-domain of machine learning that brings the model to the user's device for training, avoiding the need to share personal data with a central server. While existing works address data heterogeneity, they overlook other challenges in FL, such as device heterogeneity and communication efficiency. In this paper, we propose RE-FL, a novel approach that tackles computational and communication challenges in resource-constrained devices. Our variable pruning technique optimizes resource utilization by adapting pruning to each client's computational capabilities. We also employ knowledge distillation to reduce bandwidth consumption and communication rounds. Experimental results on image classification tasks demonstrate the effectiveness of our approach in resource-constrained environments, maintaining data privacy and performance while accommodating heterogeneous model architectures.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2234396508",
                        "name": "Humaid Ahmed Desai"
                    },
                    {
                        "authorId": "24484736",
                        "name": "Amr B. Hilal"
                    },
                    {
                        "authorId": "3355952",
                        "name": "Hoda Eldardiry"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Specifically, these cases are known as few-shot [18], [19] and zero-shot unlearning [10], [20], [21], respectively."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c646e127346af8b9ab3087b89c425c71d25e8cef",
                "externalIds": {
                    "ArXiv": "2308.09881",
                    "DBLP": "journals/corr/abs-2308-09881",
                    "DOI": "10.48550/arXiv.2308.09881",
                    "CorpusId": 261048914
                },
                "corpusId": 261048914,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c646e127346af8b9ab3087b89c425c71d25e8cef",
                "title": "Generative Adversarial Networks Unlearning",
                "abstract": "As machine learning continues to develop, and data misuse scandals become more prevalent, individuals are becoming increasingly concerned about their personal information and are advocating for the right to remove their data. Machine unlearning has emerged as a solution to erase training data from trained machine learning models. Despite its success in classifiers, research on Generative Adversarial Networks (GANs) is limited due to their unique architecture, including a generator and a discriminator. One challenge pertains to generator unlearning, as the process could potentially disrupt the continuity and completeness of the latent space. This disruption might consequently diminish the model's effectiveness after unlearning. Another challenge is how to define a criterion that the discriminator should perform for the unlearning images. In this paper, we introduce a substitution mechanism and define a fake label to effectively mitigate these challenges. Based on the substitution mechanism and fake label, we propose a cascaded unlearning approach for both item and class unlearning within GAN models, in which the unlearning and learning processes run in a cascaded manner. We conducted a comprehensive evaluation of the cascaded unlearning technique using the MNIST and CIFAR-10 datasets. Experimental results demonstrate that this approach achieves significantly improved item and class unlearning efficiency, reducing the required time by up to 185x and 284x for the MNIST and CIFAR-10 datasets, respectively, in comparison to retraining from scratch. Notably, although the model's performance experiences minor degradation after unlearning, this reduction is negligible when dealing with a minimal number of images (e.g., 64) and has no adverse effects on downstream tasks such as classification.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2198904559",
                        "name": "Hui Sun"
                    },
                    {
                        "authorId": "2185053609",
                        "name": "Tianqing Zhu"
                    },
                    {
                        "authorId": "153604695",
                        "name": "Wenhan Chang"
                    },
                    {
                        "authorId": "2134555583",
                        "name": "Wanlei Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Assuming sufficiently, through this approach, one can possibly simultaneously use a simpler and more reliable model that clinicians would be familiar with while preserving higher performance, apply a model trained on a large amount of data to a smaller dataset thereby avoiding overfitting, and reducing computational costs by simply \u2019cut-and-pasting\u2019 a pre-trained model to the problem at hand [11, 69]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fea19ea351f65115c470bdd6a2446d7e41ee799f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-08407",
                    "ArXiv": "2308.08407",
                    "DOI": "10.48550/arXiv.2308.08407",
                    "CorpusId": 260926590
                },
                "corpusId": 260926590,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fea19ea351f65115c470bdd6a2446d7e41ee799f",
                "title": "Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities",
                "abstract": "Recent advancements in AI applications to healthcare have shown incredible promise in surpassing human performance in diagnosis and disease prognosis. With the increasing complexity of AI models, however, concerns regarding their opacity, potential biases, and the need for interpretability. To ensure trust and reliability in AI systems, especially in clinical risk prediction models, explainability becomes crucial. Explainability is usually referred to as an AI system's ability to provide a robust interpretation of its decision-making logic or the decisions themselves to human stakeholders. In clinical risk prediction, other aspects of explainability like fairness, bias, trust, and transparency also represent important concepts beyond just interpretability. In this review, we address the relationship between these concepts as they are often used together or interchangeably. This review also discusses recent progress in developing explainable models for clinical risk prediction, highlighting the importance of quantitative and clinical evaluation and validation across multiple common modalities in clinical practice. It emphasizes the need for external validation and the combination of diverse interpretability methods to enhance trust and fairness. Adopting rigorous testing, such as using synthetic datasets with known generative factors, can further improve the reliability of explainability methods. Open access and code-sharing resources are essential for transparency and reproducibility, enabling the growth and trustworthiness of explainable research. While challenges exist, an end-to-end approach to explainability in clinical risk prediction, incorporating stakeholders from clinicians to developers, is essential for success.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2031505256",
                        "name": "Munib Mesinovic"
                    },
                    {
                        "authorId": "40041965",
                        "name": "P. Watkinson"
                    },
                    {
                        "authorId": "2163214121",
                        "name": "Ting Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[53] are motivated by the framework of datafree knowledge distillation [9, 40] and proposed data-free model"
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "80ce78d3e14dc3f29eb87bf98d52e56f3f61af72",
                "externalIds": {
                    "ArXiv": "2308.00958",
                    "DBLP": "journals/corr/abs-2308-00958",
                    "DOI": "10.48550/arXiv.2308.00958",
                    "CorpusId": 260379180
                },
                "corpusId": 260379180,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/80ce78d3e14dc3f29eb87bf98d52e56f3f61af72",
                "title": "Isolation and Induction: Training Robust Deep Neural Networks against Model Stealing Attacks",
                "abstract": "Despite the broad application of Machine Learning models as a Service (MLaaS), they are vulnerable to model stealing attacks. These attacks can replicate the model functionality by using the black-box query process without any prior knowledge of the target victim model. Existing stealing defenses add deceptive perturbations to the victim's posterior probabilities to mislead the attackers. However, these defenses are now suffering problems of high inference computational overheads and unfavorable trade-offs between benign accuracy and stealing robustness, which challenges the feasibility of deployed models in practice. To address the problems, this paper proposes Isolation and Induction (InI), a novel and effective training framework for model stealing defenses. Instead of deploying auxiliary defense modules that introduce redundant inference time, InI directly trains a defensive model by isolating the adversary's training gradient from the expected gradient, which can effectively reduce the inference computational cost. In contrast to adding perturbations over model predictions that harm the benign accuracy, we train models to produce uninformative outputs against stealing queries, which can induce the adversary to extract little useful knowledge from victim models with minimal impact on the benign performance. Extensive experiments on several visual classification datasets (e.g., MNIST and CIFAR10) demonstrate the superior robustness (up to 48% reduction on stealing accuracy) and speed (up to 25.4x faster) of our InI over other state-of-the-art methods. Our codes can be found in https://github.com/DIG-Beihang/InI-Model-Stealing-Defense.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2162822687",
                        "name": "Jun Guo"
                    },
                    {
                        "authorId": "153152072",
                        "name": "Aishan Liu"
                    },
                    {
                        "authorId": "1636070033",
                        "name": "Xingyu Zheng"
                    },
                    {
                        "authorId": "2114786732",
                        "name": "Siyuan Liang"
                    },
                    {
                        "authorId": "40033753",
                        "name": "Yisong Xiao"
                    },
                    {
                        "authorId": "2107976307",
                        "name": "Yichao Wu"
                    },
                    {
                        "authorId": "6820648",
                        "name": "Xianglong Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "First, their trained generators are abandoned after the students\u2019 training [5, 17, 33, 20, 14, 51].",
                "One is to have to spend extra computing costs to obtain generation data by generation module, including: DeepInv [50], CMI [18], DAFL [5], ZSKT [33], DFED [20], DFQ [11], Fast [16], MAD [14], DFD [32], and DFAD [17]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "10d6f42e346c367cea05374ecbdbc52d8c1abc2a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-16601",
                    "ArXiv": "2307.16601",
                    "DOI": "10.48550/arXiv.2307.16601",
                    "CorpusId": 260334366
                },
                "corpusId": 260334366,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/10d6f42e346c367cea05374ecbdbc52d8c1abc2a",
                "title": "Sampling to Distill: Knowledge Transfer from Open-World Data",
                "abstract": "Data-Free Knowledge Distillation (DFKD) is a novel task that aims to train high-performance student models using only the teacher network without original training data. Despite encouraging results, existing DFKD methods rely heavily on generation modules with high computational costs. Meanwhile, they ignore the fact that the generated and original data exist domain shifts due to the lack of supervision information. Moreover, knowledge is transferred through each example, ignoring the implicit relationship among multiple examples. To this end, we propose a novel Open-world Data Sampling Distillation (ODSD) method without a redundant generation process. First, we try to sample open-world data close to the original data's distribution by an adaptive sampling module. Then, we introduce a low-noise representation to alleviate the domain shifts and build a structured relationship of multiple data examples to exploit data knowledge. Extensive experiments on CIFAR-10, CIFAR-100, NYUv2, and ImageNet show that our ODSD method achieves state-of-the-art performance. Especially, we improve 1.50\\%-9.59\\% accuracy on the ImageNet dataset compared with the existing results.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2205141992",
                        "name": "Yuzheng Wang"
                    },
                    {
                        "authorId": "2111606069",
                        "name": "Zhaoyu Chen"
                    },
                    {
                        "authorId": "49050131",
                        "name": "J. Zhang"
                    },
                    {
                        "authorId": "2143920085",
                        "name": "Dingkang Yang"
                    },
                    {
                        "authorId": "2032815387",
                        "name": "Zuhao Ge"
                    },
                    {
                        "authorId": "31505222",
                        "name": "Yang Liu"
                    },
                    {
                        "authorId": "2186625567",
                        "name": "Siao Liu"
                    },
                    {
                        "authorId": "2109125571",
                        "name": "Yunquan Sun"
                    },
                    {
                        "authorId": "2159070511",
                        "name": "Wenqiang Zhang"
                    },
                    {
                        "authorId": "2861714",
                        "name": "Lizhe Qi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Compared Methods: We compare our proposed KD(3) with representative data-free methods, including DataFree Learning (DAFL) [7], Data-Free Adversarial Learning (DFAD) [15], Dual Discriminator Adversarial Distillation (DDAD) [50], DeepInversion (DI) [45], Zero-Shot Knowledge Transfer (ZSKT) [30], Pseudo Replay Enhanced Data-Free Knowledge Distillation (PRE) [3], DataFree Quantization (DFQ) [10], Contrastive Model Inversion (CMI) [16], and Data-Free Noisy Distillation (DFND) [6] (the only existing method using the webly collected data).",
                "Inspired by the Generative Adversarial Networks [20], a series of works [7, 15, 30] treat the teacher network as the discriminator to supervise a generator to produce pseudo data from random noise.",
                "Compared Methods: We compare our proposed KD3 with representative data-free methods, including DataFree Learning (DAFL) [7], Data-Free Adversarial Learning (DFAD) [15], Dual Discriminator Adversarial Distillation (DDAD) [50], DeepInversion (DI) [45], Zero-Shot Knowledge Transfer (ZSKT) [30], Pseudo Replay Enhanced Data-Free Knowledge Distillation (PRE) [3], DataFree Quantization (DFQ) [10], Contrastive Model Inversion (CMI) [16], and Data-Free Noisy Distillation (DFND) [6] (the only existing method using the webly collected data)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2dbc89ddb482a60e23517380a045c078ba8e4e57",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-11469",
                    "ArXiv": "2307.11469",
                    "DOI": "10.48550/arXiv.2307.11469",
                    "CorpusId": 260091683
                },
                "corpusId": 260091683,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2dbc89ddb482a60e23517380a045c078ba8e4e57",
                "title": "Distribution Shift Matters for Knowledge Distillation with Webly Collected Images",
                "abstract": "Knowledge distillation aims to learn a lightweight student network from a pre-trained teacher network. In practice, existing knowledge distillation methods are usually infeasible when the original training data is unavailable due to some privacy issues and data management considerations. Therefore, data-free knowledge distillation approaches proposed to collect training instances from the Internet. However, most of them have ignored the common distribution shift between the instances from original training data and webly collected data, affecting the reliability of the trained student network. To solve this problem, we propose a novel method dubbed ``Knowledge Distillation between Different Distributions\"(KD$^{3}$), which consists of three components. Specifically, we first dynamically select useful training instances from the webly collected data according to the combined predictions of teacher network and student network. Subsequently, we align both the weighted features and classifier parameters of the two networks for knowledge memorization. Meanwhile, we also build a new contrastive learning block called MixDistribution to generate perturbed data with a new distribution for instance alignment, so that the student network can further learn a distribution-invariant representation. Intensive experiments on various benchmark datasets demonstrate that our proposed KD$^{3}$ can outperform the state-of-the-art data-free knowledge distillation approaches.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51190967",
                        "name": "Jialiang Tang"
                    },
                    {
                        "authorId": "15841516",
                        "name": "Shuo Chen"
                    },
                    {
                        "authorId": "47537639",
                        "name": "Gang Niu"
                    },
                    {
                        "authorId": "67154907",
                        "name": "Masashi Sugiyama"
                    },
                    {
                        "authorId": "2143630403",
                        "name": "Chenggui Gong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Except for synthesizing real-looking images, researchers also integrate GANs into downstream tasks of network training, such as knowledge distillation [37, 20, 36], data augmentation [1, 7], continual learning [58, 59], dataset distillation [82, 13] and privacy-preserving learning [70, 63, 19]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "631f4558f685cea7c2ce0a4285e0bbb5abe7d2df",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-08526",
                    "ArXiv": "2307.08526",
                    "DOI": "10.48550/arXiv.2307.08526",
                    "CorpusId": 259937371
                },
                "corpusId": 259937371,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/631f4558f685cea7c2ce0a4285e0bbb5abe7d2df",
                "title": "Image Captions are Natural Prompts for Text-to-Image Models",
                "abstract": "With the rapid development of Artificial Intelligence Generated Content (AIGC), it has become common practice in many learning tasks to train or fine-tune large models on synthetic data due to the data-scarcity and privacy leakage problems. Albeit promising with unlimited data generation, owing to massive and diverse information conveyed in real images, it is challenging for text-to-image generative models to synthesize informative training data with hand-crafted prompts, which usually leads to inferior generalization performance when training downstream models. In this paper, we theoretically analyze the relationship between the training effect of synthetic data and the synthetic data distribution induced by prompts. Then we correspondingly propose a simple yet effective method that prompts text-to-image generative models to synthesize more informative and diverse training data. Specifically, we caption each real image with the advanced captioning model to obtain informative and faithful prompts that extract class-relevant information and clarify the polysemy of class names. The image captions and class names are concatenated to prompt generative models for training image synthesis. Extensive experiments on ImageNette, ImageNet-100, and ImageNet-1K verify that our method significantly improves the performance of models trained on synthetic training data, i.e., 10% classification accuracy improvements on average.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "152384041",
                        "name": "Shiye Lei"
                    },
                    {
                        "authorId": "2051536212",
                        "name": "Hao Chen"
                    },
                    {
                        "authorId": "51303162",
                        "name": "Senyang Zhang"
                    },
                    {
                        "authorId": "143946810",
                        "name": "Bo Zhao"
                    },
                    {
                        "authorId": "2140448089",
                        "name": "Dacheng Tao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Different from generating synthesized images solely from the generator in ZSKT and DFKD, ADI [11] proposes to synthesize images by directly training the noises to images without the generator, and leverage the batch norm statistics recorded in the teacher network to better mimic the real images.",
                "ZSKT [10] takes the student network into image synthesis and proposes the adversarial DFKD framework.",
                "ZSKT [10] extends the basic DFKD framework to the adversarial DFKD framework by making the prediction mismatch between the teacher and student.",
                "It also establishes the basic framework of DFKD. ZSKT [10] takes the student network into image synthesis and proposes the adversarial DFKD framework.",
                "We compare IFHE with advanced DFKD methods including DAFL [9], ZSKT [10], ADI [11], DFQ [22] and CMI [12]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "e593eee789c3ccf8bf36686e97f0964d6b479f0b",
                "externalIds": {
                    "DBLP": "conf/dac/ChenLRYL23",
                    "DOI": "10.1109/DAC56929.2023.10247717",
                    "CorpusId": 261900768
                },
                "corpusId": 261900768,
                "publicationVenue": {
                    "id": "021b37d3-cef1-4c12-a442-257f7900c23d",
                    "name": "Design Automation Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Des Autom Conf",
                        "DAC"
                    ],
                    "url": "http://www.dac.com/"
                },
                "url": "https://www.semanticscholar.org/paper/e593eee789c3ccf8bf36686e97f0964d6b479f0b",
                "title": "IFHE: Intermediate-Feature Heterogeneity Enhancement for Image Synthesis in Data-Free Knowledge Distillation",
                "abstract": "Data-free knowledge distillation (DFKD) explores training a compact student network only by a pre-trained teacher without real data. Prevailing DFKD methods mainly consist of image synthesis and knowledge distillation. The synthesized images are crucial to enhance the student network performance. However, the images synthesized by existing methods cause high homogeneity on intermediate features, incurring undesired distillation performance. To address this problem, we propose the Intermediate-Feature Heterogeneity Enhancement (IFHE) method, which effectively enhances the heterogeneity of synthesized images by minimizing the loss between intermediate features and pre-set labels of the synthesized images Our IFHE outperforms the SOTA results on CIFAR-10/100 datasets of representative networks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2241548934",
                        "name": "Yi Chen"
                    },
                    {
                        "authorId": "2241628565",
                        "name": "Ning Liu"
                    },
                    {
                        "authorId": "2077431779",
                        "name": "Ao Ren"
                    },
                    {
                        "authorId": "2241992955",
                        "name": "Tao Yang"
                    },
                    {
                        "authorId": "2239384399",
                        "name": "Duo Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DFKD methods generate hard samples on which the student disagree with the teacher by enlarging the divergence between their prediction distribution [11], [14]\u2013[16] (see Fig.",
                "The first one is traditional adversarial manner as the previous work [11], [14]\u2013[16], whose adversarial loss is to calculate the divergence between predictions of the teacher and student.",
                "Different from the traditional adversarial objective [11], [14]\u2013[16], we use the student model itself rather than the pre-trained teacher model to estimate the sample difficulty of the synthetic data (see Fig.",
                "DAFL [9] and ZSKT [14] are generator-based methods."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "3580d939d32a87de776d77c8ba6cae53047296b2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-04542",
                    "ArXiv": "2307.04542",
                    "DOI": "10.1109/ICME55011.2023.00312",
                    "CorpusId": 259501712
                },
                "corpusId": 259501712,
                "publicationVenue": {
                    "id": "975e1529-a14b-436f-ad7e-a09d8eabc50d",
                    "name": "IEEE International Conference on Multimedia and Expo",
                    "type": "conference",
                    "alternate_names": [
                        "ICME",
                        "Int Conf Multimedia Expo",
                        "IEEE Int Conf Multimedia Expo",
                        "International Conference on Multimedia and Expo"
                    ],
                    "issn": "1945-7871",
                    "alternate_issns": [
                        "2330-7927"
                    ],
                    "url": "http://csdl2.computer.org/persagen/ProceedingByFilter.jsp?filter=I",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conferences.jsp",
                        "http://www.wikicfp.com/cfp/program?id=1418"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3580d939d32a87de776d77c8ba6cae53047296b2",
                "title": "Customizing Synthetic Data for Data-Free Student Learning",
                "abstract": "Data-free knowledge distillation (DFKD) aims to obtain a lightweight student model without original training data. Existing works generally synthesize data from the pretrained teacher model to replace the original training data for student learning. To more effectively train the student model, the synthetic data shall be customized to the current student learning ability. However, this is ignored in the existing DFKD methods and thus negatively affects the student training. To address this issue, we propose Customizing Synthetic Data for Data-Free Student Learning (CSD) in this paper, which achieves adaptive data synthesis using a self-supervised augmented auxiliary task to estimate the student learning ability. That is, data synthesis is dynamically adjusted to enlarge the cross entropy between the labels and the predictions from the self-supervised augmented task, thus generating the hard samples for the student model. The experiments on various datasets and teacher-student models show the effectiveness of our proposed method. Code is available at: https://github.com/luoshiya/CSD",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "91557723",
                        "name": "Shiya Luo"
                    },
                    {
                        "authorId": "1684692",
                        "name": "Defang Chen"
                    },
                    {
                        "authorId": "2144350104",
                        "name": "Can Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, many methods further exploited the teacherstudent alignment of intermediate feature maps [18], instance relational graphs [64], similarity attention maps [65], [66] and generative adversarial predictions [67]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b97d4cfef7c813ba96ea2c150bc9732a6c694b21",
                "externalIds": {
                    "ArXiv": "2307.00198",
                    "DBLP": "journals/corr/abs-2307-00198",
                    "DOI": "10.48550/arXiv.2307.00198",
                    "CorpusId": 259316384
                },
                "corpusId": 259316384,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b97d4cfef7c813ba96ea2c150bc9732a6c694b21",
                "title": "Filter Pruning for Efficient CNNs via Knowledge-driven Differential Filter Sampler",
                "abstract": "Filter pruning simultaneously accelerates the computation and reduces the memory overhead of CNNs, which can be effectively applied to edge devices and cloud services. In this paper, we propose a novel Knowledge-driven Differential Filter Sampler~(KDFS) with Masked Filter Modeling~(MFM) framework for filter pruning, which globally prunes the redundant filters based on the prior knowledge of a pre-trained model in a differential and non-alternative optimization. Specifically, we design a differential sampler with learnable sampling parameters to build a binary mask vector for each layer, determining whether the corresponding filters are redundant. To learn the mask, we introduce masked filter modeling to construct PCA-like knowledge by aligning the intermediate features from the pre-trained teacher model and the outputs of the student decoder taking sampling features as the input. The mask and sampler are directly optimized by the Gumbel-Softmax Straight-Through Gradient Estimator in an end-to-end manner in combination with global pruning constraint, MFM reconstruction error, and dark knowledge. Extensive experiments demonstrate the proposed KDFS's effectiveness in compressing the base models on various datasets. For instance, the pruned ResNet-50 on ImageNet achieves $55.36\\%$ computation reduction, and $42.86\\%$ parameter reduction, while only dropping $0.35\\%$ Top-1 accuracy, significantly outperforming the state-of-the-art methods. The code is available at \\url{https://github.com/Osilly/KDFS}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3431378",
                        "name": "Shaohui Lin"
                    },
                    {
                        "authorId": "2158104876",
                        "name": "Wenxuan Huang"
                    },
                    {
                        "authorId": "2212036699",
                        "name": "Jiao Xie"
                    },
                    {
                        "authorId": "1740430",
                        "name": "Baochang Zhang"
                    },
                    {
                        "authorId": "1862967",
                        "name": "Yunhang Shen"
                    },
                    {
                        "authorId": "144007938",
                        "name": "Zhou Yu"
                    },
                    {
                        "authorId": "151481622",
                        "name": "Jungong Han"
                    },
                    {
                        "authorId": "48471936",
                        "name": "D. Doermann"
                    }
                ]
            }
        },
        {
            "contexts": [
                "2021), henceforth DFME, and MAZE (Kariyappa, Prakash, and Qureshi 2021) adapt techniques used in knowledge distillation (Fang et al. 2019; Micaelli and Storkey 2019) to generate synthetic data to train clone models for model extraction.",
                "Data-Free Model Extraction (Truong et al. 2021), henceforth DFME, and MAZE (Kariyappa, Prakash, and Qureshi 2021) adapt techniques used in knowledge distillation (Fang et al. 2019; Micaelli and Storkey 2019) to generate synthetic data to train clone models for model extraction.",
                "Other techniques utilize a GAN for training (Micaelli and Storkey 2019; Fang et al. 2019; Chen et al. 2019)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "406d4e8d2df6f6b58e65016fea31004f781d93e7",
                "externalIds": {
                    "DBLP": "conf/aaai/RosenthalEP023",
                    "DOI": "10.1609/aaai.v37i8.26150",
                    "CorpusId": 254945311
                },
                "corpusId": 254945311,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/406d4e8d2df6f6b58e65016fea31004f781d93e7",
                "title": "DisGUIDE: Disagreement-Guided Data-Free Model Extraction",
                "abstract": "Recent model-extraction attacks on Machine Learning as a Service (MLaaS) systems have moved towards data-free approaches, showing the feasibility of stealing models trained with difficult-to-access data. However, these attacks are ineffective or limited due to the low accuracy of extracted models and the high number of queries to the models under attack. The high query cost makes such techniques infeasible for online MLaaS systems that charge per query.\nWe create a novel approach to get higher accuracy and query efficiency than prior data-free model extraction techniques. Specifically, we introduce a novel generator training scheme that maximizes the disagreement loss between two clone models that attempt to copy the model under attack. This loss, combined with diversity loss and experience replay, enables the generator to produce better instances to train the clone models. Our evaluation on popular datasets CIFAR-10 and CIFAR-100 shows that our approach improves the final model accuracy by up to 3.42% and 18.48% respectively. The average number of queries required to achieve the accuracy of the prior state of the art is reduced by up to 64.95%. We hope this will promote future work on feasible data-free model extraction and defenses against such attacks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2061113229",
                        "name": "Jonathan Rosenthal"
                    },
                    {
                        "authorId": "2184923665",
                        "name": "Eric Enouen"
                    },
                    {
                        "authorId": "49976542",
                        "name": "H. Pham"
                    },
                    {
                        "authorId": "2106349652",
                        "name": "Lin Tan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "2019), similarity attention maps (Ji, Heo, and Park 2021) and generative adversarial predictions (Micaelli and Storkey 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cbd86a65d38affa84976dbb968d89c58668c3bee",
                "externalIds": {
                    "DBLP": "conf/aaai/GongL0SLQ0LYM23",
                    "DOI": "10.1609/aaai.v37i6.25937",
                    "CorpusId": 259676005
                },
                "corpusId": 259676005,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/cbd86a65d38affa84976dbb968d89c58668c3bee",
                "title": "Adaptive Hierarchy-Branch Fusion for Online Knowledge Distillation",
                "abstract": "Online Knowledge Distillation (OKD) is designed to alleviate the dilemma that the high-capacity pre-trained teacher model is not available. However, the existing methods mostly focus on improving the ensemble prediction accuracy from multiple students (a.k.a. branches), which often overlook the homogenization problem that makes student model saturate quickly and hurts the performance. We assume that the intrinsic bottleneck of the homogenization problem comes from the identical branch architecture and coarse ensemble strategy. We propose a novel Adaptive Hierarchy-Branch Fusion framework for Online Knowledge Distillation, termed AHBF-OKD, which designs hierarchical branches and adaptive hierarchy-branch fusion module to boost the model diversity and aggregate complementary knowledge. Specifically, we first introduce hierarchical branch architectures to construct diverse peers by increasing the depth of branches monotonously on the basis of target branch. To effectively transfer knowledge from the most complex branch to the simplest target branch, we propose an adaptive hierarchy-branch fusion module to create hierarchical teacher assistants recursively, which regards the target branch as the smallest teacher assistant. During the training, the teacher assistant from the previous hierarchy is explicitly distilled by the teacher assistant and the branch from the current hierarchy. Thus, the important scores to different branches are effectively and adaptively allocated to reduce the branch homogenization. Extensive experiments demonstrate the effectiveness of AHBF-OKD on different datasets, including CIFAR-10/100 and ImageNet 2012. For example, on ImageNet 2012, the distilled ResNet-18 achieves Top-1 error of 29.28\\%, which significantly outperforms the state-of-the-art methods. The source code is available at https://github.com/linruigong965/AHBF.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2183509044",
                        "name": "Linrui Gong"
                    },
                    {
                        "authorId": "3431378",
                        "name": "Shaohui Lin"
                    },
                    {
                        "authorId": "1740430",
                        "name": "Baochang Zhang"
                    },
                    {
                        "authorId": "1862967",
                        "name": "Yunhang Shen"
                    },
                    {
                        "authorId": "144954689",
                        "name": "Ke Li"
                    },
                    {
                        "authorId": "2835750",
                        "name": "Ruizhi Qiao"
                    },
                    {
                        "authorId": "1999889514",
                        "name": "Bohan Ren"
                    },
                    {
                        "authorId": "152303295",
                        "name": "Muqing Li"
                    },
                    {
                        "authorId": "144007938",
                        "name": "Zhou Yu"
                    },
                    {
                        "authorId": "2149343311",
                        "name": "Lizhuang Ma"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[86] optimized an adversarial generator to search for difficult images and then used these images to train the student.",
                "Beyond the final output, the target data can be generated using the information from the teacher\u2019s feature representations [86,90]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "be20d0da24aaaebbc4340497a2aadd5813e17a39",
                "externalIds": {
                    "ArXiv": "2306.10687",
                    "DBLP": "journals/corr/abs-2306-10687",
                    "DOI": "10.48550/arXiv.2306.10687",
                    "CorpusId": 259203401
                },
                "corpusId": 259203401,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/be20d0da24aaaebbc4340497a2aadd5813e17a39",
                "title": "Categories of Response-Based, Feature-Based, and Relation-Based Knowledge Distillation",
                "abstract": "Deep neural networks have achieved remarkable performance for artificial intelligence tasks. The success behind intelligent systems often relies on large-scale models with high computational complexity and storage costs. The over-parameterized networks are often easy to optimize and can achieve better performance. However, it is challenging to deploy them over resource-limited edge-devices. Knowledge Distillation (KD) aims to optimize a lightweight network from the perspective of over-parameterized training. The traditional offline KD transfers knowledge from a cumbersome teacher to a small and fast student network. When a sizeable pre-trained teacher network is unavailable, online KD can improve a group of models by collaborative or mutual learning. Without needing extra models, Self-KD boosts the network itself using attached auxiliary architectures. KD mainly involves knowledge extraction and distillation strategies these two aspects. Beyond KD schemes, various KD algorithms are widely used in practical applications, such as multi-teacher KD, cross-modal KD, attention-based KD, data-free KD and adversarial KD. This paper provides a comprehensive KD survey, including knowledge categories, distillation schemes and algorithms, as well as some empirical studies on performance comparison. Finally, we discuss the open challenges of existing KD works and prospect the future directions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "102756770",
                        "name": "Chuanguang Yang"
                    },
                    {
                        "authorId": "2220327648",
                        "name": "Xinqiang Yu"
                    },
                    {
                        "authorId": "2127813",
                        "name": "Zhulin An"
                    },
                    {
                        "authorId": "2146648122",
                        "name": "Yongjun Xu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, [25] finds that arbitrary transfer sets can be useful, and work such as [4] and [19] have found success in performing knowledge distillation using samples out of the distribution of the teacher model."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fe43c57a9c6b33e71e958390cd27a07d4d116a0c",
                "externalIds": {
                    "ArXiv": "2306.09306",
                    "DBLP": "journals/corr/abs-2306-09306",
                    "DOI": "10.48550/arXiv.2306.09306",
                    "CorpusId": 259165330
                },
                "corpusId": 259165330,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fe43c57a9c6b33e71e958390cd27a07d4d116a0c",
                "title": "Propagating Knowledge Updates to LMs Through Distillation",
                "abstract": "Modern language models have the capacity to store and use immense amounts of knowledge about real-world entities, but it remains unclear how to update their implicit\"knowledge bases.'' While prior methods for updating knowledge in LMs successfully inject facts, updated LMs then fail to make inferences based on these injected facts. In this work, we demonstrate that a context distillation-based approach can both impart knowledge about entities and propagate that knowledge to enable broader inferences. Our approach consists of two stages: transfer set generation and distillation on the transfer set. We first generate a transfer set by simply prompting a language model to generate a continuation from the entity definition. Then, we update the model parameters so that the distribution of the LM (the student) matches the distribution of the LM conditioned on the definition (the teacher) on the transfer set. Our experiments demonstrate that this approach is more effective in propagating knowledge updates compared to fine-tuning and other gradient-based knowledge-editing methods without compromising performance in other contexts, even when injecting the definitions of up to 150 entities at once.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2204461780",
                        "name": "Shankar Padmanabhan"
                    },
                    {
                        "authorId": "115412405",
                        "name": "Yasumasa Onoe"
                    },
                    {
                        "authorId": "2129403254",
                        "name": "Michael J.Q. Zhang"
                    },
                    {
                        "authorId": "1814094",
                        "name": "Greg Durrett"
                    },
                    {
                        "authorId": "2890423",
                        "name": "Eunsol Choi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some methods generate pseudo samples via adversarial training [9], [13]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6d101b76c4bb1e5e2aeef44afe2d9b29a4e1cffd",
                "externalIds": {
                    "DBLP": "conf/bmsb/QiX23",
                    "DOI": "10.1109/BMSB58369.2023.10211458",
                    "CorpusId": 260934192
                },
                "corpusId": 260934192,
                "publicationVenue": {
                    "id": "6813b010-0f27-487d-afd0-c3d67cd5654f",
                    "name": "IEEE international Symposium on Broadband Multimedia Systems and Broadcasting",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE int Symp Broadband Multimedia Syst Broadcast",
                        "International Symposium on Broadband Multimedia Systems and Broadcasting",
                        "BMSB",
                        "Int Symp Broadband Multimedia Syst Broadcast"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6d101b76c4bb1e5e2aeef44afe2d9b29a4e1cffd",
                "title": "Data-free Distillation with Evolutionary Pseudo Data for Text Classification",
                "abstract": "Most knowledge distillation methods make a strong assumption that the training datasets for downstream tasks are available. However, due to the privacy issue and transmission concerns, such an assumption is sometimes impractical in realworld scenarios, which motivates the research of data-free knowledge distillation. However, few studies have investigated the efficient approaches for data-free knowledge distillation for NLP in any systematic way. To mitigate this, our paper first establishes two guidelines that contribute to data-free distillation efficiency. Motivated by the guidelines, we then develop an evolutionbased approach to construct pseudo samples and incorporate the approach with knowledge titillation, resulting a novel framework Evo-Distil. Experiments demonstrate that our proposed data-free distillation method is superior to the existing strong baselines and rivals the distillation methods with real training data. Specifically, it achieves 90%-99% performance of the regular distillation models with real training data, relying on no assumption of access to annotated training data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2231808369",
                        "name": "Zongshuai Qi"
                    },
                    {
                        "authorId": "2213492489",
                        "name": "Yiyang Xiong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We use ZSKT (Micaelli & Storkey, 2019), CMI (Fang et al., 2021), and OOD (Asano & Saeed, 2021) as the baseline distillation methods.",
                "We evaluated one vanilla KD using clean training data (Hinton et al., 2015) and three training-datafree KD method which use synthetic data (ZSKT (Micaelli & Storkey, 2019) & CMI (Fang et al., 2021)) or\nout-of-distribution (OOD) data as surrogate distillation data (Asano & Saeed, 2021).",
                "The key difference between data-free KD and vanilla KD is that the samples used for KD are synthetic (Chen et al., 2019; Micaelli & Storkey, 2019) or sampled from out-of-distribution (OOD) domains (Asano & Saeed, 2021).",
                "Following the setup of ZSKT (Micaelli & Storkey, 2019), we use WideResNet (Zagoruyko & Komodakis, 2016) for training 10-way or 43-way classifiers on CIFAR10 and GTSR-B, respectively.",
                "Here, representative implementations include the first adversarial data-free distillation, Zero-Shot Knowledge Transfer (ZSKT) (Micaelli & Storkey, 2019), the state-of-theart data-free KD methods, CMI (Fang et al., 2021)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "9add9e7f6fe00a14672364f4080714025cdfadf7",
                "externalIds": {
                    "ArXiv": "2306.02368",
                    "DBLP": "conf/icml/HongZYLJZ23",
                    "DOI": "10.48550/arXiv.2306.02368",
                    "CorpusId": 259075567
                },
                "corpusId": 259075567,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9add9e7f6fe00a14672364f4080714025cdfadf7",
                "title": "Revisiting Data-Free Knowledge Distillation with Poisoned Teachers",
                "abstract": "Data-free knowledge distillation (KD) helps transfer knowledge from a pre-trained model (known as the teacher model) to a smaller model (known as the student model) without access to the original training data used for training the teacher model. However, the security of the synthetic or out-of-distribution (OOD) data required in data-free KD is largely unknown and under-explored. In this work, we make the first effort to uncover the security risk of data-free KD w.r.t. untrusted pre-trained models. We then propose Anti-Backdoor Data-Free KD (ABD), the first plug-in defensive method for data-free KD methods to mitigate the chance of potential backdoors being transferred. We empirically evaluate the effectiveness of our proposed ABD in diminishing transferred backdoor knowledge while maintaining compatible downstream performances as the vanilla KD. We envision this work as a milestone for alarming and mitigating the potential backdoors in data-free KD. Codes are released at https://github.com/illidanlab/ABD.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2110805917",
                        "name": "Junyuan Hong"
                    },
                    {
                        "authorId": "2111107467",
                        "name": "Yijun Zeng"
                    },
                    {
                        "authorId": "2116618893",
                        "name": "Shuyang Yu"
                    },
                    {
                        "authorId": "3366777",
                        "name": "L. Lyu"
                    },
                    {
                        "authorId": "39823639",
                        "name": "R. Jia"
                    },
                    {
                        "authorId": "2143462929",
                        "name": "Jiayu Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DFKD can be leveraged successfully in FDG, wherein the student model is trained using multiple teachers that have been trained on diverse source domains, facilitating the generalization capability to previously unseen target domains.",
                "Knowledge distillation: Data-free knowledge distillation (DFKD) [113], [156], [157] is a technique used to transfer knowledge from a large, well-trained model to a smaller, more compact model without the need for labeled training data.",
                "(DFKD) [113], [156], [157] is a technique used to transfer knowledge from a large, well-trained model to a smaller, more compact model without the need for labeled training data.",
                "[113] developed a training approach in which a student network learns to align its predictions with those of a teacher network, solely relying on an adversarial generator to discover images where the student exhibits poor alignment with the teacher and utilizing them for student training, without relying"
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "5d73bd138049d50de55ae53232768b925071f320",
                "externalIds": {
                    "ArXiv": "2306.01334",
                    "DBLP": "journals/corr/abs-2306-01334",
                    "DOI": "10.48550/arXiv.2306.01334",
                    "CorpusId": 259063682
                },
                "corpusId": 259063682,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5d73bd138049d50de55ae53232768b925071f320",
                "title": "Federated Domain Generalization: A Survey",
                "abstract": "Machine learning typically relies on the assumption that training and testing distributions are identical and that data is centrally stored for training and testing. However, in real-world scenarios, distributions may differ significantly and data is often distributed across different devices, organizations, or edge nodes. Consequently, it is imperative to develop models that can effectively generalize to unseen distributions where data is distributed across different domains. In response to this challenge, there has been a surge of interest in federated domain generalization (FDG) in recent years. FDG combines the strengths of federated learning (FL) and domain generalization (DG) techniques to enable multiple source domains to collaboratively learn a model capable of directly generalizing to unseen domains while preserving data privacy. However, generalizing the federated model under domain shifts is a technically challenging problem that has received scant attention in the research area so far. This paper presents the first survey of recent advances in this area. Initially, we discuss the development process from traditional machine learning to domain adaptation and domain generalization, leading to FDG as well as provide the corresponding formal definition. Then, we categorize recent methodologies into four classes: federated domain alignment, data manipulation, learning strategies, and aggregation optimization, and present suitable algorithms in detail for each category. Next, we introduce commonly used datasets, applications, evaluations, and benchmarks. Finally, we conclude this survey by providing some potential research topics for the future.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2153643375",
                        "name": "Ying Li"
                    },
                    {
                        "authorId": "2157203726",
                        "name": "Xingwei Wang"
                    },
                    {
                        "authorId": "2771693",
                        "name": "Rongfei Zeng"
                    },
                    {
                        "authorId": "147786874",
                        "name": "Praveen Kumar Donta"
                    },
                    {
                        "authorId": "50857417",
                        "name": "Ilir Murturi"
                    },
                    {
                        "authorId": "2152600128",
                        "name": "Min Huang"
                    },
                    {
                        "authorId": "1691109",
                        "name": "S. Dustdar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For data-free knowledge distillation [46, 47], generators are optimized to maximize the divergence between teacher and student predictions."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c65feaf2681a5676204a43101edff4d897d37ffb",
                "externalIds": {
                    "DOI": "10.1007/s11280-023-01150-6",
                    "CorpusId": 259053994
                },
                "corpusId": 259053994,
                "publicationVenue": {
                    "id": "9cf86359-a093-42b4-8bc0-6fd56465c360",
                    "name": "World wide web (Bussum)",
                    "type": "journal",
                    "alternate_names": [
                        "World Wide Web",
                        "World wide web (bussum"
                    ],
                    "issn": "1386-145X",
                    "url": "https://link.springer.com/journal/11280"
                },
                "url": "https://www.semanticscholar.org/paper/c65feaf2681a5676204a43101edff4d897d37ffb",
                "title": "Explanation-based data-free model extraction attacks",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "119725888",
                        "name": "Anli Yan"
                    },
                    {
                        "authorId": "151185787",
                        "name": "Ruitao Hou"
                    },
                    {
                        "authorId": "3188142",
                        "name": "Hongyang Yan"
                    },
                    {
                        "authorId": "2109052595",
                        "name": "Xiaozhang Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Unlike previous DFKD methods, which store synthetic images [7, 10, 13, 27, 29, 43], we use a feature pool to store the hidden features of the synthetic images when optimizing G and use them to improve the diversity of later training image generation.",
                "Recently, data-free knowledge distillation (DFKD) [4,7,10,13,14,22,25,27,29,43,46] seeks to perform KD by generating synthetic data instead of accessing the original training data used by the teacher network to train the student network.",
                "Since the existing complete opensource SOTA methods do not report results on ImageNet or\n*https://github.com/fastai/imagenette \u2020https : / / www . kaggle . com / datasets / ambityga /\nimagenet100\nits subsets in their papers, we use two open-sourced methods CMI and ZSKT for comparison.",
                "As shown in Table 2, when evaluated on high-resolution datasets, CMI and ZSKT-trained student networks perform significantly worse than the teacher network, while the student network trained by our method only has a small performance gap with the teacher network.",
                "Some KD work [16, 18, 29, 32, 49] differs from previous KD via logit distillation, in that they impose consistency constraints on the middle layer features of the network.",
                ", methods based on generative networks [8, 9, 12, 13, 27, 29, 45].",
                ", DAFL [7], ZSKT [29], ADI [43], DFQ [10], LS-GDFD [27], and CMI [13]) when using the same teacher network.",
                "Table 1 shows the DFKD results by our method and several state-of-the-art (SOTA) methods, i.e., DAFL [7], ZSKT [29], ADI [43], DFQ [10], LS-GDFD [27], and CMI [13]) when using the same teacher network.",
                "DAFL [7] ZSKT [29] ADI [43] DFQ [10] LS-GDFD [27] CMI [13] SpaceshipNet",
                ", non-conditional generative network-based methods [8,9,12,13,29] and conditional generative network-based methods [27, 45].",
                "Synthetic data generation methods in DFKD mainly consist of noise image optimization-based methods [4, 26, 31, 44] and generative network-based methods [8,9,12,13,27,29,45].",
                "Next, we compare our mSARC with the feature-level constraint used in several feature-level constraints used in recent KD methods [16, 18, 29, 32, 49]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "868b7d1e537e7b4a01aa4d84d1efadedb9c0c879",
                "externalIds": {
                    "DBLP": "conf/cvpr/YuC0J23",
                    "DOI": "10.1109/CVPR52729.2023.02324",
                    "CorpusId": 260869955
                },
                "corpusId": 260869955,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/868b7d1e537e7b4a01aa4d84d1efadedb9c0c879",
                "title": "Data-Free Knowledge Distillation via Feature Exchange and Activation Region Constraint",
                "abstract": "Despite the tremendous progress on data-free knowledge distillation (DFKD) based on synthetic data generation, there are still limitations in diverse and efficient data synthesis. It is naive to expect that a simple combination of generative network-based data synthesis and data augmentation will solve these issues. Therefore, this paper proposes a novel data-free knowledge distillation method (Spaceship-Net) based on channel-wise feature exchange (CFE) and multi-scale spatial activation region consistency (mSARC) constraint. Specifically, CFE allows our generative network to better sample from the feature space and efficiently synthesize diverse images for learning the student network. However, using CFE alone can severely amplify the unwanted noises in the synthesized images, which may result in failure to improve distillation learning and even have negative effects. Therefore, we propose mSARC to assure the student network can imitate not only the logit output but also the spatial activation region of the teacher network in order to alleviate the influence of unwanted noises in diverse synthetic images on distillation learning. Extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet, Imagenette, and ImageNet100 show that our method can work well with different backbone networks, and outperform the state-of-the-art DFKD methods. Code will be available at: https://github.com/skgyu/Spaceship-Net.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2112273513",
                        "name": "Shikang Yu"
                    },
                    {
                        "authorId": null,
                        "name": "Jiachen Chen"
                    },
                    {
                        "authorId": "2112487923",
                        "name": "H. Han"
                    },
                    {
                        "authorId": "1696610",
                        "name": "Shuqiang Jiang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Considering that the proxy dataset may not always exist, some recent works proposed distillation in a data-free manner including reconstructing samples used for training the teacher [20,23] or learning a generator [35]."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "624fc6f366a513fcc3b95e825ad7be66580107b3",
                "externalIds": {
                    "DBLP": "conf/cvpr/WangLX0ZZ23",
                    "DOI": "10.1109/CVPR52729.2023.01955",
                    "CorpusId": 260067820
                },
                "corpusId": 260067820,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/624fc6f366a513fcc3b95e825ad7be66580107b3",
                "title": "DaFKD: Domain-aware Federated Knowledge Distillation",
                "abstract": "Federated Distillation (FD) has recently attracted increasing attention for its efficiency in aggregating multiple diverse local models trained from statistically heterogeneous data of distributed clients. Existing FD methods generally treat these models equally by merely computing the average of their output soft predictions for some given input distillation sample, which does not take the diversity across all local models into account, thus leading to degraded performance of the aggregated model, especially when some local models learn little knowledge about the sample. In this paper, we propose a new perspective that treats the local data in each client as a specific domain and design a novel domain knowledge aware federated distillation method, dubbed DaFKD, that can discern the importance of each model to the distillation sample, and thus is able to optimize the ensemble of soft predictions from diverse models. Specifically, we employ a domain discriminator for each client, which is trained to identify the correlation factor between the sample and the corresponding domain. Then, to facilitate the training of the domain discriminator while saving communication costs, we propose sharing its partial parameters with the classification model. Extensive experiments on various datasets and settings show that the proposed method can improve the model accuracy by up to 6.02% compared to state-of-the-art baselines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51175126",
                        "name": "Haozhao Wang"
                    },
                    {
                        "authorId": "1798489",
                        "name": "Yichen Li"
                    },
                    {
                        "authorId": "50232004",
                        "name": "Wenchao Xu"
                    },
                    {
                        "authorId": "145765726",
                        "name": "Rui Li"
                    },
                    {
                        "authorId": "3311244",
                        "name": "Yufeng Zhan"
                    },
                    {
                        "authorId": "2224377786",
                        "name": "Zhigang Zeng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Inspired by adversarial knowledge distillation (AKD), which aims to iteratively improve the student model\u2019s performance by forcing it to learn from generated hard samples (Fang et al., 2019; Micaelli and Storkey, 2019a; Heo et al., 2019), we propose an adversarial framework for distilling a proprietary LLM into a compact student model.",
                "This innovative use of AKD in LLMs underscores the transformative potential of iterative knowledge transfer.",
                "To verify the efficiency and efficacy of our method, we apply our AKD framework to transfer the knowledge of ChatGPT 2 onto an open-source foundation LLM, known as LLaMA (Touvron et al., 2023), consisting of 7 billion parameters.",
                "This paper presents an innovative adversarial knowledge distillation (AKD) framework for distilling a proprietary large language model (LLM) into a compact, open-source student model.",
                "We select Alpaca\u2019s training data (generated from only 175 manually selected seed instructions) as the initial training instructions and execute three iterations of AKD, resulting in a total of 70K data that our model is trained on. we\u2019ve christened our model as Lion, drawing inspiration from the art of \u201cdistillation\u201d.",
                "Inspired by the success of adversarial knowledge distillation (AKD) (Fang et al., 2019; Micaelli and Storkey, 2019a; Heo et al., 2019), we turn to optimize an upper bound of the expectation \u2014the expectation of the model discrepancy on \u201chard samples\u201d, where the teacher T and the student S have a relatively large performance gap.",
                "Nevertheless, these AKD methodologies necessitate accessibility to the weights or gradients of the teacher model, which can not be directly adapted to our setting."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "4b23dfd94666dbb505b7ffca148ffe1c439f80b2",
                "externalIds": {
                    "ArXiv": "2305.12870",
                    "DBLP": "journals/corr/abs-2305-12870",
                    "DOI": "10.48550/arXiv.2305.12870",
                    "CorpusId": 258833333
                },
                "corpusId": 258833333,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4b23dfd94666dbb505b7ffca148ffe1c439f80b2",
                "title": "Lion: Adversarial Distillation of Closed-Source Large Language Model",
                "abstract": "The practice of transferring knowledge from a sophisticated, closed-source large language model (LLM) to a compact, open-source LLM has garnered considerable attention. Previous works have focused on a unidirectional knowledge distillation way by aligning the responses of the student model with those of the teacher model to a set of instructions. Nevertheless, they overlooked the possibility of incorporating any reciprocal\"feedback\"--identifying challenging instructions where the student model's performance falls short--to boost the student model's proficiency iteratively. To this end, we propose a novel adversarial distillation framework for a more efficient knowledge transfer. Leveraging the versatile role adaptability of LLMs, we prompt the closed-source model to identify\"hard\"instructions and generate new\"hard\"instructions for the student model, creating a three-stage adversarial loop of imitation, discrimination, and generation. By applying this adversarial framework, we successfully transfer knowledge from ChatGPT to a 7B student model (named Lion), achieving nearly 95% capability approximation using a mere 70k training data. We aspire that this proposed model may serve as the baseline to reflect the performance of ChatGPT, especially the open-source instruction-following language model baseline for our community.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2116150120",
                        "name": "Yuxin Jiang"
                    },
                    {
                        "authorId": "2216598559",
                        "name": "Chunkit Chan"
                    },
                    {
                        "authorId": "2134563447",
                        "name": "Mingyang Chen"
                    },
                    {
                        "authorId": "2158629711",
                        "name": "Wei Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Michaeli and Storkey [60] also proposed a novel method to transfer knowledge to students using a generative adversarial network for a zero-shot knowledge transfer."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8194076afd89218ce65a846ce0872ee29c042125",
                "externalIds": {
                    "PubMedCentral": "10184906",
                    "DOI": "10.1371/journal.pone.0285668",
                    "CorpusId": 258714078,
                    "PubMed": "37186614"
                },
                "corpusId": 258714078,
                "publicationVenue": {
                    "id": "0aed7a40-85f3-4c66-9e1b-c1556c57001b",
                    "name": "PLoS ONE",
                    "type": "journal",
                    "alternate_names": [
                        "Plo ONE",
                        "PLOS ONE",
                        "PLO ONE"
                    ],
                    "issn": "1932-6203",
                    "url": "https://journals.plos.org/plosone/",
                    "alternate_urls": [
                        "http://www.plosone.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8194076afd89218ce65a846ce0872ee29c042125",
                "title": "Mitigating carbon footprint for knowledge distillation based deep learning model compression",
                "abstract": "Deep learning techniques have recently demonstrated remarkable success in numerous domains. Typically, the success of these deep learning models is measured in terms of performance metrics such as accuracy and mean average precision (mAP). Generally, a model\u2019s high performance is highly valued, but it frequently comes at the expense of substantial energy costs and carbon footprint emissions during the model building step. Massive emission of CO2 has a deleterious impact on life on earth in general and is a serious ethical concern that is largely ignored in deep learning research. In this article, we mainly focus on environmental costs and the means of mitigating carbon footprints in deep learning models, with a particular focus on models created using knowledge distillation (KD). Deep learning models typically contain a large number of parameters, resulting in a \u2018heavy\u2019 model. A heavy model scores high on performance metrics but is incompatible with mobile and edge computing devices. Model compression techniques such as knowledge distillation enable the creation of lightweight, deployable models for these low-resource devices. KD generates lighter models and typically performs with slightly less accuracy than the heavier teacher model (model accuracy by the teacher model on CIFAR 10, CIFAR 100, and TinyImageNet is 95.04%, 76.03%, and 63.39%; model accuracy by KD is 91.78%, 69.7%, and 60.49%). Although the distillation process makes models deployable on low-resource devices, they were found to consume an exorbitant amount of energy and have a substantial carbon footprint (15.8, 17.9, and 13.5 times more carbon compared to the corresponding teacher model). The enormous environmental cost is primarily attributable to the tuning of the hyperparameter, Temperature (\u03c4). In this article, we propose measuring the environmental costs of deep learning work (in terms of GFLOPS in millions, energy consumption in kWh, and CO2 equivalent in grams). In order to create lightweight models with low environmental costs, we propose a straightforward yet effective method for selecting a hyperparameter (\u03c4) using a stochastic approach for each training batch fed into the models. We applied knowledge distillation (including its data-free variant) to problems involving image classification and object detection. To evaluate the robustness of our method, we ran experiments on various datasets (CIFAR 10, CIFAR 100, Tiny ImageNet, and PASCAL VOC) and models (ResNet18, MobileNetV2, Wrn-40-2). Our novel approach reduces the environmental costs by a large margin by eliminating the requirement of expensive hyperparameter tuning without sacrificing performance. Empirical results on the CIFAR 10 dataset show that the stochastic technique achieves an accuracy of 91.67%, whereas tuning achieves an accuracy of 91.78%\u2014however, the stochastic approach reduces the energy consumption and CO2 equivalent each by a factor of 19. Similar results have been obtained with CIFAR 100 and TinyImageNet dataset. This pattern is also observed in object detection classification on the PASCAL VOC dataset, where the tuning technique performs similarly to the stochastic technique, with a difference of 0.03% mAP favoring the stochastic technique while reducing the energy consumptions and CO2 emission each by a factor of 18.5.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2217415274",
                        "name": "Kazi Rafat"
                    },
                    {
                        "authorId": "2217311059",
                        "name": "Sadia Islam"
                    },
                    {
                        "authorId": "2217423103",
                        "name": "Abdullah Al Mahfug"
                    },
                    {
                        "authorId": "2110864495",
                        "name": "Md. Ismail Hossain"
                    },
                    {
                        "authorId": "2053079635",
                        "name": "Fuad Rahman"
                    },
                    {
                        "authorId": "2599479",
                        "name": "S. Momen"
                    },
                    {
                        "authorId": "51245102",
                        "name": "Shafin Rahman"
                    },
                    {
                        "authorId": "2146777033",
                        "name": "Nabeel Mohammed"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Current data-free KD methods can be roughly divided into two categories: adversarial training [77], [78], [79], which focuses on generating worstcase synthetic samples for student learning, and data prior matching [80], [81], [82], where synthetic samples are forced to satisfy priors like class prior, activation regularization, and batch normalization statistics."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c80a9eeb1e6804075ed3f09513b62331b08a96d2",
                "externalIds": {
                    "ArXiv": "2303.15361",
                    "DBLP": "journals/corr/abs-2303-15361",
                    "DOI": "10.48550/arXiv.2303.15361",
                    "CorpusId": 257767040
                },
                "corpusId": 257767040,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c80a9eeb1e6804075ed3f09513b62331b08a96d2",
                "title": "A Comprehensive Survey on Test-Time Adaptation under Distribution Shifts",
                "abstract": "Machine learning methods strive to acquire a robust model during training that can generalize well to test samples, even under distribution shifts. However, these methods often suffer from a performance drop due to unknown test distributions. Test-time adaptation (TTA), an emerging paradigm, has the potential to adapt a pre-trained model to unlabeled data during testing, before making predictions. Recent progress in this paradigm highlights the significant benefits of utilizing unlabeled data for training self-adapted models prior to inference. In this survey, we divide TTA into several distinct categories, namely, test-time (source-free) domain adaptation, test-time batch adaptation, online test-time adaptation, and test-time prior adaptation. For each category, we provide a comprehensive taxonomy of advanced algorithms, followed by a discussion of different learning scenarios. Furthermore, we analyze relevant applications of TTA and discuss open challenges and promising areas for future research. A comprehensive list of TTA methods can be found at \\url{https://github.com/tim-learn/awesome-test-time-adaptation}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "143932869",
                        "name": "Jian Liang"
                    },
                    {
                        "authorId": "2053865391",
                        "name": "R. He"
                    },
                    {
                        "authorId": "2192236453",
                        "name": "Tien-Ping Tan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similarly, ZSKT [29] only uses adversarial loss in the generation process to widen the prediction gap between teacher and student, ignoring the diversity of data.",
                "[29] introduce the method of adversarial generation.",
                "We compare our proposed method with different data-free generation methods, including: Dream [1], DeepInv [44], DAFL [4], ZSKT [29], DFQ [7], CMI [12], and Fast [11]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "28210bd914030c0f185e66c815afd3ca0b09448c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-11611",
                    "ArXiv": "2303.11611",
                    "DOI": "10.48550/arXiv.2303.11611",
                    "CorpusId": 257637125
                },
                "corpusId": 257637125,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/28210bd914030c0f185e66c815afd3ca0b09448c",
                "title": "Model Robustness Meets Data Privacy: Adversarial Robustness Distillation without Original Data",
                "abstract": "Large-scale deep learning models have achieved great performance based on large-scale datasets. Moreover, the existing Adversarial Training (AT) can further improve the robustness of these large models. However, these large models are difficult to deploy to mobile devices, and the effect of AT on small models is very limited. In addition, the data privacy issue (e.g., face data and diagnosis report) may lead to the original data being unavailable, which relies on data-free knowledge distillation technology for training. To tackle these issues, we propose a challenging novel task called Data-Free Adversarial Robustness Distillation (DFARD), which tries to train small, easily deployable, robust models without relying on the original data. We find the combination of existing techniques resulted in degraded model performance due to fixed training objectives and scarce information content. First, an interactive strategy is designed for more efficient knowledge transfer to find more suitable training objectives at each epoch. Then, we explore an adaptive balance method to suppress information loss and obtain more data information than previous methods. Experiments show that our method improves baseline performance on the novel task.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2115862616",
                        "name": "Yuzheng Wang"
                    },
                    {
                        "authorId": "2111606069",
                        "name": "Zhaoyu Chen"
                    },
                    {
                        "authorId": "2143920085",
                        "name": "Dingkang Yang"
                    },
                    {
                        "authorId": "2190969187",
                        "name": "Pinxue Guo"
                    },
                    {
                        "authorId": "2161719405",
                        "name": "Kaixun Jiang"
                    },
                    {
                        "authorId": "2108126306",
                        "name": "Wenqiang Zhang"
                    },
                    {
                        "authorId": "2861714",
                        "name": "Lizhe Qi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Metric-Agnostic Adversarial Estimation: Inspired by robust optimization [2], existing literature on data-free knowledge distillation [10,35] has shown that the efficiency of the distillation process can be significantly improved by identifying the samples which are the hardest for the student to classify.",
                "This restrains us from measuring the agreement between teachers and students in a straightforward way, thus preventing the direct application of state-of-the-art approaches from the DFKD literature like data-free adversarial distillation [10, 35].",
                "The standard approach for performing DFL is based on the principle of model inversion [33] \u2013 given a pre-trained model (teacher), the aim is to reconstruct its train set distribution by analyzing its activation patterns [8, 30, 35, 57].",
                "As previously observed in the data-free knowledge distillation literature [10, 35], training the encoder on such samples ensures its robustness to semantic variations."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "aad463d5e33b64f61608bc52e0e4ee41e2db9b82",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-07775",
                    "ArXiv": "2303.07775",
                    "DOI": "10.1109/CVPR52729.2023.01163",
                    "CorpusId": 257504829
                },
                "corpusId": 257504829,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/aad463d5e33b64f61608bc52e0e4ee41e2db9b82",
                "title": "Data-Free Sketch-Based Image Retrieval",
                "abstract": "Rising concerns about privacy and anonymity preservation of deep learning models have facilitated research in data-free learning (DFL). For the first time, we identify that for data-scarce tasks like Sketch-Based Image Retrieval (SBIR), where the difficulty in acquiring paired photos and hand-drawn sketches limits data-dependent cross-modal learning algorithms, DFL can prove to be a much more practical paradigm. We thus propose Data-Free (DF)-SBIR, where, unlike existing DFL problems, pre-trained, single-modality classification models have to be leveraged to learn a cross-modal metric-space for retrieval without access to any training data. The widespread availability of pre-trained classification models, along with the difficulty in acquiring paired photo-sketch datasets for SBIR justify the practicality of this setting. We present a methodology for DF-SBIR, which can leverage knowledge from models independently trained to perform classification on photos and sketches. We evaluate our model on the Sketchy, TU-Berlin, and QuickDraw benchmarks, designing a variety of baselines based on state-of-the-art DFL literature, and observe that our method surpasses all of them by significant margins. Our method also achieves mAPs competitive with data-dependent approaches, all the while requiring no training data. Implementation is available at https://github.com/abhrac/data-free-sbir.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "150008622",
                        "name": "Abhra Chaudhuri"
                    },
                    {
                        "authorId": "3046649",
                        "name": "A. Bhunia"
                    },
                    {
                        "authorId": "2115712433",
                        "name": "Yi-Zhe Song"
                    },
                    {
                        "authorId": "2149815080",
                        "name": "Anjan Dutta"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1ba8eb7938f0b8d078a9f8e9f7b49a2d0141abe3",
                "externalIds": {
                    "DBLP": "journals/bspc/BorupKPM23",
                    "DOI": "10.1016/j.bspc.2022.104496",
                    "CorpusId": 254805555
                },
                "corpusId": 254805555,
                "publicationVenue": {
                    "id": "1bac31b4-014a-4981-ae41-af2a40acc162",
                    "name": "Biomedical Signal Processing and Control",
                    "type": "journal",
                    "alternate_names": [
                        "Biomed Signal Process Control"
                    ],
                    "issn": "1746-8094",
                    "url": "https://www.journals.elsevier.com/biomedical-signal-processing-and-control",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/17468094"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1ba8eb7938f0b8d078a9f8e9f7b49a2d0141abe3",
                "title": "Automatic sleep scoring using patient-specific ensemble models and knowledge distillation for ear-EEG data",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2051550661",
                        "name": "Kenneth Borup"
                    },
                    {
                        "authorId": "1815835",
                        "name": "P. Kidmose"
                    },
                    {
                        "authorId": "8092167",
                        "name": "Huy P Phan"
                    },
                    {
                        "authorId": "3389543",
                        "name": "Kaare B. Mikkelsen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Student update strategies: (a) Typical student update by optimizing the Knowledge-Acquisition loss (LAcq) with the batch pseudo samples (x\u0302), produced by the generator (G) [5, 10, 20].",
                "Adversarial DFKD methods [10, 19, 20] investigate an adversarial exploration framework to seek pseudo-samples.",
                "Noise optimization [11,22,30] and generative reconstruction [4,10,20] are the two primary ways to replace the original training data used in the distillation process with synthetic or pseudo samples.",
                "ZSKT [20] attempts data-free knowledge transfer by first training a generator in an adversarial fashion to look for samples on which the student and teacher do not match well.",
                "ZSKDa [22] ADIb [30] CMIb [11] DeGANc [1] EATSKDc [21] KEGNETa [31] ZSKTb [20] DDADa [34] DAFLd [4] DFADd [10] DFQd [5] MB-DFKDd [3] PRE-DFKDd [2] Ours-1 Ours-2"
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "41d7f0fa740bb3a2f3342cb3e47cae8277654cf3",
                "externalIds": {
                    "ArXiv": "2302.14290",
                    "DBLP": "conf/cvpr/PatelMQ23",
                    "DOI": "10.1109/CVPR52729.2023.00752",
                    "CorpusId": 257233113
                },
                "corpusId": 257233113,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/41d7f0fa740bb3a2f3342cb3e47cae8277654cf3",
                "title": "Learning to Retain while Acquiring: Combating Distribution-Shift in Adversarial Data-Free Knowledge Distillation",
                "abstract": "Data-free Knowledge Distillation (DFKD) has gained popularity recently, with the fundamental idea of carrying out knowledge transfer from a Teacher neural network to a Student neural network in the absence of training data. However, in the Adversarial DFKD framework, the student network's accuracy, suffers due to the non-stationary distribution of the pseudo-samples under multiple generator updates. To this end, at every generator update, we aim to maintain the student's performance on previously encountered examples while acquiring knowledge from samples of the current distribution. Thus, we propose a meta-learning inspired framework by treating the task of Knowledge-Acquisition (learning from newly generated samples) and Knowledge-Retention (retaining knowledge on previously met samples) as meta-train and meta-test, respectively. Hence, we dub our method as Learning to Retain while Acquiring. Moreover, we identify an implicit aligning factor between the Knowledge-Retention and Knowledge-Acquisition tasks indicating that the proposed student update strategy enforces a common gradient direction for both tasks, alleviating interference between the two objectives. Finally, we support our hypothesis by exhibiting extensive evaluation and comparison of our method with prior arts on multiple datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2056853311",
                        "name": "Gaurav Patel"
                    },
                    {
                        "authorId": "2217000",
                        "name": "Konda Reddy Mopuri"
                    },
                    {
                        "authorId": "2077648",
                        "name": "Qiang Qiu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[20] use the generated samples that can confuse the discriminator to make student learning more efficient."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bee8fcfb269975c08690561c757dc46c04e422dd",
                "externalIds": {
                    "ArXiv": "2302.08771",
                    "DBLP": "journals/corr/abs-2302-08771",
                    "DOI": "10.48550/arXiv.2302.08771",
                    "CorpusId": 257019907
                },
                "corpusId": 257019907,
                "publicationVenue": {
                    "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
                    "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Acoust Speech Signal Process",
                        "IEEE Int Conf Acoust Speech Signal Process",
                        "ICASSP",
                        "International Conference on Acoustics, Speech, and Signal Processing"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
                },
                "url": "https://www.semanticscholar.org/paper/bee8fcfb269975c08690561c757dc46c04e422dd",
                "title": "Explicit and Implicit Knowledge Distillation via Unlabeled Data",
                "abstract": "Data-free knowledge distillation is a challenging model lightweight task for scenarios in which the original dataset is not available. Previous methods require a lot of extra computational costs to update one or more generators and their naive imitate-learning lead to lower distillation efficiency. Based on these observations, we first propose an efficient unlabeled sample selection method to replace high computational generators and focus on improving the training efficiency of the selected samples. Then, a class-dropping mechanism is designed to suppress the label noise caused by the data domain shifts. Finally, we propose a distillation method that incorporates explicit features and implicit structured relations to improve the effect of distillation. Experimental results show that our method can quickly converge and obtain higher accuracy than other state-of-the-art methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2115862616",
                        "name": "Yuzheng Wang"
                    },
                    {
                        "authorId": "2032815387",
                        "name": "Zuhao Ge"
                    },
                    {
                        "authorId": "2111606069",
                        "name": "Zhaoyu Chen"
                    },
                    {
                        "authorId": "2144227016",
                        "name": "Xiangjian Liu"
                    },
                    {
                        "authorId": "2112486439",
                        "name": "Chuang Ma"
                    },
                    {
                        "authorId": "2109125571",
                        "name": "Yunquan Sun"
                    },
                    {
                        "authorId": "2861714",
                        "name": "Lizhe Qi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2b71defcd83889711ef8768ab8b57f9e41e1fa2d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-12006",
                    "ArXiv": "2301.12006",
                    "DOI": "10.48550/arXiv.2301.12006",
                    "CorpusId": 231800060
                },
                "corpusId": 231800060,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2b71defcd83889711ef8768ab8b57f9e41e1fa2d",
                "title": "Improved knowledge distillation by utilizing backward pass knowledge in neural networks",
                "abstract": "Knowledge distillation (KD) is one of the prominent techniques for model compression. In this method, the knowledge of a large network (teacher) is distilled into a model (student) with usually signi\ufb01cantly fewer parameters. KD tries to better-match the output of the student model to that of the teacher model based on the knowledge extracts from the forward pass of the teacher network. Although conventional KD is effective for matching the two networks over the given data points, there is no guarantee that these models would match in other areas for which we do not have enough training samples. In this work, we address that problem by generating new auxiliary training samples based on extracting knowledge from the backward pass of the teacher in the areas where the student diverges greatly from the teacher. We compute the difference between the teacher and the student and generate new data samples that maximize the divergence. This is done by perturbing data samples in the direction of the gradient of the difference between the student and the teacher. Augmenting the training set by adding this auxiliary improves the performance of KD signi\ufb01cantly and leads to a closer match between the student and the teacher. Using this approach, when data samples come from a discrete domain, such as applications of natural language processing (NLP) and language understanding, is not trivial. However, we show how this technique can be used successfully in such applications. We evaluated the performance of our method on various tasks in computer vision and NLP domains and got promising results.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "31036999",
                        "name": "A. Jafari"
                    },
                    {
                        "authorId": "2066076226",
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "authorId": "38565890",
                        "name": "A. Ghodsi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "924595c6898100ec8696701cc6aa58eb37d203f6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-04338",
                    "ArXiv": "2301.04338",
                    "DOI": "10.1016/j.eswa.2023.120327",
                    "CorpusId": 255595916
                },
                "corpusId": 255595916,
                "publicationVenue": {
                    "id": "987139ae-a65d-49bb-aaf6-fb764dc40b19",
                    "name": "Expert systems with applications",
                    "type": "journal",
                    "alternate_names": [
                        "Expert syst appl",
                        "Expert Systems With Applications",
                        "Expert Syst Appl"
                    ],
                    "issn": "0957-4174",
                    "url": "https://www.journals.elsevier.com/expert-systems-with-applications/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/expert-systems-with-applications",
                        "http://www.sciencedirect.com/science/journal/09574174"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/924595c6898100ec8696701cc6aa58eb37d203f6",
                "title": "Synthetic data generation method for data-free knowledge distillation in regression neural networks",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2163011403",
                        "name": "Tianxun Zhou"
                    },
                    {
                        "authorId": "4287306",
                        "name": "K. Chiam"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It most closely resembles a form of data-free distillation (Micaelli and Storkey, 2019; Nayak et al., 2019; Shen et al., 2021), where the"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a9e3e5dd7b30890553b7ae1c41f932e99192bb44",
                "externalIds": {
                    "ACL": "2023.acl-long.830",
                    "DBLP": "journals/corr/abs-2212-10071",
                    "ArXiv": "2212.10071",
                    "DOI": "10.48550/arXiv.2212.10071",
                    "CorpusId": 254877399
                },
                "corpusId": 254877399,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/a9e3e5dd7b30890553b7ae1c41f932e99192bb44",
                "title": "Large Language Models Are Reasoning Teachers",
                "abstract": "Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude. We propose Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tune smaller models. We evaluate our method on a wide range of public models and complex tasks. We find that Fine-tune-CoT enables substantial reasoning capability in small models, far outperforming prompt-based baselines and even the teacher model in many tasks. Additionally, we extend our method by leveraging the teacher model\u2019s ability to generate multiple distinct rationales for each original sample. Enriching the fine-tuning data with such diverse reasoning results in a substantial performance boost across datasets, even for very small models. We conduct ablations and sample studies to understand the emergence of reasoning capabilities of student models. Our code implementation and data are available at https://github.com/itsnamgyu/reasoning-teacher.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2047104015",
                        "name": "Namgyu Ho"
                    },
                    {
                        "authorId": "145753172",
                        "name": "Laura Schmid"
                    },
                    {
                        "authorId": "70509252",
                        "name": "Se-Young Yun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The adversary can train a duplication through zero-knowledge distillation [15] or datafree knowledge distillation [7], or use a local independent dataset to fine-tune the victim model."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f85b3886ebcf8093cf5aefb797aa0ccbd5c8b767",
                "externalIds": {
                    "DBLP": "conf/uic/YinQ22",
                    "DOI": "10.1109/SmartWorld-UIC-ATC-ScalCom-DigitalTwin-PriComp-Metaverse56740.2022.00173",
                    "CorpusId": 260254724
                },
                "corpusId": 260254724,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f85b3886ebcf8093cf5aefb797aa0ccbd5c8b767",
                "title": "Verify Deep Learning Models Ownership via Preset Embedding",
                "abstract": "A well-trained deep neural network (DNNs) requires massive computing resources and data, therefore it belongs to the model owners\u2019 Intellectual Property (IP). Recent works have shown that the model can be stolen by the adversary without any training data or internal parameters of the model. Currently, there were some defense methods to resist it, by increasing the cost of model stealing attack or detecting the theft afterwards.In this paper, We propose a method to determine theft by detecting whether the victim\u2019s preset embedding exists in the adversary model. Firstly, we convert some training images into grayscale images as embedding and inject them to the training set. Then, we train a binary classifier to determine whether the model is stolen from the victim. The main intuition behind our approach is that the stolen model should contain embedded knowledge learned by the victim model. Our results demonstrate that our method is effective in defending against different types of model theft methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2214847842",
                        "name": "Wenxuan Yin"
                    },
                    {
                        "authorId": "34525206",
                        "name": "Hai-feng Qian"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026al. 2022), 2) teacher-student architecture, e.g., simplified (Li et al. 2020), quantized (Polino, Pascanu, and Alistarh 2018), condensed (Xie et al. 2020), and 3) distillation strategy, e.g., multi-teacher (Yuan et al. 2021), graph-based (Yao et al. 2020), adversarial (Micaelli and Storkey 2019)."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "be6d5c3d95854d27edb911eb20c5e1c4af630828",
                "externalIds": {
                    "DBLP": "conf/aaai/DingJZGL23",
                    "ArXiv": "2211.14466",
                    "DOI": "10.48550/arXiv.2211.14466",
                    "CorpusId": 254043742
                },
                "corpusId": 254043742,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/be6d5c3d95854d27edb911eb20c5e1c4af630828",
                "title": "SKDBERT: Compressing BERT via Stochastic Knowledge Distillation",
                "abstract": "In this paper, we propose Stochastic Knowledge Distillation (SKD) to obtain compact BERT-style language model dubbed SKDBERT. In each distillation iteration, SKD samples a teacher model from a pre-defined teacher team, which consists of multiple teacher models with multi-level capacities, to transfer knowledge into student model in an one-to-one manner. Sampling distribution plays an important role in SKD. We heuristically present three types of sampling distributions to assign appropriate probabilities for multi-level teacher models. SKD has two advantages: 1) it can preserve the diversities of multi-level teacher models via stochastically sampling single teacher model in each distillation iteration, and 2) it can also improve the efficacy of knowledge distillation via multi-level teacher models when large capacity gap exists between the teacher model and the student model. Experimental results on GLUE benchmark show that SKDBERT reduces the size of a BERT model by 40% while retaining 99.5% performances of language understanding and being 100% faster.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "83352946",
                        "name": "Zixiang Ding"
                    },
                    {
                        "authorId": "46642783",
                        "name": "Guoqing Jiang"
                    },
                    {
                        "authorId": "2192666926",
                        "name": "Shuai Zhang"
                    },
                    {
                        "authorId": "2192710757",
                        "name": "Lin Guo"
                    },
                    {
                        "authorId": "2602987",
                        "name": "W. Lin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This can be achieved either by looking at some of the teacher model statistics to generate synthetic transfer data [34, 37, 6], or by training a GAN in parallel [36, 10, 2]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3899ab88ac3d74f111b0c4b392f520483342a412",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-10943",
                    "ArXiv": "2211.10943",
                    "DOI": "10.48550/arXiv.2211.10943",
                    "CorpusId": 253734490
                },
                "corpusId": 253734490,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3899ab88ac3d74f111b0c4b392f520483342a412",
                "title": "Scalable Collaborative Learning via Representation Sharing",
                "abstract": "Privacy-preserving machine learning has become a key conundrum for multi-party artificial intelligence. Federated learning (FL) and Split Learning (SL) are two frameworks that enable collaborative learning while keeping the data private (on device). In FL, each data holder trains a model locally and releases it to a central server for aggregation. In SL, the clients must release individual cut-layer activations (smashed data) to the server and wait for its response (during both inference and back propagation). While relevant in several settings, both of these schemes have a high communication cost, rely on server-level computation algorithms and do not allow for tunable levels of collaboration. In this work, we present a novel approach for privacy-preserving machine learning, where the clients collaborate via online knowledge distillation using a contrastive loss (contrastive w.r.t. the labels). The goal is to ensure that the participants learn similar features on similar classes without sharing their input data. To do so, each client releases averaged last hidden layer activations of similar labels to a central server that only acts as a relay (i.e., is not involved in the training or aggregation of the models). Then, the clients download these last layer activations (feature representations) of the ensemble of users and distill their knowledge in their personal model using a contrastive objective. For cross-device applications (i.e., small local datasets and limited computational capacity), this approach increases the utility of the models compared to independent learning and other federated knowledge distillation (FD) schemes, is communication efficient and is scalable with the number of clients. We prove theoretically that our framework is well-posed, and we benchmark its performance against standard FD and FL on various datasets using different model architectures.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2183082271",
                        "name": "Fr'ed'eric Berdoz"
                    },
                    {
                        "authorId": "2034349211",
                        "name": "Abhishek Singh"
                    },
                    {
                        "authorId": "2456863",
                        "name": "Martin Jaggi"
                    },
                    {
                        "authorId": "2070747078",
                        "name": "Ramesh Raskar"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "051224ccbd7fcd11c4ead520a01ee9cd9d97e276",
                "externalIds": {
                    "ArXiv": "2211.10938",
                    "DBLP": "journals/corr/abs-2211-10938",
                    "DOI": "10.48550/arXiv.2211.10938",
                    "CorpusId": 253735083
                },
                "corpusId": 253735083,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/051224ccbd7fcd11c4ead520a01ee9cd9d97e276",
                "title": "AI-KD: Adversarial learning and Implicit regularization for self-Knowledge Distillation",
                "abstract": "We present a novel adversarial penalized self-knowledge distillation method, named adversarial learning and implicit regularization for self-knowledge distillation (AI-KD), which regularizes the training procedure by adversarial learning and implicit distillations. Our model not only distills the deterministic and progressive knowledge which are from the pre-trained and previous epoch predictive probabilities but also transfers the knowledge of the deterministic predictive distributions using adversarial learning. The motivation is that the self-knowledge distillation methods regularize the predictive probabilities with soft targets, but the exact distributions may be hard to predict. Our method deploys a discriminator to distinguish the distributions between the pre-trained and student models while the student model is trained to fool the discriminator in the trained procedure. Thus, the student model not only can learn the pre-trained model's predictive probabilities but also align the distributions between the pre-trained and student models. We demonstrate the effectiveness of the proposed method with network architectures on multiple datasets and show the proposed method achieves better performance than state-of-the-art methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2135937738",
                        "name": "Hyungmin Kim"
                    },
                    {
                        "authorId": "2012740",
                        "name": "Sungho Suh"
                    },
                    {
                        "authorId": "122283689",
                        "name": "Sunghyun Baek"
                    },
                    {
                        "authorId": "2154956217",
                        "name": "Daehwan Kim"
                    },
                    {
                        "authorId": "2056896408",
                        "name": "Daun Jeong"
                    },
                    {
                        "authorId": "2153293324",
                        "name": "Hansang Cho"
                    },
                    {
                        "authorId": "1769295",
                        "name": "Junmo Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "AVKD [16] extends the ZSKT [12] and formulates the adversarial exploration process as variational autoencoders (VAE).",
                "ZSKT [12] achieves data-free knowledge transfer by firstly training an adversarial generator to search for images on which student poorly matches the teacher, and then using them to train the student."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f0c39f21e81a0676ce0c1bab4410d21a52117a11",
                "externalIds": {
                    "DBLP": "conf/icip/FuWCHPTY22",
                    "DOI": "10.1109/ICIP46576.2022.9897652",
                    "CorpusId": 253337820
                },
                "corpusId": 253337820,
                "publicationVenue": {
                    "id": "b6369c33-5d70-463c-8e82-95a54efa3cc8",
                    "name": "International Conference on Information Photonics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Image Process",
                        "ICIP",
                        "Int Conf Inf Photonics",
                        "International Conference on Image Processing"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f0c39f21e81a0676ce0c1bab4410d21a52117a11",
                "title": "Meta-BNS FOR Adversarial Data-Free Quantization",
                "abstract": "Data-free quantization has recently been a promising method to perform quantization without access to the original data. However, the drawback of such approaches is the homogenization of synthetic data due to low efficiency for diverse data generation and the performance collapse of the generator. To alleviate the above issue, we propose a novel Meta-BNS for adversarial data-free quantization scheme which consists of Meta-BNS module and adversarial exploration module. Meta-BNS module automatically learns an enhancement coefficient matrix function for BN loss module to provide a suitable constrain on the generator. Adversarial exploration module leverages minimax game between the generator and quantized model via input gradient to encourage the generator to learn high-dimensional and complex real data distribution. The experimental results show that our method achieves state-of-the-art performance for various settings on data-free quantization.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2182294169",
                        "name": "Siming Fu"
                    },
                    {
                        "authorId": "2155981033",
                        "name": "Hualiang Wang"
                    },
                    {
                        "authorId": "2037737604",
                        "name": "Yuchen Cao"
                    },
                    {
                        "authorId": "2043870",
                        "name": "Haoji Hu"
                    },
                    {
                        "authorId": "145560079",
                        "name": "Bo Peng"
                    },
                    {
                        "authorId": "51151126",
                        "name": "Wenming Tan"
                    },
                    {
                        "authorId": "2108746950",
                        "name": "Ting Ye"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Finally, we optimize the closeness of activations [41] between the last k layers of model M and B on the forget set Df"
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b1f692f7a5d120499cf513db19a9aa9ceee676f2",
                "externalIds": {
                    "ArXiv": "2210.08196",
                    "DBLP": "journals/corr/abs-2210-08196",
                    "DOI": "10.48550/arXiv.2210.08196",
                    "CorpusId": 252917585
                },
                "corpusId": 252917585,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b1f692f7a5d120499cf513db19a9aa9ceee676f2",
                "title": "Deep Regression Unlearning",
                "abstract": "With the introduction of data protection and privacy regulations, it has become crucial to remove the lineage of data on demand from a machine learning (ML) model. In the last few years, there have been notable developments in machine unlearning to remove the information of certain training data efficiently and effectively from ML models. In this work, we explore unlearning for the regression problem, particularly in deep learning models. Unlearning in classification and simple linear regression has been considerably investigated. However, unlearning in deep regression models largely remains an untouched problem till now. In this work, we introduce deep regression unlearning methods that generalize well and are robust to privacy attacks. We propose the Blindspot unlearning method which uses a novel weight optimization process. A randomly initialized model, partially exposed to the retain samples and a copy of the original model are used together to selectively imprint knowledge about the data that we wish to keep and scrub off the information of the data we wish to forget. We also propose a Gaussian fine tuning method for regression unlearning. The existing unlearning metrics for classification are not directly applicable to regression unlearning. Therefore, we adapt these metrics for the regression setting. We conduct regression unlearning experiments for computer vision, natural language processing and forecasting applications. Our methods show excellent performance for all these datasets across all the metrics. Source code: https://github.com/ayu987/deep-regression-unlearning",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2141034263",
                        "name": "Ayush K Tarun"
                    },
                    {
                        "authorId": "2141033650",
                        "name": "Vikram S Chundawat"
                    },
                    {
                        "authorId": "2113781247",
                        "name": "Murari Mandal"
                    },
                    {
                        "authorId": "145977143",
                        "name": "Mohan S. Kankanhalli"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Micaelli and Storkey [166] achieved data-free distillation by training an adversarial generator to search for samples on which the student poorly matches the teacher and then using them to teach the student model."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e467ad57b7872a48b5210af756f5260dad8a5ab4",
                "externalIds": {
                    "ArXiv": "2210.04505",
                    "DBLP": "journals/corr/abs-2210-04505",
                    "DOI": "10.48550/arXiv.2210.04505",
                    "CorpusId": 252780731
                },
                "corpusId": 252780731,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e467ad57b7872a48b5210af756f5260dad8a5ab4",
                "title": "A Survey on Heterogeneous Federated Learning",
                "abstract": "Federated learning (FL) has been proposed to protect data privacy and virtually assemble the isolated data silos by cooperatively training models among organizations without breaching privacy and security. However, FL faces heterogeneity from various aspects, including data space, statistical, and system heterogeneity. For example, collaborative organizations without conflict of interest often come from different areas and have heterogeneous data from different feature spaces. Participants may also want to train heterogeneous personalized local models due to non-IID and imbalanced data distribution and various resource-constrained devices. Therefore, heterogeneous FL is proposed to address the problem of heterogeneity in FL. In this survey, we comprehensively investigate the domain of heterogeneous FL in terms of data space, statistical, system, and model heterogeneity. We first give an overview of FL, including its definition and categorization. Then, We propose a precise taxonomy of heterogeneous FL settings for each type of heterogeneity according to the problem setting and learning objective. We also investigate the transfer learning methodologies to tackle the heterogeneity in FL. We further present the applications of heterogeneous FL. Finally, we highlight the challenges and opportunities and envision promising future research directions toward new framework design and trustworthy approaches.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2082157791",
                        "name": "Dashan Gao"
                    },
                    {
                        "authorId": "2115586803",
                        "name": "Xin Yao"
                    },
                    {
                        "authorId": "144286907",
                        "name": "Qian Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This increases the total training time of by about 40% compared to ABM.",
                "Adversarial Belief Matching (ABM) [31] proposed an adversarial learning framework between S and G via optimizing the following min-max objective:",
                "We consider two related baselines of MAD which are ABM [31] and DFKD-Mem [3].",
                "In this experiment, we found that DFKD-Mem often performs worse than ABM on CIFAR100 and TinyImageNet.",
                "For example, MAD achieves about 1.5/2.8%, 2.5/3.6%, and 4.2/2.2% higher accuracy than ABM/DFKD-Mem on CIFAR100, TinyImageNet, and ImageNet, respectively.",
                "Through extensive experiments on three small and three large image datasets, we demonstrate that our proposed method is far better than related baselines [3, 31] in dealing with the large distribution shift problem.",
                "2, it is clear that MAD significantly outperforms both ABM and DFKD-Mem on all datasets.",
                "A common DFKD approach is to use a generator network to synthesize training data and jointly train the generator and the student in an adversarial manner [13, 31, 46].",
                "Most methods of this type are derived from the ABM [31] discussed in Section 2 with additional objectives to improve the quality and/or diversity of synthetic data.",
                "2 Comparison with related baselines We consider two related baselines of MAD which are ABM [31] and DFKD-Mem [3].",
                "In ABM, LKD(x) is the Kullback-Leibler (KL) divergence between class probabilities of T and S computed on x:\nLKD(x) , DKL (Tp(x)\u2016Sp(x)) = C\u2211 c=1 Tp(x)[c] \u00b7 (log Tp(x)[c]\u2212 log Sp(x)[c]) , (3)\nwhere Tp(x) = softmax(T(x)) and Sp(x) = softmax(S(x)) denote the class probabilities of T and S computed on x, respectively; and C is the total number of classes.",
                "We trained ABM and DFKD-Mem using exactly the same settings for training MAD.",
                "Adversarial Belief Matching (ABM) [31] proposed an adversarial learning framework between S and G via optimizing the following min-max objective:\nmin S max G\nEz\u223cp(z) [LKD(G(z))] (1)\n\u21d4min S max G Ez\u223cp(z),x=G(z) [LKD(x)] , (2)\nwhere LKD(x) denotes the knowledge distillation (KD) loss, i.e., the discrepancy between S(x) and T(x).",
                "ABM learns the student S with only synthetic samples from G. DFKD-Mem, on the other hand, stores past synthetic samples in a memory bank, and uses samples from this memory bank (dubbed \u201cmemory samples\u201d) as well as those generated by G as training data for S."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "b443f3f5558f6255d1e8feaf14ef7bddac92afc4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-10359",
                    "ArXiv": "2209.10359",
                    "DOI": "10.48550/arXiv.2209.10359",
                    "CorpusId": 252407775
                },
                "corpusId": 252407775,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b443f3f5558f6255d1e8feaf14ef7bddac92afc4",
                "title": "Momentum Adversarial Distillation: Handling Large Distribution Shifts in Data-Free Knowledge Distillation",
                "abstract": "Data-free Knowledge Distillation (DFKD) has attracted attention recently thanks to its appealing capability of transferring knowledge from a teacher network to a student network without using training data. The main idea is to use a generator to synthesize data for training the student. As the generator gets updated, the distribution of synthetic data will change. Such distribution shift could be large if the generator and the student are trained adversarially, causing the student to forget the knowledge it acquired at previous steps. To alleviate this problem, we propose a simple yet effective method called Momentum Adversarial Distillation (MAD) which maintains an exponential moving average (EMA) copy of the generator and uses synthetic samples from both the generator and the EMA generator to train the student. Since the EMA generator can be considered as an ensemble of the generator's old versions and often undergoes a smaller change in updates compared to the generator, training on its synthetic samples can help the student recall the past knowledge and prevent the student from adapting too quickly to new updates of the generator. Our experiments on six benchmark datasets including big datasets like ImageNet and Places365 demonstrate the superior performance of MAD over competing methods for handling the large distribution shift problem. Our method also compares favorably to existing DFKD methods and even achieves state-of-the-art results in some cases.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "36072771",
                        "name": "Kien Do"
                    },
                    {
                        "authorId": "2145242474",
                        "name": "Hung Le"
                    },
                    {
                        "authorId": "2112292189",
                        "name": "D. Nguyen"
                    },
                    {
                        "authorId": "145442756",
                        "name": "Dang Nguyen"
                    },
                    {
                        "authorId": "2137584728",
                        "name": "Haripriya Harikumar"
                    },
                    {
                        "authorId": "6254479",
                        "name": "T. Tran"
                    },
                    {
                        "authorId": "2867032",
                        "name": "Santu Rana"
                    },
                    {
                        "authorId": "2068804643",
                        "name": "S. Venkatesh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Hence, inspired by [107], Chundawat et al."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2ee88c51d565622445540d6c203bdf331d9ade44",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-02299",
                    "ArXiv": "2209.02299",
                    "DOI": "10.48550/arXiv.2209.02299",
                    "CorpusId": 252089272
                },
                "corpusId": 252089272,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2ee88c51d565622445540d6c203bdf331d9ade44",
                "title": "A Survey of Machine Unlearning",
                "abstract": "Today, computer systems hold large amounts of personal data. Yet while such an abundance of data allows breakthroughs in artificial intelligence, and especially machine learning (ML), its existence can be a threat to user privacy, and it can weaken the bonds of trust between humans and AI. Recent regulations now require that, on request, private information about a user must be removed from both computer systems and from ML models, i.e. ``the right to be forgotten''). While removing data from back-end databases should be straightforward, it is not sufficient in the AI context as ML models often `remember' the old data. Contemporary adversarial attacks on trained models have proven that we can learn whether an instance or an attribute belonged to the training data. This phenomenon calls for a new paradigm, namely machine unlearning, to make ML models forget about particular data. It turns out that recent works on machine unlearning have not been able to completely solve the problem due to the lack of common frameworks and resources. Therefore, this paper aspires to present a comprehensive examination of machine unlearning's concepts, scenarios, methods, and applications. Specifically, as a category collection of cutting-edge studies, the intention behind this article is to serve as a comprehensive resource for researchers and practitioners seeking an introduction to machine unlearning and its formulations, design criteria, removal requests, algorithms, and applications. In addition, we aim to highlight the key findings, current trends, and new research areas that have not yet featured the use of machine unlearning but could benefit greatly from it. We hope this survey serves as a valuable resource for ML researchers and those seeking to innovate privacy technologies. Our resources are publicly available at https://github.com/tamlhp/awesome-machine-unlearning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2117824517",
                        "name": "T. Nguyen"
                    },
                    {
                        "authorId": "152399820",
                        "name": "T. T. Huynh"
                    },
                    {
                        "authorId": "2143967163",
                        "name": "Phi-Le Nguyen"
                    },
                    {
                        "authorId": "1733300",
                        "name": "Alan Wee-Chung Liew"
                    },
                    {
                        "authorId": "47064303",
                        "name": "H. Yin"
                    },
                    {
                        "authorId": "144133815",
                        "name": "Quoc Viet Hung Nguyen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "87bdbea393f91b35e8789c3b0ca834c59b84f072",
                "externalIds": {
                    "DBLP": "journals/ijis/ZhangLKPHY22",
                    "DOI": "10.1002/int.23021",
                    "CorpusId": 252028105
                },
                "corpusId": 252028105,
                "publicationVenue": {
                    "id": "05528bac-d212-46a6-9c84-314d4bd77368",
                    "name": "International Journal of Intelligent Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Intell Syst"
                    ],
                    "issn": "0884-8173",
                    "url": "http://www3.interscience.wiley.com/cgi-bin/jhome/36062",
                    "alternate_urls": [
                        "https://onlinelibrary.wiley.com/journal/1098111X"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/87bdbea393f91b35e8789c3b0ca834c59b84f072",
                "title": "KD\u2010GAN: An effective membership inference attacks defence framework",
                "abstract": "Over the past few years, a variety of membership inference attacks against deep learning models have emerged, raising significant privacy concerns. These attacks can easily infer whether a sample exists in the training set of the target model with little adversary knowledge, and the inference accuracy is often much higher than random guessing, which causes serious privacy leakage. To this end, defenses against membership inference attacks have attracted great interest. However, the current available defense methods such as regularization, differential privacy, and knowledge distillation are unable to balance the trade\u2010off between privacy and utility well. In this paper, we combine knowledge distillation and generative adversarial networks to propose a novel training framework that can effectively defend against membership inference attacks, called KD\u2010GAN. Extensive experiments show that our method implements an attack success rate of nearly 0.5 (random guesses) which can successfully defend against membership inference attacks without causing significant damage to model utility, and consistently outperforming other defense methods in the balance of privacy and utility.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2158343962",
                        "name": "Zhenxin Zhang"
                    },
                    {
                        "authorId": "2165210470",
                        "name": "Guanbiao Lin"
                    },
                    {
                        "authorId": "2918554",
                        "name": "Lishan Ke"
                    },
                    {
                        "authorId": "2072714126",
                        "name": "Shiyu Peng"
                    },
                    {
                        "authorId": "2118848520",
                        "name": "Li Hu"
                    },
                    {
                        "authorId": "3188142",
                        "name": "Hongyang Yan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To address this challenge, Data-Free Knowledge Distillation (DFKD) or Zero-Shot Knowledge Distillation (ZSKD) [40] has been proposed.",
                "For the baselines, we compare state-of-the-art DFKD methods as DAFL[7], ZSKT[41], ADI[61], DFQ[9], CMI[16] and PRE-DFKD[4].",
                "ZSKD[43, 57, 41] and SoftTarget[56] model the output label distribution or intermediate feature maps by simple distributions.",
                "Data-Free Knowledge Distillation (DFKD, or ZSKD[43, 57, 41]) aims at training student models without training data.",
                "ZSKD [29, 40, 27] and SoftTarget [39] model the output label distribution or intermediate feature maps with simple distributions.",
                "For the baselines, we compare SOTA DFKD methods as DAFL[7], ZSKT[27], ADI[43], DFQ[9], CMI[14] and PRE-DFKD[4]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "de75336a5772e371a1461af9023acbd1513c7a06",
                "externalIds": {
                    "DBLP": "journals/isci/LiZLWBY23",
                    "ArXiv": "2208.13648",
                    "DOI": "10.1016/j.ins.2023.119202",
                    "CorpusId": 255941901
                },
                "corpusId": 255941901,
                "publicationVenue": {
                    "id": "e46002a1-d7a6-4681-aae9-36bc3a6a1f93",
                    "name": "Information Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Information Scientist",
                        "Inf Sci"
                    ],
                    "issn": "0020-0255",
                    "alternate_issns": [
                        "0020-0263"
                    ],
                    "url": "http://www.sciencedirect.com/science/journal/00200255"
                },
                "url": "https://www.semanticscholar.org/paper/de75336a5772e371a1461af9023acbd1513c7a06",
                "title": "Dynamic data-free knowledge distillation by easy-to-hard learning strategy",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109015309",
                        "name": "Jingru Li"
                    },
                    {
                        "authorId": "2156158437",
                        "name": "Sheng Zhou"
                    },
                    {
                        "authorId": "2145730944",
                        "name": "Liangcheng Li"
                    },
                    {
                        "authorId": "2267401",
                        "name": "Haishuai Wang"
                    },
                    {
                        "authorId": "2064698184",
                        "name": "Jiajun Bu"
                    },
                    {
                        "authorId": "2139424603",
                        "name": "Zhi Yu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "13df9e045024fcbc5ed0e33829ca8e02986bcbcb",
                "externalIds": {
                    "DBLP": "conf/iclr/ZaheerRKYJVFK23",
                    "ArXiv": "2208.06825",
                    "DOI": "10.48550/arXiv.2208.06825",
                    "CorpusId": 251564473
                },
                "corpusId": 251564473,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/13df9e045024fcbc5ed0e33829ca8e02986bcbcb",
                "title": "Teacher Guided Training: An Efficient Framework for Knowledge Transfer",
                "abstract": "The remarkable performance gains realized by large pretrained models, e.g., GPT-3, hinge on the massive amounts of data they are exposed to during training. Analogously, distilling such large models to compact models for efficient deployment also necessitates a large amount of (labeled or unlabeled) training data. In this paper, we propose the teacher-guided training (TGT) framework for training a high-quality compact model that leverages the knowledge acquired by pretrained generative models, while obviating the need to go through a large volume of data. TGT exploits the fact that the teacher has acquired a good representation of the underlying data domain, which typically corresponds to a much lower dimensional manifold than the input space. Furthermore, we can use the teacher to explore input space more efficiently through sampling or gradient-based methods; thus, making TGT especially attractive for limited data or long-tail settings. We formally capture this benefit of proposed data-domain exploration in our generalization bounds. We find that TGT can improve accuracy on several image classification benchmarks as well as a range of text classification and retrieval tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1771307",
                        "name": "M. Zaheer"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2109548913",
                        "name": "Seungyeon Kim"
                    },
                    {
                        "authorId": "1878841",
                        "name": "Chong You"
                    },
                    {
                        "authorId": "2059143344",
                        "name": "Himanshu Jain"
                    },
                    {
                        "authorId": "2799898",
                        "name": "Andreas Veit"
                    },
                    {
                        "authorId": "2276554",
                        "name": "R. Fergus"
                    },
                    {
                        "authorId": "49596260",
                        "name": "Surinder Kumar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "overcome the real-world dataset dependency for KD, data-free KD was proposed in which synthetic data or metadata was used for KD [20]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ba21511c5b00c6ccffe2fa328c75addc687d9c52",
                "externalIds": {
                    "DBLP": "journals/tii/KumarPBSS22",
                    "DOI": "10.1109/TII.2021.3138919",
                    "CorpusId": 245560838
                },
                "corpusId": 245560838,
                "publicationVenue": {
                    "id": "2135230a-3b24-4b71-9583-60624389377a",
                    "name": "IEEE Transactions on Industrial Informatics",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Ind Informatics"
                    ],
                    "issn": "1551-3203",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=9424",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9424"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ba21511c5b00c6ccffe2fa328c75addc687d9c52",
                "title": "MediSecFed: Private and Secure Medical Image Classification in the Presence of Malicious Clients",
                "abstract": "Deep learning demonstrates its efficacy and potential to solve challenging computer vision problems in medical and other industrial applications. Federated learning is a learning paradigm that facilitates collaborative learning in a federation of users without exchanging actual data with a single authority like a server. However, federated learning provides only a basic level of privacy and robustness and is vulnerable to model poisoning and model inversion attacks in hostile training environments. Hence, in this article, we propose MediSecFed\u2014a secure framework for federated learning in a hostile environment. Compared to the widely used FedAvg, our method relies on simple and practical ideas from knowledge distillation and model inversion to ensure additional security and privacy features. Our approach achieves knowledge exchange among participating entities without sharing model parameters as FedAvg does, thus protecting the privacy of the local data from the server and significantly reducing communication costs. We evaluate our method on two chest X-ray datasets. Our method outperforms FedAvg by 15% on both datasets in a hostile environment. Our method will also continue to maintain good performance even if the number of malicious participating entities increases. Robustness to learn in a malicious environment while preserving privacy with reduced communication costs makes our method more desirable and efficient than that of FedAvg.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109224465",
                        "name": "Abhinav Kumar"
                    },
                    {
                        "authorId": "2074170957",
                        "name": "Vishal Purohit"
                    },
                    {
                        "authorId": "38265917",
                        "name": "Vandana Bharti"
                    },
                    {
                        "authorId": "4020452",
                        "name": "Rishav Singh"
                    },
                    {
                        "authorId": "2118413737",
                        "name": "S. Singh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Note that we adopt mean square error (MSE) as objective in both phases other than KL divergence adopted in [20], because logit matching has better generalization capacity [16].",
                "[4,20] alternatively uses a GAN architecture to synthesize images, where they fix a trained network as a discriminator and optimize a generator to derive images that can be adopted to distill knowledge from the fixed network to a new network.",
                "In addition to the adopted method [20], DeepInversion [30] provides an alternative solution to data-free replay.",
                "In [20], the authors also indicate that the uncertain samples (i.",
                "In observing the fact that distilling knowledge using uncertain data is more effective since they are usually close to the model\u2019s decision boundaries [20], we introduce an entropy-regularized method to explicitly encourage the replayed data to be close to decision boundaries given by the reference model.",
                "In a few-shot incrementally trained model, the high entropy response of input usually can be identified as the case that the input is on its decision boundary [20] or is learned in a few-shot incremental session (i.",
                "We follow [20] to train the generator by including an auxiliary model A(\u00b7; \u03b8A) as a helper to assist the convergence of the generator.",
                "We use the toy experiment proposed by [20] to illustrate the effects of our entropy regularization in Figure 5."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "74ffcb247b17e86a529d9dc27825ce2439ba9117",
                "externalIds": {
                    "DBLP": "conf/eccv/LiuGCWYCT22",
                    "ArXiv": "2207.11213",
                    "DOI": "10.48550/arXiv.2207.11213",
                    "CorpusId": 251018677
                },
                "corpusId": 251018677,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/74ffcb247b17e86a529d9dc27825ce2439ba9117",
                "title": "Few-Shot Class-Incremental Learning via Entropy-Regularized Data-Free Replay",
                "abstract": "Few-shot class-incremental learning (FSCIL) has been proposed aiming to enable a deep learning system to incrementally learn new classes with limited data. Recently, a pioneer claims that the commonly used replay-based method in class-incremental learning (CIL) is ineffective and thus not preferred for FSCIL. This has, if truth, a significant influence on the fields of FSCIL. In this paper, we show through empirical results that adopting the data replay is surprisingly favorable. However, storing and replaying old data can lead to a privacy concern. To address this issue, we alternatively propose using data-free replay that can synthesize data by a generator without accessing real data. In observing the the effectiveness of uncertain data for knowledge distillation, we impose entropy regularization in the generator training to encourage more uncertain examples. Moreover, we propose to relabel the generated data with one-hot-like labels. This modification allows the network to learn by solely minimizing the cross-entropy loss, which mitigates the problem of balancing different objectives in the conventional knowledge distillation approach. Finally, we show extensive experimental results and analysis on CIFAR-100, miniImageNet and CUB-200 to demonstrate the effectiveness of our proposed one.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146397271",
                        "name": "Huan Liu"
                    },
                    {
                        "authorId": "2048254240",
                        "name": "Li Gu"
                    },
                    {
                        "authorId": "35793956",
                        "name": "Zhixiang Chi"
                    },
                    {
                        "authorId": "46396571",
                        "name": "Yang Wang"
                    },
                    {
                        "authorId": "1787848",
                        "name": "Yuanhao Yu"
                    },
                    {
                        "authorId": "2155464194",
                        "name": "Junming Chen"
                    },
                    {
                        "authorId": "1443774210",
                        "name": "Jingshan Tang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another approach to deal with the absence of source data is highly related to the Datafree Knowledge Distillation [9, 10, 11] through reconstructing source distribution from the source model.",
                "However, because the reconstructed source samples tend to fall on the decision boundaries of source model [11], the reconstructed distribution may not well represent the source distribution."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "584ab98792e44c75495747af7629558913eded73",
                "externalIds": {
                    "DBLP": "conf/icmcs/YangKH22",
                    "DOI": "10.1109/ICME52920.2022.9859581",
                    "CorpusId": 251848390
                },
                "corpusId": 251848390,
                "publicationVenue": {
                    "id": "975e1529-a14b-436f-ad7e-a09d8eabc50d",
                    "name": "IEEE International Conference on Multimedia and Expo",
                    "type": "conference",
                    "alternate_names": [
                        "ICME",
                        "Int Conf Multimedia Expo",
                        "IEEE Int Conf Multimedia Expo",
                        "International Conference on Multimedia and Expo"
                    ],
                    "issn": "1945-7871",
                    "alternate_issns": [
                        "2330-7927"
                    ],
                    "url": "http://csdl2.computer.org/persagen/ProceedingByFilter.jsp?filter=I",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conferences.jsp",
                        "http://www.wikicfp.com/cfp/program?id=1418"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/584ab98792e44c75495747af7629558913eded73",
                "title": "Source Free Domain Adaptation for Semantic Segmentation via Distribution Transfer and Adaptive Class-Balanced Self-Training",
                "abstract": "Unsupervised Domain Adaptation (UDA) for semantic seg-mentation aims to transfer the knowledge learned from the source domain to the target domain. Unlike the source-available UDA setting, Source-Free Domain Adaptation (SFDA) has no access to the source data and rely solely on the well-trained source model for adaptation. Without the source data for reference, SDFA often leads to unstable adaptation and mostly focuses on common semantic classes. In this pa-per, we propose a Distribution Transfer and Adaptive Class-balanced self-training (DTAC) framework to tackle the issues of SFDA for semantic segmentation. First, in the distribution transfer stage, we propose to narrow the domain gap by aligning the implicit feature characteristics of source model with the feature statistics of the target data. Next, in the self-training stage, we propose a multi-class negative learning method with adaptive thresholding to dynamically select per-class pseudo labels for self-supervision. Experimental re-sults on urban scene benchmarks show that DTAC outper-forms other SFDA baselines and even achieves competitive results with source-available UDA methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2154173293",
                        "name": "Cheng-Yu Yang"
                    },
                    {
                        "authorId": "2182887549",
                        "name": "Yuan-Jhe Kuo"
                    },
                    {
                        "authorId": "1687384",
                        "name": "Chiou-Ting Hsu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6ecd420a28f6dbe9acedf473950a68daad4b19c6",
                "externalIds": {
                    "DBLP": "journals/ijon/WangYZHS22",
                    "DOI": "10.1016/j.neucom.2022.07.055",
                    "CorpusId": 250631682
                },
                "corpusId": 250631682,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6ecd420a28f6dbe9acedf473950a68daad4b19c6",
                "title": "TC3KD: Knowledge distillation via teacher-student cooperative curriculum customization",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2135747990",
                        "name": "Chaofei Wang"
                    },
                    {
                        "authorId": "2176842203",
                        "name": "Ke Yang"
                    },
                    {
                        "authorId": "2156548999",
                        "name": "Shaowei Zhang"
                    },
                    {
                        "authorId": "2115218570",
                        "name": "Gao Huang"
                    },
                    {
                        "authorId": "30619669",
                        "name": "S. Song"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Data-free Knowledge Distillation Data-free knowledge distillation transfers knowledge of a teacher model to a student model without original dataset [35].",
                "A generative model is trained to synthesize data samples for students to query teacher in data-free manner [9, 12, 35]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "df415323353b9344154b966949b70e3bd7c4cad9",
                "externalIds": {
                    "DBLP": "conf/cvpr/00810X0DZ022",
                    "DOI": "10.1109/CVPR52688.2022.01469",
                    "CorpusId": 249951086
                },
                "corpusId": 249951086,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/df415323353b9344154b966949b70e3bd7c4cad9",
                "title": "Towards Efficient Data Free Blackbox Adversarial Attack",
                "abstract": "Classic black-box adversarial attacks can take advantage of transferable adversarial examples generated by a similar substitute model to successfully fool the target model. However, these substitute models need to be trained by target models' training data, which is hard to acquire due to privacy or transmission reasons. Recognizing the limited availability of real data for adversarial queries, recent works proposed to train substitute models in a data-free black-box scenario. However, their generative adversarial networks (GANs) based framework suffers from the convergence failure and the model collapse, resulting in low efficiency. In this paper, by rethinking the collaborative relationship between the generator and the substitute model, we design a novel black-box attack framework. The proposed method can efficiently imitate the target model through a small number of queries and achieve high attack success rate. The comprehensive experiments over six datasets demonstrate the effectiveness of our method against the state-of-the-art attacks. Especially, we conduct both label-only and probability-only attacks on the Microsoft Azure online model, and achieve a 100% attack success rate with only 0.46% query budget of the SOTA method [49].",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51250527",
                        "name": "J Zhang"
                    },
                    {
                        "authorId": "71788673",
                        "name": "Bo Li"
                    },
                    {
                        "authorId": "2143509839",
                        "name": "Jianghe Xu"
                    },
                    {
                        "authorId": "2117212193",
                        "name": "Shuang Wu"
                    },
                    {
                        "authorId": "7406856",
                        "name": "Shouhong Ding"
                    },
                    {
                        "authorId": "1429345566",
                        "name": "Lei Zhang"
                    },
                    {
                        "authorId": "2146290239",
                        "name": "Chao Wu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Moreover, some papers [8, 11, 30] transferred knowledge in an adversarial strategy."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2835f60d74ae2209f3faf1ba3bc32848fd638a4c",
                "externalIds": {
                    "DBLP": "conf/cvpr/LuWD22",
                    "DOI": "10.1109/CVPR52688.2022.00718",
                    "CorpusId": 250602600
                },
                "corpusId": 250602600,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2835f60d74ae2209f3faf1ba3bc32848fd638a4c",
                "title": "Augmented Geometric Distillation for Data-Free Incremental Person ReID",
                "abstract": "Incremental learning (IL) remains an open issue for Person Re-identification (ReID), where a ReID system is expected to preserve preceding knowledge while learning incrementally. However, due to the strict privacy licenses and the open-set retrieval setting, it is intractable to adapt existing class IL methods to ReID. In this work, we propose an Augmented Geometric Distillation (AGD) framework to tackle these issues. First, a general data-free incremental framework with dreaming memory is constructed to avoid privacy disclosure. On this basis, we reveal a \u201cnoisy distillation\u201d problem stemming from the noise in dreaming memory, and further propose to augment distillation in a pairwise and cross-wise pattern over different views of memory to mitigate it. Second, for the open-set retrieval property, we propose to maintain feature space structure during evolving via a novel geometric way and preserve relationships between exemplars when representations drift. Extensive experiments demonstrate the superiority of our AGD to baseline with a margin of 6.0% mAP/7.9% R@1 and it could be generalized to class IL. Code is available here11\u2020https://github.com/eddielyc/Augmented-Geometric-Distillation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2156141201",
                        "name": "Yichen Lu"
                    },
                    {
                        "authorId": "50469060",
                        "name": "Mei Wang"
                    },
                    {
                        "authorId": "1774956",
                        "name": "Weihong Deng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Adversarial distillation is a typical training scheme for generator-based data-free KD approaches [35].",
                "Adversarial distillation [3], [35] is an efficient scheme that trains the generator and the student together by a min-max game."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a30d656881948233824e329d0bcfca5a40bd1c8e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-11845",
                    "ArXiv": "2205.11845",
                    "DOI": "10.1109/TMM.2022.3192663",
                    "CorpusId": 249017991
                },
                "corpusId": 249017991,
                "publicationVenue": {
                    "id": "10e76a35-58d6-443c-9683-fc16f2dd0a92",
                    "name": "IEEE transactions on multimedia",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Transactions on Multimedia",
                        "IEEE Trans Multimedia",
                        "IEEE trans multimedia"
                    ],
                    "issn": "1520-9210",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=6046"
                },
                "url": "https://www.semanticscholar.org/paper/a30d656881948233824e329d0bcfca5a40bd1c8e",
                "title": "CDFKD-MFS: Collaborative Data-Free Knowledge Distillation via Multi-Level Feature Sharing",
                "abstract": "Recently, the compression and deployment of powerful deep neural networks (DNNs) on resource-limited edge devices to provide intelligent services have become attractive tasks. Although knowledge distillation (KD) is a feasible solution for compression, its requirement on the original dataset raises privacy concerns. In addition, it is common to integrate multiple pretrained models to achieve satisfactory performance. How to compress multiple models into a tiny model is challenging, especially when the original data are unavailable. To tackle this challenge, we propose a framework termed collaborative data-free knowledge distillation via multi-level feature sharing (CDFKD-MFS), which consists of a multi-header student module, an asymmetric adversarial data-free KD module, and an attention-based aggregation module. In this framework, the student model equipped with a multi-level feature-sharing structure learns from multiple teacher models and is trained together with a generator in an asymmetric adversarial manner. When some real samples are available, the attention module adaptively aggregates predictions of the student headers, which can further improve performance. We conduct extensive experiments on three popular computer visual datasets. In particular, compared with the most competitive alternative, the accuracy of the proposed framework is 1.18% higher on the CIFAR-100 dataset, 1.67% higher on the Caltech-101 dataset, and 2.99% higher on the mini-ImageNet dataset.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2147215540",
                        "name": "Zhiwei Hao"
                    },
                    {
                        "authorId": "2150649639",
                        "name": "Yong Luo"
                    },
                    {
                        "authorId": "2135451924",
                        "name": "Zhi Wang"
                    },
                    {
                        "authorId": "46177189",
                        "name": "Han Hu"
                    },
                    {
                        "authorId": "2151864079",
                        "name": "J. An"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "469dcb13083d763081d8a9863d97298497c7d461",
                "externalIds": {
                    "ArXiv": "2205.11158",
                    "DBLP": "conf/iclr/ZhangCL23",
                    "CorpusId": 248987713
                },
                "corpusId": 248987713,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/469dcb13083d763081d8a9863d97298497c7d461",
                "title": "IDEAL: Query-Efficient Data-Free Learning from Black-Box Models",
                "abstract": "Knowledge Distillation (KD) is a typical method for training a lightweight student model with the help of a well-trained teacher model. However, most KD methods require access to either the teacher's training data or model parameters, which is unrealistic. To tackle this problem, recent works study KD under data-free and black-box settings. Nevertheless, these works require a large number of queries to the teacher model, which incurs significant monetary and computational costs. To address these problems, we propose a novel method called \\emph{query-effIcient Data-free lEarning from blAck-box modeLs} (IDEAL), which aims to query-efficiently learn from black-box model APIs to train a good student without any real data. In detail, IDEAL trains the student model in two stages: data generation and model distillation. Note that IDEAL does not require any query in the data generation stage and queries the teacher only once for each sample in the distillation stage. Extensive experiments on various real-world datasets show the effectiveness of the proposed IDEAL. For instance, IDEAL can improve the performance of the best baseline method DFME by 5.83% on CIFAR10 dataset with only 0.02x the query budget of DFME.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2159188796",
                        "name": "Jie Zhang"
                    },
                    {
                        "authorId": null,
                        "name": "Chen Chen"
                    },
                    {
                        "authorId": "46436215",
                        "name": "Jiahua Dong"
                    },
                    {
                        "authorId": "39823639",
                        "name": "R. Jia"
                    },
                    {
                        "authorId": "3366777",
                        "name": "L. Lyu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Remarkably, SKD is very robust against the scale of the problem in terms of input resolution (from 32\u00d7 32 to 224\u00d7 224) and network architecture, which is also a significant advancement over existing knowledge transfer works [14], [15], [17], [45], [46].",
                "However, these methods [14], [15], [17] can only work well on simple and low-resolution datasets such as MNIST and CIFAR-100, and fail on high-resolution and fine-grained datasets."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7ad6e89309200ccb95a7e918f47b45f10a5d6cbf",
                "externalIds": {
                    "ArXiv": "2205.11071",
                    "DBLP": "journals/corr/abs-2205-11071",
                    "DOI": "10.1109/IJCNN55064.2022.9892266",
                    "CorpusId": 248987604
                },
                "corpusId": 248987604,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/7ad6e89309200ccb95a7e918f47b45f10a5d6cbf",
                "title": "Self-distilled Knowledge Delegator for Exemplar-free Class Incremental Learning",
                "abstract": "Exemplar-free incremental learning is extremely challenging due to inaccessibility of data from old tasks. In this paper, we attempt to exploit the knowledge encoded in a previously trained classification model to handle the catas-trophic forgetting problem in continual learning. Specifically, we introduce a so-called knowledge delegator, which is capable of transferring knowledge from the trained model to a randomly re-initialized new model by generating informative samples. Given the previous model only, the delegator is effectively learned using a self-distillation mechanism in a data-free manner. The knowledge extracted by the delegator is then utilized to maintain the performance of the model on old tasks in incremental learning. This simple incremental learning framework surpasses existing exemplar-free methods by a large margin on four widely used class incremental benchmarks, namely CIFAR-100, ImageNet-Subset, Caltech-101 and Flowers-102. Notably, we achieve comparable performance to some exemplar-based methods without accessing any exemplars.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152346798",
                        "name": "Fanfan Ye"
                    },
                    {
                        "authorId": "2109868694",
                        "name": "Liang Ma"
                    },
                    {
                        "authorId": "1842317",
                        "name": "Qiaoyong Zhong"
                    },
                    {
                        "authorId": "50322310",
                        "name": "Di Xie"
                    },
                    {
                        "authorId": "3290437",
                        "name": "Shiliang Pu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fb1d85fe28b5e92e22d084eca674d4a2b48cdc5a",
                "externalIds": {
                    "DBLP": "conf/ijcai/MaWF0022",
                    "ArXiv": "2205.07523",
                    "DOI": "10.48550/arXiv.2205.07523",
                    "CorpusId": 248811433
                },
                "corpusId": 248811433,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/fb1d85fe28b5e92e22d084eca674d4a2b48cdc5a",
                "title": "Prompting to Distill: Boosting Data-Free Knowledge Distillation via Reinforced Prompt",
                "abstract": "Data-free knowledge distillation (DFKD) conducts knowledge distillation via eliminating the dependence of original training data, and has recently achieved impressive results in accelerating pre-trained language models. At the heart of DFKD is to reconstruct a synthetic dataset by inverting the parameters of the uncompressed model. Prior DFKD approaches, however, have largely relied on hand-crafted priors of the target data distribution for the reconstruction, which can be inevitably biased and often incompetent to capture the intrinsic distributions. To address this problem, we propose a prompt-based method, termed as PromptDFD, that allows us to take advantage of learned language priors, which effectively harmonizes the synthetic sentences to be semantically and grammatically correct. Specifically, PromptDFD leverages a pre-trained generative model to provide language priors and introduces a reinforced topic prompter to control data synthesis, making the generated samples thematically relevant and semantically plausible, and thus friendly to downstream tasks. As shown in our experiments, the proposed method substantially improves the synthesis quality and achieves considerable improvements on distillation performance. In some cases, PromptDFD even gives rise to results on par with those from the data-driven knowledge distillation with access to the original training data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "15532066",
                        "name": "Xinyin Ma"
                    },
                    {
                        "authorId": "48631088",
                        "name": "Xinchao Wang"
                    },
                    {
                        "authorId": "150110431",
                        "name": "Gongfan Fang"
                    },
                    {
                        "authorId": "1471660296",
                        "name": "Yongliang Shen"
                    },
                    {
                        "authorId": "1776903",
                        "name": "Weiming Lu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In the extreme case where no real images are available, networks can be trained using data-free knowledge distillation methods [8, 34, 55].",
                "GAN-based methods [8, 34, 54, 62] synthesized training samples through maximizing response on the discriminator."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3a1dbfb6875bfac8251627d60db313623fbb8b04",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-12997",
                    "ArXiv": "2204.12997",
                    "DOI": "10.1109/CVPR52688.2022.01174",
                    "CorpusId": 248406101
                },
                "corpusId": 248406101,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3a1dbfb6875bfac8251627d60db313623fbb8b04",
                "title": "DearKD: Data-Efficient Early Knowledge Distillation for Vision Transformers",
                "abstract": "Transformers are successfully applied to computer vision due to their powerful modeling capacity with self-attention. However, the excellent performance of transformers heavily depends on enormous training images. Thus, a data-efficient transformer solution is urgently needed. In this work, we propose an early knowledge distillation framework, which is termed as DearKD, to improve the data efficiency required by transformers. Our DearKD is a two-stage framework that first distills the inductive biases from the early intermediate layers of a CNN and then gives the transformer full play by training without distillation. Further, our DearKD can be readily applied to the extreme data-free case where no real images are available. In this case, we propose a boundary-preserving intra-divergence loss based on DeepInversion to further close the performance gap against the full-data counterpart. Extensive experiments on ImageNet, partial ImageNet, data-free setting and other downstream tasks prove the superiority of DearKD over its baselines and state-of-the-art methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1720744322",
                        "name": "Xianing Chen"
                    },
                    {
                        "authorId": "46632720",
                        "name": "Qiong Cao"
                    },
                    {
                        "authorId": "1624475253",
                        "name": "Yujie Zhong"
                    },
                    {
                        "authorId": "1519070643",
                        "name": "Jing Zhang"
                    },
                    {
                        "authorId": "1702868",
                        "name": "Shenghua Gao"
                    },
                    {
                        "authorId": "2075330732",
                        "name": "Dacheng Tao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To unconditionally distill knowledge from a given DNN, Data-Free Distillation (DFD) methods have been proposed [14, 15].",
                "Four candidate trigger encoders were considered: the Gaussian noise [7], a random image generator network [20], the DFD generator itself [14, 15], and Wonder Filter (WF) [5]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9beabe2b6da0aa6087a24b09181978eca198b943",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-04522",
                    "ArXiv": "2204.04522",
                    "DOI": "10.48550/arXiv.2204.04522",
                    "CorpusId": 248085571
                },
                "corpusId": 248085571,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9beabe2b6da0aa6087a24b09181978eca198b943",
                "title": "Knowledge-Free Black-Box Watermark and Ownership Proof for Image Classification Neural Networks",
                "abstract": "Watermarking has become a plausible candidate for ownership verification and intellectual property protection of deep neural networks. Regarding image classification neural networks, current watermarking schemes uniformly resort to backdoor triggers. However, injecting a backdoor into a neural network requires knowledge of the training dataset, which is usually unavailable in the real-world commercialization. Meanwhile, established watermarking schemes oversight the potential damage of exposed evidence during ownership verification and the watermarking algorithms themselves. Those concerns decline current watermarking schemes from industrial applications. To confront these challenges, we propose a knowledge-free black-box watermarking scheme for image classification neural networks. The image generator obtained from a data-free distillation process is leveraged to stabilize the network's performance during the backdoor injection. A delicate encoding and verification protocol is designed to ensure the scheme's security against knowledgable adversaries. We also give a pioneering analysis of the capacity of the watermarking scheme. Experiment results proved the functionality-preserving capability and security of the proposed watermarking scheme.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146328036",
                        "name": "Fangqi Li"
                    },
                    {
                        "authorId": "2152363102",
                        "name": "Shilin Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5eafb52964f99514ae04952e3dceb63a22b3ec2f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-14001",
                    "ArXiv": "2203.14001",
                    "DOI": "10.1109/CVPR52688.2022.01163",
                    "CorpusId": 247762862
                },
                "corpusId": 247762862,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5eafb52964f99514ae04952e3dceb63a22b3ec2f",
                "title": "Knowledge Distillation with the Reused Teacher Classifier",
                "abstract": "Knowledge distillation aims to compress a powerful yet cumbersome teacher model into a lightweight student model without much sacrifice of performance. For this purpose, various approaches have been proposed over the past few years, generally with elaborately designed knowledge rep-resentations, which in turn increase the difficulty of model development and interpretation. In contrast, we empirically show that a simple knowledge distillation technique is enough to significantly narrow down the teacher-student performance gap. We directly reuse the discriminative classifier from the pre-trained teacher model for student inference and train a student encoder through feature alignment with a single \u21132 loss. In this way, the student model is able to achieve exactly the same performance as the teacher model provided that their extracted features are perfectly aligned. An additional projector is developed to help the student encoder match with the teacher classifier, which renders our technique applicable to various teacher and student architectures. Extensive experiments demonstrate that our technique achieves state-of-the-art results at the modest cost of compression ratio due to the added projector.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1684692",
                        "name": "Defang Chen"
                    },
                    {
                        "authorId": "51482485",
                        "name": "Jianhan Mei"
                    },
                    {
                        "authorId": "2155916654",
                        "name": "Hailin Zhang"
                    },
                    {
                        "authorId": "47074461",
                        "name": "C. Wang"
                    },
                    {
                        "authorId": "1692947908",
                        "name": "Yan Feng"
                    },
                    {
                        "authorId": "2109525713",
                        "name": "Chun Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For adversarial training, this paper uses the training scheme of AdvProp [10], which uses two separate batch normalization (BN) layers for clean and adversarial examples, arg \u2211   \u221d , ; , + \u2211   , ; \u2217, = arg + (7)",
                "\u2217 =   ( , ) (8) To avoid estimating the tricky \u2217 the data, a generative network is introduced G( , g) to control the data distribution [10].",
                "For adversarial training, this paper uses the training scheme of AdvProp [10], which uses two separate batch normalization (BN) layers for clean and adversarial examples,arg \u2211 \u221d , ; , +\u2211 , ; \u2217, = arg + (7)\nwhere balances the contrast loss with parameterized contrast loss and the contrast loss with \u2217 parametric ."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "1fda396122a28be1ce46cd8f49c0a93460f3d473",
                "externalIds": {
                    "DBLP": "conf/cacml/FengZ22",
                    "DOI": "10.1109/CACML55074.2022.00127",
                    "CorpusId": 251708788
                },
                "corpusId": 251708788,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1fda396122a28be1ce46cd8f49c0a93460f3d473",
                "title": "Self-supervised Image Hash Retrieval Based On Adversarial Distillation",
                "abstract": "Hash algorithms have become the mainstream of large-scale similarity image retrieval due to their high storage and search efficiency. The deep learning-based hashing greatly improves the retrieval performance with supervision, but it is difficult for the self-supervised deep hashing to achieve satisfactory performance when there is a lack of reliable supervised signals. In addition, to solve the problems of poor robustness and numerous parameters in traditional neural networks, a lightweight robust deep hash retrieval algorithm is proposed in this paper. The algorithm obtained a robust teacher network by self-supervised adversarial training, then trained the student network using optimized distillation loss and immune injection, and finally extracted image hash sequences using an attention mechanism based on convolution modules. Inspired by the fact that adversarial training is currently the most effective method to improve model robustness, and that knowledge distillation can compress the network while ensuring model performance, this paper proposes a self-supervised image hash retrieval algorithm based on adversarial distillation. The method was tested on three public datasets and compared with other hash algorithms, all of which showed satisfactory results. Specifically, the mAP of the adversarial distillation algorithm proposed in this paper is 3% and 2% higher than that of the next best SGH method at 64bits and 128bits hash lengths respectively. The experiments show that the hash retrieval model constructed in this paper has good performance while ensuring lightweight and robustness.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2142289488",
                        "name": "Ping Feng"
                    },
                    {
                        "authorId": "2119078672",
                        "name": "Hanyun Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "When real data is not available (black-box) for inference, attackers can only imitate the target model through querying synthetic examples [18, 20, 27, 37]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "35d3ac7a8a63d842a8a529a4897e3d7c2d4ec9ac",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-00008",
                    "ArXiv": "2202.00008",
                    "CorpusId": 246442324
                },
                "corpusId": 246442324,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/35d3ac7a8a63d842a8a529a4897e3d7c2d4ec9ac",
                "title": "MEGA: Model Stealing via Collaborative Generator-Substitute Networks",
                "abstract": "Deep machine learning models are increasingly deployedin the wild for providing services to users. Adversaries maysteal the knowledge of these valuable models by trainingsubstitute models according to the inference results of thetargeted deployed models. Recent data-free model stealingmethods are shown effective to extract the knowledge of thetarget model without using real query examples, but they as-sume rich inference information, e.g., class probabilities andlogits. However, they are all based on competing generator-substitute networks and hence encounter training instability.In this paper we propose a data-free model stealing frame-work,MEGA, which is based on collaborative generator-substitute networks and only requires the target model toprovide label prediction for synthetic query examples. Thecore of our method is a model stealing optimization con-sisting of two collaborative models (i) the substitute modelwhich imitates the target model through the synthetic queryexamples and their inferred labels and (ii) the generatorwhich synthesizes images such that the confidence of thesubstitute model over each query example is maximized. Wepropose a novel coordinate descent training procedure andanalyze its convergence. We also empirically evaluate thetrained substitute model on three datasets and its applicationon black-box adversarial attacks. Our results show that theaccuracy of our trained substitute model and the adversarialattack success rate over it can be up to 33% and 40% higherthan state-of-the-art data-free black-box attacks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2114044385",
                        "name": "Chi Hong"
                    },
                    {
                        "authorId": "1491248137",
                        "name": "Jiyue Huang"
                    },
                    {
                        "authorId": "14672072",
                        "name": "L. Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For the latter, the two main investigated techniques are (i) adversarially perturbing pure noise samples to minimize the OOD loss [5, 14, 28], and (ii) including a generative adversarial network [13] in the loop [6, 7, 27]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9cf0fa4bbed9abf92047ee32be5e64ee27c1be46",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-06507",
                    "ArXiv": "2201.06507",
                    "CorpusId": 246016086
                },
                "corpusId": 246016086,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9cf0fa4bbed9abf92047ee32be5e64ee27c1be46",
                "title": "Distillation from heterogeneous unlabeled collections",
                "abstract": "Compressing deep networks is essential to expand their range of applications to constrained settings. The need for compression however often arises long after the model was trained, when the original data might no longer be available. On the other hand, unlabeled data, not necessarily related to the target task, is usually plentiful, especially in image classification tasks. In this work, we propose a scheme to leverage such samples to distill the knowledge learned by a large teacher network to a smaller student. The proposed technique relies on (i) preferentially sampling datapoints that appear related, and (ii) taking better advantage of the learning signal. We show that the former speeds up the student's convergence, while the latter boosts its performance, achieving performances closed to what can be expected with the original data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3396405",
                        "name": "Jean-Michel Begon"
                    },
                    {
                        "authorId": "50206577",
                        "name": "P. Geurts"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "25658a7a4b531616dd4329bb59cb586b197c5695",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-05629",
                    "ArXiv": "2201.05629",
                    "DOI": "10.1109/TIFS.2023.3265506",
                    "CorpusId": 246015506
                },
                "corpusId": 246015506,
                "publicationVenue": {
                    "id": "d406a3f4-dc05-43be-b1f6-812f29de9c0e",
                    "name": "IEEE Transactions on Information Forensics and Security",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Inf Forensics Secur"
                    ],
                    "issn": "1556-6013",
                    "url": "http://www.ieee.org/organizations/society/sp/tifs.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=10206",
                        "http://www.signalprocessingsociety.org/publications/periodicals/forensics/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/25658a7a4b531616dd4329bb59cb586b197c5695",
                "title": "Zero-Shot Machine Unlearning",
                "abstract": "Modern privacy regulations grant citizens the right to be forgotten by products, services and companies. In case of machine learning (ML) applications, this necessitates deletion of data not only from storage archives but also from ML models. Due to an increasing need for regulatory compliance required for ML applications, machine unlearning is becoming an emerging research problem. The right to be forgotten requests come in the form of removal of a certain set or class of data from the already trained ML model. Practical considerations preclude retraining of the model from scratch after discarding the deleted data. The few existing studies use either the whole training data, or a subset of training data, or some metadata stored during training to update the model weights for unlearning. However, strict regulatory compliance requires time-bound deletion of data. Thus, in many cases, no data related to the training process or training samples may be accessible even for the unlearning purpose. We therefore ask the question: is it possible to achieve unlearning with zero training samples? In this paper, we introduce the novel problem of zero-shot machine unlearning that caters for the extreme but practical scenario where zero original data samples are available for use. We then propose two novel solutions for zero-shot machine unlearning based on (a) error minimizing-maximizing noise and (b) gated knowledge transfer. These methods remove the information of the forget data from the model while maintaining the model efficacy on the retain data. The zero-shot approach offers good protection against the model inversion attacks and membership inference attacks. We introduce a new evaluation metric, Anamnesis Index (AIN) to effectively measure the quality of the unlearning method. The experiments show promising results for unlearning in deep learning models on benchmark vision data-sets. The source code is available here: https://github.com/ayu987/zero-shot-unlearning",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2141033650",
                        "name": "Vikram S Chundawat"
                    },
                    {
                        "authorId": "2141034263",
                        "name": "Ayush K Tarun"
                    },
                    {
                        "authorId": "1888144",
                        "name": "Murari Mandal"
                    },
                    {
                        "authorId": "145977143",
                        "name": "Mohan S. Kankanhalli"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another one is that they should be optimal to close the information gap between the teacher and the student (Micaelli and Storkey 2019; Fang et al. 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f190deb9b5568fb50a8847712a29c9f2af4739e2",
                "externalIds": {
                    "DBLP": "conf/aaai/BiniciAPLM22",
                    "ArXiv": "2201.03019",
                    "DOI": "10.1609/aaai.v36i6.20556",
                    "CorpusId": 245837482
                },
                "corpusId": 245837482,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/f190deb9b5568fb50a8847712a29c9f2af4739e2",
                "title": "Robust and Resource-Efficient Data-Free Knowledge Distillation by Generative Pseudo Replay",
                "abstract": "Data-Free Knowledge Distillation (KD) allows knowledge transfer from a trained neural network (teacher) to a more compact one (student) in the absence of original training data. Existing works use a validation set to monitor the accuracy of the student over real data and report the highest performance throughout the entire process. However, validation data may not be available at distillation time either, making it infeasible to record the student snapshot that achieved the peak accuracy. Therefore, a practical data-free KD method should be robust and ideally provide monotonically increasing student accuracy during distillation. This is challenging because the student experiences knowledge degradation due to the distribution shift of the synthetic data. A straightforward approach to overcome this issue is to store and rehearse the generated samples periodically, which increases the memory footprint and creates privacy concerns. We propose to model the distribution of the previously observed synthetic samples with a generative network. In particular, we design a Variational Autoencoder (VAE) with a training objective that is customized to learn the synthetic data representations optimally. The student is rehearsed by the generative pseudo replay technique, with samples produced by the VAE. Hence knowledge degradation can be prevented without storing any samples. Experiments on image classification benchmarks show that our method optimizes the expected value of the distilled model accuracy while eliminating the large memory overhead incurred by the sample-storing methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2123039685",
                        "name": "Kuluhan Binici"
                    },
                    {
                        "authorId": "3239134",
                        "name": "Shivam Aggarwal"
                    },
                    {
                        "authorId": "1750135",
                        "name": "N. Pham"
                    },
                    {
                        "authorId": "1740968",
                        "name": "K. Leman"
                    },
                    {
                        "authorId": "144053839",
                        "name": "T. Mitra"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To this end, some researchers [39], [40], [104] study an adversarial exploration framework to synthesize alternative data, as shown in Fig.",
                "Rashid et al. [142] extend the ZSKT [39] to NLP models and introduce out-of-domain data to assist the text classification tasks.",
                "ZSKT [39] achieves data-free knowledge transfer by firstly training an adversarial generator to search for images on which the student poorly matches the teacher, and then using them to train the student.",
                "[142] extend the ZSKT [39] to NLP models and introduce out-of-domain data to assist the text classification tasks.",
                "AVKD [119] extends the ZSKT [39] and formulates the adversarial exploration process as variational autoencoders (VAE)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "85f0e5973166ad5b1d9870214821b08207302cb4",
                "externalIds": {
                    "ArXiv": "2112.15278",
                    "DBLP": "journals/corr/abs-2112-15278",
                    "CorpusId": 245634616
                },
                "corpusId": 245634616,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/85f0e5973166ad5b1d9870214821b08207302cb4",
                "title": "Data-Free Knowledge Transfer: A Survey",
                "abstract": "In the last decade, many deep learning models have been well trained and made a great success in various fields of machine intelligence, especially for computer vision and natural language processing. To better leverage the potential of these well-trained models in intra-domain or cross-domain transfer learning situations, knowledge distillation (KD) and domain adaptation (DA) are proposed and become research highlights. They both aim to transfer useful information from a well-trained model with original training data. However, the original data is not always available in many cases due to privacy, copyright or confidentiality. Recently, the data-free knowledge transfer paradigm has attracted appealing attention as it deals with distilling valuable knowledge from well-trained models without requiring to access to the training data. In particular, it mainly consists of the data-free knowledge distillation (DFKD) and source data-free domain adaptation (SFDA). On the one hand, DFKD aims to transfer the intra-domain knowledge of original data from a cumbersome teacher network to a compact student network for model compression and efficient inference. On the other hand, the goal of SFDA is to reuse the cross-domain knowledge stored in a well-trained source model and adapt it to a target domain. In this paper, we provide a comprehensive survey on data-free knowledge transfer from the perspectives of knowledge distillation and unsupervised domain adaptation, to help readers have a better understanding of the current research status and ideas. Applications and challenges of the two areas are briefly reviewed, respectively. Furthermore, we provide some insights to the subject of future research.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "103483742",
                        "name": "Yuang Liu"
                    },
                    {
                        "authorId": "49039407",
                        "name": "Wei Zhang"
                    },
                    {
                        "authorId": "2152811374",
                        "name": "Jun Wang"
                    },
                    {
                        "authorId": "2115642141",
                        "name": "Jianyong Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Two types of DFKD methods are compared in our experiments: (1) generative methods that train a generative model for synthesis, including DAFL (Chen et al. 2019), ZSKT (Micaelli and Storkey 2019), DFQ (Choi et al. 2020), and Generative DFD (Luo et al. 2020) (2) non-generative methods that craft transfer set in a batch-by-batch manner including DeepInv (Yin et al. 2019) and CMI (Fang et al. 2021b).",
                "\u2026are compared in our experiments: (1) generative methods that train a generative model for synthesis, including DAFL (Chen et al. 2019), ZSKT (Micaelli and Storkey 2019), DFQ (Choi et al. 2020), and Generative DFD (Luo et al. 2020) (2) non-generative methods that craft transfer set in a\u2026"
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "173ab50f2fa45c90c4101e1ba598be85dcd3bb81",
                "externalIds": {
                    "ArXiv": "2112.06253",
                    "CorpusId": 245124555
                },
                "corpusId": 245124555,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/173ab50f2fa45c90c4101e1ba598be85dcd3bb81",
                "title": "Up to 100$\\times$ Faster Data-free Knowledge Distillation",
                "abstract": "Data-free knowledge distillation (DFKD) has recently been attracting increasing attention from research communities, attributed to its capability to compress a model only using synthetic data. Despite the encouraging results achieved, state-of-the-art DFKD methods still suffer from the inefficiency of data synthesis, making the data-free training process extremely time-consuming and thus inapplicable for large-scale tasks. In this work, we introduce an efficacious scheme, termed as FastDFKD, that allows us to accelerate DFKD by a factor of orders of magnitude. At the heart of our approach is a novel strategy to reuse the shared common features in training data so as to synthesize different data instances. Unlike prior methods that optimize a set of data independently, we propose to learn a meta-synthesizer that seeks common features as the initialization for the fast data synthesis. As a result, FastDFKD achieves data synthesis within only a few steps, significantly enhancing the efficiency of data-free training. Experiments over CIFAR, NYUv2, and ImageNet demonstrate that the proposed FastDFKD achieves 10$\\times$ and even 100$\\times$ acceleration while preserving performances on par with state of the art. Code is available at \\url{https://github.com/zju-vipa/Fast-Datafree}.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "150110431",
                        "name": "Gongfan Fang"
                    },
                    {
                        "authorId": "2049406054",
                        "name": "Kanya Mo"
                    },
                    {
                        "authorId": "48631088",
                        "name": "Xinchao Wang"
                    },
                    {
                        "authorId": "40403685",
                        "name": "Jie Song"
                    },
                    {
                        "authorId": "2145158124",
                        "name": "Shitao Bei"
                    },
                    {
                        "authorId": "1739347431",
                        "name": "Haofei Zhang"
                    },
                    {
                        "authorId": "144646841",
                        "name": "Mingli Song"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b383b79e3d4224cc351ecca2190ee40e53381cca",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-01405",
                    "ArXiv": "2112.01405",
                    "CorpusId": 244798569
                },
                "corpusId": 244798569,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b383b79e3d4224cc351ecca2190ee40e53381cca",
                "title": "FedRAD: Federated Robust Adaptive Distillation",
                "abstract": "The robustness of federated learning (FL) is vital for the distributed training of an accurate global model that is shared among large number of clients. The collaborative learning framework by typically aggregating model updates is vulnerable to model poisoning attacks from adversarial clients. Since the shared information between the global server and participants are only limited to model parameters, it is challenging to detect bad model updates. Moreover, real-world datasets are usually heterogeneous and not independent and identically distributed (Non-IID) among participants, which makes the design of such robust FL pipeline more difficult. In this work, we propose a novel robust aggregation method, Federated Robust Adaptive Distillation (FedRAD), to detect adversaries and robustly aggregate local models based on properties of the median statistic, and then performing an adapted version of ensemble Knowledge Distillation. We run extensive experiments to evaluate the proposed method against recently published works. The results show that FedRAD outperforms all other aggregators in the presence of adversaries, as well as in heterogeneous data distributions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2142831118",
                        "name": "Stef'an P'all Sturluson"
                    },
                    {
                        "authorId": "2142820292",
                        "name": "Samuel Trew"
                    },
                    {
                        "authorId": "1399031454",
                        "name": "Luis Mu\u00f1oz-Gonz\u00e1lez"
                    },
                    {
                        "authorId": "1946926633",
                        "name": "Matei Grama"
                    },
                    {
                        "authorId": "1389979241",
                        "name": "Jonathan Passerat-Palmbach"
                    },
                    {
                        "authorId": "1717710",
                        "name": "D. Rueckert"
                    },
                    {
                        "authorId": "3157735",
                        "name": "A. Alansary"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In Table 2, we find that there is something unique about using a single image, as our method outperforms several synthetic datasets, such as FractalDB Kataoka et al. (2020), randomly initialized StyleGAN Baradad et al. (2021), as well as the GAN-based approach of (Micaelli & Storkey, 2019).",
                "These approaches are typically generation based (Chen et al., 2019; Ye et al., 2020; Micaelli & Storkey, 2019; Yin et al., 2020) and e.g. yield datasets of synthetic images that maximally activate neurons in the final layer of teacher."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d059011edb6567798b2e991a6c4a414a2429b0d9",
                "externalIds": {
                    "DBLP": "conf/iclr/AsanoS23",
                    "ArXiv": "2112.00725",
                    "CorpusId": 256194054
                },
                "corpusId": 256194054,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d059011edb6567798b2e991a6c4a414a2429b0d9",
                "title": "The Augmented Image Prior: Distilling 1000 Classes by Extrapolating from a Single Image",
                "abstract": "What can neural networks learn about the visual world when provided with only a single image as input? While any image obviously cannot contain the multitudes of all existing objects, scenes and lighting conditions - within the space of all 256^(3x224x224) possible 224-sized square images, it might still provide a strong prior for natural images. To analyze this `augmented image prior' hypothesis, we develop a simple framework for training neural networks from scratch using a single image and augmentations using knowledge distillation from a supervised pretrained teacher. With this, we find the answer to the above question to be: `surprisingly, a lot'. In quantitative terms, we find accuracies of 94%/74% on CIFAR-10/100, 69% on ImageNet, and by extending this method to video and audio, 51% on Kinetics-400 and 84% on SpeechCommands. In extensive analyses spanning 13 datasets, we disentangle the effect of augmentations, choice of data and network architectures and also provide qualitative evaluations that include lucid `panda neurons' in networks that have never even seen one.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47792365",
                        "name": "Yuki M. Asano"
                    },
                    {
                        "authorId": "9261711",
                        "name": "Aaqib Saeed"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Adversarial belief matching [23] and data-free adversarial distillation [24] methods suggested adversarially training the generator, such that the generated samples become harder to train.",
                "More recent studies have employed generator architectures similar to GAN [21] to generate synthetic samples replacing the original data [22, 23, 24, 25]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b72f590d0992539748b5e106aef447ce470cd535",
                "externalIds": {
                    "ArXiv": "2111.02625",
                    "DBLP": "journals/corr/abs-2111-02625",
                    "CorpusId": 242757321
                },
                "corpusId": 242757321,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b72f590d0992539748b5e106aef447ce470cd535",
                "title": "Qimera: Data-free Quantization with Synthetic Boundary Supporting Samples",
                "abstract": "Model quantization is known as a promising method to compress deep neural networks, especially for inferences on lightweight mobile or edge devices. However, model quantization usually requires access to the original training data to maintain the accuracy of the full-precision models, which is often infeasible in real-world scenarios for security and privacy issues. A popular approach to perform quantization without access to the original data is to use synthetically generated samples, based on batch-normalization statistics or adversarial learning. However, the drawback of such approaches is that they primarily rely on random noise input to the generator to attain diversity of the synthetic samples. We find that this is often insufficient to capture the distribution of the original data, especially around the decision boundaries. To this end, we propose Qimera, a method that uses superposed latent embeddings to generate synthetic boundary supporting samples. For the superposed embeddings to better reflect the original distribution, we also propose using an additional disentanglement mapping layer and extracting information from the full-precision model. The experimental results show that Qimera achieves state-of-the-art performances for various settings on data-free quantization. Code is available at https://github.com/iamkanghyunchoi/qimera.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2116298128",
                        "name": "Kanghyun Choi"
                    },
                    {
                        "authorId": "133833735",
                        "name": "Deokki Hong"
                    },
                    {
                        "authorId": "5166698",
                        "name": "Noseong Park"
                    },
                    {
                        "authorId": "2567514",
                        "name": "Youngsok Kim"
                    },
                    {
                        "authorId": "2108602524",
                        "name": "Jinho Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[30] proposed an adversarial method that trains a generator iteratively to craft images that cause the student to poorly match the teacher and subsequently used them to perform distillation.",
                "Recently several works have identified such issues [4, 30, 31] for classification setting.",
                "The solutions proposed for classification problems either synthesize transfer set directly using the trained Teacher model [31, 49] or learn the target data distribution through generative models [4, 30]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f0fd571f42da5c25025c3c9696bbe672595fbd2d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-14215",
                    "ArXiv": "2110.14215",
                    "CorpusId": 239998341
                },
                "corpusId": 239998341,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/f0fd571f42da5c25025c3c9696bbe672595fbd2d",
                "title": "Beyond Classification: Knowledge Distillation using Multi-Object Impressions",
                "abstract": "Knowledge Distillation (KD) utilizes training data as a transfer set to transfer knowledge from a complex network (Teacher) to a smaller network (Student). Several works have recently identified many scenarios where the training data may not be available due to data privacy or sensitivity concerns and have proposed solutions under this restrictive constraint for the classification task. Unlike existing works, we, for the first time, solve a much more challenging problem, i.e.,\"KD for object detection with zero knowledge about the training data and its statistics\". Our proposed approach prepares pseudo-targets and synthesizes corresponding samples (termed as\"Multi-Object Impressions\"), using only the pretrained Faster RCNN Teacher network. We use this pseudo-dataset as a transfer set to conduct zero-shot KD for object detection. We demonstrate the efficacy of our proposed method through several ablations and extensive experiments on benchmark datasets like KITTI, Pascal and COCO. Our approach with no training samples, achieves a respectable mAP of 64.2% and 55.5% on the student with same and half capacity while performing distillation from a Resnet-18 Teacher of 73.3% mAP on KITTI.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "143747407",
                        "name": "Gaurav Kumar Nayak"
                    },
                    {
                        "authorId": "39719833",
                        "name": "Monish Keswani"
                    },
                    {
                        "authorId": "144985940",
                        "name": "Sharan Seshadri"
                    },
                    {
                        "authorId": "1429640900",
                        "name": "Anirban Chakraborty"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, data-free knowledge distillation [30, 33, 10, 61, 59] has attracted attention from various research communities, which trains student model only with synthetic data.",
                "We compare the proposed MosaicKD to various baselines, including data-free KD methods (DAFL [7], ZSKT [33], DeepInv.",
                "original training data is available during distillation, which is vulnerable in real-world applications due to privacy or copyright reasons [33, 58, 44, 57, 45, 21, 43]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "16aa9ddb681554daf075eb13ba1fe37aff5f151d",
                "externalIds": {
                    "ArXiv": "2110.15094",
                    "DBLP": "conf/nips/FangBSWXSS21",
                    "CorpusId": 240070740
                },
                "corpusId": 240070740,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/16aa9ddb681554daf075eb13ba1fe37aff5f151d",
                "title": "Mosaicking to Distill: Knowledge Distillation from Out-of-Domain Data",
                "abstract": "Knowledge distillation~(KD) aims to craft a compact student model that imitates the behavior of a pre-trained teacher in a target domain. Prior KD approaches, despite their gratifying results, have largely relied on the premise that \\emph{in-domain} data is available to carry out the knowledge transfer. Such an assumption, unfortunately, in many cases violates the practical setting, since the original training data or even the data domain is often unreachable due to privacy or copyright reasons. In this paper, we attempt to tackle an ambitious task, termed as \\emph{out-of-domain} knowledge distillation~(OOD-KD), which allows us to conduct KD using only OOD data that can be readily obtained at a very low cost. Admittedly, OOD-KD is by nature a highly challenging task due to the agnostic domain gap. To this end, we introduce a handy yet surprisingly efficacious approach, dubbed as~\\textit{MosaicKD}. The key insight behind MosaicKD lies in that, samples from various domains share common local patterns, even though their global semantic may vary significantly; these shared local patterns, in turn, can be re-assembled analogous to mosaic tiling, to approximate the in-domain data and to further alleviating the domain discrepancy. In MosaicKD, this is achieved through a four-player min-max game, in which a generator, a discriminator, a student network, are collectively trained in an adversarial manner, partially under the guidance of a pre-trained teacher. We validate MosaicKD over {classification and semantic segmentation tasks} across various benchmarks, and demonstrate that it yields results much superior to the state-of-the-art counterparts on OOD data. Our code is available at \\url{https://github.com/zju-vipa/MosaicKD}.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "150110431",
                        "name": "Gongfan Fang"
                    },
                    {
                        "authorId": "2075377477",
                        "name": "Yifan Bao"
                    },
                    {
                        "authorId": "2143424560",
                        "name": "Jie Song"
                    },
                    {
                        "authorId": "48631088",
                        "name": "Xinchao Wang"
                    },
                    {
                        "authorId": "2054592892",
                        "name": "Don Xie"
                    },
                    {
                        "authorId": "40900125",
                        "name": "Chengchao Shen"
                    },
                    {
                        "authorId": "144646841",
                        "name": "Mingli Song"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a1b7c2142b0db640980d5b337c406837efdf7d84",
                "externalIds": {
                    "DOI": "10.1109/CAC53003.2021.9727869",
                    "CorpusId": 247460965
                },
                "corpusId": 247460965,
                "publicationVenue": {
                    "id": "bfa6e440-6762-47f0-b1b9-2a43c02d9f62",
                    "name": "ACM Cloud and Autonomic Computing Conference",
                    "type": "conference",
                    "alternate_names": [
                        "ACM Cloud Auton Comput Conf",
                        "Chinese Automation Congress",
                        "Chin Autom Congr",
                        "CAC"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a1b7c2142b0db640980d5b337c406837efdf7d84",
                "title": "Continuous Teacher-Student Learning for Class-Incremental SAR Target Identification",
                "abstract": "In this paper, we propose a class-incremental SAR target identification approach based on continuous teacher-student learning. The main challenge of class-incremental SAR target identification is catastrophic forgetting: as the learned model tend to adapt to the most recently seen new identification task, they forget what they have learned before and therefore lose performance on the tasks that were learned previously. Our method aims at introducing the teacher model, which can utilize data from tasks so far, to prevent the student model from catastrophic forgetting. For each task, the teacher model learn to capture the knowledge contained in the tasks by now. When a new task is presented, the student model is encouraged to learn from the teacher model so that the information on which the previous task relied is retained. At the same time, we also make the student model to review its own knowledge to further alleviate catastrophic forgetting. The evaluation of continuous SAR target recognition task shows that this method reduces forgetting effect.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2140026597",
                        "name": "Yuting Lu"
                    },
                    {
                        "authorId": "2833510",
                        "name": "Zaidao Wen"
                    },
                    {
                        "authorId": "2108168246",
                        "name": "Xiaoxu Wang"
                    },
                    {
                        "authorId": "2140196807",
                        "name": "Jiarui Wang"
                    },
                    {
                        "authorId": "144468149",
                        "name": "Q. Pan"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0d8277950a9d0f4ed8216479cb512e40f0631b72",
                "externalIds": {
                    "DBLP": "conf/mm/HaoL0A021",
                    "DOI": "10.1145/3474085.3475329",
                    "CorpusId": 239012132
                },
                "corpusId": 239012132,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0d8277950a9d0f4ed8216479cb512e40f0631b72",
                "title": "Data-Free Ensemble Knowledge Distillation for Privacy-conscious Multimedia Model Compression",
                "abstract": "Recent advances in deep learning bring impressive performance for multimedia applications. Hence, compressing and deploying these applications on resource-limited edge devices via model compression becomes attractive. Knowledge distillation (KD) is one of the most popular model compression techniques. However, most well-behaved KD approaches require the original dataset, which is usually unavailable due to privacy issues, while existing data-free KD methods perform much worse than data-required counterparts. In this paper, we analyze previous data-free KD methods from the data perspective and point out that using a single pre-trained model limits the performance of these approaches. We then propose a Data-Free Ensemble knowledge Distillation (DFED) framework, which contains a student network, a generator network, and multiple pre-trained teacher networks. During training, the student mimics behaviors of the ensemble of teachers using samples synthesized by a generator, which aims to enlarge the prediction discrepancy between the student and teachers. A moment matching loss term assists the generator training by minimizing the distance between activations of synthesized samples and real samples. We evaluate DFED on three popular image classification datasets. Results demonstrate that our method achieves significant performance improvements compared with previous works. We also design an ablation study to verify the effectiveness of each component of the proposed framework.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2147215540",
                        "name": "Zhiwei Hao"
                    },
                    {
                        "authorId": "143610445",
                        "name": "Yong Luo"
                    },
                    {
                        "authorId": "46177189",
                        "name": "Han Hu"
                    },
                    {
                        "authorId": "2151864031",
                        "name": "Jianping An"
                    },
                    {
                        "authorId": "145868454",
                        "name": "Yonggang Wen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In light of increasing data privacy concerns, this alternative has recently enjoyed a surge of interest [47, 8, 53, 40, 36, 31, 82, 1].",
                "Hereby, knowledge is transferred from one [47, 53, 8, 10, 82, 43, 87] or multiple [39] teacher(s) to the student model via the generation of synthetic data, either by optimizing random noise examples [53, 82, 87] or by training a generator network [47, 8, 10, 43].",
                "While these methods rely on the original data, Data-Free Knowledge Distillation (DFKD) methods were recently developed [42, 47, 53, 8].",
                "Although Data-Free Knowledge Distillation (DFKD) methods were developed to transfer knowledge from a teacher model to a student model without any access to the original data [42, 47, 8, 53, 82, 10], only single-teacher scenarios with no domain shift were studied."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "30131a0dcaa236a4c78094653035e801213ea653",
                "externalIds": {
                    "DBLP": "conf/acml/0002CKRT22",
                    "ArXiv": "2110.04545",
                    "CorpusId": 238582914
                },
                "corpusId": 238582914,
                "publicationVenue": {
                    "id": "2486528b-036c-4f3c-953f-c574eb381d12",
                    "name": "Asian Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "Asian Conf Mach Learn",
                        "ACML"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=40"
                },
                "url": "https://www.semanticscholar.org/paper/30131a0dcaa236a4c78094653035e801213ea653",
                "title": "Towards Data-Free Domain Generalization",
                "abstract": "In this work, we investigate the unexplored intersection of domain generalization (DG) and data-free learning. In particular, we address the question: How can knowledge contained in models trained on different source domains be merged into a single model that generalizes well to unseen target domains, in the absence of source and target domain data? Machine learning models that can cope with domain shift are essential for real-world scenarios with often changing data distributions. Prior DG methods typically rely on using source domain data, making them unsuitable for private decentralized data. We define the novel problem of Data-Free Domain Generalization (DFDG), a practical setting where models trained on the source domains separately are available instead of the original datasets, and investigate how to effectively solve the domain generalization problem in that case. We propose DEKAN, an approach that extracts and fuses domain-specific knowledge from the available teacher models into a student model robust to domain shift. Our empirical evaluation demonstrates the effectiveness of our method which achieves first state-of-the-art results in DFDG by significantly outperforming data-free knowledge distillation and ensemble baselines.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2560012",
                        "name": "A. Frikha"
                    },
                    {
                        "authorId": "12775277",
                        "name": "Haokun Chen"
                    },
                    {
                        "authorId": "2614774",
                        "name": "Denis Krompass"
                    },
                    {
                        "authorId": "1727058",
                        "name": "T. Runkler"
                    },
                    {
                        "authorId": "1742501819",
                        "name": "Volker Tresp"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Moreover, the upper-bound of performance for both these works and generative works is often considered to be the student\u2019s performance with full data on supervised training, or on original knowledge distillation [24, 28, 40, 2].",
                "Another line of related work is zero-shot distillation, where synthesized data impressions from the teacher are used as surrogates to train the student [24, 28]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "32429931ab2b1821b545f21c80aa64a4077c65ef",
                "externalIds": {
                    "DBLP": "conf/iccvw/Banitalebi-Dehkordi21",
                    "DOI": "10.1109/ICCVW54120.2021.00091",
                    "CorpusId": 244532187
                },
                "corpusId": 244532187,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/32429931ab2b1821b545f21c80aa64a4077c65ef",
                "title": "Knowledge Distillation for Low-Power Object Detection: A Simple Technique and Its Extensions for Training Compact Models Using Unlabeled Data",
                "abstract": "The existing solutions for object detection distillation rely on the availability of both a teacher model and ground-truth labels. We propose a new perspective to relax this constraint. In our framework, a student is first trained with pseudo labels generated by the teacher, and then fine-tuned using labeled data, if any available. Extensive experiments demonstrate improvements over existing object detection distillation algorithms. In addition, decoupling the teacher and ground-truth distillation in this framework provides interesting properties such as: 1) using unlabeled data to further improve the student\u2019s performance, 2) combining multiple teacher models of different architectures, even with different object categories, and 3) reducing the need for labeled data (with only 20% of COCO labels, this method achieves the same performance as the model trained on the entire set of labels). Furthermore, a by-product of this approach is the potential usage for domain adaptation. We verify these properties through extensive experiments.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1398288379",
                        "name": "Amin Banitalebi-Dehkordi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The concept of data leaving its impression on a trained model has also been observed in prior work in model inversion attacks in computer vision (Micaelli and Storkey 2019; Nayak et al. 2019)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "95aa28dd2e8c1b148cec1cc0faf373b58a2347e7",
                "externalIds": {
                    "ArXiv": "2109.12406",
                    "DBLP": "conf/aaai/SinglaP0CKS22",
                    "DOI": "10.1609/aaai.v36i10.21384",
                    "CorpusId": 237940436
                },
                "corpusId": 237940436,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/95aa28dd2e8c1b148cec1cc0faf373b58a2347e7",
                "title": "MINIMAL: Mining Models for Universal Adversarial Triggers",
                "abstract": "It is well known that natural language models are vulnerable to adversarial attacks, which are mostly input-specific in nature. Recently, it has been shown that there also exist input-agnostic attacks in NLP models, called universal adversarial triggers. However, existing methods to craft universal triggers are data intensive. They require large amounts of data samples to generate adversarial triggers, which are typically inaccessible by attackers. For instance, previous works take 3000 data samples per class for the SNLI dataset to generate adversarial triggers. In this paper, we present a novel data-free approach, MINIMAL, to mine input-agnostic adversarial triggers from models. Using the triggers produced with our data-free algorithm, we reduce the accuracy of Stanford Sentiment Treebank\u2019s positive class from 93.6% to 9.6%. Similarly, for the Stanford Natural LanguageInference (SNLI), our single-word trigger reduces the accuracy of the entailment class from 90.95% to less than 0.6%. Despite being completely data-free, we get equivalent accuracy drops as data-dependent methods",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1478442491",
                        "name": "Swapnil Parekh"
                    },
                    {
                        "authorId": "50793081",
                        "name": "Yaman Kumar Singla"
                    },
                    {
                        "authorId": "2108291106",
                        "name": "Somesh Singh"
                    },
                    {
                        "authorId": "1752041",
                        "name": "Changyou Chen"
                    },
                    {
                        "authorId": "145846953",
                        "name": "Balaji Krishnamurthy"
                    },
                    {
                        "authorId": "1753278",
                        "name": "R. Shah"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fc69dac929a86ae4c9f68e3f38bb2cd88dae0e13",
                "externalIds": {
                    "DBLP": "journals/tnn/FuLZWFZPL23",
                    "DOI": "10.1109/TNNLS.2021.3107317",
                    "CorpusId": 237607356,
                    "PubMed": "34550892"
                },
                "corpusId": 237607356,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fc69dac929a86ae4c9f68e3f38bb2cd88dae0e13",
                "title": "Elastic Knowledge Distillation by Learning From Recollection",
                "abstract": "Model performance can be further improved with the extra guidance apart from the one-hot ground truth. To achieve it, recently proposed recollection-based methods utilize the valuable information contained in the past training history and derive a \u201crecollection\u201d from it to provide data-driven prior to guide the training. In this article, we focus on two fundamental aspects of this method, i.e., recollection construction and recollection utilization. Specifically, to meet the various demands of models with different capacities and at different training periods, we propose to construct a set of recollections with diverse distributions from the same training history. After that, all the recollections collaborate together to provide guidance, which is adaptive to different model capacities, as well as different training periods, according to our similarity-based elastic knowledge distillation (KD) algorithm. Without any external prior to guide the training, our method achieves a significant performance gain and outperforms the methods of the same category, even as well as KD with well-trained teacher. Extensive experiments and further analysis are conducted to demonstrate the effectiveness of our method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2116099385",
                        "name": "Yongjian Fu"
                    },
                    {
                        "authorId": "47319889",
                        "name": "Songyuan Li"
                    },
                    {
                        "authorId": "30558038",
                        "name": "Hanbin Zhao"
                    },
                    {
                        "authorId": "2145532981",
                        "name": "Wenfu Wang"
                    },
                    {
                        "authorId": "96359247",
                        "name": "Weihao Fang"
                    },
                    {
                        "authorId": "2056432541",
                        "name": "Y. Zhuang"
                    },
                    {
                        "authorId": "2069544314",
                        "name": "Zhijie Pan"
                    },
                    {
                        "authorId": "2116226546",
                        "name": "Xi Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It should be mentioned that most prior knowledge distillation approaches perform well when extracting a small student model from a large teacher model, while some recent research [33], [35] has shown that high distillation accuracy can be achieved even when the teacher model has a smaller and different architecture than the student\u2019s."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1cf62103982fb6efe95e19e3ee9795f41896e403",
                "externalIds": {
                    "DBLP": "conf/icdcs/ZhangWY22",
                    "ArXiv": "2109.03775",
                    "DOI": "10.1109/ICDCS54860.2022.00094",
                    "CorpusId": 247996828
                },
                "corpusId": 247996828,
                "publicationVenue": {
                    "id": "ffe5bb5c-04ed-488e-985d-d3a7b39542cf",
                    "name": "IEEE International Conference on Distributed Computing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Distributed Computing Systems",
                        "IEEE Int Conf Distrib Comput Syst",
                        "Int Conf Device Circuit Syst",
                        "ICDCS",
                        "Int Conf Distrib Comput Syst",
                        "International Conference on Devices, Circuits and Systems"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000213/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/1cf62103982fb6efe95e19e3ee9795f41896e403",
                "title": "FedZKT: Zero-Shot Knowledge Transfer towards Resource-Constrained Federated Learning with Heterogeneous On-Device Models",
                "abstract": "Federated learning enables multiple distributed devices to collaboratively learn a shared prediction model without centralizing their on-device data. Most of the current algorithms require comparable individual efforts for local training with the same structure and size of on-device models, which, however, impedes participation from resource-constrained devices. Given the widespread yet heterogeneous devices nowadays, in this paper, we propose an innovative federated learning framework with heterogeneous on-device models through Zero-shot Knowledge Transfer, named by FedZKT. Specifically, FedZKT allows devices to independently determine the on-device models upon their local resources. To achieve knowledge transfer across these heterogeneous on-device models, a zero-shot distillation approach is designed without any prerequisites for private on-device data, which is contrary to certain prior research based on a public dataset or a pre-trained data generator. Moreover, this compute-intensive distillation task is assigned to the server to allow the participation of resource-constrained devices, where a generator is adversarially learned with the ensemble of collected on-device models. The distilled central knowledge is then sent back in the form of the corresponding on-device model parameters, which can be easily absorbed on the device side. Extensive experimental studies demonstrate the effectiveness and robustness of FedZKT towards on-device knowledge agnostic, on-device model heterogeneity, and other challenging federated learning scenarios, such as heterogeneous on-device data and straggler effects.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2131620609",
                        "name": "Lan Zhang"
                    },
                    {
                        "authorId": "2152704660",
                        "name": "Dapeng Wu"
                    },
                    {
                        "authorId": "152162529",
                        "name": "Xiaoyong Yuan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The first one is that, using only newly generated samples to train the student after each time generator\u2019s weights are updated [4, 6, 10, 18, 19], could cause the student network to forget the knowledge it acquired in the earlier steps.",
                "teacher have maximum disagreement [6, 19], could yield a student that is optimal for a different distribution than the original data.",
                "Recognition of this problematic coupling of KD with data has recently attracted attention from the scientific community [19, 5, 18].",
                "2, targeting the generation of novel samples has demonstrated promising results on several benchmarks [6, 19]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "e2b9b322d66d6165e0cc82f9828672b3789d02ad",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-05698",
                    "ArXiv": "2108.05698",
                    "DOI": "10.1109/WACV51458.2022.00368",
                    "CorpusId": 236987212
                },
                "corpusId": 236987212,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/e2b9b322d66d6165e0cc82f9828672b3789d02ad",
                "title": "Preventing Catastrophic Forgetting and Distribution Mismatch in Knowledge Distillation via Synthetic Data",
                "abstract": "With the increasing popularity of deep learning on edge devices, compressing large neural networks to meet the hardware requirements of resource-constrained devices became a significant research direction. Numerous compression methodologies are currently being used to reduce the memory sizes and energy consumption of neural networks. Knowledge distillation (KD) is among such methodologies and it functions by using data samples to transfer the knowledge captured by a large model (teacher) to a smaller one (student). However, due to various reasons, the original training data might not be accessible at the compression stage. Therefore, data-free model compression is an ongoing research problem that has been addressed by various works. In this paper, we point out that catastrophic forgetting is a problem that can potentially be observed in existing data-free distillation methods. Moreover, the sample generation strategies in some of these methods could result in a mismatch between the synthetic and real data distributions. To prevent such problems, we propose a data-free KD framework that maintains a dynamic collection of generated samples over time. Additionally, we add the constraint of matching the real data distribution in sample generation strategies that target maximum information gain. Our experiments demonstrate that we can improve the accuracy of the student models obtained via KD when compared with state-of-the-art approaches on the SVHN, Fashion MNIST and CIFAR100 datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2123039685",
                        "name": "Kuluhan Binici"
                    },
                    {
                        "authorId": "1750135",
                        "name": "N. Pham"
                    },
                    {
                        "authorId": "144053839",
                        "name": "T. Mitra"
                    },
                    {
                        "authorId": "1740968",
                        "name": "K. Leman"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e56332141f5414fc7e38853bab8e3a108024148a",
                "externalIds": {
                    "DBLP": "journals/eswa/KangK21",
                    "MAG": "3134191546",
                    "DOI": "10.1016/J.ESWA.2021.114813",
                    "CorpusId": 233582109
                },
                "corpusId": 233582109,
                "publicationVenue": {
                    "id": "987139ae-a65d-49bb-aaf6-fb764dc40b19",
                    "name": "Expert systems with applications",
                    "type": "journal",
                    "alternate_names": [
                        "Expert syst appl",
                        "Expert Systems With Applications",
                        "Expert Syst Appl"
                    ],
                    "issn": "0957-4174",
                    "url": "https://www.journals.elsevier.com/expert-systems-with-applications/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/expert-systems-with-applications",
                        "http://www.sciencedirect.com/science/journal/09574174"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e56332141f5414fc7e38853bab8e3a108024148a",
                "title": "Data-free knowledge distillation in neural networks for regression",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1617911303",
                        "name": "Myeonginn Kang"
                    },
                    {
                        "authorId": "2356823",
                        "name": "Seokho Kang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Further, [44, 45], also encourage generating samples the student and teacher disagree on."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fb1787f2726b1d3618eb725ee0da83abd727d45d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-08039",
                    "ArXiv": "2107.08039",
                    "CorpusId": 236034208
                },
                "corpusId": 236034208,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fb1787f2726b1d3618eb725ee0da83abd727d45d",
                "title": "Representation Consolidation for Training Expert Students",
                "abstract": "Traditionally, distillation has been used to train a student model to emulate the input/output functionality of a teacher. A more useful goal than emulation, yet under-explored, is for the student to learn feature representations that transfer well to future tasks. However, we observe that standard distillation of task-specific teachers actually *reduces* the transferability of student representations to downstream tasks. We show that a multi-head, multi-task distillation method using an unlabeled proxy dataset and a generalist teacher is sufficient to consolidate representations from task-specific teacher(s) and improve downstream performance, outperforming the teacher(s) and the strong baseline of ImageNet pretrained features. Our method can also combine the representational knowledge of multiple teachers trained on one or multiple domains into a single model, whose representation is improved on all teachers' domain(s).",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109714662",
                        "name": "Zhizhong Li"
                    },
                    {
                        "authorId": "2529423",
                        "name": "Avinash Ravichandran"
                    },
                    {
                        "authorId": "143800213",
                        "name": "Charless C. Fowlkes"
                    },
                    {
                        "authorId": "32235780",
                        "name": "M. Polito"
                    },
                    {
                        "authorId": "3243878",
                        "name": "Rahul Bhotika"
                    },
                    {
                        "authorId": "1715959",
                        "name": "Stefano Soatto"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "03dc32d7c5d578d451870c3da9538e97eaa39690",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-06993",
                    "ArXiv": "2107.06993",
                    "CorpusId": 235899214
                },
                "corpusId": 235899214,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/03dc32d7c5d578d451870c3da9538e97eaa39690",
                "title": "Confidence Conditioned Knowledge Distillation",
                "abstract": "In this paper, a novel confidence conditioned knowledge distillation (CCKD) scheme for transferring the knowledge from a teacher model to a student model is proposed. Existing state-of-the-art methods employ fixed loss functions for this purpose and ignore the different levels of information that need to be transferred for different samples. In addition to that, these methods are also inefficient in terms of data usage. CCKD addresses these issues by leveraging the confidence assigned by the teacher model to the correct class to devise sample-specific loss functions (CCKD-L formulation) and targets (CCKD-T formulation). Further, CCKD improves the data efficiency by employing self-regulation to stop those samples from participating in the distillation process on which the student model learns faster. Empirical evaluations on several benchmark datasets show that CCKD methods achieve at least as much generalization performance levels as other state-of-the-art methods while being data efficient in the process. Student models trained through CCKD methods do not retain most of the misclassifications commited by the teacher model on the training set. Distillation through CCKD methods improves the resilience of the student models against adversarial attacks compared to the conventional KD method. Experiments show at least 3% increase in performance against adversarial attacks for the MNIST and the Fashion MNIST datasets, and at least 6% increase for the CIFAR10 dataset.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2051291542",
                        "name": "Sourav Mishra"
                    },
                    {
                        "authorId": "2000307852",
                        "name": "S. Sundaram"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d6cbc823ab6d2b2ecf65d9253b93ee0bf1ded9cf",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-10575",
                    "ArXiv": "2106.10575",
                    "CorpusId": 235490013
                },
                "corpusId": 235490013,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d6cbc823ab6d2b2ecf65d9253b93ee0bf1ded9cf",
                "title": "EvoGrad: Efficient Gradient-Based Meta-Learning and Hyperparameter Optimization",
                "abstract": "Gradient-based meta-learning and hyperparameter optimization have seen significant progress recently, enabling practical end-to-end training of neural networks together with many hyperparameters. Nevertheless, existing approaches are relatively expensive as they need to compute second-order derivatives and store a longer computational graph. This cost prevents scaling them to larger network architectures. We present EvoGrad, a new approach to meta-learning that draws upon evolutionary techniques to more efficiently compute hypergradients. EvoGrad estimates hypergradient with respect to hyperparameters without calculating second-order gradients, or storing a longer computational graph, leading to significant improvements in efficiency. We evaluate EvoGrad on three substantial recent meta-learning applications, namely cross-domain few-shot learning with feature-wise transformations, noisy label learning with Meta-Weight-Net and low-resource cross-lingual learning with meta representation transformation. The results show that EvoGrad significantly improves efficiency and enables scaling meta-learning to bigger architectures such as from ResNet10 to ResNet34.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1749549997",
                        "name": "Ondrej Bohdal"
                    },
                    {
                        "authorId": "2653152",
                        "name": "Yongxin Yang"
                    },
                    {
                        "authorId": "1697755",
                        "name": "Timothy M. Hospedales"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Generative adversarial networks (GANs) (Goodfellow et al., 2014) are leveraged in (Chen et al., 2019; Micaelli & Storkey, 2019) to solve this task so that pseudo sample synthesis and student network training can be conducted simultaneously."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "923e2bef7957cc4a58619c0debe8f1f860d9430e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-03310",
                    "MAG": "3169059075",
                    "ArXiv": "2106.03310",
                    "CorpusId": 235358983
                },
                "corpusId": 235358983,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/923e2bef7957cc4a58619c0debe8f1f860d9430e",
                "title": "Zero-Shot Knowledge Distillation from a Decision-Based Black-Box Model",
                "abstract": "Knowledge distillation (KD) is a successful approach for deep neural network acceleration, with which a compact network (student) is trained by mimicking the softmax output of a pre-trained high-capacity network (teacher). In tradition, KD usually relies on access to the training samples and the parameters of the white-box teacher to acquire the transferred knowledge. However, these prerequisites are not always realistic due to storage costs or privacy issues in real-world applications. Here we propose the concept of decision-based black-box (DB3) knowledge distillation, with which the student is trained by distilling the knowledge from a black-box teacher (parameters are not accessible) that only returns classes rather than softmax outputs. We start with the scenario when the training set is accessible. We represent a sample's robustness against other classes by computing its distances to the teacher's decision boundaries and use it to construct the soft label for each training sample. After that, the student can be trained via standard KD. We then extend this approach to a more challenging scenario in which even accessing the training data is not feasible. We propose to generate pseudo samples distinguished by the teacher's decision boundaries to the largest extent and construct soft labels for them, which are used as the transfer set. We evaluate our approaches on various benchmark networks and datasets and experiment results demonstrate their effectiveness. Codes are available at: this https URL.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1687515",
                        "name": "Z. Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "26ef01a65f45f3b3b53c286d482e487953174ea4",
                "externalIds": {
                    "DBLP": "conf/cvpr/ZhangC0DXW21",
                    "DOI": "10.1109/CVPR46437.2021.00776",
                    "CorpusId": 235703058
                },
                "corpusId": 235703058,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/26ef01a65f45f3b3b53c286d482e487953174ea4",
                "title": "Data-Free Knowledge Distillation For Image Super-Resolution",
                "abstract": "Convolutional network compression methods require training data for achieving acceptable results, but training data is routinely unavailable due to some privacy and transmission limitations. Therefore, recent works focus on learning efficient networks without original training data, i.e., data-free model compression. Wherein, most of existing algorithms are developed for image recognition or segmentation tasks. In this paper, we study the data-free compression approach for single image super-resolution (SISR) task which is widely used in mobile phones and smart cameras. Specifically, we analyze the relationship between the outputs and inputs from the pre-trained network and explore a generator with a series of loss functions for maximally capturing useful information. The generator is then trained for synthesizing training samples which have similar distribution to that of the original data. To further alleviate the training difficulty of the student network using only the synthetic data, we introduce a progressive distillation scheme. Experiments on various datasets and architectures demonstrate that the pro-posed method is able to be utilized for effectively learning portable student networks without the original data, e.g., with 0.16dB PSNR drop on Set5 for \u00d72 super resolution. Code will be available at https://github.com/huawei-noah/Data-Efficient-Model-Compression.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108440680",
                        "name": "Yiman Zhang"
                    },
                    {
                        "authorId": "2118023932",
                        "name": "Hanting Chen"
                    },
                    {
                        "authorId": "1736061",
                        "name": "Xinghao Chen"
                    },
                    {
                        "authorId": "2115656092",
                        "name": "Yiping Deng"
                    },
                    {
                        "authorId": "1691522",
                        "name": "Chunjing Xu"
                    },
                    {
                        "authorId": "2108702980",
                        "name": "Yunhe Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3d91ed05e66ffebef989fe6e3c08f7c6f65edcc7",
                "externalIds": {
                    "ArXiv": "2105.12151",
                    "DBLP": "conf/ijcai/ZhuHPLA21",
                    "DOI": "10.24963/ijcai.2021/478",
                    "CorpusId": 235196043
                },
                "corpusId": 235196043,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/3d91ed05e66ffebef989fe6e3c08f7c6f65edcc7",
                "title": "AutoReCon: Neural Architecture Search-based Reconstruction for Data-free Compression",
                "abstract": "Data-free compression raises a new challenge because the original training dataset for a pre-trained model to be compressed is not available due to privacy or transmission issues. Thus, a common approach is to compute a reconstructed training dataset before compression. The current reconstruction methods compute the reconstructed training dataset with a generator by exploiting information from the pre-trained model. However, current reconstruction methods focus on extracting more information from the pre-trained model but do not leverage network engineering. This work is the first to consider network engineering as an approach to design the reconstruction method. Specifically, we propose the AutoReCon method, which is a neural architecture search-based reconstruction method. In the proposed AutoReCon method, the generator architecture is designed automatically given the pre-trained model for reconstruction. Experimental results show that using generators discovered by the AutoRecon method always improve the performance of data-free compression.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "30573335",
                        "name": "Baozhou Zhu"
                    },
                    {
                        "authorId": "11983170",
                        "name": "P. Hofstee"
                    },
                    {
                        "authorId": "8751826",
                        "name": "J. Peltenburg"
                    },
                    {
                        "authorId": "2108602524",
                        "name": "Jinho Lee"
                    },
                    {
                        "authorId": "1387519224",
                        "name": "Z. Al-Ars"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another line of related work is zero-shot distillation, where synthesized data impressions from the teacher are used as surrogates to train the student [24, 28].",
                "Moreover, the upper-bound of performance for both these works and generative works is often considered to be the student\u2019s performance with full data on supervised training, or on original knowledge distillation [2, 24, 28, 40]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "162be0afad8c2c5fa0caa7f1496cb8d4707cb0c8",
                "externalIds": {
                    "ArXiv": "2105.10633",
                    "DBLP": "journals/corr/abs-2105-10633",
                    "CorpusId": 235166347
                },
                "corpusId": 235166347,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/162be0afad8c2c5fa0caa7f1496cb8d4707cb0c8",
                "title": "Revisiting Knowledge Distillation for Object Detection",
                "abstract": "The existing solutions for object detection distillation rely on the availability of both a teacher model and ground-truth labels. We propose a new perspective to relax this constraint. In our framework, a student is first trained with pseudo labels generated by the teacher, and then fine-tuned using labeled data, if any available. Extensive experiments demonstrate improvements over existing object detection distillation algorithms. In addition, decoupling the teacher and ground-truth distillation in this framework provides interesting properties such: as 1) using unlabeled data to further improve the student's performance, 2) combining multiple teacher models of different architectures, even with different object categories, and 3) reducing the need for labeled data (with only 20% of COCO labels, this method achieves the same performance as the model trained on the entire set of labels). Furthermore, a by-product of this approach is the potential usage for domain adaptation. We verify these properties through extensive experiments.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1398288379",
                        "name": "Amin Banitalebi-Dehkordi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Along the same spirit, (Micaelli & Storkey, 2019) learns a generator by adversarial training.",
                "Later there emerges data-free KD approaches which aim to reconstruct samples used for training the teacher (Yoo et al., 2019; Micaelli & Storkey, 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3fdcf2d8fe58dcbc2c353e5974e2030f8281c799",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-10056",
                    "ArXiv": "2105.10056",
                    "CorpusId": 235125689,
                    "PubMed": "35480385"
                },
                "corpusId": 235125689,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3fdcf2d8fe58dcbc2c353e5974e2030f8281c799",
                "title": "Data-Free Knowledge Distillation for Heterogeneous Federated Learning",
                "abstract": "Federated Learning (FL) is a decentralized machine-learning paradigm in which a global server iteratively aggregates the model parameters of local users without accessing their data. User heterogeneity has imposed significant challenges to FL, which can incur drifted global models that are slow to converge. Knowledge Distillation has recently emerged to tackle this issue, by refining the server model using aggregated knowledge from heterogeneous users, other than directly aggregating their model parameters. This approach, however, depends on a proxy dataset, making it impractical unless such prerequisite is satisfied. Moreover, the ensemble knowledge is not fully utilized to guide local model learning, which may in turn affect the quality of the aggregated model. In this work, we propose a data-free knowledge distillation approach to address heterogeneous FL, where the server learns a lightweight generator to ensemble user information in a data-free manner, which is then broadcasted to users, regulating local training using the learned knowledge as an inductive bias. Empirical studies powered by theoretical implications show that, our approach facilitates FL with better generalization performance using fewer communication rounds, compared with the state-of-the-art.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "30451228",
                        "name": "Zhuangdi Zhu"
                    },
                    {
                        "authorId": "2110805917",
                        "name": "Junyuan Hong"
                    },
                    {
                        "authorId": "145487992",
                        "name": "Jiayu Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Adversarial Distillation is motivated by robust optimization, where the x is forced to produce large disagreement between teacher ft(x; \u03b8t) and student fs(x; \u03b8s) [Micaelli and Storkey, 2019; Fang et al., 2019], i.",
                "Adversarial training is motivated by robust optimization, where worst-case samples are synthesized for student learning [Micaelli and Storkey, 2019; Fang et al., 2019].",
                "\u2026Distillation is motivated by robust optimization, where the x is forced to produce large disagreement between teacher ft(x; \u03b8t) and student fs(x; \u03b8s) [Micaelli and Storkey, 2019; Fang et al., 2019], i.e., maximize a Kullback\u2013Leibler divergence term:\nLadv(x) = \u2212KL(ft(x)/\u03c4\u2016fs(x)/\u03c4) (3) Unified\u2026",
                "We compare our approach with the following baselines: DAFL [Chen et al., 2019], ZSKT [Micaelli and Storkey, 2019], ADI [Yin et al., 2020], DFQ [Choi et al., 2020] and\nLS-GDFD [Luo et al., 2020]."
            ],
            "intents": [
                "methodology",
                "background",
                "result"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "dace340be1d8aaaf11e555e44edb24384221fc36",
                "externalIds": {
                    "ArXiv": "2105.08584",
                    "DBLP": "journals/corr/abs-2105-08584",
                    "CorpusId": 234763273
                },
                "corpusId": 234763273,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/dace340be1d8aaaf11e555e44edb24384221fc36",
                "title": "Contrastive Model Inversion for Data-Free Knowledge Distillation",
                "abstract": "Model inversion, whose goal is to recover training data from a pre-trained model, has been recently proved feasible. However, existing inversion methods usually suffer from the mode collapse problem, where the synthesized instances are highly similar to each other and thus show limited effectiveness for downstream tasks, such as knowledge distillation. In this paper, we propose Contrastive Model Inversion~(CMI), where the data diversity is explicitly modeled as an optimizable objective, to alleviate the mode collapse issue. Our main observation is that, under the constraint of the same amount of data, higher data diversity usually indicates stronger instance discrimination. To this end, we introduce in CMI a contrastive learning objective that encourages the synthesizing instances to be distinguishable from the already synthesized ones in previous batches. Experiments of pre-trained models on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that CMI not only generates more visually plausible instances than the state of the arts, but also achieves significantly superior performance when the generated data are used for knowledge distillation. Code is available at \\url{https://github.com/zju-vipa/DataFree}.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "150110431",
                        "name": "Gongfan Fang"
                    },
                    {
                        "authorId": "40403685",
                        "name": "Jie Song"
                    },
                    {
                        "authorId": "48631088",
                        "name": "Xinchao Wang"
                    },
                    {
                        "authorId": "144600668",
                        "name": "Chen Shen"
                    },
                    {
                        "authorId": "2144802278",
                        "name": "Xingen Wang"
                    },
                    {
                        "authorId": "144646841",
                        "name": "Mingli Song"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Such low entropy predictions are characteristics of the members of Dtr, however, non-members with low entropy can be obtained (or generated using GANs (Micaelli and Storkey 2019)) due to large input feature space.",
                "Finally, to achieve the desired tradeoffs, we give a criterion to tune the selection or generation (e.g., using GANs) of reference data used in DMP.\nNotations Dtr is a private training data."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7750e0f88603992999b8079fb624b83a0f508741",
                "externalIds": {
                    "DBLP": "conf/aaai/ShejwalkarH21",
                    "DOI": "10.1609/aaai.v35i11.17150",
                    "CorpusId": 235349092
                },
                "corpusId": 235349092,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/7750e0f88603992999b8079fb624b83a0f508741",
                "title": "Membership Privacy for Machine Learning Models Through Knowledge Transfer",
                "abstract": "Large capacity machine learning (ML) models are prone to membership inference attacks (MIAs), which aim to infer whether the target sample is a member of the target model's training dataset. The serious privacy concerns due to the membership inference have motivated multiple defenses against MIAs, e.g., differential privacy and adversarial regularization. Unfortunately, these defenses produce ML models with unacceptably low classification performances.\n\nOur work proposes a new defense, called distillation for membership privacy (DMP), against MIAs that preserves the utility of the resulting models significantly better than prior defenses. DMP leverages knowledge distillation to train ML models with membership privacy. We provide a novel criterion to tune the data used for knowledge transfer in order to amplify the membership privacy of DMP.\n\nOur extensive evaluation shows that DMP provides significantly better tradeoffs between membership privacy and classification accuracies compared to state-of-the-art MIA defenses. For instance, DMP achieves ~100% accuracy improvement over adversarial regularization for DenseNet trained on CIFAR100, for similar membership privacy (measured using MIA risk): when the MIA risk is 53.7%, adversarially regularized DenseNet is 33.6% accurate, while DMP-trained DenseNet is 65.3% accurate. We have released our code at github.com/vrt1shjwlkr/AAAI21-MIA-Defense.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "148318826",
                        "name": "Virat Shejwalkar"
                    },
                    {
                        "authorId": "1972973",
                        "name": "A. Houmansadr"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2ad67ffe7e6abd732d6501ef99d40f65168700a4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-07519",
                    "ArXiv": "2105.07519",
                    "DOI": "10.24963/ijcai.2021/320",
                    "CorpusId": 234742506
                },
                "corpusId": 234742506,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/2ad67ffe7e6abd732d6501ef99d40f65168700a4",
                "title": "Graph-Free Knowledge Distillation for Graph Neural Networks",
                "abstract": "Knowledge distillation (KD) transfers knowledge from a teacher network to a student by enforcing the student to mimic the outputs of the pretrained teacher on training data. However, data samples are not always accessible in many cases due to large data sizes, privacy, or confidentiality. Many efforts have been made on addressing this problem for convolutional neural networks (CNNs) whose inputs lie in a grid domain within a continuous space such as images and videos, but largely overlook graph neural networks (GNNs) that handle non-grid data with different topology structures within a discrete space. The inherent differences between their inputs make these CNN-based approaches not applicable to GNNs. In this paper, we propose to our best knowledge the first dedicated approach to distilling knowledge from a GNN without graph data. The proposed graph-free KD (GFKD) learns graph topology structures for knowledge transfer by modeling them with multinomial distribution. We then introduce a gradient estimator to optimize this framework. Essentially, the gradients w.r.t. graph structures are obtained by only using GNN forward-propagation without back-propagation, which means that GFKD is compatible with modern GNN libraries such as DGL and Geometric. Moreover, we provide the strategies for handling different types of prior knowledge in the graph data or the GNNs. Extensive experiments demonstrate that GFKD achieves the state-of-the-art performance for distilling knowledge from GNNs without training data.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2150478789",
                        "name": "Xiang Deng"
                    },
                    {
                        "authorId": "2118748124",
                        "name": "Zhongfei Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "When ZSL is implemented via KD strategies, it is common to use data synthesis techniques through generative adversarial frameworks where the generator generates fake samples [19, 20] or through KD using activation or output statistics from pre-trained teacher models to synthesize pseudo samples [21, 22]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a14d274ac3940c4d195eb106d3b47ea02ffbf3ad",
                "externalIds": {
                    "ArXiv": "2105.03544",
                    "DBLP": "conf/waspaa/KimK21",
                    "DOI": "10.1109/WASPAA52581.2021.9632771",
                    "CorpusId": 234341879
                },
                "corpusId": 234341879,
                "publicationVenue": {
                    "id": "0bf449f6-ac85-4357-b5a8-0d5dc353c203",
                    "name": "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Workshop Appl Signal Process Audio Acoust",
                        "Workshop on Applications of Signal Processing to Audio and Acoustics",
                        "Workshop Appl Signal Process Audio Acoust",
                        "WASPAA"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=3007"
                },
                "url": "https://www.semanticscholar.org/paper/a14d274ac3940c4d195eb106d3b47ea02ffbf3ad",
                "title": "Test-Time Adaptation Toward Personalized Speech Enhancement: Zero-Shot Learning with Knowledge Distillation",
                "abstract": "In realistic speech enhancement settings for end-user devices, we often encounter only a few speakers and noise types that tend to reoccur in the specific acoustic environment. We propose a novel personalized speech enhancement method to adapt a compact denoising model to the test-time specificity. Our goal in this test-time adaptation is to utilize no clean speech target of the test speaker, thus fulfilling the requirement for zero-shot learning. To complement the lack of clean speech, we employ the knowledge distillation framework: we distill the more advanced denoising results from an overly large teacher model, and use them as the pseudo target to train the small student model. This zero-shot learning procedure circumvents the process of collecting users' clean speech, a process that users are reluctant to comply due to privacy concerns and technical difficulty of recording clean voice. Experiments on various test-time conditions show that the proposed personalization method can significantly improve the compact models' performance during the test time. Furthermore, since the personalized models outperform larger non-personalized baseline models, we claim that personalization achieves model compression with no loss of denoising performance. As expected, the student models underperform the state-of-the-art teacher models.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109652166",
                        "name": "Sunwoo Kim"
                    },
                    {
                        "authorId": "33752120",
                        "name": "Minje Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The adversary has complete access to the victim model, and uses data-free knowledge transfer (Micaelli & Storkey, 2019; Fang et al., 2019) to train a student model."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b3086fbbc678a7616ac390a41945f45e0d0ab001",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-10706",
                    "ArXiv": "2104.10706",
                    "CorpusId": 231609191
                },
                "corpusId": 231609191,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b3086fbbc678a7616ac390a41945f45e0d0ab001",
                "title": "Dataset Inference: Ownership Resolution in Machine Learning",
                "abstract": "With increasingly more data and computation involved in their training, machine learning models constitute valuable intellectual property. This has spurred interest in model stealing, which is made more practical by advances in learning with partial, little, or no supervision. Existing defenses focus on inserting unique watermarks in a model's decision surface, but this is insufficient: the watermarks are not sampled from the training distribution and thus are not always preserved during model stealing. In this paper, we make the key observation that knowledge contained in the stolen model's training set is what is common to all stolen copies. The adversary's goal, irrespective of the attack employed, is always to extract this knowledge or its by-products. This gives the original model's owner a strong advantage over the adversary: model owners have access to the original training data. We thus introduce $dataset$ $inference$, the process of identifying whether a suspected model copy has private knowledge from the original model's dataset, as a defense against model stealing. We develop an approach for dataset inference that combines statistical testing with the ability to estimate the distance of multiple data points to the decision boundary. Our experiments on CIFAR10, SVHN, CIFAR100 and ImageNet show that model owners can claim with confidence greater than 99% that their model (or dataset as a matter of fact) was stolen, despite only exposing 50 of the stolen model's training points. Dataset inference defends against state-of-the-art attacks even when the adversary is adaptive. Unlike prior work, it does not require retraining or overfitting the defended model.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "153742303",
                        "name": "Pratyush Maini"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Instead of distilling teacher knowledge on a given training dataset, data-free knowledge distillation (DFKD) [30, 35, 3, 33, 8, 47] first generates training data and then learns a student network on this generated dataset.",
                "Training data can be generated by aligning feature statistics [30, 8, 47], enforcing high teacher confidence [30, 35, 3, 8, 47], and adversarial generation of hard examples for the student [33, 47]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "31fac8d48ee0f1dd40dc66bd5c63471dd9eb4827",
                "externalIds": {
                    "MAG": "3153080569",
                    "ArXiv": "2104.10602",
                    "DBLP": "journals/corr/abs-2104-10602",
                    "DOI": "10.1109/CVPR46437.2021.01361",
                    "CorpusId": 233324187
                },
                "corpusId": 233324187,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/31fac8d48ee0f1dd40dc66bd5c63471dd9eb4827",
                "title": "Visualizing Adapted Knowledge in Domain Transfer",
                "abstract": "A source model trained on source data and a target model learned through unsupervised domain adaptation (UDA) usually encode different knowledge. To understand the adaptation process, we portray their knowledge difference with image translation. Specifically, we feed a translated image and its original version to the two models respectively, formulating two branches. Through updating the translated image, we force similar outputs from the two branches. When such requirements are met, differences between the two images can compensate for and hence represent the knowledge difference between models. To enforce similar outputs from the two branches and depict the adapted knowledge, we propose a source-free image translation method that generates source-style images using only target images and the two models. We visualize the adapted knowledge on several datasets with different UDA methods and find that generated images successfully capture the style difference between the two domains. For application, we show that generated images enable further tuning of the target model without accessing source data. Code available at https://github.com/houyz/DA_visualization.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "80447592",
                        "name": "Yunzhong Hou"
                    },
                    {
                        "authorId": "144802394",
                        "name": "Liang Zheng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Multiple works have also looked into training GANs given only a pretrained model [6, 34], but result in images that lack details or perceptual similarities to original data."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0fd50e1483f761ba2bae44de54c5e8db6e35de5a",
                "externalIds": {
                    "DBLP": "conf/cvpr/YinMVAKM21",
                    "ArXiv": "2104.07586",
                    "DOI": "10.1109/CVPR46437.2021.01607",
                    "CorpusId": 233241017
                },
                "corpusId": 233241017,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0fd50e1483f761ba2bae44de54c5e8db6e35de5a",
                "title": "See through Gradients: Image Batch Recovery via GradInversion",
                "abstract": "Training deep neural networks requires gradient estimation from data batches to update parameters. Gradients per parameter are averaged over a set of data and this has been presumed to be safe for privacy-preserving training in joint, collaborative, and federated learning applications. Prior work only showed the possibility of recovering input data given gradients under very restrictive conditions \u2013 a single input point, or a network with no non-linearities, or a small 32 \u00d7 32 px input batch. Therefore, averaging gradients over larger batches was thought to be safe. In this work, we introduce GradInversion, using which input images from a larger batch (8 \u2013 48 images) can also be recovered for large networks such as ResNets (50 layers), on complex datasets such as ImageNet (1000 classes, 224 \u00d7 224 px). We formulate an optimization task that converts random noise into natural images, matching gradients while regularizing image fidelity. We also propose an algorithm for target class label recovery given gradients. We further propose a group consistency regularization framework, where multiple agents starting from different random seeds work together to find an enhanced reconstruction of the original data batch. We show that gradients encode a surprisingly large amount of information, such that all the individual images can be recovered with high fidelity via GradInversion, even for complex datasets, deep networks, and large batch sizes.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1989015",
                        "name": "Hongxu Yin"
                    },
                    {
                        "authorId": "36508529",
                        "name": "Arun Mallya"
                    },
                    {
                        "authorId": "3214848",
                        "name": "Arash Vahdat"
                    },
                    {
                        "authorId": "2974008",
                        "name": "J. \u00c1lvarez"
                    },
                    {
                        "authorId": "1690538",
                        "name": "J. Kautz"
                    },
                    {
                        "authorId": "2824500",
                        "name": "Pavlo Molchanov"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[33] conduct pilot research on data-free knowledge distillation.",
                "[33] further distill knowledge by generated samples from an adversarial generator.",
                ", ZSKT [33], KEGNET [36], DAFL [32], DFAD [38], DFQ [58] and CMI [59], achieve the test accuracies of 85.",
                "As can be seen, our approach achieves 42.4% mIoU on Cityscapes, which gains the improvements of DFAD, DAFL and ZSKT by 3%, 12.3% and 36.1%, respectively.",
                ", ZSKT [33], KEGNET [36], DAFL [32], DFAD [38], DFQ [58] and CMI [59].",
                "For baselines, we adopt the vanilla KD [24] method and several very recent data-free knowledge distillation works that achieve strong performance, including two non-generative based methods ZSKD [34] and BNS [39], and some generative based data-free methods, i.e., ZSKT [33], KEGNET [36], DAFL [32], DFAD [38], DFQ [58] and CMI [59].",
                "Zero-Shot Transfer(ZSKT) [33] ResNet-18 \u2013 85.",
                "The other generative based data-free methods, i.e., ZSKT [33], KEGNET [36], DAFL [32], DFAD [38], DFQ [58] and CMI [59], achieve the test accuracies of 85.95%, 91.83%, 92.22%, 93.30%, 93.26% and 94.08% on CIFAR-10 dataset, 66.29%, 73.91%, 74.47%, 69.43%, 67.01% and 74.01% on CIFAR-100 dataset.",
                "For VOC dataset, our method yields mIoU 40.7%, which boosts the performance of DFAD, DAFL and ZSKT by 5%, 11.4% and 29.9%, respectively.",
                "[33] also utilize a simple adversarial fashion to train a generator for searching images on which the student network poorly matches the teacher network.",
                "[33] propose to search images that poorly match the teacher network."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "390417d959b7853e5f082e8472b3f196f7235a7d",
                "externalIds": {
                    "DBLP": "journals/mlc/ZhaoSDMZY22",
                    "ArXiv": "2104.05382",
                    "DOI": "10.1007/s13042-021-01443-0",
                    "CorpusId": 233210081
                },
                "corpusId": 233210081,
                "publicationVenue": {
                    "id": "a0c45882-7c78-4f0c-8886-d3481ba02586",
                    "name": "International Journal of Machine Learning and Cybernetics",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Mach Learn Cybern"
                    ],
                    "issn": "1868-8071",
                    "url": "http://www.springer.com/engineering/mathematical/journal/13042",
                    "alternate_urls": [
                        "https://link.springer.com/journal/13042"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/390417d959b7853e5f082e8472b3f196f7235a7d",
                "title": "Dual discriminator adversarial distillation for data-free model compression",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50981688",
                        "name": "Haoran Zhao"
                    },
                    {
                        "authorId": "144326521",
                        "name": "Xin Sun"
                    },
                    {
                        "authorId": "1964397",
                        "name": "Junyu Dong"
                    },
                    {
                        "authorId": "2185595070",
                        "name": "Milos Manic"
                    },
                    {
                        "authorId": "46544755",
                        "name": "Huiyu Zhou"
                    },
                    {
                        "authorId": "145429878",
                        "name": "Hui Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The student model is trained simultaneously with the generator via KD. Adversarial Belief Matching (ABM) was proposed in (Micaelli and Storkey 2019), which trains a generative adversarial network (Goodfellow et al. 2014) to search for samples on which the student model poorly matches the teacher,\u2026",
                "Adversarial Belief Matching (ABM) was proposed in (Micaelli and Storkey 2019), which trains a generative adversarial network (Goodfellow et al.",
                "Although a few studies have been proposed (Nayak et al. 2019; Chen et al. 2019; Micaelli and Storkey 2019), KD in the absence of prior training data is still not well studied and there are clear opportunities to improve on the performance of existing approaches."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3827fb3155c316a01b0e42877177dcf3fcf8df05",
                "externalIds": {
                    "ArXiv": "2104.04868",
                    "DBLP": "conf/aaai/Wang21b",
                    "DOI": "10.1609/aaai.v35i11.17228",
                    "CorpusId": 233210322
                },
                "corpusId": 233210322,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/3827fb3155c316a01b0e42877177dcf3fcf8df05",
                "title": "Data-Free Knowledge Distillation with Soft Targeted Transfer Set Synthesis",
                "abstract": "Knowledge distillation (KD) has proved to be an effective approach for deep neural network compression, which learns a compact network (student) by transferring the knowledge from a pre-trained, over-parameterized network (teacher). In traditional KD, the transferred knowledge is usually obtained by feeding training samples to the teacher network to obtain the class probabilities. However, the original training dataset is not always available due to storage costs or privacy issues. In this study, we propose a novel data-free KD approach by modeling the intermediate feature space of the teacher with a multivariate normal distribution and leveraging the soft targeted labels generated by the distribution to synthesize pseudo samples as the transfer set. Several student networks trained with these synthesized transfer sets present competitive performance compared to the networks trained with the original training set and other data-free KD approaches.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1687515",
                        "name": "Z. Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent works [13]\u2013[15] have demonstrated that KD can pass the knowledge in a teacher pre-trained with specific domains to a student, while unseen domains can also be added to increase the robustness of models and reduce over-fitting on seen domains.",
                "Recent works show that KD yields a great success in preserving previously learned knowledge when learning new knowledge and confessing robustness to pretrained models with unseen data [13]\u2013[16]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "18df40d33b14ef7b243bd1a1b6ac50eb776c4b53",
                "externalIds": {
                    "DBLP": "conf/icde/DaiZ021",
                    "DOI": "10.1109/ICDE51399.2021.00152",
                    "CorpusId": 235618582
                },
                "corpusId": 235618582,
                "publicationVenue": {
                    "id": "764e3630-ddac-4c21-af4b-9d32ffef082e",
                    "name": "IEEE International Conference on Data Engineering",
                    "type": "conference",
                    "alternate_names": [
                        "ICDE",
                        "Int Conf Data Eng",
                        "IEEE Int Conf Data Eng",
                        "International Conference on Data Engineering"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1331"
                },
                "url": "https://www.semanticscholar.org/paper/18df40d33b14ef7b243bd1a1b6ac50eb776c4b53",
                "title": "Automatic Webpage Briefing",
                "abstract": "We introduce the task of webpage briefing (WB) to provide a summary of a webpage in a hierarchical manner, from the broad topic of the webpage, to finer level key attributes. A straightforward approach for this task is to train a machine learning model for generating topics and extracting key attributes. However, such a model may not perform well on webpages that are from domains not seen in the training data. An ideal model should be able to adapt to unseen domains while preserving knowledge learned from the seen domains. Knowledge distillation (KD) offers a potential solution, in which a teacher pre-trained with specific domains can pass the knowledge to a student, while unseen domains can also be added to increase the robustness of the models. However, existing works usually assume the models have no access to seen domains during distillation and the knowledge on seen domains may be lost. In our setting, we have access to the generated topics, which contain representative knowledge of seen domains and can help preserve that knowledge during distillation. Moreover, a vanilla KD does not pass on the knowledge about the location patterns of the informative contents in webpages, which are essential for identifying the topics to be generated or the key attributes to be extracted. To preserve more knowledge of seen domains and to better utilize the location patterns, we propose a Dual Distillation model which consists of identification distillation (ID) and understanding distillation (UD); ID distills knowledge on the identification of informative contents under the guidance of the learned topics of seen domains, while UD distills knowledge on topic generation or key attribute extraction. Since topics and key attributes are distilled separately in two students in Dual Distillation, the inherent correlations between them are not utilized. To better exploit such correlations, we propose a Triple Distillation model which consists of a shared ID and two UDs, one for topic generation and the other for key attribute extraction. We further propose a joint model for WB with signal enhancement and exchange among a key attribute extractor, a topic generator, and an informative section predictor. Experiments on real-world webpages show that our models achieve high performances for WB, and validate the superiority of Dual Distillation and Triple Distillation in their target settings. Experiments also show that the proposed joint model outperforms single-task baselines and other joint models.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "20636865",
                        "name": "Yimeng Dai"
                    },
                    {
                        "authorId": "2118403427",
                        "name": "Rui Zhang"
                    },
                    {
                        "authorId": "39899794",
                        "name": "Jianzhong Qi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a633ea2dea6a9bd24942e1f7c0cea4f7a94670df",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-16372",
                    "ArXiv": "2103.16372",
                    "DOI": "10.1109/CVPR46437.2021.00127",
                    "CorpusId": 232417486
                },
                "corpusId": 232417486,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a633ea2dea6a9bd24942e1f7c0cea4f7a94670df",
                "title": "Source-Free Domain Adaptation for Semantic Segmentation",
                "abstract": "Unsupervised Domain Adaptation (UDA) can tackle the challenge that convolutional neural network (CNN)-based approaches for semantic segmentation heavily rely on the pixel-level annotated data, which is labor-intensive. However, existing UDA approaches in this regard inevitably require the full access to source datasets to reduce the gap between the source and target domains during model adaptation, which are impractical in the real scenarios where the source datasets are private, and thus cannot be released along with the well-trained source models. To cope with this issue, we propose a source-free domain adaptation framework for semantic segmentation, namely SFDA, in which only a well-trained source model and an unlabeled target domain dataset are available for adaptation. SFDA not only enables to recover and preserve the source domain knowledge from the source model via knowledge transfer during model adaptation, but also distills valuable information from the target domain for self-supervised learning. The pixel-and patch-level optimization objectives tailored for semantic segmentation are seamlessly integrated in the framework. The extensive experimental results on numerous benchmark datasets highlight the effectiveness of our framework against the existing UDA approaches relying on source data.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "103483742",
                        "name": "Yuang Liu"
                    },
                    {
                        "authorId": "49039407",
                        "name": "Wei Zhang"
                    },
                    {
                        "authorId": "2152811374",
                        "name": "Jun Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "27ce83180648b63f045d531036055cd6608eebe5",
                "externalIds": {
                    "DBLP": "conf/cvpr/ShenYWLSS21",
                    "ArXiv": "2103.00430",
                    "DOI": "10.1109/CVPR46437.2021.00336",
                    "CorpusId": 232075932
                },
                "corpusId": 232075932,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/27ce83180648b63f045d531036055cd6608eebe5",
                "title": "Training Generative Adversarial Networks in One Stage",
                "abstract": "Generative Adversarial Networks (GANs) have demonstrated unprecedented success in various image generation tasks. The encouraging results, however, come at the price of a cumbersome training process, during which the generator and discriminator are alternately updated in two stages. In this paper, we investigate a general training scheme that enables training GANs efficiently in only one stage. Based on the adversarial losses of the generator and discriminator, we categorize GANs into two classes, Symmetric GANs and Asymmetric GANs, and introduce a novel gradient decomposition method to unify the two, allowing us to train both classes in one stage and hence alleviate the training effort. We also computationally analyze the efficiency of the proposed method, and empirically demonstrate that, the proposed method yields a solid 1.5\u00d7 acceleration across various datasets and network architectures. Furthermore, we show that the proposed method is readily applicable to other adversarial-training scenarios, such as data-free knowledge distillation. The code is available at https://github.com/zju-vipa/OSGAN.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "40900125",
                        "name": "Chengchao Shen"
                    },
                    {
                        "authorId": "2034017386",
                        "name": "Youtan Yin"
                    },
                    {
                        "authorId": "48631088",
                        "name": "Xinchao Wang"
                    },
                    {
                        "authorId": "2108184787",
                        "name": "Xubin Li"
                    },
                    {
                        "authorId": "40403685",
                        "name": "Jie Song"
                    },
                    {
                        "authorId": "144646841",
                        "name": "Mingli Song"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018), data-free distillation (Lopes et al., 2017; Nayak et al., 2019; Micaelli and Storkey, 2019; Chen et al., 2019; Fang et al., 2019), data distillation (Radosavovic et al.",
                "\u2026the transfer procedure, such as self-distillation (Furlanello et al., 2018), data-free distillation (Lopes et al., 2017; Nayak et al., 2019; Micaelli and Storkey, 2019; Chen et al., 2019; Fang et al., 2019), data distillation (Radosavovic et al., 2018), residual knowledge distillation (Gao\u2026"
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "474920196d13e26658cca3b373e2756088e5fa96",
                "externalIds": {
                    "DBLP": "conf/nips/BorupA21",
                    "ArXiv": "2102.13088",
                    "CorpusId": 232045950
                },
                "corpusId": 232045950,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/474920196d13e26658cca3b373e2756088e5fa96",
                "title": "Even your Teacher Needs Guidance: Ground-Truth Targets Dampen Regularization Imposed by Self-Distillation",
                "abstract": "Knowledge distillation is classically a procedure where a neural network is trained on the output of another network along with the original targets in order to transfer knowledge between the architectures. The special case of self-distillation, where the network architectures are identical, has been observed to improve generalization accuracy. In this paper, we consider an iterative variant of self-distillation in a kernel regression setting, in which successive steps incorporate both model outputs and the ground-truth targets. This allows us to provide the first theoretical results on the importance of using the weighted ground-truth targets in self-distillation. Our focus is on fitting nonlinear functions to training data with a weighted mean square error objective function suitable for distillation, subject to $\\ell_2$ regularization of the model parameters. We show that any such function obtained with self-distillation can be calculated directly as a function of the initial fit, and that infinite distillation steps yields the same optimization problem as the original with amplified regularization. Furthermore, we provide a closed form solution for the optimal choice of weighting parameter at each step, and show how to efficiently estimate this weighting parameter for deep learning and significantly reduce the computational requirements compared to a grid search.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2051550661",
                        "name": "Kenneth Borup"
                    },
                    {
                        "authorId": "123729926",
                        "name": "L. Andersen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Data-free knowledge distillation[8, 9, 10, 11, 12, 13, 14, 15, 16] can utilize these pre-trained models to accomplish model compression without accessing original data.",
                "Typical data-free adversarial framework [16][15] deploys a generator to transfer the knowledge of the pre-trained teacher model to a student model without access to original data."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c6a3698093e8158f2db2e179426e272cbaa7245a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-11638",
                    "ArXiv": "2102.11638",
                    "DOI": "10.1109/ICASSP39728.2021.9413483",
                    "CorpusId": 232014518
                },
                "corpusId": 232014518,
                "publicationVenue": {
                    "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
                    "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Acoust Speech Signal Process",
                        "IEEE Int Conf Acoust Speech Signal Process",
                        "ICASSP",
                        "International Conference on Acoustics, Speech, and Signal Processing"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
                },
                "url": "https://www.semanticscholar.org/paper/c6a3698093e8158f2db2e179426e272cbaa7245a",
                "title": "Enhancing Data-Free Adversarial Distillation with Activation Regularization and Virtual Interpolation",
                "abstract": "Knowledge distillation refers to a technique of transferring the knowledge from a large learned model or an ensemble of learned models to a small model. This method relies on access to the original training set, which might not always be available. A possible solution is a data-free adversarial distillation framework, which deploys a generative network to transfer the teacher model\u2019s knowledge to the student model. However, the data generation efficiency is low in the data-free adversarial distillation. We add an activation regularizer and a virtual interpolation method to improve the data generation efficiency. The activation regularizer enables the students to match the teacher\u2019s predictions close to activation boundaries and decision boundaries. The virtual interpolation method can generate virtual samples and labels in-between decision boundaries. Our experiments show that our approach surpasses state-of-the-art data-free distillation methods. The student model can achieve 95.42% accuracy on CIFAR-10 and 77.05% accuracy on CIFAR-100 without any original training data. Our model\u2019s accuracy is 13.8% higher than the state-of-the-art data-free method on CIFAR-100.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2624637",
                        "name": "Xiaoyang Qu"
                    },
                    {
                        "authorId": "66063851",
                        "name": "Jianzong Wang"
                    },
                    {
                        "authorId": "91353860",
                        "name": "Jing Xiao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d28e6325aed119d93fca0fad36035f3d1ba545ba",
                "externalIds": {
                    "ArXiv": "2101.08430",
                    "DBLP": "journals/corr/abs-2101-08430",
                    "DOI": "10.1109/CVPRW53098.2021.00335",
                    "CorpusId": 231662186
                },
                "corpusId": 231662186,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d28e6325aed119d93fca0fad36035f3d1ba545ba",
                "title": "Generative Zero-shot Network Quantization",
                "abstract": "Convolutional neural networks are able to learn realistic image priors from numerous training samples in low-level image generation and restoration [66]. We show that, for high-level image recognition tasks, we can further reconstruct \"realistic\" images of each category by leveraging intrinsic Batch Normalization (BN) statistics without any training data. Inspired by the popular VAE/GAN methods, we regard the zero-shot optimization process of synthetic images as generative modeling to match the distribution of BN statistics. The generated images serve as a calibration set for the following zero-shot network quantizations. Our method meets the needs for quantizing models based on sensitive information, e.g., due to privacy concerns, no data is available. Extensive experiments on benchmark datasets show that, with the help of generated data, our approach consistently outperforms existing data-free quantization methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48535072",
                        "name": "Xiangyu He"
                    },
                    {
                        "authorId": "2571792",
                        "name": "Qinghao Hu"
                    },
                    {
                        "authorId": "1656803942",
                        "name": "Peisong Wang"
                    },
                    {
                        "authorId": "143949499",
                        "name": "Jian Cheng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Summary of Differences With Data-Free Methods: Several methods such as [5], [12], [13], [15], [17], [18], [23], [29], [30], [31] have been proposed in the data-free set up towards different applications which are specifically designed.",
                "While multiple works such as [12], [13] have studied data-free approaches for training deep neural networks, to",
                "train the GAN with multiple objectives to ensure learning (i) difficult pseudo (or proxy) samples on which the Teacher and Student differ ([13]), (ii) uniform distributions over the underlying classes ([14]), and (iii) samples predicted with a strong confidence by the Teacher model, ([12]) etc.",
                "involved in the loop, this scheme is application dependent and is also very similar to [13]."
            ],
            "intents": [
                "methodology",
                "background",
                "result"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "4cf6d19d683eb65fe29ab8e2604827c4113278f5",
                "externalIds": {
                    "ArXiv": "2101.06069",
                    "DBLP": "journals/pami/NayakMJC22",
                    "DOI": "10.1109/TPAMI.2021.3112816",
                    "CorpusId": 236318353,
                    "PubMed": "34529560"
                },
                "corpusId": 236318353,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4cf6d19d683eb65fe29ab8e2604827c4113278f5",
                "title": "Mining Data Impressions From Deep Models as Substitute for the Unavailable Training Data",
                "abstract": "Pretrained deep models hold their learnt knowledge in the form of model parameters. These parameters act as \u201cmemory\u201d for the trained models and help them generalize well on unseen data. However, in absence of training data, the utility of a trained model is merely limited to either inference or better initialization towards a target task. In this paper, we go further and extract synthetic data by leveraging the learnt model parameters. We dub them Data Impressions, which act as proxy to the training data and can be used to realize a variety of tasks. These are useful in scenarios where only the pretrained models are available and the training data is not shared (e.g., due to privacy or sensitivity concerns). We show the applicability of data impressions in solving several computer vision tasks such as unsupervised domain adaptation, continual learning as well as knowledge distillation. We also study the adversarial robustness of lightweight models trained via knowledge distillation using these data impressions. Further, we demonstrate the efficacy of data impressions in generating data-free Universal Adversarial Perturbations (UAPs) with better fooling rates. Extensive experiments performed on benchmark datasets demonstrate competitive performance achieved using data impressions in absence of original training data.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "143747407",
                        "name": "Gaurav Kumar Nayak"
                    },
                    {
                        "authorId": "2217000",
                        "name": "Konda Reddy Mopuri"
                    },
                    {
                        "authorId": "66320045",
                        "name": "Saksham Jain"
                    },
                    {
                        "authorId": "1429640900",
                        "name": "Anirban Chakraborty"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Alternatively, Chen et al. [5] and Micaelli and Storkey [25] reformulate the classification network as a discriminator and train an external generator network to synthesize images that maximize the discriminator\u2019s response.",
                "[5] and Micaelli and Storkey [25] reformulate the classification network as a discriminator and train an external generator network to synthesize images that maximize the discriminator\u2019s response."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f38dacf69cbe327e9c4f3622dd12525f4692cf21",
                "externalIds": {
                    "DBLP": "conf/wacv/ChawlaYMA21",
                    "DOI": "10.1109/WACV48630.2021.00333",
                    "CorpusId": 231401119
                },
                "corpusId": 231401119,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/f38dacf69cbe327e9c4f3622dd12525f4692cf21",
                "title": "Data-free Knowledge Distillation for Object Detection",
                "abstract": "We present DeepInversion for Object Detection (DIODE) to enable data-free knowledge distillation for neural networks trained on the object detection task. From a data-free perspective, DIODE synthesizes images given only an off-the-shelf pre-trained detection network and without any prior domain knowledge, generator network, or pre-computed activations. DIODE relies on two key components\u2014first, an extensive set of differentiable augmentations to improve image fidelity and distillation effectiveness. Second, a novel automated bounding box and category sampling scheme for image synthesis enabling generating a large number of images with a diverse set of spatial and category objects. The resulting images enable data-free knowledge distillation from a teacher to a student detector, initialized from scratch.In an extensive set of experiments, we demonstrate that DIODE\u2019s ability to match the original training distribution consistently enables more effective knowledge distillation than out-of-distribution proxy datasets, which unavoidably occur in a data-free setup given the absence of the original domain knowledge.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "28758216",
                        "name": "Akshay Chawla"
                    },
                    {
                        "authorId": "1989015",
                        "name": "Hongxu Yin"
                    },
                    {
                        "authorId": "2824500",
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "authorId": "2974008",
                        "name": "J. \u00c1lvarez"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Inspired by (Micaelli and Storkey, 2019) and on the promise of adversarial training for NLP (Zhu et al.",
                "Some of the concerns preventing access include data privacy, intellectual property, size and transience (Micaelli and Storkey, 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8b28d9e3ca408b8a41d32f8bd4da7fbbd4f12a4b",
                "externalIds": {
                    "ACL": "2021.emnlp-main.526",
                    "DBLP": "conf/emnlp/RashidLGR21",
                    "ArXiv": "2012.15495",
                    "DOI": "10.18653/v1/2021.emnlp-main.526",
                    "CorpusId": 229923739
                },
                "corpusId": 229923739,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/8b28d9e3ca408b8a41d32f8bd4da7fbbd4f12a4b",
                "title": "Towards Zero-Shot Knowledge Distillation for Natural Language Processing",
                "abstract": "Knowledge distillation (KD) is a common knowledge transfer algorithm used for model compression across a variety of deep learning based natural language processing (NLP) solutions. In its regular manifestations, KD requires access to the teacher\u2019s training data for knowledge transfer to the student network. However, privacy concerns, data regulations and proprietary reasons may prevent access to such data. We present, to the best of our knowledge, the first work on Zero-shot Knowledge Distillation for NLP, where the student learns from the much larger teacher without any task specific data. Our solution combines out-of-domain data and adversarial training to learn the teacher\u2019s output distribution. We investigate six tasks from the GLUE benchmark and demonstrate that we can achieve between 75% and 92% of the teacher\u2019s classification score (accuracy or F1) while compressing the model 30 times.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2064509318",
                        "name": "Ahmad Rashid"
                    },
                    {
                        "authorId": "3814221",
                        "name": "Vasileios Lioutas"
                    },
                    {
                        "authorId": "3448973",
                        "name": "Abbas Ghaddar"
                    },
                    {
                        "authorId": "1924511",
                        "name": "Mehdi Rezagholizadeh"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0762dde70cc60a49c719e9bd13ea867ab6183a7e",
                "externalIds": {
                    "DBLP": "journals/cmig/LiuCHXZY21",
                    "DOI": "10.1016/j.compmedimag.2020.101842",
                    "CorpusId": 230282418,
                    "PubMed": "33387812"
                },
                "corpusId": 230282418,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0762dde70cc60a49c719e9bd13ea867ab6183a7e",
                "title": "Knowledge transfer between brain lesion segmentation tasks with increased model capacity",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2108202998",
                        "name": "Yanling Liu"
                    },
                    {
                        "authorId": "49080838",
                        "name": "Wenhui Cui"
                    },
                    {
                        "authorId": "2043923117",
                        "name": "Qing Ha"
                    },
                    {
                        "authorId": "2043878081",
                        "name": "Xiaoliang Xiong"
                    },
                    {
                        "authorId": "7724630",
                        "name": "Xiangzhu Zeng"
                    },
                    {
                        "authorId": "34952310",
                        "name": "Chuyang Ye"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There are also a few methods that synthesize input data via generative image modeling [5, 11, 41], which create substitute data much more efficiently than optimizing input noise."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ea68ba918a92fd4460be9dddc7326724cdccbb5e",
                "externalIds": {
                    "ArXiv": "2012.05578",
                    "MAG": "3111286375",
                    "DBLP": "journals/corr/abs-2012-05578",
                    "CorpusId": 228083866
                },
                "corpusId": 228083866,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ea68ba918a92fd4460be9dddc7326724cdccbb5e",
                "title": "Large-Scale Generative Data-Free Distillation",
                "abstract": "Knowledge distillation is one of the most popular and effective techniques for knowledge transfer, model compression and semi-supervised learning. Most existing distillation approaches require the access to original or augmented training samples. But this can be problematic in practice due to privacy, proprietary and availability concerns. Recent work has put forward some methods to tackle this problem, but they are either highly time-consuming or unable to scale to large datasets. To this end, we propose a new method to train a generative image model by leveraging the intrinsic normalization layers' statistics of the trained teacher network. This enables us to build an ensemble of generators without training data that can efficiently produce substitute inputs for subsequent distillation. The proposed method pushes forward the data-free distillation performance on CIFAR-10 and CIFAR-100 to 95.02% and 77.02% respectively. Furthermore, we are able to scale it to ImageNet dataset, which to the best of our knowledge, has never been done using generative models in a data-free setting.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51225788",
                        "name": "Liangchen Luo"
                    },
                    {
                        "authorId": "144882893",
                        "name": "M. Sandler"
                    },
                    {
                        "authorId": "2112304965",
                        "name": "Zi Lin"
                    },
                    {
                        "authorId": "3422677",
                        "name": "A. Zhmoginov"
                    },
                    {
                        "authorId": "144727050",
                        "name": "Andrew G. Howard"
                    }
                ]
            }
        },
        {
            "contexts": [
                "ZSKT (Micaelli and Storkey 2019) and DFAD (Fang et al.",
                "ZSKT (Micaelli and Storkey 2019) and DFAD (Fang et al. 2019) introduce an adversarial strategy to synthesize training samples for knowledge distillation."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "883ea3b74ec1e26ad20f2c5c94f45e0d6db16364",
                "externalIds": {
                    "MAG": "3113223504",
                    "ArXiv": "2012.04915",
                    "DBLP": "journals/corr/abs-2012-04915",
                    "DOI": "10.1609/aaai.v35i3.16356",
                    "CorpusId": 228063796
                },
                "corpusId": 228063796,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/883ea3b74ec1e26ad20f2c5c94f45e0d6db16364",
                "title": "Progressive Network Grafting for Few-Shot Knowledge Distillation",
                "abstract": "Knowledge distillation has demonstrated encouraging performances in deep model compression. Most existing approaches, however, require massive labeled data to accomplish the knowledge transfer, making the model compression a cumbersome and costly process. In this paper, we investigate the practical few-shot knowledge distillation scenario, where we assume only a few samples without human annotations are available for each category. To this end, we introduce a principled dual-stage distillation scheme tailored for few-shot data. In the first step, we graft the student blocks one by one onto the teacher, and learn the parameters of the grafted block intertwined with those of the other teacher blocks. In the second step, the trained student blocks are progressively connected and then together grafted onto the teacher network, allowing the learned student blocks to adapt themselves to each other and eventually replace the teacher network. Experiments demonstrate that our approach, with only a few unlabeled samples, achieves gratifying results on CIFAR10,CIFAR100, and ILSVRC-2012. On CIFAR10 and CIFAR100, our performances are even on par with those of knowledge distillation schemes that utilize the full datasets. The source code is available at https://github.com/zju-vipa/NetGraft.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "40900125",
                        "name": "Chengchao Shen"
                    },
                    {
                        "authorId": "48631088",
                        "name": "Xinchao Wang"
                    },
                    {
                        "authorId": "2034017386",
                        "name": "Youtan Yin"
                    },
                    {
                        "authorId": "40403685",
                        "name": "Jie Song"
                    },
                    {
                        "authorId": "48730399",
                        "name": "Sihui Luo"
                    },
                    {
                        "authorId": "144646841",
                        "name": "Mingli Song"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Prior work on knowledge distillation showed that a student model S can learn from a teacher and reach high accuracy even though its architecture is smaller and different [9, 27].",
                "Our work builds on recent advances in data-free knowledge distillation, which involve a generative model to synthesize queries that maximize disagreement between the student and teacher models [27, 14].",
                "Techniques addressing data-free knowledge distillation have relied on training a generative model to synthesize the queries that the student makes to the teacher [10, 27].",
                "Our work systematically transitions from a data-free knowledge distillation paradigm [14, 27] to a data-free model extraction scenario.",
                "While the model owner usually performs knowledge distillation, the original dataset used to train the teacher model may not be available during distillation [27], e.",
                ", data-free knowledge distillation [14, 27].",
                "Naively, one could generate these queries randomly [27, 14]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "287e82b08cc5b8c8aae03825b466fcb73860b0c4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2011-14779",
                    "MAG": "3109310454",
                    "ArXiv": "2011.14779",
                    "DOI": "10.1109/CVPR46437.2021.00474",
                    "CorpusId": 227228405
                },
                "corpusId": 227228405,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/287e82b08cc5b8c8aae03825b466fcb73860b0c4",
                "title": "Data-Free Model Extraction",
                "abstract": "Current model extraction attacks assume that the adversary has access to a surrogate dataset with characteristics similar to the proprietary data used to train the victim model. This requirement precludes the use of existing model extraction techniques on valuable models, such as those trained on rare or hard to acquire datasets. In contrast, we propose data-free model extraction methods that do not require a surrogate dataset. Our approach adapts techniques from the area of data-free knowledge transfer for model extraction. As part of our study, we identify that the choice of loss is critical to ensuring that the extracted model is an accurate replica of the victim model. Further-more, we address difficulties arising from the adversary\u2019s limited access to the victim model in a black-box setting. For example, we recover the model\u2019s logits from its probability predictions to approximate gradients. We find that the proposed data-free model extraction approach achieves high-accuracy with reasonable query complexity \u2013 0.99\u00d7 and 0.92\u00d7 the victim model accuracy on SVHN and CIFAR- 10 datasets given 2M and 20M queries respectively.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2029496490",
                        "name": "Jean-Baptiste Truong"
                    },
                    {
                        "authorId": "153742303",
                        "name": "Pratyush Maini"
                    },
                    {
                        "authorId": "2329545",
                        "name": "R. Walls"
                    },
                    {
                        "authorId": "1967156",
                        "name": "Nicolas Papernot"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similar GAN-based formulations have been developed [41]\u2013[45]."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "404bebddcdd449301e66bf8d130555f6640ca7fb",
                "externalIds": {
                    "DBLP": "conf/icpr/HortonJFR22",
                    "ArXiv": "2011.09058",
                    "MAG": "3106178055",
                    "DOI": "10.1109/ICPR56361.2022.9956237",
                    "CorpusId": 227011888
                },
                "corpusId": 227011888,
                "publicationVenue": {
                    "id": "48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                    "name": "International Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Pattern Recognit (ICPR Proc Int Conf",
                        "Int Conf Pattern Recognit",
                        "ICPR",
                        "International conference on pattern recognition",
                        "Int conf pattern recognit",
                        "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                    ],
                    "issn": "1041-3278",
                    "alternate_issns": [
                        "1051-4651"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=4740202"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/404bebddcdd449301e66bf8d130555f6640ca7fb",
                "title": "Layer-Wise Data-Free CNN Compression",
                "abstract": "We present a computationally efficient method for compressing a trained neural network without using real data. We break the problem of data-free network compression into independent layer-wise compressions. We show how to efficiently generate layer-wise training data using only a pretrained network. We use this data to perform independent layer-wise compressions on the pretrained network. We also show how to precondition the network to improve the accuracy of our layer-wise compression method. We present results for layer-wise compression using quantization and pruning. When quantizing, we compress with higher accuracy than related works while using orders of magnitude less compute. When compressing MobileNetV2 and evaluating on ImageNet, our method outperforms existing methods for quantization at all bit-widths, achieving a +0.34% improvement in 8-bit quantization, and a stronger improvement at lower bit-widths (up to a +28.50% improvement at 5 bits). When pruning, we outperform baselines of a similar compute envelope, achieving 1.5 times the sparsity rate at the same accuracy. We also show how to combine our efficient method with high-compute generative methods to improve upon their results.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "46211341",
                        "name": "Maxwell Horton"
                    },
                    {
                        "authorId": "30750450",
                        "name": "Yanzi Jin"
                    },
                    {
                        "authorId": "143787583",
                        "name": "Ali Farhadi"
                    },
                    {
                        "authorId": "32371083",
                        "name": "Mohammad Rastegari"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[17, 15]) since it is common now-a-days that many popular pre-trained models are released without providing access to the training data (e.",
                "[15, 17, 2, 1]) compose synthetic transfer set and achieve effective distillation.",
                "[15, 4, 1] show that properly optimized generative models can generate samples to be strongly classified by the Teacher models.",
                "In order to cope with the constrained operational conditions, recent efforts [15, 4, 17, 1] attempt to distill in a data-free scenario via artificially generated transfer set."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "8d6e707f7e36fbab242e3d88cc7991427879301a",
                "externalIds": {
                    "MAG": "3101941678",
                    "ArXiv": "2011.09113",
                    "DBLP": "conf/wacv/NayakMC21",
                    "DOI": "10.1109/WACV48630.2021.00147",
                    "CorpusId": 227013462
                },
                "corpusId": 227013462,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/8d6e707f7e36fbab242e3d88cc7991427879301a",
                "title": "Effectiveness of Arbitrary Transfer Sets for Data-free Knowledge Distillation",
                "abstract": "Knowledge Distillation is an effective method to transfer the learning across deep neural networks. Typically, the dataset originally used for training the Teacher model is chosen as the \"Transfer Set\" to conduct the knowledge transfer to the Student. However, this original training data may not always be freely available due to privacy or sensitivity concerns. In such scenarios, existing approaches either iteratively compose a synthetic set representative of the original training dataset, one sample at a time or learn a generative model to compose such a transfer set. However, both these approaches involve complex optimization (GAN training or several backpropagation steps to synthesize one sample) and are often computationally expensive. In this paper, as a simple alternative, we investigate the effectiveness of \"arbitrary transfer sets\" such as random noise, publicly available synthetic, and natural datasets, all of which are completely unrelated to the original training dataset in terms of their visual or semantic contents. Through extensive experiments on multiple benchmark datasets such as MNIST, FMNIST, CIFAR-10 and CIFAR-100, we discover and validate surprising effectiveness of using arbitrary data to conduct knowledge distillation when this dataset is \"target-class balanced\". We believe that this important observation can potentially lead to designing base-lines for the data-free knowledge distillation task.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "143747407",
                        "name": "Gaurav Kumar Nayak"
                    },
                    {
                        "authorId": "2217000",
                        "name": "Konda Reddy Mopuri"
                    },
                    {
                        "authorId": "1429640900",
                        "name": "Anirban Chakraborty"
                    }
                ]
            }
        },
        {
            "contexts": [
                "An interesting future direction is to explore the performance of data-free distillation strategies on dynamic graph representation learning approaches [43], [44]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7de4d72195ed35429229eff8f55a3e2f64bdf9e3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2011-05664",
                    "ArXiv": "2011.05664",
                    "MAG": "3106261282",
                    "DOI": "10.1109/ASONAM49781.2020.9381315",
                    "CorpusId": 226300026
                },
                "corpusId": 226300026,
                "publicationVenue": {
                    "id": "255a371e-2a0c-42d1-b4e7-c3cf3e21c89a",
                    "name": "International Conference on Advances in Social Networks Analysis and Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Advances in Social Networks Analysis and Mining",
                        "Int Conf Adv Soc Netw Anal Min",
                        "ASONAM",
                        "Adv Soc Netw Anal Min"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=239"
                },
                "url": "https://www.semanticscholar.org/paper/7de4d72195ed35429229eff8f55a3e2f64bdf9e3",
                "title": "Distill2Vec: Dynamic Graph Representation Learning with Knowledge Distillation",
                "abstract": "Dynamic graph representation learning strategies are based on different neural architectures to capture the graph evolution over time. However, the underlying neural architectures require a large amount of parameters to train and suffer from high online inference latency, that is several model parameters have to be updated when new data arrive online. In this study we propose Distill2Vec, a knowledge distillation strategy to train a compact model with a low number of trainable parameters, so as to reduce the latency of online inference and maintain the model accuracy high. We design a distillation loss function based on Kullback-Leibler divergence to transfer the acquired knowledge from a teacher model trained on offline data, to a small-size student model for online data. Our experiments with publicly available datasets show the superiority of our proposed model over several state-of-the-art approaches with relative gains up to 5% in the link prediction task. In addition, we demonstrate the effectiveness of our knowledge distillation strategy, in terms of number of required parameters, where Distill2Vec achieves a compression ratio up to 7:100 when compared with baseline approaches. For reproduction purposes, our implementation is publicly available at https://stefanosantaris.github.io/Distill2Vec.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1853653",
                        "name": "Stefanos Antaris"
                    },
                    {
                        "authorId": "3120333",
                        "name": "Dimitrios Rafailidis"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "57369bbef218b4087c71b54c86474f4308a80056",
                "externalIds": {
                    "ArXiv": "2011.05961",
                    "DBLP": "journals/corr/abs-2011-05961",
                    "MAG": "3098251575",
                    "CorpusId": 226299772
                },
                "corpusId": 226299772,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/57369bbef218b4087c71b54c86474f4308a80056",
                "title": "Real-Time Decentralized knowledge Transfer at the Edge",
                "abstract": "Proliferation of edge networks creates islands of learning agents working on local streams of data. Transferring knowledge between these agents in real-time without exposing private data allows for collaboration to decrease learning time, and increase model confidence. Incorporating knowledge from data that was not seen by a local model creates an ability to debias a local model, or add to classification abilities on data never before seen. Transferring knowledge in a decentralized approach allows for models to retain their local insights, in turn allowing for local flavors of a machine learning model. This approach suits the decentralized architecture of edge networks, as a local edge node will serve a community of learning agents that will likely encounter similar data. We propose a method based on knowledge distillation for pairwise knowledge transfer pipelines, and compare to other popular knowledge transfer methods. Additionally, we test different scenarios of knowledge transfer network construction and show the practicality of our approach. Based on our experiments we show knowledge transfer using our model outperforms common methods in a real time transfer scenario.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "49805856",
                        "name": "Orpaz Goldstein"
                    },
                    {
                        "authorId": "48092603",
                        "name": "Mohammad Kachuee"
                    },
                    {
                        "authorId": "2008562447",
                        "name": "Dereck Shiell"
                    },
                    {
                        "authorId": "1684837",
                        "name": "M. Sarrafzadeh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The existing approaches either create a sample generator [9, 16] or synthesize a number of data impressions from the teacher directly [17, 18, 19].",
                "Among the generator-based data-free KD techniques, the generator can be trained either separately [9] or simultaneously with the student model [16]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4cfd2e1fe3c5d88785d253cb2596257a73969a76",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2011-03749",
                    "MAG": "3103536899",
                    "ArXiv": "2011.03749",
                    "DOI": "10.1109/ICASSP39728.2021.9414674",
                    "CorpusId": 226281452
                },
                "corpusId": 226281452,
                "publicationVenue": {
                    "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
                    "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Acoust Speech Signal Process",
                        "IEEE Int Conf Acoust Speech Signal Process",
                        "ICASSP",
                        "International Conference on Acoustics, Speech, and Signal Processing"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
                },
                "url": "https://www.semanticscholar.org/paper/4cfd2e1fe3c5d88785d253cb2596257a73969a76",
                "title": "Robustness and Diversity Seeking Data-Free Knowledge Distillation",
                "abstract": "Knowledge distillation (KD) has enabled remarkable progress in model compression and knowledge transfer. However, KD requires a large volume of original data or their representation statistics that are not usually available in practice. Data-free KD has recently been proposed to resolve this problem, wherein teacher and student models are fed by a synthetic sample generator trained from the teacher. Nonetheless, existing data-free KD methods rely on fine-tuning of weights to balance multiple losses, and ignore the diversity of generated samples, resulting in limited accuracy and robustness. To overcome this challenge, we propose robustness and diversity seeking data-free KD (RDSKD) in this paper. The generator loss function is crafted to produce samples with high authenticity, class diversity, and inter-sample diversity. Without real data, the objectives of seeking high sample authenticity and class diversity often conflict with each other, causing frequent loss fluctuations. We mitigate this by exponentially penalizing loss increments. With MNIST, CIFAR-10, and SVHN datasets, our experiments show that RDSKD achieves higher accuracy with more robustness over different hyperparameter settings, compared to other data-free KD methods such as DAFL, MSKD, ZSKD, and DeepInversion.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2598318",
                        "name": "Pengchao Han"
                    },
                    {
                        "authorId": "48490823",
                        "name": "Jihong Park"
                    },
                    {
                        "authorId": "50695457",
                        "name": "Shiqiang Wang"
                    },
                    {
                        "authorId": "7895174",
                        "name": "Yejun Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Micaelli and Storkey [17] also make use of an adversarial framework."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5eb128fe52b21abc5e5a16eb638ae83f26fb94c0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2011-00809",
                    "MAG": "3095377493",
                    "ArXiv": "2011.00809",
                    "CorpusId": 226226567
                },
                "corpusId": 226226567,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5eb128fe52b21abc5e5a16eb638ae83f26fb94c0",
                "title": "Data-free Knowledge Distillation for Segmentation using Data-Enriching GAN",
                "abstract": "Distilling knowledge from huge pre-trained networks to improve the performance of tiny networks has favored deep learning models to be used in many real-time and mobile applications. Several approaches that demonstrate success in this field have made use of the true training dataset to extract relevant knowledge. In absence of the True dataset, however, extracting knowledge from deep networks is still a challenge. Recent works on data-free knowledge distillation demonstrate such techniques on classification tasks. To this end, we explore the task of data-free knowledge distillation for segmentation tasks. First, we identify several challenges specific to segmentation. We make use of the DeGAN training framework to propose a novel loss function for enforcing diversity in a setting where a few classes are underrepresented. Further, we explore a new training framework for performing knowledge distillation in a data-free setting. We get an improvement of 6.93% in Mean IoU over previous approaches.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51939901",
                        "name": "K. Bhogale"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our work is related to zero-shot knowledge distillation methods [1, 3, 4, 8, 24, 28, 38], with the difference that we regard the teacher model as a black box, and to model stealing methods [17, 30, 31, 32, 35, 36], with the difference that we focus on accuracy and not on minimizing the number of API calls to the black box.",
                "In this context, we train the student on a proxy data set with images and classes different from those used to train the black-box, in a setting known as zero-shot or data-free knowledge distillation [1, 3, 4, 8, 24, 28, 38].",
                "Many formulations have been developed to alleviate this requirement [1, 8, 4, 24, 28], with methods either requiring a small subset of the original data [3, 4], or none at all [1].",
                "[24] developed a method for zero-shot knowledge transfer by jointly training a generative model and the student, such that the generated samples are easily classified by the teacher, but hard for the student."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "5c395c17415c89be902c199d52183fc5b9ca9f53",
                "externalIds": {
                    "MAG": "3094357937",
                    "DBLP": "journals/corr/abs-2010-11158",
                    "ArXiv": "2010.11158",
                    "CorpusId": 224814207
                },
                "corpusId": 224814207,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5c395c17415c89be902c199d52183fc5b9ca9f53",
                "title": "Black-Box Ripper: Copying black-box models using generative evolutionary algorithms",
                "abstract": "We study the task of replicating the functionality of black-box neural models, for which we only know the output class probabilities provided for a set of input images. We assume back-propagation through the black-box model is not possible and its training images are not available, e.g. the model could be exposed only through an API. In this context, we present a teacher-student framework that can distill the black-box (teacher) model into a student model with minimal accuracy loss. To generate useful data samples for training the student, our framework (i) learns to generate images on a proxy data set (with images and classes different from those used to train the black-box) and (ii) applies an evolutionary strategy to make sure that each generated data sample exhibits a high response for a specific class when given as input to the black box. Our framework is compared with several baseline and state-of-the-art methods on three benchmark data sets. The empirical evidence indicates that our model is superior to the considered baselines. Although our method does not back-propagate through the black-box network, it generally surpasses state-of-the-art methods that regard the teacher as a glass-box model. Our code is available at: this https URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1739398670",
                        "name": "Antonio B\u0103rb\u0103l\u0103u"
                    },
                    {
                        "authorId": "66618729",
                        "name": "Adrian Cosma"
                    },
                    {
                        "authorId": "1817759",
                        "name": "Radu Tudor Ionescu"
                    },
                    {
                        "authorId": "49006356",
                        "name": "M. Popescu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "263e662de5089b60e8a4b292de9618083ab39af7",
                "externalIds": {
                    "ArXiv": "2010.07334",
                    "DBLP": "journals/corr/abs-2010-07334",
                    "MAG": "3092843084",
                    "CorpusId": 222379850
                },
                "corpusId": 222379850,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/263e662de5089b60e8a4b292de9618083ab39af7",
                "title": "Towards Accurate Quantization and Pruning via Data-free Knowledge Transfer",
                "abstract": "When large scale training data is available, one can obtain compact and accurate networks to be deployed in resource-constrained environments effectively through quantization and pruning. However, training data are often protected due to privacy concerns and it is challenging to obtain compact networks without data. We study data-free quantization and pruning by transferring knowledge from trained large networks to compact networks. Auxiliary generators are simultaneously and adversarially trained with the targeted compact networks to generate synthetic inputs that maximize the discrepancy between the given large network and its quantized or pruned version. We show theoretically that the alternating optimization for the underlying minimax problem converges under mild conditions for pruning and quantization. Our data-free compact networks achieve competitive accuracy to networks trained and fine-tuned with training data. Our quantized and pruned networks achieve good performance while being more compact and lightweight. Further, we demonstrate that the compact structure and corresponding initialization from the Lottery Ticket Hypothesis can also help in data-free training.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1431754650",
                        "name": "Chen Zhu"
                    },
                    {
                        "authorId": "2149237996",
                        "name": "Zheng Xu"
                    },
                    {
                        "authorId": "3246287",
                        "name": "Ali Shafahi"
                    },
                    {
                        "authorId": "1643697854",
                        "name": "Manli Shu"
                    },
                    {
                        "authorId": "115752784",
                        "name": "Amin Ghiasi"
                    },
                    {
                        "authorId": "1962083",
                        "name": "T. Goldstein"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Exploratory researches (Micaelli and Storkey, 2019; Fang et al., 2019) also show that GANs can synthesize harder and more diversified images by exploiting disagreements between teachers and students.",
                "Prior works (Micaelli and Storkey, 2019; Fang et al., 2019) maximize the discrepancy between the teacher and student to encourage difficulty in samples and avoid synthesizing redundant images.",
                "We do not include Modified-ZSKT because samples of Modified-ZSKT vastly outnumber the other two approaches.",
                "ZSKT trains an adversarial generator to search for images in which the student\u2019s prediction poorly matches that of the teacher\u2019s and reaches state-of-the-art performance.",
                "ModifiedZSKT performs worse than Random Text on DBPedia.",
                "Modified-ZSKT Modified-ZSKT is extended from ZSKT (Micaelli and Storkey, 2019).",
                "Train-\ning epochs are 2.5k(AG News), 10k(DBPedia), 10k(IMDb) with 48 samples per batch for all methods except ZSKT, which needs to train its generator from scratch (25k epochs in Modified-ZSKT)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "71b769812974c2e04bcd2ffd9554015052f7cfd5",
                "externalIds": {
                    "DBLP": "conf/emnlp/MaSFCJL20",
                    "ArXiv": "2010.04883",
                    "ACL": "2020.emnlp-main.499",
                    "MAG": "3105183573",
                    "DOI": "10.18653/v1/2020.emnlp-main.499",
                    "CorpusId": 222290473
                },
                "corpusId": 222290473,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/71b769812974c2e04bcd2ffd9554015052f7cfd5",
                "title": "Adversarial Self-Supervised Data Free Distillation for Text Classification",
                "abstract": "Large pre-trained transformer-based language models have achieved impressive results on a wide range of NLP tasks. In the past few years, Knowledge Distillation(KD) has become a popular paradigm to compress a computationally expensive model to a resource-efficient lightweight model. However, most KD algorithms, especially in NLP, rely on the accessibility of the original training dataset, which may be unavailable due to privacy issues. To tackle this problem, we propose a novel two-stage data-free distillation method, named Adversarial self-Supervised Data-Free Distillation (AS-DFD), which is designed for compressing large-scale transformer-based models (e.g., BERT). To avoid text generation in discrete space, we introduce a Plug & Play Embedding Guessing method to craft pseudo embeddings from the teacher's hidden knowledge. Meanwhile, with a self-supervised module to quantify the student's ability, we adapt the difficulty of pseudo embeddings in an adversarial training manner. To the best of our knowledge, our framework is the first data-free distillation framework designed for NLP tasks. We verify the effectiveness of our method on several text classification datasets.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "15532066",
                        "name": "Xinyin Ma"
                    },
                    {
                        "authorId": "1471660296",
                        "name": "Yongliang Shen"
                    },
                    {
                        "authorId": "150110431",
                        "name": "Gongfan Fang"
                    },
                    {
                        "authorId": "2127380426",
                        "name": "Chen Chen"
                    },
                    {
                        "authorId": "153822757",
                        "name": "Chenghao Jia"
                    },
                    {
                        "authorId": "1776903",
                        "name": "Weiming Lu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The idea of transferring knowledge from a complex model (the teacher) to a simpler one (the student) been explored in other works, for example (Bucila et al., 2006; Hinton et al., 2015; Micaelli and Storkey, 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b5b98051b65da6b1b3b579862b0407d48c5bef48",
                "externalIds": {
                    "DBLP": "journals/fdata/BelleP21",
                    "PubMedCentral": "8281957",
                    "MAG": "3089162373",
                    "ArXiv": "2009.11698",
                    "DOI": "10.3389/fdata.2021.688969",
                    "CorpusId": 221878773,
                    "PubMed": "34278297"
                },
                "corpusId": 221878773,
                "publicationVenue": {
                    "id": "165fa1b5-e07f-4b6e-9203-04493f6a7c5c",
                    "name": "Frontiers in Big Data",
                    "alternate_names": [
                        "Front Big Data"
                    ],
                    "issn": "2624-909X",
                    "url": "https://www.frontiersin.org/journals/big-data"
                },
                "url": "https://www.semanticscholar.org/paper/b5b98051b65da6b1b3b579862b0407d48c5bef48",
                "title": "Principles and Practice of Explainable Machine Learning",
                "abstract": "Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact is coupled with a significant challenge: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus specifically on data-driven methods\u2014machine learning (ML) and pattern recognition models in particular\u2014so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions. From an organization viewpoint, after motivating the area broadly, we discuss the main developments, including the principles that allow us to study transparent models vs. opaque models, as well as model-specific or model-agnostic post-hoc explainability approaches. We also briefly reflect on deep learning models, and conclude with a discussion about future research directions.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "144893617",
                        "name": "Vaishak Belle"
                    },
                    {
                        "authorId": "40911590",
                        "name": "I. Papantonis"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "97df2c7a4501f229807fc2e795e728d5cf8ce8ad",
                "externalIds": {
                    "MAG": "3049725806",
                    "ArXiv": "2008.07514",
                    "DBLP": "journals/corr/abs-2008-07514",
                    "CorpusId": 221139723
                },
                "corpusId": 221139723,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/97df2c7a4501f229807fc2e795e728d5cf8ce8ad",
                "title": "Source Free Domain Adaptation with Image Translation",
                "abstract": "Effort in releasing large-scale datasets may be compromised by privacy and intellectual property considerations. A feasible alternative is to release pre-trained models instead. While these models are strong on their original task (source domain), their performance might degrade significantly when deployed directly in a new environment (target domain), which might not contain labels for training under realistic settings. Domain adaptation (DA) is a known solution to the domain gap problem, but usually requires labeled source data. In this paper, we study the problem of source free domain adaptation (SFDA), whose distinctive feature is that the source domain only provides a pre-trained model, but no source data. Being source free adds significant challenges to DA, especially when considering that the target dataset is unlabeled. To solve the SFDA problem, we propose an image translation approach that transfers the style of target images to that of unseen source images. To this end, we align the batch-wise feature statistics of generated images to that stored in batch normalization layers of the pre-trained model. Compared with directly classifying target images, higher accuracy is obtained with these style transferred images using the pre-trained model. On several image classification datasets, we show that the above-mentioned improvements are consistent and statistically significant.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "80447592",
                        "name": "Yunzhong Hou"
                    },
                    {
                        "authorId": "144802394",
                        "name": "Liang Zheng"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2ec6b1ca0e54445a5ada7126daf74559c4b383a7",
                "externalIds": {
                    "DBLP": "journals/tii/WangYPZRA21",
                    "MAG": "3041631353",
                    "DOI": "10.1109/TII.2020.3007407",
                    "CorpusId": 226457373
                },
                "corpusId": 226457373,
                "publicationVenue": {
                    "id": "2135230a-3b24-4b71-9583-60624389377a",
                    "name": "IEEE Transactions on Industrial Informatics",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Ind Informatics"
                    ],
                    "issn": "1551-3203",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=9424",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9424"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2ec6b1ca0e54445a5ada7126daf74559c4b383a7",
                "title": "Industrial Cyber-Physical Systems-Based Cloud IoT Edge for Federated Heterogeneous Distillation",
                "abstract": "Deep convoloutional networks have been widely deployed in modern cyber\u2013physical systems performing different visual classification tasks. As the fog and edge devices have different computing capacity and perform different subtasks, models trained for one device may not be deployable on another. Knowledge distillation technique can effectively compress well trained convolutional neural networks into light-weight models suitable to different devices. However, due to privacy issue and transmission cost, manually annotated data for training the deep learning models are usually gradually collected and archived in different sites. Simply training a model on powerful cloud servers and compressing them for particular edge devices failed to use the distributed data stored at different sites. This offline training approach is also inefficient to deal with new data collected from the edge devices. To overcome these obstacles, in this article, we propose the heterogeneous brain storming (HBS) method for object recognition tasks in real-world Internet of Things (IoT) scenarios. Our method enables flexible bidirectional federated learning of heterogeneous models trained on distributed datasets with a new \u201cbrain storming\u201d mechanism and optimizable temperature parameters. In our comparison experiments, this HBS method outperformed multiple state-of-the-art single-model compression methods, as well as the newest multinetwork knowledge distillation methods with both homogeneous and heterogeneous classifiers. The ablation experiment results proved that the trainable temperature parameter into the conventional knowledge distillation loss can effectively ease the learning process of student networks in different methods. To the best of authors\u2019 knowledge, this is the first IoT-oriented method that allows asynchronous bidirectional heterogeneous knowledge distillation in deep networks.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2109431217",
                        "name": "Chengjia Wang"
                    },
                    {
                        "authorId": "2152636732",
                        "name": "Guang Yang"
                    },
                    {
                        "authorId": "6846362",
                        "name": "G. Papanastasiou"
                    },
                    {
                        "authorId": "2596774",
                        "name": "Heye Zhang"
                    },
                    {
                        "authorId": "152171655",
                        "name": "J. Rodrigues"
                    },
                    {
                        "authorId": "51905607",
                        "name": "V. H. C. de Albuquerque"
                    }
                ]
            }
        },
        {
            "contexts": [
                "They focus on the problem of distilling a dataset or model [20] into a small number of example images, which are then used to train a new model."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6b67b1f55ebad02eaa73328c3989d64c1dc23dff",
                "externalIds": {
                    "MAG": "3035559424",
                    "ArXiv": "2006.08572",
                    "DBLP": "journals/corr/abs-2006-08572",
                    "CorpusId": 219686980
                },
                "corpusId": 219686980,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6b67b1f55ebad02eaa73328c3989d64c1dc23dff",
                "title": "Flexible Dataset Distillation: Learn Labels Instead of Images",
                "abstract": "We study the problem of dataset distillation - creating a small set of synthetic examples capable of training a good model. In particular, we study the problem of label distillation - creating synthetic labels for a small set of real images, and show it to be more effective than the prior image-based approach to dataset distillation. Interestingly, label distillation can be applied across datasets, for example enabling learning Japanese character recognition by training only on synthetically labeled English letters. Methodologically, we introduce a more robust and flexible meta-learning algorithm for distillation, as well as an effective first-order strategy based on convex optimization layers. Distilling labels with our new algorithm leads to improved results over prior image-based distillation. More importantly, it leads to clear improvements in flexibility of the distilled dataset in terms of compatibility with off-the-shelf optimizers and diverse neural architectures.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1749549997",
                        "name": "Ondrej Bohdal"
                    },
                    {
                        "authorId": "2653152",
                        "name": "Yongxin Yang"
                    },
                    {
                        "authorId": "1697755",
                        "name": "Timothy M. Hospedales"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "053f4d6715a4dba6f8103456fc1bb5fd6a5266c4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2006-07242",
                    "MAG": "3035453001",
                    "ArXiv": "2006.07242",
                    "CorpusId": 219636007
                },
                "corpusId": 219636007,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/053f4d6715a4dba6f8103456fc1bb5fd6a5266c4",
                "title": "Ensemble Distillation for Robust Model Fusion in Federated Learning",
                "abstract": "Federated Learning (FL) is a machine learning setting where many devices collaboratively train a machine learning model while keeping the training data decentralized. In most of the current training schemes the central model is refined by averaging the parameters of the server model and the updated parameters from the client side. However, directly averaging model parameters is only possible if all models have the same structure and size, which could be a restrictive constraint in many scenarios. \nIn this work we investigate more powerful and more flexible aggregation schemes for FL. Specifically, we propose ensemble distillation for model fusion, i.e. training the central classifier through unlabeled data on the outputs of the models from the clients. This knowledge distillation technique mitigates privacy risk and cost to the same extent as the baseline FL algorithms, but allows flexible aggregation over heterogeneous client models that can differ e.g. in size, numerical precision or structure. We show in extensive empirical experiments on various CV/NLP datasets (CIFAR-10/100, ImageNet, AG News, SST2) and settings (heterogeneous models/data) that the server model can be trained much faster, requiring fewer communication rounds than any existing FL technique so far.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "145724662",
                        "name": "Tao Lin"
                    },
                    {
                        "authorId": "2069275317",
                        "name": "Lingjing Kong"
                    },
                    {
                        "authorId": "2127057",
                        "name": "S. Stich"
                    },
                    {
                        "authorId": "2456863",
                        "name": "Martin Jaggi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Furthermore, Micaelli and Storkey (2019) utilized an adversarial generator to generate hard examples for knowledge transfer.",
                "d to generate synthetic data, which is either directly used as the training dataset (Chen et al., 2019a) or used to augment the training dataset (Liu et al., 2018b), shown in Fig. 8 (a). Furthermore, Micaelli and Storkey (2019) utilized an adversarial generator to generate hard examples for knowledge transfer. 2) A discriminator is introduced to distinguish the samples from the student and the teacher models by using either",
                "\u2026to enable the teacher and student networks to have a better understanding of the true data distribution (Wang et al. 2018e; Xu et al. 2018a; Micaelli and Storkey 2019; Xu et al. 2018b; Liu et al. 2018; Wang et al. 2018f; Chen et al. 2019a; Shen et al. 2019d; Shu et al. 2019; Liu et al.\u2026",
                "Specifically, in (Chen et al. 2019a; Ye et al. 2020;Micaelli and Storkey 2019; Yoo et al. 2019; Hu et al. 2020), the transfer data is generated by a GAN.",
                "In fact, the target data in (Micaelli and Storkey 2019; Nayak et al. 2019) is generated by using the information from the feature representations of teacher networks.",
                "\u2026have been proposed to overcome problems with unavailable data arising from privacy, legality, security and confidentiality concerns (Chen et al. 2019a; Lopes et al. 2017; Nayak et al. 2019; Micaelli and Storkey 2019; Haroush et al. 2020; Ye et al. 2020; Nayak et al. 2021; Chawla et al. 2021).",
                "sarial knowledge distillation methods have been proposed to enable the teacher and student networks to have a better understanding of the true data distribution (Wang et al., 2018d; Xu et al., 2018a; Micaelli and Storkey, 2019; Xu et al., 2018b; Liu et al., 2018b; Wang et al., 2018e; Chen et al., 2019a; Shen et al., 2019b; Shu et al., 2019; Liu et al., 2018a; Belagiannis et al., 2018). T/D S Distillation G Data (a) T S Dis",
                "free KD methods have been proposed to overcome problems with unavailable data arising from privacy,legality,securityandcon\ufb01dentialityconcerns(Chen et al., 2019a; Lopes et al.,2017; Nayak et al., 2019;Micaelli and Storkey, 2019). Just as \u201cdata free\u201d implies, there is no training data. Instead, the data is newly or synthetically generated.In (Chen et al., 2019a; Micaelli and Storkey, 2019), the transfer data is generated by a",
                "o-shot knowledgedistillationmethod that doesnot useexistingdata. The transfer data is produced by modelling the softmax space using the parameters of the teacher network. In fact, the target data in (Micaelli and Storkey, 2019; Nayak et al., 2019) is generated by using the information from the feature representations of teacher networks. Similar to zero-shot learning, a knowledge distillation method with few-shot learning "
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "1728cb805a9573b59330890ba9723e73d6c3c974",
                "externalIds": {
                    "DBLP": "journals/ijcv/GouYMT21",
                    "MAG": "3034368386",
                    "ArXiv": "2006.05525",
                    "DOI": "10.1007/s11263-021-01453-z",
                    "CorpusId": 219559263
                },
                "corpusId": 219559263,
                "publicationVenue": {
                    "id": "939ee07c-6009-43f8-b884-69238b40659e",
                    "name": "International Journal of Computer Vision",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Comput Vis"
                    ],
                    "issn": "0920-5691",
                    "url": "https://www.springer.com/computer/image+processing/journal/11263",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11263",
                        "http://link.springer.com/journal/11263"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1728cb805a9573b59330890ba9723e73d6c3c974",
                "title": "Knowledge Distillation: A Survey",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "38978232",
                        "name": "Jianping Gou"
                    },
                    {
                        "authorId": "2425630",
                        "name": "B. Yu"
                    },
                    {
                        "authorId": "144555237",
                        "name": "S. Maybank"
                    },
                    {
                        "authorId": "143719920",
                        "name": "D. Tao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, data-free distillation, a novel scenario in which the original data for the teacher is unavailable to students, has also been extensively studied [6, 7, 25, 40]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2bdfc6d8f6d03b38b80b8aa4112088323b6b552f",
                "externalIds": {
                    "MAG": "3035409869",
                    "ArXiv": "2006.05065",
                    "DBLP": "conf/nips/ZhangS20",
                    "CorpusId": 219558831
                },
                "corpusId": 219558831,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2bdfc6d8f6d03b38b80b8aa4112088323b6b552f",
                "title": "Self-Distillation as Instance-Specific Label Smoothing",
                "abstract": "It has been recently demonstrated that multi-generational self-distillation can improve generalization. Despite this intriguing observation, reasons for the enhancement remain poorly understood. In this paper, we first demonstrate experimentally that the improved performance of multi-generational self-distillation is in part associated with the increasing diversity in teacher predictions. With this in mind, we offer a new interpretation for teacher-student training as amortized MAP estimation, such that teacher predictions enable instance-specific regularization. Our framework allows us to theoretically relate self-distillation to label smoothing, a commonly used technique that regularizes predictive uncertainty, and suggests the importance of predictive diversity in addition to predictive uncertainty. We present experimental results using multiple datasets and neural network architectures that, overall, demonstrate the utility of predictive diversity. Finally, we propose a novel instance-specific label smoothing technique that promotes predictive diversity without the need for a separately trained teacher model. We provide an empirical evaluation of the proposed method, which, we find, often outperforms classical label smoothing.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1491240545",
                        "name": "Zhilu Zhang"
                    },
                    {
                        "authorId": "2369409",
                        "name": "M. Sabuncu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Not used N/A [45]* Inferred in the image domain [33], [34] [43], [41]* Generated from generators N/A [35], [36], Ours*",
                "We compare our scheme to the previous data-free KD methods in [35,36,41] and show that we achieve the state-of-the-art data-free KD performance in all evaluation cases.",
                "In [36], adversarial examples can be any images far different from the original data, which degrade the KD performance.",
                "[36] used the mismatch between the teacher and the student as an adversarial loss for training a generator to produce adversarial examples for KD.",
                "Adversarial learning was introduced to produce dynamic samples for which the teacher and the student poorly matched in their classification output and to perform KD on those adversarial samples [36].",
                "On the other hand, in [35, 36], generators are introduced to produce synthetic samples for KD.",
                "The proposed scheme shows the state-of-the-art data-free KD performance on residual networks [37] and wide residual networks [38] for SVHN [39], CIFAR-10, CIFAR-100 [40], and Tiny-ImageNet1, compared to the previous work [35, 36, 41].",
                "The key difference from [36] lies in the fact that given any metadata, we utilize them to constrain a generator in the adversarial learning framework.",
                "If \u03b1 = 0 in (2), the proposed scheme reduces to the adversarial belief matching presented in [36].",
                "On the other hand, some of the previous approaches introduce another network, called generator, that yields synthetic samples for training student networks [35, 36, 44]."
            ],
            "intents": [
                "methodology",
                "background",
                "result"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "9fcc5bbd70d490adae5a3f00c58adccc356bb321",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2005-04136",
                    "MAG": "3020927126",
                    "ArXiv": "2005.04136",
                    "DOI": "10.1109/CVPRW50498.2020.00363",
                    "CorpusId": 218571034
                },
                "corpusId": 218571034,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9fcc5bbd70d490adae5a3f00c58adccc356bb321",
                "title": "Data-Free Network Quantization With Adversarial Knowledge Distillation",
                "abstract": "Network quantization is an essential procedure in deep learning for development of efficient fixed-point inference models on mobile or edge platforms. However, as datasets grow larger and privacy regulations become stricter, data sharing for model compression gets more difficult and restricted. In this paper, we consider data-free network quantization with synthetic data. The synthetic data are generated from a generator, while no data are used in training the generator and in quantization. To this end, we propose data-free adversarial knowledge distillation, which minimizes the maximum distance between the outputs of the teacher and the (quantized) student for any adversarial samples from a generator. To generate adversarial samples similar to the original data, we additionally propose matching statistics from the batch normalization layers for generated data and the original data in the teacher. Furthermore, we show the gain of producing diverse adversarial samples by using multiple generators and multiple students. Our experiments show the state-of-the-art data-free model compression and quantization results for (wide) residual networks and MobileNet on SVHN, CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets. The accuracy losses compared to using the original datasets are shown to be very minimal.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2501862",
                        "name": "Yoojin Choi"
                    },
                    {
                        "authorId": "33761905",
                        "name": "Jihwan P. Choi"
                    },
                    {
                        "authorId": "1382637019",
                        "name": "Mostafa El-Khamy"
                    },
                    {
                        "authorId": "35462690",
                        "name": "Jungwon Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Note that, much like ABM, DTD also requires white-box access to the target model since the optimization problem in Eqn.",
                "Due to these requirements, ABM cannot be directly used in the black-box setting of model stealing attacks.",
                "Adversarial BeliefMatching (ABM) [24]:ABMperforms knowledge distillation by using images generated from a generative model G(z;\u03c6).",
                "Moreover, ABM also uses AT, which requires access to the intermediate activations of T .",
                "The training process of the generator model in ABM assumes white-box access to the target model as the loss function of G (Eqn.",
                "Recent works on data-free KD [3, 8, 15, 24, 41, 42] have shown that it is possible to perform KD without knowledge of the training data, however, they require white-box access to the teacher model.",
                "Adversarial BeliefMatching (ABM) [24]:ABMperforms knowledge distillation by using images generated from a generative model G(z;\u03d5).",
                "Similar objectives have been used by recent work on data-free Knowledge Distillation (KD) [24, 41].",
                "By iteratively updating the generator and student model, ABM performs knowledge distillation between T and S .\nx = G(z) (19) LG = \u2212DKL (T (x) \u2225 S (x)) (20) LS = DKL (T (x) \u2225 S (x)) (21)\nIn addition to the basic idea presented above, ABM also uses an additional Attention Transfer [43] term in the loss function of the student.",
                "Similar to recent works in data-free KD [8, 24, 41], MAZE trains the generator to produce queries that maximize the disagreement between the predictions of the teacher and the student by maximizing the KL-divergence between \u00ae yT and \u00ae yC ."
            ],
            "intents": [
                "methodology",
                "background",
                "result"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "d5ffa58133940646d1339c2610cb35f27442e0d3",
                "externalIds": {
                    "DBLP": "conf/cvpr/Kariyappa0Q21",
                    "MAG": "3023663521",
                    "ArXiv": "2005.03161",
                    "DOI": "10.1109/CVPR46437.2021.01360",
                    "CorpusId": 218538003
                },
                "corpusId": 218538003,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d5ffa58133940646d1339c2610cb35f27442e0d3",
                "title": "MAZE: Data-Free Model Stealing Attack Using Zeroth-Order Gradient Estimation",
                "abstract": "High quality Machine Learning (ML) models are often considered valuable intellectual property by companies. Model Stealing (MS) attacks allow an adversary with blackbox access to a ML model to replicate its functionality by training a clone model using the predictions of the target model for different inputs. However, best available existing MS attacks fail to produce a high-accuracy clone without access to the target dataset or a representative dataset necessary to query the target model. In this paper, we show that preventing access to the target dataset is not an adequate defense to protect a model. We propose MAZE \u2013 a data-free model stealing attack using zeroth-order gradient estimation that produces high-accuracy clones. In contrast to prior works, MAZE uses only synthetic data created using a generative model to perform MS.Our evaluation with four image classification models shows that MAZE provides a normalized clone accuracy in the range of 0.90\u00d7 to 0.99\u00d7, and outperforms even the recent attacks that rely on partial data (JBDA, clone accuracy 0.13\u00d7 to 0.69\u00d7) and on surrogate data (KnockoffNets, clone accuracy 0.52\u00d7 to 0.97\u00d7). We also study an extension of MAZE in the partial-data setting, and develop MAZE-PD, which generates synthetic data closer to the target distribution. MAZE-PD further improves the clone accuracy (0.97\u00d7 to 1.0\u00d7) and reduces the query budget required for the attack by 2\u00d7-24\u00d7.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51110538",
                        "name": "S. Kariyappa"
                    },
                    {
                        "authorId": "49428189",
                        "name": "A. Prakash"
                    },
                    {
                        "authorId": "143904156",
                        "name": "Moinuddin K. Qureshi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Considering the limitation of metadata and similarity-based distillation methods, some works [31], [52], [157], [251], [255], [257] propose novel data-free KD methods via adversarial learning [65], [222], [224].",
                "To handle this problem, data-free KD paradigms [21], [31], [52], [78], [112], [143], [157], [169], [251], [255], [257] are newly developed."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2528a82dd2266600d4ee2b54165556a984de94d4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2004-05937",
                    "ArXiv": "2004.05937",
                    "MAG": "3015735225",
                    "DOI": "10.1109/TPAMI.2021.3055564",
                    "CorpusId": 215745611,
                    "PubMed": "33513099"
                },
                "corpusId": 215745611,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2528a82dd2266600d4ee2b54165556a984de94d4",
                "title": "Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks",
                "abstract": "Deep neural models, in recent years, have been successful in almost every field, even solving the most complex problem statements. However, these models are huge in size with millions (and even billions) of parameters, demanding heavy computation power and failing to be deployed on edge devices. Besides, the performance boost is highly dependent on redundant labeled data. To achieve faster speeds and to handle the problems caused by the lack of labeled data, knowledge distillation (KD) has been proposed to transfer information learned from one model to another. KD is often characterized by the so-called \u2018Student-Teacher\u2019 (S-T) learning framework and has been broadly applied in model compression and knowledge transfer. This paper is about KD and S-T learning, which are being actively studied in recent years. First, we aim to provide explanations of what KD is and how/why it works. Then, we provide a comprehensive survey on the recent progress of KD methods together with S-T frameworks typically used for vision tasks. In general, we investigate some fundamental questions that have been driving this research area and thoroughly generalize the research progress and technical details. Additionally, we systematically analyze the research status of KD in vision applications. Finally, we discuss the potentials and open challenges of existing methods and prospect the future directions of KD and S-T learning.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2144734901",
                        "name": "Lin Wang"
                    },
                    {
                        "authorId": "51182421",
                        "name": "Kuk-Jin Yoon"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "32a939d781623bbd080e52ce6d8dbebf8d1e5f66",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2003-03603",
                    "ArXiv": "2003.03603",
                    "MAG": "3010449038",
                    "DOI": "10.1007/978-3-030-58610-2_1",
                    "CorpusId": 212633494
                },
                "corpusId": 212633494,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/32a939d781623bbd080e52ce6d8dbebf8d1e5f66",
                "title": "Generative Low-bitwidth Data Free Quantization",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2157422974",
                        "name": "Shoukai Xu"
                    },
                    {
                        "authorId": "1796268589",
                        "name": "Haokun Li"
                    },
                    {
                        "authorId": "3194022",
                        "name": "Bohan Zhuang"
                    },
                    {
                        "authorId": "49270464",
                        "name": "Jing Liu"
                    },
                    {
                        "authorId": "32879676",
                        "name": "Jiezhang Cao"
                    },
                    {
                        "authorId": "1557349917",
                        "name": "Chuangrun Liang"
                    },
                    {
                        "authorId": "2823637",
                        "name": "Mingkui Tan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026has shown merit for improving model performance across a range of scenarios, including student models lacking access to portions of training data (Micaelli & Storkey, 2019), quantized low-precision networks (Polino et al., 2018; Mishra & Marr, 2017), protection against adversarial attacks\u2026"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "381bec98f037d4c6d46d887a6930c56e5e78c5e7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2001-04974",
                    "ArXiv": "2001.04974",
                    "MAG": "2996765603",
                    "CorpusId": 209319194
                },
                "corpusId": 209319194,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/381bec98f037d4c6d46d887a6930c56e5e78c5e7",
                "title": "Noisy Machines: Understanding Noisy Neural Networks and Enhancing Robustness to Analog Hardware Errors Using Distillation",
                "abstract": "The success of deep learning has brought forth a wave of interest in computer hardware design to better meet the high demands of neural network inference. In particular, analog computing hardware has been heavily motivated specifically for accelerating neural networks, based on either electronic, optical or photonic devices, which may well achieve lower power consumption than conventional digital electronics. However, these proposed analog accelerators suffer from the intrinsic noise generated by their physical components, which makes it challenging to achieve high accuracy on deep neural networks. Hence, for successful deployment on analog accelerators, it is essential to be able to train deep neural networks to be robust to random continuous noise in the network weights, which is a somewhat new challenge in machine learning. In this paper, we advance the understanding of noisy neural networks. We outline how a noisy neural network has reduced learning capacity as a result of loss of mutual information between its input and output. To combat this, we propose using knowledge distillation combined with noise injection during training to achieve more noise robust networks, which is demonstrated experimentally across different networks and datasets, including ImageNet. Our method achieves models with as much as two times greater noise tolerance compared with the previous best attempts, which is a significant step towards making analog hardware practical for deep learning.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "143946900",
                        "name": "Chuteng Zhou"
                    },
                    {
                        "authorId": "40950228",
                        "name": "Prad Kadambi"
                    },
                    {
                        "authorId": "39045061",
                        "name": "Matthew Mattina"
                    },
                    {
                        "authorId": "3313708",
                        "name": "P. Whatmough"
                    }
                ]
            }
        },
        {
            "contexts": [
                "the model [10] which exploits the increased confidence of the model on the training data to reconstruct images used for training; [23] proposes a method for performing zero-shot knowledge distillation by adversarially generating a set of exciting images to train a student network."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c956d133b78e0d9b20885593809f0b636ce34093",
                "externalIds": {
                    "MAG": "2985940692",
                    "ArXiv": "1911.04933",
                    "DBLP": "conf/cvpr/GolatkarAS20",
                    "DOI": "10.1109/cvpr42600.2020.00932",
                    "CorpusId": 207863297
                },
                "corpusId": 207863297,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c956d133b78e0d9b20885593809f0b636ce34093",
                "title": "Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks",
                "abstract": "We explore the problem of selectively forgetting a particular subset of the data used for training a deep neural network. While the effects of the data to be forgotten can be hidden from the output of the network, insights may still be gleaned by probing deep into its weights. We propose a method for \"scrubbing\" the weights clean of information about a particular set of training data. The method does not require retraining from scratch, nor access to the data originally used for training. Instead, the weights are modified so that any probing function of the weights is indistinguishable from the same function applied to the weights of a network trained without the data to be forgotten. This condition is a generalized and weaker form of Differential Privacy. Exploiting ideas related to the stability of stochastic gradient descent, we introduce an upper-bound on the amount of information remaining in the weights, which can be estimated efficiently even for deep neural networks.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "35838711",
                        "name": "Aditya Golatkar"
                    },
                    {
                        "authorId": "16163297",
                        "name": "A. Achille"
                    },
                    {
                        "authorId": "1715959",
                        "name": "Stefano Soatto"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026work (Trame\u0300r et al., 2016) reports successful extraction on SVMs and 1-layer networks using i.i.d noise, but no prior work has scaled this idea to deeper neural networks for which a single class tends to dominate model predictions on most noise inputs (Micaelli & Storkey, 2019; Pal et al., 2019).",
                "Our work is related to prior work on data-efficient distillation, which attempts to distill knowledge from a larger model to a small model with access to limited input data (Li et al., 2018) or in a zeroshot setting (Micaelli & Storkey, 2019; Nayak et al., 2019)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ac713aebdcc06f15f8ea61e1140bb360341fdf27",
                "externalIds": {
                    "MAG": "2994896922",
                    "DBLP": "journals/corr/abs-1910-12366",
                    "ArXiv": "1910.12366",
                    "CorpusId": 204907203
                },
                "corpusId": 204907203,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ac713aebdcc06f15f8ea61e1140bb360341fdf27",
                "title": "Thieves on Sesame Street! Model Extraction of BERT-based APIs",
                "abstract": "We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al. 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction---membership classification and API watermarking---which while successful against naive adversaries, are ineffective against more sophisticated ones.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "26161085",
                        "name": "Kalpesh Krishna"
                    },
                    {
                        "authorId": "32012022",
                        "name": "Gaurav Singh Tomar"
                    },
                    {
                        "authorId": "144729897",
                        "name": "Ankur P. Parikh"
                    },
                    {
                        "authorId": "1967156",
                        "name": "Nicolas Papernot"
                    },
                    {
                        "authorId": "2136562",
                        "name": "Mohit Iyyer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This is a natural assumption for a theft-motivated adversary who wishes to steal the oracle for local use\u2014the adversary has data they want to learn the labels of without querying the model! For other adversaries, progress in generative modeling is likely to offer ways to remove this assumption [29]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "52a222d38a8640499010d470d5589a81882bc425",
                "externalIds": {
                    "MAG": "3010489274",
                    "DBLP": "conf/uss/JagielskiCBKP20",
                    "ArXiv": "1909.01838",
                    "CorpusId": 211858541
                },
                "corpusId": 211858541,
                "publicationVenue": {
                    "id": "54649c1d-6bcc-4232-9cd1-aa446867b8d0",
                    "name": "USENIX Security Symposium",
                    "type": "conference",
                    "alternate_names": [
                        "USENIX Secur Symp"
                    ],
                    "url": "http://www.usenix.org/events/bytopic/security.html"
                },
                "url": "https://www.semanticscholar.org/paper/52a222d38a8640499010d470d5589a81882bc425",
                "title": "High Accuracy and High Fidelity Extraction of Neural Networks",
                "abstract": "In a model extraction attack, an adversary steals a copy of a remotely deployed machine learning model, given oracle prediction access. We taxonomize model extraction attacks around two objectives: *accuracy*, i.e., performing well on the underlying learning task, and *fidelity*, i.e., matching the predictions of the remote victim classifier on any input. \nTo extract a high-accuracy model, we develop a learning-based attack exploiting the victim to supervise the training of an extracted model. Through analytical and empirical arguments, we then explain the inherent limitations that prevent any learning-based strategy from extracting a truly high-fidelity model---i.e., extracting a functionally-equivalent model whose predictions are identical to those of the victim model on all possible inputs. Addressing these limitations, we expand on prior work to develop the first practical functionally-equivalent extraction attack for direct extraction (i.e., without training) of a model's weights. \nWe perform experiments both on academic datasets and a state-of-the-art image classifier trained with 1 billion proprietary images. In addition to broadening the scope of model extraction research, our work demonstrates the practicality of model extraction attacks against production-grade systems.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "40844378",
                        "name": "Matthew Jagielski"
                    },
                    {
                        "authorId": "2483738",
                        "name": "Nicholas Carlini"
                    },
                    {
                        "authorId": "39835551",
                        "name": "David Berthelot"
                    },
                    {
                        "authorId": "145714153",
                        "name": "Alexey Kurakin"
                    },
                    {
                        "authorId": "1967156",
                        "name": "Nicolas Papernot"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For other adversaries, progress in generative modeling is likely to offer ways to remove this assumption [29].",
                "Zero shot knowledge transfer [29] has been proposed to transfer knowledge without assumptions on training data."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1a8dd8e74e657332de14c87287d9d1ba7bc7aed6",
                "externalIds": {
                    "MAG": "2972304568",
                    "DBLP": "journals/corr/abs-1909-01838",
                    "CorpusId": 202541324
                },
                "corpusId": 202541324,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1a8dd8e74e657332de14c87287d9d1ba7bc7aed6",
                "title": "High-Fidelity Extraction of Neural Network Models",
                "abstract": "Model extraction allows an adversary to steal a copy of a remotely deployed machine learning model given access to its predictions. Adversaries are motivated to mount such attacks for a variety of reasons, ranging from reducing their computational costs, to eliminating the need to collect expensive training data, to obtaining a copy of a model in order to find adversarial examples, perform membership inference, or model inversion attacks. In this paper, we taxonomize the space of model extraction attacks around two objectives: accuracy, i.e., performing well on the underlying learning task, and fidelity, i.e., matching the predictions of the remote victim classifier on any input. To extract a high-accuracy model, we develop a learning-based attack which exploits the victim to supervise the training of an extracted model. Through analytical and empirical arguments, we then explain the inherent limitations that prevent any learningbased strategy from extracting a truly high-fidelity model\u2014i.e., extracting a functionally-equivalent model whose predictions are identical to those of the victim model on all possible inputs. Addressing these limitations, we expand on prior work to develop the first practical functionally-equivalent extraction attack for direct extraction (i.e., without training) of a model\u2019s weights. We perform experiments both on academic datasets and a state-of-the-art image classifier trained with 1 billion proprietary images. In addition to broadening the scope of model extraction research, our work demonstrates the practicality of model extraction attacks against production-grade systems.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "40844378",
                        "name": "Matthew Jagielski"
                    },
                    {
                        "authorId": "2483738",
                        "name": "Nicholas Carlini"
                    },
                    {
                        "authorId": "39835551",
                        "name": "David Berthelot"
                    },
                    {
                        "authorId": "145714153",
                        "name": "Alexey Kurakin"
                    },
                    {
                        "authorId": "1967156",
                        "name": "Nicolas Papernot"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In this framework, Micaelli & Storkey (2019) targeted generating samples that would cause maximum information gain to the student when learned, however, it also suffers from similar drawbacks as MATE-KD noted above."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a4311dba246f205c9597ec41f854e622eeec2628",
                "externalIds": {
                    "CorpusId": 259902510
                },
                "corpusId": 259902510,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a4311dba246f205c9597ec41f854e622eeec2628",
                "title": "T EACHER G UIDED T RAINING : A N E FFICIENT F RAMEWORK FOR K NOWLEDGE T RANSFER",
                "abstract": "The remarkable performance gains realized by large pretrained models, e.g., GPT-3, hinge on the massive amounts of data they are exposed to during training. Analogously, distilling such large models to compact models for efficient deployment also necessitates a large amount of (labeled or unlabeled) training data. In this paper, we propose the teacher-guided training (TGT) framework for training a high-quality compact model that leverages the knowledge acquired by pretrained generative models, while obviating the need to go through a large volume of data. TGT exploits the fact that the teacher has acquired a good representation of the underlying data domain, which typically corresponds to a much lower dimensional manifold than the input space. Furthermore, we can use the teacher to explore input space more efficiently through sampling or gradient-based methods; thus, making TGT especially attractive for limited data or long-tail settings. We formally capture this benefit of proposed data-domain exploration in our generalization bounds. We find that TGT can improve accuracy on several image classification benchmarks as well as a range of text classification and retrieval tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1771307",
                        "name": "M. Zaheer"
                    },
                    {
                        "authorId": "2241094",
                        "name": "A. Rawat"
                    },
                    {
                        "authorId": "2059143344",
                        "name": "Himanshu Jain"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We adopt a public library4 to reproduce the results of compared approaches: ZSKT [10], DAFL [3] and CMI [5], with the default model hyper-parameters."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "85c62da5734400cc04193e8976b1886106f1d98c",
                "externalIds": {
                    "CorpusId": 249916904
                },
                "corpusId": 249916904,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/85c62da5734400cc04193e8976b1886106f1d98c",
                "title": "Knowledge Distillation with the Reused Teacher Classifier \u2013 Supplementary Material \u2013",
                "abstract": "We adopt two datasets including CIFAR-100 [8] and ImageNet [13] to conduct experiments. All images are normalized by channel means and standard deviations. A horizontal flip is used for data augmentation. CIFAR-1001 contains 50,000 training images and 10,000 test images from 100 classes. Each training image is padded by 4 pixels on each size and randomly cropped as a 32\u00d732 sample. ImageNet2 contains about 1.3 million training images and 50,000 validation images from 1,000 classes. Each image is randomly cropped as a 224x224 sample without padding. The top-1 test accuracy of the teacher model (ResNet-50) is 76.26%. Multi-Teacher Knowledge Distillation. The training hyper-parameters of multi-teacher KD are exactly the same as those of single-teacher KD on CIFAR-100. We first pre-train multiple teacher models with different initialization and then distill their knowledge into a student model. The accuracies of compared AEKD and AEKD-F [4] are obtained by running a public library3 with default model hyper-parameters on our teacher-student combinations [22]. The top-1 test accuracy of two groups of teacher models used in our main submission are: 1 Three ResNet-32x4 models (79.32, 79.43, 79.45), 2 Two ResNet-32x4 models (79.43, 79.45) and one ResNet-110x2 model (78.18). Data-Free Knowledge Distillation. We adopt a public library4 to reproduce the results of compared approaches: ZSKT [10], DAFL [3] and CMI [5], with the default model hyper-parameters. In our experiment, the top-1 test accuracy of the teacher model (WRN-40-2) is 76.31%. The performance of the student model trained with original dataset is included for comparison.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1684692",
                        "name": "Defang Chen"
                    },
                    {
                        "authorId": "51482485",
                        "name": "Jianhan Mei"
                    },
                    {
                        "authorId": "2155916654",
                        "name": "Hailin Zhang"
                    },
                    {
                        "authorId": "2144350104",
                        "name": "Can Wang"
                    },
                    {
                        "authorId": "1692947908",
                        "name": "Yan Feng"
                    },
                    {
                        "authorId": "2109525713",
                        "name": "Chun Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "ZSKD [8]\u2013[10] and SoftTarget [23] models the output label distribution or intermediate feature maps by simple distributions.",
                "Data-Free Knowledge Distillation (DFKD, or ZSKD [8]\u2013 [10]) aims at training student models without training data.",
                "For the baselines, we compare state-of-the-art DFKD methods as DAFL [11], ZSKT [10], ADI [12], DFQ [13], CMI [16] and PRE-DFKD [17]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b70c235a47b1f9d7847dcf5e89b75fbbec80d681",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-13648",
                    "DOI": "10.48550/arXiv.2208.13648",
                    "CorpusId": 251903102
                },
                "corpusId": 251903102,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b70c235a47b1f9d7847dcf5e89b75fbbec80d681",
                "title": "How to Teach: Learning Data-Free Knowledge Distillation from Curriculum",
                "abstract": "\u2014Data-free knowledge distillation (DFKD) aims at training lightweight student networks from teacher networks without training data. Existing approaches mainly follow the paradigm of generating informative samples and progressively updating student models by targeting data priors, boundary sam- ples or memory samples. However, it is dif\ufb01cult for the previous DFKD methods to dynamically adjust the generation strategy at different training stages, which in turn makes it dif\ufb01cult to achieve ef\ufb01cient and stable training. In this paper, we explore how to teach students the model from a curriculum learning (CL) perspective and propose a new approach, namely \u201dCuDFKD\u201d, i.e., \u201dData-Free Knowledge Distillation with Curriculum\u201d. It gradually learns from easy samples to dif\ufb01cult samples, which is similar to the way humans learn. In addition, we provide a theoretical analysis of the majorization minimization (MM) algorithm and explain the convergence of CuDFKD. Experiments conducted on benchmark datasets show that with a simple course design strategy, CuDFKD achieves the best performance over state-of-the-art DFKD methods and different benchmarks, such as 95.28% top1 accuracy of the ResNet18 model on CIFAR10, which is better than training from scratch with data. The training is fast, reaching the highest accuracy of 90% within 30 epochs, and the variance during training is stable. Also in this paper, the applicability of CuDFKD is also analyzed and discussed.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109015309",
                        "name": "Jingru Li"
                    },
                    {
                        "authorId": "2156158437",
                        "name": "Sheng Zhou"
                    },
                    {
                        "authorId": "2145730944",
                        "name": "Liangcheng Li"
                    },
                    {
                        "authorId": "145026971",
                        "name": "Xi Yan"
                    },
                    {
                        "authorId": "2139424603",
                        "name": "Zhi Yu"
                    },
                    {
                        "authorId": "2064698184",
                        "name": "Jiajun Bu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, there has been work [42; 47; 19; 70; 12; 45] on data-free knowledge distillation."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fde921116d09dc16530eff1c2bb41149600355fd",
                "externalIds": {
                    "CorpusId": 261333050
                },
                "corpusId": 261333050,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fde921116d09dc16530eff1c2bb41149600355fd",
                "title": "Domain Adaptation for Image Classification and Crowd Counting",
                "abstract": "We consider a problem of domain adaptation in image classification and crowd counting. Given a pre-trained model learned from a source domain, our goal is to adapt this model to a target domain using unlabeled data. The solution of this problem has a lot of potential applications in computer vision research that require a neural network model adapted to a target dataset. In this thesis, we propose two different approaches for domain adaptation. First, inspired by a source free domain adaptation, we propose a black-box model adaptation and distillation for image classification. The key challenge of this problem setting is that we do not have access to any internal information of the source model, including model architecture, model parameters, or even intermediate feature maps. We can only access the output of the source model, hence the source model is a \u201cblack-box\u201d to us. Once the model is adapted to the target domain, we perform knowledge distillation to obtain a compact model for deployment. Second, we apply dynamic transfer for solving domain adaptation problems in crowd counting. The key insight is that adapting the model for the target domain is achieved by adapting the model across the data samples. The experimental results on several benchmark datasets demonstrate the effectiveness of our approaches.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Yang Wang"
                    },
                    {
                        "authorId": "1474574140",
                        "name": "Shekhor Chanda"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2022 To solve an inherent biased sample generation problem of AL-based C-ZSKD, we propose a method to increase the variance of the adversarial sample distribution by using the convolution of probability distributions and Taylor series approximation.",
                "Therefore, it was experimentally proved that the proposed method greatly mitigates the biased sample generation problem, which is an inherent problem of AL-based C-ZSKD.",
                "A state-of-the-art AL-based C-ZSKD [7] realized AL using Eqs.",
                "The tops of (b) to (d) are the distribution of adversarial samples generated by [7].",
                "The numerical figures in the last row indicate the performance improvements compared to [7].",
                "Performance comparison of the proposed method, [6] and [7] for the teacher model trained with the CIFAR-10 dataset.",
                "B. ANALYSIS FOR EFFECTIVE ADVERSARIAL SAMPLES This section analyzes the characteristics of adversarial samples effective for AL-based C-ZSKD.",
                "On the other hand, the ALbased method [7] need not select the parametric probability distribution for each task.",
                "The other one is complete ZSKD (C-ZSKD) [6], [7] that does not use external training data at all.",
                "In the future, we will expand the AL-based C-ZSKD study in the direction of generating adversarial samples with high entropy in the embedding space.",
                "Numerical figures in the last row indicate improvements over [7].",
                "The second approach is to create adversarial samples to transfer decision boundary information of T to S [7].",
                "As for [7], attention transfer [14] was used as a constraint term for the stability of training, and the distillation loss of [3] was jointly used.",
                "Additionally, by analyzing the distribution of adversarial samples on the embedding space, the characteristic of the most effective\n45460 VOLUME 9, 2021\nadversarial samples for AL-based C-ZSKD is qualitatively demonstrated.",
                "\u2022 By analyzing the distribution of adversarial samples in the embedding space, this paper provides an insight into the characteristics of adversarial samples that are useful for AL-based C-ZSKD.",
                "The numerical figures in the last row indicate improvements over [7].",
                "Therefore, this paper adopts the AL-based C-ZSKD approach for further effective training of the student model.",
                "A. BIASED SAMPLE GENERATION PROBLEM AND A SOLUTION A state-of-the-art AL-based C-ZSKD [7] realized AL using Eqs."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "4ada7d55ed2d924e8a699c9ef30e34c04ee54efb",
                "externalIds": {
                    "DBLP": "journals/access/LeeLS21",
                    "DOI": "10.1109/ACCESS.2021.3066513",
                    "CorpusId": 232413792
                },
                "corpusId": 232413792,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4ada7d55ed2d924e8a699c9ef30e34c04ee54efb",
                "title": "Zero-Shot Knowledge Distillation Using Label-Free Adversarial Perturbation With Taylor Approximation",
                "abstract": "Knowledge distillation (KD) is one of the most effective neural network light-weighting techniques when training data is available. However, KD is seldom applicable to an environment where it is difficult or impossible to access training data. To solve this problem, a complete zero-shot KD (C-ZSKD) based on adversarial learning has been recently proposed, but the so-called biased sample generation problem limits the performance of C-ZSKD. To overcome this limitation, this paper proposes a novel C-ZSKD algorithm that utilizes a label-free adversarial perturbation. The proposed adversarial perturbation derives a constraint of the squared norm of gradient style by using the convolution of probability distributions and the 2nd order Taylor series approximation. The constraint serves to increase the variance of the adversarial sample distribution, which makes the student model learn the decision boundary of the teacher model more accurately without labeled data. Through analysis of the distribution of adversarial samples on the embedded space, this paper also provides an insight into the characteristics of adversarial samples that are effective for adversarial learning-based C-ZSKD.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Kang Il Lee"
                    },
                    {
                        "authorId": "2117177611",
                        "name": "Seunghyun Lee"
                    },
                    {
                        "authorId": "10774886",
                        "name": "B. Song"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We used the same training hyperparameters as in [20] with the proposed loss introduced in Eq.",
                "To further evaluate the influence of the hyperparameter \u03c4 on the student distillation, we performed ablation with \u03c4 \u2208 [2, 5, 10, 15, 20].",
                "Similar to the original paper [20], we also use an attention-transfer loss LAT .",
                "However, recent research [31, 20, 2] has shown the efficacy of KD even under the \"data-free\" scenario where the training data may not be available for the student to get trained.",
                "skeptical student on data-free distillation [20] from a teacher.",
                "To evaluate this, we use recently proposed zero-shot knowledge transfer [20] with the skeptical students using a loss function enhanced by the auxiliary self KD, LSDF = LKL ( \u03c3(g\u03a6S (x, y), \u03c4), \u03c3(g\u03a6T (x, y), \u03c4) ) + LKL ( \u03c3(g\u03a6S (x, y), \u03c4), \u03c3(g\u03a6S (x, y), \u03c4) ) + \u03b3atLAT (4) The first term takes care of knowledge transfer from the teacher, while the second term helps train the final classifier.",
                "To demonstrate skeptical student\u2019s performance under data-free scenario, we leverage the idea of zero shot knowledge transfer [20], a state-of-the-art data-free distillation technique."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "43ef6ccf9030b292a12c1162560be7a9af8f32b4",
                "externalIds": {
                    "DBLP": "conf/nips/KunduSFPB21",
                    "CorpusId": 245123260
                },
                "corpusId": 245123260,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/43ef6ccf9030b292a12c1162560be7a9af8f32b4",
                "title": "Analyzing the Confidentiality of Undistillable Teachers in Knowledge Distillation",
                "abstract": "Knowledge distillation (KD) has recently been identified as a method that can unintentionally leak private information regarding the details of a teacher model to an unauthorized student. Recent research in developing undistillable nasty teachers that can protect model confidentiality has gained significant attention. However, the level of protection these nasty models offer has been largely untested. In this paper, we show that transferring knowledge to a shallow sub-section of a student can largely reduce a teacher\u2019s influence. By exploring the depth of the shallow subsection, we then present a distillation technique that enables a skeptical student model to learn even from a nasty teacher. To evaluate the efficacy of our skeptical students, we conducted experiments with several models with KD under both training data-available and data-free scenarios for various datasets. While distilling from nasty teachers, compared to the normal student models, skeptical students consistently provide superior classification performance of up to \u223c59.5%. Moreover, similar to normal students, skeptical students maintain high classification accuracy when distilled from a normal teacher, showing their efficacy irrespective of the teacher being nasty or not. We believe the ability of skeptical students to largely diminish the KD-immunity of a potentially nasty teacher will motivate the research community to create more robust mechanisms for model confidentiality. We have open-sourced the code at github.com/ksouvik52/Skeptical2021.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2965493",
                        "name": "Souvik Kundu"
                    },
                    {
                        "authorId": "2112386078",
                        "name": "Qirui Sun"
                    },
                    {
                        "authorId": "143785074",
                        "name": "Yao Fu"
                    },
                    {
                        "authorId": "69467609",
                        "name": "M. Pedram"
                    },
                    {
                        "authorId": "2658716",
                        "name": "P. Beerel"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Here the teacher model is WideR40-2 and student is WideR40-1 for comparability to [45].",
                "In Table 4, we find that there is something unique about using a single image, as our method outperforms several synthetic datasets, as well as the GAN-based approach of [45].",
                "CIFAR-10 Fractals [32] StyleGAN [4] ZeroSKD [45] 1-Image (Ours)",
                "These approaches are typically GAN based [14, 45, 80], for example generating datasets of synthetic images that maximally activate neurons in the final layer of teacher."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "a3bb0b8eb848bbc0d25b3328cbe1ae3936ea58fd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-00725",
                    "CorpusId": 244773471
                },
                "corpusId": 244773471,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a3bb0b8eb848bbc0d25b3328cbe1ae3936ea58fd",
                "title": "Extrapolating from a Single Image to a Thousand Classes using Distillation",
                "abstract": "What can neural networks learn about the visual world from a single image? While it obviously cannot contain the multitudes of possible objects, scenes and lighting condi-tions that exist \u2013 within the space of all possible 256 3 \u00b7 224 \u00b7 224 224 -sized square images, it might still provide a strong prior for natural images. To analyze this hypothesis, we develop a framework for training neural networks from scratch using a single image by means of knowledge distillation from a supervised pretrained teacher. With this, we \ufb01nd that the answer to the above question is: \u2018surprisingly, a lot\u2019. In quantitative terms, we \ufb01nd top-1 accuracies of 94% / 74% on CIFAR-10/100, 59 % on ImageNet and, by extending this method to audio, 84 % on SpeechCommands. In extensive analyses we disentangle the effect of augmentations, choice of source image and network architectures and also discover \u201cpanda neurons\u201d in networks that have never seen a panda. This work shows that one image can be used to extrapolate to thousands of object classes and motivates a renewed research agenda on the fundamental interplay of augmentations and images. Webpage: 1",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47792365",
                        "name": "Yuki M. Asano"
                    },
                    {
                        "authorId": "9261711",
                        "name": "Aaqib Saeed"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While several works such as [10], [11] have studied datafree approaches to training deep neural networks, to the best of our knowledge, we are the first to study the effectiveness of our approach from an adversarial robustness perspective.",
                "Unlike the similar works [10], [11] we perform robustness study on student models trained in data-free setup."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "af043101bd547bed75256041fcedcf213a8f06ec",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2101-06069",
                    "CorpusId": 231627596
                },
                "corpusId": 231627596,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/af043101bd547bed75256041fcedcf213a8f06ec",
                "title": "Data Impressions: Mining Deep Models to Extract Samples for Data-free Applications",
                "abstract": "Pretrained deep models hold their learnt knowledge in the form of the model parameters. These parameters act as memory for the trained models and help them generalize well on unseen data. However, in absence of training data, the utility of a trained model is merely limited to either inference or better initialization towards a target task. In this paper, we go further and extract synthetic data by leveraging the learnt model parameters. We dub them Data Impressions, which act as proxy to the training data and can be used to realize a variety of tasks. These are useful in scenarios where only the pretrained models are available and the training data is not shared (e.g., due to privacy or sensitivity concerns). We show the applicability of data impressions in solving several computer vision tasks such as unsupervised domain adaptation, continual learning as well as knowledge distillation. We also study the adversarial robustness of the lightweight models trained via knowledge distillation using these data impressions. Further, we demonstrate the efficacy of data impressions in generating UAPs with better fooling rates. Extensive experiments performed on several benchmark datasets demonstrate competitive performance achieved using data impressions in absence of the original training data.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "143747407",
                        "name": "Gaurav Kumar Nayak"
                    },
                    {
                        "authorId": "2217000",
                        "name": "Konda Reddy Mopuri"
                    },
                    {
                        "authorId": "66320045",
                        "name": "Saksham Jain"
                    },
                    {
                        "authorId": "1429640900",
                        "name": "Anirban Chakraborty"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7629bba5aa89cc6b00789efc5a2d60d1f775dc48",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-03775",
                    "CorpusId": 237440388
                },
                "corpusId": 237440388,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7629bba5aa89cc6b00789efc5a2d60d1f775dc48",
                "title": "FedZKT: Zero-Shot Knowledge Transfer towards Heterogeneous On-Device Models in Federated Learning",
                "abstract": "Federated learning enables distributed devices to collaboratively learn a shared prediction model without centralizing on-device training data. Most of the current algorithms require comparable individual efforts to train on-device models with the same structure and size, impeding participation from resource-constrained devices. Given the widespread yet heterogeneous devices nowadays, this paper proposes a new framework supporting federated learning across heterogeneous on-device models via Zero-shot Knowledge Transfer, named by FedZKT. Specifically, FedZKT allows participating devices to independently determine their on-device models. To transfer knowledge across on-device models, FedZKT develops a zero-shot distillation approach contrary to certain prior research based on a public dataset or a pre-trained data generator. To utmostly reduce on-device workload, the resource-intensive distillation task is assigned to the server, which constructs a generator to adversarially train with the ensemble of the received heterogeneous on-device models. The distilled central knowledge will then be sent back in the form of the corresponding on-device model parameters, which can be easily absorbed at the device side. Experimental studies demonstrate the effectiveness and the robustness of FedZKT towards heterogeneous on-device models and challenging federated learning scenarios, such as non-iid data distribution and straggler effects.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2131620609",
                        "name": "Lan Zhang"
                    },
                    {
                        "authorId": "2115844647",
                        "name": "Xiaoyong Yuan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For all of the methods used to derive the results of this paper, we used the PyTorch framework to train our deep networks along with external components such as Adversarial Belief Matching.",
                "In the experiments of [1], the Kullback-Leibler divergence is used.",
                "\u2022 In attention transfer, the authors in [3] suggest that the best way to extract the spatial attention map would be to use the sum of the square of each individual pixel per channel, but the authors of [1] use the squared mean instead.",
                "In detail, we had to integrate the following settings in our work, which were not mentioned in the paper[1] but implemented in the official repository of the authors: \u2022 To our knowledge, there is no mention about weight initialization in [2] or [3] from the authors of Wide ResNets.",
                "1 Wide Residual Networks Wide Residual Networks (WRNs) were originally proposed in [2] and are used as the main framework for both the teacher and student network in the few-shot knowledge distillation setting of [3] and zero-shot knowledge transfer setting of [1].",
                "Following the notations of [1], we let T (x), S(x; \u03b8) and G(z;\u03c6) be pretrained teacher network, student network and generator, where the weights \u03b8 and \u03c6 parameterize their respective networks that are to be trained.",
                "To follow the notation of [3] and [1] for the rest of this paper we refer to this method as KD-AT.",
                "\u2022 In the zero-shot method of [1] the paper does not mention that weight clipping is performed on both the student and generator networks.",
                "In this work, we reproduce the paper Zero-shot Knowledge Transfer via Adversarial Belief Matching [1], where the authors present a method for distilling the knowledge of a larger pre-trained network to a smaller one, without the use of real data from the side of the student network.",
                "However, in both the few-shot KD and zero-shot settings of [1] teacher and student are compared with the use of KL divergence between the softmax activations of the former and the log-softmax of the latter (KL for the zero-shot model is stated in the paper).",
                "\u2022 There is no description of the Generator network in [1] apart from \"We use a generic generator with only three convolutional layers, and our input noise z has 100 dimensions\"."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "6b2874ce73abf451065c4ce8e426c49daa3362f5",
                "externalIds": {
                    "CorpusId": 250585123
                },
                "corpusId": 250585123,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6b2874ce73abf451065c4ce8e426c49daa3362f5",
                "title": "[Re] Zero-Shot Knowledge Transfer via Adversarial Belief Matching",
                "abstract": "We reproduce the work in Zero-shot Knowledge Transfer via Adversarial Belief Matching, which describes a novel approach for knowledge transfer. A teacher network trained on real samples distills knowledge to a student network that is trained solely on pseudo data extracted from a generator network, with the student trying to mimic the teacher\u2019s outputs on these datapoints. To this end, we additionally re-implement Wide Residual Networks which are used as the main framework for both teacher and student networks and train them from scratch on CIFAR10 and SVHN. We compare the results of the proposed method with a few-shot knowledge distillation attention transfer setting implemented and trained from scratch. We suggest an approach for further exploitation of the learnt mechanics of the generator network in the zero-shot setting, which operates on top of the main method, and brie\ufb02y discuss the bene\ufb01ts and drawbacks of this approach. Our code can be found publicly available in https://github.com/AlexandrosFerles/NIPS_2019_Reproducibilty_ Challenge_Zero-shot_Knowledge_Transfer_via_Adversarial_Belief_Matching .",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2034917720",
                        "name": "Alexandros Ferles"
                    },
                    {
                        "authorId": "2176511241",
                        "name": "Alexander N\u00f6u"
                    },
                    {
                        "authorId": "3442459",
                        "name": "Leonidas Valavanis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Performing KD pre-training using random garbage inputs yields a strong baseline when compared to ABM pre-training.",
                "We adapt the original ABM method for discrete sequences of text by applying first permuting the input sequence within the continuous embedding space, then quantizing the input such that it becomes a valid discrete sequence.",
                "We propose a more nuanced approach based on Adversarial Belief Matching (ABM) [10] which crafts a targeted input that maximizes the KL divergence between the teacher\u2019s output distribution and the student\u2019s output distribution."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "5de7c0f63b95c1ea8b3eae36717e4fa18dc65f1b",
                "externalIds": {
                    "CorpusId": 218578566
                },
                "corpusId": 218578566,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5de7c0f63b95c1ea8b3eae36717e4fa18dc65f1b",
                "title": "Distilling Task-Specific Knowledge from BERT via Adversarial Belief Matching",
                "abstract": "Large pre-trained language models such as BERT [1] have achieved strong results when fine-tuned on a variety of natural language tasks but are cumbersome to deploy. Applying knowledge distillation (KD) [2] to compress these pre-trained models for a specific downstream task is challenging due to the small amount of task-specific labeled data, resulting in poor performance by the compressed model. Considerable efforts have been spent to improve the distillation process for BERT, involving techniques such as leveraging intermediate hints [3], student pre-training [4] and data augmentation [5].",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "35356152",
                        "name": "H. H. Mao"
                    },
                    {
                        "authorId": "3165738",
                        "name": "Bodhisattwa Prasad Majumder"
                    },
                    {
                        "authorId": "35660011",
                        "name": "Julian McAuley"
                    },
                    {
                        "authorId": "48524582",
                        "name": "G. Cottrell"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also reduce the need for collecting and learning from personal data [76]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "928f0a2126fa6f9231c49f87237bd04079fd051f",
                "externalIds": {
                    "MAG": "3102601130",
                    "DBLP": "conf/nips/HuynhE20",
                    "CorpusId": 227275116
                },
                "corpusId": 227275116,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/928f0a2126fa6f9231c49f87237bd04079fd051f",
                "title": "Compositional Zero-Shot Learning via Fine-Grained Dense Feature Composition",
                "abstract": "We develop a novel generative model for zero-shot learning to recognize finegrained unseen classes without training samples. Our observation is that generating holistic features of unseen classes fails to capture every attribute needed to distinguish small differences among classes. We propose a feature composition framework that learns to extract attribute-based features from training samples and combines them to construct fine-grained features for unseen classes. Feature composition allows us to not only selectively compose features of unseen classes from only relevant training samples, but also obtain diversity among composed features via changing samples used for composition. In addition, instead of building a global feature of an unseen class, we use all attribute-based features to form a dense representation consisting of fine-grained attribute details. To recognize unseen classes, we propose a novel training scheme that uses a discriminative model to construct features that are subsequently used to train itself. Therefore, we directly train the discriminative model on composed features without learning separate generative models. We conduct experiments on four popular datasets of DeepFashion, AWA2, CUB, and SUN, showing that our method significantly improves the state of the art.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1954481",
                        "name": "Dat T. Huynh"
                    },
                    {
                        "authorId": "47126776",
                        "name": "Ehsan Elhamifar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The adversary has complete access to the victim model, and uses data-free knowledge transfer (Micaelli & Storkey, 2019; Fang et al., 2019) to train a student model."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "240a4da5158334d5d24c1381cf953cf91ba2bf92",
                "externalIds": {
                    "CorpusId": 251200797
                },
                "corpusId": 251200797,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/240a4da5158334d5d24c1381cf953cf91ba2bf92",
                "title": "ATASET I NFERENCE : O WNERSHIP R ESOLUTION IN M ACHINE L EARNING",
                "abstract": "With increasingly more data and computation involved in their training, machine learning models constitute valuable intellectual property. This has spurred interest in model stealing attacks, which are made more practical by advances in learning with partial, little, or no supervision. Existing defenses focus on inserting unique watermarks in the model\u2019s decision surface, but this is insufficient: since the watermarks are not sampled from the training distribution, they are not always preserved during model stealing. In this paper, we make the key observation that knowledge contained in the stolen model\u2019s training set is what is common to all stolen copies. The adversary\u2019s goal, irrespective of the attack employed, is always to extract this knowledge or its by-products. This gives the original model\u2019s owner a strong advantage over the adversary: model owners have access to the original training data. We thus introduce dataset inference, the process of identifying whether a suspected model copy has private knowledge from the original model\u2019s dataset, as a defense against model stealing. We develop an approach for dataset inference that combines statistical testing with the ability to estimate the distance of multiple data points to the decision boundary. Our experiments on CIFAR10 and CIFAR100 show that model owners can claim with confidence greater than 99% that their model (or dataset as a matter of fact) was stolen, despite only exposing 50 of the stolen model\u2019s training points. Dataset inference defends against state-of-the-art attacks, even when the adversary is adaptive. Unlike prior work, it also does not require retraining or overfitting the defended model.",
                "year": 2020,
                "authors": []
            }
        }
    ]
}