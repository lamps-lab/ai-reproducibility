{
    "offset": 0,
    "data": [
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "In works such as [16, 25, 32, 35, 47], a table is represented by a group of cells."
            ],
            "citingPaper": {
                "paperId": "3e797270bd42586b0e46ce597a5ef6d754465e43",
                "externalIds": {
                    "ArXiv": "2309.14962",
                    "CorpusId": 262825477
                },
                "corpusId": 262825477,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3e797270bd42586b0e46ce597a5ef6d754465e43",
                "title": "GridFormer: Towards Accurate Table Structure Recognition via Grid Prediction",
                "abstract": "All tables can be represented as grids. Based on this observation, we propose GridFormer, a novel approach for interpreting unconstrained table structures by predicting the vertex and edge of a grid. First, we propose a flexible table representation in the form of an MXN grid. In this representation, the vertexes and edges of the grid store the localization and adjacency information of the table. Then, we introduce a DETR-style table structure recognizer to efficiently predict this multi-objective information of the grid in a single shot. Specifically, given a set of learned row and column queries, the recognizer directly outputs the vertexes and edges information of the corresponding rows and columns. Extensive experiments on five challenging benchmarks which include wired, wireless, multi-merge-cell, oriented, and distorted tables demonstrate the competitive performance of our model over other methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "10344582",
                        "name": "Pengyuan Lyu"
                    },
                    {
                        "authorId": "47121217",
                        "name": "Weihong Ma"
                    },
                    {
                        "authorId": "2109798334",
                        "name": "Hongyi Wang"
                    },
                    {
                        "authorId": "1631405123",
                        "name": "Yu Yu"
                    },
                    {
                        "authorId": "2248958848",
                        "name": "Chengquan Zhang"
                    },
                    {
                        "authorId": "2140371004",
                        "name": "Kun Yao"
                    },
                    {
                        "authorId": "2114922667",
                        "name": "Yang Xue"
                    },
                    {
                        "authorId": "1688516",
                        "name": "Jingdong Wang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Early works [37, 34] directly use visual features encoded by convolutional neural networks (CNN) [19] for layout units detection [36, 30, 35, 17], and have been proven to be effective.",
                "Recent deep learning works [37, 34] consider DLA as a classic visual object detection or segmentation problem and utilize convolutional neural networks (CNN) [19] to solve this task [36, 30, 35, 17]."
            ],
            "citingPaper": {
                "paperId": "7d96fa57d5c009c99f04913786ceaf68926d9acf",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-14978",
                    "ArXiv": "2308.14978",
                    "DOI": "10.48550/arXiv.2308.14978",
                    "CorpusId": 261277255
                },
                "corpusId": 261277255,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7d96fa57d5c009c99f04913786ceaf68926d9acf",
                "title": "Vision Grid Transformer for Document Layout Analysis",
                "abstract": "Document pre-trained models and grid-based models have proven to be very effective on various tasks in Document AI. However, for the document layout analysis (DLA) task, existing document pre-trained models, even those pre-trained in a multi-modal fashion, usually rely on either textual features or visual features. Grid-based models for DLA are multi-modality but largely neglect the effect of pre-training. To fully leverage multi-modal information and exploit pre-training techniques to learn better representation for DLA, in this paper, we present VGT, a two-stream Vision Grid Transformer, in which Grid Transformer (GiT) is proposed and pre-trained for 2D token-level and segment-level semantic understanding. Furthermore, a new dataset named D$^4$LA, which is so far the most diverse and detailed manually-annotated benchmark for document layout analysis, is curated and released. Experiment results have illustrated that the proposed VGT model achieves new state-of-the-art results on DLA tasks, e.g. PubLayNet ($95.7\\%$$\\rightarrow$$96.2\\%$), DocBank ($79.6\\%$$\\rightarrow$$84.1\\%$), and D$^4$LA ($67.7\\%$$\\rightarrow$$68.8\\%$). The code and models as well as the D$^4$LA dataset will be made publicly available ~\\url{https://github.com/AlibabaResearch/AdvancedLiterateMachinery}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2065329174",
                        "name": "Cheng Da"
                    },
                    {
                        "authorId": "7619267",
                        "name": "Chuwei Luo"
                    },
                    {
                        "authorId": "2114172383",
                        "name": "Qi Zheng"
                    },
                    {
                        "authorId": "2173739231",
                        "name": "Cong Yao"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "13de1c39dfe5733363ff1a8d4c6d577914f79618",
                "externalIds": {
                    "DBLP": "conf/ijcai/ShenGWQZLC23",
                    "DOI": "10.24963/ijcai.2023/152",
                    "CorpusId": 260853966
                },
                "corpusId": 260853966,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/13de1c39dfe5733363ff1a8d4c6d577914f79618",
                "title": "Divide Rows and Conquer Cells: Towards Structure Recognition for Large Tables",
                "abstract": "Recent advanced Table Structure Recognition (TSR) models adopt image-to-text solutions to parse table structure. These methods can be formulated as image caption problem, i.e., input a single-table image and output table structure description in a specific text format, e.g., HTML. With the impressive success of Transformer in text generation tasks, these methods use Transformer architecture to predict HTML table text in an autoregressive manner. However, tables always emerge with a large variety of shapes and sizes. Autoregressive models usually suffer from the error accumulation problem as the length of predicted text increases, which results in unsatisfactory performance for large tables. In this paper, we propose a novel image-to-text based TSR method that relieves error accumulation problems and improves performance noticeably. At the core of our method is a cascaded two-step decoder architecture with the former decoder predicting HTML table row tags non-autoregressively and the latter predicting HTML table cell tags of each row in a semi-autoregressive manner. Compared with existing methods that predict HTML text autoregressively, the superiority of our row-to-cell progressive table parsing is twofold: (1) it generates an HTML tag sequence with a vertical-and-horizontal two-step `scanning', which better fits the inherent 2D structure of image data, (2) it performs substantially better for large tables (long sequence prediction) since it alleviates error accumulation problem specific to autoregressive models. Extensive experiments demonstrate that our method achieves competitive performance on three public benchmarks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2230137426",
                        "name": "Huawen Shen"
                    },
                    {
                        "authorId": "2149397987",
                        "name": "Xiang Gao"
                    },
                    {
                        "authorId": "2111524263",
                        "name": "Jin Wei"
                    },
                    {
                        "authorId": "2065512194",
                        "name": "Liang Qiao"
                    },
                    {
                        "authorId": "49454803",
                        "name": "Yu Zhou"
                    },
                    {
                        "authorId": "2229647956",
                        "name": "Qiang Li"
                    },
                    {
                        "authorId": "2398015",
                        "name": "Zhanzhan Cheng"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Therefore, the existing approaches [28, 44, 26] to TDR are vision-only approaches.",
                ", table cells) and leverages this prior knowledge in the model design and post-processing [28, 44, 26]."
            ],
            "citingPaper": {
                "paperId": "85e6c2d34304a01e3f7071cdc7200405c4af93d0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-07929",
                    "ArXiv": "2307.07929",
                    "DOI": "10.48550/arXiv.2307.07929",
                    "CorpusId": 259937511
                },
                "corpusId": 259937511,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/85e6c2d34304a01e3f7071cdc7200405c4af93d0",
                "title": "DocTr: Document Transformer for Structured Information Extraction in Documents",
                "abstract": "We present a new formulation for structured information extraction (SIE) from visually rich documents. It aims to address the limitations of existing IOB tagging or graph-based formulations, which are either overly reliant on the correct ordering of input text or struggle with decoding a complex graph. Instead, motivated by anchor-based object detectors in vision, we represent an entity as an anchor word and a bounding box, and represent entity linking as the association between anchor words. This is more robust to text ordering, and maintains a compact graph for entity linking. The formulation motivates us to introduce 1) a DOCument TRansformer (DocTr) that aims at detecting and associating entity bounding boxes in visually rich documents, and 2) a simple pre-training strategy that helps learn entity detection in the context of language. Evaluations on three SIE benchmarks show the effectiveness of the proposed formulation, and the overall approach outperforms existing solutions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145585312",
                        "name": "Haofu Liao"
                    },
                    {
                        "authorId": "2895705",
                        "name": "Aruni RoyChowdhury"
                    },
                    {
                        "authorId": "48624767",
                        "name": "Weijian Li"
                    },
                    {
                        "authorId": "2068427",
                        "name": "Ankan Bansal"
                    },
                    {
                        "authorId": "2108148342",
                        "name": "Yuting Zhang"
                    },
                    {
                        "authorId": "144035504",
                        "name": "Z. Tu"
                    },
                    {
                        "authorId": "1710219",
                        "name": "R. Satzoda"
                    },
                    {
                        "authorId": "1758550",
                        "name": "R. Manmatha"
                    },
                    {
                        "authorId": "48493294",
                        "name": "V. Mahadevan"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "bfdff6cb6fee8d04b41bd99c61556252908b94e4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-13526",
                    "ArXiv": "2306.13526",
                    "DOI": "10.48550/arXiv.2306.13526",
                    "CorpusId": 259243687
                },
                "corpusId": 259243687,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bfdff6cb6fee8d04b41bd99c61556252908b94e4",
                "title": "Bridging the Performance Gap between DETR and R-CNN for Graphical Object Detection in Document Images",
                "abstract": "This paper takes an important step in bridging the performance gap between DETR and R-CNN for graphical object detection. Existing graphical object detection approaches have enjoyed recent enhancements in CNN-based object detection methods, achieving remarkable progress. Recently, Transformer-based detectors have considerably boosted the generic object detection performance, eliminating the need for hand-crafted features or post-processing steps such as Non-Maximum Suppression (NMS) using object queries. However, the effectiveness of such enhanced transformer-based detection algorithms has yet to be verified for the problem of graphical object detection. Essentially, inspired by the latest advancements in the DETR, we employ the existing detection transformer with few modifications for graphical object detection. We modify object queries in different ways, using points, anchor boxes and adding positive and negative noise to the anchors to boost performance. These modifications allow for better handling of objects with varying sizes and aspect ratios, more robustness to small variations in object positions and sizes, and improved image discrimination between objects and non-objects. We evaluate our approach on the four graphical datasets: PubTables, TableBank, NTable and PubLaynet. Upon integrating query modifications in the DETR, we outperform prior works and achieve new state-of-the-art results with the mAP of 96.9\\%, 95.7\\% and 99.3\\% on TableBank, PubLaynet, PubTables, respectively. The results from extensive ablations show that transformer-based methods are more effective for document analysis analogous to other applications. We hope this study draws more attention to the research of using detection transformers in document image analysis.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2029681143",
                        "name": "Tahira Shehzadi"
                    },
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "CascadeTabNet [5], another deep learning-based approach, is proposed for solving two problems: table detection (solved using instance segmentation); followed by table cell segmentation, which is performed, only if the table is unordered.",
                "Different models have been proposed to date, which have been quite successful in localizing the tables between text and images like CDeC-Net (Composite Deformable Cascade Network) [3], Multi-TypeTD-TSR [4] and CascadeTabNet [5].",
                "Among the previous models, the best performance is attained by the CascadeTabNet model, with the corresponding metric values being just greater than 30% on the same IOU.",
                "\u2022 CascadeTabNet [5]:- CascadeTabNet is a three-staged model for Table Detection and Table Structure Recognition."
            ],
            "citingPaper": {
                "paperId": "943a55e5a15b462837c9d317b0df9462982a00a0",
                "externalIds": {
                    "DBLP": "conf/ijcnn/DasGDSM23",
                    "DOI": "10.1109/IJCNN54540.2023.10192028",
                    "CorpusId": 260387207
                },
                "corpusId": 260387207,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/943a55e5a15b462837c9d317b0df9462982a00a0",
                "title": "\u201cFind the Table\u201d: A Contrastive Learning-based Approach with Faster RCNN for Establishing Tabular Entity Relationships",
                "abstract": "Financial industries rely on a variety of data to understand an organization's financial health and performance, and analysis of financial statements is used for decision-making and understanding business activities. Financial statements often have complex, unstructured formats, making it difficult to extract useful information for decision-making. Organizing and refining these data is crucial for effective analysis. Previous research in the field of table detection has primarily centred around object detection methods, with limited exploration of methods for cell-wise information extraction by identifying row and column entities. In order to address this research gap, this paper proposes a novel dataset called TERED (Tabular Entity Relationship Establishment Dataset) to train a model to identify relationships among elements in large financial tables, such as statements and balance sheets, using computer vision techniques which have yet to be fully explored in financial analysis. The dataset contains more than 10,000 tabular data in scanned image and pdf formats and is divided into 12 classes. We trained CLF-RCNN, a Contrastive Learning based Faster RCNN model which in turn is a state-of-the-art object detection model on this dataset and achieved an F1 score of 93 % for table detection and 74% for identifying tabular entities and relationships. Additionally, we introduced a new loss term influenced by contrastive learning that improves prediction performances with our developed algorithm to return the sequential order of the unordered predictions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2226438701",
                        "name": "Sarmistha Das"
                    },
                    {
                        "authorId": "2190514830",
                        "name": "Tuhinangshu Gangopadhyay"
                    },
                    {
                        "authorId": "2226486078",
                        "name": "Atulya Deep"
                    },
                    {
                        "authorId": "145470045",
                        "name": "S. Saha"
                    },
                    {
                        "authorId": "104838863",
                        "name": "A. Maurya"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Later work tackled both table detection and structure recognition simultaneously [20]."
            ],
            "citingPaper": {
                "paperId": "9255caa2757db1728190f22dabcb729b52ad0648",
                "externalIds": {
                    "ArXiv": "2306.07968",
                    "DBLP": "journals/corr/abs-2306-07968",
                    "DOI": "10.48550/arXiv.2306.07968",
                    "CorpusId": 259145157
                },
                "corpusId": 259145157,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9255caa2757db1728190f22dabcb729b52ad0648",
                "title": "arXiVeri: Automatic table verification with GPT",
                "abstract": "Without accurate transcription of numerical data in scientific documents, a scientist cannot draw accurate conclusions. Unfortunately, the process of copying numerical data from one paper to another is prone to human error. In this paper, we propose to meet this challenge through the novel task of automatic table verification (AutoTV), in which the objective is to verify the accuracy of numerical data in tables by cross-referencing cited sources. To support this task, we propose a new benchmark, arXiVeri, which comprises tabular data drawn from open-access academic papers on arXiv. We introduce metrics to evaluate the performance of a table verifier in two key areas: (i) table matching, which aims to identify the source table in a cited document that corresponds to a target table, and (ii) cell matching, which aims to locate shared cells between a target and source table and identify their row and column indices accurately. By leveraging the flexible capabilities of modern large language models (LLMs), we propose simple baselines for table verification. Our findings highlight the complexity of this task, even for state-of-the-art LLMs like OpenAI's GPT-4. The code and benchmark will be made publicly available.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "104387308",
                        "name": "Gyungin Shin"
                    },
                    {
                        "authorId": "10096695",
                        "name": "Weidi Xie"
                    },
                    {
                        "authorId": "7641268",
                        "name": "Samuel Albanie"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "4bcaef5baa082bf4cca420bbfca91c9508111267",
                "externalIds": {
                    "DOI": "10.1109/ICCES57224.2023.10192739",
                    "CorpusId": 260386295
                },
                "corpusId": 260386295,
                "publicationVenue": {
                    "id": "4a0a181c-3807-4883-8ac2-b0ddf5f46497",
                    "name": "International Conference on Communication and Electronics Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Commun Electron Syst",
                        "ICCES",
                        "Int Conf Comput Eng Syst",
                        "International Conference on Computer Engineering and Systems"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4bcaef5baa082bf4cca420bbfca91c9508111267",
                "title": "Table Detection & Data Extraction from Documents using Object Detection",
                "abstract": "Table detection and data extraction from documents is a crucial task in the field of computer vision and document analysis. The goal is to automatically detect tables in documents and extract relevant information from them. This can be achieved through object detection techniques, which use machine learning algorithms to identify objects in images and extract information from them. The process involves training a model on a dataset of table images, and then using the trained model to detect tables in new images. Once tables are detected, data extraction algorithms can be used to extract text and numerical information from the cells of the table. This information can then be used for a variety of applications, such as database creation, business intelligence, and data analysis.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2227045096",
                        "name": "Mohammad Talha Khan"
                    },
                    {
                        "authorId": "9409896",
                        "name": "T. Jeyaprakash"
                    },
                    {
                        "authorId": "40964034",
                        "name": "D. chandar"
                    },
                    {
                        "authorId": "83337181",
                        "name": "V. Durga"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "CascadeTabNet [4] 100 100 100 TableDet [3] 100 100 100 DeCNT [36] 99.",
                "Current state-of-the-art methods [3, 4] for the TD problem usually employ two-stage object detectors, which require dense candidates, and apply data augmentation and multiple-stage transfer learning techniques.",
                "The experimental results of CascadeTabNet [4], TableDet [3], DeCNT [36], YOLOv3TD [33], DeepDeSRT [35], TableNet [45], GAN-TD [46], TableRadar [43] and NLPR-PAL [43] in Table 2, 3 and 4 are from study [3].",
                "CascadeTabNet [4] extends the Cascade Mask R-CNN [19] model and uses HRNet [32] as the backbone network."
            ],
            "citingPaper": {
                "paperId": "86badf878ebdc2b945fad1cf9d204268f62346e7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-19181",
                    "ArXiv": "2305.19181",
                    "DOI": "10.48550/arXiv.2305.19181",
                    "CorpusId": 258967592
                },
                "corpusId": 258967592,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/86badf878ebdc2b945fad1cf9d204268f62346e7",
                "title": "Table Detection for Visually Rich Document Images",
                "abstract": "Table Detection (TD) is a fundamental task towards visually rich document understanding. Current studies usually formulate the TD problem as an object detection problem, then leverage Intersection over Union (IoU) based metrics to evaluate the model performance and IoU-based loss functions to optimize the model. TD applications usually require the prediction results to cover all the table contents and avoid information loss. However, IoU and IoU-based loss functions cannot directly reflect the degree of information loss for the prediction results. Therefore, we propose to decouple IoU into a ground truth coverage term and a prediction coverage term, in which the former can be used to measure the information loss of the prediction results. Besides, tables in the documents are usually large, sparsely distributed, and have no overlaps because they are designed to summarize essential information to make it easy to read and interpret for human readers. Therefore, in this study, we use SparseR-CNN as the base model, and further improve the model by using Gaussian Noise Augmented Image Size region proposals and many-to-one label assignments. To demonstrate the effectiveness of proposed method and compare with state-of-the-art methods fairly, we conduct experiments and use IoU-based evaluation metrics to evaluate the model performance. The experimental results show that the proposed method can consistently outperform state-of-the-art methods under different IoU-based metric on a variety of datasets. We conduct further experiments to show the superiority of the proposed decoupled IoU for the TD applications by replacing the IoU-based loss functions and evaluation metrics with proposed decoupled IoU counterparts. The experimental results show that our proposed decoupled IoU loss can encourage the model to alleviate information loss.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2165556493",
                        "name": "Bin Xiao"
                    },
                    {
                        "authorId": "48778270",
                        "name": "Murat Simsek"
                    },
                    {
                        "authorId": "2497479",
                        "name": "B. Kantarci"
                    },
                    {
                        "authorId": "3082067",
                        "name": "A. A. Alkheir"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "In [2] and [1], the authors use erosion and dilation, an image morphology-based method, to create a mask of table borders.",
                "CascadeTabNet [1], and Multi-Type-TD-TSR [2] are two table recognition CNN-based methods.",
                "Many literature works have studied table detection based on Convolutional Neural Networks (CNN), such as CascadeTabNet [1]."
            ],
            "citingPaper": {
                "paperId": "fa96163c2be80d9f4163e78e2945be051092e0c1",
                "externalIds": {
                    "DBLP": "conf/cscwd/YanTZP023",
                    "DOI": "10.1109/CSCWD57460.2023.10152650",
                    "CorpusId": 259235595
                },
                "corpusId": 259235595,
                "publicationVenue": {
                    "id": "a966c5e8-76dc-45c0-942a-3c7e41ac9b1a",
                    "name": "International Conference on Computer Supported Cooperative Work in Design",
                    "type": "conference",
                    "alternate_names": [
                        "Computer Supported Cooperative Work in Design",
                        "Int Conf Comput Support Cooperative Work Des",
                        "CSCWD",
                        "Comput Support Cooperative Work Des"
                    ],
                    "url": "http://www.cscwd.org/"
                },
                "url": "https://www.semanticscholar.org/paper/fa96163c2be80d9f4163e78e2945be051092e0c1",
                "title": "A Novel Encoder-Decoder Architecture for Table Border Segmentation of Scanned Documents",
                "abstract": "Robotic Process Automation (RPA) has been widely used in business and enterprises to automate the processing of digital documents and collect information and acquire knowledge. Table structure reconstruction in scanned documents has been extensively studied as an essential application of RPA. However, the detection of table borders often ignores broken borders, which makes it unsuitable for natural scenes. To address this, our paper heavily employs a data augmentation approach to synthesize fake scanned documents to train our table-border semantic segmentation model. We propose a novel segmentation model for table borders based on semantic segmentation. We compare traditional morphology-based line detection algorithms with existing semantic segmentation-based approaches. The results indicate that our proposed algorithm can solve the frame line detection problem effectively, even for low-quality scanned images. Actual cases show that we can reconstruct the table\u2019s structure and obtain the knowledge in the table.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2047542417",
                        "name": "Kaihong Yan"
                    },
                    {
                        "authorId": "2153135475",
                        "name": "Hao Tang"
                    },
                    {
                        "authorId": "47539632",
                        "name": "Jing Zhang"
                    },
                    {
                        "authorId": "2072677041",
                        "name": "Peng Peng"
                    },
                    {
                        "authorId": "2154853698",
                        "name": "Hongwei Wang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "[30] Devashish Prasad, Ayan Gadpal, Kshitij Kapadni, Manish",
                "Mainstream approaches include object detection-based models [2, 16, 30], segmentationbased models [4, 15, 40], and multi-modal methods [27, 41, 43]."
            ],
            "citingPaper": {
                "paperId": "5fcf4200caba72f0b342fff4fc41b081bba25b45",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-08719",
                    "ArXiv": "2305.08719",
                    "DOI": "10.1109/CVPR52729.2023.01453",
                    "CorpusId": 258685624
                },
                "corpusId": 258685624,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5fcf4200caba72f0b342fff4fc41b081bba25b45",
                "title": "M6Doc: A Large-Scale Multi-Format, Multi-Type, Multi-Layout, Multi-Language, Multi-Annotation Category Dataset for Modern Document Layout Analysis",
                "abstract": "Document layout analysis is a crucial prerequisite for document understanding, including document retrieval and conversion. Most public datasets currently contain only PDF documents and lack realistic documents. Models trained on these datasets may not generalize well to real-world scenarios. Therefore, this paper introduces a large and diverse document layout analysis dataset called M6Doc. The M6 designation represents six properties: (1) Multi-Format (including scanned, photographed, and PDF documents); (2) Multi-Type (such as scientific articles, textbooks, books, test papers, magazines, newspapers, and notes); (3) Multi-Layout (rectangular, Manhattan, non-Manhattan, and multi-column Manhattan); (4) Multi-Language (Chinese and English); (5) Multi-Annotation Category (74 types of annotation labels with 237,116 annotation instances in 9,080 manually annotated pages); and (6) Modern documents. Additionally, we propose a transformer-based document layout analysis method called TransDLANet, which leverages an adaptive element matching mechanism that enables query embedding to better match ground truth to improve recall, and constructs a segmentation branch for more precise document image instance segmentation. We conduct a comprehensive evaluation of M6Doc with various layout analysis methods and demonstrate its effectiveness. TransDLANet achieves state-of-the-art performance on M6 Doc with 64.5% mAP. The M6Doc dataset will be available at https://github.com/HcIILAB/M6Doc.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2193150480",
                        "name": "Hiuyi Cheng"
                    },
                    {
                        "authorId": "2115513599",
                        "name": "Pei-yu Zhang"
                    },
                    {
                        "authorId": "66939932",
                        "name": "Sihang Wu"
                    },
                    {
                        "authorId": "2118131879",
                        "name": "Jiaxin Zhang"
                    },
                    {
                        "authorId": "1476704317",
                        "name": "Qi Zhu"
                    },
                    {
                        "authorId": "2002678",
                        "name": "Zecheng Xie"
                    },
                    {
                        "authorId": "2152914623",
                        "name": "Jing Li"
                    },
                    {
                        "authorId": "2053138829",
                        "name": "Kai Ding"
                    },
                    {
                        "authorId": "144838978",
                        "name": "Lianwen Jin"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "%) in the Table Detection task on the TableBank dataset as it has only one category and comparatively less challenging layouts.",
                "We use large-scale annotated datasets like PubLayNet [49], TableBank [36], and Historical Japanese (HJ) [41] as well as small-scale PRIMA [10] for evaluating our proposed segmentation approach in this work (Please refer to Table 1 for a detailed description).",
                "Here, we need a temperature hyperparameter \u03c4 to tune the layers to enhance the features based on the datasets we have used. \u03c4 = 0.02, 0.6, 0.1, and 0.2 for PublayNet, Prima, HJ, and TableBank respectively."
            ],
            "citingPaper": {
                "paperId": "f72376882f788a49c5943046d27becc0ad54c0ad",
                "externalIds": {
                    "DBLP": "conf/icdar/BanerjeeBLP23",
                    "ArXiv": "2305.04609",
                    "DOI": "10.48550/arXiv.2305.04609",
                    "CorpusId": 258556923
                },
                "corpusId": 258556923,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/f72376882f788a49c5943046d27becc0ad54c0ad",
                "title": "SwinDocSegmenter: An End-to-End Unified Domain Adaptive Transformer for Document Instance Segmentation",
                "abstract": "Instance-level segmentation of documents consists in assigning a class-aware and instance-aware label to each pixel of the image. It is a key step in document parsing for their understanding. In this paper, we present a unified transformer encoder-decoder architecture for en-to-end instance segmentation of complex layouts in document images. The method adapts a contrastive training with a mixed query selection for anchor initialization in the decoder. Later on, it performs a dot product between the obtained query embeddings and the pixel embedding map (coming from the encoder) for semantic reasoning. Extensive experimentation on competitive benchmarks like PubLayNet, PRIMA, Historical Japanese (HJ), and TableBank demonstrate that our model with SwinL backbone achieves better segmentation performance than the existing state-of-the-art approaches with the average precision of \\textbf{93.72}, \\textbf{54.39}, \\textbf{84.65} and \\textbf{98.04} respectively under one billion parameters. The code is made publicly available at: \\href{https://github.com/ayanban011/SwinDocSegmenter}{github.com/ayanban011/SwinDocSegmenter}",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2153585985",
                        "name": "Ayan Banerjee"
                    },
                    {
                        "authorId": "2150473007",
                        "name": "Sanket Biswas"
                    },
                    {
                        "authorId": "2215822671",
                        "name": "Josep Llad'os"
                    },
                    {
                        "authorId": "144167309",
                        "name": "U. Pal"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "On the other hand, the problem of table structure recognition (TSR) is a lot more challenging and remains a very active area of research, in which many novel machine learning algorithms are being explored [3,4,5,9,11,12,13,14,17,18,21,22].",
                "Object-detection based methods [11,12,13,14,21] rely on tablestructure annotation using (overlapping) bounding boxes for training, and produce bounding-box predictions to define table cells, rows, and columns on a table image."
            ],
            "citingPaper": {
                "paperId": "88b556a6108b98cd0e12ae2945cc6597fff9101c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-03393",
                    "ArXiv": "2305.03393",
                    "DOI": "10.48550/arXiv.2305.03393",
                    "CorpusId": 258546918
                },
                "corpusId": 258546918,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/88b556a6108b98cd0e12ae2945cc6597fff9101c",
                "title": "Optimized Table Tokenization for Table Structure Recognition",
                "abstract": "Extracting tables from documents is a crucial task in any document conversion pipeline. Recently, transformer-based models have demonstrated that table-structure can be recognized with impressive accuracy using Image-to-Markup-Sequence (Im2Seq) approaches. Taking only the image of a table, such models predict a sequence of tokens (e.g. in HTML, LaTeX) which represent the structure of the table. Since the token representation of the table structure has a significant impact on the accuracy and run-time performance of any Im2Seq model, we investigate in this paper how table-structure representation can be optimised. We propose a new, optimised table-structure language (OTSL) with a minimized vocabulary and specific rules. The benefits of OTSL are that it reduces the number of tokens to 5 (HTML needs 28+) and shortens the sequence length to half of HTML on average. Consequently, model accuracy improves significantly, inference time is halved compared to HTML-based models, and the predicted table structures are always syntactically correct. This in turn eliminates most post-processing needs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1578242957",
                        "name": "Maksym Lysak"
                    },
                    {
                        "authorId": "73241238",
                        "name": "Ahmed Nassar"
                    },
                    {
                        "authorId": "2051177400",
                        "name": "Nikolaos Livathinos"
                    },
                    {
                        "authorId": "2065064783",
                        "name": "Christoph Auer"
                    },
                    {
                        "authorId": "1831851",
                        "name": "P. Staar"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "For direct comparison against the prior state-of-the-art [67], we provide results on the modern datasets with an IoU threshold ranging from 0.",
                "Furthermore, [63,67,69] applied different image transformation approaches, such as coloration and dilation, to improve the results further.",
                "Later, more efficient single-stage object detectors like RetinaNet [58] and YOLO [59] and two-stage object detectors like Fast R-CNN [11], Faster R-CNN [12], Mask R-CNN [60], and Cascade Mask R-CNN [61] were applied for other document objects such as figures and formulas detection in document images [62,63,64,65,66,67,68,16,17,18]."
            ],
            "citingPaper": {
                "paperId": "8f87ad8807a4b149c69bac1f169d9f28d13f8928",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-02769",
                    "ArXiv": "2305.02769",
                    "DOI": "10.48550/arXiv.2305.02769",
                    "CorpusId": 258480298
                },
                "corpusId": 258480298,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/8f87ad8807a4b149c69bac1f169d9f28d13f8928",
                "title": "Towards End-to-End Semi-Supervised Table Detection with Deformable Transformer",
                "abstract": "Table detection is the task of classifying and localizing table objects within document images. With the recent development in deep learning methods, we observe remarkable success in table detection. However, a significant amount of labeled data is required to train these models effectively. Many semi-supervised approaches are introduced to mitigate the need for a substantial amount of label data. These approaches use CNN-based detectors that rely on anchor proposals and post-processing stages such as NMS. To tackle these limitations, this paper presents a novel end-to-end semi-supervised table detection method that employs the deformable transformer for detecting table objects. We evaluate our semi-supervised method on PubLayNet, DocBank, ICADR-19 and TableBank datasets, and it achieves superior performance compared to previous methods. It outperforms the fully supervised method (Deformable transformer) by +3.4 points on 10\\% labels of TableBank-both dataset and the previous CNN-based semi-supervised approach (Soft Teacher) by +1.8 points on 10\\% labels of PubLayNet dataset. We hope this work opens new possibilities towards semi-supervised and unsupervised table detection methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2029681143",
                        "name": "Tahira Shehzadi"
                    },
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "CascadeTabNet [27] is a typical two-stage model which is applied to the Table Detection problem."
            ],
            "citingPaper": {
                "paperId": "f35ae7f12b3713fae216688c358d8a528973d286",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-04833",
                    "ArXiv": "2305.04833",
                    "DOI": "10.48550/arXiv.2305.04833",
                    "CorpusId": 258557161
                },
                "corpusId": 258557161,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f35ae7f12b3713fae216688c358d8a528973d286",
                "title": "Revisiting Table Detection Datasets for Visually Rich Documents",
                "abstract": "Table Detection has become a fundamental task for visually rich document understanding with the surging number of electronic documents. There have been some open datasets widely used in many studies. However, popular available datasets have some inherent limitations, including the noisy and inconsistent samples, and the limit number of training samples, and the limit number of data-sources. These limitations make these datasets unreliable to evaluate the model performance and cannot reflect the actual capacity of models. Therefore, in this paper, we revisit some open datasets with high quality of annotations, identify and clean the noise, and align the annotation definitions of these datasets to merge a larger dataset, termed with Open-Tables. Moreover, to enrich the data sources, we propose a new dataset, termed with ICT-TD, using the PDF files of Information and communication technologies (ICT) commodities which is a different domain containing unique samples that hardly appear in open datasets. To ensure the label quality of the dataset, we annotated the dataset manually following the guidance of a domain expert. The proposed dataset has a larger intra-variance and smaller inter-variance, making it more challenging and can be a sample of actual cases in the business context. We built strong baselines using various state-of-the-art object detection models and also built the baselines in the cross-domain setting. Our experimental results show that the domain difference among existing open datasets are small, even they have different data-sources. Our proposed Open-tables and ICT-TD are more suitable for the cross domain setting, and can provide more reliable evaluation for model because of their high quality and consistent annotations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2165556493",
                        "name": "Bin Xiao"
                    },
                    {
                        "authorId": "48778270",
                        "name": "Murat Simsek"
                    },
                    {
                        "authorId": "2497479",
                        "name": "B. Kantarci"
                    },
                    {
                        "authorId": "3082067",
                        "name": "A. A. Alkheir"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Several studies have proposed detectors for detecting cells or their contents [28,31,30,49].",
                "CascadeTabNet[28] proposed a novel approach to classify table and cell regions simultaneously even though they used a single model.",
                "To the best of our knowledge, only two such researches [27,28] have been studied so far.",
                "Some TD methods are not directly compared here, because 1) CascadeTabNet [28] used a subset of test images for evaluation, and 2) CDeC-Net [1] and\nother methods [36,11,42] reported their performance using IoU-based metrics.",
                "Some TD methods are not directly compared here, because 1) CascadeTabNet [28] used a subset of test images for evaluation, and 2) CDeC-Net [1] and",
                "Recently, thanks to its instance segmentation ability, Mask R-CNN-based methods [28,1] were studied."
            ],
            "citingPaper": {
                "paperId": "eca4dfc1703529bd87e1b4fea1fad1baa06e0ac9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-00630",
                    "ArXiv": "2305.00630",
                    "DOI": "10.48550/arXiv.2305.00630",
                    "CorpusId": 258427145
                },
                "corpusId": 258427145,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/eca4dfc1703529bd87e1b4fea1fad1baa06e0ac9",
                "title": "TRACE: Table Reconstruction Aligned to Corner and Edges",
                "abstract": "A table is an object that captures structured and informative content within a document, and recognizing a table in an image is challenging due to the complexity and variety of table layouts. Many previous works typically adopt a two-stage approach; (1) Table detection(TD) localizes the table region in an image and (2) Table Structure Recognition(TSR) identifies row- and column-wise adjacency relations between the cells. The use of a two-stage approach often entails the consequences of error propagation between the modules and raises training and inference inefficiency. In this work, we analyze the natural characteristics of a table, where a table is composed of cells and each cell is made up of borders consisting of edges. We propose a novel method to reconstruct the table in a bottom-up manner. Through a simple process, the proposed method separates cell boundaries from low-level features, such as corners and edges, and localizes table positions by combining the cells. A simple design makes the model easier to train and requires less computation than previous two-stage methods. We achieve state-of-the-art performance on the ICDAR2013 table competition benchmark and Wired Table in the Wild(WTW) dataset.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2057310228",
                        "name": "Youngmin Baek"
                    },
                    {
                        "authorId": "2064808754",
                        "name": "Daehyun Nam"
                    },
                    {
                        "authorId": "10787779",
                        "name": "Jaeheung Surh"
                    },
                    {
                        "authorId": "2111068603",
                        "name": "Seung Shin"
                    },
                    {
                        "authorId": "2109603647",
                        "name": "Seonghyeon Kim"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "fe211819c3959eaa5382177016e8091465e84861",
                "externalIds": {
                    "DBLP": "journals/pr/MondalAJ23",
                    "DOI": "10.1016/j.patcog.2023.109698",
                    "CorpusId": 258826399
                },
                "corpusId": 258826399,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fe211819c3959eaa5382177016e8091465e84861",
                "title": "Dataset agnostic document object detection",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2896521",
                        "name": "Ajoy Mondal"
                    },
                    {
                        "authorId": "2148629212",
                        "name": "Madhav Agarwal"
                    },
                    {
                        "authorId": "1694502",
                        "name": "C. V. Jawahar"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "contexts": [
                "Only 2 papers [26, 6] were identified as conditionally replicable on a similar dataset, and none of the papers was identified as replicable with respect to the new dataset.",
                "The results shown in Figure 3 and Figure 4 indicate that none of the 4 methods that allow inference on custom data [26, 16, 6, 38] was replicable with respect to the GenTSR dataset, under a threshold of 10% absolute F1-score.",
                "Recently, several end-to-end solutions based on neural networks have been proposed [26, 6, 16].",
                "In this work, we investigate the reproducibility and replicability of methods for table structure recognition, an AI task aimed at parsing tables in digital documents and automatically identifying rows, columns, and cell positions in a detected table image within a document [26].",
                "Prasad et al.[26](5) 2019 Cascade TabNet CVPR Marmot + ICDAR 2019 ICDAR 2019 Track-B2 \u2713 \u2713 PR"
            ],
            "citingPaper": {
                "paperId": "50903f5f5e714e5746152b3e369c9329655e0018",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-10439",
                    "ArXiv": "2304.10439",
                    "DOI": "10.48550/arXiv.2304.10439",
                    "CorpusId": 258236507
                },
                "corpusId": 258236507,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/50903f5f5e714e5746152b3e369c9329655e0018",
                "title": "A Study on Reproducibility and Replicability of Table Structure Recognition Methods",
                "abstract": "Concerns about reproducibility in artificial intelligence (AI) have emerged, as researchers have reported unsuccessful attempts to directly reproduce published findings in the field. Replicability, the ability to affirm a finding using the same procedures on new data, has not been well studied. In this paper, we examine both reproducibility and replicability of a corpus of 16 papers on table structure recognition (TSR), an AI task aimed at identifying cell locations of tables in digital documents. We attempt to reproduce published results using codes and datasets provided by the original authors. We then examine replicability using a dataset similar to the original as well as a new dataset, GenTSR, consisting of 386 annotated tables extracted from scientific papers. Out of 16 papers studied, we reproduce results consistent with the original in only four. Two of the four papers are identified as replicable using the similar dataset under certain IoU values. No paper is identified as replicable using the new dataset. We offer observations on the causes of irreproducibility and irreplicability. All code and data are available on Codeocean at https://codeocean.com/capsule/6680116/tree.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "11359490",
                        "name": "Kehinde E. Ajayi"
                    },
                    {
                        "authorId": "2214808132",
                        "name": "Muntabhir Hasan Choudhury"
                    },
                    {
                        "authorId": "2528276",
                        "name": "S. Rajtmajer"
                    },
                    {
                        "authorId": "97569165",
                        "name": "Jian Wu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "3e0c97baa8e7b629a3d689e00aa21b94e2193766",
                "externalIds": {
                    "ArXiv": "2303.11615",
                    "DBLP": "journals/corr/abs-2303-11615",
                    "DOI": "10.48550/arXiv.2303.11615",
                    "CorpusId": 257636821
                },
                "corpusId": 257636821,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3e0c97baa8e7b629a3d689e00aa21b94e2193766",
                "title": "Robust Table Structure Recognition with Dynamic Queries Enhanced Detection Transformer",
                "abstract": "We present a new table structure recognition (TSR) approach, called TSRFormer, to robustly recognizing the structures of complex tables with geometrical distortions from various table images. Unlike previous methods, we formulate table separation line prediction as a line regression problem instead of an image segmentation problem and propose a new two-stage dynamic queries enhanced DETR based separation line regression approach, named DQ-DETR, to predict separation lines from table images directly. Compared to Vallina DETR, we propose three improvements in DQ-DETR to make the two-stage DETR framework work efficiently and effectively for the separation line prediction task: 1) A new query design, named Dynamic Query, to decouple single line query into separable point queries which could intuitively improve the localization accuracy for regression tasks; 2) A dynamic queries based progressive line regression approach to progressively regressing points on the line which further enhances localization accuracy for distorted tables; 3) A prior-enhanced matching strategy to solve the slow convergence issue of DETR. After separation line prediction, a simple relation network based cell merging module is used to recover spanning cells. With these new techniques, our TSRFormer achieves state-of-the-art performance on several benchmark datasets, including SciTSR, PubTabNet, WTW and FinTabNet. Furthermore, we have validated the robustness and high localization accuracy of our approach to tables with complex structures, borderless cells, large blank spaces, empty or spanning cells as well as distorted or even curved shapes on a more challenging real-world in-house dataset.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2187553093",
                        "name": "Jiawei Wang"
                    },
                    {
                        "authorId": "2108811137",
                        "name": "Weihong Lin"
                    },
                    {
                        "authorId": "2894335",
                        "name": "Chixiang Ma"
                    },
                    {
                        "authorId": "2181111465",
                        "name": "Mingze Li"
                    },
                    {
                        "authorId": "2145130513",
                        "name": "Zhengmao Sun"
                    },
                    {
                        "authorId": "2110833051",
                        "name": "Lei Sun"
                    },
                    {
                        "authorId": "2316043",
                        "name": "Qiang Huo"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                ", 2021), many researchers only focused on the table structure recognition problem (Prasad et al., 2020; Raja et al., 2020; Schreiber et al., 2017).",
                "In recent years, motivated by the success of deep learning, especially in object detection and semantic segmentation, many deep learning-based methods (Prasad et al., 2020; Raja et al., 2020; Schreiber et al., 2017) have been proposed and proven to be powerful models for table structure recognition."
            ],
            "citingPaper": {
                "paperId": "bc8fc30c2546979cfd7e1658b868fdbb02f9a22a",
                "externalIds": {
                    "ArXiv": "2303.07641",
                    "DBLP": "journals/corr/abs-2303-07641",
                    "DOI": "10.5220/0011682600003411",
                    "CorpusId": 257356700
                },
                "corpusId": 257356700,
                "publicationVenue": {
                    "id": "8ef5945c-5b25-4774-b55a-15cd5450f6e4",
                    "name": "International Conference on Pattern Recognition Applications and Methods",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Pattern Recognit Appl Method",
                        "ICPRAM"
                    ],
                    "url": "http://icpram.org/"
                },
                "url": "https://www.semanticscholar.org/paper/bc8fc30c2546979cfd7e1658b868fdbb02f9a22a",
                "title": "Rethinking Image-based Table Recognition Using Weakly Supervised Methods",
                "abstract": "Most of the previous methods for table recognition rely on training datasets containing many richly annotated table images. Detailed table image annotation, e.g., cell or text bounding box annotation, however, is costly and often subjective. In this paper, we propose a weakly supervised model named WSTabNet for table recognition that relies only on HTML (or LaTeX) code-level annotations of table images. The proposed model consists of three main parts: an encoder for feature extraction, a structure decoder for generating table structure, and a cell decoder for predicting the content of each cell in the table. Our system is trained end-to-end by stochastic gradient descent algorithms, requiring only table images and their ground-truth HTML (or LaTeX) representations. To facilitate table recognition with deep learning, we create and release WikiTableSet, the largest publicly available image-based table recognition dataset built from Wikipedia. WikiTableSet contains nearly 4 million English table images, 590K Japanese table images, and 640k French table images with corresponding HTML representation and cell bounding boxes. The extensive experiments on WikiTableSet and two large-scale datasets: FinTabNet and PubTabNet demonstrate that the proposed weakly supervised model achieves better, or similar accuracies compared to the state-of-the-art models on all benchmark datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "40325274",
                        "name": "N. Ly"
                    },
                    {
                        "authorId": "144752372",
                        "name": "A. Takasu"
                    },
                    {
                        "authorId": "2100790",
                        "name": "Phuc Nguyen"
                    },
                    {
                        "authorId": "2052440572",
                        "name": "H. Takeda"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "5966cd3ac4460a97b27b4177a89c0d1041726881",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-06949",
                    "ArXiv": "2303.06949",
                    "DOI": "10.1109/CVPR52729.2023.01071",
                    "CorpusId": 257495840
                },
                "corpusId": 257495840,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5966cd3ac4460a97b27b4177a89c0d1041726881",
                "title": "Improving Table Structure Recognition with Visual-Alignment Sequential Coordinate Modeling",
                "abstract": "Table structure recognition aims to extract the logical and physical structure of unstructured table images into a machine-readable format. The latest end-to-end image-to-text approaches simultaneously predict the two structures by two decoders, where the prediction of the physical structure (the bounding boxes of the cells) is based on the representation of the logical structure. However, the previous methods struggle with imprecise bounding boxes as the logical representation lacks local visual information. To address this issue, we propose an end-to-end sequential modeling framework for table structure recognition called VAST. It contains a novel coordinate sequence decoder triggered by the representation of the non-empty cell from the logical structure decoder. In the coordinate sequence decoder, we model the bounding box coordinates as a language sequence, where the left, top, right and bottom coordinates are decoded sequentially to leverage the inter-coordinate dependency. Furthermore, we propose an auxiliary visual-alignment loss to enforce the logical representation of the non-empty cells to contain more local visual details, which helps produce better cell bounding boxes. Extensive experiments demonstrate that our proposed method can achieve state-of-the-art results in both logical and physical structure recognition. The ablation study also validates that the proposed coordinate sequence decoder and the visual-alignment loss are the keys to the success of our method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145769099",
                        "name": "Yongshuai Huang"
                    },
                    {
                        "authorId": "2054645836",
                        "name": "Ning Lu"
                    },
                    {
                        "authorId": "1679279",
                        "name": "Dapeng Chen"
                    },
                    {
                        "authorId": "2110470261",
                        "name": "Yibo Li"
                    },
                    {
                        "authorId": "2002678",
                        "name": "Zecheng Xie"
                    },
                    {
                        "authorId": "2901597",
                        "name": "Shenggao Zhu"
                    },
                    {
                        "authorId": "1777642",
                        "name": "Liangcai Gao"
                    },
                    {
                        "authorId": "2187312795",
                        "name": "Wei Peng"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "2b61d74b30df4034f6b3f8a2e9b8455a05a983cb",
                "externalIds": {
                    "DBLP": "journals/sncs/NamyslEBK23",
                    "DOI": "10.1007/s42979-022-01659-z",
                    "CorpusId": 257335296
                },
                "corpusId": 257335296,
                "publicationVenue": {
                    "id": "7a7dc89b-e1a6-44df-a496-46c330a87840",
                    "name": "SN Computer Science",
                    "type": "journal",
                    "alternate_names": [
                        "SN Comput Sci"
                    ],
                    "issn": "2661-8907",
                    "alternate_issns": [
                        "2662-995X"
                    ],
                    "url": "https://link.springer.com/journal/42979"
                },
                "url": "https://www.semanticscholar.org/paper/2b61d74b30df4034f6b3f8a2e9b8455a05a983cb",
                "title": "Flexible Hybrid Table Recognition and Semantic Interpretation System",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "134442417",
                        "name": "Marcin Namysl"
                    },
                    {
                        "authorId": "153486145",
                        "name": "Alexander M. Esser"
                    },
                    {
                        "authorId": "1699019",
                        "name": "Sven Behnke"
                    },
                    {
                        "authorId": "144306556",
                        "name": "J. K\u00f6hler"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "These developments have enabled significant advances in deep learning (DL) modeling for TE [18,16,25,21,13,12]."
            ],
            "citingPaper": {
                "paperId": "d943616a2b732c8b7cee18b44118db8d9a1af5e8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-00716",
                    "ArXiv": "2303.00716",
                    "DOI": "10.48550/arXiv.2303.00716",
                    "CorpusId": 257255197
                },
                "corpusId": 257255197,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/d943616a2b732c8b7cee18b44118db8d9a1af5e8",
                "title": "Aligning benchmark datasets for table structure recognition",
                "abstract": "Benchmark datasets for table structure recognition (TSR) must be carefully processed to ensure they are annotated consistently. However, even if a dataset's annotations are self-consistent, there may be significant inconsistency across datasets, which can harm the performance of models trained and evaluated on them. In this work, we show that aligning these benchmarks$\\unicode{x2014}$removing both errors and inconsistency between them$\\unicode{x2014}$improves model performance significantly. We demonstrate this through a data-centric approach where we adopt one model architecture, the Table Transformer (TATR), that we hold fixed throughout. Baseline exact match accuracy for TATR evaluated on the ICDAR-2013 benchmark is 65% when trained on PubTables-1M, 42% when trained on FinTabNet, and 69% combined. After reducing annotation mistakes and inter-dataset inconsistency, performance of TATR evaluated on ICDAR-2013 increases substantially to 75% when trained on PubTables-1M, 65% when trained on FinTabNet, and 81% combined. We show through ablations over the modification steps that canonicalization of the table annotations has a significantly positive effect on performance, while other choices balance necessary trade-offs that arise when deciding a benchmark dataset's final composition. Overall we believe our work has significant implications for benchmark design for TSR and potentially other tasks as well. Dataset processing and training code will be released at https://github.com/microsoft/table-transformer.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2745998",
                        "name": "B. Smock"
                    },
                    {
                        "authorId": "2130464272",
                        "name": "Rohith Pesala"
                    },
                    {
                        "authorId": "2128094499",
                        "name": "Robin Abraham"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In the field of Natural Language Processing (NLP), deep learning models have been applied to tasks such as text classification, sentiment analysis, machine translation, speech recognition[1], table detection and recognition[2, 3, 4]."
            ],
            "citingPaper": {
                "paperId": "4179dc3ac961592feb289fcea334dba31da63554",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-01786",
                    "ArXiv": "2302.01786",
                    "DOI": "10.48550/arXiv.2302.01786",
                    "CorpusId": 256598117
                },
                "corpusId": 256598117,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4179dc3ac961592feb289fcea334dba31da63554",
                "title": "Customer Profiling, Segmentation, and Sales Prediction using AI in Direct Marketing",
                "abstract": "In an increasingly customer-centric business environment, effective communication between marketing and senior management is crucial for success. With the rise of globalization and increased competition, utilizing new data mining techniques to identify potential customers is essential for direct marketing efforts. This paper proposes a data mining preprocessing method for developing a customer profiling system to improve sales performance, including customer equity estimation and customer action prediction. The RFM-analysis methodology is used to evaluate client capital and a boosting tree for prediction. The study highlights the importance of customer segmentation methods and algorithms to increase the accuracy of the prediction. The main result of this study is the creation of a customer profile and forecast for the sale of goods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2204464290",
                        "name": "Mahmoud SalahEldin Kasem"
                    },
                    {
                        "authorId": "2088695641",
                        "name": "Mohamed Hamada"
                    },
                    {
                        "authorId": "1405340824",
                        "name": "I. Taj-Eddin"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "ca2fabed8604b296d713262794a427e5c4b51ffa",
                "externalIds": {
                    "DBLP": "journals/apin/WanZLZS23",
                    "DOI": "10.1007/s10489-022-04420-4",
                    "CorpusId": 255362168
                },
                "corpusId": 255362168,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ca2fabed8604b296d713262794a427e5c4b51ffa",
                "title": "Contextual transformer sequence-based recognition network for medical examination reports",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152912519",
                        "name": "Honglin Wan"
                    },
                    {
                        "authorId": "2199057485",
                        "name": "Zongfeng Zhong"
                    },
                    {
                        "authorId": "2263987",
                        "name": "Tianping Li"
                    },
                    {
                        "authorId": "2856513",
                        "name": "Huaxiang Zhang"
                    },
                    {
                        "authorId": "51299154",
                        "name": "Jiande Sun"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "ae6e34d182fbc42bc99ba609369a3b7645d205cb",
                "externalIds": {
                    "DBLP": "journals/prl/WangXZJ23",
                    "DOI": "10.1016/j.patrec.2022.12.014",
                    "CorpusId": 255089449
                },
                "corpusId": 255089449,
                "publicationVenue": {
                    "id": "f35e3e87-9df4-497b-aa0d-bb8584197290",
                    "name": "Pattern Recognition Letters",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit Lett"
                    ],
                    "issn": "0167-8655",
                    "url": "https://www.journals.elsevier.com/pattern-recognition-letters/",
                    "alternate_urls": [
                        "http://www.journals.elsevier.com/pattern-recognition-letters/",
                        "http://www.sciencedirect.com/science/journal/01678655"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ae6e34d182fbc42bc99ba609369a3b7645d205cb",
                "title": "Scene table structure recognition with segmentation collaboration and alignment",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109799388",
                        "name": "Hongyi Wang"
                    },
                    {
                        "authorId": "46364544",
                        "name": "Yang Xue"
                    },
                    {
                        "authorId": "2118131879",
                        "name": "Jiaxin Zhang"
                    },
                    {
                        "authorId": "144838978",
                        "name": "Lianwen Jin"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "As such, table detection approaches such as CascadeTabNet [34], CDEC-Net [35] , TableNet [36] can play a significant role in detecting such tables."
            ],
            "citingPaper": {
                "paperId": "1279d1090eed92a8259086c3124853403e328554",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-14607",
                    "ArXiv": "2211.14607",
                    "DOI": "10.48550/arXiv.2211.14607",
                    "CorpusId": 254044385
                },
                "corpusId": 254044385,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1279d1090eed92a8259086c3124853403e328554",
                "title": "Sketch2FullStack: Generating Skeleton Code of Full Stack Website and Application from Sketch using Deep Learning and Computer Vision",
                "abstract": "\u2014For a full-stack web or app development, it requires a software \ufb01rm or more speci\ufb01cally a team of experienced developers to contribute a large portion of their time and resources to design the website and then convert it to code. As a result, the ef\ufb01ciency of the development team is signi\ufb01cantly reduced when it comes to converting UI wireframes and database schemas into an actual working system. It would save valuable resources and fasten the overall work\ufb02ow if the clients or developers can automate this process of converting the pre-made full-stack website design to get a partially working if not fully working code. In this paper, we present a novel approach of generating the skeleton code from sketched images using Deep Learning and Computer Vision approaches. The dataset for training are \ufb01rst-hand sketched images of low \ufb01delity wireframes, database schemas and class diagrams. The approach consists of three parts. First, the front-end or UI elements detection and extraction from custom-made UI wireframes. Second, individual database table creation from schema designs and lastly, creating a class \ufb01le from class diagrams.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2192606835",
                        "name": "Somoy Subandhu Barua"
                    },
                    {
                        "authorId": "2192607050",
                        "name": "Imam Mohammad Zulkarnain"
                    },
                    {
                        "authorId": "2153931746",
                        "name": "Abhishek Roy"
                    },
                    {
                        "authorId": "2401685",
                        "name": "Md. Golam Rabiul Alam"
                    },
                    {
                        "authorId": "3241032",
                        "name": "Md. Zia Uddin"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "\u2026bounding boxes of table candidates on document images (Shafait and Smith, 2010; Schreiber et al., 2017; Siddiqui et al., 2018; Paliwal et al., 2019; Prasad et al., 2020; Hashmi et al., 2021b; Zheng et al., 2021); TD searches for text lines signaling table candidates in plain-text (Hu et al., 1999;\u2026"
            ],
            "citingPaper": {
                "paperId": "cf5f42b60003f9fc11d14a6eb126130411f861a4",
                "externalIds": {
                    "DBLP": "journals/widm/Shigarov23",
                    "DOI": "10.1002/widm.1482",
                    "CorpusId": 253823145
                },
                "corpusId": 253823145,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cf5f42b60003f9fc11d14a6eb126130411f861a4",
                "title": "Table understanding: Problem overview",
                "abstract": "Tables are probably the most natural way to represent relational data in various media and formats. They store a large number of valuable facts that could be utilized for question answering, knowledge base population, natural language generation, and other applications. However, many tables are not accompanied by semantics for the automatic interpretation of the information they present. Table Understanding (TU) aims at recovering the missing semantics that enables the extraction of facts from tables. This problem covers a range of issues from table detection in document images to semantic table interpretation with the help of external knowledge bases. To date, the TU research has been ongoing on for 30\u2009years. Nevertheless, there is no common point of view on the scope of TU; the terminology still needs agreement and unification. In recent years, science and technology have shown a rapidly increasing interest in TU. Nowadays, it is especially important to check the meaning of this research problem once again. This article gives a comprehensive characterization of the TU problem, including a description of its subproblems, tasks, subtasks, and applications. It also discusses the common limitations used in the existing problem statements and proposes some directions for further research that would help overcome the corresponding limitations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3355714",
                        "name": "A. Shigarov"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "D Prasad[95] ICDAR2019 Cascade mask R-CNN HRNet Precision - - - - - - - - - - 2020 Recall - - - - - - - - - - F1-Score - - 94.",
                "D Prasad[95] ICDAR2013 Cascade mask R-CNN HRNet Precision 100 - - - - - - - - - 2020 Recall 100 - - - - - - - - - F1-Score 100 - - - - - - - - - -",
                "D Prasad [95] Cascade mask Region-based CNN High-Resolution Network-based model The study shows how iterative transfer learning may be used to transform pictures, which can lessen the need on huge datasets.",
                "com/Academic-Hammer/SciTSR D Prasad [95] CascadeTabNet 2020 Pytorch https://github.",
                "D Prasad [95] Cascade mask Regionbased CNN HighResolution Networkbased model Direct regression occurs at cellular boundaries using an end-to-end method.",
                "com/holms-ur/fine-tuning M Li [95] TableBank 2020 Pytorch, Detectron2 https://github.",
                "D Prasad [95] presents an automatic table detection approach for interpreting tabular data in document pictures, which primarily entails addressing two issues: table detection and table structure recognition.",
                "D Prasad[95] ICDAR2019 Object Detection Methods Precision - - - - - - - - - - 2020 Recall - - - - - - - - - - F1-Score - - 43."
            ],
            "citingPaper": {
                "paperId": "a81f504a1a101b9c6fe4a88957dd9ec450cf389d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-08469",
                    "ArXiv": "2211.08469",
                    "DOI": "10.48550/arXiv.2211.08469",
                    "CorpusId": 253553399
                },
                "corpusId": 253553399,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a81f504a1a101b9c6fe4a88957dd9ec450cf389d",
                "title": "Deep learning for table detection and structure recognition: A survey",
                "abstract": "Tables are everywhere, from scientific journals, papers, websites, and newspapers all the way to items we buy at the supermarket. Detecting them is thus of utmost importance to automatically understanding the content of a document. The performance of table detection has substantially increased thanks to the rapid development of deep learning networks. The goals of this survey are to provide a profound comprehension of the major developments in the field of Table Detection, offer insight into the different methodologies, and provide a systematic taxonomy of the different approaches. Furthermore, we provide an analysis of both classic and new applications in the field. Lastly, the datasets and source code of the existing models are organized to provide the reader with a compass on this vast literature. Finally, we go over the architecture of utilizing various object detection and table structure recognition methods to create an effective and efficient system, as well as a set of development trends to keep up with state-of-the-art algorithms and future research. We have also set up a public GitHub repository where we will be updating the most recent publications, open data, and source code. The GitHub repository is available at https://github.com/abdoelsayed2016/ table-detection-structure-recognition. Preprint submitted to Elsevier November 17, 2022 ar X iv :2 21 1. 08 46 9v 1 [ cs .C V ] 1 5 N ov 2 02 2",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "87034581",
                        "name": "M. Kasem"
                    },
                    {
                        "authorId": "115665360",
                        "name": "Abdelrahman Abdallah"
                    },
                    {
                        "authorId": "2115948777",
                        "name": "Alexander Berendeyev"
                    },
                    {
                        "authorId": "2190955162",
                        "name": "Ebrahem Elkady"
                    },
                    {
                        "authorId": "2190955581",
                        "name": "Mahmoud Abdalla"
                    },
                    {
                        "authorId": "2163883464",
                        "name": "Mohamed Mahmoud"
                    },
                    {
                        "authorId": "2057870328",
                        "name": "Mohamed Hamada"
                    },
                    {
                        "authorId": "3253700",
                        "name": "D. Nurseitov"
                    },
                    {
                        "authorId": "1405340824",
                        "name": "I. Taj-Eddin"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "CascadeTabNet [42] is an end-to-end table acquisition program utilizing Mask RCNN along with the augmentation",
                "[42] along with other postprocessing methods could be leveraged to achieve better results."
            ],
            "citingPaper": {
                "paperId": "e35ef6f4cfdc2319a641735608562d3886fa9fdd",
                "externalIds": {
                    "DOI": "10.1109/ICCCIS56430.2022.10037664",
                    "CorpusId": 256743427
                },
                "corpusId": 256743427,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e35ef6f4cfdc2319a641735608562d3886fa9fdd",
                "title": "Using CoordConv for Tabular Data Detection and Structure Recognition",
                "abstract": "This paper explores the usage of CoordConv, the novel upgrade to general convolutional layers in the problem of Tabular Data Detection and Cell-Based Structure Recognition. CoordConv has been shown to provide considerably better results in the domain of Object Detection than its counterpart. The authors integrate it within the established Anchor optimization approach which leverages guided anchors to accomplish the task of recognizing rows and columns present in tabular data. In contrast to the majority of techniques implemented for Table Structure Recognition, the authors attempt to recognize the cells present in the tabular images instead of the rows and columns. They evaluate this method on the coveted ICDAR-19 dataset (International Conference on Document Analysis and Recognition - 2019) which comprises of scanned document images containing tabular regions and achieve results surpassing those of many popular techniques. They also apply this approach for the task of Table Detection and achieve results comparable to other established techniques.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2205330992",
                        "name": "Apoorva Ambulgekar"
                    },
                    {
                        "authorId": "2205330976",
                        "name": "Naman Lad"
                    },
                    {
                        "authorId": "2183415732",
                        "name": "Krunal Doshi"
                    },
                    {
                        "authorId": "2128946657",
                        "name": "Pranit Bari"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "CascadeTabNet [8] is a variation of MaskRNN, which uses HRNetV2p [9] as the backbone and proposes"
            ],
            "citingPaper": {
                "paperId": "769935b79b9aa7c9e04406a1e2025005cd44b030",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-02128",
                    "ArXiv": "2211.02128",
                    "DOI": "10.1109/GLOBECOM48099.2022.10001041",
                    "CorpusId": 253370721
                },
                "corpusId": 253370721,
                "publicationVenue": {
                    "id": "b189dec0-41d0-4cea-a906-7c5186895904",
                    "name": "Global Communications Conference",
                    "type": "conference",
                    "alternate_names": [
                        "GLOBECOM",
                        "Glob Commun Conf"
                    ],
                    "url": "http://www.ieee-globecom.org/"
                },
                "url": "https://www.semanticscholar.org/paper/769935b79b9aa7c9e04406a1e2025005cd44b030",
                "title": "Efficient Information Sharing in ICT Supply Chain Social Network via Table Structure Recognition",
                "abstract": "The global Information and Communications Technology (ICT) supply chain is a complex network consisting of all types of participants. It is often formulated as a Social Network to discuss the supply chain network's relations, properties, and development in supply chain management. Information sharing plays a crucial role in improving the efficiency of the supply chain, and datasheets are the most common data format to describe e-component commodities in the ICT supply chain because of human readability. However, with the surging number of electronic documents, it has been far beyond the capacity of human readers, and it is also challenging to process tabular data automatically because of the complex table structures and heterogeneous layouts. Table Structure Recognition (TSR) aims to represent tables with complex structures in a machine-interpretable format so that the tabular data can be processed automatically. In this paper, we formulate TSR as an object detection problem and propose to generate an intuitive representation of a complex table structure to enable structuring of the tabular data related to the commodities. To cope with border-less and small layouts, we propose a cost-sensitive loss function by considering the detection difficulty of each class. Besides, we propose a novel anchor generation method using the character of tables that columns in a table should share an identical height, and rows in a table should share the same width. We implement our proposed method based on Faster-RCNN and achieve 94.79% on mean Average Precision (AP), and consistently improve more than 1.5% AP for different benchmark models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146517209",
                        "name": "Bin Xiao"
                    },
                    {
                        "authorId": "2130012695",
                        "name": "Yakup Akkaya"
                    },
                    {
                        "authorId": "48778270",
                        "name": "Murat Simsek"
                    },
                    {
                        "authorId": "2497479",
                        "name": "B. Kantarci"
                    },
                    {
                        "authorId": "3082067",
                        "name": "A. A. Alkheir"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "bace5ddca96de2dbc5be6197bea7aee2e91f783d",
                "externalIds": {
                    "ArXiv": "2211.01165",
                    "DBLP": "journals/corr/abs-2211-01165",
                    "DOI": "10.48550/arXiv.2211.01165",
                    "CorpusId": 253173975
                },
                "corpusId": 253173975,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bace5ddca96de2dbc5be6197bea7aee2e91f783d",
                "title": "RegCLR: A Self-Supervised Framework for Tabular Representation Learning in the Wild",
                "abstract": "Recent advances in self-supervised learning (SSL) using large models to learn visual representations from natural images are rapidly closing the gap between the results produced by fully supervised learning and those produced by SSL on downstream vision tasks. Inspired by this advancement and primarily motivated by the emergence of tabular and structured document image applications, we investigate which self-supervised pretraining objectives, architectures, and fine-tuning strategies are most effective. To address these questions, we introduce RegCLR, a new self-supervised framework that combines contrastive and regularized methods and is compatible with the standard Vision Transformer architecture. Then, RegCLR is instantiated by integrating masked autoencoders as a representative example of a contrastive method and enhanced Barlow Twins as a representative example of a regularized method with configurable input image augmentations in both branches. Several real-world table recognition scenarios (e.g., extracting tables from document images), ranging from standard Word and Latex documents to even more challenging electronic health records (EHR) computer screen images, have been shown to benefit greatly from the representations learned from this new framework, with detection average-precision (AP) improving relatively by 4.8% for Table, 11.8% for Column, and 11.1% for GUI objects over a previous fully supervised baseline on real-world EHR screen images.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "7634810",
                        "name": "Weiyao Wang"
                    },
                    {
                        "authorId": "50435752",
                        "name": "Byung-Hak Kim"
                    },
                    {
                        "authorId": "1981419",
                        "name": "Varun Ganapathi"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "[35] proposed CascadeTab-Net, which tries to solve the table detection and table structure recognition together."
            ],
            "citingPaper": {
                "paperId": "4960e24f6cfab1afd36b7e89c5d88d107ba55863",
                "externalIds": {
                    "DOI": "10.3390/app122010578",
                    "CorpusId": 253075939
                },
                "corpusId": 253075939,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4960e24f6cfab1afd36b7e89c5d88d107ba55863",
                "title": "Rethinking Learnable Proposals for Graphical Object Detection in Scanned Document Images",
                "abstract": "In the age of deep learning, researchers have looked at domain adaptation under the pre-training and fine-tuning paradigm to leverage the gains in the natural image domain. These backbones and subsequent networks are designed for object detection in the natural image domain. They do not consider some of the critical characteristics of document images. Document images are sparse in contextual information, and the graphical page objects are logically clustered. This paper investigates the effectiveness of deep and robust backbones in the document image domain. Further, it explores the idea of learnable object proposals through Sparse R-CNN. This paper shows that simple domain adaptation of top-performing object detectors to the document image domain does not lead to better results. Furthermore, empirically showing that detectors based on dense object priors like Faster R-CNN, Mask R-CNN, and Cascade Mask R-CNN are perhaps not best suited for graphical page object detection. Detectors that reduce the number of object candidates while making them learnable are a step towards a better approach. We formulate and evaluate the Sparse R-CNN (SR-CNN) model on the IIIT-AR-13k, PubLayNet, and DocBank datasets and hope to inspire a rethinking of object proposals in the domain of graphical page object detection.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1576277442",
                        "name": "Sankalp Sinha"
                    },
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "1771057",
                        "name": "A. Pagani"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[4] used data augmentation with dilation and smudge techniques.",
                "SCORE F-1 IMPLEMENTATION OF DATA AUGMENTATION [4]",
                "[4] Table recognition using Mask RCNN deep learning model with data augmentation Mask R-CNN model is created by performing data augmentation dilation and smudged as the Training."
            ],
            "citingPaper": {
                "paperId": "0e73eb7777fc650405103658a0cf966d25118f32",
                "externalIds": {
                    "DOI": "10.1109/TSSA56819.2022.10063909",
                    "CorpusId": 257586166
                },
                "corpusId": 257586166,
                "publicationVenue": {
                    "id": "1eeaf3cb-1484-458d-b20f-72c0d39e4394",
                    "name": "International Conference on Telecommunication Systems, Services, and Applications",
                    "type": "conference",
                    "alternate_names": [
                        "TSSA",
                        "Int Conf Telecommun Syst Serv Appl"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0e73eb7777fc650405103658a0cf966d25118f32",
                "title": "Table Information Extraction Using Data Augmentation on Deep Learning and Image Processing",
                "abstract": "Generally, the extraction of information in the table is done quickly if the table is within a document with a tabular structure. However, in the case of tables presented in the image document, steps are needed first to detect the table. Seeing a table in image documents becomes more complicated if the table to be seen does not have clear boundaries. This research focuses on extracting information from borderless tables in image documents. The study applies the Mask RCNN-FPN deep learning model to detect borderless tables using augmentation data. The use of data augmentation is expected to increase the accuracy of deep learning models even though there is only a small amount of training data available. The data augmentation technique proposed in this study is a fine-tuning method with CutMask augmentation data. For model formation and testing, this study uses the UNLV data set. This data set consists of scanned images of documents from various sources, including financial reports, journals, and different tabular research papers. The total amount of data used is 427 samples. After data augmentation, the amount of data used is 854 samples. The table detection model is based on the Mask RCNN created using Python. The testing parameters used for table detection quality are precise detection, partial detection, false detection, Precision, Recall, and F-Measure. The table's structure recognition rate is measured from the detection intersection value, rows, columns, and cells compared to ground truth. The test results show that using data augmentation with the CutMask technique can improve the performance of deep learning models to detect borderless tables. The use of image processing is shown to enhance table segmentation. However, the table structure recognition result does not offer a good result compared to the effects of other research.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2211944553",
                        "name": "Izuardo Zulkarnain"
                    },
                    {
                        "authorId": "147946906",
                        "name": "Rin Rin Nurmalasari"
                    },
                    {
                        "authorId": "3273275",
                        "name": "F. N. Azizah"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Studies on table detection [26, 32, 33, 41, 44] and table recognition [23, 41, 44\u201346, 66, 74] have attracted great attention and been successfully used to convert image-based tables into the digital structural formats."
            ],
            "citingPaper": {
                "paperId": "4ebf44ebb9634cb5c60f5dc7fab0c0f84aed0939",
                "externalIds": {
                    "DBLP": "conf/mm/LiL0LCNPL22",
                    "DOI": "10.1145/3503161.3547885",
                    "CorpusId": 252782335
                },
                "corpusId": 252782335,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4ebf44ebb9634cb5c60f5dc7fab0c0f84aed0939",
                "title": "End-to-End Compound Table Understanding with Multi-Modal Modeling",
                "abstract": "Table is a widely used data form in webpages, spreadsheets, or PDFs to organize and present structural data. Although studies on table structure recognition have been successfully used to convert image-based tables into digital structural formats, solving many real problems still relies on further understanding of the table, such as cell relationship extraction. The current datasets related to table understanding are all based on the digit format. To boost research development, we release a new benchmark named ComFinTab with rich annotations that support both table recognition and understanding tasks. Unlike previous datasets containing the basic tables, ComFinTab contains a large ratio of compound tables, which is much more challenging and requires methods using multiple information sources. Based on the dataset, we also propose a uniform, concise task form with the evaluation metric to better evaluate the model's performance on the table understanding task in compound tables. Finally, a framework named CTUNet is proposed to integrate the compromised visual, semantic, and position features with a graph attention network, which can solve the table recognition task and the challenging table understanding task as a whole. Experimental results compared with some previous advanced table understanding methods demonstrate the effectiveness of our proposed model. Code and dataset are available at \\urlhttps://github.com/hikopensource/DAVAR-Lab-OCR.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109614687",
                        "name": "Zaisheng Li"
                    },
                    {
                        "authorId": "2187429408",
                        "name": "Yi Li"
                    },
                    {
                        "authorId": "2187308758",
                        "name": "Qiao Liang"
                    },
                    {
                        "authorId": "2144902872",
                        "name": "Pengfei Li"
                    },
                    {
                        "authorId": "2398015",
                        "name": "Zhanzhan Cheng"
                    },
                    {
                        "authorId": "1490934795",
                        "name": "Yi Niu"
                    },
                    {
                        "authorId": "3290437",
                        "name": "Shiliang Pu"
                    },
                    {
                        "authorId": "2116227961",
                        "name": "Xi Li"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Empirically, we conduct our experiments with four state-ofthe-art detection methods: Faster Dynamic Faster R-CNN [23], Guided Anchoring Faster R-CNN [21], PointRend [10], and CascadeTabNet [18].",
                "4) CascadeTabNet: CascadeTabNet [18] is a new approach based on Cascade Mask R-CNN using CNN HRNet architecture as the backbone.",
                "[18] proposed CascadeTabNet as a new approach based on Cascade Mask R-CNN with HRNet backbone for table structure detection and recognition.",
                "Page object detection [20] [18] is a task that has received much attention in DIU, in which the detector takes an image document page as input then returns the objects\u2019 locations (if any) in the input image."
            ],
            "citingPaper": {
                "paperId": "aab8b147debecee15271a5ee2275db56d7698085",
                "externalIds": {
                    "DBLP": "conf/mapr/NguyenTVN22",
                    "DOI": "10.1109/MAPR56351.2022.9924997",
                    "CorpusId": 253123649
                },
                "corpusId": 253123649,
                "publicationVenue": {
                    "id": "1e3b2c00-e4cd-4b5a-b5f1-23b9e4d77569",
                    "name": "International Conference on Multimedia Analysis and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "MAPR",
                        "International Conference Multimedia Analysis and Pattern Recognition",
                        "Int Conf Multimedia Anal Pattern Recognit"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/aab8b147debecee15271a5ee2275db56d7698085",
                "title": "GaDocNet: Rethinking the Anchoring Scheme and Loss Function in Vietnamese Document Images",
                "abstract": "In recent years, page object detection has received much attention from document image understanding. However, its application has many limitations in Vietnamese document images. In this paper, we address the page object detection problem in the Vietnamese document image. Specially, we experiment with four state-of-the-art object detection methods: Dynamic Faster R-CNN, Guided Anchoring Faster R-CNN, PointRend, and CascadeTabNet on the Vietnamese image document dataset named UIT-DODV. UIT-DODV dataset is the first Vietnamese document image dataset with four objects: Table, Figure, Caption, and Formula. In addition, we further evaluate the bounding box regression loss functions of the IoU family. Then we propose the EIoU loss function for efficiently page object detection in Vietnamese document images. Based on the preliminary experimental results, we present GaDocNet along with the EIoU loss function. The proposal achieves 76.1%, which is 1.6% higher than the baseline on the UIT-DODV dataset. Moreover, we evaluate with Deformable DETR, PAA, Reppoints, Foveabox, FSAF, and ATSS on UIT-DODV. The empirical evaluation points out the advantages of our approach, which is the foundation for further works.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2137957503",
                        "name": "Thuan Trong Nguyen"
                    },
                    {
                        "authorId": "145450576",
                        "name": "D. Truong"
                    },
                    {
                        "authorId": "2198372",
                        "name": "Nguyen D. Vo"
                    },
                    {
                        "authorId": "1399684830",
                        "name": "Khang Nguyen"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "The idea of formulating a table as an object and the document image as a natural scene has produced state-of-the-art results on several publicly available table detection datasets [13,14].",
                "Cascade Mask R-CNN [42] has also been employed to enhance the performance of table detection with a conventional backbone network [13] and recursive feature pyramid networks [14]."
            ],
            "citingPaper": {
                "paperId": "c18b3479a389ae4bc955338a862e6bc4f68d6ac1",
                "externalIds": {
                    "DOI": "10.3390/app12188969",
                    "CorpusId": 252171274
                },
                "corpusId": 252171274,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c18b3479a389ae4bc955338a862e6bc4f68d6ac1",
                "title": "Continual Learning for Table Detection in Document Images",
                "abstract": "The growing amount of data demands methods that can gradually learn from new samples. However, it is not trivial to continually train a network. Retraining a network with new data usually results in a phenomenon called \u201ccatastrophic forgetting\u201d. In a nutshell, the performance of the model on the previous data drops by learning from the new instances. This paper explores this issue in the table detection problem. While there are multiple datasets and sophisticated methods for table detection, the utilization of continual learning techniques in this domain has not been studied. We employed an effective technique called experience replay and performed extensive experiments on several datasets to investigate the effects of catastrophic forgetting. The results show that our proposed approach mitigates the performance drop by 15 percent. To the best of our knowledge, this is the first time that continual learning techniques have been adopted for table detection, and we hope this stands as a baseline for future research.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1752929623",
                        "name": "Mohammad Minouei"
                    },
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "2757942",
                        "name": "M. Soheili"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "(Cascade Mask R-CNN) based model is used to detect the regions of tables, for identifying the structural body cells from the detected tables simultaneously in [11]."
            ],
            "citingPaper": {
                "paperId": "6727ee4a7b8440849fb810e9e11aaee3f85126a6",
                "externalIds": {
                    "DBLP": "conf/etfa/ChowdhuryAA22",
                    "DOI": "10.1109/ETFA52439.2022.9921455",
                    "CorpusId": 253123378
                },
                "corpusId": 253123378,
                "publicationVenue": {
                    "id": "22466e99-8720-4629-9169-4469e385e4d8",
                    "name": "IEEE International Conference on Emerging Technologies and Factory Automation",
                    "type": "conference",
                    "alternate_names": [
                        "Emerg Technol Fact Autom",
                        "IEEE Int Conf Emerg Technol Fact Autom",
                        "ETFA",
                        "Emerging Technologies and Factory Automation"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=937"
                },
                "url": "https://www.semanticscholar.org/paper/6727ee4a7b8440849fb810e9e11aaee3f85126a6",
                "title": "Towards Tabular Data Extraction From Richly-Structured Documents Using Supervised and Weakly-Supervised Learning",
                "abstract": "Tabular information extraction from richly structured documents is a challenging task, due to rich table and document structures. Supervised document table detection approaches include image classification and object localization methods, typically relying on manually annotated data which is often costly to acquire specially on domain specific dataset. Self-supervised learning is quickly closing the gap with supervised methods in computer vision research [1]. This paper investigates the impact of a self-supervised image classifier as the primary backbone in supervised object detection for document table detection. Furthermore, we study an approach for table structure recognition based on the pix2pix Generative Adversarial Networks (GAN) approach [2]. We propose these approaches as the basis of a machine learning pipeline for table detection and structure recognition. Our evaluation results on different publicly available datasets, as well as a domain specific dataset demonstrate the efficacy of the presented approaches towards tabular information extraction pipelines from richly structured documents.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1675904155",
                        "name": "A. Chowdhury"
                    },
                    {
                        "authorId": "2189543171",
                        "name": "Martin ben Ahmed"
                    },
                    {
                        "authorId": "2142735959",
                        "name": "Martin Atzmueller"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "More recent studies have thus focused mostly on developing deep CNN-based table detection and recognition approaches (see [2, 26, 9, 13, 27, 23] and references therein), leading to significant performance improvements.",
                "As addressed in Section 3.1, there are lots of state-of-theart methods for table detection, which include but not limited to YOLOv5 [14], DETR-R50 [3], deformable DETR [32], CascadeTabNet [26], DeCNT [28], DeepDeSRT [27], TableNet [23], CDeC-Net [2], and TableRadar [8].",
                "A more sophisticated cascade mask R-CNN model was developed in [26] along with a high resolution based backbone (HRNet); iterative transfer learning and customized image augmentation techniques were shown to further enhance the performance.",
                "1, there are lots of state-of-theart methods for table detection, which include but not limited to YOLOv5 [14], DETR-R50 [3], deformable DETR [32], CascadeTabNet [26], DeCNT [28], DeepDeSRT [27], TableNet [23], CDeC-Net [2], and TableRadar [8]."
            ],
            "citingPaper": {
                "paperId": "98d8ca558e6e7cf7657a7b9e30701b8b4aa500e6",
                "externalIds": {
                    "DBLP": "journals/kbs/KwonALS22",
                    "ArXiv": "2211.06648",
                    "DOI": "10.1016/j.knosys.2022.109946",
                    "CorpusId": 252651951
                },
                "corpusId": 252651951,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/98d8ca558e6e7cf7657a7b9e30701b8b4aa500e6",
                "title": "DATa: Domain Adaptation-Aided Deep Table Detection Using Visual-Lexical Representations",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2186749262",
                        "name": "Hyebin Kwon"
                    },
                    {
                        "authorId": "2186691027",
                        "name": "Joungbin An"
                    },
                    {
                        "authorId": "2232923040",
                        "name": "Dongwook Lee"
                    },
                    {
                        "authorId": "2186706815",
                        "name": "Won-Yong Shin"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "A Cascade Mask R-CNN (CascadeTabNet) is used in [32] to detect tables and body cells, arranging them in columns and rows based on their positions."
            ],
            "citingPaper": {
                "paperId": "f3b6939ce0244e89656cf6ff93f6dff8b2bef547",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-11203",
                    "ArXiv": "2208.11203",
                    "DOI": "10.1109/ICPR56361.2022.9956590",
                    "CorpusId": 251765514
                },
                "corpusId": 251765514,
                "publicationVenue": {
                    "id": "48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                    "name": "International Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Pattern Recognit (ICPR Proc Int Conf",
                        "Int Conf Pattern Recognit",
                        "ICPR",
                        "International conference on pattern recognition",
                        "Int conf pattern recognit",
                        "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                    ],
                    "issn": "1041-3278",
                    "alternate_issns": [
                        "1051-4651"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=4740202"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f3b6939ce0244e89656cf6ff93f6dff8b2bef547",
                "title": "Graph Neural Networks and Representation Embedding for Table Extraction in PDF Documents",
                "abstract": "Tables are widely used in several types of documents since they can bring important information in a structured way. In scientific papers, tables can sum up novel discoveries and summarize experimental results, making the research comparable and easily understandable by scholars. Several methods perform table analysis working on document images, losing useful information during the conversion from the PDF files since OCR tools can be prone to recognition errors, in particular for text inside tables. The main contribution of this work is to tackle the problem of table extraction, exploiting Graph Neural Networks. Node features are enriched with suitably designed representation embeddings. These representations help to better distinguish not only tables from the other parts of the paper, but also table cells from table headers. We experimentally evaluated the proposed approach on a new dataset obtained by merging the information provided in the PubLayNet and PubTables-1M datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2182515911",
                        "name": "Andrea Gemelli"
                    },
                    {
                        "authorId": "2182519264",
                        "name": "Emanuele Vivoli"
                    },
                    {
                        "authorId": "3285734",
                        "name": "S. Marinai"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "fb6fcca5762b26f225313f86b1f33a1cf198bfd7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-04921",
                    "ArXiv": "2208.04921",
                    "DOI": "10.1145/3503161.3548038",
                    "CorpusId": 251442367
                },
                "corpusId": 251442367,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fb6fcca5762b26f225313f86b1f33a1cf198bfd7",
                "title": "TSRFormer: Table Structure Recognition with Transformers",
                "abstract": "We present a new table structure recognition (TSR) approach, called TSRFormer, to robustly recognizing the structures of complex tables with geometrical distortions from various table images. Unlike previous methods, we formulate table separation line prediction as a line regression problem instead of an image segmentation problem and propose a new two-stage DETR based separator prediction approach, dubbed Sep arator RE gression TR ansformer (SepRETR), to predict separation lines from table images directly. To make the two-stage DETR framework work efficiently and effectively for the separation line prediction task, we propose two improvements: 1) A prior-enhanced matching strategy to solve the slow convergence issue of DETR; 2) A new cross attention module to sample features from a high-resolution convolutional feature map directly so that high localization accuracy is achieved with low computational cost. After separation line prediction, a simple relation network based cell merging module is used to recover spanning cells. With these new techniques, our TSRFormer achieves state-of-the-art performance on several benchmark datasets, including SciTSR, PubTabNet and WTW. Furthermore, we have validated the robustness of our approach to tables with complex structures, borderless cells, large blank spaces, empty or spanning cells as well as distorted or even curved shapes on a more challenging real-world in-house dataset.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108811137",
                        "name": "Weihong Lin"
                    },
                    {
                        "authorId": "2145130513",
                        "name": "Zhengmao Sun"
                    },
                    {
                        "authorId": "2894335",
                        "name": "Chixiang Ma"
                    },
                    {
                        "authorId": "2181111465",
                        "name": "Mingze Li"
                    },
                    {
                        "authorId": "2110132415",
                        "name": "Jiawei Wang"
                    },
                    {
                        "authorId": "2110833051",
                        "name": "Lei Sun"
                    },
                    {
                        "authorId": "2316043",
                        "name": "Qiang Huo"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "79af7ca206d1da599760bd99c9581a32cb4e8c7e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-06823",
                    "ArXiv": "2207.06823",
                    "DOI": "10.48550/arXiv.2207.06823",
                    "CorpusId": 250526703
                },
                "corpusId": 250526703,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/79af7ca206d1da599760bd99c9581a32cb4e8c7e",
                "title": "DEXTER: An end-to-end system to extract table contents from electronic medical health documents",
                "abstract": "In this paper, we propose DEXTER, an end to end system to extract information from tables present in medical health documents, such as electronic health records (EHR) and explanation of benefits (EOB). DEXTER consists of four sub-system stages: i) table detection ii) table type classification iii) cell detection; and iv) cell content extraction. We propose a two-stage transfer learning-based approach using CDeC-Net architecture along with Non-Maximal suppression for table detection. We design a conventional computer vision-based approach for table type classification and cell detection using parameterized kernels based on image size for detecting rows and columns. Finally, we extract the text from the detected cells using pre-existing OCR engine Tessaract. To evaluate our system, we manually annotated a sample of the real-world medical dataset (referred to as Meddata) consisting of wide variations of documents (in terms of appearance) covering different table structures, such as bordered, partially bordered, borderless, or coloured tables. We experimentally show that DEXTER outperforms the commercially available Amazon Textract and Microsoft Azure Form Recognizer systems on the annotated real-world medical dataset",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2176184231",
                        "name": "PR Nandhinee"
                    },
                    {
                        "authorId": "2176184223",
                        "name": "Harinath Krishnamoorthy"
                    },
                    {
                        "authorId": "88862568",
                        "name": "K. Srivatsan"
                    },
                    {
                        "authorId": "46479704",
                        "name": "Anil Goyal"
                    },
                    {
                        "authorId": "35628253",
                        "name": "Sudarsun Santhiappan"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "644b991959bf5fb036d28d67e9918d419b212e75",
                "externalIds": {
                    "DBLP": "journals/pr/LiYDL22",
                    "DOI": "10.1016/j.patcog.2022.108946",
                    "CorpusId": 251140530
                },
                "corpusId": 251140530,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/644b991959bf5fb036d28d67e9918d419b212e75",
                "title": "Table Structure Recognition and Form Parsing by End-to-End Object Detection and Relation Parsing",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118890077",
                        "name": "Xiaohui Li"
                    },
                    {
                        "authorId": "145820427",
                        "name": "Fei Yin"
                    },
                    {
                        "authorId": "2179477671",
                        "name": "He-Sen Dai"
                    },
                    {
                        "authorId": "1689269",
                        "name": "Cheng-Lin Liu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "1aa8da3bbd2bca7071fae029d6fa8ea851f805b7",
                "externalIds": {
                    "DBLP": "conf/clef/SkalickySUS22",
                    "ArXiv": "2206.11229",
                    "DOI": "10.48550/arXiv.2206.11229",
                    "CorpusId": 249926391
                },
                "corpusId": 249926391,
                "publicationVenue": {
                    "id": "ab453bce-d4ec-48ec-ad78-ef19dc9333ab",
                    "name": "Conference and Labs of the Evaluation Forum",
                    "type": "conference",
                    "alternate_names": [
                        "CLEF",
                        "Conf Lab Evaluation Forum",
                        "Cross-language Evaluation Forum",
                        "Cross-Language Evaluation Forum"
                    ],
                    "url": "http://www.clef-initiative.eu/"
                },
                "url": "https://www.semanticscholar.org/paper/1aa8da3bbd2bca7071fae029d6fa8ea851f805b7",
                "title": "Business Document Information Extraction: Towards Practical Benchmarks",
                "abstract": "Information extraction from semi-structured documents is crucial for frictionless business-to-business (B2B) communication. While machine learning problems related to Document Information Extraction (IE) have been studied for decades, many common problem definitions and benchmarks do not reflect domain-specific aspects and practical needs for automating B2B document communication. We review the landscape of Document IE problems, datasets and benchmarks. We highlight the practical aspects missing in the common definitions and define the Key Information Localization and Extraction (KILE) and Line Item Recognition (LIR) problems. There is a lack of relevant datasets and benchmarks for Document IE on semi-structured business documents as their content is typically legally protected or sensitive. We discuss potential sources of available documents including synthetic data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2071515342",
                        "name": "Maty\u00e1s Skalick\u00fd"
                    },
                    {
                        "authorId": "46369981",
                        "name": "Step\u00e1n Simsa"
                    },
                    {
                        "authorId": "3406363",
                        "name": "Michal U\u0159i\u010d\u00e1\u0159"
                    },
                    {
                        "authorId": "2052167",
                        "name": "Milan \u0160ulc"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "[44,45,47] prove that the improved networkmodule can strengthen the learning ability, but there are still certain limitations without considering the table itself."
            ],
            "citingPaper": {
                "paperId": "0b3b6e22fba1c9dc5e5b386128f1194618692158",
                "externalIds": {
                    "DBLP": "journals/ijdar/ZhangMGJZ23",
                    "DOI": "10.1007/s10032-022-00400-z",
                    "CorpusId": 248581054
                },
                "corpusId": 248581054,
                "publicationVenue": {
                    "id": "46722266-4327-4a22-871d-5a9fe3c5ad0c",
                    "name": "International Journal on Document Analysis and Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Doc Anal Recognit"
                    ],
                    "issn": "1433-2825",
                    "url": "https://link.springer.com/journal/10032"
                },
                "url": "https://www.semanticscholar.org/paper/0b3b6e22fba1c9dc5e5b386128f1194618692158",
                "title": "YOLO-table: disclosure document table detection with involution",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2164852187",
                        "name": "Daqian Zhang"
                    },
                    {
                        "authorId": "1380055201",
                        "name": "Ruibin Mao"
                    },
                    {
                        "authorId": "1387681701",
                        "name": "R. Guo"
                    },
                    {
                        "authorId": "2164709014",
                        "name": "Yang Jiang"
                    },
                    {
                        "authorId": "2164850015",
                        "name": "Jing Zhu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [
                ", 2015), table detection and recognition(Abdallah et al., 2022; Kasem et al., 2022; Prasad et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "58027e0084ed66f35794dbae09516e70036e616a",
                "externalIds": {
                    "ArXiv": "2204.14224",
                    "CorpusId": 248476037
                },
                "corpusId": 248476037,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/58027e0084ed66f35794dbae09516e70036e616a",
                "title": "Enhancing Core Image Classification Using Generative Adversarial Networks (GANs)",
                "abstract": "In the thrilling world of oil exploration, drill core samples are key to unlocking geological information critical to finding lucrative oil deposits. Despite the importance of these samples, traditional core logging techniques are known to be laborious and, worse still, subjective. Thankfully, the industry has embraced an innovative solution core imaging that allows for nondestructive and noninvasive rapid characterization of large quantities of drill cores. Our preeminent research paper aims to tackle the pressing problem of core detection and classification. Using state-of-the-art techniques, we present a groundbreaking solution that will transform the industry. Our first challenge is detecting the cores and segmenting the holes in images, which we will achieve using the Faster RCNN and Mask RCNN models, respectively. Then, we will address the problem of filling the hole in the core image, utilizing the powerful Generative Adversarial Networks (GANs) and employing Contextual Residual Aggregation (CRA) to create high-frequency residuals for missing contents in images. Finally, we will apply sophisticated texture recognition models for the classification of core images, revealing crucial information to oil companies in their quest to uncover valuable oil deposits. Our research paper presents an innovative and groundbreaking approach to tackling the complex issues surrounding core detection and classification. By harnessing cutting-edge techniques and technologies, we are poised to revolutionize the industry and make significant contributions to the field of oil exploration.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2037963368",
                        "name": "Galymzhan Abdimanap"
                    },
                    {
                        "authorId": "9297705",
                        "name": "K. Bostanbekov"
                    },
                    {
                        "authorId": "115665360",
                        "name": "Abdelrahman Abdallah"
                    },
                    {
                        "authorId": "104462976",
                        "name": "Anel Alimova"
                    },
                    {
                        "authorId": "2088248842",
                        "name": "Darkhan Kurmangaliyev"
                    },
                    {
                        "authorId": "3253700",
                        "name": "D. Nurseitov"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "6ede460c4e461396f113a44f98f8eda83a333378",
                "externalIds": {
                    "DOI": "10.1155/2022/6306025",
                    "CorpusId": 248259012
                },
                "corpusId": 248259012,
                "publicationVenue": {
                    "id": "6b6df2de-21bc-4137-9859-3fcef46f6a21",
                    "name": "Mobile Information Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Mob Inf Syst"
                    ],
                    "issn": "1574-017X",
                    "url": "https://www.hindawi.com/journals/misy/",
                    "alternate_urls": [
                        "https://www.iospress.nl/journal/mobile-information-systems/",
                        "https://www.iospress.nl/html/1574017x.php",
                        "http://content.iospress.com/journals/mobile-information-systems"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6ede460c4e461396f113a44f98f8eda83a333378",
                "title": "Analysis of English Writing Text Features Based on Random Forest and Logistic Regression Classification Algorithm",
                "abstract": "The characteristics of English writing text in natural scenes are characterized by low character detection rate, difficulty in small character detection, and various character detection categories. In order to improve the classification effect of English-written texts, solve the problem of feature loss and precision reduction of the prediction model based on dimensionality reduction neural network classifier in the analysis process. An improved stacking model combining random forest and logistic regression is proposed to analyze the characteristics of written English texts. The model uses multiple undersampling, trains multiple random forests as primary classifier, and uses logistic regression as secondary classifier. Experimental results show that this model can effectively improve the classification efficiency of unbalanced text classification. While ensuring the main features, the accuracy of prediction is substantially improved. It is proved that the model has high practicability in analyzing the features of English writing texts.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2162934653",
                        "name": "Chuan Sun"
                    },
                    {
                        "authorId": "2162873112",
                        "name": "Bo Luo"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "a968b9165a3b77bd746c00fb99031f378802900f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-00052",
                    "ArXiv": "2204.00052",
                    "DOI": "10.48550/arXiv.2204.00052",
                    "CorpusId": 247922289
                },
                "corpusId": 247922289,
                "publicationVenue": {
                    "id": "75d7a8c1-d871-42db-a8e4-7cf5146fdb62",
                    "name": "Social Science Research Network",
                    "type": "journal",
                    "alternate_names": [
                        "SSRN, Social Science Research Network (SSRN) home page",
                        "SSRN Electronic Journal",
                        "Soc Sci Res Netw",
                        "SSRN",
                        "SSRN Home Page",
                        "SSRN Electron J",
                        "Social Science Electronic Publishing presents Social Science Research Network"
                    ],
                    "issn": "1556-5068",
                    "url": "http://www.ssrn.com/",
                    "alternate_urls": [
                        "www.ssrn.com/",
                        "https://fatcat.wiki/container/tol7woxlqjeg5bmzadeg6qrg3e",
                        "https://www.wikidata.org/wiki/Q53949192",
                        "www.ssrn.com/en",
                        "http://www.ssrn.com/en/",
                        "http://umlib.nl/ssrn",
                        "umlib.nl/ssrn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a968b9165a3b77bd746c00fb99031f378802900f",
                "title": "Digitizing Historical Balance Sheet Data: A Practitioner's Guide",
                "abstract": "This paper discusses how to successfully digitize large-scale historical micro-data by augmenting optical character recognition (OCR) engines with pre- and post-processing methods. Although OCR software has improved dramatically in recent years due to improvements in machine learning, off-the-shelf OCR applications still present high error rates which limit their applications for accurate extraction of structured information. Complementing OCR with additional methods can however dramatically increase its success rate, making it a powerful and cost-efficient tool for economic historians. This paper showcases these methods and explains why they are useful. We apply them against two large balance sheet datasets and introduce quipucamayoc, a Python package containing these methods in a unified framework.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145932584",
                        "name": "Sergio Correia"
                    },
                    {
                        "authorId": "38861223",
                        "name": "Stephan Luck"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[10] proposed to use image transformation techniques, e."
            ],
            "citingPaper": {
                "paperId": "29942b5588d9978af5da046ebfc642e4779b8597",
                "externalIds": {
                    "ArXiv": "2203.09056",
                    "DBLP": "journals/pr/MaLSH23",
                    "DOI": "10.1016/j.patcog.2022.109006",
                    "CorpusId": 247518554
                },
                "corpusId": 247518554,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/29942b5588d9978af5da046ebfc642e4779b8597",
                "title": "Robust Table Detection and Structure Recognition from Heterogeneous Document Images",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2894335",
                        "name": "Chixiang Ma"
                    },
                    {
                        "authorId": "2108811137",
                        "name": "Weihong Lin"
                    },
                    {
                        "authorId": "2110833051",
                        "name": "Lei Sun"
                    },
                    {
                        "authorId": "2316043",
                        "name": "Qiang Huo"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "A benchmark evaluation of GloSAT (individual cell and coarse segmentation cell separately), cTDaR19, and their combination (+cTDaR19) using CascadeTabNet [134] and CascadeTabNet with additional postprocessing proposed by the authors was presented.",
                "A benchmark evaluation of GloSAT (individual cell and coarse segmentation cell separately), cTDaR19, and their combination (+cTDaR19) using CascadeTabNet [134] and CascadeTabNet with additional post-processing proposed by the authors was presented."
            ],
            "citingPaper": {
                "paperId": "d6aa48d4381361efd5d016d868052da045ee5599",
                "externalIds": {
                    "DBLP": "journals/ijdar/NikolaidouSML22",
                    "ArXiv": "2203.08504",
                    "DOI": "10.1007/s10032-022-00405-8",
                    "CorpusId": 247475991
                },
                "corpusId": 247475991,
                "publicationVenue": {
                    "id": "46722266-4327-4a22-871d-5a9fe3c5ad0c",
                    "name": "International Journal on Document Analysis and Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Doc Anal Recognit"
                    ],
                    "issn": "1433-2825",
                    "url": "https://link.springer.com/journal/10032"
                },
                "url": "https://www.semanticscholar.org/paper/d6aa48d4381361efd5d016d868052da045ee5599",
                "title": "A survey of historical document image datasets",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "13734856",
                        "name": "Konstantina Nikolaidou"
                    },
                    {
                        "authorId": "2700495",
                        "name": "Mathias Seuret"
                    },
                    {
                        "authorId": "2369656",
                        "name": "Hamam Mokayed"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "contexts": [
                "In this full pipeline, Step(1) almost does not introduce any errors to the final results because many table detection models, such as CascadTabNet [20] report an F1 score of 1.0 under the ICDAR2013 dataset.",
                "CascadeTabNet [20] is another deep learning network aiming at detecting tables and recognizing the structural body cells at the same time.",
                "In this full pipeline, Step(1) almost does not introduce any errors to the final results because many table detection models, such as CascadTabNet [20] report an F1 score of 1."
            ],
            "citingPaper": {
                "paperId": "007f2dc393e363abcfd023c1a1b47f5b84d4b2ae",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-03819",
                    "ArXiv": "2203.03819",
                    "DOI": "10.48550/arXiv.2203.03819",
                    "CorpusId": 247315039
                },
                "corpusId": 247315039,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/007f2dc393e363abcfd023c1a1b47f5b84d4b2ae",
                "title": "Table Structure Recognition with Conditional Attention",
                "abstract": "Tabular data in digital documents is widely used to express compact and important information for readers. However, it is challenging to parse tables from unstructured digital documents, such as PDFs and images, into machine-readable format because of the complexity of table structures and the missing of meta-information. Table Structure Recognition (TSR) problem aims to recognize the structure of a table and transform the unstructured tables into a structured and machine-readable format so that the tabular data can be further analysed by the down-stream tasks, such as semantic modeling and information retrieval. In this study, we hypothesize that a complicated table structure can be represented by a graph whose vertices and edges represent the cells and association between cells, respectively. Then we define the table structure recognition problem as a cell association classification problem and propose a conditional attention network (CATT-Net). The experimental results demonstrate the superiority of our proposed method over the state-of-the-art methods on various datasets. Besides, we investigate whether the alignment of a cell bounding box or a text-focused approach has more impact on the model performance. Due to the lack of public dataset annotations based on these two approaches, we further annotate the ICDAR2013 dataset providing both types of bounding boxes, which can be a new benchmark dataset for evaluating the methods in this field. Experimental results show that the alignment of a cell bounding box can help improve the Micro-averaged F1 score from 0.915 to 0.963, and the Macro-average F1 score from 0.787 to 0.923.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144025674",
                        "name": "Bin Xiao"
                    },
                    {
                        "authorId": "48778270",
                        "name": "Murat Simsek"
                    },
                    {
                        "authorId": "2497479",
                        "name": "B. Kantarci"
                    },
                    {
                        "authorId": "3082067",
                        "name": "A. A. Alkheir"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "The first problem is called table-location and has been previously addressed [30, 38, 19, 21, 23, 26, 8] with stateof-the-art object-detection networks (e."
            ],
            "citingPaper": {
                "paperId": "081312b993ef1df7e64e1a5f8715852a2e7ed527",
                "externalIds": {
                    "ArXiv": "2203.01017",
                    "DBLP": "journals/corr/abs-2203-01017",
                    "DOI": "10.1109/CVPR52688.2022.00457",
                    "CorpusId": 247218660
                },
                "corpusId": 247218660,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/081312b993ef1df7e64e1a5f8715852a2e7ed527",
                "title": "TableFormer: Table Structure Understanding with Transformers",
                "abstract": "Tables organize valuable content in a concise and compact representation. This content is extremely valuable for systems such as search engines, Knowledge Graph's, etc, since they enhance their predictive capabilities. Unfortu-nately, tables come in a large variety of shapes and sizes. Furthermore, they can have complex column/row-header configurations, multiline rows, different variety of separation lines, missing entries, etc. As such, the correct iden-tification of the table-structure from an image is a nontrivial task. In this paper, we present a new table-structure identification model. The latter improves the latest end-to-end deep learning model (i.e. encoder-dual-decoder from PubTabNet) in two significant ways. First, we introduce a new object detection decoder for table-cells. In this way, we can obtain the content of the table-cells from program-matic PDF's directly from the PDF source and avoid the training of the custom OCR decoders. This architectural change leads to more accurate table-content extraction and allows us to tackle non-english tables. Second, we replace the LSTM decoders with transformer based decoders. This upgrade improves significantly the previous state-of-the-art tree-editing-distance-score (TEDS) from 91% to 98.5% on simple tables and from 88.7% to 95% on complex tables.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "37525891",
                        "name": "A. Nassar"
                    },
                    {
                        "authorId": "2051177400",
                        "name": "Nikolaos Livathinos"
                    },
                    {
                        "authorId": "1578242957",
                        "name": "Maksym Lysak"
                    },
                    {
                        "authorId": "1831851",
                        "name": "P. Staar"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Document Intelligence can be considered as an umbrella term covering problems of Key Information Extraction [10,54], Table Detection [41,38] and Structure Recognition [39,55], Document Layout Segmentation [5,4] Document Layout Generation [6,36,3,48], Document Visual Question Answering [51,50,32], Document Image Enhancement [49,22,47] which involves the understanding of visually rich semantic information and structure of different layout entities of a whole page."
            ],
            "citingPaper": {
                "paperId": "6289874e3b499593dddd51ae229062fe07d30b56",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-12985",
                    "ArXiv": "2202.12985",
                    "DOI": "10.1007/978-3-031-25069-9_16",
                    "CorpusId": 247158436
                },
                "corpusId": 247158436,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6289874e3b499593dddd51ae229062fe07d30b56",
                "title": "OCR-IDL: OCR Annotations for Industry Document Library Dataset",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "35570245",
                        "name": "Ali Furkan Biten"
                    },
                    {
                        "authorId": "134682605",
                        "name": "Rub\u00e8n P\u00e9rez Tito"
                    },
                    {
                        "authorId": "51231577",
                        "name": "Llu\u00eds G\u00f3mez"
                    },
                    {
                        "authorId": "2864362",
                        "name": "Ernest Valveny"
                    },
                    {
                        "authorId": "1694974",
                        "name": "Dimosthenis Karatzas"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "3a562b74e3dbc9cefd2245a2015346558b63df81",
                "externalIds": {
                    "DBLP": "journals/sncs/AjijPRH22",
                    "DOI": "10.1007/s42979-022-01041-z",
                    "CorpusId": 246809635
                },
                "corpusId": 246809635,
                "publicationVenue": {
                    "id": "7a7dc89b-e1a6-44df-a496-46c330a87840",
                    "name": "SN Computer Science",
                    "type": "journal",
                    "alternate_names": [
                        "SN Comput Sci"
                    ],
                    "issn": "2661-8907",
                    "alternate_issns": [
                        "2662-995X"
                    ],
                    "url": "https://link.springer.com/journal/42979"
                },
                "url": "https://www.semanticscholar.org/paper/3a562b74e3dbc9cefd2245a2015346558b63df81",
                "title": "Robust Detection of Tables in Documents Using Scores from Table Cell Cores",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "7536937",
                        "name": "Md. Ajij"
                    },
                    {
                        "authorId": "3267968",
                        "name": "Sanjoy Pratihar"
                    },
                    {
                        "authorId": "36929516",
                        "name": "D. S. Roy"
                    },
                    {
                        "authorId": "1797464",
                        "name": "T. Hanne"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "4a76712d6bbb21a69f039f8e01cb52a9cddbb5f8",
                "externalIds": {
                    "ArXiv": "2202.01178",
                    "DBLP": "journals/corr/abs-2202-01178",
                    "CorpusId": 246473091
                },
                "corpusId": 246473091,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4a76712d6bbb21a69f039f8e01cb52a9cddbb5f8",
                "title": "Information Extraction through AI techniques: The KIDs use case at CONSOB",
                "abstract": "In this paper we report on the initial activities carried out within a collaboration between Consob and Sapienza University. We focus on Information Extraction from documents describing financial instruments. We discuss how we automate this task, via both rule-based and machine learning-based methods and provide our first results.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1739846",
                        "name": "D. Lembo"
                    },
                    {
                        "authorId": "2133418189",
                        "name": "Alessandra Limosani"
                    },
                    {
                        "authorId": "2065582153",
                        "name": "F. Medda"
                    },
                    {
                        "authorId": "2152022004",
                        "name": "Alessandra Monaco"
                    },
                    {
                        "authorId": "146748492",
                        "name": "Federico Maria Scafoglieri"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "For the purpose of validation, we have considered some important benchmark datasets (PublayNet, PRIMA, HJ and TableBank) with different document object categorization.",
                "Also, model\nTable 1: Experimental dataset description (instance level)\nPublayNet PRIMA Historical Japanese TableBank\nObject Train Eval Object Train Eval Object Train Eval Object Train Eval\nText 2,343,356 88,625 Text 6401 1531 Body 1443 308 Table 2835 1418 Title 627,125 18,801 Image 761 163 Row 7742 1538 - - - Lists 80,759 4239 Table 37 10 Title 33,637 7271 - - -\nperformance for evaluating each categorical document instance has also been computed as proposed in [2].",
                "With the advent of document understanding principles and the transformation of the problem motivation over the years, it lead to the release of large-scale annotated datasets like PubLayNet [31], HJ [32], TableBank [33] as well as small scale PRIMA [34] which we use for evaluating our proposed segmentation approach in this work (Please refer to the Table 1 for detailed description).",
                "Extensive experimentation on competitive benchmarks like PubLayNet, PRIMA, Historical Japanese (HJ) and TableBank demonstrate that our model achieved comparable or better segmentation performance than the existing state-of-the-art approaches with the average precision of 89.4, 40.3, 83.4 and 93.3.",
                "Lastly, a simple downstream task of table detection has been demonstrated from a TableBank example shown in Fig."
            ],
            "citingPaper": {
                "paperId": "9634b09af379de5839657b9f0ddea73806db138b",
                "externalIds": {
                    "ArXiv": "2201.11438",
                    "DBLP": "journals/corr/abs-2201-11438",
                    "CorpusId": 246294806
                },
                "corpusId": 246294806,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9634b09af379de5839657b9f0ddea73806db138b",
                "title": "DocSegTr: An Instance-Level End-to-End Document Image Segmentation Transformer",
                "abstract": "Understanding documents with rich layouts is an essential step towards information extraction. Business intelligence processes often require the extraction of useful semantic content from documents at a large scale for subsequent decision-making tasks. In this context, instance-level segmentation of different document objects (title, sections, figures etc.) has emerged as an interesting problem for the document analysis and understanding community. To advance the research in this direction, we present a transformer-based model called \\emph{DocSegTr} for end-to-end instance segmentation of complex layouts in document images. The method adapts a twin attention module, for semantic reasoning, which helps to become highly computationally efficient compared with the state-of-the-art. To the best of our knowledge, this is the first work on transformer-based document segmentation. Extensive experimentation on competitive benchmarks like PubLayNet, PRIMA, Historical Japanese (HJ) and TableBank demonstrate that our model achieved comparable or better segmentation performance than the existing state-of-the-art approaches with the average precision of 89.4, 40.3, 83.4 and 93.3. This simple and flexible framework could serve as a promising baseline for instance-level recognition tasks in document images.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2150473007",
                        "name": "Sanket Biswas"
                    },
                    {
                        "authorId": "2153585985",
                        "name": "Ayan Banerjee"
                    },
                    {
                        "authorId": "2117637297",
                        "name": "Josep Llad'os"
                    },
                    {
                        "authorId": "144167309",
                        "name": "U. Pal"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "contexts": [
                "In Appendix A, we illustrate the architecture of Mask R-CNN used in this paper.",
                "In Mask R-CNN, the maximum number of entity predictions per image is set to 100.",
                "The model structure (Mask R-CNN) can also be found here.",
                "Namely, we utilize Detectron2 to apply an updated version of Mask R-CNN [25].",
                "Keywords table structure parsing, table annotation, Mask R-CNN, weak supervision, domain adaptation",
                "2Some recent works on Cascade R-CNN [5, 6] manage to push",
                "For technical details of Mask R-CNN, we refer to DocParser [1].",
                "TableParser works in conjunction with TableAnnotator (Figure 1) which efficiently assists developers in visualizing the output, as well as help users to generate high-quality human annotations.3 To gener-\n2Some recent works on Cascade R-CNN [5, 6] manage to push the frontier of table detection.",
                "In Figure 6, we illustrate the Mask R-CNN model used.",
                "The success of DL has marked the revisiting of table structure parsing by [7], which inspired follow-up research [27, 1, 6, 2, 28, 29, 30, 19, 31, 18, 32, 17]."
            ],
            "citingPaper": {
                "paperId": "10c3efc40e72674b615864c94a231e1f11913619",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-01654",
                    "ArXiv": "2201.01654",
                    "CorpusId": 245704311
                },
                "corpusId": 245704311,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/10c3efc40e72674b615864c94a231e1f11913619",
                "title": "TableParser: Automatic Table Parsing with Weak Supervision from Spreadsheets",
                "abstract": "Tables have been an ever-existing structure to store data. There exist now different approaches to store tabular data physically. PDFs, images, spreadsheets, and CSVs are leading examples. Being able to parse table structures and extract content bounded by these structures is of high importance in many applications. In this paper, we devise TableParser, a system capable of parsing tables in both native PDFs and scanned images with high precision. We have conducted extensive experiments to show the efficacy of domain adaptation in developing such a tool. Moreover, we create TableAnnotator and ExcelAnnotator, which constitute a spreadsheet-based weak supervision mechanism and a pipeline to enable table parsing. We share these resources with the research community to facilitate further research in this interesting direction.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1576005199",
                        "name": "Susie Xi Rao"
                    },
                    {
                        "authorId": "5185738",
                        "name": "Johannes Rausch"
                    },
                    {
                        "authorId": "2074406990",
                        "name": "P. Egger"
                    },
                    {
                        "authorId": "1776014",
                        "name": "Ce Zhang"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "[23] detected cell locations to identify the structure of the table from document images.",
                "a An example of text-region representation as in [23] fails to detect three cells (T1, 0, 80.",
                "Other researchers have used transfer learning to alleviate the lack of training data  [23, 26].",
                "Track B consists of modern (track B2) and archival documents (track B1 and B2), where we use only modern documents as in [23, 36].",
                "The authors of [23] considered a table cell (more precisely, the text contents in a cell) as a type of object and developed rules to determine the pairwise relationships of the detected cells.",
                "This also further facilitates the training process because we need only hundreds of training samples and do not need additional training strategies as in [23].",
                "CascadeTabNet  [23] is another method that can produce the table structure with cell-level information, but it also suffers from the same limitations.",
                "of parameters CascadeTabNet [23] 82,852,033 TabStructNet [25] 68,636,098 SPLERGE [32] 255,862 Ours 1,120,692",
                "We compare the performance of our method with three coventional methods: CascadeTabNet [23], TabStructNet [25], and SPLERGE [32].",
                "Results of CascadeTabNet [23], TabStructNet [25], SPLERGE [32], and the proposed method, from top to bottom 5844 Multimedia Tools and Applications (2022) 81:5827\u20135848"
            ],
            "citingPaper": {
                "paperId": "d9c676eccdff80a7d1154660a09c4d2ea5164610",
                "externalIds": {
                    "DOI": "10.1007/s11042-021-11819-7",
                    "CorpusId": 254860167
                },
                "corpusId": 254860167,
                "publicationVenue": {
                    "id": "477368e9-7a8e-475a-8c93-6d623797fd06",
                    "name": "Multimedia tools and applications",
                    "type": "journal",
                    "alternate_names": [
                        "Multimedia Tools and Applications",
                        "Multimedia Tool Appl",
                        "Multimedia tool appl"
                    ],
                    "issn": "1380-7501",
                    "url": "https://www.springer.com/computer/information+systems+and+applications/journal/11042",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11042"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d9c676eccdff80a7d1154660a09c4d2ea5164610",
                "title": "Deep-learning and graph-based approach to table structure recognition",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2115242015",
                        "name": "Eunji Lee"
                    },
                    {
                        "authorId": "2370571",
                        "name": "Jaewoo Park"
                    },
                    {
                        "authorId": "2463454",
                        "name": "H. Koo"
                    },
                    {
                        "authorId": "1707645",
                        "name": "N. Cho"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "fffb4d2f028cef2628f342e8125aba9ce70a3a25",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-14070",
                    "ArXiv": "2112.14070",
                    "CorpusId": 245537893
                },
                "corpusId": 245537893,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fffb4d2f028cef2628f342e8125aba9ce70a3a25",
                "title": "Intelligent Document Processing - Methods and Tools in the real world",
                "abstract": "The originality of this publication is to look at the subject of IDP (Intelligent Document Processing) from the perspective of an end-user and industrialist and not that of a Computer Science researcher. This domain is one part of the challenge of information digitalisation that constitutes the Industrial Revolution of the twenty first century (Industry 4.0) and this paper looks specifically at the difficult areas of classifying, extracting information and subsequent integration into business processes with respect to forms and invoices. Since the focus is on practical implementation a brief review is carried out of the market in commercial tools for OCR, document classification and data extraction in so far as this is publicly available together with pricing (if known). Brief definitions of the main terms encountered in Computer Science publications and commercial prospectuses are provided in order to de-mystify the language for the layman. A small number of practical tests are carried out on a few real documents in order to illustrate the capabilities of tools that are commonly available at a reasonable price. The unsolved (so far) issue of tables contained in invoices is raised. The case of a typical large industrial company is evoked where the requirement is to extract 100 per cent of the information with 100 per cent reliability in order to integrate into the back-end Enterprise Resource Planning system. Finally a brief description is given of the state-of-the-art research by the huge corporations who are pushing the boundaries of deep learning techniques further and further with massive computing and financial power - progress that will undoubtedly trickle down into the real world at some later date. The paper finishes by asking the question whether the objectives and timing of the commercial world and the progress of Computer Science are fully aligned.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2148498372",
                        "name": "Graham A. Cutting"
                    },
                    {
                        "authorId": "1399442452",
                        "name": "A. Cutting-Decelle"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "[15], this paper proposed data conversion for document images, including the text, tables, and blank areas in the document; therefore, the model can better understand the data by thickening the text areas and reducing the blank areas.",
                "[15] proposed that CascadeTabNet use Cascade-RCNN combined with transfer learning, and data enhancement to improve the process and detect tables and their structures.",
                "As the demand for digital documents to replace paper documents is growing rapidly [15], the development of automatic document analysis focuses on Natural Language Processing (NLP) [16]."
            ],
            "citingPaper": {
                "paperId": "86c3f187c3553e5a8eb5980b689db9636ffe053c",
                "externalIds": {
                    "DOI": "10.3390/app112311446",
                    "CorpusId": 244891812
                },
                "corpusId": 244891812,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/86c3f187c3553e5a8eb5980b689db9636ffe053c",
                "title": "U-SSD: Improved SSD Based on U-Net Architecture for End-to-End Table Detection in Document Images",
                "abstract": "Tables are an important element in a document and can express more information with fewer words. Due to the different arrangements of tables and texts, as well as the variety of layouts, table detection is a challenge in the field of document analysis. Nowadays, as Optical Character Recognition technology has gradually matured, it can help us to obtain text information quickly, and the ability to accurately detect table structures can improve the efficiency of obtaining text content. The process of document digitization is influenced by the editor\u2019s style on the table layout. In addition, many industries rely on a large number of people to process data, which has high expense, thus, the industry imports artificial intelligence and Robotic Process Automation to handle simple and complicated routine text digitization work. Therefore, this paper proposes an end-to-end table detection model, U-SSD, as based on the object detection method of deep learning, takes the Single Shot MultiBox Detector (SSD) as the basic model architecture, improves it by U-Net, and adds dilated convolution to enhance the feature learning capability of the network. The experiment in this study uses the dataset of accident claim documents, as provided by a Taiwanese Law Firm, and conducts table detection. The experimental results show that the proposed method is effective. In addition, the results of the evaluation on open dataset of TableBank, Github, and ICDAR13 show that the SSD-based network architectures can achieve good performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2116353555",
                        "name": "Shih-Hsiung Lee"
                    },
                    {
                        "authorId": "2108261827",
                        "name": "Hung-Chun Chen"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "F1 of 93.34%, compared to those of 90.1%by theCascadeTabNet and 91.8%byusing the MMDetection framework with three staged Cascade mask R-CNN [18]3.",
                "For improving the accuracy, CascadeTableNet combines two approaches: Cascade RCNN [20] to solve the paradox of high-quality detection and amodified HRNet to attain reliable high-resolution representations and multi-level representations for semantic segmentation as well as for object detection [18].",
                "We also compare performance of several best methods on this dataset in Table 5, including rule-based method using text extracted from PDF files [43] and page images [2], deep CNN-basedmethods likeDeepDeSRT [16], TableNet [23], CascadeTableNet [18], and the result of two commercial systems ABBYY FineReader 11.0 and OmniPage 18 Professional.",
                "The DeepDeSRT [16], DeCNT [17], and CascadeTableNet [18] use Faster R-CNN as the basic framework for table detection.",
                "smudge images with different distance transforms [18,24].",
                "8%byusing the MMDetection framework with three staged Cascade mask R-CNN [18]3.",
                "The R-CNN (Region-based CNN) [8] and Fast R-CNN [3] apply the selective search for finding 2000 potential regions while the Faster R-CNN [9] uses a small network called Region Proposal Network (RPN) to detect those proposals.",
                "The collected data was cleaned by removing ground-truth annotation errors as done by [18].",
                "We also compare performance of several best methods on this dataset in Table 5, including rule-based method using text extracted from PDF files [43] and page images [2], deep CNN-basedmethods likeDeepDeSRT [16], TableNet [23], CascadeTableNet [18], and the result of two commercial systems ABBYY FineReader 11.",
                "The HRNet backbone (hrnetv2p-w32) used by CascadeTabNet and Cascade mask R-CNN has about 45 million parameters [13], much larger than the 8.1 million parameters of TableSegNet.",
                "Cascade mask R-CNN [18] ICDAR19 Extended 0.",
                "The CascadeTableNet used 40 randomly chosen images for finetuning and the rest for testing [18].",
                "This property is vital, because TableSegNet could be adapted for different tasks and trained with any input images, with three-channel RGB or one-channel grayscale image, or a combination of transformed images like in [18]."
            ],
            "citingPaper": {
                "paperId": "9eb3421792bf9e411b12d7f82c3256cafa7cf2a0",
                "externalIds": {
                    "DOI": "10.1007/s10032-021-00390-4",
                    "CorpusId": 254112157
                },
                "corpusId": 254112157,
                "publicationVenue": {
                    "id": "46722266-4327-4a22-871d-5a9fe3c5ad0c",
                    "name": "International Journal on Document Analysis and Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Doc Anal Recognit"
                    ],
                    "issn": "1433-2825",
                    "url": "https://link.springer.com/journal/10032"
                },
                "url": "https://www.semanticscholar.org/paper/9eb3421792bf9e411b12d7f82c3256cafa7cf2a0",
                "title": "TableSegNet: a fully convolutional network for table detection and segmentation in document images",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2149981",
                        "name": "D. Nguyen"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "34c61db889ce5fd1002b9c1cd2331bfa1072cef7",
                "externalIds": {
                    "ArXiv": "2111.08609",
                    "DBLP": "journals/corr/abs-2111-08609",
                    "CorpusId": 244130192
                },
                "corpusId": 244130192,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/34c61db889ce5fd1002b9c1cd2331bfa1072cef7",
                "title": "Document AI: Benchmarks, Models and Applications",
                "abstract": "Document AI, or Document Intelligence, is a relatively new research topic that refers to the techniques for automatically reading, understanding, and analyzing business documents. It is an important research direction for natural language processing and computer vision. In recent years, the popularity of deep learning technology has greatly advanced the development of Document AI, such as document layout analysis, visual information extraction, document visual question answering, document image classification, etc. This paper briefly reviews some of the representative models, tasks, and benchmark datasets. Furthermore, we also introduce early-stage heuristic rule-based document analysis, statistical machine learning algorithms, and deep learning approaches especially pre-training methods. Finally, we look into future directions for Document AI research.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2114843952",
                        "name": "Lei Cui"
                    },
                    {
                        "authorId": "3032611",
                        "name": "Yiheng Xu"
                    },
                    {
                        "authorId": "1379581011",
                        "name": "Tengchao Lv"
                    },
                    {
                        "authorId": "49807919",
                        "name": "Furu Wei"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Some recent data-driven methods include works by [1, 32, 21, 33]."
            ],
            "citingPaper": {
                "paperId": "d499cd8a77e7fc4e12d1f5bd7875ce2e3f5465a4",
                "externalIds": {
                    "DBLP": "conf/wacv/RajaMV22",
                    "ArXiv": "2111.07129",
                    "DOI": "10.1109/WACV51458.2022.00260",
                    "CorpusId": 240285297
                },
                "corpusId": 240285297,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/d499cd8a77e7fc4e12d1f5bd7875ce2e3f5465a4",
                "title": "Visual Understanding of Complex Table Structures from Document Images",
                "abstract": "Table structure recognition is necessary for a comprehensive understanding of documents. Tables in unstructured business documents are tough to parse due to the high diversity of layouts, varying alignments of contents, and the presence of empty cells. The problem is particularly difficult because of challenges in identifying individual cells using visual or linguistic contexts or both. Accurate detection of table cells (including empty cells) simplifies structure extraction and hence, it becomes the prime focus of our work. We propose a novel object-detection-based deep model that captures the inherent alignments of cells within tables and is fine-tuned for fast optimization. Despite accurate detection of cells, recognizing structures for dense tables may still be challenging because of difficulties in capturing long-range row/column dependencies in presence of multi-row/column spanning cells. Therefore, we also aim to improve structure recognition by deducing a novel rectilinear graph-based formulation. From a semantics perspective, we highlight the significance of empty cells in a table. To take these cells into account, we suggest an enhancement to a popular evaluation criterion. Finally, we introduce a modestly sized evaluation dataset with an annotation style inspired by human cognition to encourage new approaches to the problem. Our framework improves the previous state-of-the-art performance by a 2.7% average F1-score on benchmark datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2053910624",
                        "name": "S. Raja"
                    },
                    {
                        "authorId": "2896521",
                        "name": "Ajoy Mondal"
                    },
                    {
                        "authorId": "2226828175",
                        "name": "Jawahar C V"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [
                "Following this route, [19] focuses on identifying figures and formulae while [20] performs table structure recognition."
            ],
            "citingPaper": {
                "paperId": "5261737e877eb9da6277997893fee04b839721ad",
                "externalIds": {
                    "DBLP": "journals/pr/RamanSV22",
                    "ArXiv": "2111.06016",
                    "DOI": "10.1016/j.patcog.2022.108660",
                    "CorpusId": 243986021
                },
                "corpusId": 243986021,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5261737e877eb9da6277997893fee04b839721ad",
                "title": "Synthetic Document Generator for Annotation-free Layout Recognition",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "32319713",
                        "name": "Natraj Raman"
                    },
                    {
                        "authorId": "36532736",
                        "name": "Sameena Shah"
                    },
                    {
                        "authorId": "2058284590",
                        "name": "M. Veloso"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "p, en, t , g share the denomination from II-A1 b is a boolean assessing if the i-th token is bold f i is the font type the i-th token f i size is the font size of the i-th token ttab is a boolean assessing if the i-th token is in a table or not, it can be obtained by parsing the PDF graphics on simple cases or using more advanced computer vision based solution for the extraction [27][28] c is the color of the i-th token"
            ],
            "citingPaper": {
                "paperId": "e8b1e628d1daeb4aeb5d40f3b2d37c2fd6dda415",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-04045",
                    "ArXiv": "2111.04045",
                    "DOI": "10.1109/ICPR56361.2022.9956120",
                    "CorpusId": 243847897
                },
                "corpusId": 243847897,
                "publicationVenue": {
                    "id": "48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                    "name": "International Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Pattern Recognit (ICPR Proc Int Conf",
                        "Int Conf Pattern Recognit",
                        "ICPR",
                        "International conference on pattern recognition",
                        "Int conf pattern recognit",
                        "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                    ],
                    "issn": "1041-3278",
                    "alternate_issns": [
                        "1051-4651"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=4740202"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e8b1e628d1daeb4aeb5d40f3b2d37c2fd6dda415",
                "title": "Information Extraction from Visually Rich Documents with Font Style Embeddings",
                "abstract": "Information extraction (IE) from documents is an intensive area of research with a large set of industrial applications. Current state-of-the-art methods focus on scanned documents with approaches combining computer vision, natural language processing and layout representation. We propose to challenge the usage of computer vision in the case where both token style and visual representation are available (i.e native PDF documents). Our experiments on three real-world complex datasets demonstrate that using token style attributes based embedding instead of a raw visual embedding in LayoutLM model is beneficial. Depending on the dataset, such an embedding yields an improvement of 0.18% to 2.29% in weighted F1-score with a decrease of 30.7% in the final number of trainable parameters of the model, leading to an improvement in both efficiency and effectiveness.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2139730733",
                        "name": "Ismail Oussaid"
                    },
                    {
                        "authorId": "2139705350",
                        "name": "William Vanhuffel"
                    },
                    {
                        "authorId": "1637246846",
                        "name": "Pirashanth Ratnamogan"
                    },
                    {
                        "authorId": "1641613036",
                        "name": "Mhamed Hajaiej"
                    },
                    {
                        "authorId": "2139709308",
                        "name": "Alexis Mathey"
                    },
                    {
                        "authorId": null,
                        "name": "Thomas Gilles"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "To answer questions over unstructured table images, both computer vision (CV) and natural language processing (NLP) methods are required [4, 10, 16, 17]."
            ],
            "citingPaper": {
                "paperId": "0ab6cfb57d3de5159c8d6b36f38d1efc199de11b",
                "externalIds": {
                    "DBLP": "conf/mm/XueCWLYZT21",
                    "DOI": "10.1145/3474085.3478558",
                    "CorpusId": 239011591
                },
                "corpusId": 239011591,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0ab6cfb57d3de5159c8d6b36f38d1efc199de11b",
                "title": "A Question Answering System for Unstructured Table Images",
                "abstract": "Question answering over tables is a very popular semantic parsing task in natural language processing (NLP). However, few existing methods focus on table images, even though there are usually large-scale unstructured tables in practice (e.g., table images). Table parsing from images is nontrivial since it is closely related to not only NLP but also computer vision (CV) to parse the tabular structure from an image. In this demo, we present a question answering system for unstructured table images. The proposed system mainly consists of 1) a table recognizer to recognize the tabular structure from an image and 2) a table parser to generate the answer to a natural language question over the table. In addition, to train the model, we further provide table images and structure annotations for two widely used semantic parsing datasets. Specifically, the test set is used for this demo, from where the users can either choose from default questions or enter a new custom question.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51469630",
                        "name": "Wenyuan Xue"
                    },
                    {
                        "authorId": "2112619444",
                        "name": "Siqi Cai"
                    },
                    {
                        "authorId": "2119293226",
                        "name": "Wen Wang"
                    },
                    {
                        "authorId": "1612980447",
                        "name": "Qingyong Li"
                    },
                    {
                        "authorId": "2110431812",
                        "name": "Baosheng Yu"
                    },
                    {
                        "authorId": "1895813",
                        "name": "Yibing Zhan"
                    },
                    {
                        "authorId": "143719920",
                        "name": "D. Tao"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "A group of methods [27, 29, 41, 43] try to recover the relations of elements based on heuristic algorithms."
            ],
            "citingPaper": {
                "paperId": "7b99f9995a4b086c0e6913a8d2f109a5dbaa5014",
                "externalIds": {
                    "DBLP": "conf/mm/LiuLLJLRJ21",
                    "DOI": "10.1145/3474085.3481534",
                    "CorpusId": 239011898
                },
                "corpusId": 239011898,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7b99f9995a4b086c0e6913a8d2f109a5dbaa5014",
                "title": "Show, Read and Reason: Table Structure Recognition with Flexible Context Aggregator",
                "abstract": "We investigate the challenging problem of table structure recognition in this work. Many recent methods adopt graph-based context aggregator with strong inductive bias to reason sparse contextual relationships of table elements. However, the strong constraints may be too restrictive to represent the complicated table relationships. In order to learn more appropriate inductive bias from data, we try to introduce Transformer as context aggregator in this work. Nevertheless, Transformer taking dense context as input requires larger scale data and may suffer from unstable training procedure due to the weakening of inductive bias. To overcome the above limitations, we in this paper design a FLAG (FLexible context AGgregator), which marries Transformer with graph-based context aggregator in an adaptive way. Based on FLAG, an end-to-end framework requiring no extra meta-data or OCR information, termed FLAG-Net, is proposed to flexibly modulate the aggregation of dense context and sparse one for the relational reasoning of table elements. We investigate the modulation pattern in FLAG and show what contextual information is focused, which is vital for recognizing table structure. Extensive experimental results on benchmarks demonstrate the performance of our proposed FLAG-Net surpasses other compared methods by a large margin.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48446712",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "2153898418",
                        "name": "Xin Li"
                    },
                    {
                        "authorId": "2149124539",
                        "name": "Bing Liu"
                    },
                    {
                        "authorId": "3336726",
                        "name": "Deqiang Jiang"
                    },
                    {
                        "authorId": "2136370566",
                        "name": "Yinsong Liu"
                    },
                    {
                        "authorId": "2064646914",
                        "name": "Bo Ren"
                    },
                    {
                        "authorId": "145592290",
                        "name": "R. Ji"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[14] achieved the highest structure detection results on ICDAR 2019 dataset by using Cascade Mask R-CNN with HighResolution Network (HRNet) backbone.",
                "The datasets are hand-labelled in cell format for table structure recognition as used in [14, 15].",
                "[14] utilized Cascade Mask R-CNN, which is developed upon Mask R-CNN and achieved the highest accuracy results on the ICDAR 2019 structure detection dataset.",
                "[14] present CascadeTabNet that detects tables with their types as bordered and borderless by utilizing Cascade-Mask-RCNN with High Resolution Network (HRNet)."
            ],
            "citingPaper": {
                "paperId": "b8bdf103e70e5bf962f85007462a4d14cfb22c78",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-02933",
                    "ArXiv": "2110.02933",
                    "DOI": "10.1016/j.neucom.2022.09.094",
                    "CorpusId": 238407904
                },
                "corpusId": 238407904,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b8bdf103e70e5bf962f85007462a4d14cfb22c78",
                "title": "On Cropped versus Uncropped Training Sets in Tabular Structure Detection",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2130012695",
                        "name": "Yakup Akkaya"
                    },
                    {
                        "authorId": "48778270",
                        "name": "Murat Simsek"
                    },
                    {
                        "authorId": "2497479",
                        "name": "B. Kantarci"
                    },
                    {
                        "authorId": "152235107",
                        "name": "Shahzad Khan"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Other approaches use custom pipelines that branch to consider different cases separately, such as training separate models to recognize tables with and without visible borders surrounding every cell [13, 21].",
                "Recently, there has been a shift in the research literature from traditional rule-based methods [3,10,17] for TE to data-driven methods based on deep learning (DL) [13,16,21].",
                "Modeling approaches One of the most common modeling approaches for TSR is to frame the task as some form of object detection [13, 16, 21]."
            ],
            "citingPaper": {
                "paperId": "a94c3e400fc5426a0b8a650b924242abcc1e46f2",
                "externalIds": {
                    "ArXiv": "2110.00061",
                    "DBLP": "conf/cvpr/SmockPA22",
                    "DOI": "10.1109/CVPR52688.2022.00459",
                    "CorpusId": 244462899
                },
                "corpusId": 244462899,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a94c3e400fc5426a0b8a650b924242abcc1e46f2",
                "title": "PubTables-1M: Towards comprehensive table extraction from unstructured documents",
                "abstract": "Recently, significant progress has been made applying machine learning to the problem of table structure inference and extraction from unstructured documents. However, one of the greatest challenges remains the creation of datasets with complete, unambiguous ground truth at scale. To address this, we develop a new, more comprehensive dataset for table extraction, called PubTables-1M. PubTables-1M contains nearly one million tables from scientific articles, supports multiple input modalities, and contains detailed header and location information for table structures, making it useful for a wide variety of modeling approaches. It also addresses a significant source of ground truth inconsistency observed in prior datasets called oversegmentation, using a novel canonicalization procedure. We demonstrate that these improvements lead to a significant increase in training performance and a more reliable estimate of model performance at evaluation for table structure recognition. Further, we show that transformer-based object detection models trained on PubTables-1M produce excellent results for all three tasks of detection, structure recognition, and functional analysis without the need for any special customization for these tasks. Data and code will be released at https://github.com/microsoft/table-transformer.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2745998",
                        "name": "B. Smock"
                    },
                    {
                        "authorId": "2130464272",
                        "name": "Rohith Pesala"
                    },
                    {
                        "authorId": "2394344",
                        "name": "Robin Abraham"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Using the table regions identified by CascadeTabNet, we are able to improve the table extraction accuracy for bulletins.",
                "A.2 Deep-learning augmented PDF parsing\nIn Figure 3, we show an example of CascadeTabNet enhanced table extraction.",
                "While native table extraction fails to correctly identify the tables due to insufficient separation between the tables, CascadeTabNet correctly detects seven out of the eight tables on the page.",
                "To correct for such errors, we utilize CascadeTabNet [19], a state-of-the-art convolutional neural network that identifies table regions and structure."
            ],
            "citingPaper": {
                "paperId": "5dd080cba0f36faebbe6aaad3bb1b845440481ad",
                "externalIds": {
                    "ArXiv": "2110.02311",
                    "CorpusId": 244909487
                },
                "corpusId": 244909487,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5dd080cba0f36faebbe6aaad3bb1b845440481ad",
                "title": "COVID-19 India Dataset: Parsing COVID-19 Data in Daily Health Bulletins from States in India (preprint)/ en",
                "abstract": "While India has been one of the hotspots of COVID-19, data about the pandemic from the country has proved to be largely inaccessible at scale. Much of the data exists in unstructured form on the web, and limited aspects of such data are available through public APIs maintained manually through volunteer effort. This has proved to be difficult both in terms of ease of access to detailed data and with regards to the maintenance of manual data-keeping over time. This paper reports on our effort at automating the extraction of such data from public health bulletins with the help of a combination of classical PDF parsers and state-of-the-art machine learning techniques. In this paper, we will describe the automated data-extraction technique, the nature of the generated data, and exciting avenues of ongoing work.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145625761",
                        "name": "Mayank Agarwal"
                    },
                    {
                        "authorId": "2500065",
                        "name": "T. Chakraborti"
                    },
                    {
                        "authorId": "145800526",
                        "name": "Sachin Grover"
                    },
                    {
                        "authorId": "1380490598",
                        "name": "Arunima Chaudhary"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Deep learning has been widely used in several fields nowadays, such as Medical applications like Cancers diagnoses, detection, and classification [1], image captioning [2], speech recognition [3] and in medical question answers [4, 5, 6], table detection and recognition [7, 8] also deep learning has been used in software engineering such as optimizing the time and schedule of the software projects [9, 10] and one of the most usages of Deep Learning is handwritten recognition for different languages as we will discuss."
            ],
            "citingPaper": {
                "paperId": "cef37afb6f3d1e8b3225516f2f76730d57ebdea2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-04075",
                    "ArXiv": "2110.04075",
                    "DOI": "10.1016/j.image.2022.116827",
                    "CorpusId": 238531347
                },
                "corpusId": 238531347,
                "publicationVenue": {
                    "id": "5e329303-0790-4991-832f-dd8a737ec6fb",
                    "name": "Signal processing. Image communication",
                    "type": "journal",
                    "alternate_names": [
                        "Signal process Image commun",
                        "Signal Process Commun",
                        "Signal Processing-image Communication"
                    ],
                    "issn": "0923-5965",
                    "url": "https://www.journals.elsevier.com/signal-processing-image-communication"
                },
                "url": "https://www.semanticscholar.org/paper/cef37afb6f3d1e8b3225516f2f76730d57ebdea2",
                "title": "KOHTD: Kazakh Offline Handwritten Text Dataset",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2089680772",
                        "name": "N. Toiganbayeva"
                    },
                    {
                        "authorId": "87034581",
                        "name": "M. Kasem"
                    },
                    {
                        "authorId": "2037963368",
                        "name": "Galymzhan Abdimanap"
                    },
                    {
                        "authorId": "9297705",
                        "name": "K. Bostanbekov"
                    },
                    {
                        "authorId": "115665360",
                        "name": "Abdelrahman Abdallah"
                    },
                    {
                        "authorId": "104462976",
                        "name": "Anel Alimova"
                    },
                    {
                        "authorId": "3253700",
                        "name": "D. Nurseitov"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "It is shown that the state-of-the-art table structure parsing approaches CascadeTabNet [12] and Split+Heuristic [19] for well-conditioned tabular images obtain poor performance on the challenging WTW dataset that has many wild images.",
                "Recently, the deep learning-based approaches were proposed to automatically learn informative visual features [5, 2, 24, 12].",
                "However, these methods mainly focuses on the well-conditioned document images, where the tables [16, 23, 12, 9, 23, 14] are well-aligned to the image axes.",
                "To evaluate the performance of Cycle-Centernet on the WTW dataset, we compare it with some latest table recognition methods including Split+Heuristic [19] and CascadeTabNet [12]."
            ],
            "citingPaper": {
                "paperId": "7a4c901e00fef01d66efb125bf0e2fdd773d8bc6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-02199",
                    "ArXiv": "2109.02199",
                    "DOI": "10.1109/ICCV48922.2021.00098",
                    "CorpusId": 237420694
                },
                "corpusId": 237420694,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/7a4c901e00fef01d66efb125bf0e2fdd773d8bc6",
                "title": "Parsing Table Structures in the Wild",
                "abstract": "This paper tackles the problem of table structure parsing (TSP) from images in the wild. In contrast to existing studies that mainly focus on parsing well-aligned tabular images with simple layouts from scanned PDF documents, we aim to establish a practical table structure parsing system for real-world scenarios where tabular input images are taken or scanned with severe deformation, bending or occlusions. For designing such a system, we propose an approach named Cycle-CenterNet on the top of CenterNet with a novel cycle-pairing module to simultaneously detect and group tabular cells into structured tables. In the cycle-pairing module, a new pairing loss function is proposed for the network training. Alongside with our Cycle-CenterNet, we also present a large-scale dataset, named Wired Table in the Wild (WTW), which includes well-annotated structure parsing of multiple style tables in several scenes like photo, scanning files, web pages, etc.. In experiments, we demonstrate that our Cycle-CenterNet consistently achieves the best accuracy of table structure parsing on the new WTW dataset by 24.6% absolute improvement evaluated by the TEDS metric. A more comprehensive experimental analysis also validates the advantages of our proposed methods for the TSP task.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "143799988",
                        "name": "Rujiao Long"
                    },
                    {
                        "authorId": "2119293226",
                        "name": "Wen Wang"
                    },
                    {
                        "authorId": "2685089",
                        "name": "Nan Xue"
                    },
                    {
                        "authorId": "2112256",
                        "name": "Feiyu Gao"
                    },
                    {
                        "authorId": "2109432908",
                        "name": "Zhibo Yang"
                    },
                    {
                        "authorId": "153709848",
                        "name": "Yongpan Wang"
                    },
                    {
                        "authorId": "39943835",
                        "name": "Gui-Song Xia"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Our first benchmark algorithmwas the CascadeTabNet [9] model (CascadeTabNet model).",
                "A multistage extension of Mask R-CNN is used by CDeC-Net [1], and one of the latest Regionbased Convolutional Neural Network (R-CNN) approaches is CascadeTabNet [9], which is a variation of a R-CNN using a cascade mask Region-based CNN.",
                "We evaluate table structure recognition using a benchmark algorithm are based on the successful CascadeTabNet [9] model which uses a Region CNN model architecture.",
                "The CascadeTabNet authors [9] state that \"high-end post-processing can improve the results significantly\", so we developed our own simple but novel post-processing method which we describe next."
            ],
            "citingPaper": {
                "paperId": "637c7cd9aefbf6af01431b4d26e4be548053dc9c",
                "externalIds": {
                    "DBLP": "conf/icdar/ZiomekM21",
                    "DOI": "10.1145/3476887.3476890",
                    "CorpusId": 240289942
                },
                "corpusId": 240289942,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/637c7cd9aefbf6af01431b4d26e4be548053dc9c",
                "title": "GloSAT Historical Measurement Table Dataset: Enhanced Table Structure Recognition Annotation for Downstream Historical Data Rescue",
                "abstract": "Understanding and extracting tables from documents is a research problem that has been studied for decades. Table structure recognition is the labelling of components within a detected table, which can be detected automatically or manually provided. This paper presents the GloSAT historical measurement table dataset designed to train table structure recognition models for use in downstream historical data rescue applications. The dataset contains 500 scanned and manually annotated images of pages from meteorological measurement logbooks. We enhance standard full table and individual cell annotations by adding additional annotations for headings, headers, and table bodies. We also provide annotations for coarse segmentation cells consisting of multiple data cells logically grouped by ruling lines of ink or whitespace in the table, which often represent data cells that are semantically grouped. Our dataset annotations are provided in VOC2007 and ICDAR-2019 Competition on Table Detection and Recognition (cTDaR-19) XML formats, and our dataset can easily be aggregated with the cTDaR-19 dataset. We report results running a series of benchmark algorithms on our new dataset, concluding that post-processing is very important for performance, and that page style is not as significant a feature as table type on model performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1884921320",
                        "name": "Juliusz Ziomek"
                    },
                    {
                        "authorId": "3097680",
                        "name": "S. Middleton"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "contexts": [
                "This enables our results to have a straightforward comparison with earlier state-of-the-art results [11].",
                "To establish a straightforward comparison with the recently achieved state-of-theart results [11] on TableBank, we report the results on the IoU threshold of 0.",
                "Moreover, they either rely on external pre-/post-processing methods to further refine their predictions [11,13] or incorporate memory intensive deformable convolutions [12,20].",
                "Furthermore, this paper empirically establishes that, instead of incorporating heavy backbone networks [11,12] and memory exhaustive deformable convolutions [20], state-of-the-art results are achievable by employing a relativelyightweight backbone network (ResNet-50) with SAC.",
                "In order to have a direct comparison against prior stateof-the-art [11], we report results on the modern datasets with an IoU threshold ranging from 0."
            ],
            "citingPaper": {
                "paperId": "d10d8164e472864968725424d2195789fb5e67bb",
                "externalIds": {
                    "PubMedCentral": "8540682",
                    "MAG": "3197383755",
                    "DBLP": "journals/jimaging/HashmiPLSA21",
                    "DOI": "10.3390/jimaging7100214",
                    "CorpusId": 239202542,
                    "PubMed": "34677300"
                },
                "corpusId": 239202542,
                "publicationVenue": {
                    "id": "c0fc53c7-b0ed-487d-9191-1262c8322621",
                    "name": "Journal of Imaging",
                    "type": "journal",
                    "alternate_names": [
                        "J Imaging"
                    ],
                    "issn": "2313-433X",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-556372",
                    "alternate_urls": [
                        "https://www.mdpi.com/journal/jimaging",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-556372"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d10d8164e472864968725424d2195789fb5e67bb",
                "title": "CasTabDetectoRS: Cascade Network for Table Detection in Document Images with Recursive Feature Pyramid and Switchable Atrous Convolution",
                "abstract": "Table detection is a preliminary step in extracting reliable information from tables in scanned document images. We present CasTabDetectoRS, a novel end-to-end trainable table detection framework that operates on Cascade Mask R-CNN, including Recursive Feature Pyramid network and Switchable Atrous Convolution in the existing backbone architecture. By utilizing a comparativelyightweight backbone of ResNet-50, this paper demonstrates that superior results are attainable without relying on pre- and post-processing methods, heavier backbone networks (ResNet-101, ResNeXt-152), and memory-intensive deformable convolutions. We evaluate the proposed approach on five different publicly available table detection datasets. Our CasTabDetectoRS outperforms the previous state-of-the-art results on four datasets (ICDAR-19, TableBank, UNLV, and Marmot) and accomplishes comparable results on ICDAR-17 POD. Upon comparing with previous state-of-the-art results, we obtain a significant relative error reduction of 56.36%, 20%, 4.5%, and 3.5% on the datasets of ICDAR-19, TableBank, UNLV, and Marmot, respectively. Furthermore, this paper sets a new benchmark by performing exhaustive cross-datasets evaluations to exhibit the generalization capabilities of the proposed method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "1771057",
                        "name": "A. Pagani"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "3adcc337a333650350a876be569ac7c3dfedcacd",
                "externalIds": {
                    "DBLP": "conf/mlcad/ChenLLWC21",
                    "DOI": "10.1109/MLCAD52597.2021.9531275",
                    "CorpusId": 237474135
                },
                "corpusId": 237474135,
                "publicationVenue": {
                    "id": "e539173b-8278-4cc8-9244-377441945a7a",
                    "name": "Workshop on Machine Learning for CAD",
                    "type": "conference",
                    "alternate_names": [
                        "MLCAD",
                        "Workshop Mach Learn CAD"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3adcc337a333650350a876be569ac7c3dfedcacd",
                "title": "Massive Figure Extraction and Classification in Electronic Component Datasheets for Accelerating PCB Design Preparation",
                "abstract": "Before starting printed-circuit-board (PCB) design, it is usually very time-consuming for PCB and system designers to review a large amount of electronic component datasheets in order to determine the best integration of electronic components for the target electronic systems. Each datasheet may contain over hundred figures and tables, while the figures and tables usually present the most important electronic component specifications. This paper categorizes various figures, including tables, in electronic component datasheets, and proposes the ECS-YOLO model for massive figure extraction and classification in order to accelerate PCB design preparation process. The experimental results show that, compared with the state-of-the-art object detection model, the proposed ECS-YOLO can consistently achieve better accuracy for figure extraction and classification in electronic component datasheets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2110723027",
                        "name": "Kuan-Chun Chen"
                    },
                    {
                        "authorId": "2145035454",
                        "name": "Chou-Chen Lee"
                    },
                    {
                        "authorId": "2651071",
                        "name": "Mark Po-Hung Lin"
                    },
                    {
                        "authorId": "51288120",
                        "name": "Yan-Jhih Wang"
                    },
                    {
                        "authorId": "2109060307",
                        "name": "Yi-Ting Chen"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "2dc7726a6e77fa993a1d58acaead8dee3a8183a5",
                "externalIds": {
                    "DOI": "10.1109/ICOIM52180.2021.9524385",
                    "CorpusId": 237403119
                },
                "corpusId": 237403119,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2dc7726a6e77fa993a1d58acaead8dee3a8183a5",
                "title": "Form Recognition Based on Lightweight U-Net and Tesseract after Multi-level Retraining",
                "abstract": "With the rapid development of Internet information technology and the advancement of enterprise digitization, the digitization of paper forms has also received extensive attention. The automatic conversion of paper form documents into electronic form documents mainly faces three problems. The first is that the format of the form file is diverse and the structure is complex. This article uses the XML file of the form to accurately analyze the structure of the file, which is more accurate than the current semantic segmentation method. The second problem is table area detection, this paper uses traditional algorithms to find the contours of the candidate table area, and screens according to the characteristics of the table area to complete the detection and extraction of the table area. The third is that the recognition of the table text is more difficult, not only the interference information such as the table frame will also affect the accuracy of text recognition, and the type of text information in the table is complex, including Chinese, English, numbers, symbols and mixed types, which bring huge challenges to text recognition. This paper uses the lightweight U-Net network model to segment the text area at pixel level, eliminating the interference information of text recognition. The neural network of Tesseract was retrained in a multiple, multi-level manner, and successfully realized the recognition of complex types of text information with an accuracy of about 96%. Based on deep learning and XML table structure analysis algorithm, this paper realizes the recognition of paper version of the form file and the reconstruction of the electronic version of the file.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2155122015",
                        "name": "Gang Li"
                    },
                    {
                        "authorId": "2815027",
                        "name": "Junhui Huang"
                    },
                    {
                        "authorId": "2144712727",
                        "name": "Zhao Wang"
                    },
                    {
                        "authorId": "8418310",
                        "name": "Jianmin Gao"
                    },
                    {
                        "authorId": "2110717926",
                        "name": "Kun Chen"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "contexts": [
                "It is essential to highlight that instead of the whole dataset, we utilize 1500 images each from Word and Latex split and 3000 images from the Word+Latex split to compare our results with prior state-of-the-art approach [8].",
                "9 which allows us to perform direct comparison with state-of-the-art approaches [8] [7][12] [6] in tables detection and segmentation.",
                "TableBank(Latex) Cascade-TabNet [8] 0.",
                "The current 72 state-of-the-art approach [8] has already achieved the perfect f1-score of 1.",
                "We use a smaller train-test split for training which is defined by the current 4 state-of-the-art approach [8].",
                "This makes our technique and approach far superior to CascadeTabNet [8].",
                "Moreover, CascadeTabNet [8] 84 apply line correction on the test data as an image postprocessing technique to improve 85 their results.",
                "Table 6 summarizes the comparison of HybridTabNet and current state-of-the-art 48 approaches [8] [71] on ICDAR-2019-TrackA-Modern dataset.",
                "CascadeTabNet [8] is an end-to-end table detection system that operates on Cascade Mask R-CNN with is an extension of Cascade R-CNN [17].",
                "CascadeTabNet [8] is an end-to-end table detection system that operates on Cascade Mask R-CNN, which is an extension of Cascade R-CNN [17].",
                "Moreover, CascadeTabNet [8] applies line correction on the test data as an image postprocessing technique to improve their results.",
                "Cascade-TabNet evaluates Latex, Word, and a mixture of Latex and Word documents only at the IoU threshold of 0.5.",
                "Table 6 shows the comparison of HybridTabNet and the current state-of-the-art approach Cascade-TabNet [8].",
                "We have used similar evaluation metrics to current state-of-the-art approaches [8] [7][12] for comparison of our results.",
                "Table 6 shows the comparison of HybridTabNet 78 and current state-of-the-art approach Cascade-TabNet [8]."
            ],
            "citingPaper": {
                "paperId": "209ac7327a0153ed2c03c1b07d067a822c35c8ce",
                "externalIds": {
                    "MAG": "3196203992",
                    "DOI": "10.20944/preprints202108.0360.v1",
                    "CorpusId": 238713567
                },
                "corpusId": 238713567,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/209ac7327a0153ed2c03c1b07d067a822c35c8ce",
                "title": "HybridTabNet: Towards Better Table Detection in Scanned Document Images",
                "abstract": "Tables in the document image are one of the most important entities since they contain crucial information. Therefore, accurate table detection can significantly improve information extraction from tables. In this work, we present a novel end-to-end trainable pipeline, HybridTabNet, for table detection in scanned document images. Our two-stage table detector uses the ResNeXt-101 backbone for feature extraction and Hybrid Task Cascade (HTC) to localize the tables in scanned document images. Moreover, we replace conventional convolutions with deformable convolutions in the backbone network. This enables our network to detect tables of arbitrary layouts precisely. We evaluate our approach comprehensively on ICDAR-13, ICDAR-17 POD, ICDAR-19, TableBank, Marmot, and UNLV. Apart from the ICDAR-17 POD dataset, our proposed HybridTabNet outperforms earlier state-of-the-art results without depending on pre and post-processing steps. Furthermore, to investigate how the proposed method generalizes unseen data, we conduct an exhaustive leave-one-out-evaluation. In comparison to prior state-of-the-art results, our method reduces the relative error by 27.57% on ICDAR-2019-TrackA-Modern, 42.64% on TableBank (Latex), 41.33% on TableBank (Word), 55.73% on TableBank (Latex + Word), 10% on Marmot, and 9.67% on UNLV dataset. The achieved results reflect the superior performance of the proposed method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    },
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    },
                    {
                        "authorId": "52053159",
                        "name": "Danish Nazir"
                    },
                    {
                        "authorId": "1771057",
                        "name": "A. Pagani"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "CascadeTabNet [5] is an end-to-end deep learning approach to table detection and table structure recognition using the Cascade Mask R-CNNHRNet model.",
                "We start with a pretrained CascadeTabNet model [5], fine-tuned it on our dataset, and used it to predict and draw bounding boxes around the table and station name regions."
            ],
            "citingPaper": {
                "paperId": "c96ebc1238245835960d768d1adb773b6b58edc5",
                "externalIds": {
                    "DBLP": "conf/doceng/OdunayoSBCPSCL21",
                    "DOI": "10.1145/3469096.3474929",
                    "CorpusId": 237099597
                },
                "corpusId": 237099597,
                "publicationVenue": {
                    "id": "32c90593-78aa-40de-8700-dd31a625b15a",
                    "name": "ACM Symposium on Document Engineering",
                    "type": "conference",
                    "alternate_names": [
                        "Doc Eng",
                        "Document Engineering",
                        "DocEng",
                        "ACM Symp Doc Eng"
                    ],
                    "url": "http://www.documentengineering.org/"
                },
                "url": "https://www.semanticscholar.org/paper/c96ebc1238245835960d768d1adb773b6b58edc5",
                "title": "Rescuing historical climate observations to support hydrological research: a case study of solar radiation data",
                "abstract": "The acceleration of climate change and its impact highlight the need for long-term reliable climate data at high spatiotemporal resolution to answer key science questions in cold regions hydrology. Prior to the digital age, climate records were archived on paper. For example, from the 1950s to the 1990s, solar radiation data from recording stations worldwide were published in booklets by the former Union of Soviet Socialist Republics (USSR) Hydrometeorological Service. As a result, the data are not easily accessible by most researchers. The overarching aim of this research is to develop techniques to convert paper-based climate records into a machine-readable format to support environmental research in cold regions. This study compares the performance of a proprietary optical character recognition (OCR) service with an open-source OCR tool for digitizing hydrometeorological data. We built a digitization pipeline combining different image preprocessing techniques, semantic segmentation, and an open-source OCR engine for extracting data and metadata recorded in the scanned documents. Each page contains blocks of text with station names and tables containing the climate data. The process begins with image preprocessing to reduce noise and to improve quality before the page content is segmented to detect tables and finally run through an OCR engine for text extraction. We outline the digitization process and report on initial results, including different segmentation approaches, preprocessing image algorithms, and OCR techniques to ensure accurate extraction and organization of relevant metadata from thousands of scanned climate records. We evaluated the performance of Tesseract OCR and ABBYY FineReader on text extraction. We find that although ABBY FineReader has better accuracy on the sample data, our custom extraction pipeline using Tesseract is efficient and scalable because it is flexible and allows for more customization.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2123350980",
                        "name": "Ogundepo Odunayo"
                    },
                    {
                        "authorId": "2123350394",
                        "name": "Naveela N. Sookoo"
                    },
                    {
                        "authorId": "2123350907",
                        "name": "Gautam Bathla"
                    },
                    {
                        "authorId": "2123350994",
                        "name": "Anthony Cavallin"
                    },
                    {
                        "authorId": "1573942961",
                        "name": "B. Persaud"
                    },
                    {
                        "authorId": "52124321",
                        "name": "Kathy Szigeti"
                    },
                    {
                        "authorId": "16346713",
                        "name": "P. Cappellen"
                    },
                    {
                        "authorId": "2154743379",
                        "name": "Jimmy Lin"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "0e134368116588bbd2bab3f0e0761550a6560321",
                "externalIds": {
                    "DBLP": "conf/doceng/MishraPS21",
                    "DOI": "10.1145/3469096.3475059",
                    "CorpusId": 237099404
                },
                "corpusId": 237099404,
                "publicationVenue": {
                    "id": "32c90593-78aa-40de-8700-dd31a625b15a",
                    "name": "ACM Symposium on Document Engineering",
                    "type": "conference",
                    "alternate_names": [
                        "Doc Eng",
                        "Document Engineering",
                        "DocEng",
                        "ACM Symp Doc Eng"
                    ],
                    "url": "http://www.documentengineering.org/"
                },
                "url": "https://www.semanticscholar.org/paper/0e134368116588bbd2bab3f0e0761550a6560321",
                "title": "Towards extraction of theorems and proofs in scholarly articles",
                "abstract": "Scholarly articles in mathematical fields often feature mathematical statements (theorems, propositions, etc.) and their proofs. In this paper, we present preliminary work for extracting such information from PDF documents, with several types of approaches: vision (using YOLO), natural language (with transformers), and styling information (with linear conditional random fields). Our main task is to identify which parts of the paper to label as theorem-like environments and proofs. We rely on a dataset collected from arXiv, with LATeX sources of research articles used to train the models.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2134908474",
                        "name": "Shrey Mishra"
                    },
                    {
                        "authorId": "2123349224",
                        "name": "Lucas Pluvinage"
                    },
                    {
                        "authorId": "1734682",
                        "name": "P. Senellart"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[21] used a cascade architecture for both table detection and cell segmentation."
            ],
            "citingPaper": {
                "paperId": "6e26602986e56d3524c325601674decb05cd8f2b",
                "externalIds": {
                    "DBLP": "conf/iccv/XueYWTL21",
                    "ArXiv": "2106.10598",
                    "DOI": "10.1109/ICCV48922.2021.00133",
                    "CorpusId": 235490364
                },
                "corpusId": 235490364,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/6e26602986e56d3524c325601674decb05cd8f2b",
                "title": "TGRNet: A Table Graph Reconstruction Network for Table Structure Recognition",
                "abstract": "A table arranging data in rows and columns is a very effective data structure, which has been widely used in business and scientific research. Considering large-scale tabular data in online and offline documents, automatic table recognition has attracted increasing attention from the document analysis community. Though human can easily understand the structure of tables, it remains a challenge for machines to understand that, especially due to a variety of different table layouts and styles. Existing methods usually model a table as either the markup sequence or the adjacency matrix between different table cells, failing to address the importance of the logical location of table cells, e.g., a cell is located in the first row and the second column of the table. In this paper, we reformulate the problem of table structure recognition as the table graph reconstruction, and propose an end-to-end trainable table graph reconstruction network (TGRNet) for table structure recognition. Specifically, the proposed method has two main branches, a cell detection branch and a cell logical location branch, to jointly predict the spatial location and the logical location of different cells. Experimental results on three popular table recognition datasets and a new dataset with table graph annotations (TableGraph-350K) demonstrate the effectiveness of the proposed TGRNet for table structure recognition. Code and annotations will be made publicly available at https://github.com/xuewenyuan/TGRNet.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51469630",
                        "name": "Wenyuan Xue"
                    },
                    {
                        "authorId": "2110431812",
                        "name": "Baosheng Yu"
                    },
                    {
                        "authorId": "2119293226",
                        "name": "Wen Wang"
                    },
                    {
                        "authorId": "2075330732",
                        "name": "Dacheng Tao"
                    },
                    {
                        "authorId": "1612980447",
                        "name": "Qingyong Li"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "CascadeTabNet [48] is a deep learning-based end-to-end solution that uses a single Convolution Neural Network (CNN) model to solve both table detection and structure recognition problems.",
                "CascadeTabNet proposed by [48] combined by Cascade-Mask-RCNN and High-Resolution Net (HRNet) and achieved a 1."
            ],
            "citingPaper": {
                "paperId": "0fdc3ba52e00518c29795d44b1303f3ccb400d76",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-15322",
                    "ArXiv": "2106.15322",
                    "DOI": "10.1016/j.neucom.2021.11.101",
                    "CorpusId": 235669757
                },
                "corpusId": 235669757,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0fdc3ba52e00518c29795d44b1303f3ccb400d76",
                "title": "TNCR: Table Net Detection and Classification Dataset",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "115665360",
                        "name": "Abdelrahman Abdallah"
                    },
                    {
                        "authorId": "2115948777",
                        "name": "Alexander Berendeyev"
                    },
                    {
                        "authorId": "2115949553",
                        "name": "Islam Nuradin"
                    },
                    {
                        "authorId": "3253700",
                        "name": "D. Nurseitov"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Here, we did not use the smudge technique to enhance the image as old hydrometeorological images are of poorer quality than the original modern images used in [9], the use of smudge will cause the image to lose the features.",
                "First, we use common data to evaluate the panel detection of two models Cascade mask R-CNN HRNetv2pW32 (CascadeTabNet [9]) and Cascade mask R-CNN HRNetv2pW40 (W represents feature width).",
                "The dataset has been labeled and provided by Devashish Prasad [9].",
                "Devashish Prasad et al. [9] in 2020 proposed an endto-end deep learning model CascadeTabNet used to detect and recognize the table structure.",
                "9 Cascade mask RCNN HRNetv2pW32 [9] 0.",
                "[9] in 2020 proposed an endto-end deep learning model CascadeTabNet used to detect and recognize the table structure.",
                "The proposed model achieves higher results than the Cascade mask RCNN HRNetv2pW32 model in most IoU indexes with an average accuracy of 96.75%.\nb) Experimental results on the TableBank data set\nSecond, we evaluate the table detection quality of the model on the Tablebank dataset."
            ],
            "citingPaper": {
                "paperId": "9275ed7cf4d3adb5e30a90e8f8d660c1d5d4a636",
                "externalIds": {
                    "MAG": "3184371482",
                    "DOI": "10.32913/mic-ict-research.v2021.n1.974",
                    "CorpusId": 237810909
                },
                "corpusId": 237810909,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9275ed7cf4d3adb5e30a90e8f8d660c1d5d4a636",
                "title": "An Integrated Approach for Table Detection and Structure Recognition",
                "abstract": "Detecting and identifying the table structure is an important issue in document digitization. Although there have been many great strides based on current deep learning techniques, table structure identification is still a difficult and difficult problem, especially when solving the problem of digitizing text in practice. The paper proposes a solution to digitize table documents based on the Cascade R-CNN HRNet network to detect, classify tables and integrate image processing algorithms to improve table data identification results. The proposed algorithm proved effective on real data - the hydrometeorological station record book contains tables including simple and complex structures tables with over 98% accuracy.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "8092281",
                        "name": "Hai-Hong Phan"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Prasad et al. (2020) proposed CascadeTab-Net that uses the instance segmentation technique to detect tables and segment the cells in a single inference step."
            ],
            "citingPaper": {
                "paperId": "8a1b6d3ad9af207da162439018562f1d2792d607",
                "externalIds": {
                    "DBLP": "conf/visapp/NamyslEBK22",
                    "ArXiv": "2105.11879",
                    "DOI": "10.5220/0010767600003124",
                    "CorpusId": 244800818
                },
                "corpusId": 244800818,
                "publicationVenue": {
                    "id": "3e53351c-7355-4159-a3f2-f3b03a3aa989",
                    "name": "VISIGRAPP",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Comput Vis Imaging Comput Graph Theory Appl",
                        "International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications"
                    ],
                    "issn": "2184-4321",
                    "alternate_issns": [
                        "2184-5921"
                    ],
                    "url": "http://www.visigrapp.org/"
                },
                "url": "https://www.semanticscholar.org/paper/8a1b6d3ad9af207da162439018562f1d2792d607",
                "title": "Flexible Table Recognition and Semantic Interpretation System",
                "abstract": "Table extraction is an important but still unsolved problem. In this paper, we introduce a flexible and modular table extraction system. We develop two rule-based algorithms that perform the complete table recognition process, including table detection and segmentation, and support the most frequent table formats. Moreover, to incorporate the extraction of semantic information, we develop a graph-based table interpretation method. We conduct extensive experiments on the challenging table recognition benchmarks ICDAR 2013 and ICDAR 2019, achieving results competitive with state-of-the-art approaches. Our complete information extraction system exhibited a high F1 score of 0.7380. To support future research on information extraction from documents, we make the resources (ground-truth annotations, evaluation scripts, algorithm parameters) from our table interpretation experiment publicly available.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "134442417",
                        "name": "Marcin Namysl"
                    },
                    {
                        "authorId": "153486145",
                        "name": "Alexander M. Esser"
                    },
                    {
                        "authorId": "1699019",
                        "name": "Sven Behnke"
                    },
                    {
                        "authorId": "152162939",
                        "name": "Joachim Kohler"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "We evaluate using F1-scores by analogy to [10] with IoU (Intersection over Union) thresholds of 0.",
                "The second one utilizes a conventional algorithm based on [10] that is specialized for bordered tables.",
                "[10], since their classification only considers two table types and also includes a slightly different definition of bordered and unbordered tables than ours.",
                "[10], who introduced the erosion and dilation operation for bordered tables, and extend this approach to implement a robust algorithm that can handle all table types.",
                "Figure 8: Annotation example from: a) the original validation dataset of [10], b) the manually labeled validation dataset of Multi-Type-TD-TSR.",
                "To enable comparability of Multi-Type-TD-TSR with other state-of-the-art approaches [10], we reuse their datasets.",
                "[10], we use erosion on bordered tables to detect vertical and horizontal borders, which need to be retained, while removing the font and characters from the table cells resulting in a grid-cell image.",
                "[10], which utilizes the erosion and dilation operation for extracting the row-column grid cell image without any text or characters.",
                "[10] randomly chose 342 images out of 600 of the ICDAR 19 training set to get 534 tables and 24,920 cells, with all these entities annotated accordingly.",
                "[10] presented an end-to-end TD and TSR model based on transfer learning."
            ],
            "citingPaper": {
                "paperId": "928c1ee7131998e7b961c39c6768f5809a268a86",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-11021",
                    "ArXiv": "2105.11021",
                    "DOI": "10.1007/978-3-030-87626-5_8",
                    "CorpusId": 235166100
                },
                "corpusId": 235166100,
                "publicationVenue": {
                    "id": "bff947de-0729-467e-b9cd-57b193e706a7",
                    "name": "Deutsche Jahrestagung f\u00fcr K\u00fcnstliche Intelligenz",
                    "type": "conference",
                    "alternate_names": [
                        "KI",
                        "Dtsch Jahrestag K\u00fcnstl Intell"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/928c1ee7131998e7b961c39c6768f5809a268a86",
                "title": "Multi-Type-TD-TSR - Extracting Tables from Document Images using a Multi-stage Pipeline for Table Detection and Table Structure Recognition: from OCR to Structured Table Representations",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2072761274",
                        "name": "Pascal Fischer"
                    },
                    {
                        "authorId": "2104443511",
                        "name": "Alen Smajic"
                    },
                    {
                        "authorId": "1733174",
                        "name": "Alexander Mehler"
                    },
                    {
                        "authorId": "2064953909",
                        "name": "Giuseppe Abrami"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In a sense, our method is akin to treating the ambiguity as a cascading step [27]."
            ],
            "citingPaper": {
                "paperId": "92f2bed2e335c09e2d841b2b5e2e7fca1f1d5f53",
                "externalIds": {
                    "DBLP": "conf/icdar/Ling0MI0SLS0G21",
                    "ArXiv": "2105.14931",
                    "DOI": "10.1007/978-3-030-86549-8_32",
                    "CorpusId": 235254299
                },
                "corpusId": 235254299,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/92f2bed2e335c09e2d841b2b5e2e7fca1f1d5f53",
                "title": "Document Domain Randomization for Deep Learning Document Layout Extraction",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2004871801",
                        "name": "Meng Ling"
                    },
                    {
                        "authorId": "49251914",
                        "name": "Jian Chen"
                    },
                    {
                        "authorId": "2076656173",
                        "name": "Torsten Moller"
                    },
                    {
                        "authorId": "1767660",
                        "name": "Petra Isenberg"
                    },
                    {
                        "authorId": "145322699",
                        "name": "Tobias Isenberg"
                    },
                    {
                        "authorId": "3020591",
                        "name": "M. Sedlmair"
                    },
                    {
                        "authorId": "1703037",
                        "name": "R. Laramee"
                    },
                    {
                        "authorId": "39073212",
                        "name": "Han-Wei Shen"
                    },
                    {
                        "authorId": "46365617",
                        "name": "Jian Wu"
                    },
                    {
                        "authorId": "145157784",
                        "name": "C. Lee Giles"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "One such open-source solution [3] that was explored as part of the literature survey was fairly accurate and was able to detect tables, but missed out on detecting and extracting cells from the table.",
                "The techniques employed in extracting tabular data from images usually involves advanced machine techniques such as Deep learning [3], Graph Neural networks [4] etc."
            ],
            "citingPaper": {
                "paperId": "eb7fd03770b478a0de3dcb9101632e55f43c477d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-09137",
                    "ArXiv": "2105.09137",
                    "CorpusId": 234778251
                },
                "corpusId": 234778251,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/eb7fd03770b478a0de3dcb9101632e55f43c477d",
                "title": "TableZa - A classical Computer Vision approach to Tabular Extraction",
                "abstract": "Computer aided Tabular Data Extraction has always been a very challenging and error prone task because it demands both Spectral and Spatial Sanity of data. In this paper we discuss an approach for Tabular Data Extraction in the realm of document comprehension. Given the different kinds of the Tabular formats that are often found across various documents, we discuss a novel approach using Computer Vision for extraction of tabular data from images or vector pdf(s) converted to image(s).",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1935310175",
                        "name": "Saumya Banthia"
                    },
                    {
                        "authorId": "2109671235",
                        "name": "Anantha Sharma"
                    },
                    {
                        "authorId": "89440985",
                        "name": "R. Mangipudi"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "After that, a group of methods [36,23,38] tries to recover the cell relations based on some heuristic rules and algorithms.",
                "With the great success of deep neural network in computer vision field, works began to focus on the image-based table with more general structures [21,30,24,9,13,36,23,14,33].",
                "That is why many segmentation-based methods [24,23,22] struggle with complicated post-processing, such as fracture completion and threshold setting."
            ],
            "citingPaper": {
                "paperId": "ea25282d27368d3d04db91b165b5003d63e335d6",
                "externalIds": {
                    "DBLP": "conf/icdar/QiaoLCZPNRT021",
                    "ArXiv": "2105.06224",
                    "DOI": "10.1007/978-3-030-86549-8_7",
                    "CorpusId": 234482682
                },
                "corpusId": 234482682,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/ea25282d27368d3d04db91b165b5003d63e335d6",
                "title": "LGPMA: Complicated Table Structure Recognition with Local and Global Pyramid Mask Alignment",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2065512194",
                        "name": "Liang Qiao"
                    },
                    {
                        "authorId": "2109614687",
                        "name": "Zaisheng Li"
                    },
                    {
                        "authorId": "2398015",
                        "name": "Zhanzhan Cheng"
                    },
                    {
                        "authorId": "2151333065",
                        "name": "Peng Zhang"
                    },
                    {
                        "authorId": "3290437",
                        "name": "Shiliang Pu"
                    },
                    {
                        "authorId": "1490934795",
                        "name": "Yi Niu"
                    },
                    {
                        "authorId": "144850642",
                        "name": "Wenqi Ren"
                    },
                    {
                        "authorId": "51151126",
                        "name": "Wenming Tan"
                    },
                    {
                        "authorId": "144894837",
                        "name": "Fei Wu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "bc8863440e26e48866a64f63438051d24880b793",
                "externalIds": {
                    "DBLP": "journals/access/HashmiLSAAA21",
                    "ArXiv": "2104.14272",
                    "DOI": "10.1109/ACCESS.2021.3087865",
                    "CorpusId": 233444247
                },
                "corpusId": 233444247,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bc8863440e26e48866a64f63438051d24880b793",
                "title": "Current Status and Performance Analysis of Table Recognition in Document Images With Deep Neural Networks",
                "abstract": "The first phase of table recognition is to detect the tabular area in a document. Subsequently, the tabular structures are recognized in the second phase in order to extract information from the respective cells. Table detection and structural recognition are pivotal problems in the domain of table understanding. However, table analysis is a perplexing task due to the colossal amount of diversity and asymmetry in tables. Therefore, it is an active area of research in document image analysis. Recent advances in the computing capabilities of graphical processing units have enabled the deep neural networks to outperform traditional state-of-the-art machine learning methods. Table understanding has substantially benefited from the recent breakthroughs in deep neural networks. However, there has not been a consolidated description of the deep learning methods for table detection and table structure recognition. This review paper provides a thorough analysis of the modern methodologies that utilize deep neural networks. Moreover, it presents a comprehensive understanding of the current state-of-the-art and related challenges of table understanding in document images. The leading datasets and their intricacies have been elaborated along with the quantitative results. Furthermore, a brief overview is given regarding the promising directions that can further improve table analysis in document images.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    },
                    {
                        "authorId": "2206035070",
                        "name": "Muhammad Adnan Afzal"
                    },
                    {
                        "authorId": "2206035072",
                        "name": "Muhammad Ahtsham Afzal"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "CascadeTabNet [11] proposed a Cascade mask Region-based R-CNN model to detect tables and found intersections with computer vision method to manage structure recognition."
            ],
            "citingPaper": {
                "paperId": "c6cfc294be67465d9230245b40edbf0a4a0c9172",
                "externalIds": {
                    "DBLP": "conf/apvis/JungCPKKS21",
                    "DOI": "10.1109/PacificVis52677.2021.00022",
                    "CorpusId": 235206640
                },
                "corpusId": 235206640,
                "publicationVenue": {
                    "id": "5e8a55cb-fc9d-4567-be61-ca880f358422",
                    "name": "IEEE Pacific Visualization Symposium",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Pac Vis Symp",
                        "PacificVis"
                    ],
                    "issn": "2165-8765",
                    "alternate_issns": [
                        "2165-8773"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1001657"
                },
                "url": "https://www.semanticscholar.org/paper/c6cfc294be67465d9230245b40edbf0a4a0c9172",
                "title": "Mixed-Initiative Approach to Extract Data from Pictures of Medical Invoice",
                "abstract": "Extracting data from pictures of medical records is a common task in the insurance industry as the patients often send their medical invoices taken by smartphone cameras. However, the overall process is still challenging to be fully automated because of low image quality and variation of templates that exist in the status quo. In this paper, we propose a mixed-initiative pipeline for extracting data from pictures of medical invoices, where deep-learning-based automatic prediction models and task-specific heuristics work together under the mediation of a user. In the user study with 12 participants, we confirmed our mixed-initiative approach can supplement the drawbacks of a fully automated approach within an acceptable completion time. We further discuss the findings, limitations, and future works for designing a mixed-initiative system to extract data from pictures of a complicated table.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2110516062",
                        "name": "Seokweon Jung"
                    },
                    {
                        "authorId": "116012112",
                        "name": "Kiroong Choe"
                    },
                    {
                        "authorId": "2118886415",
                        "name": "Seokhyeon Park"
                    },
                    {
                        "authorId": "2065492498",
                        "name": "Hyung-Kwon Ko"
                    },
                    {
                        "authorId": "2108051792",
                        "name": "Youngtaek Kim"
                    },
                    {
                        "authorId": "2016076",
                        "name": "Jinwook Seo"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "300cbaf1644920b15a97692e6309f5d58e1abc0e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-15348",
                    "ArXiv": "2103.15348",
                    "DOI": "10.1007/978-3-030-86549-8_9",
                    "CorpusId": 232404723
                },
                "corpusId": 232404723,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/300cbaf1644920b15a97692e6309f5d58e1abc0e",
                "title": "LayoutParser: A Unified Toolkit for Deep Learning Based Document Image Analysis",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "101568984",
                        "name": "Zejiang Shen"
                    },
                    {
                        "authorId": "49775305",
                        "name": "Ruochen Zhang"
                    },
                    {
                        "authorId": "2065276972",
                        "name": "Melissa Dell"
                    },
                    {
                        "authorId": "145485725",
                        "name": "B. Lee"
                    },
                    {
                        "authorId": "2060946945",
                        "name": "Jacob Carlson"
                    },
                    {
                        "authorId": "2108801852",
                        "name": "Weining Li"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [
                "[8] proposed CascadeTabNet which is a Cascade mask Regionbased CNN High-Resolution Network model combining the transfer learning, image transformation and data augmentation technique to improve the process."
            ],
            "citingPaper": {
                "paperId": "70445005b2d4ed9f19c316792e315af1f3efd4d6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-10287",
                    "ArXiv": "2102.10287",
                    "CorpusId": 231986434
                },
                "corpusId": 231986434,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/70445005b2d4ed9f19c316792e315af1f3efd4d6",
                "title": "Deep Structured Feature Networks for Table Detection and Tabular Data Extraction from Scanned Financial Document Images",
                "abstract": "Automatic table detection in PDF documents has achieved a great success but tabular data extraction are still challenging due to the integrity and noise issues in detected table areas. The accurate data extraction is extremely crucial in finance area. Inspired by this, the aim of this research is proposing an automated table detection and tabular data extraction from financial PDF documents. We proposed a method that consists of three main processes, which are detecting table areas with a Faster R-CNN (Region-based Convolutional Neural Network) model with Feature Pyramid Network (FPN) on each page image, extracting contents and structures by a compounded layout segmentation technique based on optical character recognition (OCR) and formulating regular expression rules for table header separation. The tabular data extraction feature is embedded with rule-based filtering and restructuring functions that are highly scalable. We annotate a new Financial Documents dataset with table regions for the experiment. The excellent table detection performance of the detection model is obtained from our customized dataset. The main contributions of this paper are proposing the Financial Documents dataset with table-area annotations, the superior detection model and the rule-based layout segmentation technique for the tabular data extraction from PDF files.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "39209233",
                        "name": "Siwen Luo"
                    },
                    {
                        "authorId": "2145173098",
                        "name": "Mengting Wu"
                    },
                    {
                        "authorId": "2005537071",
                        "name": "Yiwen Gong"
                    },
                    {
                        "authorId": "2008166532",
                        "name": "Wanying Zhou"
                    },
                    {
                        "authorId": "144179461",
                        "name": "Josiah Poon"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "9dbb26e4f04f38aa2a11289cbc82a2d9ed533741",
                "externalIds": {
                    "ArXiv": "2102.08445",
                    "DBLP": "journals/corr/abs-2102-08445",
                    "DOI": "10.1145/3397482.3450718",
                    "CorpusId": 231942681
                },
                "corpusId": 231942681,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9dbb26e4f04f38aa2a11289cbc82a2d9ed533741",
                "title": "TableLab: An Interactive Table Extraction System with Adaptive Deep Learning",
                "abstract": "Table extraction from PDF and image documents is a ubiquitous task in the real-world. Perfect extraction quality is difficult to achieve with one single out-of-box model due to (1) the wide variety of table styles, (2) the lack of training data representing this variety and (3) the inherent ambiguity and subjectivity of table definitions between end-users. Meanwhile, building customized models from scratch can be difficult due to the expensive nature of annotating table data. We attempt to solve these challenges with TableLab by providing a system where users and models seamlessly work together to quickly customize high-quality extraction models with a few labelled examples for the user\u2019s document collection, which contains pages with tables. Given an input document collection, TableLab first detects tables with similar structures (templates) by clustering embeddings from the extraction model. Document collections often contain tables created with a limited set of templates or similar structures. It then selects a few representative table examples already extracted with a pre-trained base deep learning model. Via an easy-to-use user interface, users provide feedback to these selections without necessarily having to identify every single error. TableLab then applies such feedback to finetune the pre-trained model and returns the results of the finetuned model back to the user. The user can choose to repeat this process iteratively until obtaining a customized model with satisfactory performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "31856827",
                        "name": "N. Wang"
                    },
                    {
                        "authorId": "143894459",
                        "name": "D. Burdick"
                    },
                    {
                        "authorId": "1718694",
                        "name": "Yunyao Li"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "6c6354b0c4d55890399aecd7fdd9e2432710aea4",
                "externalIds": {
                    "MAG": "3126481501",
                    "DBLP": "journals/ijon/JiangSKK21",
                    "DOI": "10.1016/j.neucom.2021.01.103",
                    "CorpusId": 233873553
                },
                "corpusId": 233873553,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6c6354b0c4d55890399aecd7fdd9e2432710aea4",
                "title": "TabCellNet: Deep learning-based tabular cell structure detection",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1995868863",
                        "name": "JiChu Jiang"
                    },
                    {
                        "authorId": "48778270",
                        "name": "Murat Simsek"
                    },
                    {
                        "authorId": "2497479",
                        "name": "B. Kantarci"
                    },
                    {
                        "authorId": "152235107",
                        "name": "Shahzad Khan"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "19887be4f0dc5be93ae2968e9270f66a6e592723",
                "externalIds": {
                    "MAG": "3175634416",
                    "DOI": "10.1007/978-3-030-80478-7_13",
                    "CorpusId": 238097021
                },
                "corpusId": 238097021,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/19887be4f0dc5be93ae2968e9270f66a6e592723",
                "title": "Method for Processing Document with Tables in Russian-Language Automated Information Systems",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1580466066",
                        "name": "Dudnikov Sergey"
                    },
                    {
                        "authorId": "1580472290",
                        "name": "Mikheev Petr"
                    },
                    {
                        "authorId": "2135873873",
                        "name": "Dobrovolskiy Alexander"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "For analysing the tables in scanned documents, (Schreiber et al. 2017; Paliwal et al. 2019; Prasad et al. 2020) propose different modifications to standard CNN network architectures such as VGGNet (Si-\n.\nmonyan and Zisserman 2014) used for classification and Faster R-CNN (Ren et al. 2015) for\u2026",
                "For analysing the tables in scanned documents, (Schreiber et al. 2017; Paliwal et al. 2019; Prasad et al. 2020) propose different modifications to standard CNN network architectures such as VGGNet (SiFigure 3: Results of Table Token Classification."
            ],
            "citingPaper": {
                "paperId": "d1cec8d516931428b492a8e4fbbcef38957bce2b",
                "externalIds": {
                    "ArXiv": "2009.14457",
                    "MAG": "3090669478",
                    "DBLP": "journals/corr/abs-2009-14457",
                    "CorpusId": 222067107
                },
                "corpusId": 222067107,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d1cec8d516931428b492a8e4fbbcef38957bce2b",
                "title": "Towards a Multi-modal, Multi-task Learning based Pre-training Framework for Document Representation Learning",
                "abstract": "In this paper, we propose a multi-task learning-based framework that utilizes a combination of self-supervised and supervised pre-training tasks to learn a generic document representation. We design the network architecture and the pre-training tasks to incorporate the multi-modal document information across text, layout, and image dimensions and allow the network to work with multi-page documents. We showcase the applicability of our pre-training framework on a variety of different real-world document tasks such as document classification, document information extraction, and document retrieval. We conduct exhaustive experiments to compare performance against different ablations of our framework and state-of-the-art baselines. We discuss the current limitations and next steps for our work.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "51011359",
                        "name": "S. Pramanik"
                    },
                    {
                        "authorId": "33906968",
                        "name": "Shashank Mujumdar"
                    },
                    {
                        "authorId": "1557607422",
                        "name": "Hima Patel"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "c9c61347473f59e2934357916e7518b68ed89506",
                "externalIds": {
                    "DOI": "10.1109/cvprw50498.2020.00004",
                    "CorpusId": 242864768
                },
                "corpusId": 242864768,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c9c61347473f59e2934357916e7518b68ed89506",
                "title": "CVPRW 2020 TOC",
                "abstract": null,
                "year": 2020,
                "authors": []
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Also, [23, 26] used different train/test split from the original competition without publishing their split and so cannot be compared directly.",
                "Deep learning approaches include two categories: (a) End-to-end image-to-sequence models [18, 36]; (b) Object detection based methods [26, 33, 23]."
            ],
            "citingPaper": {
                "paperId": "73a906a988e54defee536a120125f957059d595e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2005-00589",
                    "MAG": "3022479961",
                    "ArXiv": "2005.00589",
                    "DOI": "10.1109/WACV48630.2021.00074",
                    "CorpusId": 218487305
                },
                "corpusId": 218487305,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/73a906a988e54defee536a120125f957059d595e",
                "title": "Global Table Extractor (GTE): A Framework for Joint Table Identification and Cell Structure Recognition Using Visual Context",
                "abstract": "Documents are often used for knowledge sharing and preservation in business and science, within which are tables that capture most of the critical data. Unfortunately, most documents are stored and distributed as PDF or scanned images, which fail to preserve logical table structure. Recent vision-based deep learning approaches have been proposed to address this gap, but most still cannot achieve state-of-the-art results. We present Global Table Extractor (GTE), a vision-guided systematic framework for joint table detection and cell structured recognition, which could be built on top of any object detection model. With GTE-Table, we invent a new penalty based on the natural cell containment constraint of tables to train our table network aided by cell location predictions. GTE-Cell is a new hierarchical cell detection network that leverages table styles. Further, we design a method to automatically label table and cell structure in existing documents to cheaply create a large corpus of training and test data. We use this to enhance PubTabNet with cell labels and create FinTabNet, real-world and complex scientific and financial datasets with detailed table structure annotations to help train and test structure recognition. Our framework surpasses previous state-of-the-art results on the ICDAR 2013 and ICDAR 2019 table competition in both table detection and cell structure recognition. Further experiments demonstrate a greater than 45% improvement in cell structure recognition when compared to a vanilla RetinaNet object detection model in our new out-of-domain FinTabNet.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1662767314",
                        "name": "Xinyi Zheng"
                    },
                    {
                        "authorId": "143894459",
                        "name": "D. Burdick"
                    },
                    {
                        "authorId": "145378077",
                        "name": "Lucian Popa"
                    },
                    {
                        "authorId": "31856827",
                        "name": "N. Wang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "6c411f52c151fe537ee76c310f45f0dd3da88e92",
                "externalIds": {
                    "DBLP": "series/sbcs/Bhowmik23",
                    "DOI": "10.1007/978-981-99-4277-0",
                    "CorpusId": 260337124
                },
                "corpusId": 260337124,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6c411f52c151fe537ee76c310f45f0dd3da88e92",
                "title": "Document Layout Analysis",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3272848",
                        "name": "Showmik Bhowmik"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "\u2022 Table Detection and Table Structure Recognition Module, which uses the pre-trained CascadeTabNet model to identify the masks (i.e. bounding boxes) of borderless/bordered tables and cells.",
                "In particular, for subtasks 1-2 we use CascadeTabNet [6], a recent implementation of a CNN model which was trained by the authors on the detection of border/borderless tables and cells first on general tables (e.",
                "In particular, for subtasks 1-2 we use CascadeTabNet [6], a recent implementation of a CNN model which was trained by the authors on the detection of border/borderless tables and cells first on general tables (e.g. word, latex documents), then on ICDAR-193.",
                "We trained the model using MMDetection, an open source object detection toolbox, loading the pre-trained weights of CascadeTabNet model and training for 10 epochs using our annotated data."
            ],
            "citingPaper": {
                "paperId": "3b1ac00a8ba07471e321abfe1f301a7e65a8d129",
                "externalIds": {
                    "DBLP": "conf/sebd/ScafoglieriMNLL22",
                    "CorpusId": 251770943
                },
                "corpusId": 251770943,
                "publicationVenue": {
                    "id": "c545d136-74ba-4905-8ed9-b9ec76a5e444",
                    "name": "Sistemi Evoluti per Basi di Dati",
                    "type": "conference",
                    "alternate_names": [
                        "SEBD",
                        "Sist Evol Basi Dati"
                    ],
                    "url": "http://www.sebd.org/"
                },
                "url": "https://www.semanticscholar.org/paper/3b1ac00a8ba07471e321abfe1f301a7e65a8d129",
                "title": "Automatic Information Extraction from Investment Product Documents",
                "abstract": "In this paper we report on the activities carried out within a collaboration between Consob and Sapienza University. The developed project focus on Information Extraction from documents describing financial investment products. We discuss how we automate this task, via both rule-based and machine learning-based methods, and describe the performances of our approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "146748492",
                        "name": "Federico Maria Scafoglieri"
                    },
                    {
                        "authorId": "2152022004",
                        "name": "Alessandra Monaco"
                    },
                    {
                        "authorId": "2182552491",
                        "name": "Giulia Neccia"
                    },
                    {
                        "authorId": "1739846",
                        "name": "D. Lembo"
                    },
                    {
                        "authorId": "2133418189",
                        "name": "Alessandra Limosani"
                    },
                    {
                        "authorId": "2065582153",
                        "name": "F. Medda"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "After that, we have the supervised methods such as in [5, 11] that used Faster R-CNN based model for table detection, or [22, 23] used CNN to performed table detection and table structure recognition at the same time.",
                "More precisely, for the ICDAR 2013 dataset, [23]",
                "Further comparisons with other methods in the literature are also available in the referenced papers [8, 22, 23, 25].",
                "[23] and [22] are deep-learning approaches based on convolution networks which allow to detect the regions of tables and recognize the structural body cells from the detected tables at the same time."
            ],
            "citingPaper": {
                "paperId": "b5460d0ac2f2944276d70afdab378f6a907161f4",
                "externalIds": {
                    "DBLP": "conf/iciap/Ngo22",
                    "DOI": "10.1007/978-3-031-06430-2_18",
                    "CorpusId": 248865723
                },
                "corpusId": 248865723,
                "publicationVenue": {
                    "id": "c89c0957-b21e-40c3-9e6c-0ab46adf1e53",
                    "name": "International Conference on Image Analysis and Processing",
                    "type": "conference",
                    "alternate_names": [
                        "ICIAP",
                        "Int Conf Image Anal Process"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1380"
                },
                "url": "https://www.semanticscholar.org/paper/b5460d0ac2f2944276d70afdab378f6a907161f4",
                "title": "Digital Line Segment Detection for Table Reconstruction in Document Images",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "143866646",
                        "name": "P. Ngo"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "We also plan to introduce a table region detection module into our method by using state-of-the-art\ntable detection algorithms such as (Riba et al., 2019) and (Prasad et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "fc846fd6cd62b1bcb7bd73314ff4c18a3182bb74",
                "externalIds": {
                    "DBLP": "conf/icpram/AoyagiKTUO22",
                    "DOI": "10.5220/0010817700003122",
                    "CorpusId": 246966938
                },
                "corpusId": 246966938,
                "publicationVenue": {
                    "id": "8ef5945c-5b25-4774-b55a-15cd5450f6e4",
                    "name": "International Conference on Pattern Recognition Applications and Methods",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Pattern Recognit Appl Method",
                        "ICPRAM"
                    ],
                    "url": "http://icpram.org/"
                },
                "url": "https://www.semanticscholar.org/paper/fc846fd6cd62b1bcb7bd73314ff4c18a3182bb74",
                "title": "Table-structure Recognition Method Consisting of Plural Neural Network Modules",
                "abstract": "In academic papers, tables are often used to summarize experimental results. However, graphs are more suitable than tables for grasping many experimental results at a glance because of the high visibility. Therefore, automatic graph generation from a table has been studied. Because the structure and style of a table vary depending on the authors, this paper proposes a table-structure recognition method using plural neural network (NN) modules. The proposed method consists of four NN modules: two of them merge detected tokens in a table, one estimates implicit ruled lines that are necessary to separate cells but undrawn, and the last estimates cells by merging the tokens. We demonstrated the effectiveness of the proposed method by experiments using the ICDAR 2013 table competition dataset. Consequently, the proposed method achieved an F-measure of 0.972, outperforming those of our earlier work (Ohta et al., 2021) by 1.7 percentage points and of the topranked participant in that competition by 2.6 percentage points.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144827799",
                        "name": "Hiroyuki Aoyagi"
                    },
                    {
                        "authorId": "1894424",
                        "name": "T. Kanazawa"
                    },
                    {
                        "authorId": "144752372",
                        "name": "A. Takasu"
                    },
                    {
                        "authorId": "7672960",
                        "name": "Fumito Uwano"
                    },
                    {
                        "authorId": "46592126",
                        "name": "Manabu Ohta"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "44220fb755ed8259a6d1183e47e5e957ab5b0874",
                "externalIds": {
                    "CorpusId": 253857301
                },
                "corpusId": 253857301,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/44220fb755ed8259a6d1183e47e5e957ab5b0874",
                "title": "Relative Layout Matching for Document Data Extraction",
                "abstract": "This thesis explores the field of business document information extraction, emphasizing one-shot learning systems that improve their performance by utilizing a database of previously processed documents. A benchmark to evaluate one-shot information extraction systems was defined and used with a newly created dataset. A novel representation-learning approach to one-shot document information extraction was proposed. For a newly received document, the proposed approach uses learned document representation to first retrieve field representations from similar documents. Retrieved representations are then used to localize information on the newly received document. The proposed method was evaluated and compared against several proposed baselines showing an improvement on fields with high positional variance. The baseline method still achieves better results on fields that remain fixed within the layout.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2071515342",
                        "name": "Maty\u00e1s Skalick\u00fd"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "2ce9974fe0fffffb6f852e56e23672cfb41b6567",
                "externalIds": {
                    "CorpusId": 255227048
                },
                "corpusId": 255227048,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2ce9974fe0fffffb6f852e56e23672cfb41b6567",
                "title": "OVERVIEW OF OCR TOOLS FOR THE TASK OF RECOGNIZING TABLES AND GRAPHS IN DOCUMENTS",
                "abstract": "This study describes OCR tools for recognizing tables and graphs. There is a great demand for solutions that can effectively automate the processing of an extensive array of documents. Existing OCR solutions can efficiently recognize text, but recognizing graphical elements, such as charts and tables, is still in the making. Solutions that can increase the accuracy of visual data recognition can be valuable for technical document processing, such as scientific, financial, and analytical documents.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "143737606",
                        "name": "O. Yaroshenko"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "CascadeTabNet [14] is an Mask-RCNN based network trained to localize table regions in reasonably structured documents.",
                "As Table 3a shows, recent models developed specifically for table localization (CascadeTabNet [14], TableBank [13]) show good performance.",
                "Modified v/s Original U-Net: To begin with, we compared the performance of original and modified U-Net.",
                "TableBank [13] 17M YOLO-v3 [17] 65M pix2pixHD [23] 188M CascadeTabNet [14] 83M Original U-Net [14] 8M Modified U-Net (Ours) 3.",
                "In addition, we trained CascadeTabNet [14], a state-of-the-art segmentation model developed specifically for table segmentation."
            ],
            "citingPaper": {
                "paperId": "d22282974925d53d57ab11bcd91f8dc27c2f9f4a",
                "externalIds": {
                    "DBLP": "conf/icdar/DeshpandePS21",
                    "DOI": "10.1007/978-3-030-86198-8_9",
                    "CorpusId": 237457885
                },
                "corpusId": 237457885,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d22282974925d53d57ab11bcd91f8dc27c2f9f4a",
                "title": "MediTables: A New Dataset and Deep Network for Multi-category Table Localization in Medical Documents",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2090545649",
                        "name": "A. Deshpande"
                    },
                    {
                        "authorId": "2126073962",
                        "name": "Vaishnav Potlapalli"
                    },
                    {
                        "authorId": "2126074083",
                        "name": "R. Sarvadevabhatla"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[15] proposed CascadeTab-Net that uses the instance segmentation technique to detect tables and segment the cells in a single inference step."
            ],
            "citingPaper": {
                "paperId": "ae184fcffda938790b0eb40a3957ea96cd9da0c8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-11879",
                    "CorpusId": 235187264
                },
                "corpusId": 235187264,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ae184fcffda938790b0eb40a3957ea96cd9da0c8",
                "title": "Tab.IAIS: Flexible Table Recognition and Semantic Interpretation System",
                "abstract": "Table extraction is an important but still unsolved problem. In this paper, we introduce a flexible end-to-end table extraction system. We develop two rule-based algorithms that perform the complete table recognition process and support the most frequent table formats found in the scientific literature. Moreover, to incorporate the extraction of semantic information into the table recognition process, we develop a graph-based table interpretation method. We conduct extensive experiments on the challenging table recognition benchmarks ICDAR 2013 and ICDAR 2019. Our table recognition approach achieves results competitive with state-of-the-art approaches. Moreover, our complete information extraction system exhibited a high F1 score of 0.7380 proving the utility of our approach.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "134442417",
                        "name": "Marcin Namysl"
                    },
                    {
                        "authorId": "153486145",
                        "name": "Alexander M. Esser"
                    },
                    {
                        "authorId": "1699019",
                        "name": "Sven Behnke"
                    },
                    {
                        "authorId": "152162939",
                        "name": "Joachim Kohler"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [],
            "contexts": [],
            "citingPaper": {
                "paperId": "46a660a998dba7eecffc6a4b6e37dd4010fd57a5",
                "externalIds": {
                    "DOI": "10.1088/1742-6596/1978/1/012010",
                    "CorpusId": 236464127
                },
                "corpusId": 236464127,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/46a660a998dba7eecffc6a4b6e37dd4010fd57a5",
                "title": "MT-YOLOv5: Mobile terminal table detection model based on YOLOv5",
                "abstract": "Table detection is an important task of optical character recognition(OCR). At present, table detection for desktop applications has basically reached commercial requirements. With the advancement of informatization, personal demand for table detection has gradually increased. There is an urgent need to establish a table detection method that can be deployed on handheld devices. This paper proposes a mobile terminal table detection model based on YOLOv5. First, we used YOLOv5 as the main framework of the model. However, considering the problem of connection redundancy in the backbone of YOLOv5, on the basis of retaining the YOLOv5 multi-scale detection head, we replaced the backbone of YOLOv5 with the same excellent Mobilenetv2. In addition, considering the non-linear defects of the lightweight model, we use deformable convolution to make up for it. This paper has been evaluated on the ICDAR 2019 dataset, and the results show that compared with the baseline model, the model reduces the number of parameters by half and increases the detection speed by 47%. At the same time, the model can reach 35.25 FPS on ordinary Android phones.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "14977483",
                        "name": "Z. Ning"
                    },
                    {
                        "authorId": "2145502331",
                        "name": "Xinjiao Wu"
                    },
                    {
                        "authorId": "2143850157",
                        "name": "Jing Yang"
                    },
                    {
                        "authorId": "2108775224",
                        "name": "Yanqin Yang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Recently, there has been a shift in the research literature from traditional rule-based methods [7\u20139] for TE to data-driven methods based on deep learning (DL) [2, 10, 11]."
            ],
            "citingPaper": {
                "paperId": "17119b49d68800bbc57bbd786ca339917c6f00fa",
                "externalIds": {
                    "CorpusId": 238253171
                },
                "corpusId": 238253171,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/17119b49d68800bbc57bbd786ca339917c6f00fa",
                "title": "Scientific evidence extraction",
                "abstract": "Recently, interest has grown in applying machine learning to the problem of table structure inference and extraction from unstructured documents. However, progress in this area has been challenging both to make and to measure, due to several issues that arise in training and evaluating models from labeled data. This includes challenges as fundamental as the lack of a single definitive ground truth output for each input sample and the lack of an ideal metric for measuring partial correctness for this task. To address these we propose a new dataset, PubMed Tables One Million (PubTables-1M), and a new class of metric, grid table similarity (GriTS). PubTables-1M is nearly twice as large as the previous largest comparable dataset, can be used for models across multiple architectures and modalities, and addresses issues such as ambiguity and lack of consistency in the annotations. We apply DETR [1] to table extraction for the first time and show that object detection models trained on PubTables-1M produce excellent results out-of-the-box for all three tasks of detection, structure recognition, and functional analysis. We describe the dataset in detail to enable others to build on our work and combine this data with other datasets for these and related tasks. It is our hope that PubTables-1M and the proposed metrics can further progress in this area by creating a benchmark suitable for training and evaluating a wide variety of models for table extraction. Data and code will be released at https: //github.com/microsoft/table-transformer.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2745998",
                        "name": "B. Smock"
                    },
                    {
                        "authorId": "2130464272",
                        "name": "Rohith Pesala"
                    },
                    {
                        "authorId": "2394344",
                        "name": "Robin Abraham"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Recently, there has been a shift in the research literature from traditional rule-based methods [7\u20139] for TE to data-driven methods based on deep learning (DL) [2, 10, 11]."
            ],
            "citingPaper": {
                "paperId": "2773c7621402cf014b9871796f0e3ba788a95f60",
                "externalIds": {
                    "CorpusId": 238634624
                },
                "corpusId": 238634624,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2773c7621402cf014b9871796f0e3ba788a95f60",
                "title": "PubTables-1M: Towards a universal dataset and metrics for training and evaluating table extraction models",
                "abstract": "Recently, interest has grown in applying machine learning to the problem of table structure inference and extraction from unstructured documents. However, progress in this area has been challenging both to make and to measure, due to several issues that arise in training and evaluating models from labeled data. This includes challenges as fundamental as the lack of a single definitive ground truth output for each input sample and the lack of an ideal metric for measuring partial correctness for this task. To address these issues we propose a new dataset, PubMed Tables One Million (PubTables-1M), and a new class of metric, grid table similarity (GriTS). PubTables-1M is nearly twice as large as the previous largest comparable dataset, contains highly-detailed structure annotations, and can be used for models across multiple architectures and modalities. Further, it addresses issues such as ambiguity and lack of consistency in the annotations via a novel canonicalization and quality control procedure. We apply DETR [1] to table extraction for the first time and show that object detection models trained on PubTables-1M produce excellent results out-of-the-box for all three tasks of detection, structure recognition, and functional analysis. It is our hope that PubTables-1M and GriTS can further progress in this area by creating data and metrics suitable for training and evaluating a wide variety of models for table extraction. Data and code will be released at https://github.com/microsoft/table-transformer.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2745998",
                        "name": "B. Smock"
                    },
                    {
                        "authorId": "2130464272",
                        "name": "Rohith Pesala"
                    },
                    {
                        "authorId": "2394344",
                        "name": "Robin Abraham"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "contexts": [
                "Recently, there has been a shift in the research literature from traditional rule-based methods [7\u20139] 34 for table extraction to data-driven methods based on deep learning (DL) [2, 10, 11]."
            ],
            "citingPaper": {
                "paperId": "4f723c06164b3eacfb6787e9cd732e04a31e5a93",
                "externalIds": {
                    "CorpusId": 259311838
                },
                "corpusId": 259311838,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4f723c06164b3eacfb6787e9cd732e04a31e5a93",
                "title": "Towards a universal dataset and metrics for training and evaluating table extraction models",
                "abstract": "Recently, interest has grown in applying machine learning approaches to the 1 problem of table structure inference and extraction from unstructured documents. 2 However, progress in this area has been challenging not only to make but to 3 measure, due to several issues that arise in both training and evaluating such 4 systems from labeled data. This includes challenges as fundamental as the lack of 5 a single definitive ground truth output for a given input sample and the lack of an 6 ideal metric for measuring partial correctness for this task. To address these we 7 propose a new dataset, PubMed Tables One Million (PubTables1M), and a new 8 class of metric, grid table similarity (GriTS). PubTables1M is nearly twice as large 9 as the current largest comparable dataset, can be used for models across multiple 10 architectures and modalities, and addresses issues such as ambiguity and lack of 11 consistency in the annotations. We apply DETR [1] to table extraction for the first 12 time and show that object detection models trained on images and bounding boxes 13 derived from this data produce excellent results out-of-the-box for all three tasks of 14 detection, structure recognition, and functional analysis. In addition to releasing 15 the data, we describe the dataset creation process in detail to enable others to build 16 on our work and to ensure forward and backward compatibility of this data for 17 combining it with other datasets created for these tasks. It is our hope that this data 18 and the proposed metrics can further progress in this area by serving as a single 19 source of data for training and evaluation of a wide variety of models for table 20 extraction. 21",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51981031",
                        "name": "Microsoft"
                    },
                    {
                        "authorId": "2063070799",
                        "name": "Redmond"
                    }
                ]
            }
        }
    ]
}