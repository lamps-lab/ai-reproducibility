{
    "offset": 0,
    "data": [
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "3669f1328180968e0341cb1007cddf1f2e9393d6",
                "externalIds": {
                    "DOI": "10.1016/j.engappai.2023.106920",
                    "CorpusId": 260879639
                },
                "corpusId": 260879639,
                "publicationVenue": {
                    "id": "1a24ea21-4c37-41d8-9e76-ab802d4afb3e",
                    "name": "Engineering applications of artificial intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "Eng appl artif intell",
                        "Eng Appl Artif Intell",
                        "Engineering Applications of Artificial Intelligence"
                    ],
                    "issn": "0952-1976",
                    "url": "http://www.sciencedirect.com/science/journal/09521976"
                },
                "url": "https://www.semanticscholar.org/paper/3669f1328180968e0341cb1007cddf1f2e9393d6",
                "title": "PolarGAN: Creating realistic Arctic sea ice concentration images with user-defined geometric preferences",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1399927979",
                        "name": "Mingyu Kim"
                    },
                    {
                        "authorId": "71710414",
                        "name": "Jaekyeong Lee"
                    },
                    {
                        "authorId": "2214930103",
                        "name": "Leechan Choi"
                    },
                    {
                        "authorId": "5501792",
                        "name": "Minjoo Choi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "67783e818f585b8c6332d2c9def730b23eec4e55",
                "externalIds": {
                    "ArXiv": "2310.00936",
                    "CorpusId": 263605866
                },
                "corpusId": 263605866,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/67783e818f585b8c6332d2c9def730b23eec4e55",
                "title": "Trained Latent Space Navigation to Prevent Lack of Photorealism in Generated Images on Style-based Models",
                "abstract": "Recent studies on StyleGAN variants show promising performances for various generation tasks. In these models, latent codes have traditionally been manipulated and searched for the desired images. However, this approach sometimes suffers from a lack of photorealism in generated images due to a lack of knowledge about the geometry of the trained latent space. In this paper, we show a simple unsupervised method that provides well-trained local latent subspace, enabling latent code navigation while preserving the photorealism of the generated images. Specifically, the method identifies densely mapped latent spaces and restricts latent manipulations within the local latent subspace. Experimental results demonstrate that images generated within the local latent subspace maintain photorealism even when the latent codes are significantly and repeatedly manipulated. Moreover, experiments show that the method can be applied to latent code optimization for various types of style-based models. Our empirical evidence of the method will benefit applications in style-based models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2075435961",
                        "name": "Takumi Harada"
                    },
                    {
                        "authorId": "2253398768",
                        "name": "Kazuyuki Aihara"
                    },
                    {
                        "authorId": "2253400230",
                        "name": "Hiroyuki Sakai"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Surpassing GANs in image generation quality [41; 100] and exhibiting good mode coverage, they are becoming a popular choice for high-fidelity image generation and restoration tasks.",
                "Different from the Medical Image Analysis area, GAN-based GDA networks are still the main methods in Agricultural Image Analysis as even the emergence of GANs in this area can be dated to recent years[211].",
                "For a visual perspective on GAN\u2019s intricate structure, one may refer to the top section of Figure 2.",
                "Takezaki, Kishida et al.[254] get the classification accuracy for heart sound improved by 1% with Synthetic Spectrogram-based GANs(SSG).",
                "Unlike conventional approaches, GANs are designed to generate samples directly from the desired data distribution, bypassing the need to explicitly model the underlying probability density function.",
                "Nevertheless, VAEs tend to generate samples of lower visual quality compared to GANs.",
                "\u2022 Generative Adversarial Networks: GANs are known for rapidly producing high-quality samples.",
                "Lu, Chen et al. reviewed the application of GANs in agriculture in September 2022.",
                "Salvador, Javier et al. apply evolutionary conditional GANs to generate grape berry cluster images.",
                "Furthermore, [107] tapped into the potential of GANs to create synthetic data augmentations for medical images, exemplifying the versatility of generative models in diverse imaging contexts.",
                "Diffusion models achieved the best trade-off between sample fidelity and diversity and obtained the highest Fr\u00e9chet Inception Distance, compared to GANs[232].",
                "However, the types of GANs used are similar and the tasks are narrow, though it seems that GANs have achieved success in many precise tasks.",
                "GANs generate high-quality samples across the use of discriminators.",
                "Esteban et al. adopted recurrent conditional GANs for the generation of realvalued medical time series data, underscoring the significance of generative models in healthcare applications [106].",
                "From the conventional VAEs[16; 17; 18; 19; 20; 21] and GANs[22; 23; 24; 25; 26; 27; 28; 29; 30] to the burgeoning GPT-based[31; 32; 33; 34; 35; 36; 37; 38; 39; 40] and diffusionbased innovations[41; 42; 43; 44; 45; 46; 47], we elucidate their mechanisms, pros, cons, and use-case scenarios."
            ],
            "isInfluential": true,
            "intents": [],
            "citingPaper": {
                "paperId": "2a0b45c54486d38907b60debd574ad58edcb8dbd",
                "externalIds": {
                    "ArXiv": "2310.00277",
                    "CorpusId": 263334452
                },
                "corpusId": 263334452,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2a0b45c54486d38907b60debd574ad58edcb8dbd",
                "title": "A Unified Framework for Generative Data Augmentation: A Comprehensive Survey",
                "abstract": "Generative data augmentation (GDA) has emerged as a promising technique to alleviate data scarcity in machine learning applications. This thesis presents a comprehensive survey and unified framework of the GDA landscape. We first provide an overview of GDA, discussing its motivation, taxonomy, and key distinctions from synthetic data generation. We then systematically analyze the critical aspects of GDA - selection of generative models, techniques to utilize them, data selection methodologies, validation approaches, and diverse applications. Our proposed unified framework categorizes the extensive GDA literature, revealing gaps such as the lack of universal benchmarks. The thesis summarises promising research directions, including , effective data selection, theoretical development for large-scale models' application in GDA and establishing a benchmark for GDA. By laying a structured foundation, this thesis aims to nurture more cohesive development and accelerate progress in the vital arena of generative data augmentation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144864392",
                        "name": "Yunhao Chen"
                    },
                    {
                        "authorId": "2110077671",
                        "name": "Zihui Yan"
                    },
                    {
                        "authorId": "2190158402",
                        "name": "Yunjie Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While GANSpace was shown to extract interpretable controls for image generation, similar PCA-based techniques are also used in the audio domain to identify the most informative parts of the latent space, without necessarily providing interpretability [10].",
                "In other words, GANSpace enabled to reduce dimensionality of our trained WaveGAN from a 10-to a three-dimensional latent space, while preserving 75% of its learned representation. that continuously changing one latent parameter triggers a continuous morphing between generated breathing waveforms.",
                "GANSpace enables to reduce dimensionality of a latent space by applying principal component analysis (PCA) at one given network layer of a GAN, then using the first few components as latent parameters for generation.",
                "Technically speaking, we implemented and applied the GANSpace algorithm at each of the five layers of our trained WaveGAN.",
                "The first and fourth authors first searched to reduce dimensionality of our deep generative model by implementing GANSpace [27].",
                "After training, dimensionality of latent spaces can be reduced using generic algorithms such as principal component analysis [10, 27].",
                "On the left panel, we put three sliders which we respectively connected to the three latent parameters computed with GANSpace."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "8d9572eb5597cef32af417f92d411dd49ebff0e1",
                "externalIds": {
                    "DOI": "10.1145/3610099",
                    "CorpusId": 263622733
                },
                "corpusId": 263622733,
                "publicationVenue": {
                    "id": "425553d6-e479-478a-8dd7-ae59d3f32b72",
                    "name": "Proceedings of the ACM on Human-Computer Interaction",
                    "alternate_names": [
                        "Proc ACM Human-computer Interact"
                    ],
                    "issn": "2573-0142",
                    "url": "https://dl.acm.org/citation.cfm?id=3120954",
                    "alternate_urls": [
                        "https://dl.acm.org/citation.cfm?id=J1598&picked=prox"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8d9572eb5597cef32af417f92d411dd49ebff0e1",
                "title": "Probing Respiratory Care With Generative Deep Learning",
                "abstract": "This paper combines design, machine learning and social computing to explore generative deep learning as both tool and probe for respiratory care. We first present GANspire, a deep learning tool that generates fine-grained breathing waveforms, which we crafted in collaboration with one respiratory physician, attending to joint materialities of human breathing data and deep generative models. We then relate a probe, produced with breathing waveforms generated with GANspire, and led with a group of ten respiratory care experts, responding to its material attributes. Qualitative annotations showed that respiratory care experts interpreted both realistic and ambiguous attributes of breathing waveforms generated with GANspire, according to subjective aspects of physiology, activity and emotion. Semi-structured interviews also revealed experts' broader perceptions, expectations and ethical concerns on AI technology, based on their clinical practice of respiratory care, and reflexive analysis of GANspire. These findings suggest design implications for technological aids in respiratory care, and show how ambiguity of deep generative models can be leveraged as a resource for qualitative inquiry, enabling socio-material research with generative deep learning. Our paper contributes to the CSCW community by broadening how generative deep learning may be approached not only as a tool to design human-computer interactions, but also as a probe to provoke open conversations with communities of practice about their current and speculative uses of AI technology.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "21196663",
                        "name": "Hugo Scurto"
                    },
                    {
                        "authorId": "2244332012",
                        "name": "T. Similowski"
                    },
                    {
                        "authorId": "2253693327",
                        "name": "Samuel Bianchini"
                    },
                    {
                        "authorId": "2046048",
                        "name": "Baptiste Caramiaux"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0e35b17286baccb683489659269bd04ee16d8aa2",
                "externalIds": {
                    "ArXiv": "2309.15381",
                    "CorpusId": 263151643
                },
                "corpusId": 263151643,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0e35b17286baccb683489659269bd04ee16d8aa2",
                "title": "Subjective Face Transform using Human First Impressions",
                "abstract": "Humans tend to form quick subjective first impressions of non-physical attributes when seeing someone's face, such as perceived trustworthiness or attractiveness. To understand what variations in a face lead to different subjective impressions, this work uses generative models to find semantically meaningful edits to a face image that change perceived attributes. Unlike prior work that relied on statistical manipulation in feature space, our end-to-end framework considers trade-offs between preserving identity and changing perceptual attributes. It maps identity-preserving latent space directions to changes in attribute scores, enabling transformation of any input face along an attribute axis according to a target change. We train on real and synthetic faces, evaluate for in-domain and out-of-domain images using predictive models and human ratings, demonstrating the generalizability of our approach. Ultimately, such a framework can be used to understand and explain biases in subjective interpretation of faces that are not dependent on the identity.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2154626736",
                        "name": "Chaitanya Roygaga"
                    },
                    {
                        "authorId": "2248292049",
                        "name": "Joshua Krinsky"
                    },
                    {
                        "authorId": "2248870552",
                        "name": "Kai Zhang"
                    },
                    {
                        "authorId": "2247894723",
                        "name": "Kenny Kwok"
                    },
                    {
                        "authorId": "5014060",
                        "name": "Aparna Bharati"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "cda3d73a5ee542d9344c9f436a519405fd7b2968",
                "externalIds": {
                    "ArXiv": "2309.14883",
                    "CorpusId": 262824031
                },
                "corpusId": 262824031,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cda3d73a5ee542d9344c9f436a519405fd7b2968",
                "title": "Locality-preserving Directions for Interpreting the Latent Space of Satellite Image GANs",
                "abstract": "We present a locality-aware method for interpreting the latent space of wavelet-based Generative Adversarial Networks (GANs), that can well capture the large spatial and spectral variability that is characteristic to satellite imagery. By focusing on preserving locality, the proposed method is able to decompose the weight-space of pre-trained GANs and recover interpretable directions that correspond to high-level semantic concepts (such as urbanization, structure density, flora presence) - that can subsequently be used for guided synthesis of satellite imagery. In contrast to typically used approaches that focus on capturing the variability of the weight-space in a reduced dimensionality space (i.e., based on Principal Component Analysis, PCA), we show that preserving locality leads to vectors with different angles, that are more robust to artifacts and can better preserve class information. Via a set of quantitative and qualitative examples, we further show that the proposed approach can outperform both baseline geometric augmentations, as well as global, PCA-based approaches for data synthesis in the context of data augmentation for satellite scene classification.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2246904880",
                        "name": "Georgia Kourmouli"
                    },
                    {
                        "authorId": "91073385",
                        "name": "Nikos Kostagiolas"
                    },
                    {
                        "authorId": "1780393",
                        "name": "Yannis Panagakis"
                    },
                    {
                        "authorId": "1752913",
                        "name": "M. Nicolaou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following that, a large number of works [6], [8], [26], [27], [28], [29], [30], [31], [32] have shown that the latent spaces of GANs contain rich semantic knowledge of image attributes and can be used to edit images via arithmetic of attribute vectors in latent spaces.",
                "More recently, the authors of GANSpace [31] conducted unsupervised Principal Component Analysis (PCA) on the latent space, and then applied principal directions to achieve interpretable control over the output images."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "8196ebdc4996409c408f821689cf7bdc50e9178b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-13956",
                    "ArXiv": "2309.13956",
                    "DOI": "10.48550/arXiv.2309.13956",
                    "CorpusId": 262465175
                },
                "corpusId": 262465175,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8196ebdc4996409c408f821689cf7bdc50e9178b",
                "title": "In-Domain GAN Inversion for Faithful Reconstruction and Editability",
                "abstract": "Generative Adversarial Networks (GANs) have significantly advanced image synthesis through mapping randomly sampled latent codes to high-fidelity synthesized images. However, applying well-trained GANs to real image editing remains challenging. A common solution is to find an approximate latent code that can adequately recover the input image to edit, which is also known as GAN inversion. To invert a GAN model, prior works typically focus on reconstructing the target image at the pixel level, yet few studies are conducted on whether the inverted result can well support manipulation at the semantic level. This work fills in this gap by proposing in-domain GAN inversion, which consists of a domain-guided encoder and a domain-regularized optimizer, to regularize the inverted code in the native latent space of the pre-trained GAN model. In this way, we manage to sufficiently reuse the knowledge learned by GANs for image reconstruction, facilitating a wide range of editing applications without any retraining. We further make comprehensive analyses on the effects of the encoder structure, the starting inversion point, as well as the inversion parameter space, and observe the trade-off between the reconstruction quality and the editing property. Such a trade-off sheds light on how a GAN model represents an image with various semantics encoded in the learned latent distribution. Code, models, and demo are available at the project page: https://genforce.github.io/idinvert/.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47054925",
                        "name": "Jiapeng Zhu"
                    },
                    {
                        "authorId": "2117687899",
                        "name": "Yujun Shen"
                    },
                    {
                        "authorId": "121983635",
                        "name": "Yinghao Xu"
                    },
                    {
                        "authorId": "1678783",
                        "name": "Deli Zhao"
                    },
                    {
                        "authorId": "143832240",
                        "name": "Qifeng Chen"
                    },
                    {
                        "authorId": "145291669",
                        "name": "Bolei Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "d11fa95a5992d452b44764f521bfe6559de404fe",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-14267",
                    "ArXiv": "2309.14267",
                    "DOI": "10.48550/arXiv.2309.14267",
                    "CorpusId": 262826035
                },
                "corpusId": 262826035,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d11fa95a5992d452b44764f521bfe6559de404fe",
                "title": "Identity-preserving Editing of Multiple Facial Attributes by Learning Global Edit Directions and Local Adjustments",
                "abstract": "Semantic facial attribute editing using pre-trained Generative Adversarial Networks (GANs) has attracted a great deal of attention and effort from researchers in recent years. Due to the high quality of face images generated by StyleGANs, much work has focused on the StyleGANs' latent space and the proposed methods for facial image editing. Although these methods have achieved satisfying results for manipulating user-intended attributes, they have not fulfilled the goal of preserving the identity, which is an important challenge. We present ID-Style, a new architecture capable of addressing the problem of identity loss during attribute manipulation. The key components of ID-Style include Learnable Global Direction (LGD), which finds a shared and semi-sparse direction for each attribute, and an Instance-Aware Intensity Predictor (IAIP) network, which finetunes the global direction according to the input instance. Furthermore, we introduce two losses during training to enforce the LGD to find semi-sparse semantic directions, which along with the IAIP, preserve the identity of the input instance. Despite reducing the size of the network by roughly 95% as compared to similar state-of-the-art works, it outperforms baselines by 10% and 7% in Identity preserving metric (FRS) and average accuracy of manipulation (mACC), respectively.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2166047162",
                        "name": "Najmeh Mohammadbagheri"
                    },
                    {
                        "authorId": "2246883917",
                        "name": "Fardin Ayar"
                    },
                    {
                        "authorId": "1780566",
                        "name": "A. Nickabadi"
                    },
                    {
                        "authorId": "2246883824",
                        "name": "Reza Safabakhsh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following infoGAN, many attempts have been made to facilitate the discovery of semantically meaningful traversal directions through regularization [33, 42, 89, 34, 100, 66, 77, 90, 98, 84, 99, 78, 62]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "ae52090320b461b7e656904c90a78ae76fbc4688",
                "externalIds": {
                    "ArXiv": "2309.13167",
                    "DBLP": "journals/corr/abs-2309-13167",
                    "DOI": "10.48550/arXiv.2309.13167",
                    "CorpusId": 262464471
                },
                "corpusId": 262464471,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ae52090320b461b7e656904c90a78ae76fbc4688",
                "title": "Flow Factorized Representation Learning",
                "abstract": "A prominent goal of representation learning research is to achieve representations which are factorized in a useful manner with respect to the ground truth factors of variation. The fields of disentangled and equivariant representation learning have approached this ideal from a range of complimentary perspectives; however, to date, most approaches have proven to either be ill-specified or insufficiently flexible to effectively separate all realistic factors of interest in a learned latent space. In this work, we propose an alternative viewpoint on such structured representation learning which we call Flow Factorized Representation Learning, and demonstrate it to learn both more efficient and more usefully structured representations than existing frameworks. Specifically, we introduce a generative model which specifies a distinct set of latent probability paths that define different input transformations. Each latent flow is generated by the gradient field of a learned potential following dynamic optimal transport. Our novel setup brings new understandings to both \\textit{disentanglement} and \\textit{equivariance}. We show that our model achieves higher likelihoods on standard representation learning benchmarks while simultaneously being closer to approximately equivariant models. Furthermore, we demonstrate that the transformations learned by our model are flexibly composable and can also extrapolate to new data, implying a degree of robustness and generalizability approaching the ultimate goal of usefully factorized representation learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152601878",
                        "name": "Yue Song"
                    },
                    {
                        "authorId": "47255130",
                        "name": "Thomas Anderson Keller"
                    },
                    {
                        "authorId": "1703601",
                        "name": "N. Sebe"
                    },
                    {
                        "authorId": "2241950105",
                        "name": "Max Welling"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "7b837ffaeb90bf9a7d4002e7900725d5961301a4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-12033",
                    "ArXiv": "2309.12033",
                    "DOI": "10.48550/arXiv.2309.12033",
                    "CorpusId": 262083830
                },
                "corpusId": 262083830,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7b837ffaeb90bf9a7d4002e7900725d5961301a4",
                "title": "Face Identity-Aware Disentanglement in StyleGAN",
                "abstract": "Conditional GANs are frequently used for manipulating the attributes of face images, such as expression, hairstyle, pose, or age. Even though the state-of-the-art models successfully modify the requested attributes, they simultaneously modify other important characteristics of the image, such as a person's identity. In this paper, we focus on solving this problem by introducing PluGeN4Faces, a plugin to StyleGAN, which explicitly disentangles face attributes from a person's identity. Our key idea is to perform training on images retrieved from movie frames, where a given person appears in various poses and with different attributes. By applying a type of contrastive loss, we encourage the model to group images of the same person in similar regions of latent space. Our experiments demonstrate that the modifications of face attributes performed by PluGeN4Faces are significantly less invasive on the remaining characteristics of the image than in the existing state-of-the-art models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2243335065",
                        "name": "Adrian Suwala"
                    },
                    {
                        "authorId": "2170538720",
                        "name": "Bartosz W'ojcik"
                    },
                    {
                        "authorId": "2243334689",
                        "name": "Magdalena Proszewska"
                    },
                    {
                        "authorId": "145541197",
                        "name": "J. Tabor"
                    },
                    {
                        "authorId": "1790922",
                        "name": "P. Spurek"
                    },
                    {
                        "authorId": "48177231",
                        "name": "M. Smieja"
                    }
                ]
            }
        },
        {
            "contexts": [
                "On the other hand, other methods [10, 11, 15, 23, 42] resort to latent space manipulation [9, 34]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "42f43de1558c8fc006bc6b0d0911ea4a44ef0bd5",
                "externalIds": {
                    "ArXiv": "2309.11321",
                    "DBLP": "journals/corr/abs-2309-11321",
                    "DOI": "10.48550/arXiv.2309.11321",
                    "CorpusId": 262066278
                },
                "corpusId": 262066278,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/42f43de1558c8fc006bc6b0d0911ea4a44ef0bd5",
                "title": "Face Aging via Diffusion-based Editing",
                "abstract": "In this paper, we address the problem of face aging: generating past or future facial images by incorporating age-related changes to the given face. Previous aging methods rely solely on human facial image datasets and are thus constrained by their inherent scale and bias. This restricts their application to a limited generatable age range and the inability to handle large age gaps. We propose FADING, a novel approach to address Face Aging via DIffusion-based editiNG. We go beyond existing methods by leveraging the rich prior of large-scale language-image diffusion models. First, we specialize a pre-trained diffusion model for the task of face age editing by using an age-aware fine-tuning scheme. Next, we invert the input image to latent noise and obtain optimized null text embeddings. Finally, we perform text-guided local age editing via attention control. The quantitative and qualitative analyses demonstrate that our method outperforms existing approaches with respect to aging accuracy, attribute preservation, and aging quality.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2243950612",
                        "name": "Xiangyi Chen"
                    },
                    {
                        "authorId": "3099587",
                        "name": "St\u00e9phane Lathuili\u00e8re"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a807d1e07138020dace507cd15fb42d56e23822f",
                "externalIds": {
                    "DOI": "10.1155/2023/9351345",
                    "CorpusId": 261988894
                },
                "corpusId": 261988894,
                "publicationVenue": {
                    "id": "1b052a14-813e-4d66-8518-0be67d838692",
                    "name": "Journal of Electrical and Computer Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "J Electr Comput Eng"
                    ],
                    "issn": "2090-0147",
                    "url": "https://www.hindawi.com/journals/jece/"
                },
                "url": "https://www.semanticscholar.org/paper/a807d1e07138020dace507cd15fb42d56e23822f",
                "title": "Designing an Efficient System for Emotion Recognition Using CNN",
                "abstract": "Implementing an efficient system for emotion recognition has recently posed a challenge that has not been fully developed yet. Facial emotion recognition (FER) is an important subject matter in the fields of artificial intelligence (AI) since it exhibits a greater commercial potential. This technique is used to analyse various sentiments and reveal a person\u2019s behavior. It could be related to the mental or physiological state of mind. This paper mainly focuses on a human emotion recognition system through a detected human face. Its accuracy was improved via different data augmentation tools, early stopping, and generative adversarial networks (GANs). Compared to previous methods, experimental results show that the proposed method provides a 0.55% to 35.7% gain performance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1742529238",
                        "name": "D. Ammous"
                    },
                    {
                        "authorId": "150008517",
                        "name": "Achraf Chabbouh"
                    },
                    {
                        "authorId": "2209094359",
                        "name": "Awatef Edhib"
                    },
                    {
                        "authorId": "2056472769",
                        "name": "A. Chaari"
                    },
                    {
                        "authorId": "2112384",
                        "name": "F. Kammoun"
                    },
                    {
                        "authorId": "2242689118",
                        "name": "Nouri Masmoudi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "By properly exploiting their latent structure, prior works succeeded in image editing [33, 79, 80, 81, 82, 83, 84], semantic control [55, 56, 85], disentangled control direction discovery [86, 87, 88, 89], etc."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "448e7e873a6e520072301b7d9c1d0d9289778418",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-06380",
                    "ArXiv": "2309.06380",
                    "DOI": "10.48550/arXiv.2309.06380",
                    "CorpusId": 261697392
                },
                "corpusId": 261697392,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/448e7e873a6e520072301b7d9c1d0d9289778418",
                "title": "InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation",
                "abstract": "Diffusion models have revolutionized text-to-image generation with its exceptional quality and creativity. However, its multi-step sampling process is known to be slow, often requiring tens of inference steps to obtain satisfactory results. Previous attempts to improve its sampling speed and reduce computational costs through distillation have been unsuccessful in achieving a functional one-step model. In this paper, we explore a recent method called Rectified Flow, which, thus far, has only been applied to small datasets. The core of Rectified Flow lies in its \\emph{reflow} procedure, which straightens the trajectories of probability flows, refines the coupling between noises and images, and facilitates the distillation process with student models. We propose a novel text-conditioned pipeline to turn Stable Diffusion (SD) into an ultra-fast one-step model, in which we find reflow plays a critical role in improving the assignment between noise and images. Leveraging our new pipeline, we create, to the best of our knowledge, the first one-step diffusion-based text-to-image generator with SD-level image quality, achieving an FID (Frechet Inception Distance) of $23.3$ on MS COCO 2017-5k, surpassing the previous state-of-the-art technique, progressive distillation, by a significant margin ($37.2$ $\\rightarrow$ $23.3$ in FID). By utilizing an expanded network with 1.7B parameters, we further improve the FID to $22.4$. We call our one-step models \\emph{InstaFlow}. On MS COCO 2014-30k, InstaFlow yields an FID of $13.1$ in just $0.09$ second, the best in $\\leq 0.1$ second regime, outperforming the recent StyleGAN-T ($13.9$ in $0.1$ second). Notably, the training of InstaFlow only costs 199 A100 GPU days. Project page:~\\url{https://github.com/gnobitab/InstaFlow}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "46521757",
                        "name": "Xingchao Liu"
                    },
                    {
                        "authorId": "2239159377",
                        "name": "Xiwen Zhang"
                    },
                    {
                        "authorId": "2239252325",
                        "name": "Jianzhu Ma"
                    },
                    {
                        "authorId": "2239711135",
                        "name": "Jian Peng"
                    },
                    {
                        "authorId": "48873756",
                        "name": "Q. Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "73e2ac03c0be5cc852d859d2f5517abd4aa62c27",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-05314",
                    "ArXiv": "2309.05314",
                    "DOI": "10.48550/arXiv.2309.05314",
                    "CorpusId": 261682386
                },
                "corpusId": 261682386,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/73e2ac03c0be5cc852d859d2f5517abd4aa62c27",
                "title": "Semantic Latent Decomposition with Normalizing Flows for Face Editing",
                "abstract": "Navigating in the latent space of StyleGAN has shown effectiveness for face editing. However, the resulting methods usually encounter challenges in complicated navigation due to the entanglement among different attributes in the latent space. To address this issue, this paper proposes a novel framework, termed SDFlow, with a semantic decomposition in original latent space using continuous conditional normalizing flows. Specifically, SDFlow decomposes the original latent code into different irrelevant variables by jointly optimizing two components: (i) a semantic encoder to estimate semantic variables from input faces and (ii) a flow-based transformation module to map the latent code into a semantic-irrelevant variable in Gaussian distribution, conditioned on the learned semantic variables. To eliminate the entanglement between variables, we employ a disentangled learning strategy under a mutual information framework, thereby providing precise manipulation controls. Experimental results demonstrate that SDFlow outperforms existing state-of-the-art face editing methods both qualitatively and quantitatively. The source code is made available at https://github.com/phil329/SDFlow.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2239158051",
                        "name": "Binglei Li"
                    },
                    {
                        "authorId": "2158554903",
                        "name": "Zhizhong Huang"
                    },
                    {
                        "authorId": "3449207",
                        "name": "Hongming Shan"
                    },
                    {
                        "authorId": "2144127198",
                        "name": "Junping Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a7cc93456eba6437be5419161f37ea847ec8aa04",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-04626",
                    "ArXiv": "2309.04626",
                    "DOI": "10.48550/arXiv.2309.04626",
                    "CorpusId": 261681881
                },
                "corpusId": 261681881,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a7cc93456eba6437be5419161f37ea847ec8aa04",
                "title": "Perceptual adjustment queries and an inverted measurement paradigm for low-rank metric learning",
                "abstract": "We introduce a new type of query mechanism for collecting human feedback, called the perceptual adjustment query ( PAQ). Being both informative and cognitively lightweight, the PAQ adopts an inverted measurement scheme, and combines advantages from both cardinal and ordinal queries. We showcase the PAQ in the metric learning problem, where we collect PAQ measurements to learn an unknown Mahalanobis distance. This gives rise to a high-dimensional, low-rank matrix estimation problem to which standard matrix estimators cannot be applied. Consequently, we develop a two-stage estimator for metric learning from PAQs, and provide sample complexity guarantees for this estimator. We present numerical simulations demonstrating the performance of the estimator and its notable properties.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2064635172",
                        "name": "Austin Xu"
                    },
                    {
                        "authorId": "2072268697",
                        "name": "Andrew D. McRae"
                    },
                    {
                        "authorId": "2239057108",
                        "name": "Jingyan Wang"
                    },
                    {
                        "authorId": "2238951562",
                        "name": "Mark A. Davenport"
                    },
                    {
                        "authorId": "3043393",
                        "name": "A. Pananjady"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "578c36c2920777495ce2e69836ce897ba737bc42",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-03904",
                    "ArXiv": "2309.03904",
                    "DOI": "10.48550/arXiv.2309.03904",
                    "CorpusId": 261582468
                },
                "corpusId": 261582468,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/578c36c2920777495ce2e69836ce897ba737bc42",
                "title": "Exploring Sparse MoE in GANs for Text-conditioned Image Synthesis",
                "abstract": "Due to the difficulty in scaling up, generative adversarial networks (GANs) seem to be falling from grace on the task of text-conditioned image synthesis. Sparsely-activated mixture-of-experts (MoE) has recently been demonstrated as a valid solution to training large-scale models with limited computational resources. Inspired by such a philosophy, we present Aurora, a GAN-based text-to-image generator that employs a collection of experts to learn feature processing, together with a sparse router to help select the most suitable expert for each feature point. To faithfully decode the sampling stochasticity and the text condition to the final synthesis, our router adaptively makes its decision by taking into account the text-integrated global latent code. At 64x64 image resolution, our model trained on LAION2B-en and COYO-700M achieves 6.2 zero-shot FID on MS COCO. We release the code and checkpoints to facilitate the community for further development.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47054925",
                        "name": "Jiapeng Zhu"
                    },
                    {
                        "authorId": "49984891",
                        "name": "Ceyuan Yang"
                    },
                    {
                        "authorId": "2238208275",
                        "name": "Kecheng Zheng"
                    },
                    {
                        "authorId": "121983635",
                        "name": "Yinghao Xu"
                    },
                    {
                        "authorId": "2125701522",
                        "name": "Zifan Shi"
                    },
                    {
                        "authorId": "2117687899",
                        "name": "Yujun Shen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "1 [10] Erik H\u00e4rk\u00f6nen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris.",
                "Latent Space Manipulation GANs allow the generation of images that are controlled by semantic directions [19, 14, 17, 25, 23, 10].",
                "The identification of latent directions based on the principal component analysis (PCA) was proposed in [10].",
                "In particular, in the work [10] authors proposed estimating the subspaces that are invariant under random-walk diffusion for identification.",
                "Additional qualitative comparisons for editing We conducted additional comparative experiments of the proposed WRanGAN approach and the PTI inversion method for the StyleGAN 2 model in two domains: the FFHQ domain, with semantic directions corresponding to binary image attributes [23], and the LSUN Church domain, with the first 4 vectors obtained by PCA approach [10].",
                "Moreover, several works [19, 14, 17, 25, 23, 10] have demonstrated that GANs possess a wide range of interpretable semantics, providing the basis for image editing."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a079eebefba96342cddc9f3ffa63a5030fbf18e3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-16510",
                    "ArXiv": "2308.16510",
                    "DOI": "10.48550/arXiv.2308.16510",
                    "CorpusId": 261395054
                },
                "corpusId": 261395054,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a079eebefba96342cddc9f3ffa63a5030fbf18e3",
                "title": "Robust GAN inversion",
                "abstract": "Recent advancements in real image editing have been attributed to the exploration of Generative Adversarial Networks (GANs) latent space. However, the main challenge of this procedure is GAN inversion, which aims to map the image to the latent space accurately. Existing methods that work on extended latent space $W+$ are unable to achieve low distortion and high editability simultaneously. To address this issue, we propose an approach which works in native latent space $W$ and tunes the generator network to restore missing image details. We introduce a novel regularization strategy with learnable coefficients obtained by training randomized StyleGAN 2 model - WRanGAN. This method outperforms traditional approaches in terms of reconstruction quality and computational efficiency, achieving the lowest distortion with 4 times fewer parameters. Furthermore, we observe a slight improvement in the quality of constructing hyperplanes corresponding to binary image attributes. We demonstrate the effectiveness of our approach on two complex datasets: Flickr-Faces-HQ and LSUN Church.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1833714529",
                        "name": "Egor Sevriugov"
                    },
                    {
                        "authorId": "1738205",
                        "name": "I. Oseledets"
                    }
                ]
            }
        },
        {
            "contexts": [
                "2014) have led to a wide range of applications, including image manipulation (Voynov and Babenko 2020; Shen et al. 2020; H\u00e4rk\u00f6nen et al. 2020), domain translation (Isola et al."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "dfe501316149b7c3c6e0f5fb3245a3107fb459ea",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-00107",
                    "ArXiv": "2309.00107",
                    "DOI": "10.48550/arXiv.2309.00107",
                    "CorpusId": 261493929
                },
                "corpusId": 261493929,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/dfe501316149b7c3c6e0f5fb3245a3107fb459ea",
                "title": "Unsupervised evaluation of GAN sample quality: Introducing the TTJac Score",
                "abstract": "Evaluation metrics are essential for assessing the performance of generative models in image synthesis. However, existing metrics often involve high memory and time consumption as they compute the distance between generated samples and real data points. In our study, the new evaluation metric called the\"TTJac score\"is proposed to measure the fidelity of individual synthesized images in a data-free manner. The study first establishes a theoretical approach to directly evaluate the generated sample density. Then, a method incorporating feature extractors and discrete function approximation through tensor train is introduced to effectively assess the quality of generated samples. Furthermore, the study demonstrates that this new metric can be used to improve the fidelity-variability trade-off when applying the truncation trick. The experimental results of applying the proposed metric to StyleGAN 2 and StyleGAN 2 ADA models on FFHQ, AFHQ-Wild, LSUN-Cars, and LSUN-Horse datasets are presented. The code used in this research will be made publicly available online for the research community to access and utilize.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1833714529",
                        "name": "Egor Sevriugov"
                    },
                    {
                        "authorId": "1738205",
                        "name": "I. Oseledets"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "d7ff83cb989e5bdf7b9de766e235c37394614189",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-12696",
                    "ArXiv": "2308.12696",
                    "DOI": "10.48550/arXiv.2308.12696",
                    "CorpusId": 261100658
                },
                "corpusId": 261100658,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d7ff83cb989e5bdf7b9de766e235c37394614189",
                "title": "Disentanglement Learning via Topology",
                "abstract": "We propose TopDis (Topological Disentanglement), a method for learning disentangled representations via adding multi-scale topological loss term. Disentanglement is a crucial property of data representations substantial for the explainability and robustness of deep learning models and a step towards high-level cognition. The state-of-the-art method based on VAE minimizes the total correlation of the joint distribution of latent variables. We take a different perspective on disentanglement by analyzing topological properties of data manifolds. In particular, we optimize the topological similarity for data manifolds traversals. To the best of our knowledge, our paper is the first one to propose a differentiable topological loss for disentanglement. Our experiments have shown that the proposed topological loss improves disentanglement scores such as MIG, FactorVAE score, SAP score and DCI disentanglement score with respect to state-of-the-art results. Our method works in an unsupervised manner, permitting to apply it for problems without labeled factors of variation. Additionally, we show how to use the proposed topological loss to find disentangled directions in a trained GAN.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2148757496",
                        "name": "Nikita Balabin"
                    },
                    {
                        "authorId": "2045072148",
                        "name": "D. Voronkova"
                    },
                    {
                        "authorId": "144035647",
                        "name": "I. Trofimov"
                    },
                    {
                        "authorId": "51139941",
                        "name": "Evgeny Burnaev"
                    },
                    {
                        "authorId": "101098891",
                        "name": "S. Barannikov"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Algorithms in the image domain [9], [10], statistically",
                "In computer vision, algorithms such as [9], [10], [48], [49], [50], [51] leverage StyleGANs ability to disentangle the latent space to find directional vectors for editing semantics"
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "598047155a82602d1da10adaa7b4e8f94785fd14",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-11859",
                    "ArXiv": "2308.11859",
                    "DOI": "10.48550/arXiv.2308.11859",
                    "CorpusId": 261076498
                },
                "corpusId": 261076498,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/598047155a82602d1da10adaa7b4e8f94785fd14",
                "title": "Example-Based Framework for Perceptually Guided Audio Texture Generation",
                "abstract": "Generative models for synthesizing audio textures explicitly encode controllability by conditioning the model with labelled data. While datasets for audio textures can be easily recorded in-the-wild, semantically labeling them is expensive, time-consuming, and prone to errors due to human annotator subjectivity. Thus, to control generation, there is a need to automatically infer user-defined perceptual factors of variation in the latent space of a generative model while modelling unlabeled textures. In this paper, we propose an example-based framework to determine vectors to guide texture generation based on user-defined semantic attributes. By synthesizing a few synthetic examples to indicate the presence or absence of a semantic attribute, we can infer the guidance vectors in the latent space of a generative model to control that attribute during generation. Our results show that our method is capable of finding perceptually relevant and deterministic guidance vectors for controllable generation for both discrete as well as continuous textures. Furthermore, we demonstrate the application of this method to other tasks such as selective semantic attribute transfer.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2053521330",
                        "name": "Purnima Kamath"
                    },
                    {
                        "authorId": "3040433",
                        "name": "Chitralekha Gupta"
                    },
                    {
                        "authorId": "1736541",
                        "name": "L. Wyse"
                    },
                    {
                        "authorId": "1486464114",
                        "name": "Suranga Nanayakkara"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Exploring meaningful directions in the latent space: The methods proposed in [12, 16, 17] aim to discover directions in the latent representations such that moving on these directions yields humanly interpretable image transformations such as a change in aging, gender, hairstyle, lighting and etc."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "4f3e14e1d39ca360c76239dde618ea44500ed98a",
                "externalIds": {
                    "DOI": "10.21437/interspeech.2023-1549",
                    "CorpusId": 260917153
                },
                "corpusId": 260917153,
                "publicationVenue": {
                    "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
                    "name": "Interspeech",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Int Speech Commun Assoc",
                        "INTERSPEECH",
                        "Conference of the International Speech Communication Association"
                    ],
                    "issn": "2308-457X",
                    "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
                    "alternate_urls": [
                        "http://www.isca-speech.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4f3e14e1d39ca360c76239dde618ea44500ed98a",
                "title": "Interpretable Latent Space Using Space-Filling Curves for Phonetic Analysis in Voice Conversion",
                "abstract": "Vector quantized variational autoencoders (VQ-VAE) are well-known deep generative models, which map input data to a latent space that is used for data generation. Such latent spaces are unstructured and can thus be difficult to interpret. Some earlier approaches have introduced a structure to the latent space through supervised learning by defining data labels as latent variables. In contrast, we propose an unsupervised technique incorporating space-filling curves into vector quantization (VQ), which yields an arranged form of latent vectors such that adjacent elements in the VQ codebook refer to similar content. We applied this technique to the latent codebook vectors of a VQ-VAE, which encode the phonetic information of a speech signal in a voice conversion task. Our experiments show there is a clear arrangement in latent vectors representing speech phones, which clarifies what phone each latent vector corresponds to and facilitates other detailed interpretations of latent vectors.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51220266",
                        "name": "Mohammad Hassan Vali"
                    },
                    {
                        "authorId": "2139423309",
                        "name": "Tom B\u00e4ckstr\u00f6m"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c043b2f78d3db1fa541e65933b65485c83aba01b",
                "externalIds": {
                    "DOI": "10.21437/interspeech.2023-858",
                    "CorpusId": 260912871
                },
                "corpusId": 260912871,
                "publicationVenue": {
                    "id": "af90489e-312f-4514-bea2-bcb399cb8ece",
                    "name": "Interspeech",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Int Speech Commun Assoc",
                        "INTERSPEECH",
                        "Conference of the International Speech Communication Association"
                    ],
                    "issn": "2308-457X",
                    "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech",
                    "alternate_urls": [
                        "http://www.isca-speech.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c043b2f78d3db1fa541e65933b65485c83aba01b",
                "title": "Controllable Generation of Artificial Speaker Embeddings through Discovery of Principal Directions",
                "abstract": "Customizing voice and speaking style in a speech synthesis sys-tem with intuitive and fine-grained controls is challenging, given that little data with appropriate labels is available. Furthermore, editing an existing human\u2019s voice also comes with ethical concerns. In this paper, we propose a method to generate artificial speaker embeddings that cannot be linked to a real human while offering intuitive and fine-grained control over the voice and speaking style of the embeddings, without requiring any labels for speaker or style. The artificial and controllable embeddings can be fed to a speech synthesis system, conditioned on embed-dings of real humans during training, without sacrificing privacy during inference.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1672834397",
                        "name": "Florian Lux"
                    },
                    {
                        "authorId": "2101056075",
                        "name": "Pascal Tilli"
                    },
                    {
                        "authorId": "2141498369",
                        "name": "Sarina Meyer"
                    },
                    {
                        "authorId": "4160376",
                        "name": "Ngoc Thang Vu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e3d32d74f5835422cc3743514c8b74245484e244",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-09717",
                    "ArXiv": "2308.09717",
                    "DOI": "10.48550/arXiv.2308.09717",
                    "CorpusId": 261031186
                },
                "corpusId": 261031186,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e3d32d74f5835422cc3743514c8b74245484e244",
                "title": "Smoothness Similarity Regularization for Few-Shot GAN Adaptation",
                "abstract": "The task of few-shot GAN adaptation aims to adapt a pre-trained GAN model to a small dataset with very few training images. While existing methods perform well when the dataset for pre-training is structurally similar to the target dataset, the approaches suffer from training instabilities or memorization issues when the objects in the two domains have a very different structure. To mitigate this limitation, we propose a new smoothness similarity regularization that transfers the inherently learned smoothness of the pre-trained GAN to the few-shot target domain even if the two domains are very different. We evaluate our approach by adapting an unconditional and a class-conditional GAN to diverse few-shot target domains. Our proposed method significantly outperforms prior few-shot GAN adaptation methods in the challenging case of structurally dissimilar source-target domains, while performing on par with the state of the art for similar source-target domains.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2034002419",
                        "name": "V. Sushko"
                    },
                    {
                        "authorId": "2108683610",
                        "name": "Ruyu Wang"
                    },
                    {
                        "authorId": "145689714",
                        "name": "Juergen Gall"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANSpace [3] and InterFaceGAN [4], etc."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "6d5d692339137dd7bace757d7d465a5a65648f1b",
                "externalIds": {
                    "DOI": "10.1117/12.2685479",
                    "CorpusId": 260939387
                },
                "corpusId": 260939387,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6d5d692339137dd7bace757d7d465a5a65648f1b",
                "title": "Face attribute editing network based on style-content disentanglement and convolutional attention",
                "abstract": "Face attribute editing is a research hotspot in the field of computer vision, which aims to modify a certain attribute of a face image to generate a new face image. The current methods based on Generative Adversarial Networks (GAN) have attribute entanglement problems and the implementation process is relatively complicated. To this end, this paper proposes a face attribute editing network based on style-content disentanglement and convolutional attention. Adding convolutional attention (CAT) module to the StyleGAN generator makes the network's control of content features no longer affected by the overall style of the image, and realizes the separation of spatial content and style from coarse to fine. In addition, the hierarchical CAT modules control different levels of attribute features, and changing the input of any layer of CAT can change the corresponding attribute features. The experimental results on the CelebA-HQ dataset show that the method in this paper can achieve disentangled editing of face attributes, and the scores of various indicators are better than the existing models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2231978567",
                        "name": "Jiansheng Cui"
                    },
                    {
                        "authorId": "1706079",
                        "name": "Quansheng Dou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "where \u2212 \u2192n denotes the semantic direction acquired by arbitrary semantic editing techniques [23, 24, 5, 42, 41].",
                "It is evidenced that the latent space of a well-trained GAN is semantically organized, and shifting the latent code along with a specific direction results in the manipulation of a corresponding attribute [23, 24, 5, 19, 45, 32]."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "f790753bb7f6a13c8d3ee35b63ccc063a381f81e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-06097",
                    "ArXiv": "2308.06097",
                    "DOI": "10.48550/arXiv.2308.06097",
                    "CorpusId": 260866104
                },
                "corpusId": 260866104,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f790753bb7f6a13c8d3ee35b63ccc063a381f81e",
                "title": "RIGID: Recurrent GAN Inversion and Editing of Real Face Videos",
                "abstract": "GAN inversion is indispensable for applying the powerful editability of GAN to real images. However, existing methods invert video frames individually often leading to undesired inconsistent results over time. In this paper, we propose a unified recurrent framework, named \\textbf{R}ecurrent v\\textbf{I}deo \\textbf{G}AN \\textbf{I}nversion and e\\textbf{D}iting (RIGID), to explicitly and simultaneously enforce temporally coherent GAN inversion and facial editing of real videos. Our approach models the temporal relations between current and previous frames from three aspects. To enable a faithful real video reconstruction, we first maximize the inversion fidelity and consistency by learning a temporal compensated latent code. Second, we observe incoherent noises lie in the high-frequency domain that can be disentangled from the latent space. Third, to remove the inconsistency after attribute manipulation, we propose an \\textit{in-between frame composition constraint} such that the arbitrary frame must be a direct composite of its neighboring frames. Our unified framework learns the inherent coherence between input frames in an end-to-end manner, and therefore it is agnostic to a specific attribute and can be applied to arbitrary editing of the same video without re-training. Extensive experiments demonstrate that RIGID outperforms state-of-the-art methods qualitatively and quantitatively in both inversion and editing tasks. The deliverables can be found in \\url{https://cnnlstm.github.io/RIGID}",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2154894041",
                        "name": "Yangyang Xu"
                    },
                    {
                        "authorId": "2115300590",
                        "name": "Shengfeng He"
                    },
                    {
                        "authorId": "2198281849",
                        "name": "Kwan-Yee K. Wong"
                    },
                    {
                        "authorId": "47571885",
                        "name": "Ping Luo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There have also been many efforts to manipulate latent codes for spatial editing [2, 13, 17, 18, 22, 35, 45, 46, 48, 49, 55], such as object movement, rotation, and zooming."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "2ac97ca9eb1d73a99f3cd18a9b76d41bbc0883e6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-06027",
                    "ArXiv": "2308.06027",
                    "DOI": "10.48550/arXiv.2308.06027",
                    "CorpusId": 260865930
                },
                "corpusId": 260865930,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2ac97ca9eb1d73a99f3cd18a9b76d41bbc0883e6",
                "title": "Masked-Attention Diffusion Guidance for Spatially Controlling Text-to-Image Generation",
                "abstract": "Text-to-image synthesis has achieved high-quality results with recent advances in diffusion models. However, text input alone has high spatial ambiguity and limited user controllability. Most existing methods allow spatial control through additional visual guidance (e.g, sketches and semantic masks) but require additional training with annotated images. In this paper, we propose a method for spatially controlling text-to-image generation without further training of diffusion models. Our method is based on the insight that the cross-attention maps reflect the positional relationship between words and pixels. Our aim is to control the attention maps according to given semantic masks and text prompts. To this end, we first explore a simple approach of directly swapping the cross-attention maps with constant maps computed from the semantic regions. Moreover, we propose masked-attention guidance, which can generate images more faithful to semantic masks than the first approach. Masked-attention guidance indirectly controls attention to each word and pixel according to the semantic regions by manipulating noise images fed to diffusion models. Experiments show that our method enables more accurate spatial control than baselines qualitatively and quantitatively.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2420042",
                        "name": "Yuki Endo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Different methods have been developed, including PCA analysis to extract important latent directions [28], semantic analysis to control various attributes [29], and composing a new latent vector to control multiple attributes [30]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "ccd055ee2c775d86597f527057b3c1722d1c2b89",
                "externalIds": {
                    "ArXiv": "2308.06057",
                    "DBLP": "journals/corr/abs-2308-06057",
                    "DOI": "10.48550/arXiv.2308.06057",
                    "CorpusId": 260866033
                },
                "corpusId": 260866033,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ccd055ee2c775d86597f527057b3c1722d1c2b89",
                "title": "Head Rotation in Denoising Diffusion Models",
                "abstract": "Denoising Diffusion Models (DDM) are emerging as the cutting-edge technology in the realm of deep generative modeling, challenging the dominance of Generative Adversarial Networks. However, effectively exploring the latent space's semantics and identifying compelling trajectories for manipulating and editing important attributes of the generated samples remains challenging, primarily due to the high-dimensional nature of the latent space. In this study, we specifically concentrate on face rotation, which is known to be one of the most intricate editing operations. By leveraging a recent embedding technique for Denoising Diffusion Implicit Models (DDIM), we achieve, in many cases, noteworthy manipulations encompassing a wide rotation angle of $\\pm 30^o$, preserving the distinct characteristics of the individual. Our methodology exploits the computation of trajectories approximating clouds of latent representations of dataset samples with different yaw rotations through linear regression. Specific trajectories are obtained by restricting the analysis to subsets of data sharing significant attributes with the source image. One of these attributes is the light provenance: a byproduct of our research is a labeling of CelebA, categorizing images into three major groups based on the illumination direction: left, center, and right.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1795634",
                        "name": "A. Asperti"
                    },
                    {
                        "authorId": "52040847",
                        "name": "G. Colasuonno"
                    },
                    {
                        "authorId": "2056392274",
                        "name": "Ant\u00f3nio Guerra"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "761462208a066addb027bb9f47a47d3a11422534",
                "externalIds": {
                    "ArXiv": "2308.05441",
                    "DBLP": "journals/corr/abs-2308-05441",
                    "DOI": "10.48550/arXiv.2308.05441",
                    "CorpusId": 260775792
                },
                "corpusId": 260775792,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/761462208a066addb027bb9f47a47d3a11422534",
                "title": "Benchmarking Algorithmic Bias in Face Recognition: An Experimental Approach Using Synthetic Faces and Human Evaluation",
                "abstract": "We propose an experimental method for measuring bias in face recognition systems. Existing methods to measure bias depend on benchmark datasets that are collected in the wild and annotated for protected (e.g., race, gender) and non-protected (e.g., pose, lighting) attributes. Such observational datasets only permit correlational conclusions, e.g.,\"Algorithm A's accuracy is different on female and male faces in dataset X.\". By contrast, experimental methods manipulate attributes individually and thus permit causal conclusions, e.g.,\"Algorithm A's accuracy is affected by gender and skin color.\"Our method is based on generating synthetic faces using a neural face generator, where each attribute of interest is modified independently while leaving all other attributes constant. Human observers crucially provide the ground truth on perceptual identity similarity between synthetic image pairs. We validate our method quantitatively by evaluating race and gender biases of three research-grade face recognition models. Our synthetic pipeline reveals that for these algorithms, accuracy is lower for Black and East Asian population subgroups. Our method can also quantify how perceptual changes in attributes affect face identity distances reported by these models. Our large synthetic dataset, consisting of 48,000 synthetic face image pairs (10,200 unique synthetic faces) and 555,000 human annotations (individual attributes and pairwise identity comparisons) is available to researchers in this important area.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2111207246",
                        "name": "Hao Liang"
                    },
                    {
                        "authorId": "1690922",
                        "name": "P. Perona"
                    },
                    {
                        "authorId": "47231927",
                        "name": "Guha Balakrishnan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Many works have chosen to extend StyleGAN2 including [28, 48, 50, 52] thus allowing many possible applications including image editing with SeFa [50]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "bd3066bb0f2da5ced4be7f9d0001ff54d22930a0",
                "externalIds": {
                    "DBLP": "conf/kdd/JiangLCGZQYN23",
                    "ArXiv": "2309.08159",
                    "DOI": "10.1145/3580305.3599770",
                    "CorpusId": 260499486
                },
                "corpusId": 260499486,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/bd3066bb0f2da5ced4be7f9d0001ff54d22930a0",
                "title": "AdSEE: Investigating the Impact of Image Style Editing on Advertisement Attractiveness",
                "abstract": "Online advertisements are important elements in e-commerce sites, social media platforms, and search engines. With the increasing popularity of mobile browsing, many online ads are displayed with visual information in the form of a cover image in addition to text descriptions to grab the attention of users. Various recent studies have focused on predicting the click rates of online advertisements aware of visual features or composing optimal advertisement elements to enhance visibility. In this paper, we propose Advertisement Style Editing and Attractiveness Enhancement (AdSEE), which explores whether semantic editing to ads images can affect or alter the popularity of online advertisements. We introduce StyleGAN-based facial semantic editing and inversion to ads images and train a click rate predictor attributing GAN-based face latent representations in addition to traditional visual and textual features to click rates. Through a large collected dataset named QQ-AD, containing 20,527 online ads, we perform extensive offline tests to study how different semantic directions and their edit coefficients may impact click rates. We further design a Genetic Advertisement Editor to efficiently search for the optimal edit directions and intensity given an input ad cover image to enhance its projected click rates. Online A/B tests performed over a period of 5 days have verified the increased click-through rates of AdSEE-edited samples as compared to a control group of original ads, verifying the relation between image styles and ad popularity. We open source the code for AdSEE research at https://github.com/LiyaoJiang1998/adsee.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2227583189",
                        "name": "Liyao Jiang"
                    },
                    {
                        "authorId": "2128672010",
                        "name": "Chenglin Li"
                    },
                    {
                        "authorId": "2019329",
                        "name": "Haolan Chen"
                    },
                    {
                        "authorId": "2149398302",
                        "name": "Xiao-Rong Gao"
                    },
                    {
                        "authorId": "2113310138",
                        "name": "Xinwang Zhong"
                    },
                    {
                        "authorId": "2227496173",
                        "name": "Yang Qiu"
                    },
                    {
                        "authorId": "2227554473",
                        "name": "Shani Ye"
                    },
                    {
                        "authorId": "2218555775",
                        "name": "Di Niu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "04c43c8010df73a228df38e4d5009ab615593095",
                "externalIds": {
                    "DBLP": "journals/cg/LiuSWZ23",
                    "DOI": "10.1016/j.cag.2023.06.022",
                    "CorpusId": 259811907
                },
                "corpusId": 259811907,
                "publicationVenue": {
                    "id": "ca858314-5beb-4241-a7e4-f7748b3f2081",
                    "name": "Computers & graphics",
                    "type": "journal",
                    "alternate_names": [
                        "Computer Graphics",
                        "Comput graph",
                        "Computers & Graphics",
                        "Comput  Graph",
                        "Comput Graph",
                        "Computer graphics",
                        "Comput  graph"
                    ],
                    "issn": "0097-8493",
                    "alternate_issns": [
                        "0097-8930",
                        "1558-4569"
                    ],
                    "url": "https://www.sciencedirect.com/journal/computers-and-graphics",
                    "alternate_urls": [
                        "http://portal.acm.org/siggraph/newsletter",
                        "https://dl.acm.org/newsletter/siggraph",
                        "http://www.sciencedirect.com/science/journal/00978493"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/04c43c8010df73a228df38e4d5009ab615593095",
                "title": "High-fidelity GAN inversion by frequency domain guidance",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47185560",
                        "name": "F. Liu"
                    },
                    {
                        "authorId": "134473682",
                        "name": "Mingwen Shao"
                    },
                    {
                        "authorId": "49451350",
                        "name": "Fan Wang"
                    },
                    {
                        "authorId": "2222925195",
                        "name": "Lixu Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "653e27f11e25384bbd6146ea7bc985e80438e78f",
                "externalIds": {
                    "DOI": "10.1016/j.cag.2023.08.008",
                    "CorpusId": 260832261
                },
                "corpusId": 260832261,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/653e27f11e25384bbd6146ea7bc985e80438e78f",
                "title": "Neural 3D face rendering conditioned on 2D appearance via GAN disentanglement method",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "70202903",
                        "name": "Ruizhao Chen"
                    },
                    {
                        "authorId": "2073168604",
                        "name": "Ran Yi"
                    },
                    {
                        "authorId": "2098438",
                        "name": "Tuanfeng Y. Wang"
                    },
                    {
                        "authorId": "2149343311",
                        "name": "Lizhuang Ma"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "5e98192e28fd2c5223f83f6ba8fca2e342fca51b",
                "externalIds": {
                    "DBLP": "conf/ijcai/Li00ST23",
                    "DOI": "10.24963/ijcai.2023/128",
                    "CorpusId": 260853423
                },
                "corpusId": 260853423,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/5e98192e28fd2c5223f83f6ba8fca2e342fca51b",
                "title": "Analyzing and Combating Attribute Bias for Face Restoration",
                "abstract": "Face restoration (FR) recovers high resolution (HR) faces from low resolution (LR) faces and is challenging due to its ill-posed nature. With years of development, existing methods can produce quality HR faces with realistic details. However, we observe that key facial attributes (e.g., age and gender) of the restored faces could be dramatically different from the LR faces and call this phenomenon attribute bias, which is fatal when using FR for applications such as surveillance and security. Thus, we argue that FR should consider not only image quality as in existing works but also attribute bias. To this end, we thoroughly analyze attribute bias with extensive experiments and find that two major causes are the lack of attribute information in LR faces and bias in the training data. Moreover, we propose the DebiasFR framework to produce HR faces with high image quality and accurate facial attributes. The key design is to explicitly model the facial attributes, which also allows to adjust facial attributes for the output HR faces. Experiment results show that DebiasFR has comparable image quality but significantly smaller attribute bias when compared with state-of-the-art FR methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2109967987",
                        "name": "Zelin Li"
                    },
                    {
                        "authorId": "39422721",
                        "name": "Dan Zeng"
                    },
                    {
                        "authorId": "145837716",
                        "name": "Xiao Yan"
                    },
                    {
                        "authorId": "2647686",
                        "name": "Qiaomu Shen"
                    },
                    {
                        "authorId": "2084612700",
                        "name": "Bo Tang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "64b5e0381be9317ff6320991fd2a600f69ac8cbd",
                "externalIds": {
                    "DOI": "10.1016/j.segan.2023.101157",
                    "CorpusId": 261179927
                },
                "corpusId": 261179927,
                "publicationVenue": {
                    "id": "56f97015-4cce-4dca-b90a-91d69edc2f5d",
                    "name": "Sustainable Energy, Grids and Networks",
                    "type": "journal",
                    "alternate_names": [
                        "Sustain Energy Grid Netw"
                    ],
                    "issn": "2352-4677",
                    "url": "https://www.journals.elsevier.com/sustainable-energy-grids-and-networks",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/article/pii/S2352-4677(15)00039-9",
                        "http://www.sciencedirect.com/science/journal/23524677/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/64b5e0381be9317ff6320991fd2a600f69ac8cbd",
                "title": "A scenario framework for electricity grid using Generative Adversarial Networks",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51067822",
                        "name": "Bilgi Yilmaz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Numerous works have explored this disentanglement latent space to identify semantic directions in a supervised [4, 14, 36], unsupervised [15, 37, 41] or self-supervised [19, 32] manner."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "da83e745187902530c2ac3364fdbff2179b2a206",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-16151",
                    "ArXiv": "2307.16151",
                    "DOI": "10.48550/arXiv.2307.16151",
                    "CorpusId": 260333994
                },
                "corpusId": 260333994,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/da83e745187902530c2ac3364fdbff2179b2a206",
                "title": "StylePrompter: All Styles Need Is Attention",
                "abstract": "GAN inversion aims at inverting given images into corresponding latent codes for Generative Adversarial Networks (GANs), especially StyleGAN where exists a disentangled latent space that allows attribute-based image manipulation at latent level. As most inversion methods build upon Convolutional Neural Networks (CNNs), we transfer a hierarchical vision Transformer backbone innovatively to predict $\\mathcal{W^+}$ latent codes at token level. We further apply a Style-driven Multi-scale Adaptive Refinement Transformer (SMART) in $\\mathcal{F}$ space to refine the intermediate style features of the generator. By treating style features as queries to retrieve lost identity information from the encoder's feature maps, SMART can not only produce high-quality inverted images but also surprisingly adapt to editing tasks. We then prove that StylePrompter lies in a more disentangled $\\mathcal{W^+}$ and show the controllability of SMART. Finally, quantitative and qualitative experiments demonstrate that StylePrompter can achieve desirable performance in balancing reconstruction quality and editability, and is\"smart\"enough to fit into most edits, outperforming other $\\mathcal{F}$-involved inversion methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "12791174",
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "authorId": "2088908371",
                        "name": "Pan Gao"
                    },
                    {
                        "authorId": "118065745",
                        "name": "A. Smolic"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0519b69cf8fb73e3bec223a02326f341160400ef",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-15033",
                    "ArXiv": "2307.15033",
                    "DOI": "10.48550/arXiv.2307.15033",
                    "CorpusId": 260203133
                },
                "corpusId": 260203133,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0519b69cf8fb73e3bec223a02326f341160400ef",
                "title": "Diverse Inpainting and Editing with GAN Inversion",
                "abstract": "Recent inversion methods have shown that real images can be inverted into StyleGAN's latent space and numerous edits can be achieved on those images thanks to the semantically rich feature representations of well-trained GAN models. However, extensive research has also shown that image inversion is challenging due to the trade-off between high-fidelity reconstruction and editability. In this paper, we tackle an even more difficult task, inverting erased images into GAN's latent space for realistic inpaintings and editings. Furthermore, by augmenting inverted latent codes with different latent samples, we achieve diverse inpaintings. Specifically, we propose to learn an encoder and mixing network to combine encoded features from erased images with StyleGAN's mapped features from random samples. To encourage the mixing network to utilize both inputs, we train the networks with generated data via a novel set-up. We also utilize higher-rate features to prevent color inconsistencies between the inpainted and unerased parts. We run extensive experiments and compare our method with state-of-the-art inversion and inpainting methods. Qualitative metrics and visual comparisons show significant improvements.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2211733091",
                        "name": "Ahmet Burak Yildirim"
                    },
                    {
                        "authorId": "2199012510",
                        "name": "Hamza Pehlivan"
                    },
                    {
                        "authorId": "2167316936",
                        "name": "Bahri Batuhan Bilecen"
                    },
                    {
                        "authorId": "2130620",
                        "name": "A. Dundar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANSpace [24] uses principal component analysis (PCA) in the feature space to identify the important latent directions, which represent the interpretable variations."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3efd3c69cfd6d004bb4537ed72a873ef91476d4b",
                "externalIds": {
                    "ArXiv": "2307.14051",
                    "DBLP": "journals/corr/abs-2307-14051",
                    "DOI": "10.48550/arXiv.2307.14051",
                    "CorpusId": 260164774
                },
                "corpusId": 260164774,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3efd3c69cfd6d004bb4537ed72a873ef91476d4b",
                "title": "3D Semantic Subspace Traverser: Empowering 3D Generative Model with Shape Editing Capability",
                "abstract": "Shape generation is the practice of producing 3D shapes as various representations for 3D content creation. Previous studies on 3D shape generation have focused on shape quality and structure, without or less considering the importance of semantic information. Consequently, such generative models often fail to preserve the semantic consistency of shape structure or enable manipulation of the semantic attributes of shapes during generation. In this paper, we proposed a novel semantic generative model named 3D Semantic Subspace Traverser that utilizes semantic attributes for category-specific 3D shape generation and editing. Our method utilizes implicit functions as the 3D shape representation and combines a novel latent-space GAN with a linear subspace model to discover semantic dimensions in the local latent space of 3D shapes. Each dimension of the subspace corresponds to a particular semantic attribute, and we can edit the attributes of generated shapes by traversing the coefficients of those dimensions. Experimental results demonstrate that our method can produce plausible shapes with complex structures and enable the editing of semantic attributes. The code and trained models are available at https://github.com/TrepangCat/3D_Semantic_Subspace_Traverser",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108695254",
                        "name": "Ruowei Wang"
                    },
                    {
                        "authorId": "2146401655",
                        "name": "Yu Liu"
                    },
                    {
                        "authorId": "2224888994",
                        "name": "Pei Su"
                    },
                    {
                        "authorId": "2225202660",
                        "name": "Jianwei Zhang"
                    },
                    {
                        "authorId": "7345195",
                        "name": "Qijun Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "843733640bd0b06b2d742d70288db0bbf29d8fc4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-12868",
                    "ArXiv": "2307.12868",
                    "DOI": "10.48550/arXiv.2307.12868",
                    "CorpusId": 260125707
                },
                "corpusId": 260125707,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/843733640bd0b06b2d742d70288db0bbf29d8fc4",
                "title": "Understanding the Latent Space of Diffusion Models through the Lens of Riemannian Geometry",
                "abstract": "Despite the success of diffusion models (DMs), we still lack a thorough understanding of their latent space. To understand the latent space $\\mathbf{x}_t \\in \\mathcal{X}$, we analyze them from a geometrical perspective. Specifically, we utilize the pullback metric to find the local latent basis in $\\mathcal{X}$ and their corresponding local tangent basis in $\\mathcal{H}$, the intermediate feature maps of DMs. The discovered latent basis enables unsupervised image editing capability through latent space traversal. We investigate the discovered structure from two perspectives. First, we examine how geometric structure evolves over diffusion timesteps. Through analysis, we show that 1) the model focuses on low-frequency components early in the generative process and attunes to high-frequency details later; 2) At early timesteps, different samples share similar tangent spaces; and 3) The simpler datasets that DMs trained on, the more consistent the tangent space for each timestep. Second, we investigate how the geometric structure changes based on text conditioning in Stable Diffusion. The results show that 1) similar prompts yield comparable tangent spaces; and 2) the model depends less on text conditions in later timesteps. To the best of our knowledge, this paper is the first to present image editing through $\\mathbf{x}$-space traversal and provide thorough analyses of the latent structure of DMs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2209948157",
                        "name": "Yong-Hyun Park"
                    },
                    {
                        "authorId": "2182293854",
                        "name": "Mingi Kwon"
                    },
                    {
                        "authorId": "40007189",
                        "name": "J. Choi"
                    },
                    {
                        "authorId": "1832935",
                        "name": "Junghyo Jo"
                    },
                    {
                        "authorId": "2847986",
                        "name": "Youngjung Uh"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "d869434e3e7d462937a916e4f1784bed9ea92c8a",
                "externalIds": {
                    "DBLP": "conf/siggraph/MensahKALL23",
                    "DOI": "10.1145/3588432.3591563",
                    "CorpusId": 259339621
                },
                "corpusId": 259339621,
                "publicationVenue": {
                    "id": "cf6b5e76-9274-46e6-a2dd-7c190ec2ec5f",
                    "name": "International Conference on Computer Graphics and Interactive Techniques",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Graph Interact Tech",
                        "SIGGRAPH"
                    ],
                    "url": "http://www.siggraph.org/"
                },
                "url": "https://www.semanticscholar.org/paper/d869434e3e7d462937a916e4f1784bed9ea92c8a",
                "title": "A Hybrid Generator Architecture for Controllable Face Synthesis",
                "abstract": "Modern data-driven image generation models often surpass traditional graphics techniques in quality. However, while traditional modeling and animation tools allow precise control over the image generation process in terms of interpretable quantities \u2014 e.g., shapes and reflectances \u2014 endowing learned models with such controls is generally difficult. In the context of human faces, we seek a data-driven generator architecture that simultaneously retains the photorealistic quality of modern generative adversarial networks (GAN) and allows explicit, disentangled controls over head shapes, expressions, identity, background, and illumination. While our high-level goal is shared by a large body of previous work, we approach the problem with a different philosophy: We treat the problem as an unconditional synthesis task, and engineer interpretable inductive biases into the model that make it easy for the desired behavior to emerge. Concretely, our generator is a combination of learned neural networks and fixed-function blocks, such as a 3D morphable head model and texture-mapping rasterizer, and we leave it up to the training process to figure out how they should be used together. This greatly simplifies the training problem by removing the need for labeled training data; we learn the distributions of the independent variables that drive the model instead of requiring that their values are known for each training image. Furthermore, we need no contrastive or imitation learning for correct behavior. We show that our design successfully encourages the generative model to make use of the internal, interpretable representations in a semantically meaningful manner. This allows sampling of different aspects of the image independently, as well as precise control of the results by manipulating the internal state of the interpretable blocks within the generator. This enables, for instance, facial animation using traditional animation tools.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "150971847",
                        "name": "Dann Mensah"
                    },
                    {
                        "authorId": "2119401410",
                        "name": "N. Kim"
                    },
                    {
                        "authorId": "1907688",
                        "name": "M. Aittala"
                    },
                    {
                        "authorId": "36436218",
                        "name": "S. Laine"
                    },
                    {
                        "authorId": "49244945",
                        "name": "J. Lehtinen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b26fafffe8aa4e7c88294dabfa657b9c8c47e700",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-11141",
                    "ArXiv": "2307.11141",
                    "DOI": "10.48550/arXiv.2307.11141",
                    "CorpusId": 260091544
                },
                "corpusId": 260091544,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b26fafffe8aa4e7c88294dabfa657b9c8c47e700",
                "title": "Towards General Game Representations: Decomposing Games Pixels into Content and Style",
                "abstract": "On-screen game footage contains rich contextual information that players process when playing and experiencing a game. Learning pixel representations of games can benefit artificial intelligence across several downstream tasks including game-playing agents, procedural content generation, and player modelling. The generalizability of these methods, however, remains a challenge, as learned representations should ideally be shared across games with similar game mechanics. This could allow, for instance, game-playing agents trained on one game to perform well in similar games with no re-training. This paper explores how generalizable pre-trained computer vision encoders can be for such tasks, by decomposing the latent space into content embeddings and style embeddings. The goal is to minimize the domain gap between games of the same genre when it comes to game content critical for downstream tasks, and ignore differences in graphical style. We employ a pre-trained Vision Transformer encoder and a decomposition technique based on game genres to obtain separate content and style embeddings. Our findings show that the decomposed embeddings achieve style invariance across multiple games while still maintaining strong content extraction capabilities. We argue that the proposed decomposition of content and style offers better generalization capacities across game environments independently of the downstream task.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "37498103",
                        "name": "C. Trivedi"
                    },
                    {
                        "authorId": "2091878621",
                        "name": "Konstantinos Makantasis"
                    },
                    {
                        "authorId": "1713331",
                        "name": "Antonios Liapis"
                    },
                    {
                        "authorId": "1686193",
                        "name": "Georgios N. Yannakakis"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "ee5300708ad5d8e81117888afdf15b3c899c74c9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-08995",
                    "ArXiv": "2307.08995",
                    "DOI": "10.48550/arXiv.2307.08995",
                    "CorpusId": 259951497
                },
                "corpusId": 259951497,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ee5300708ad5d8e81117888afdf15b3c899c74c9",
                "title": "Revisiting Latent Space of GAN Inversion for Real Image Editing",
                "abstract": "The exploration of the latent space in StyleGANs and GAN inversion exemplify impressive real-world image editing, yet the trade-off between reconstruction quality and editing quality remains an open problem. In this study, we revisit StyleGANs' hyperspherical prior $\\mathcal{Z}$ and combine it with highly capable latent spaces to build combined spaces that faithfully invert real images while maintaining the quality of edited images. More specifically, we propose $\\mathcal{F}/\\mathcal{Z}^{+}$ space consisting of two subspaces: $\\mathcal{F}$ space of an intermediate feature map of StyleGANs enabling faithful reconstruction and $\\mathcal{Z}^{+}$ space of an extended StyleGAN prior supporting high editing quality. We project the real images into the proposed space to obtain the inverted codes, by which we then move along $\\mathcal{Z}^{+}$, enabling semantic editing without sacrificing image quality. Comprehensive experiments show that $\\mathcal{Z}^{+}$ can replace the most commonly-used $\\mathcal{W}$, $\\mathcal{W}^{+}$, and $\\mathcal{S}$ spaces while preserving reconstruction quality, resulting in reduced distortion of edited images.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2036954391",
                        "name": "Kai Katsumata"
                    },
                    {
                        "authorId": "2967089",
                        "name": "Duc Minh Vo"
                    },
                    {
                        "authorId": "2127734772",
                        "name": "Bei Liu"
                    },
                    {
                        "authorId": "48731103",
                        "name": "Hideki Nakayama"
                    }
                ]
            }
        },
        {
            "contexts": [
                "2020], or in an unsupervised manner [H\u00e4rk\u00f6nen et al. 2020; Shen and Zhou 2021; Voynov and Babenko 2020].",
                "Recent works [H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020] have demonstrated that StyleGAN learns disentangled attributes, making it possible to find directions in its latent space to generate images that possess such desired attributes.",
                "Recent works [H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020] have demonstrated that StyleGAN learns disentangled attributes, making it possible to ind directions in its latent space to generate images that possess such desired attributes.",
                "Such a direction can be found by either using explicit supervision of image attribute annotations [Abdal et al. 2021; Shen et al. 2020; Wu et al. 2020], or in an unsupervised manner [H\u00e4rk\u00f6nen et al. 2020; Shen and Zhou 2021; Voynov and Babenko 2020]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "result"
            ],
            "citingPaper": {
                "paperId": "41c8a25676d6a16b6b9a25f50d279e0d70fc9a6e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-08397",
                    "ArXiv": "2307.08397",
                    "DOI": "10.1145/3610287",
                    "CorpusId": 259937486
                },
                "corpusId": 259937486,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/41c8a25676d6a16b6b9a25f50d279e0d70fc9a6e",
                "title": "CLIP-guided StyleGAN Inversion for Text-driven Real Image Editing",
                "abstract": "Researchers have recently begun exploring the use of StyleGAN-based models for real image editing. One particularly interesting application is using natural language descriptions to guide the editing process. Existing approaches for editing images using language either resort to instance-level latent code optimization or map predefined text prompts to some editing directions in the latent space. However, these approaches have inherent limitations. The former is not very efficient, while the latter often struggles to effectively handle multi-attribute changes. To address these weaknesses, we present CLIPInverter, a new text-driven image editing approach that is able to efficiently and reliably perform multi-attribute changes. The core of our method is the use of novel, lightweight text-conditioned adapter layers integrated into pretrained GAN-inversion networks. We demonstrate that by conditioning the initial inversion step on the Contrastive Language-Image Pre-training (CLIP) embedding of the target description, we are able to obtain more successful edit directions. Additionally, we use a CLIP-guided refinement step to make corrections in the resulting residual latent codes, which further improves the alignment with the text prompt. Our method outperforms competing approaches in terms of manipulation accuracy and photo-realism on various domains including human faces, cats, and birds, as shown by our qualitative and quantitative results.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "46860633",
                        "name": "A. Baykal"
                    },
                    {
                        "authorId": "2223596965",
                        "name": "Abdul Basit Annes"
                    },
                    {
                        "authorId": "39686979",
                        "name": "Duygu Ceylan"
                    },
                    {
                        "authorId": "152330322",
                        "name": "Erkut Erdem"
                    },
                    {
                        "authorId": "14364286",
                        "name": "Aykut Erdem"
                    },
                    {
                        "authorId": "2099665033",
                        "name": "D. Yurt"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, lots of methods explore disentangling the latent space to achieve image editing by moving the latent code in the identified interpretable directions [7, 3, 22, 49, 18, 58, 71, 50, 19, 43, 56, 59, 57, 32, 57, 33, 62, 23, 46, 9, 55, 53].",
                "Previous latent semantics discovery approaches [15, 58, 18, 49, 50] consider the GAN manipulation as edit(G(z))=G(z + \u03b1n) where G(\u00b7) represents the generator, z\u2208R denotes the latent code of dimension d, n\u2208R is the identified semantically meaningful direction, and \u03b1 represents the perturbation strength."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "8c92db56fdcc0cdc28ab23b163a6461644dbb8c9",
                "externalIds": {
                    "ArXiv": "2307.08012",
                    "DBLP": "journals/corr/abs-2307-08012",
                    "DOI": "10.48550/arXiv.2307.08012",
                    "CorpusId": 259937237
                },
                "corpusId": 259937237,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8c92db56fdcc0cdc28ab23b163a6461644dbb8c9",
                "title": "Householder Projector for Unsupervised Latent Semantics Discovery",
                "abstract": "Generative Adversarial Networks (GANs), especially the recent style-based generators (StyleGANs), have versatile semantics in the structured latent space. Latent semantics discovery methods emerge to move around the latent code such that only one factor varies during the traversal. Recently, an unsupervised method proposed a promising direction to directly use the eigenvectors of the projection matrix that maps latent codes to features as the interpretable directions. However, one overlooked fact is that the projection matrix is non-orthogonal and the number of eigenvectors is too large. The non-orthogonality would entangle semantic attributes in the top few eigenvectors, and the large dimensionality might result in meaningless variations among the directions even if the matrix is orthogonal. To avoid these issues, we propose Householder Projector, a flexible and general low-rank orthogonal matrix representation based on Householder transformations, to parameterize the projection matrix. The orthogonality guarantees that the eigenvectors correspond to disentangled interpretable semantics, while the low-rank property encourages that each identified direction has meaningful variations. We integrate our projector into pre-trained StyleGAN2/StyleGAN3 and evaluate the models on several benchmarks. Within only $1\\%$ of the original training steps for fine-tuning, our projector helps StyleGANs to discover more disentangled and precise semantic attributes without sacrificing image fidelity.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152601878",
                        "name": "Yue Song"
                    },
                    {
                        "authorId": "50560752",
                        "name": "Jichao Zhang"
                    },
                    {
                        "authorId": "1429806753",
                        "name": "N. Sebe"
                    },
                    {
                        "authorId": "40397893",
                        "name": "Wei Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANSpace [14] and SeFa [35] find editing directions from the principal components of the latent space.",
                "Unsupervised methods [2,14,35,40] are to discover the interpretable directions using PCA [14,35] or texts [2].",
                "Previous literature [2, 14, 34, 35, 41] usually edits the facial attribute ai by linear interpolation in the latent space with certain editing directions nai \u2208 R(512), which can be formulated as follows:",
                "In this paper, we study an adaptive nonlinear transformation rather than linear interpolation in [2,14,25,34,35,40,41], and investigate a density regularization to encourage indistribution latent transformation.",
                "nai can be learned by training a hyperplane/fully-connected layers in the latent space [34, 41], or discovered from the principal components [14, 35] and text information [2].",
                "Differently, the unsupervised methods [1,2,9,14,20,35,37] do not need the valuable annotated data.",
                "In particular, the pre-trained StyleGAN generator presents a meaningful intermediate latent space, traversing on which the faces can be semantically manipulated [2, 3, 14, 34, 35, 38, 40, 41]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b82c78c1ee5458518e61b428a97f7bfe099a99d8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-07790",
                    "ArXiv": "2307.07790",
                    "DOI": "10.48550/arXiv.2307.07790",
                    "CorpusId": 259937662
                },
                "corpusId": 259937662,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b82c78c1ee5458518e61b428a97f7bfe099a99d8",
                "title": "Adaptive Nonlinear Latent Transformation for Conditional Face Editing",
                "abstract": "Recent works for face editing usually manipulate the latent space of StyleGAN via the linear semantic directions. However, they usually suffer from the entanglement of facial attributes, need to tune the optimal editing strength, and are limited to binary attributes with strong supervision signals. This paper proposes a novel adaptive nonlinear latent transformation for disentangled and conditional face editing, termed AdaTrans. Specifically, our AdaTrans divides the manipulation process into several finer steps; i.e., the direction and size at each step are conditioned on both the facial attributes and the latent codes. In this way, AdaTrans describes an adaptive nonlinear transformation trajectory to manipulate the faces into target attributes while keeping other attributes unchanged. Then, AdaTrans leverages a predefined density model to constrain the learned trajectory in the distribution of latent codes by maximizing the likelihood of transformed latent code. Moreover, we also propose a disentangled learning strategy under a mutual information framework to eliminate the entanglement among attributes, which can further relax the need for labeled data. Consequently, AdaTrans enables a controllable face editing with the advantages of disentanglement, flexibility with non-binary attributes, and high fidelity. Extensive experimental results on various facial attributes demonstrate the qualitative and quantitative effectiveness of the proposed AdaTrans over existing state-of-the-art methods, especially in the most challenging scenarios with a large age gap and few labeled examples. The source code is available at https://github.com/Hzzone/AdaTrans.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2158554903",
                        "name": "Zhizhong Huang"
                    },
                    {
                        "authorId": "2168253235",
                        "name": "Siteng Ma"
                    },
                    {
                        "authorId": "2144127198",
                        "name": "Junping Zhang"
                    },
                    {
                        "authorId": "3449207",
                        "name": "Hongming Shan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "tions in the latent space of GANs [23], [24]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "d3b99cfbb221c6a7223ccec43fb0ef6f65f09c84",
                "externalIds": {
                    "DBLP": "journals/tip/HyunLCKH23",
                    "DOI": "10.1109/TIP.2023.3293767",
                    "CorpusId": 259857841,
                    "PubMed": "37440396"
                },
                "corpusId": 259857841,
                "publicationVenue": {
                    "id": "e117fa7f-05b7-4dd6-b64b-0a0a7c0393d8",
                    "name": "IEEE Transactions on Image Processing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Image Process"
                    ],
                    "issn": "1057-7149",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=83",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d3b99cfbb221c6a7223ccec43fb0ef6f65f09c84",
                "title": "Frequency-Based Motion Representation for Video Generative Adversarial Networks",
                "abstract": "Videos contain motions of various speeds. For example, the motions of one\u2019s head and mouth differ in terms of speed \u2014 the head being relatively stable and the mouth moving rapidly as one speaks. Despite its diverse nature, previous video GANs generate video based on a single unified motion representation without considering the aspect of speed. In this paper, we propose a frequency-based motion representation for video GANs to realize the concept of speed in video generation process. In detail, we represent motions as continuous sinusoidal signals of various frequencies by introducing a coordinate-based motion generator. We show, in that case, frequency is highly related to the speed of motion. Based on this observation, we present frequency-aware weight modulation that enables manipulation of motions within a specific range of speed, which could not be achieved with the previous techniques. Extensive experiments validate that the proposed method outperforms state-of-the-art video GANs in terms of generation quality by its capability to model various speed of motions. Furthermore, we also show that our temporally continuous representation enables to further synthesize intermediate and future frames of generated videos.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "148062416",
                        "name": "Sangeek Hyun"
                    },
                    {
                        "authorId": "2141283301",
                        "name": "Jaihyun Lew"
                    },
                    {
                        "authorId": "2223118644",
                        "name": "Jiwoo Chung"
                    },
                    {
                        "authorId": "2149341272",
                        "name": "Euiyeon Kim"
                    },
                    {
                        "authorId": "7212202",
                        "name": "Jae-Pil Heo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "One line of works find semantic controls in the generator\u2019s latent space which enable controlled synthesis and image editing pipelines [Abdal et al. 2020; H\u00e4rk\u00f6nen et al. 2020; Patashnik et al. 2021; Shen et al. 2020; Shen and Zhou 2020; Voynov and Babenko 2020]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "81152f0986d612eb602afcab393f19f67df977c9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-06307",
                    "ArXiv": "2307.06307",
                    "DOI": "10.48550/arXiv.2307.06307",
                    "CorpusId": 259837057
                },
                "corpusId": 259837057,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/81152f0986d612eb602afcab393f19f67df977c9",
                "title": "Facial Reenactment Through a Personalized Generator",
                "abstract": "In recent years, the role of image generative models in facial reenactment has been steadily increasing. Such models are usually subject-agnostic and trained on domain-wide datasets. The appearance of the reenacted individual is learned from a single image, and hence, the entire breadth of the individual's appearance is not entirely captured, leading these methods to resort to unfaithful hallucination. Thanks to recent advancements, it is now possible to train a personalized generative model tailored specifically to a given individual. In this paper, we propose a novel method for facial reenactment using a personalized generator. We train the generator using frames from a short, yet varied, self-scan video captured using a simple commodity camera. Images synthesized by the personalized generator are guaranteed to preserve identity. The premise of our work is that the task of reenactment is thus reduced to accurately mimicking head poses and expressions. To this end, we locate the desired frames in the latent space of the personalized generator using carefully designed latent optimization. Through extensive evaluation, we demonstrate state-of-the-art performance for facial reenactment. Furthermore, we show that since our reenactment takes place in a semantic latent space, it can be semantically edited and stylized in post-processing.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2007375766",
                        "name": "Ariel Elazary"
                    },
                    {
                        "authorId": "1702992975",
                        "name": "Yotam Nitzan"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANSpace [11] utilized Principal Component Analysis (PCA) applied on the feature space of pretrained GANs to create interpretable controls for image synthesis.",
                "Similar to GANSpace [11], InterFaceGAN [25] proposed a framework for semantic face editing.",
                "However, these approaches, GANSpace [11] and InterFaceGAN [25], mainly relied on human labels or attribute classifiers for visual attribute manipulation without any restriction on identity information."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "688221dbecf3273ab7c34d6c4c96cfe04c3f65d7",
                "externalIds": {
                    "ArXiv": "2307.05151",
                    "DBLP": "journals/corr/abs-2307-05151",
                    "DOI": "10.48550/arXiv.2307.05151",
                    "CorpusId": 259766529
                },
                "corpusId": 259766529,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/688221dbecf3273ab7c34d6c4c96cfe04c3f65d7",
                "title": "ExFaceGAN: Exploring Identity Directions in GAN's Learned Latent Space for Synthetic Identity Generation",
                "abstract": "Deep generative models have recently presented impressive results in generating realistic face images of random synthetic identities. To generate multiple samples of a certain synthetic identity, previous works proposed to disentangle the latent space of GANs by incorporating additional supervision or regularization, enabling the manipulation of certain attributes. Others proposed to disentangle specific factors in unconditional pretrained GANs latent spaces to control their output, which also requires supervision by attribute classifiers. Moreover, these attributes are entangled in GAN's latent space, making it difficult to manipulate them without affecting the identity information. We propose in this work a framework, ExFaceGAN, to disentangle identity information in pretrained GANs latent spaces, enabling the generation of multiple samples of any synthetic identity. Given a reference latent code of any synthetic image and latent space of pretrained GAN, our ExFaceGAN learns an identity directional boundary that disentangles the latent space into two sub-spaces, with latent codes of samples that are either identity similar or dissimilar to a reference image. By sampling from each side of the boundary, our ExFaceGAN can generate multiple samples of synthetic identity without the need for designing a dedicated architecture or supervision from attribute classifiers. We demonstrate the generalizability and effectiveness of ExFaceGAN by integrating it into learned latent spaces of three SOTA GAN approaches. As an example of the practical benefit of our ExFaceGAN, we empirically prove that data generated by ExFaceGAN can be successfully used to train face recognition models (\\url{https://github.com/fdbtrs/ExFaceGAN}).",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "52224165",
                        "name": "F. Boutros"
                    },
                    {
                        "authorId": "2124211696",
                        "name": "Marcel Klemt"
                    },
                    {
                        "authorId": "1944448620",
                        "name": "Meiling Fang"
                    },
                    {
                        "authorId": "145307900",
                        "name": "Arjan Kuijper"
                    },
                    {
                        "authorId": "2265721",
                        "name": "N. Damer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "After that, many StyleGAN-based methods have been proposed for controllable face generation [9]\u2013[11]."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "94d3d65e8041d9a42a89b3bce763dc41ae4b1082",
                "externalIds": {
                    "DBLP": "conf/icmcs/ZhangSX23",
                    "DOI": "10.1109/ICME55011.2023.00045",
                    "CorpusId": 261127464
                },
                "corpusId": 261127464,
                "publicationVenue": {
                    "id": "975e1529-a14b-436f-ad7e-a09d8eabc50d",
                    "name": "IEEE International Conference on Multimedia and Expo",
                    "type": "conference",
                    "alternate_names": [
                        "ICME",
                        "Int Conf Multimedia Expo",
                        "IEEE Int Conf Multimedia Expo",
                        "International Conference on Multimedia and Expo"
                    ],
                    "issn": "1945-7871",
                    "alternate_issns": [
                        "2330-7927"
                    ],
                    "url": "http://csdl2.computer.org/persagen/ProceedingByFilter.jsp?filter=I",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conferences.jsp",
                        "http://www.wikicfp.com/cfp/program?id=1418"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/94d3d65e8041d9a42a89b3bce763dc41ae4b1082",
                "title": "Expression-Guided Attention GAN for Fine-Grained Facial Expression Editing",
                "abstract": "Facial expression editing aims at manipulating the expression of a face image with fine-grained conditions, while keeping the irrelevant regions unchanged. Previous methods have certain limitations in the quality of generated images and usually suffer from changing condition-irrelevant regions, such as background and facial details. In this paper, we propose an expression-guided attention GAN (EGA-GAN) to achieve fine-grained facial expression editing. We take the relative Facial Action Units (AUs) as the condition for target expression. The conditions are used to control two modules called latent space manipulation and multi-scale feature manipulation. The former is designed to edit expression and generate realistic and natural expression changes, and the latter is designed to preserve the unrelated region. To take advantages of both modules, we generate an attention mask from the expression condition and the input image to fuse the features of the two modules in a fine-grained manner. Through experiments, we show that our method achieves state-of-the-art performance on the accuracy of expression editing and can better disentangle the change of expression from irrelevant regions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2155540345",
                        "name": "Hui Zhang"
                    },
                    {
                        "authorId": "2589625",
                        "name": "Shiqi Shen"
                    },
                    {
                        "authorId": "47883160",
                        "name": "Jinhua Xu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "16ae4e6ed7bcf5b01c862e381c9cbb9d76fe9df4",
                "externalIds": {
                    "DBLP": "conf/icmcs/ZhouJYHH23",
                    "DOI": "10.1109/ICME55011.2023.00106",
                    "CorpusId": 261126527
                },
                "corpusId": 261126527,
                "publicationVenue": {
                    "id": "975e1529-a14b-436f-ad7e-a09d8eabc50d",
                    "name": "IEEE International Conference on Multimedia and Expo",
                    "type": "conference",
                    "alternate_names": [
                        "ICME",
                        "Int Conf Multimedia Expo",
                        "IEEE Int Conf Multimedia Expo",
                        "International Conference on Multimedia and Expo"
                    ],
                    "issn": "1945-7871",
                    "alternate_issns": [
                        "2330-7927"
                    ],
                    "url": "http://csdl2.computer.org/persagen/ProceedingByFilter.jsp?filter=I",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conferences.jsp",
                        "http://www.wikicfp.com/cfp/program?id=1418"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/16ae4e6ed7bcf5b01c862e381c9cbb9d76fe9df4",
                "title": "DF-CLIP: Towards Disentangled and Fine-grained Image Editing from Text",
                "abstract": "Inspired by CLIP\u2019s excellent image/text representation capability and StyleGAN\u2019s disentangled latent space, text-guide image editing techniques make significant progress. However, as CLIP cannot perform local fine-grained image/text alignment, existing methods suffer from entanglement problems. Moreover, there lacks a deep interaction between textual tokens and visual features, which may lead to unfaithful editing results. In this paper, we propose DF-CLIP for Disentangled and Fine-grained text-guide image editing. Specifically, we design a novel dual-branch LatentMask module to generate more accurate editing directions in StyleGAN\u2019s latent space, which can avoid changes in text-unrelated areas. Furthermore, we present a Multi-modal Interaction module to associate the text embedding with the image embedding and perform a deep interaction between them, which greatly enhance the guidance of text in image editing process and accelerate the training convergence. Extensive experiments show that our models perform more disentangled and natural editing results with a shorter training time.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2194029193",
                        "name": "Xinjiao Zhou"
                    },
                    {
                        "authorId": "145942580",
                        "name": "Bin Jiang"
                    },
                    {
                        "authorId": "1637066875",
                        "name": "Chao Yang"
                    },
                    {
                        "authorId": "2180475713",
                        "name": "Haotian Hu"
                    },
                    {
                        "authorId": "2193644794",
                        "name": "Xiaofei Huo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our method can be adapted to use other editing methods like InterfaceGAN [40] and GANSpace [20].",
                "Finally, we can edit an inverted image by manipulating the latent code, which can be described as:\nI\u2032edit = G(w + + \u2206w+; \u03b8p), (4)\nwhere \u2206w+ could be any editing directions from InterfaceGAN [40] or GANSpace [20].",
                "where \u2206w could be any editing directions from InterfaceGAN [40] or GANSpace [20]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4ad77eb56954d196b04cfb031ccb355c223713d5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-17123",
                    "ArXiv": "2306.17123",
                    "DOI": "10.1111/cgf.14890",
                    "CorpusId": 259287360
                },
                "corpusId": 259287360,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4ad77eb56954d196b04cfb031ccb355c223713d5",
                "title": "PVP: Personalized Video Prior for Editable Dynamic Portraits using StyleGAN",
                "abstract": "Portrait synthesis creates realistic digital avatars which enable users to interact with others in a compelling way. Recent advances in StyleGAN and its extensions have shown promising results in synthesizing photorealistic and accurate reconstruction of human faces. However, previous methods often focus on frontal face synthesis and most methods are not able to handle large head rotations due to the training data distribution of StyleGAN. In this work, our goal is to take as input a monocular video of a face, and create an editable dynamic portrait able to handle extreme head poses. The user can create novel viewpoints, edit the appearance, and animate the face. Our method utilizes pivotal tuning inversion (PTI) to learn a personalized video prior from a monocular video sequence. Then we can input pose and expression coefficients to MLPs and manipulate the latent vectors to synthesize different viewpoints and expressions of the subject. We also propose novel loss functions to further disentangle pose and expression in the latent space. Our algorithm shows much better performance over previous approaches on monocular video datasets, and it is also capable of running in real\u2010time at 54 FPS on an RTX 3080.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "35453735",
                        "name": "Kai-En Lin"
                    },
                    {
                        "authorId": "1993540993",
                        "name": "Alex Trevithick"
                    },
                    {
                        "authorId": "2140904",
                        "name": "Ke-Li Cheng"
                    },
                    {
                        "authorId": "1730688",
                        "name": "M. Sarkis"
                    },
                    {
                        "authorId": "2469581",
                        "name": "Mohsen Ghafoorian"
                    },
                    {
                        "authorId": "2066769443",
                        "name": "N. Bi"
                    },
                    {
                        "authorId": "1766402",
                        "name": "Gerhard Reitmayr"
                    },
                    {
                        "authorId": "1752236",
                        "name": "R. Ramamoorthi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In recent years, style-based generator [19, 20] emerged as a powerful image synthesis model, and a plethora of recent works [1, 2, 18, 37, 42, 45] tend to explore the rich interpretable semantics inside the latent space of fixed GAN models.",
                "GANSpace [18] carries out Principal Component Analysis (PCA) in the latent space of generative networks and explores interpretable controls in an unsupervised manner."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "10c9a466cdf400bf01cabbcb1bc9a14555730195",
                "externalIds": {
                    "ArXiv": "2306.16894",
                    "DBLP": "journals/corr/abs-2306-16894",
                    "DOI": "10.48550/arXiv.2306.16894",
                    "CorpusId": 259287333
                },
                "corpusId": 259287333,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/10c9a466cdf400bf01cabbcb1bc9a14555730195",
                "title": "PFB-Diff: Progressive Feature Blending Diffusion for Text-driven Image Editing",
                "abstract": "Diffusion models have showcased their remarkable capability to synthesize diverse and high-quality images, sparking interest in their application for real image editing. However, existing diffusion-based approaches for local image editing often suffer from undesired artifacts due to the pixel-level blending of the noised target images and diffusion latent variables, which lack the necessary semantics for maintaining image consistency. To address these issues, we propose PFB-Diff, a Progressive Feature Blending method for Diffusion-based image editing. Unlike previous methods, PFB-Diff seamlessly integrates text-guided generated content into the target image through multi-level feature blending. The rich semantics encoded in deep features and the progressive blending scheme from high to low levels ensure semantic coherence and high quality in edited images. Additionally, we introduce an attention masking mechanism in the cross-attention layers to confine the impact of specific words to desired regions, further improving the performance of background editing. PFB-Diff can effectively address various editing tasks, including object/background replacement and object attribute editing. Our method demonstrates its superior performance in terms of image fidelity, editing accuracy, efficiency, and faithfulness to the original image, without the need for fine-tuning or training.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "151492345",
                        "name": "Wenjing Huang"
                    },
                    {
                        "authorId": "1701972",
                        "name": "Shikui Tu"
                    },
                    {
                        "authorId": "2109329726",
                        "name": "Lei Xu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "5cde780c2bad086fa5267a11068bb40d320616b4",
                "externalIds": {
                    "DBLP": "conf/aaai/XiaSWL0WWL23",
                    "DOI": "10.1609/aaai.v37i3.25394",
                    "CorpusId": 259758797
                },
                "corpusId": 259758797,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/5cde780c2bad086fa5267a11068bb40d320616b4",
                "title": "FEditNet: Few-Shot Editing of Latent Semantics in GAN Spaces",
                "abstract": "Generative Adversarial networks (GANs) have demonstrated their powerful capability of synthesizing high-resolution images, and great efforts have been made to interpret the semantics in the latent spaces of GANs. However, existing works still have the following limitations: (1) the majority of works rely on either pretrained attribute predictors or large-scale labeled datasets, which are difficult to collect in most cases, and (2) some other methods are only suitable for restricted cases, such as focusing on interpretation of human facial images using prior facial semantics. In this paper, we propose a GAN-based method called FEditNet, aiming to discover latent semantics using very few labeled data without any pretrained predictors or prior knowledge. Specifically, we reuse the knowledge from the pretrained GANs, and by doing so, avoid overfitting during the few-shot training of FEditNet. Moreover, our layer-wise objectives which take content consistency into account also ensure the disentanglement between attributes. Qualitative and quantitative results demonstrate that our method outperforms the state-of-the-art methods on various datasets. The code is available at https://github.com/THU-LYJ-Lab/FEditNet.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1646919482",
                        "name": "Mengfei Xia"
                    },
                    {
                        "authorId": "1388378798",
                        "name": "Yezhi Shu"
                    },
                    {
                        "authorId": "2115658160",
                        "name": "Yujie Wang"
                    },
                    {
                        "authorId": "7827503",
                        "name": "Yu-Kun Lai"
                    },
                    {
                        "authorId": "48934185",
                        "name": "Qiang Li"
                    },
                    {
                        "authorId": "37124370",
                        "name": "Pengfei Wan"
                    },
                    {
                        "authorId": "2135392923",
                        "name": "Zhong-ming Wang"
                    },
                    {
                        "authorId": "2144470995",
                        "name": "Yong-jin Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "d5e0c4bf2018e6ba59918878fe7e2b376c688b8d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-13531",
                    "ArXiv": "2306.13531",
                    "DOI": "10.48550/arXiv.2306.13531",
                    "CorpusId": 259243922
                },
                "corpusId": 259243922,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d5e0c4bf2018e6ba59918878fe7e2b376c688b8d",
                "title": "WBCAtt: A White Blood Cell Dataset Annotated with Detailed Morphological Attributes",
                "abstract": "The examination of blood samples at a microscopic level plays a fundamental role in clinical diagnostics, influencing a wide range of medical conditions. For instance, an in-depth study of White Blood Cells (WBCs), a crucial component of our blood, is essential for diagnosing blood-related diseases such as leukemia and anemia. While multiple datasets containing WBC images have been proposed, they mostly focus on cell categorization, often lacking the necessary morphological details to explain such categorizations, despite the importance of explainable artificial intelligence (XAI) in medical domains. This paper seeks to address this limitation by introducing comprehensive annotations for WBC images. Through collaboration with pathologists, a thorough literature review, and manual inspection of microscopic images, we have identified 11 morphological attributes associated with the cell and its components (nucleus, cytoplasm, and granules). We then annotated ten thousand WBC images with these attributes. Moreover, we conduct experiments to predict these attributes from images, providing insights beyond basic WBC classification. As the first public dataset to offer such extensive annotations, we also illustrate specific applications that can benefit from our attribute annotations. Overall, our dataset paves the way for interpreting WBC recognition models, further advancing XAI in the fields of pathology and hematology.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3727644",
                        "name": "Satoshi Tsutsui"
                    },
                    {
                        "authorId": "33499770",
                        "name": "Winnie Pang"
                    },
                    {
                        "authorId": "1766554",
                        "name": "B. Wen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "489b35b66fab138ea28ade179f68135a1cd06ff9",
                "externalIds": {
                    "ArXiv": "2306.11719",
                    "DBLP": "journals/corr/abs-2306-11719",
                    "DOI": "10.48550/arXiv.2306.11719",
                    "CorpusId": 259204144
                },
                "corpusId": 259204144,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/489b35b66fab138ea28ade179f68135a1cd06ff9",
                "title": "Diffusion with Forward Models: Solving Stochastic Inverse Problems Without Direct Supervision",
                "abstract": "Denoising diffusion models are a powerful type of generative models used to capture complex distributions of real-world signals. However, their applicability is limited to scenarios where training samples are readily available, which is not always the case in real-world applications. For example, in inverse graphics, the goal is to generate samples from a distribution of 3D scenes that align with a given image, but ground-truth 3D scenes are unavailable and only 2D images are accessible. To address this limitation, we propose a novel class of denoising diffusion probabilistic models that learn to sample from distributions of signals that are never directly observed. Instead, these signals are measured indirectly through a known differentiable forward model, which produces partial observations of the unknown signal. Our approach involves integrating the forward model directly into the denoising process. This integration effectively connects the generative modeling of observations with the generative modeling of the underlying signals, allowing for end-to-end training of a conditional generative model over signals. During inference, our approach enables sampling from the distribution of underlying signals that are consistent with a given partial observation. We demonstrate the effectiveness of our method on three challenging computer vision tasks. For instance, in the context of inverse graphics, our model enables direct sampling from the distribution of 3D scenes that align with a single 2D input image.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "9102722",
                        "name": "A. Tewari"
                    },
                    {
                        "authorId": "2068568214",
                        "name": "Tianwei Yin"
                    },
                    {
                        "authorId": "19296006",
                        "name": "George Cazenavette"
                    },
                    {
                        "authorId": "2140722280",
                        "name": "Semon Rezchikov"
                    },
                    {
                        "authorId": "1763295",
                        "name": "J. Tenenbaum"
                    },
                    {
                        "authorId": "145403226",
                        "name": "F. Durand"
                    },
                    {
                        "authorId": "1768236",
                        "name": "W. Freeman"
                    },
                    {
                        "authorId": "5021307",
                        "name": "V. Sitzmann"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "44aa93686ed7428599f1f2574fc0c563f1e8aa63",
                "externalIds": {
                    "DBLP": "conf/ijcnn/ZhengAK23",
                    "DOI": "10.1109/IJCNN54540.2023.10191767",
                    "CorpusId": 260386291
                },
                "corpusId": 260386291,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/44aa93686ed7428599f1f2574fc0c563f1e8aa63",
                "title": "Facial Image Manipulation via Discriminative Decomposition of Semantic Space",
                "abstract": "Generative Adversarial Networks (GANs) can learn a highly informative latent space. By manipulating the latent representation in the latent space, we can control attributes in the generated images without modifying the model of GAN itself. Most existing studies find the direction vector of each semantic in the latent space and steer the latent vectors using linear computation to control semantic changes in the images. However, moving against the semantic direction still makes the latent representation subject to the entanglement of attributes in the latent space. Therefore, to solve the problem of attribute entanglement in latent space, we propose a decomposition method that decomposes the identity information subspace and the attribute information subspace from the pre-trained latent space and realize the semantic change of the latent representation by moving in the attribute information subspace. In our experiments, we evaluated its effectiveness and compared the images after the attribute change. The experiments demonstrated that our method can further solve the attribute entanglement problem in the latent space and effectively reduce the impact on other contents in the latent representation after changing the attributes.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2214934578",
                        "name": "Jiazhou Zheng"
                    },
                    {
                        "authorId": "134044998",
                        "name": "Hiroaki Aizawa"
                    },
                    {
                        "authorId": "145375983",
                        "name": "Takio Kurita"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Unsupervised methods [Cherepkov et al. 2021; H\u00e4rk\u00f6nen et al. 2020; Shen and Zhou 2021; Voynov\nand Babenko 2020; Wei et al. 2021] find the interpretable direction without using annotated samples, e.g., by a PCA decomposition on the network weights or on the latent codes."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a0f8bc8e59430ce0edc0be0422d7cd1075419d24",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-08226",
                    "ArXiv": "2306.08226",
                    "DOI": "10.48550/arXiv.2306.08226",
                    "CorpusId": 259164518
                },
                "corpusId": 259164518,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a0f8bc8e59430ce0edc0be0422d7cd1075419d24",
                "title": "CLIPXPlore: Coupled CLIP and Shape Spaces for 3D Shape Exploration",
                "abstract": "This paper presents CLIPXPlore, a new framework that leverages a vision-language model to guide the exploration of the 3D shape space. Many recent methods have been developed to encode 3D shapes into a learned latent shape space to enable generative design and modeling. Yet, existing methods lack effective exploration mechanisms, despite the rich information. To this end, we propose to leverage CLIP, a powerful pre-trained vision-language model, to aid the shape-space exploration. Our idea is threefold. First, we couple the CLIP and shape spaces by generating paired CLIP and shape codes through sketch images and training a mapper network to connect the two spaces. Second, to explore the space around a given shape, we formulate a co-optimization strategy to search for the CLIP code that better matches the geometry of the shape. Third, we design three exploration modes, binary-attribute-guided, text-guided, and sketch-guided, to locate suitable exploration trajectories in shape space and induce meaningful changes to the shape. We perform a series of experiments to quantitatively and visually compare CLIPXPlore with different baselines in each of the three exploration modes, showing that CLIPXPlore can produce many meaningful exploration results that cannot be achieved by the existing solutions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2118517986",
                        "name": "Jingyu Hu"
                    },
                    {
                        "authorId": "1405688230",
                        "name": "Ka-Hei Hui"
                    },
                    {
                        "authorId": "2926099",
                        "name": "Zhengzhe Liu"
                    },
                    {
                        "authorId": "145140331",
                        "name": "Haotong Zhang"
                    },
                    {
                        "authorId": "144856288",
                        "name": "Chi-Wing Fu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Compared with the other emergent generative models, GANs have the advantages of rapid inference [29], generating fixed objects [29, 56] and controllable latent distribution [33, 34, 32, 3, 21, 64]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "27eaed5c55547855301904c3130f46ed16160be6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-07716",
                    "ArXiv": "2306.07716",
                    "DOI": "10.48550/arXiv.2306.07716",
                    "CorpusId": 259145006
                },
                "corpusId": 259145006,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/27eaed5c55547855301904c3130f46ed16160be6",
                "title": "Dynamically Masked Discriminator for Generative Adversarial Networks",
                "abstract": "Training Generative Adversarial Networks (GANs) remains a challenging problem. The discriminator trains the generator by learning the distribution of real/generated data. However, the distribution of generated data changes throughout the training process, which is difficult for the discriminator to learn. In this paper, we propose a novel method for GANs from the viewpoint of online continual learning. We observe that the discriminator model, trained on historically generated data, often slows down its adaptation to the changes in the new arrival generated data, which accordingly decreases the quality of generated results. By treating the generated data in training as a stream, we propose to detect whether the discriminator slows down the learning of new knowledge in generated data. Therefore, we can explicitly enforce the discriminator to learn new knowledge fast. Particularly, we propose a new discriminator, which automatically detects its retardation and then dynamically masks its features, such that the discriminator can adaptively learn the temporally-vary distribution of generated data. Experimental results show our method outperforms the state-of-the-art approaches.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2143397013",
                        "name": "Wentian Zhang"
                    },
                    {
                        "authorId": "46935952",
                        "name": "Haozhe Liu"
                    },
                    {
                        "authorId": "2156072517",
                        "name": "Bing Li"
                    },
                    {
                        "authorId": "2220635949",
                        "name": "Jinheng Xie"
                    },
                    {
                        "authorId": "46844189",
                        "name": "Yawen Huang"
                    },
                    {
                        "authorId": "1758100",
                        "name": "Yuexiang Li"
                    },
                    {
                        "authorId": "1890905492",
                        "name": "Yefeng Zheng"
                    },
                    {
                        "authorId": "2931652",
                        "name": "Bernard Ghanem"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "65447380346a02fe05977ee9d587b6454cd242dc",
                "externalIds": {
                    "ArXiv": "2306.07476",
                    "DBLP": "conf/siggraph/Huang0FM23",
                    "DOI": "10.1145/3588432.3591548",
                    "CorpusId": 259145056
                },
                "corpusId": 259145056,
                "publicationVenue": {
                    "id": "cf6b5e76-9274-46e6-a2dd-7c190ec2ec5f",
                    "name": "International Conference on Computer Graphics and Interactive Techniques",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Graph Interact Tech",
                        "SIGGRAPH"
                    ],
                    "url": "http://www.siggraph.org/"
                },
                "url": "https://www.semanticscholar.org/paper/65447380346a02fe05977ee9d587b6454cd242dc",
                "title": "AniFaceDrawing: Anime Portrait Exploration during Your Sketching",
                "abstract": "This paper focuses on how artificial intelligence (AI) can be used to assist general users in the creation of professional portraits, that is, consistently converting rough sketches into high-quality anime portraits during their sketching process. The input to this task is a sequence of incomplete human freehand sketches that are gradually refined stroke by stroke, while the output is a sequence of high-quality anime portraits that correspond to the input sketches as guidance. Although recent GANs can generate high quality images, it is a challenging problem to maintain the high quality of generated images from sketches with a low degree of completion due to ill-posed problems in conditional image generation. Even with the latest sketch-to-image (S2I) technology, it is still difficult to create high-quality images from incomplete rough sketches for anime portraits because the lines in anime style tend to be more abstract than in realistic style. In this paper, we addressed this problem using the latent space exploration of StyleGAN with a two-stage training strategy. Specifically, we consider the input strokes of a freehand sketch to correspond to edge information-related attributes in the latent structural code of StyleGAN, and term the matching between strokes and these attributes \u201cstroke-level disentanglement.\u201d In the first stage, we trained an image encoder with the pre-trained StyleGAN model as a teacher encoder. In the second stage, we simulated the drawing process of the generated images and trained the sketch encoder for incomplete progressive sketches to generate high-quality portrait images with feature alignment to the disentangled representations at the stroke level in the teacher encoder. We verified the proposed progressive S2I system with both qualitative and quantitative evaluations and achieved high-quality anime portraits from incomplete progressive sketches. What\u2019s more, our user study proved its effectiveness in art creation assistance for the anime style.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2151324188",
                        "name": "Zhengyu Huang"
                    },
                    {
                        "authorId": "2087119718",
                        "name": "Haoran Xie"
                    },
                    {
                        "authorId": "33439074",
                        "name": "Tsukasa Fukusato"
                    },
                    {
                        "authorId": "145730883",
                        "name": "K. Miyata"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some methods mitigate this problem by finding latent semantic vectors existing in pre-trained generators [8], [9], [10], [11].",
                "[10], [11], [50], [51], [52], [53], [54]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "c188322f62b6f7af7fcb943541bfbfc2bea0b6fc",
                "externalIds": {
                    "DBLP": "journals/pami/ZhouXNT23",
                    "DOI": "10.1109/TPAMI.2023.3285648",
                    "CorpusId": 259153616,
                    "PubMed": "37310846"
                },
                "corpusId": 259153616,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c188322f62b6f7af7fcb943541bfbfc2bea0b6fc",
                "title": "CIPS-3D++: End-to-End Real-Time High-Resolution 3D-Aware GANs for GAN Inversion and Stylization",
                "abstract": "Style-based GANs achieve state-of-the-art results for generating high-quality images, but lack explicit and precise control over camera poses. Recently proposed NeRF-based GANs have made great progress towards <bold>3D-aware</bold> image generation. However, the methods either rely on convolution operators which are not rotationally invariant, or utilize complex yet suboptimal training procedures to integrate both NeRF and CNN sub-structures, yielding un-robust, low-quality images with a large computational burden. This article presents an upgraded version called <bold>CIPS-3D++</bold>, aiming at high-robust, high-resolution and high-efficiency 3D-aware GANs. On the one hand, our basic model CIPS-3D, encapsulated in a style-based architecture, features a shallow NeRF-based 3D shape encoder as well as a deep MLP-based 2D image decoder, achieving robust image generation/editing with rotation-invariance. On the other hand, our proposed CIPS-3D++, inheriting the rotational invariance of CIPS-3D, together with geometric regularization and upsampling operations, encourages high-resolution high-quality image generation/editing with great computational efficiency. Trained on raw single-view images, without any bells and whistles, CIPS-3D++ sets new records for 3D-aware image synthesis, with an impressive FID of 3.2 on FFHQ at the <inline-formula><tex-math notation=\"LaTeX\">$1024\\times 1024$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>1024</mml:mn><mml:mo>\u00d7</mml:mo><mml:mn>1024</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"zhou-ieq1-3285648.gif\"/></alternatives></inline-formula> resolution. In the meantime, CIPS-3D++ runs efficiently and enjoys a low GPU memory footprint so that it can be trained end-to-end on high-resolution images directly, in contrast to previous alternate/progressive methods. Based on the infrastructure of CIPS-3D++, we propose a 3D-aware GAN inversion algorithm named <bold>FlipInversion</bold>, which can reconstruct the 3D object from a single-view image. We also provide a 3D-aware stylization method for real images based on CIPS-3D++ and FlipInversion. In addition, we analyze the problem of mirror symmetry suffered in training, and solve it by introducing an auxiliary discriminator for the NeRF network. Overall, CIPS-3D++ provides a strong base model that can serve as a testbed for transferring GAN-based image editing methods from 2D to 3D.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220038672",
                        "name": "Peng Zhou"
                    },
                    {
                        "authorId": "3041937",
                        "name": "Lingxi Xie"
                    },
                    {
                        "authorId": "5796401",
                        "name": "Bingbing Ni"
                    },
                    {
                        "authorId": "2056267867",
                        "name": "Qi Tian"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "08088138a2fc9c6b1a54691f476ba7adb8d9e38a",
                "externalIds": {
                    "DBLP": "conf/mir/HwangKL023",
                    "DOI": "10.1145/3591106.3592273",
                    "CorpusId": 259112729
                },
                "corpusId": 259112729,
                "publicationVenue": {
                    "id": "b7d34536-73df-402d-8967-50a9cdf73c01",
                    "name": "International Conference on Multimedia Retrieval",
                    "type": "conference",
                    "alternate_names": [
                        "ICMR",
                        "Int Conf Multimedia Retr"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1424"
                },
                "url": "https://www.semanticscholar.org/paper/08088138a2fc9c6b1a54691f476ba7adb8d9e38a",
                "title": "Unlocking Potential of 3D-aware GAN for More Expressive Face Generation",
                "abstract": "As style-based image generators have achieved disentanglement in features by converting latent vector space to style vector space, numerous efforts have been made to enhance the controllability of the latent. However, existing methods for controllable models have limitations in precisely creating high-resolution faces with large expressions. The degradation is due to the dependence on the training dataset, as the high-resolution face datasets do not have sufficient expressive images. To tackle this challenge, we propose a robust training framework for 3D-aware generative adversarial networks to learn the high-quality generation of more expressive faces through a signed distance field. First, we propose a novel 3D enforcement loss to generate more expressive images in an unsupervised manner. Second, we introduce a partial training method to fine-tune the network on multiple datasets without loss of image resolution. Finally, we propose a ray-scaling scheme for the volume renderer to represent a face at arbitrary scales. Through the proposed framework, the network learns 3D face priors, such as expressional shapes of the parametric facial model, to generate detailed faces. The experimental results outperform the methods of the state of the art, showing strong benefits in the generation of high-resolution facial expressions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220600689",
                        "name": "Juheon Hwang"
                    },
                    {
                        "authorId": "143661625",
                        "name": "Jiwoo Kang"
                    },
                    {
                        "authorId": "2110246363",
                        "name": "Kyoungoh Lee"
                    },
                    {
                        "authorId": "2144570602",
                        "name": "Sanghoon Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, many StyleGAN variants [Abdal et al. 2021; Gal et al. 2022; H\u00e4rk\u00f6nen et al. 2020; Patashnik et al. 2021; Shoshan et al. 2021; Tov et al. 2021; Wang et al. 2021a, 2022; Wu et al. 2021] have been introduced to address this problem.",
                "Built on the success of StyleGAN, a large number of methods [Abdal et al. 2021; Gal et al. 2022; H\u00e4rk\u00f6nen et al. 2020; Patashnik et al. 2021; Shoshan et al. 2021; Tov et al. 2021; Wang et al. 2021a, 2022; Wu et al. 2021] use it as a prior for semantic face editing and other image enhancement tasks,\u2026",
                "GANSpace [H\u00e4rk\u00f6nen et al. 2020] attempts to analyze the GAN space by identifying latent directions based on principal component analysis (PCA), applied either in latent space or feature space.",
                "Built on the success of StyleGAN, a large number of methods [Abdal et al. 2021; Gal et al. 2022; H\u00e4rk\u00f6nen et al. 2020; Patashnik et al. 2021; Shoshan et al. 2021; Tov et al. 2021; Wang et al. 2021a, 2022; Wu et al. 2021] use it as a prior for semantic face editing and other image enhancement tasks, such as inpainting and super-resolution."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b743d1aad948b69cdd33c8b33d59e3305cb2b124",
                "externalIds": {
                    "ArXiv": "2306.04865",
                    "DBLP": "journals/corr/abs-2306-04865",
                    "DOI": "10.48550/arXiv.2306.04865",
                    "CorpusId": 259108442
                },
                "corpusId": 259108442,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b743d1aad948b69cdd33c8b33d59e3305cb2b124",
                "title": "MyStyle++: A Controllable Personalized Generative Prior",
                "abstract": "In this paper, we propose an approach to obtain a personalized generative prior with explicit control over a set of attributes. We build upon MyStyle, a recently introduced method, that tunes the weights of a pre-trained StyleGAN face generator on a few images of an individual. This system allows synthesizing, editing, and enhancing images of the target individual with high fidelity to their facial features. However, MyStyle does not demonstrate precise control over the attributes of the generated images. We propose to address this problem through a novel optimization system that organizes the latent space in addition to tuning the generator. Our key contribution is to formulate a loss that arranges the latent codes, corresponding to the input images, along a set of specific directions according to their attributes. We demonstrate that our approach, dubbed MyStyle++, is able to synthesize, edit, and enhance images of an individual with great control over the attributes, while preserving the unique facial characteristics of that individual.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2114138299",
                        "name": "Libing Zeng"
                    },
                    {
                        "authorId": "152875073",
                        "name": "Lele Chen"
                    },
                    {
                        "authorId": "121983635",
                        "name": "Yinghao Xu"
                    },
                    {
                        "authorId": "1717070",
                        "name": "N. Kalantari"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Can we construct computer vision systems that may likewise understand, recombine, and imagine the visual world? Most existing work in concept discovery focus on discovering latent vectors or directions representing individual concepts [15, 24, 18, 44, 55], but require supervised data labeling each concept.",
                "Existing works in concept discovery in computer vision typically focus on discovering a latent space to manipulate images [15, 24, 18, 44, 55, 42] but require supervised data to specify each concept."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e38ba8795bcd7488a831f52b5911a85ca94f5387",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-05357",
                    "ArXiv": "2306.05357",
                    "DOI": "10.48550/arXiv.2306.05357",
                    "CorpusId": 259108434
                },
                "corpusId": 259108434,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e38ba8795bcd7488a831f52b5911a85ca94f5387",
                "title": "Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models",
                "abstract": "Text-to-image generative models have enabled high-resolution image synthesis across different domains, but require users to specify the content they wish to generate. In this paper, we consider the inverse problem -- given a collection of different images, can we discover the generative concepts that represent each image? We present an unsupervised approach to discover generative concepts from a collection of images, disentangling different art styles in paintings, objects, and lighting from kitchen scenes, and discovering image classes given ImageNet images. We show how such generative concepts can accurately represent the content of images, be recombined and composed to generate new artistic and hybrid images, and be further used as a representation for downstream classification tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2087010550",
                        "name": "Nan Liu"
                    },
                    {
                        "authorId": "15394275",
                        "name": "Yilun Du"
                    },
                    {
                        "authorId": "145015904",
                        "name": "Shuang Li"
                    },
                    {
                        "authorId": "1763295",
                        "name": "J. Tenenbaum"
                    },
                    {
                        "authorId": "143805211",
                        "name": "A. Torralba"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "45549f8845f1fe6daea0f300e52d5df7061632d4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-05414",
                    "ArXiv": "2306.05414",
                    "DOI": "10.48550/arXiv.2306.05414",
                    "CorpusId": 259287564
                },
                "corpusId": 259287564,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/45549f8845f1fe6daea0f300e52d5df7061632d4",
                "title": "Improving Tuning-Free Real Image Editing with Proximal Guidance",
                "abstract": "DDIM inversion has revealed the remarkable potential of real image editing within diffusion-based methods. However, the accuracy of DDIM reconstruction degrades as larger classifier-free guidance (CFG) scales being used for enhanced editing. Null-text inversion (NTI) optimizes null embeddings to align the reconstruction and inversion trajectories with larger CFG scales, enabling real image editing with cross-attention control. Negative-prompt inversion (NPI) further offers a training-free closed-form solution of NTI. However, it may introduce artifacts and is still constrained by DDIM reconstruction quality. To overcome these limitations, we propose proximal guidance and incorporate it to NPI with cross-attention control. We enhance NPI with a regularization term and reconstruction guidance, which reduces artifacts while capitalizing on its training-free nature. Additionally, we extend the concepts to incorporate mutual self-attention control, enabling geometry and layout alterations in the editing process. Our method provides an efficient and straightforward approach, effectively addressing real image editing tasks with minimal computational overhead.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3471102",
                        "name": "Ligong Han"
                    },
                    {
                        "authorId": "1902406591",
                        "name": "Song Wen"
                    },
                    {
                        "authorId": "2157956022",
                        "name": "Qi Chen"
                    },
                    {
                        "authorId": "2128662401",
                        "name": "Zhixing Zhang"
                    },
                    {
                        "authorId": "8799238",
                        "name": "Kunpeng Song"
                    },
                    {
                        "authorId": "30106897",
                        "name": "Mengwei Ren"
                    },
                    {
                        "authorId": "151184349",
                        "name": "Ruijiang Gao"
                    },
                    {
                        "authorId": "2109168002",
                        "name": "Yuxiao Chen"
                    },
                    {
                        "authorId": "1771885",
                        "name": "Ding Liu"
                    },
                    {
                        "authorId": "2004341222",
                        "name": "Qilong Zhangli"
                    },
                    {
                        "authorId": "1749519378",
                        "name": "Anastasis Stathopoulos"
                    },
                    {
                        "authorId": "3807016",
                        "name": "Jindong Jiang"
                    },
                    {
                        "authorId": "2151464031",
                        "name": "Zhaoyang Xia"
                    },
                    {
                        "authorId": "2018087",
                        "name": "Akash Srivastava"
                    },
                    {
                        "authorId": "1711560",
                        "name": "Dimitris N. Metaxas"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "de583aaf716964e86afb53dec8bc3bdc7fed4d6c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-04636",
                    "ArXiv": "2306.04636",
                    "DOI": "10.1109/TPAMI.2023.3284003",
                    "CorpusId": 259095564,
                    "PubMed": "37289604"
                },
                "corpusId": 259095564,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/de583aaf716964e86afb53dec8bc3bdc7fed4d6c",
                "title": "GP-UNIT: Generative Prior for Versatile Unsupervised Image-to-Image Translation",
                "abstract": "Recent advances in deep learning have witnessed many successful unsupervised image-to-image translation models that learn correspondences between two visual domains without paired data. However, it is still a great challenge to build robust mappings between various domains especially for those with drastic visual discrepancies. In this paper, we introduce a novel versatile framework, Generative Prior-guided UNsupervised Image-to-image Translation (GP-UNIT), that improves the quality, applicability and controllability of the existing translation models. The key idea of GP-UNIT is to distill the generative prior from pre-trained class-conditional GANs to build coarse-level cross-domain correspondences, and to apply the learned prior to adversarial translations to excavate fine-level correspondences. With the learned multi-level content correspondences, GP-UNIT is able to perform valid translations between both close domains and distant domains. For close domains, GP-UNIT can be conditioned on a parameter to determine the intensity of the content correspondences during translation, allowing users to balance between content and style consistency. For distant domains, semi-supervised learning is explored to guide GP-UNIT to discover accurate semantic correspondences that are hard to learn solely from the appearance. We validate the superiority of GP-UNIT over state-of-the-art translation models in robust, high-quality and diversified translations between various domains through extensive experiments.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2159711748",
                        "name": "Shuai Yang"
                    },
                    {
                        "authorId": "94106850",
                        "name": "Liming Jiang"
                    },
                    {
                        "authorId": "2117940996",
                        "name": "Ziwei Liu"
                    },
                    {
                        "authorId": "1717179",
                        "name": "Chen Change Loy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Specifically, previous methods mainly used pre-trained GAN models to enable image manipulation [8, 9]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "0f05df93c7d0994de63fecfccb61e9b69b731ad3",
                "externalIds": {
                    "ArXiv": "2306.04396",
                    "DBLP": "journals/corr/abs-2306-04396",
                    "DOI": "10.48550/arXiv.2306.04396",
                    "CorpusId": 259095477
                },
                "corpusId": 259095477,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0f05df93c7d0994de63fecfccb61e9b69b731ad3",
                "title": "Improving Diffusion-based Image Translation using Asymmetric Gradient Guidance",
                "abstract": "Diffusion models have shown significant progress in image translation tasks recently. However, due to their stochastic nature, there's often a trade-off between style transformation and content preservation. Current strategies aim to disentangle style and content, preserving the source image's structure while successfully transitioning from a source to a target domain under text or one-shot image conditions. Yet, these methods often require computationally intense fine-tuning of diffusion models or additional neural networks. To address these challenges, here we present an approach that guides the reverse process of diffusion sampling by applying asymmetric gradient guidance. This results in quicker and more stable image manipulation for both text-guided and image-guided image translation. Our model's adaptability allows it to be implemented with both image- and latent-diffusion models. Experiments show that our method outperforms various state-of-the-art models in image translation tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "116153377",
                        "name": "Gihyun Kwon"
                    },
                    {
                        "authorId": "30547794",
                        "name": "Jong-Chul Ye"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "52372c6e01d463420169a7b4ef9f68ae4b2d6e9d",
                "externalIds": {
                    "ArXiv": "2306.04793",
                    "DBLP": "journals/corr/abs-2306-04793",
                    "DOI": "10.48550/arXiv.2306.04793",
                    "CorpusId": 259108693
                },
                "corpusId": 259108693,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/52372c6e01d463420169a7b4ef9f68ae4b2d6e9d",
                "title": "On the Joint Interaction of Models, Data, and Features",
                "abstract": "Learning features from data is one of the defining characteristics of deep learning, but our theoretical understanding of the role features play in deep learning is still rudimentary. To address this gap, we introduce a new tool, the interaction tensor, for empirically analyzing the interaction between data and model through features. With the interaction tensor, we make several key observations about how features are distributed in data and how models with different random seeds learn different features. Based on these observations, we propose a conceptual framework for feature learning. Under this framework, the expected accuracy for a single hypothesis and agreement for a pair of hypotheses can both be derived in closed-form. We demonstrate that the proposed framework can explain empirically observed phenomena, including the recently discovered Generalization Disagreement Equality (GDE) that allows for estimating the generalization error with only unlabeled data. Further, our theory also provides explicit construction of natural data distributions that break the GDE. Thus, we believe this work provides valuable new insight into our understanding of feature learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "4614137",
                        "name": "Yiding Jiang"
                    },
                    {
                        "authorId": "144561756",
                        "name": "Christina Baek"
                    },
                    {
                        "authorId": "145116464",
                        "name": "J. Z. Kolter"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "5ad18991f516c21299fdde71a4139b9e3ab6807c",
                "externalIds": {
                    "DOI": "10.1109/icassp49357.2023.10095858",
                    "CorpusId": 258530018
                },
                "corpusId": 258530018,
                "publicationVenue": {
                    "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
                    "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Acoust Speech Signal Process",
                        "IEEE Int Conf Acoust Speech Signal Process",
                        "ICASSP",
                        "International Conference on Acoustics, Speech, and Signal Processing"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
                },
                "url": "https://www.semanticscholar.org/paper/5ad18991f516c21299fdde71a4139b9e3ab6807c",
                "title": "Minimising Distortion for GAN-Based Facial Attribute Manipulation",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2208251216",
                        "name": "Mingyu Shao"
                    },
                    {
                        "authorId": "2215402608",
                        "name": "Li Lu"
                    },
                    {
                        "authorId": "2132698082",
                        "name": "Ye Ding"
                    },
                    {
                        "authorId": "2055482356",
                        "name": "Qing Liao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a9d8442c6dc42a96b63c27f3abb1d99610342718",
                "externalIds": {
                    "DOI": "10.1109/icassp49357.2023.10094861",
                    "CorpusId": 258540989
                },
                "corpusId": 258540989,
                "publicationVenue": {
                    "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
                    "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Acoust Speech Signal Process",
                        "IEEE Int Conf Acoust Speech Signal Process",
                        "ICASSP",
                        "International Conference on Acoustics, Speech, and Signal Processing"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
                },
                "url": "https://www.semanticscholar.org/paper/a9d8442c6dc42a96b63c27f3abb1d99610342718",
                "title": "Classification of Synthetic Facial Attributes by Means of Hybrid Classification/Localization Patch-Based Analysis",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "66063792",
                        "name": "J. Wang"
                    },
                    {
                        "authorId": "2488892",
                        "name": "B. Tondi"
                    },
                    {
                        "authorId": "144625576",
                        "name": "M. Barni"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our StyleIPSB outperforms Ganspace [13], which is also a basis constructed in W+ space using unsupervised learning.",
                "We compare our method with GANSpace [13] and InterfaceGAN [34] in the editing of facial expression.",
                "To facilitate attribute manipulation in an unsupervised manner, GANSpace [13] and SeFa [36] perform decomposition to find primary directions\nin the latent space and explore the interpretable directions among the primary directions.",
                "To facilitate attribute manipulation in an unsupervised manner, GANSpace [13] and SeFa [36] perform decomposition to find primary directions",
                "We compare our method with InterFaceGAN [34], GANSpace [13], GIF [12] and SeFa [36]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "5f04f791eaa23c5c1683bb10565a8972a28ad1b8",
                "externalIds": {
                    "DBLP": "conf/cvpr/JiangS0023",
                    "DOI": "10.1109/CVPR52729.2023.00042",
                    "CorpusId": 260881771
                },
                "corpusId": 260881771,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5f04f791eaa23c5c1683bb10565a8972a28ad1b8",
                "title": "StyleIPSB: Identity-Preserving Semantic Basis of StyleGAN for High Fidelity Face Swapping",
                "abstract": "Recent researches reveal that StyleGAN can generate highly realistic images, inspiring researchers to use pretrained StyleGAN to generate high-fidelity swapped faces. However, existing methods fail to meet the expectations in two essential aspects of high-fidelity face swapping. Their results are blurry without pore-level details and fail to preserve identity for challenging cases. To overcome the above artifacts, we innovatively construct a series of identity-preserving semantic bases of StyleGAN (called StyleIPSB) in respect of pose, expression, and illumination. Each basis of StyleIPSB controls one specific semantic attribute and disentangles with the others. The StyleIPSB constrains style code in the subspace of W+ space to preserve pore-level details and gives us a novel tool for high-fidelity face swapping, and we propose a three-stage framework for face swapping with StyleIPSB. Firstly, we transform the target facial images' attributes to the source image. We learn the mapping from 3D Morphable Model (3DMM) parameters, which capture the prominent semantic variance, to the coordinates of StyleIPSB that show higher identity-preserving and fidelity. Secondly, to transform detailed attributes which 3DMM does not capture, we learn the residual attribute between the reenacted face and the target face. Finally, the face is blended into the background of the target image. Extensive results and comparisons demonstrate that StyleIPSB can effectively preserve identity and pore-level details. The results of face swapping can achieve state-of-the-art performance. We will release our code at https://github.com/a686432/StyleIPSB",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "103718437",
                        "name": "Diqiong Jiang"
                    },
                    {
                        "authorId": "143711380",
                        "name": "Dan Song"
                    },
                    {
                        "authorId": "3128157",
                        "name": "Ruofeng Tong"
                    },
                    {
                        "authorId": "47605163",
                        "name": "Min Tang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "StyleGAN-based image editing requires either an encoder trained to map a given image to the latent space [2, 35], or specifying latent update direction which requires explicit ground-truth annotations [1, 16, 52, 57]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6613a8eb62126426401e49ec0b6d4fceb7a9a15f",
                "externalIds": {
                    "ArXiv": "2306.00783",
                    "DBLP": "journals/corr/abs-2306-00783",
                    "DOI": "10.48550/arXiv.2306.00783",
                    "CorpusId": 258999493
                },
                "corpusId": 258999493,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6613a8eb62126426401e49ec0b6d4fceb7a9a15f",
                "title": "FDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and Relighting with Diffusion Models",
                "abstract": "The ability to create high-quality 3D faces from a single image has become increasingly important with wide applications in video conferencing, AR/VR, and advanced video editing in movie industries. In this paper, we propose Face Diffusion NeRF (FDNeRF), a new generative method to reconstruct high-quality Face NeRFs from single images, complete with semantic editing and relighting capabilities. FDNeRF utilizes high-resolution 3D GAN inversion and expertly trained 2D latent-diffusion model, allowing users to manipulate and construct Face NeRFs in zero-shot learning without the need for explicit 3D data. With carefully designed illumination and identity preserving loss, as well as multi-modal pre-training, FD-NeRF offers users unparalleled control over the editing process enabling them to create and edit face NeRFs using just single-view images, text prompts, and explicit target lighting. The advanced features of FDNeRF have been designed to produce more impressive results than existing 2D editing approaches that rely on 2D segmentation maps for editable attributes. Experiments show that our FDNeRF achieves exceptionally realistic results and unprecedented flexibility in editing compared with state-of-the-art 3D face reconstruction and editing methods. Our code will be available at https://github.com/BillyXYB/FDNeRF.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "39497427",
                        "name": "Hao Zhang"
                    },
                    {
                        "authorId": "2143558044",
                        "name": "Yanbo Xu"
                    },
                    {
                        "authorId": "37304639",
                        "name": "Tianyuan Dai"
                    },
                    {
                        "authorId": "2219331852",
                        "name": "Yu-Wing"
                    },
                    {
                        "authorId": "2218816815",
                        "name": "Tai Chi-Keung Tang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "25cdc0197a78870634cc4bc93eea5748b873eee8",
                "externalIds": {
                    "ArXiv": "2306.00559",
                    "DBLP": "journals/corr/abs-2306-00559",
                    "DOI": "10.48550/arXiv.2306.00559",
                    "CorpusId": 258999729
                },
                "corpusId": 258999729,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/25cdc0197a78870634cc4bc93eea5748b873eee8",
                "title": "We never go out of Style: Motion Disentanglement by Subspace Decomposition of Latent Space",
                "abstract": "Real-world objects perform complex motions that involve multiple independent motion components. For example, while talking, a person continuously changes their expressions, head, and body pose. In this work, we propose a novel method to decompose motion in videos by using a pretrained image GAN model. We discover disentangled motion subspaces in the latent space of widely used style-based GAN models that are semantically meaningful and control a single explainable motion component. The proposed method uses only a few $(\\approx10)$ ground truth video sequences to obtain such subspaces. We extensively evaluate the disentanglement properties of motion subspaces on face and car datasets, quantitatively and qualitatively. Further, we present results for multiple downstream tasks such as motion editing, and selective motion transfer, e.g. transferring only facial expressions without training for it.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1669572692",
                        "name": "Rishubh Parihar"
                    },
                    {
                        "authorId": "2191087856",
                        "name": "Raghav Magazine"
                    },
                    {
                        "authorId": "7905854",
                        "name": "P. Tiwari"
                    },
                    {
                        "authorId": "144682140",
                        "name": "R. Venkatesh Babu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[52] searched for attribute development\u2019s potential path by performing principal component analysis operations between attributes and latent code."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3ff6ef61fb9fae57106d25aaec12705e6bbce6a0",
                "externalIds": {
                    "DBLP": "journals/tamd/NingXNZ0CLJ23",
                    "DOI": "10.1109/TCDS.2022.3182650",
                    "CorpusId": 249701813
                },
                "corpusId": 249701813,
                "publicationVenue": {
                    "id": "f35f148a-0a3c-45db-b610-3d89e09ddf21",
                    "name": "IEEE Transactions on Cognitive and Developmental Systems",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Cogn Dev Syst"
                    ],
                    "issn": "2379-8920",
                    "url": "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7274989"
                },
                "url": "https://www.semanticscholar.org/paper/3ff6ef61fb9fae57106d25aaec12705e6bbce6a0",
                "title": "Face Editing Based on Facial Recognition Features",
                "abstract": "Face editing generates a face image with the target attributes without changing the identity or other information. Current methods have achieved considerable performance; however, they cannot effectively retain the face\u2019s identity and semantic information while controlling the attribute intensity. Inspired by two human cognitive characteristics, namely, the principle of global precedence and the principle of homology continuity, we propose a novel face editing approach called the information retention and intensity control generative adversarial network (IricGAN). It includes a learnable hierarchical feature combination (HFC) function, which can construct a sample\u2019s source space through multiscale feature mixing; it can guarantee the integrity of the source space while significantly compressing the network. Additionally, the attribute regression module (ARM) can decouple different attribute paradigms in the source space to ensure the correct modification of the required attributes and preserve the other areas. The gradual process of modifying the face attributes can be simulated by applying different control strengths in the source space. In face editing experiments, both qualitative and quantitative results demonstrate that IricGAN achieves the best overall results among state-of-the-art alternatives. Target attributes can be continuously modified by refeeding the relationship of the source space and the image, and the independence of each attribute can be retained to the greatest extent. IricGAN:https://github.com/nanfangzhe/IricGAN.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "144866658",
                        "name": "X. Ning"
                    },
                    {
                        "authorId": "2110826402",
                        "name": "Shaohui Xu"
                    },
                    {
                        "authorId": "1632900621",
                        "name": "Fangzhe Nan"
                    },
                    {
                        "authorId": "1742304971",
                        "name": "Qingliang Zeng"
                    },
                    {
                        "authorId": "2170510574",
                        "name": "Chen Wang"
                    },
                    {
                        "authorId": "47414070",
                        "name": "Weiwei Cai"
                    },
                    {
                        "authorId": "2128482521",
                        "name": "Weijun Li"
                    },
                    {
                        "authorId": "2146420312",
                        "name": "Yizhang Jiang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "6a0fef32ac41d0ce66de7b0824110125fd44284a",
                "externalIds": {
                    "DBLP": "conf/cvpr/LiuWZGP023",
                    "DOI": "10.1109/CVPR52729.2023.02331",
                    "CorpusId": 260083660
                },
                "corpusId": 260083660,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6a0fef32ac41d0ce66de7b0824110125fd44284a",
                "title": "FlowGrad: Controlling the Output of Generative ODEs with Gradients",
                "abstract": "Generative modeling with ordinary differential equations (ODEs) has achieved fantastic results on a variety of applications. Yet, few works have focused on controlling the generated content of a pre-trained ODE-based generative model. In this paper, we propose to optimize the output of ODE models according to a guidance function to achieve controllable generation. We point out that, the gradients can be efficiently back-propagated from the output to any intermediate time steps on the ODE trajectory, by decomposing the back-propagation and computing vectorJacobian products. To further accelerate the computation of the back-propagation, we propose to use a non-uniform discretization to approximate the ODE trajectory, where we measure how straight the trajectory is and gather the straight parts into one discretization step. This allows us to save \u223c 90% of the back-propagation time with ignorable error. Our framework, named FlowGrad, outperforms the state-of-the-art baselines on text-guided image manipulation. Moreover, FlowGrad enables us to find global semantic directions in frozen ODE-based generative models that can be used to manipulate new images without extra optimization.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "46521757",
                        "name": "Xingchao Liu"
                    },
                    {
                        "authorId": "8687492",
                        "name": "Lemeng Wu"
                    },
                    {
                        "authorId": "2107944048",
                        "name": "Shujian Zhang"
                    },
                    {
                        "authorId": "29777869",
                        "name": "Chengyue Gong"
                    },
                    {
                        "authorId": "2056440915",
                        "name": "Wei Ping"
                    },
                    {
                        "authorId": "2165566517",
                        "name": "Qiang Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "66fa862339ec07c17ea75b741288a2becd3f1876",
                "externalIds": {
                    "DBLP": "conf/cvpr/PerezATAG23",
                    "DOI": "10.1109/CVPRW59228.2023.00037",
                    "CorpusId": 260815728
                },
                "corpusId": 260815728,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/66fa862339ec07c17ea75b741288a2becd3f1876",
                "title": "Towards Characterizing the Semantic Robustness of Face Recognition",
                "abstract": "Deep Neural Networks (DNNs) lack robustness against imperceptible perturbations to their input. Face Recognition Models (FRMs) based on DNNs inherit this vulnerability. We propose a methodology for assessing and characterizing the robustness of FRMs against semantic perturbations to their input. Our methodology causes FRMs to malfunction by designing adversarial attacks that search for identity-preserving modifications to faces. In particular, given a face, our attacks find identity-preserving variants of the face such that an FRM fails to recognize the images belonging to the same identity. We model these identity-preserving semantic modifications via direction- and magnitude-constrained perturbations in the latent space of StyleGAN. We further propose to characterize the semantic robustness of an FRM by statistically describing the perturbations that induce the FRM to malfunction. Finally, we combine our methodology with a certification technique, thus providing (i) theoretical guarantees on the performance of an FRM, and (ii) a formal description of how an FRM may model the notion of face identity.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "152978710",
                        "name": "Juan C. Perez"
                    },
                    {
                        "authorId": "92400655",
                        "name": "Motasem Alfarra"
                    },
                    {
                        "authorId": "35869086",
                        "name": "Ali K. Thabet"
                    },
                    {
                        "authorId": "1778133",
                        "name": "Pablo Arbel\u00e1ez"
                    },
                    {
                        "authorId": "2931652",
                        "name": "Bernard Ghanem"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "4c4175d502b999641b6cdd2aa1c6f2853564a2c0",
                "externalIds": {
                    "DBLP": "conf/cvpr/DoYKL023",
                    "DOI": "10.1109/CVPR52729.2023.00824",
                    "CorpusId": 261023164
                },
                "corpusId": 261023164,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4c4175d502b999641b6cdd2aa1c6f2853564a2c0",
                "title": "Quantitative Manipulation of Custom Attributes on 3D-Aware Image Synthesis",
                "abstract": "While 3D-based GAN techniques have been successfully applied to render photo-realistic 3D images with a variety of attributes while preserving view consistency, there has been little research on how to fine-control 3D images without limiting to a specific category of objects of their properties. To fill such research gap, we propose a novel image manipulation model of 3D-based GAN representations for a fine-grained control of specific custom attributes. By extending the latest 3D-based GAN models (e.g., EG3D), our user-friendly quantitative manipulation model enables a fine yet normalized control of 3D manipulation of multi-attribute quantities while achieving view consistency. We validate the effectiveness of our proposed technique both qualitatively and quantitatively through various experiments.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1978805583",
                        "name": "Hoseok Do"
                    },
                    {
                        "authorId": "2185324491",
                        "name": "Eunkyung Yoo"
                    },
                    {
                        "authorId": "46760211",
                        "name": "Taehyeong Kim"
                    },
                    {
                        "authorId": "2232563871",
                        "name": "Chul Lee"
                    },
                    {
                        "authorId": "2232594817",
                        "name": "Jin young Choi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To discover semantically meaningful latent directions without supervision, an effective approach was to perform principal component analysis on the latent vectors of the training images [13, 14]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "aed98730c4ac843e170109a89ee6d7101ce02e1b",
                "externalIds": {
                    "DBLP": "conf/cvpr/XieXX00W23",
                    "DOI": "10.1109/CVPR52729.2023.00542",
                    "CorpusId": 261081511
                },
                "corpusId": 261081511,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/aed98730c4ac843e170109a89ee6d7101ce02e1b",
                "title": "Blemish-aware and Progressive Face Retouching with Limited Paired Data",
                "abstract": "Face retouching aims to remove facial blemishes, while at the same time maintaining the textual details of a given input image. The main challenge lies in distinguishing blemishes from the facial characteristics, such as moles. Training an image-to-image translation network with pixel-wise supervision suffers from the problem of expensive paired training data, since professional retouching needs specialized experience and is time-consuming. In this paper, we propose a Blemish-aware and Progressive Face Retouching model, which is referred to as BPFRe. Our framework can be partitioned into two manageable stages to perform progressive blemish removal. Specifically, an encoder-decoder-based module learns to coarsely remove the blemishes at the first stage, and the resulting intermediate features are injected into a generator to enrich local detail at the second stage. We find that explicitly suppressing the blemishes can contribute to an effective collaboration among the components. Toward this end, we incorporate an attention module, which learns to infer a blemish-aware map and further determine the corresponding weights, which are then used to refine the intermediate features transferred from the encoder to the decoder, and from the decoder to the generator. Therefore, BPFRe is able to deliver significant performance gains on a wide range of face retouching tasks. It is worth noting that we reduce the dependence of BPFRe on paired training samples by imposing effective regularization on unpaired ones.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2088380299",
                        "name": "Lianxin Xie"
                    },
                    {
                        "authorId": "2233132337",
                        "name": "Wen Xue"
                    },
                    {
                        "authorId": "121832519",
                        "name": "Zhen Xu"
                    },
                    {
                        "authorId": "98566535",
                        "name": "Si Wu"
                    },
                    {
                        "authorId": "144861834",
                        "name": "Zhiwen Yu"
                    },
                    {
                        "authorId": "145892864",
                        "name": "H. Wong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent works [15,37,43] have discovered that the latent space of StyleGAN possesses semantic disentanglement properties, enabling a variety of image editing operations via latent transformations."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "1ccb7b761eb14055c94e907e3de551f67a2dfc03",
                "externalIds": {
                    "DBLP": "conf/cvpr/WeiXL00W23",
                    "DOI": "10.1109/CVPR52729.2023.01848",
                    "CorpusId": 261081322
                },
                "corpusId": 261081322,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1ccb7b761eb14055c94e907e3de551f67a2dfc03",
                "title": "Text-Guided Unsupervised Latent Transformation for Multi-Attribute Image Manipulation",
                "abstract": "Great progress has been made in StyleGAN-based image editing. To associate with preset attributes, most existing approaches focus on supervised learning for semantically meaningful latent space traversal directions, and each manipulation step is typically determined for an individual attribute. To address this limitation, we propose a Text-guided Unsupervised StyleGAN Latent Transformation (TUSLT) model, which adaptively infers a single transformation step in the latent space of StyleGAN to simultaneously manipulate multiple attributes on a given input image. Specifically, we adopt a two-stage architecture for a latent mapping network to break down the transformation process into two manageable steps. Our network first learns a diverse set of semantic directions tailored to an input image, and later nonlinearly fuses the ones associated with the target attributes to infer a residual vector. The resulting tightly interlinked two-stage architecture delivers the flexibility to handle diverse attribute combinations. By leveraging the cross-modal text-image representation of CLIP, we can perform pseudo annotations based on the semantic similarity between preset attribute text descriptions and training images, and further jointly train an auxiliary attribute classifier with the latent mapping network to provide semantic guidance. We perform extensive experiments to demonstrate that the adopted strategies contribute to the superior performance of TUSLT.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2123297243",
                        "name": "Xiwen Wei"
                    },
                    {
                        "authorId": "121832519",
                        "name": "Zhen Xu"
                    },
                    {
                        "authorId": "98694906",
                        "name": "Cheng Liu"
                    },
                    {
                        "authorId": "98566535",
                        "name": "Si Wu"
                    },
                    {
                        "authorId": "144861834",
                        "name": "Zhiwen Yu"
                    },
                    {
                        "authorId": "145892864",
                        "name": "H. Wong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Figure 3 shows editing results with GANSpace [6].",
                "The combination of GAN inversion [1\u20133, 5, 7, 14, 15, 25, 28, 29] and latent space editing [6, 16, 18] enables us to edit a wide range of image attributes such as aging, expression, and light condition, by applying editing operations [6, 16, 18] to inverted latent codes."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "d087db5c7d8fccb41fcf5c315d8f4da67d619841",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-00241",
                    "ArXiv": "2306.00241",
                    "DOI": "10.48550/arXiv.2306.00241",
                    "CorpusId": 258999574
                },
                "corpusId": 258999574,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d087db5c7d8fccb41fcf5c315d8f4da67d619841",
                "title": "Balancing Reconstruction and Editing Quality of GAN Inversion for Real Image Editing with StyleGAN Prior Latent Space",
                "abstract": "The exploration of the latent space in StyleGANs and GAN inversion exemplify impressive real-world image editing, yet the trade-off between reconstruction quality and editing quality remains an open problem. In this study, we revisit StyleGANs' hyperspherical prior $\\mathcal{Z}$ and $\\mathcal{Z}^+$ and integrate them into seminal GAN inversion methods to improve editing quality. Besides faithful reconstruction, our extensions achieve sophisticated editing quality with the aid of the StyleGAN prior. We project the real images into the proposed space to obtain the inverted codes, by which we then move along $\\mathcal{Z}^{+}$, enabling semantic editing without sacrificing image quality. Comprehensive experiments show that $\\mathcal{Z}^{+}$ can replace the most commonly-used $\\mathcal{W}$, $\\mathcal{W}^{+}$, and $\\mathcal{S}$ spaces while preserving reconstruction quality, resulting in reduced distortion of edited images.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2036954391",
                        "name": "Kai Katsumata"
                    },
                    {
                        "authorId": "2967089",
                        "name": "Duc Minh Vo"
                    },
                    {
                        "authorId": "2127734772",
                        "name": "Bei Liu"
                    },
                    {
                        "authorId": "48731103",
                        "name": "Hideki Nakayama"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Image synthesis has recently made significant advancements [1, 12, 22, 26, 39, 2, 11, 32]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "b34e36a9c8f7de7a4813c9c447dfa085d67cca11",
                "externalIds": {
                    "ArXiv": "2305.18676",
                    "DBLP": "journals/corr/abs-2305-18676",
                    "DOI": "10.48550/arXiv.2305.18676",
                    "CorpusId": 258967646
                },
                "corpusId": 258967646,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b34e36a9c8f7de7a4813c9c447dfa085d67cca11",
                "title": "LayerDiffusion: Layered Controlled Image Editing with Diffusion Models",
                "abstract": "Text-guided image editing has recently experienced rapid development. However, simultaneously performing multiple editing actions on a single image, such as background replacement and specific subject attribute changes, while maintaining consistency between the subject and the background remains challenging. In this paper, we propose LayerDiffusion, a semantic-based layered controlled image editing method. Our method enables non-rigid editing and attribute modification of specific subjects while preserving their unique characteristics and seamlessly integrating them into new backgrounds. We leverage a large-scale text-to-image model and employ a layered controlled optimization strategy combined with layered diffusion training. During the diffusion process, an iterative guidance strategy is used to generate a final image that aligns with the textual description. Experimental results demonstrate the effectiveness of our method in generating highly coherent images that closely align with the given textual description. The edited images maintain a high similarity to the features of the input image and surpass the performance of current leading image editing methods. LayerDiffusion opens up new possibilities for controllable image editing.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1770717667",
                        "name": "Pengzhi Li"
                    },
                    {
                        "authorId": "2219361267",
                        "name": "QInxuan Huang"
                    },
                    {
                        "authorId": "2152155111",
                        "name": "Yikang Ding"
                    },
                    {
                        "authorId": null,
                        "name": "Zhiheng Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, unsupervised approaches [18,19,20,21,22,23,24,25] attempt to find interpretable directions in a latent space using, e."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "b810b90710c63ae4eae89a5e8c5ea42ba791d842",
                "externalIds": {
                    "ArXiv": "2305.16759",
                    "DBLP": "journals/corr/abs-2305-16759",
                    "DOI": "10.48550/arXiv.2305.16759",
                    "CorpusId": 258947439
                },
                "corpusId": 258947439,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b810b90710c63ae4eae89a5e8c5ea42ba791d842",
                "title": "StyleHumanCLIP: Text-guided Garment Manipulation for StyleGAN-Human",
                "abstract": "This paper tackles text-guided control of StyleGAN for editing garments in full-body human images. Existing StyleGAN-based methods suffer from handling the rich diversity of garments and body shapes and poses. We propose a framework for text-guided full-body human image synthesis via an attention-based latent code mapper, which enables more disentangled control of StyleGAN than existing mappers. Our latent code mapper adopts an attention mechanism that adaptively manipulates individual latent codes on different StyleGAN layers under text guidance. In addition, we introduce feature-space masking at inference time to avoid unwanted changes caused by text inputs. Our quantitative and qualitative evaluations reveal that our method can control generated images more faithfully to given texts than existing methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2172551878",
                        "name": "Takato Yoshikawa"
                    },
                    {
                        "authorId": "2420042",
                        "name": "Yuki Endo"
                    },
                    {
                        "authorId": "2504432",
                        "name": "Yoshihiro Kanamori"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Specifically, we focused on a state-of-the-art (SOTA) unsupervised method - GANSpace (H\u00e4rk\u00f6nen et al., 2020), which utilizes Principal Component Analysis (PCA) (Pearson, 1901) to find orthogonal directions along which new semantic axes can be located.",
                "Specifically, we focused on a state-of-the-art (SOTA) unsupervised method - GANSpace (Ha\u0308rko\u0308nen et al., 2020), which utilizes Principal Component Analysis (PCA) (Pearson, 1901) to find orthogonal directions along which new semantic axes can be located.",
                "4 RESULTS AND DISCUSSION The examples of manipulations obtained by GANSpace (H\u00e4rk\u00f6nen et al., 2020) are shown in Fig.",
                "The examples of manipulations obtained by GANSpace (Ha\u0308rko\u0308nen et al., 2020) are shown in Fig.",
                "For StyleGAN2, as was done by Ha\u0308rko\u0308nen et al. (2020) we applied GANSpace in W latent space.",
                "In this work, we evaluated the transformations obtained by GANSpace (Ha\u0308rko\u0308nen et al., 2020) for both StyleGAN2 (Karras et al., 2020) and GAN from Liu et al. (2021) both visually and quantitatively.",
                "3 METHODS In this work, we evaluated the transformations obtained by GANSpace (H\u00e4rk\u00f6nen et al., 2020) for both StyleGAN2 (Karras et al."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "143f8ee84d2387493293eb3c90164ded38fb3efb",
                "externalIds": {
                    "ArXiv": "2305.14551",
                    "DBLP": "conf/iclr/PalaevLK23",
                    "DOI": "10.48550/arXiv.2305.14551",
                    "CorpusId": 258866118
                },
                "corpusId": 258866118,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/143f8ee84d2387493293eb3c90164ded38fb3efb",
                "title": "Exploring Semantic Variations in GAN Latent Spaces via Matrix Factorization",
                "abstract": "Controlled data generation with GANs is desirable but challenging due to the nonlinearity and high dimensionality of their latent spaces. In this work, we explore image manipulations learned by GANSpace, a state-of-the-art method based on PCA. Through quantitative and qualitative assessments we show: (a) GANSpace produces a wide range of high-quality image manipulations, but they can be highly entangled, limiting potential use cases; (b) Replacing PCA with ICA improves the quality and disentanglement of manipulations; (c) The quality of the generated images can be sensitive to the size of GANs, but regardless of their complexity, fundamental controlling directions can be observed in their latent spaces.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2218796960",
                        "name": "Andrey Palaev"
                    },
                    {
                        "authorId": "103695940",
                        "name": "R. Lukmanov"
                    },
                    {
                        "authorId": "34594177",
                        "name": "Adil Hamid Khan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[19] Erik H\u00e4rk\u00f6nen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris.",
                "We take inspiration from the related line of work that finds so-called \u2018interpretable directions\u2019 in latent space [18, 19] that capture high-level semantic attributes\u2013however, we learn subspaces that capture variation uniquely present amongst representations of words of particular parts of speech."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "470e12281fde486f2872f825528198384ff76702",
                "externalIds": {
                    "ArXiv": "2305.14053",
                    "DBLP": "journals/corr/abs-2305-14053",
                    "DOI": "10.48550/arXiv.2305.14053",
                    "CorpusId": 258841517
                },
                "corpusId": 258841517,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/470e12281fde486f2872f825528198384ff76702",
                "title": "Parts of Speech-Grounded Subspaces in Vision-Language Models",
                "abstract": "Latent image representations arising from vision-language models have proved immensely useful for a variety of downstream tasks. However, their utility is limited by their entanglement with respect to different visual attributes. For instance, recent work has shown that CLIP image representations are often biased toward specific visual properties (such as objects or actions) in an unpredictable manner. In this paper, we propose to separate representations of the different visual modalities in CLIP's joint vision-language space by leveraging the association between parts of speech and specific visual modes of variation (e.g. nouns relate to objects, adjectives describe appearance). This is achieved by formulating an appropriate component analysis model that learns subspaces capturing variability corresponding to a specific part of speech, while jointly minimising variability to the rest. Such a subspace yields disentangled representations of the different visual properties of an image or text in closed form while respecting the underlying geometry of the manifold on which the representations lie. What's more, we show the proposed model additionally facilitates learning subspaces corresponding to specific visual appearances (e.g. artists' painting styles), which enables the selective removal of entire visual themes from CLIP-based text-to-image synthesis. We validate the model both qualitatively, by visualising the subspace projections with a text-to-image model and by preventing the imitation of artists' styles, and quantitatively, through class invariance metrics and improvements to baseline zero-shot classification. Our code is available at: https://github.com/james-oldfield/PoS-subspaces.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2059960629",
                        "name": "James Oldfield"
                    },
                    {
                        "authorId": "1694090",
                        "name": "Christos Tzelepis"
                    },
                    {
                        "authorId": "1780393",
                        "name": "Yannis Panagakis"
                    },
                    {
                        "authorId": "1752913",
                        "name": "M. Nicolaou"
                    },
                    {
                        "authorId": "50058816",
                        "name": "I. Patras"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "7583325162eb98f5ae5106d165c7c737d239f511",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-11452",
                    "ArXiv": "2305.11452",
                    "DOI": "10.1109/CVPR52729.2023.00537",
                    "CorpusId": 258822908
                },
                "corpusId": 258822908,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7583325162eb98f5ae5106d165c7c737d239f511",
                "title": "ReDirTrans: Latent-to-Latent Translation for Gaze and Head Redirection",
                "abstract": "Learning-based gaze estimation methods require large amounts of training data with accurate gaze annotations. Facing such demanding requirements of gaze data collection and annotation, several image synthesis methods were proposed, which successfully redirected gaze directions pre-cisely given the assigned conditions. However, these methods focused on changing gaze directions of the images that only include eyes or restricted ranges of faces with low res-olution (less than $128\\times 128$) to largely reduce interference from other attributes such as hairs, which limits application scenarios. To cope with this limitation, we proposed a portable network, called ReDirTrans, achieving latent-to-latent translation for redirecting gaze directions and head orientations in an interpretable manner. ReDirTrans projects input latent vectors into aimed-attribute embed-dings only and redirects these embeddings with assigned pitch and yaw values. Then both the initial and edited embeddings are projected back (deprojected) to the initial latent space as residuals to modify the input latent vec-tors by subtraction and addition, representing old status re-moval and new status addition. The projection of aimed at-tributes only and subtraction-addition operations for status replacement essentially mitigate impacts on other attributes and the distribution of latent vectors. Thus, by combining ReDirTrans with a pretrained fixed e4e-StyleGAN pair, we created ReDirTrans-GAN, which enables accurately redi-recting gaze in full-face images with $1024\\times 1024$ resolution while preserving other attributes such as identity, expres-sion, and hairstyle. Furthermore, we presented improvements for the downstream learning-based gaze estimation task, using redirected samples as dataset augmentation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1877627581",
                        "name": "Shiwei Jin"
                    },
                    {
                        "authorId": "2118453342",
                        "name": "Zhen Wang"
                    },
                    {
                        "authorId": "2152510479",
                        "name": "Lei Wang"
                    },
                    {
                        "authorId": "2066769443",
                        "name": "N. Bi"
                    },
                    {
                        "authorId": "2116110034",
                        "name": "Truong Nguyen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other methods [24, 41, 44, 46] adopt principal component analysis or contrastive learning to explore unique editing directions in an unsupervised manner."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "470b640aacc23d885e52729f1e37fa6eb185074b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-10884",
                    "ArXiv": "2305.10884",
                    "DOI": "10.48550/arXiv.2305.10884",
                    "CorpusId": 258762325
                },
                "corpusId": 258762325,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/470b640aacc23d885e52729f1e37fa6eb185074b",
                "title": "Meta-Auxiliary Network for 3D GAN Inversion",
                "abstract": "Real-world image manipulation has achieved fantastic progress in recent years. GAN inversion, which aims to map the real image to the latent code faithfully, is the first step in this pipeline. However, existing GAN inversion methods fail to achieve high reconstruction quality and fast inference at the same time. In addition, existing methods are built on 2D GANs and lack explicitly mechanisms to enforce multi-view consistency.In this work, we present a novel meta-auxiliary framework, while leveraging the newly developed 3D GANs as generator. The proposed method adopts a two-stage strategy. In the first stage, we invert the input image to an editable latent code using off-the-shelf inversion techniques. The auxiliary network is proposed to refine the generator parameters with the given image as input, which both predicts offsets for weights of convolutional layers and sampling positions of volume rendering. In the second stage, we perform meta-learning to fast adapt the auxiliary network to the input image, then the final reconstructed image is synthesized via the meta-learned auxiliary network. Extensive experiments show that our method achieves better performances on both inversion and editing tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2190439237",
                        "name": "Bangrui Jiang"
                    },
                    {
                        "authorId": "2149505731",
                        "name": "Zhenhua Guo"
                    },
                    {
                        "authorId": "2108585311",
                        "name": "Yujiu Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other approaches compute the important semantic directions in the latent space in an unsupervised manner [H\u00e4rk\u00f6nen et al. 2020; Shen and Zhou 2020; Zhu et al. 2023]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "05b15934d837dc84afa96824742d3dcc7ec88e09",
                "externalIds": {
                    "ArXiv": "2305.10973",
                    "DBLP": "journals/corr/abs-2305-10973",
                    "DOI": "10.1145/3588432.3591500",
                    "CorpusId": 258762550
                },
                "corpusId": 258762550,
                "publicationVenue": {
                    "id": "cf6b5e76-9274-46e6-a2dd-7c190ec2ec5f",
                    "name": "International Conference on Computer Graphics and Interactive Techniques",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Graph Interact Tech",
                        "SIGGRAPH"
                    ],
                    "url": "http://www.siggraph.org/"
                },
                "url": "https://www.semanticscholar.org/paper/05b15934d837dc84afa96824742d3dcc7ec88e09",
                "title": "Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold",
                "abstract": "Synthesizing visual content that meets users\u2019 needs often requires flexible and precise controllability of the pose, shape, expression, and layout of the generated objects. Existing approaches gain controllability of generative adversarial networks (GANs) via manually annotated training data or a prior 3D model, which often lack flexibility, precision, and generality. In this work, we study a powerful yet much less explored way of controlling GANs, that is, to \"drag\" any points of the image to precisely reach target points in a user-interactive manner, as shown in Fig.1. To achieve this, we propose DragGAN, which consists of two main components: 1) a feature-based motion supervision that drives the handle point to move towards the target position, and 2) a new point tracking approach that leverages the discriminative generator features to keep localizing the position of the handle points. Through DragGAN, anyone can deform an image with precise control over where pixels go, thus manipulating the pose, shape, expression, and layout of diverse categories such as animals, cars, humans, landscapes, etc. As these manipulations are performed on the learned generative image manifold of a GAN, they tend to produce realistic outputs even for challenging scenarios such as hallucinating occluded content and deforming shapes that consistently follow the object\u2019s rigidity. Both qualitative and quantitative comparisons demonstrate the advantage of DragGAN over prior approaches in the tasks of image manipulation and point tracking. We also showcase the manipulation of real images through GAN inversion.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "14214933",
                        "name": "Xingang Pan"
                    },
                    {
                        "authorId": "9102722",
                        "name": "A. Tewari"
                    },
                    {
                        "authorId": "2422386",
                        "name": "Thomas Leimk\u00fchler"
                    },
                    {
                        "authorId": "46458089",
                        "name": "Lingjie Liu"
                    },
                    {
                        "authorId": "1953101",
                        "name": "Abhimitra Meka"
                    },
                    {
                        "authorId": "1680185",
                        "name": "C. Theobalt"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Note that in contrast to the original GANSpace algorithm [22], we do not apply PCA in the W-space.",
                "To this end, explore latent directions in an unsupervised way like [22], as we do not have additional supervision from synthetic data like [21] or attribute classifier networks like [24].",
                "Our work belongs to the last category (unsupervised latent exploration) and is based on GANSpace [22].",
                "For this, we first show the directions obtained by GANSpace [22] applied to StyleGAN2.",
                "In Figure 5, we also explore applying GANSpace in the W+-space of Urban-StyleGAN and in the S-space of different layers.",
                "Applying GANSpace in\nthe W+-space of the class \u2019car\u2019 gives only one meaningful direction of control (increasing car number and size on both sides).",
                "Note that in contrast to the original GANSpace algorithm [22], we do not apply PCA in the W+-space."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "43123aed732406ea77ecfd9e792f6e0b64637c85",
                "externalIds": {
                    "ArXiv": "2305.09602",
                    "DBLP": "journals/corr/abs-2305-09602",
                    "DOI": "10.1109/IV55152.2023.10186698",
                    "CorpusId": 258714933
                },
                "corpusId": 258714933,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/43123aed732406ea77ecfd9e792f6e0b64637c85",
                "title": "Urban-StyleGAN: Learning to Generate and Manipulate Images of Urban Scenes",
                "abstract": "A promise of Generative Adversarial Networks (GANs) is to provide cheap photorealistic data for training and validating AI models in autonomous driving. Despite their huge success, their performance on complex images featuring multiple objects is understudied. While some frameworks produce high-quality street scenes with little to no control over the image content, others offer more control at the expense of high-quality generation. A common limitation of both approaches is the use of global latent codes for the whole image, which hinders the learning of independent object distributions. Motivated by SemanticStyleGAN (SSG), a recent work on latent space disentanglement in human face generation, we propose a novel framework, Urban-StyleGAN, for urban scene generation and manipulation. We find that a straightforward application of SSG leads to poor results because urban scenes are more complex than human faces. To provide a more compact yet disentangled latent representation, we develop a class grouping strategy wherein individual classes are grouped into super-classes. Moreover, we employ an unsupervised latent exploration algorithm in the $\\mathcal{S}$-space of the generator and show that it is more efficient than the conventional ${\\mathcal{W}^ + }$-space in controlling the image content. Results on the Cityscapes and Mapillary datasets show the proposed approach achieves significantly more controllability and improved image quality than previous approaches on urban scenes and is on par with general-purpose non-controllable generative models (like StyleGAN2) in terms of quality.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3472033",
                        "name": "George Eskandar"
                    },
                    {
                        "authorId": "2161127613",
                        "name": "Youssef Farag"
                    },
                    {
                        "authorId": "1382635517",
                        "name": "Tarun Yenamandra"
                    },
                    {
                        "authorId": "1695302",
                        "name": "D. Cremers"
                    },
                    {
                        "authorId": "1388355954",
                        "name": "Karim Guirguis"
                    },
                    {
                        "authorId": "37606919",
                        "name": "B. Yang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "aeb0372f0cf0112f0053122a07b0bb7cf63f2a5c",
                "externalIds": {
                    "DBLP": "conf/cvpr/LattasMPGDZ23",
                    "ArXiv": "2305.09641",
                    "DOI": "10.1109/CVPR52729.2023.00834",
                    "CorpusId": 258714579
                },
                "corpusId": 258714579,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/aeb0372f0cf0112f0053122a07b0bb7cf63f2a5c",
                "title": "FitMe: Deep Photorealistic 3D Morphable Model Avatars",
                "abstract": "In this paper, we introduce FitMe, a facial reflectance model and a differentiable rendering optimization pipeline, that can be used to acquire high-fidelity renderable human avatars from single or multiple images. The model consists of a multi-modal style-based generator, that captures facial appearance in terms of diffuse and specular reflectance, and a PCA-based shape model. We employ a fast differentiable rendering process that can be used in an optimization pipeline, while also achieving photorealistic facial shading. Our optimization process accurately captures both the facial reflectance and shape in high-detail, by exploiting the expressivity of the style-based latent representation and of our shape model. FitMe achieves state-of-the-art reflectance acquisition and identity preservation on single \u201cin-the-wild\u201d facial images, while it produces impressive scan-like results, when given multiple unconstrained facial images pertaining to the same identity. In contrast with recent implicit avatar reconstructions, FitMe requires only one minute and produces relightable mesh and texture-based avatars, that can be used by end-user applications.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1602253142",
                        "name": "Alexandros Lattas"
                    },
                    {
                        "authorId": "24278037",
                        "name": "Stylianos Moschoglou"
                    },
                    {
                        "authorId": "2015036",
                        "name": "Stylianos Ploumpis"
                    },
                    {
                        "authorId": "2151914",
                        "name": "Baris Gecer"
                    },
                    {
                        "authorId": "3234063",
                        "name": "Jiankang Deng"
                    },
                    {
                        "authorId": "1776444",
                        "name": "S. Zafeiriou"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "12d37b36d7504a7275800f86eeea0f2eab6aa1ca",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-06671",
                    "ArXiv": "2305.06671",
                    "DOI": "10.48550/arXiv.2305.06671",
                    "CorpusId": 258615609
                },
                "corpusId": 258615609,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/12d37b36d7504a7275800f86eeea0f2eab6aa1ca",
                "title": "WeditGAN: Few-shot Image Generation via Latent Space Relocation",
                "abstract": "In few-shot image generation, directly training GAN models on just a handful of images faces the risk of overfitting. A popular solution is to transfer the models pretrained on large source domains to small target ones. In this work, we introduce WeditGAN, which realizes model transfer by editing the intermediate latent codes $w$ in StyleGANs with learned constant offsets ($\\Delta w$), discovering and constructing target latent spaces via simply relocating the distribution of source latent spaces. The established one-to-one mapping between latent spaces can naturally prevents mode collapse and overfitting. Besides, we also propose variants of WeditGAN to further enhance the relocation process by regularizing the direction or finetuning the intensity of $\\Delta w$. Experiments on a collection of widely used source/target datasets manifest the capability of WeditGAN in generating realistic and diverse images, which is simple yet highly effective in the research area of few-shot image generation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2210830546",
                        "name": "Yuxuan Duan"
                    },
                    {
                        "authorId": "1716055",
                        "name": "Li Niu"
                    },
                    {
                        "authorId": "47547671",
                        "name": "Y. Hong"
                    },
                    {
                        "authorId": "2108911758",
                        "name": "Liqing Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "7ee41430cefee39461016f3c7a647bd343b265c3",
                "externalIds": {
                    "DOI": "10.1117/12.2678914",
                    "CorpusId": 258639629
                },
                "corpusId": 258639629,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7ee41430cefee39461016f3c7a647bd343b265c3",
                "title": "RAIN: robust and adaptive image manipulation based on GAN inversion with a text condition",
                "abstract": "Recent work has shown various interesting semantic image manipulation methods based on GAN guided by text descriptions. A method based on GAN inversion can achieve versatile image manipulation functions without a time-consuming preprocessing stage. However, the method suffers from a lack of self-adaptation due to the intrinsic conflict between multi-objective losses. Meanwhile, the method applied in image manipulation guided by text conditions is not robust due to the vast and ambiguous search space. To solve the above problems, we propose a novel framework RAIN based on GAN inversion, which can achieve robust and adaptive text-driven image manipulation. As shown in Fig. 1(c), RAIN contains two main parts: CEV Initialization and RAGAN inversion. CEV Initialization can adaptively provide a Candidate Editing Vector (CEV) in a short time. RGAN inversion is a multi-stage optimization scheme utilizing the CEV as prior knowledge to prune search space. In RAGAN inversion, we explore how to improve the vision-language model's perception capability to restrict search space further. The objective of the paper is guaranteeing semantic correctness and image quality in a time-constrained scenario compared to the SOTA image manipulation methods guided by text descriptions. Extensive experiments show that RAIN can manipulate images guided by text description while meeting robustness and self-adaptation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "9446096",
                        "name": "Haoyu Cai"
                    },
                    {
                        "authorId": "2201901062",
                        "name": "L. Shang"
                    },
                    {
                        "authorId": "46350220",
                        "name": "Lei Gong"
                    },
                    {
                        "authorId": "2144446650",
                        "name": "Chao Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This is a fairly new area, and relevant to this line of work may be semantic adversarial approaches [8, 12, 17] which rely on generative models in order to generate adversarial examples, and [6] where interpretable controls are discovered for generative models."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "a74450ce1701c6c43f542946ea4bd6876c252bc6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-03212",
                    "ArXiv": "2305.03212",
                    "DOI": "10.48550/arXiv.2305.03212",
                    "CorpusId": 258547232
                },
                "corpusId": 258547232,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a74450ce1701c6c43f542946ea4bd6876c252bc6",
                "title": "LLM2Loss: Leveraging Language Models for Explainable Model Diagnostics",
                "abstract": "Trained on a vast amount of data, Large Language models (LLMs) have achieved unprecedented success and generalization in modeling fairly complex textual inputs in the abstract space, making them powerful tools for zero-shot learning. Such capability is extended to other modalities such as the visual domain using cross-modal foundation models such as CLIP, and as a result, semantically meaningful representation are extractable from visual inputs. In this work, we leverage this capability and propose an approach that can provide semantic insights into a model's patterns of failures and biases. Given a black box model, its training data, and task definition, we first calculate its task-related loss for each data point. We then extract a semantically meaningful representation for each training data point (such as CLIP embeddings from its visual encoder) and train a lightweight diagnosis model which maps this semantically meaningful representation of a data point to its task loss. We show that an ensemble of such lightweight models can be used to generate insights on the performance of the black-box model, in terms of identifying its patterns of failures and biases.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2599451",
                        "name": "Shervin Ardeshir"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a42b68b9fbd4f9c3382b54f1387547eb56d011ac",
                "externalIds": {
                    "DBLP": "conf/cvpr/Cazenavette00EZ23",
                    "ArXiv": "2305.01649",
                    "DOI": "10.1109/CVPR52729.2023.00364",
                    "CorpusId": 258437117
                },
                "corpusId": 258437117,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a42b68b9fbd4f9c3382b54f1387547eb56d011ac",
                "title": "Generalizing Dataset Distillation via Deep Generative Prior",
                "abstract": "Dataset Distillation aims to distill an entire dataset's knowledge into a few synthetic images. The idea is to synthe-size a small number of synthetic data points that, when given to a learning algorithm as training data, result in a model approximating one trained on the original data. Despite a recent upsurge of progress in the field, existing dataset dis-tillation methods fail to generalize to new architectures and scale to high-resolution datasets. To overcome the above issues, we propose to use the learned prior from pretrained deep generative models to synthesize the distilled data. To achieve this, we present a new optimization algorithm that distills a large number of images into a few intermediate feature vectors in the generative model's latent space. Our method augments existing techniques, significantly improving cross-architecture generalization in all settings.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "19296006",
                        "name": "George Cazenavette"
                    },
                    {
                        "authorId": "3978031",
                        "name": "Tongzhou Wang"
                    },
                    {
                        "authorId": "143805211",
                        "name": "A. Torralba"
                    },
                    {
                        "authorId": "1763086",
                        "name": "Alexei A. Efros"
                    },
                    {
                        "authorId": "2436356",
                        "name": "Jun-Yan Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "46e3b68212c8e1124673d10f5a36e8cc31279a73",
                "externalIds": {
                    "DBLP": "conf/siggraph/WangZSZ0YL23",
                    "ArXiv": "2305.00942",
                    "DOI": "10.1145/3588432.3591517",
                    "CorpusId": 258426596
                },
                "corpusId": 258426596,
                "publicationVenue": {
                    "id": "cf6b5e76-9274-46e6-a2dd-7c190ec2ec5f",
                    "name": "International Conference on Computer Graphics and Interactive Techniques",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Graph Interact Tech",
                        "SIGGRAPH"
                    ],
                    "url": "http://www.siggraph.org/"
                },
                "url": "https://www.semanticscholar.org/paper/46e3b68212c8e1124673d10f5a36e8cc31279a73",
                "title": "StyleAvatar: Real-time Photo-realistic Portrait Avatar from a Single Video",
                "abstract": "Face reenactment methods attempt to restore and re-animate portrait videos as realistically as possible. Existing methods face a dilemma in quality versus controllability: 2D GAN-based methods achieve higher image quality but suffer in fine-grained control of facial attributes compared with 3D counterparts. In this work, we propose StyleAvatar, a real-time photo-realistic portrait avatar reconstruction method using StyleGAN-based networks, which can generate high-fidelity portrait avatars with faithful expression control. We expand the capabilities of StyleGAN by introducing a compositional representation and a sliding window augmentation method, which enable faster convergence and improve translation generalization. Specifically, we divide the portrait scenes into three parts for adaptive adjustments: facial region, non-facial foreground region, and the background. Besides, our network leverages the best of UNet, StyleGAN and time coding for video learning, which enables high-quality video generation. Furthermore, a sliding window augmentation method together with a pre-training strategy are proposed to improve translation generalization and training performance, respectively. The proposed network can converge within two hours while ensuring high image quality and a forward rendering time of only 20 milliseconds. Furthermore, we propose a real-time live system, which further pushes research into applications. Results and experiments demonstrate the superiority of our method in terms of image quality, full portrait video generation, and real-time re-animation compared to existing facial reenactment methods. Training and inference code for this paper are at https://github.com/LizhenWangT/StyleAvatar.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108594877",
                        "name": "Lizhen Wang"
                    },
                    {
                        "authorId": "47040165",
                        "name": "Xiaochen Zhao"
                    },
                    {
                        "authorId": "2157278510",
                        "name": "Jingxiang Sun"
                    },
                    {
                        "authorId": "2108078811",
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "authorId": "49724467",
                        "name": "Hongwen Zhang"
                    },
                    {
                        "authorId": "1482570062",
                        "name": "Tao Yu"
                    },
                    {
                        "authorId": "1680777",
                        "name": "Yebin Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "ea8dcb91b50ea3536db78fa9651cf59ca0df3416",
                "externalIds": {
                    "DBLP": "journals/tcsv/YanZSM23",
                    "DOI": "10.1109/TCSVT.2022.3224190",
                    "CorpusId": 253913335
                },
                "corpusId": 253913335,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ea8dcb91b50ea3536db78fa9651cf59ca0df3416",
                "title": "Texture Brush for Fashion Inspiration Transfer: A Generative Adversarial Network With Heatmap-Guided Semantic Disentanglement",
                "abstract": "Automatically accomplishing intelligent fashion design with certain \u2018inspiration\u2019 images can greatly facilitate a designer\u2019s design process, as well as allow users to interactively participate in the process. In this research, we propose a generative adversarial network with heatmap-guided semantic disentanglement (HSD-GAN) to perform an \u2018intelligent\u2019 design with \u2018inspiration\u2019 transfer. Our model aims to learn how to integrate the feature representations, from the styles of both source fashion items and target fashion items, in an unsupervised manner. Specifically, a semantic disentanglement attention-based encoder is proposed to capture the most discriminative regions of different input fashion items and disentangle the features into two key factors: attribute and texture. A generator is then developed to synthesize mixed-style fashion items by utilizing the two factors. In addition, a heatmap-based patch loss is introduced to evaluate the visual-semantic matching degree between the texture of the generated fashion items and the input texture information. Extensive experimental results show that our proposed HSD-GAN consistently achieves superior performance, compared to other state-of-the-art methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152208647",
                        "name": "Han Yan"
                    },
                    {
                        "authorId": "49724537",
                        "name": "Haijun Zhang"
                    },
                    {
                        "authorId": "2153118892",
                        "name": "Jianyang Shi"
                    },
                    {
                        "authorId": "150152641",
                        "name": "Jianghong Ma"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "8a93d21c111e9aa78f96764b491c6c0659720829",
                "externalIds": {
                    "DBLP": "journals/tcsv/ZhouXNLT23",
                    "DOI": "10.1109/TCSVT.2022.3222456",
                    "CorpusId": 253618175
                },
                "corpusId": 253618175,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8a93d21c111e9aa78f96764b491c6c0659720829",
                "title": "HRInversion: High-Resolution GAN Inversion for Cross-Domain Image Synthesis",
                "abstract": "We investigate GAN inversion problems of using pre-trained GANs to reconstruct real images. Recent methods for such problems typically employ a VGG perceptual loss to measure the difference between images. While the perceptual loss has achieved remarkable success in various computer vision tasks, it may cause unpleasant artifacts and is sensitive to changes in input scale. This paper delivers an important message that algorithm details are crucial for achieving satisfying performance. In particular, we propose two important but undervalued design principles: (i) not down-sampling the input of the perceptual loss to avoid high-frequency artifacts; and (ii) calculating the perceptual loss using convolutional features which are robust to scale. Integrating these designs derives the proposed framework, HRInversion, that achieves superior performance in reconstructing image details. We validate the effectiveness of HRInversion on a cross-domain image synthesis task and propose a post-processing approach named local style optimization (LSO) to synthesize clean and controllable stylized images. For the evaluation of the cross-domain images, we introduce a metric named ID retrieval which captures the similarity of face identities of stylized images to content images. We also test HRInversion on non-square images. Equipped with implicit neural representation, HRInversion applies to ultra-high resolution images with more than 10 million pixels. Furthermore, we show applications of style transfer and 3D-aware GAN inversion, paving the way for extending the application range of HRInversion.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2113326303",
                        "name": "Peng Zhou"
                    },
                    {
                        "authorId": "3041937",
                        "name": "Lingxi Xie"
                    },
                    {
                        "authorId": "5796401",
                        "name": "Bingbing Ni"
                    },
                    {
                        "authorId": "2146017850",
                        "name": "Lin Liu"
                    },
                    {
                        "authorId": "1400120070",
                        "name": "Qi Tian"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "5cd1b108e30af67110e4177ac73b5fe5f2536075",
                "externalIds": {
                    "DBLP": "journals/jimaging/KorgialasPBSK23",
                    "PubMedCentral": "10219220",
                    "DOI": "10.3390/jimaging9050096",
                    "CorpusId": 258646038,
                    "PubMed": "37233315"
                },
                "corpusId": 258646038,
                "publicationVenue": {
                    "id": "c0fc53c7-b0ed-487d-9191-1262c8322621",
                    "name": "Journal of Imaging",
                    "type": "journal",
                    "alternate_names": [
                        "J Imaging"
                    ],
                    "issn": "2313-433X",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-556372",
                    "alternate_urls": [
                        "https://www.mdpi.com/journal/jimaging",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-556372"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5cd1b108e30af67110e4177ac73b5fe5f2536075",
                "title": "Face Aging by Explainable Conditional Adversarial Autoencoders",
                "abstract": "This paper deals with Generative Adversarial Networks (GANs) applied to face aging. An explainable face aging framework is proposed that builds on a well-known face aging approach, namely the Conditional Adversarial Autoencoder (CAAE). The proposed framework, namely, xAI-CAAE, couples CAAE with explainable Artificial Intelligence (xAI) methods, such as Saliency maps or Shapley additive explanations, to provide corrective feedback from the discriminator to the generator. xAI-guided training aims to supplement this feedback with explanations that provide a \u201creason\u201d for the discriminator\u2019s decision. Moreover, Local Interpretable Model-agnostic Explanations (LIME) are leveraged to provide explanations for the face areas that most influence the decision of a pre-trained age classifier. To the best of our knowledge, xAI methods are utilized in the context of face aging for the first time. A thorough qualitative and quantitative evaluation demonstrates that the incorporation of the xAI systems contributed significantly to the generation of more realistic age-progressed and regressed images.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2216427259",
                        "name": "Christos Korgialas"
                    },
                    {
                        "authorId": "3352401",
                        "name": "Evangelia Pantraki"
                    },
                    {
                        "authorId": "2217053106",
                        "name": "Angeliki Bolari"
                    },
                    {
                        "authorId": "2217053006",
                        "name": "Martha Sotiroudi"
                    },
                    {
                        "authorId": "103156940",
                        "name": "C. Kotropoulos"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Style-based generators [16, 17, 15] demonstrate the power of their semantic latent spaces, namely W , in image generation, image editing [11, 30], and video generation [33].",
                "We compute the style codes by simply adding these two different codes based on the linearity [11, 30] of the latent space W+.",
                "Furthermore, thanks to the continuous and linear nature of the latent space [16, 11, 30], we linearly manipulate the style codes using the audio input to generate lip-synced video frames.",
                "Second, the style codes form the continuous and linear [16, 11, 30] latent spaces, which enables us to design a high-level visual transformation, such as natural motion, only with a linear transformation of latent code [37]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "f8193b5e966b882dc5cf55a12dea6f4f39c1799b",
                "externalIds": {
                    "ArXiv": "2305.00521",
                    "DBLP": "journals/corr/abs-2305-00521",
                    "DOI": "10.48550/arXiv.2305.00521",
                    "CorpusId": 258426284
                },
                "corpusId": 258426284,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f8193b5e966b882dc5cf55a12dea6f4f39c1799b",
                "title": "StyleLipSync: Style-based Personalized Lip-sync Video Generation",
                "abstract": "In this paper, we present StyleLipSync, a style-based personalized lip-sync video generative model that can generate identity-agnostic lip-synchronizing video from arbitrary audio. To generate a video of arbitrary identities, we leverage expressive lip prior from the semantically rich latent space of a pre-trained StyleGAN, where we can also design a video consistency with a linear transformation. In contrast to the previous lip-sync methods, we introduce pose-aware masking that dynamically locates the mask to improve the naturalness over frames by utilizing a 3D parametric mesh predictor frame by frame. Moreover, we propose a few-shot lip-sync adaptation method for an arbitrary person by introducing a sync regularizer that preserves lips-sync generalization while enhancing the person-specific visual information. Extensive experiments demonstrate that our model can generate accurate lip-sync videos even with the zero-shot setting and enhance characteristics of an unseen face using a few seconds of target video through the proposed adaptation method. Please refer to our project page.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "31437502",
                        "name": "T. Ki"
                    },
                    {
                        "authorId": "2072376267",
                        "name": "Dong Min"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Various GAN-based editing methods rely on traversing the latent space to generate meaningful edits, highlighting the importance of the GAN inversion approach for achieving desired manipulations [66, 10, 29, 39, 14, 18, 53, 46, 47, 38, 20, 52, 48]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c4e8b8ae09ecfdd43c7548795dd69f194c064c15",
                "externalIds": {
                    "ArXiv": "2304.14403",
                    "DBLP": "journals/corr/abs-2304-14403",
                    "DOI": "10.48550/arXiv.2304.14403",
                    "CorpusId": 258352711
                },
                "corpusId": 258352711,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c4e8b8ae09ecfdd43c7548795dd69f194c064c15",
                "title": "Make It So: Steering StyleGAN for Any Image Inversion and Editing",
                "abstract": "StyleGAN's disentangled style representation enables powerful image editing by manipulating the latent variables, but accurately mapping real-world images to their latent variables (GAN inversion) remains a challenge. Existing GAN inversion methods struggle to maintain editing directions and produce realistic results. To address these limitations, we propose Make It So, a novel GAN inversion method that operates in the $\\mathcal{Z}$ (noise) space rather than the typical $\\mathcal{W}$ (latent style) space. Make It So preserves editing capabilities, even for out-of-domain images. This is a crucial property that was overlooked in prior methods. Our quantitative evaluations demonstrate that Make It So outperforms the state-of-the-art method PTI~\\cite{roich2021pivotal} by a factor of five in inversion accuracy and achieves ten times better edit quality for complex indoor scenes.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "35986726",
                        "name": "Anand Bhattad"
                    },
                    {
                        "authorId": "47330662",
                        "name": "Viraj Shah"
                    },
                    {
                        "authorId": "2433269",
                        "name": "Derek Hoiem"
                    },
                    {
                        "authorId": "144016256",
                        "name": "D. Forsyth"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several works have found that the latent spaces of StyleGAN are remarkably linear [18, 31, 17, 39], disentangling attributes such as facial expression, hair color, and pose."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "1f2e729e103f2832699e16f62a45b489a516bff6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-13681",
                    "ArXiv": "2304.13681",
                    "DOI": "10.48550/arXiv.2304.13681",
                    "CorpusId": 258332091
                },
                "corpusId": 258332091,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1f2e729e103f2832699e16f62a45b489a516bff6",
                "title": "Ray Conditioning: Trading Photo-consistency for Photo-realism in Multi-view Image Generation",
                "abstract": "Multi-view image generation attracts particular attention these days due to its promising 3D-related applications, e.g., image viewpoint editing. Most existing methods follow a paradigm where a 3D representation is first synthesized, and then rendered into 2D images to ensure photo-consistency across viewpoints. However, such explicit bias for photo-consistency sacrifices photo-realism, causing geometry artifacts and loss of fine-scale details when these methods are applied to edit real images. To address this issue, we propose ray conditioning, a geometry-free alternative that relaxes the photo-consistency constraint. Our method generates multi-view images by conditioning a 2D GAN on a light field prior. With explicit viewpoint control, state-of-the-art photo-realism and identity consistency, our method is particularly suited for the viewpoint editing task.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2113753693",
                        "name": "Eric Chen"
                    },
                    {
                        "authorId": "2215382561",
                        "name": "Sidhanth Holalkere"
                    },
                    {
                        "authorId": "2189225304",
                        "name": "Ruyu Yan"
                    },
                    {
                        "authorId": "2152981362",
                        "name": "Kai Zhang"
                    },
                    {
                        "authorId": "49935357",
                        "name": "A. Davis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Most straightforwardly, an early set of these approaches aimed to identify fixed linear directions in latent space and evolve samples along the discovered directions to create trajectories (H\u00e4rk\u00f6nen et al., 2020; Voynov & Babenko, 2020; Shen & Zhou, 2021).",
                "Most straightforwardly, an early set of these approaches aimed to identify fixed linear directions in latent space and evolve samples along the discovered directions to create trajectories (Ha\u0308rko\u0308nen et al., 2020; Voynov & Babenko, 2020; Shen & Zhou, 2021).",
                "By contrast, unsupervised methods discover interpretable directions without any prior knowledge (H\u00e4rk\u00f6nen et al., 2020; Kwon et al., 2023; Choi et al., 2022; Karmali et al., 2022; Spingarn-Eliezer et al., 2021; Ren et al., 2022; Oldfield et al., 2023).",
                "By contrast, unsupervised methods discover interpretable directions without any prior knowledge (Ha\u0308rko\u0308nen et al., 2020; Kwon et al., 2023; Choi et al., 2022; Karmali et al., 2022; Spingarn-Eliezer et al., 2021; Ren et al., 2022; Oldfield et al., 2023)."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "2f403d194b42d10c3a438736388c8812831b1361",
                "externalIds": {
                    "ArXiv": "2304.12944",
                    "DBLP": "conf/icml/SongKSW23",
                    "DOI": "10.48550/arXiv.2304.12944",
                    "CorpusId": 258309133
                },
                "corpusId": 258309133,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2f403d194b42d10c3a438736388c8812831b1361",
                "title": "Latent Traversals in Generative Models as Potential Flows",
                "abstract": "Despite the significant recent progress in deep generative models, the underlying structure of their latent spaces is still poorly understood, thereby making the task of performing semantically meaningful latent traversals an open research challenge. Most prior work has aimed to solve this challenge by modeling latent structures linearly, and finding corresponding linear directions which result in `disentangled' generations. In this work, we instead propose to model latent structures with a learned dynamic potential landscape, thereby performing latent traversals as the flow of samples down the landscape's gradient. Inspired by physics, optimal transport, and neuroscience, these potential landscapes are learned as physically realistic partial differential equations, thereby allowing them to flexibly vary over both space and time. To achieve disentanglement, multiple potentials are learned simultaneously, and are constrained by a classifier to be distinct and semantically self-consistent. Experimentally, we demonstrate that our method achieves both more qualitatively and quantitatively disentangled trajectories than state-of-the-art baselines. Further, we demonstrate that our method can be integrated as a regularization term during training, thereby acting as an inductive bias towards the learning of structured representations, ultimately improving model likelihood on similarly structured data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152601878",
                        "name": "Yue Song"
                    },
                    {
                        "authorId": "2215270642",
                        "name": "Andy Keller"
                    },
                    {
                        "authorId": "1429806753",
                        "name": "N. Sebe"
                    },
                    {
                        "authorId": "1678311",
                        "name": "M. Welling"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "1bed9b686924d1171efc52046515e14ac77ec027",
                "externalIds": {
                    "ArXiv": "2304.12539",
                    "DBLP": "journals/corr/abs-2304-12539",
                    "DOI": "10.48550/arXiv.2304.12539",
                    "CorpusId": 258309772
                },
                "corpusId": 258309772,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1bed9b686924d1171efc52046515e14ac77ec027",
                "title": "Text-guided Eyeglasses Manipulation with Spatial Constraints",
                "abstract": "Virtual try-on of eyeglasses involves placing eyeglasses of different shapes and styles onto a face image without physically trying them on. While existing methods have shown impressive results, the variety of eyeglasses styles is limited and the interactions are not always intuitive or efficient. To address these limitations, we propose a Text-guided Eyeglasses Manipulation method that allows for control of the eyeglasses shape and style based on a binary mask and text, respectively. Specifically, we introduce a mask encoder to extract mask conditions and a modulation module that enables simultaneous injection of text and mask conditions. This design allows for fine-grained control of the eyeglasses' appearance based on both textual descriptions and spatial constraints. Our approach includes a disentangled mapper and a decoupling strategy that preserves irrelevant areas, resulting in better local editing. We employ a two-stage training scheme to handle the different convergence speeds of the various modality conditions, successfully controlling both the shape and style of eyeglasses. Extensive comparison experiments and ablation analyses demonstrate the effectiveness of our approach in achieving diverse eyeglasses styles while preserving irrelevant areas.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2154572465",
                        "name": "Jiacheng Wang"
                    },
                    {
                        "authorId": "2113287145",
                        "name": "Ping Liu"
                    },
                    {
                        "authorId": "1800425",
                        "name": "Jingen Liu"
                    },
                    {
                        "authorId": "2155782544",
                        "name": "Wei Xu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Such directions can also be identified in an unsupervised fashion, using the PCA decomposition of the latent space [12], or the SVD of the first subsequent linear layer [37].",
                "When the latent space is disentangled as in StyleGAN [17,18], linear control is possible by carefully identifying and combining the latent directions of each attribute [12,36,37,50]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "e2da7924671d08d0957125d58887f914bcb5a17a",
                "externalIds": {
                    "DBLP": "conf/cvpr/ShiNLHLM23",
                    "ArXiv": "2304.12536",
                    "DOI": "10.1109/CVPRW59228.2023.00092",
                    "CorpusId": 258309719
                },
                "corpusId": 258309719,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e2da7924671d08d0957125d58887f914bcb5a17a",
                "title": "Exploring Compositional Visual Generation with Latent Classifier Guidance",
                "abstract": "Diffusion probabilistic models have achieved enormous success in the field of image generation and manipulation. In this paper, we explore a novel paradigm of using the diffusion model and classifier guidance in the latent semantic space for compositional visual tasks. Specifically, we train latent diffusion models and auxiliary latent classifiers to facilitate non-linear navigation of latent representation generation for any pre-trained generative model with a semantic latent space. We demonstrate that such conditional generation achieved by latent classifier guidance provably maximizes a lower bound of the conditional log probability during training. To maintain the original semantics during manipulation, we introduce a new guidance term, which we show is crucial for achieving compositionality. With additional assumptions, we show that the non-linear manipulation reduces to a simple latent arithmetic approach. We show that this paradigm based on latent classifier guidance is agnostic to pre-trained generative models, and present competitive results for both image generation and sequential manipulation of real and synthetic images. Our findings suggest that latent classifier guidance is a promising approach that merits further exploration, even in the presence of other strong competing methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2113917550",
                        "name": "Changhao Shi"
                    },
                    {
                        "authorId": "9612761",
                        "name": "Haomiao Ni"
                    },
                    {
                        "authorId": "3249631",
                        "name": "Kaican Li"
                    },
                    {
                        "authorId": "34288854",
                        "name": "Shaobo Han"
                    },
                    {
                        "authorId": "73445023",
                        "name": "Mingfu Liang"
                    },
                    {
                        "authorId": "5477477",
                        "name": "Martin Renqiang Min"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[11] proposed GANspace which performed PCA on early feature layers.",
                "Various works explored learning based [37, 54, 56], optimization based [1, 2, 14, 55], or hybrid [4, 60] approaches for GAN inversion, aiming to encode an image to the latent space of GANs.",
                "Ha\u0308rko\u0308nen et al. [11] proposed GANspace which performed PCA on early feature layers.",
                "Despite the various work studying the latent space of GANs, the latent space of diffusion models lack semantic meaning and cannot be easily applied for semantic manipulation.",
                "The StyleGAN [16] generator maps the random noise vector to a semantically meaningful latent space, inspiring various follow-up works exploring the controllability and interpretability of the latent space of GANs [14, 16, 27, 34, 38, 51, 55].",
                "There have been various works on learning visual representations and manipulating the latent space of GANs [11,44,45]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "fb69dd274948fdb1565abcdc2ad74fd9ef3e84f2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-11829",
                    "ArXiv": "2304.11829",
                    "DOI": "10.48550/arXiv.2304.11829",
                    "CorpusId": 258298850
                },
                "corpusId": 258298850,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fb69dd274948fdb1565abcdc2ad74fd9ef3e84f2",
                "title": "Hierarchical Diffusion Autoencoders and Disentangled Image Manipulation",
                "abstract": "Diffusion models have attained impressive visual quality for image synthesis. However, how to interpret and manipulate the latent space of diffusion models has not been extensively explored. Prior work diffusion autoencoders encode the semantic representations into a semantic latent code, which fails to reflect the rich information of details and the intrinsic feature hierarchy. To mitigate those limitations, we propose Hierarchical Diffusion Autoencoders (HDAE) that exploit the fine-grained-to-abstract and lowlevel-to-high-level feature hierarchy for the latent space of diffusion models. The hierarchical latent space of HDAE inherently encodes different abstract levels of semantics and provides more comprehensive semantic representations. In addition, we propose a truncated-feature-based approach for disentangled image manipulation. We demonstrate the effectiveness of our proposed approach with extensive experiments and applications on image reconstruction, style mixing, controllable interpolation, detail-preserving and disentangled image manipulation, and multi-modal semantic image synthesis.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2110087217",
                        "name": "Zeyu Lu"
                    },
                    {
                        "authorId": "46740411",
                        "name": "Chengyue Wu"
                    },
                    {
                        "authorId": "2130193247",
                        "name": "Xinyuan Chen"
                    },
                    {
                        "authorId": "2119049364",
                        "name": "Yaohui Wang"
                    },
                    {
                        "authorId": "145858545",
                        "name": "Y. Qiao"
                    },
                    {
                        "authorId": "46522599",
                        "name": "Xihui Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "ad24bdfa6b853266e6ab64af532d71c3ebf3b3e9",
                "externalIds": {
                    "DBLP": "conf/codaspy/LeC23",
                    "DOI": "10.1145/3577923.3583645",
                    "CorpusId": 258217217
                },
                "corpusId": 258217217,
                "publicationVenue": {
                    "id": "ebc05d3e-7ef6-450a-9496-e9f4f7118d49",
                    "name": "Conference on Data and Application Security and Privacy",
                    "type": "conference",
                    "alternate_names": [
                        "CODASPY",
                        "Conf Data Appl Secur Priv"
                    ],
                    "url": "http://www.codaspy.org/"
                },
                "url": "https://www.semanticscholar.org/paper/ad24bdfa6b853266e6ab64af532d71c3ebf3b3e9",
                "title": "IdDecoder: A Face Embedding Inversion Tool and its Privacy and Security Implications on Facial Recognition Systems",
                "abstract": "Most state-of-the-art facial recognition systems (FRS:s) use face embeddings. In this paper, we present the IdDecoder framework, capable of effectively synthesizing realistic-neutralized face images from face embeddings, and two effective attacks on state-of-the-art facial recognition models using embeddings. The first attack is a black-box version of a model inversion attack that allows the attacker to reconstruct a realistic face image that is both visually and numerically (as determined by the FRS:s) recognized as the same identity as the original face used to create a given face embedding. This attack raises significant privacy concerns regarding the membership of the gallery dataset of these systems and highlights the importance of both the people designing and deploying FRS:s paying greater attention to the protection of the face embeddings than currently done. The second attack is a novel attack that performs the model inversion, so to instead create the face of an alternative identity that is visually different from the original identity but has close identity distance (ensuring that it is recognized as being of the same identity). This attack increases the attacked system's false acceptance rate and raises significant security concerns. Finally, we use IdDecoder to visualize, evaluate, and provide insights into differences between three state-of-the-art facial embedding models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2057524923",
                        "name": "Minh-Ha Le"
                    },
                    {
                        "authorId": "1739691",
                        "name": "Niklas Carlsson"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c2003035b28cf852dd7c601b9d54e963c1c1c2b5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-10263",
                    "ArXiv": "2304.10263",
                    "DOI": "10.1109/CVPR52729.2023.00826",
                    "CorpusId": 258236149
                },
                "corpusId": 258236149,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c2003035b28cf852dd7c601b9d54e963c1c1c2b5",
                "title": "PREIM3D: 3D Consistent Precise Image Attribute Editing from a Single Image",
                "abstract": "We study the 3D-aware image attribute editing problem in this paper, which has wide applications in practice. Recent methods solved the problem by training a shared encoder to map images into a 3D generator's latent space or by per-image latent code optimization and then edited images in the latent space. Despite their promising results near the input view, they still suffer from the 3D inconsistency of produced images at large camera poses and imprecise image attribute editing, like affecting unspecified attributes during editing. For more efficient image inversion, we train a shared encoder for all images. To alleviate 3D inconsistency at large camera poses, we propose two novel methods, an alternating training scheme and a multi-view identity loss, to maintain 3D consistency and subject identity. As for imprecise image editing, we attribute the problem to the gap between the latent space of real images and that of generated images. We compare the latent space and inversion manifold of GAN models and demonstrate that editing in the inversion manifold can achieve better results in both quantitative and qualitative evaluations. Extensive experiments show that our method produces more 3D consistent images and achieves more precise image editing than previous work. Source code and pretrained models can be found on our project page: https://mybabyyh.github.io/Preim3D/.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2214856759",
                        "name": "Jianhui Li"
                    },
                    {
                        "authorId": "2143024922",
                        "name": "Jian-min Li"
                    },
                    {
                        "authorId": "2144616518",
                        "name": "Hao Zhang"
                    },
                    {
                        "authorId": "150301258",
                        "name": "Siyi Liu"
                    },
                    {
                        "authorId": "2145912320",
                        "name": "Zhengyi Wang"
                    },
                    {
                        "authorId": "9381483",
                        "name": "Zihao Xiao"
                    },
                    {
                        "authorId": "1864036526",
                        "name": "Kaiwen Zheng"
                    },
                    {
                        "authorId": "2146280251",
                        "name": "Jun Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 1) unsupervised methods that explore the semantics of generator to discover distinguishable directions [45, 49, 11] and 2) Supervised methods that use attribute labels to find meaningful latent path [44, 43, 60, 1]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "00cb67b8a8efb17d7101db912f360398416e57f4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-09463",
                    "ArXiv": "2304.09463",
                    "DOI": "10.48550/arXiv.2304.09463",
                    "CorpusId": 258213039
                },
                "corpusId": 258213039,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/00cb67b8a8efb17d7101db912f360398416e57f4",
                "title": "HyperStyle3D: Text-Guided 3D Portrait Stylization via Hypernetworks",
                "abstract": "Portrait stylization is a long-standing task enabling extensive applications. Although 2D-based methods have made great progress in recent years, real-world applications such as metaverse and games often demand 3D content. On the other hand, the requirement of 3D data, which is costly to acquire, significantly impedes the development of 3D portrait stylization methods. In this paper, inspired by the success of 3D-aware GANs that bridge 2D and 3D domains with 3D fields as the intermediate representation for rendering 2D images, we propose a novel method, dubbed HyperStyle3D, based on 3D-aware GANs for 3D portrait stylization. At the core of our method is a hyper-network learned to manipulate the parameters of the generator in a single forward pass. It not only offers a strong capacity to handle multiple styles with a single model, but also enables flexible fine-grained stylization that affects only texture, shape, or local part of the portrait. While the use of 3D-aware GANs bypasses the requirement of 3D data, we further alleviate the necessity of style images with the CLIP model being the stylization guidance. We conduct an extensive set of experiments across the style, attribute, and shape, and meanwhile, measure the 3D consistency. These experiments demonstrate the superior capability of our HyperStyle3D model in rendering 3D-consistent images in diverse styles, deforming the face shape, and editing various attributes.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2214712521",
                        "name": "Zhuo Chen"
                    },
                    {
                        "authorId": "2112692444",
                        "name": "Xudong Xu"
                    },
                    {
                        "authorId": "3460423",
                        "name": "Yichao Yan"
                    },
                    {
                        "authorId": "2110971823",
                        "name": "Ye Pan"
                    },
                    {
                        "authorId": "46995290",
                        "name": "Wenhan Zhu"
                    },
                    {
                        "authorId": "2110050420",
                        "name": "Wayne Wu"
                    },
                    {
                        "authorId": "144445937",
                        "name": "Bo Dai"
                    },
                    {
                        "authorId": "2182512007",
                        "name": "Xiaokang Yang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "fe1ea945eecb477194f4e6b7603b2892b93d7590",
                "externalIds": {
                    "DBLP": "conf/chi/DavisWJKKD23",
                    "DOI": "10.1145/3544549.3585644",
                    "CorpusId": 258217171
                },
                "corpusId": 258217171,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fe1ea945eecb477194f4e6b7603b2892b93d7590",
                "title": "Fashioning the Future: Unlocking the Creative Potential of Deep Generative Models for Design Space Exploration",
                "abstract": "This paper investigates the potential impact of deep generative models on the work of creative professionals, specifically focusing on fashion design. We argue that current generative modeling tools lack critical features that would make them useful creativity support tools, and introduce our own tool, generative.fashion1, which was designed with theoretical principles of design space exploration in mind. Through qualitative studies with fashion design apprentices, we demonstrate how generative.fashion supported both divergent and convergent thinking, and compare it with a state-of-the-art diffusion model, Stable Diffusion. In general, the apprentices preferred generative.fashion over Stable Diffusion, citing the features explicitly designed to support ideation. We conclude that the exploration and development of novel interfaces and interaction modalities that are theoretically aligned with principles of design space exploration is crucial for unlocking the creative potential of generative AI and advancing a new era of creativity.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "31720243",
                        "name": "R. Davis"
                    },
                    {
                        "authorId": "94398251",
                        "name": "Thiemo Wambsganss"
                    },
                    {
                        "authorId": "152889319",
                        "name": "Wei Jiang"
                    },
                    {
                        "authorId": "1390743685",
                        "name": "K. G. Kim"
                    },
                    {
                        "authorId": "2430247",
                        "name": "Tanja K\u00e4ser"
                    },
                    {
                        "authorId": "1799133",
                        "name": "P. Dillenbourg"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[6] have already demonstrated that PCA applied in feature space can produce interpretable controls for image synthesis."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "72b10c71c2074e34c9314d09c02fc366c761eb0c",
                "externalIds": {
                    "DBLP": "journals/cvm/PengWWYS23",
                    "DOI": "10.1007/s41095-022-0295-3",
                    "CorpusId": 249634852
                },
                "corpusId": 249634852,
                "publicationVenue": {
                    "id": "d2dfc02a-9028-4345-b6cf-556b76ac435b",
                    "name": "Computational Visual Media",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Vis Media"
                    ],
                    "issn": "2096-0433",
                    "url": "http://www.springer.com/41095",
                    "alternate_urls": [
                        "https://link.springer.com/journal/41095"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/72b10c71c2074e34c9314d09c02fc366c761eb0c",
                "title": "Unsupervised image translation with distributional semantics awareness",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2052306974",
                        "name": "Zhexi Peng"
                    },
                    {
                        "authorId": "2149698689",
                        "name": "He Wang"
                    },
                    {
                        "authorId": "143663883",
                        "name": "Y. Weng"
                    },
                    {
                        "authorId": "46286410",
                        "name": "Yin Yang"
                    },
                    {
                        "authorId": "34620893",
                        "name": "Tianjia Shao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Existing studies showed that linear perturbations along principal components of \u2207wg enable semantic editing, and such perturbation directions are often applicable over w \u223c pw (Ha\u0308rko\u0308nen et al., 2020; Zhu et al., 2021).",
                "Ha\u0308rko\u0308nen et al. (2020) apply principal component analysis on pw distribution and found semantically meaningful editing directions.",
                "Existing studies on semantic editing showed that Rdw consists of linear semantic dimensions (Ha\u0308rko\u0308nen et al., 2020; Zhu et al., 2021).",
                "Indeed, instead of local analysis on the Jacobian, (Ha\u0308rko\u0308nen et al., 2020) showed that principal component analysis directly on pw also reveals semantic dimensions."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "7494d53a4ddbc33620f35f3f8bcb3b10704c1eee",
                "externalIds": {
                    "DBLP": "conf/icml/NieKYR23",
                    "ArXiv": "2304.09752",
                    "DOI": "10.48550/arXiv.2304.09752",
                    "CorpusId": 258212639
                },
                "corpusId": 258212639,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7494d53a4ddbc33620f35f3f8bcb3b10704c1eee",
                "title": "Attributing Image Generative Models using Latent Fingerprints",
                "abstract": "Generative models have enabled the creation of contents that are indistinguishable from those taken from nature. Open-source development of such models raised concerns about the risks of their misuse for malicious purposes. One potential risk mitigation strategy is to attribute generative models via fingerprinting. Current fingerprinting methods exhibit a significant tradeoff between robust attribution accuracy and generation quality while lacking design principles to improve this tradeoff. This paper investigates the use of latent semantic dimensions as fingerprints, from where we can analyze the effects of design variables, including the choice of fingerprinting dimensions, strength, and capacity, on the accuracy-quality tradeoff. Compared with previous SOTA, our method requires minimum computation and is more applicable to large-scale models. We use StyleGAN2 and the latent diffusion model to demonstrate the efficacy of our method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2060225919",
                        "name": "Guangyu Nie"
                    },
                    {
                        "authorId": "2116705496",
                        "name": "C. Kim"
                    },
                    {
                        "authorId": "1784500",
                        "name": "Yezhou Yang"
                    },
                    {
                        "authorId": "2115242596",
                        "name": "Yi Ren"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "66aae048162bf15ffc020ebed77c4f6070c5abd6",
                "externalIds": {
                    "ArXiv": "2304.07169",
                    "DBLP": "journals/corr/abs-2304-07169",
                    "DOI": "10.48550/arXiv.2304.07169",
                    "CorpusId": 258170305
                },
                "corpusId": 258170305,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/66aae048162bf15ffc020ebed77c4f6070c5abd6",
                "title": "A Comparative Study on Generative Models for High Resolution Solar Observation Imaging",
                "abstract": "Solar activity is one of the main drivers of variability in our solar system and the key source of space weather phenomena that affect Earth and near Earth space. The extensive record of high resolution extreme ultraviolet (EUV) observations from the Solar Dynamics Observatory (SDO) offers an unprecedented, very large dataset of solar images. In this work, we make use of this comprehensive dataset to investigate capabilities of current state-of-the-art generative models to accurately capture the data distribution behind the observed solar activity states. Starting from StyleGAN-based methods, we uncover severe deficits of this model family in handling fine-scale details of solar images when training on high resolution samples, contrary to training on natural face images. When switching to the diffusion based generative model family, we observe strong improvements of fine-scale detail generation. For the GAN family, we are able to achieve similar improvements in fine-scale generation when turning to ProjectedGANs, which uses multi-scale discriminators with a pre-trained frozen feature extractor. We conduct ablation studies to clarify mechanisms responsible for proper fine-scale handling. Using distributed training on supercomputers, we are able to train generative models for up to 1024x1024 resolution that produce high quality samples indistinguishable to human experts, as suggested by the evaluation we conduct. We make all code, models and workflows used in this study publicly available at \\url{https://github.com/SLAMPAI/generative-models-for-highres-solar-images}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "40063601",
                        "name": "Mehdi Cherti"
                    },
                    {
                        "authorId": "2214520023",
                        "name": "Alexander Czernik"
                    },
                    {
                        "authorId": "3334280",
                        "name": "Stefan Kesselheim"
                    },
                    {
                        "authorId": "9857795",
                        "name": "F. Effenberger"
                    },
                    {
                        "authorId": "2191688",
                        "name": "J. Jitsev"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, StyleGAN-based approaches [1, 31, 19, 32, 20, 33, 28, 2, 10] edit the attributes of an image, such as smile and age, by obtaining an editing direction in the latent space."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "9ffcacab65191c300cb0c807d53fc1718110a833",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-07429",
                    "ArXiv": "2304.07429",
                    "DOI": "10.48550/arXiv.2304.07429",
                    "CorpusId": 258179645
                },
                "corpusId": 258179645,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9ffcacab65191c300cb0c807d53fc1718110a833",
                "title": "Identity Encoder for Personalized Diffusion",
                "abstract": "Many applications can benefit from personalized image generation models, including image enhancement, video conferences, just to name a few. Existing works achieved personalization by fine-tuning one model for each person. While being successful, this approach incurs additional computation and storage overhead for each new identity. Furthermore, it usually expects tens or hundreds of examples per identity to achieve the best performance. To overcome these challenges, we propose an encoder-based approach for personalization. We learn an identity encoder which can extract an identity representation from a set of reference images of a subject, together with a diffusion generator that can generate new images of the subject conditioned on the identity representation. Once being trained, the model can be used to generate images of arbitrary identities given a few examples even if the model hasn't been trained on the identity. Our approach greatly reduces the overhead for personalized image generation and is more applicable in many potential applications. Empirical results show that our approach consistently outperforms existing fine-tuning based approach in both image generation and reconstruction, and the outputs is preferred by users more than 95% of the time compared with the best performing baseline.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2118478414",
                        "name": "Yu-Chuan Su"
                    },
                    {
                        "authorId": "12009218",
                        "name": "Kelvin C. K. Chan"
                    },
                    {
                        "authorId": "1527095795",
                        "name": "Yandong Li"
                    },
                    {
                        "authorId": "145940718",
                        "name": "Yang Zhao"
                    },
                    {
                        "authorId": "2119078370",
                        "name": "Han-Ying Zhang"
                    },
                    {
                        "authorId": "40206014",
                        "name": "Boqing Gong"
                    },
                    {
                        "authorId": "3154495",
                        "name": "H. Wang"
                    },
                    {
                        "authorId": "34760532",
                        "name": "Xuhui Jia"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Follow-up works enable controllability by either adding conditioning input along with the sampled vectors as input (named \u201dConditional GAN\u201d) [35, 65] or manipulating the sampled vectors [82, 28, 105]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "aaaf02c44c94309ec476fc7d3df6b8acf6b7bb10",
                "externalIds": {
                    "ArXiv": "2304.06700",
                    "DBLP": "journals/corr/abs-2304-06700",
                    "DOI": "10.48550/arXiv.2304.06700",
                    "CorpusId": 258108230
                },
                "corpusId": 258108230,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/aaaf02c44c94309ec476fc7d3df6b8acf6b7bb10",
                "title": "Learning Controllable 3D Diffusion Models from Single-view Images",
                "abstract": "Diffusion models have recently become the de-facto approach for generative modeling in the 2D domain. However, extending diffusion models to 3D is challenging due to the difficulties in acquiring 3D ground truth data for training. On the other hand, 3D GANs that integrate implicit 3D representations into GANs have shown remarkable 3D-aware generation when trained only on single-view image datasets. However, 3D GANs do not provide straightforward ways to precisely control image synthesis. To address these challenges, We present Control3Diff, a 3D diffusion model that combines the strengths of diffusion models and 3D GANs for versatile, controllable 3D-aware image synthesis for single-view datasets. Control3Diff explicitly models the underlying latent distribution (optionally conditioned on external inputs), thus enabling direct control during the diffusion process. Moreover, our approach is general and applicable to any type of controlling input, allowing us to train it with the same diffusion objective without any auxiliary supervision. We validate the efficacy of Control3Diff on standard image generation benchmarks, including FFHQ, AFHQ, and ShapeNet, using various conditioning inputs such as images, sketches, and text prompts. Please see the project website (\\url{https://jiataogu.me/control3diff}) for video comparisons.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3016273",
                        "name": "Jiatao Gu"
                    },
                    {
                        "authorId": "2110024637",
                        "name": "Qingzhe Gao"
                    },
                    {
                        "authorId": "2443456",
                        "name": "Shuangfei Zhai"
                    },
                    {
                        "authorId": "2028246830",
                        "name": "Baoquan Chen"
                    },
                    {
                        "authorId": "46458089",
                        "name": "Lingjie Liu"
                    },
                    {
                        "authorId": "49158771",
                        "name": "J. Susskind"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[39] for GANs [34] and has found application beyond that [94, 21], to our knowledge its use for the purpose of identifying a basis for exposure minimization is novel."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "291d2797f1aea1c29e6083a3d440f5df4457fcf0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-05727",
                    "ArXiv": "2304.05727",
                    "DOI": "10.48550/arXiv.2304.05727",
                    "CorpusId": 258079107
                },
                "corpusId": 258079107,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/291d2797f1aea1c29e6083a3d440f5df4457fcf0",
                "title": "Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks",
                "abstract": "Explainable AI has become a popular tool for validating machine learning models. Mismatches between the explained model's decision strategy and the user's domain knowledge (e.g. Clever Hans effects) have also been recognized as a starting point for improving faulty models. However, it is less clear what to do when the user and the explanation agree. In this paper, we demonstrate that acceptance of explanations by the user is not a guarantee for a machine learning model to function well, in particular, some Clever Hans effects may remain undetected. Such hidden flaws of the model can nevertheless be mitigated, and we demonstrate this by contributing a new method, Explanation-Guided Exposure Minimization (EGEM), that preemptively prunes variations in the ML model that have not been the subject of positive explanation feedback. Experiments on natural image data demonstrate that our approach leads to models that strongly reduce their reliance on hidden Clever Hans strategies, and consequently achieve higher accuracy on new data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "4271465",
                        "name": "Lorenz Linhardt"
                    },
                    {
                        "authorId": "2113612432",
                        "name": "Klaus-Robert M\u00fcller"
                    },
                    {
                        "authorId": "144535526",
                        "name": "G. Montavon"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As a result, StyleGANs are being extensively used in various applications like face-editing [11, 42], video generation [47, 53], face reenactment [3], etc."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "9209ff89f3e579a104aba3206300dc0c1f5c0afd",
                "externalIds": {
                    "DBLP": "conf/cvpr/RangwaniBSKJB23",
                    "ArXiv": "2304.05866",
                    "DOI": "10.1109/CVPR52729.2023.00580",
                    "CorpusId": 258079238
                },
                "corpusId": 258079238,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9209ff89f3e579a104aba3206300dc0c1f5c0afd",
                "title": "NoisyTwins: Class-Consistent and Diverse Image Generation Through StyleGANs",
                "abstract": "StyleGANs are at the forefront of controllable image generation as they produce a latent space that is semantically disentangled, making it suitable for image editing and manipulation. However, the performance of StyleGANs severely degrades when trained via class-conditioning on large-scale long-tailed datasets. We find that one reason for degradation is the collapse of latents for each class in the $\\mathcal{W}$ latent space. With NoisyTwins, we first introduce an effective and inexpensive augmentation strategy for class embeddings, which then decorrelates the latents based on self-supervision in the $\\mathcal{W}$ space. This decorrelation mitigates collapse, ensuring that our method preserves intra-class diversity with class-consistency in image generation. We show the effectiveness of our approach on large-scale real-world long-tailed datasets of ImageNet-LT and iNaturalist 2019, where our method outperforms other methods by \u223c 19% on FID, establishing a new state-of-the-art.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "46224589",
                        "name": "Harsh Rangwani"
                    },
                    {
                        "authorId": "2214202686",
                        "name": "Lavish Bansal"
                    },
                    {
                        "authorId": "1571168324",
                        "name": "Kartik Sharma"
                    },
                    {
                        "authorId": "115373370",
                        "name": "Tejan Karmali"
                    },
                    {
                        "authorId": "2131639924",
                        "name": "Varun Jampani"
                    },
                    {
                        "authorId": "144682140",
                        "name": "R. Venkatesh Babu"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", so that user controls such as knobs and sliders have one primary (perceptual or semantic) effect [17, 18, 5]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "cdc3895192b54992f81c46b21052b30120637fcc",
                "externalIds": {
                    "ArXiv": "2304.04394",
                    "DBLP": "journals/corr/abs-2304-04394",
                    "DOI": "10.48550/arXiv.2304.04394",
                    "CorpusId": 258048814
                },
                "corpusId": 258048814,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cdc3895192b54992f81c46b21052b30120637fcc",
                "title": "Leveraging Neural Representations for Audio Manipulation",
                "abstract": "We investigate applying audio manipulations using pretrained neural network-based autoencoders as an alternative to traditional signal processing methods, since the former may provide greater semantic or perceptual organization. To establish the potential of this approach, we first establish if representations from these models encode information about manipulations. We carry out experiments and produce visualizations using representations from two different pretrained autoencoders. Our findings indicate that, while some information about audio manipulations is encoded, this information is both limited and encoded in a non-trivial way. This is supported by our attempts to visualize these representations, which demonstrated that trajectories of representations for common manipulations are typically nonlinear and content dependent, even for linear signal manipulations. As a result, it is not yet clear how these pretrained autoencoders can be used to manipulate audio signals, however, our results indicate this may be due to the lack of disentanglement with respect to common audio manipulations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "36696635",
                        "name": "Scott H. Hawley"
                    },
                    {
                        "authorId": "145010177",
                        "name": "C. Steinmetz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026al., 2021), which is able to infer the implicit cause-effect relationships between attributes using causal reasoning (Scho\u0308lkopf et al., 2021), and DisCo (Ha\u0308rko\u0308nen et al., 2020), that uses a contrastive learning approach to discover disentangled directions and learn disentangled representations.",
                "Examples of controllable generative models are CAGE (Mao et al., 2021), which is able to infer the implicit cause-effect relationships between attributes using causal reasoning (Scho\u0308lkopf et al., 2021), and DisCo (Ha\u0308rko\u0308nen et al., 2020), that uses a contrastive learning approach to discover disentangled directions and learn disentangled representations.",
                ", 2021), and DisCo (H\u00e4rk\u00f6nen et al., 2020), that uses a contrastive learning approach to discover disentangled directions and learn disentangled representations."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "289acac670a49757d49115bf085511b09a340d7f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-04103",
                    "ArXiv": "2304.04103",
                    "DOI": "10.48550/arXiv.2304.04103",
                    "CorpusId": 258049195
                },
                "corpusId": 258049195,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/289acac670a49757d49115bf085511b09a340d7f",
                "title": "TC-VAE: Uncovering Out-of-Distribution Data Generative Factors",
                "abstract": "Uncovering data generative factors is the ultimate goal of disentanglement learning. Although many works proposed disentangling generative models able to uncover the underlying generative factors of a dataset, so far no one was able to uncover OOD generative factors (i.e., factors of variations that are not explicitly shown on the dataset). Moreover, the datasets used to validate these models are synthetically generated using a balanced mixture of some predefined generative factors, implicitly assuming that generative factors are uniformly distributed across the datasets. However, real datasets do not present this property. In this work we analyse the effect of using datasets with unbalanced generative factors, providing qualitative and quantitative results for widely used generative models. Moreover, we propose TC-VAE, a generative model optimized using a lower bound of the joint total correlation between the learned latent representations and the input data. We show that the proposed model is able to uncover OOD generative factors on different datasets and outperforms on average the related baselines in terms of downstream disentanglement metrics.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2052095589",
                        "name": "Cristian Meo"
                    },
                    {
                        "authorId": "1996705",
                        "name": "Anirudh Goyal"
                    },
                    {
                        "authorId": "1681843",
                        "name": "J. Dauwels"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following GANSpace [20], we empirically set the first 8 layers of latents as pose latents (denoted as w), the remaining 8 layers of latents as shape and appearance latents.",
                "GANSpace [20] found that different layers in w\u2217 control different image attributes.",
                "Interpreting the latent representation of GANs [20, 32] has benefited a body of work disentangling various factors of generated objects in a 3D-controllable manner, e.",
                "We draw aspiration from GANSpace [20] and SeFa [32] to disentangle StyleGAN2."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "410469645334b02da4b8bd206f5ffe76c71a71cb",
                "externalIds": {
                    "DBLP": "conf/cvpr/LiLWMC23",
                    "ArXiv": "2304.03526",
                    "DOI": "10.1109/CVPR52729.2023.00040",
                    "CorpusId": 258041122
                },
                "corpusId": 258041122,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/410469645334b02da4b8bd206f5ffe76c71a71cb",
                "title": "Lift3D: Synthesize 3D Training Data by Lifting 2D GAN to 3D Generative Radiance Field",
                "abstract": "This work explores the use of 3D generative models to synthesize training data for 3D vision tasks. The key requirements of the generative models are that the generated data should be photorealistic to match the real-world scenarios, and the corresponding 3D attributes should be aligned with given sampling labels. However, we find that the recent NeRF-based 3D GANs hardly meet the above requirements due to their designed generation pipeline and the lack of explicit 3D supervision. In this work, we propose Lift3D, an inverted 2D-to-3D generation framework to achieve the data generation objectives. Lift3D has several merits compared to prior methods: (1) Unlike previous 3D GANs that the output resolution is fixed after training, Lift3D can generalize to any camera intrinsic with higher resolution and photorealistic output. (2) By lifting well-disentangled 2D GAN to 3D object NeRF, Lift3D provides explicit 3D information of generated objects, thus offering accurate 3D annotations for downstream tasks. We evaluate the effectiveness of our framework by augmenting autonomous driving datasets. Experimental results demonstrate that our data generation framework can effectively improve the performance of 3D object detectors. Code: len-li.github.io/lift3d-web",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2041715720",
                        "name": "Leheng Li"
                    },
                    {
                        "authorId": "2059872750",
                        "name": "Qing Lian"
                    },
                    {
                        "authorId": "2213771562",
                        "name": "Luozhou Wang"
                    },
                    {
                        "authorId": "2068605434",
                        "name": "Ningning Ma"
                    },
                    {
                        "authorId": "104375063",
                        "name": "Yingke Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "271490ea60a5f49ed030dba72c3d7bb8d00bc008",
                "externalIds": {
                    "DOI": "10.1145/3591358",
                    "CorpusId": 257955801
                },
                "corpusId": 257955801,
                "publicationVenue": {
                    "id": "bb2eb372-4df2-4181-9988-71aecb1dcc5e",
                    "name": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP)",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Multimedia Comput Commun Appl (TOMCCAP",
                        "ACM Transactions on Multimedia Computing, Communications, and Applications",
                        "ACM Trans Multimedia Comput Commun Appl"
                    ],
                    "issn": "1551-6857",
                    "url": "http://www.acm.org/tomccap/",
                    "alternate_urls": [
                        "http://tomccap.acm.org/",
                        "http://tomm.acm.org/",
                        "http://portal.acm.org/tomccap/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/271490ea60a5f49ed030dba72c3d7bb8d00bc008",
                "title": "Unsupervised Discovery and Manipulation of Continuous Disentangled Factors of Variation",
                "abstract": "Learning a disentangled representation of a distribution in a completely unsupervised way is a challenging task that has drawn attention recently. In particular, much focus has been put in separating factors of variation (i.e., attributes) within the latent code of a Generative Adversarial Network (GAN). Achieving that permits control of the presence or absence of those factors in the generated samples by simply editing a small portion of the latent code. Nevertheless, existing methods that perform very well in a noise-to-image setting often fail when dealing with a real data distribution, i.e., when the discovered attributes need to be applied to real images. However, some methods are able to extract and apply a style to a sample but struggle to maintain its content and identity, while others are not able to locally apply attributes and end up achieving only a global manipulation of the original image. In this article, we propose a completely (i.e., truly) unsupervised method that is able to extract a disentangled set of attributes from a data distribution and apply them to new samples from the same distribution by preserving their content. This is achieved by using an image-to-image GAN that maps an image and a random set of continuous attributes to a new image that includes those attributes. Indeed, these attributes are initially unknown and they are discovered during training by maximizing the mutual information between the generated samples and the attributes\u2019 vector. Finally, the obtained disentangled set of continuous attributes can be used to freely manipulate the input samples. We prove the effectiveness of our method over a series of datasets and show its application on various tasks, such as attribute editing, data augmentation, and style transfer.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "8841047",
                        "name": "Tomaso Fontanini"
                    },
                    {
                        "authorId": "2065706062",
                        "name": "Luca Donati"
                    },
                    {
                        "authorId": "1780349",
                        "name": "M. Bertozzi"
                    },
                    {
                        "authorId": "51382163",
                        "name": "A. Prati"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Pose Difference Range (FFHQ-P) [0,15) [15,30) [30,45) [45,60) [60,75) [75,90)",
                "\u2022 Pose Misalignment: When the yaw difference is between [15-30) or [30-45), a single checkmark or double checkmarks are used, respectively in Table 1."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b142d943734dce9db452ccf279418f67b1e0cc95",
                "externalIds": {
                    "DBLP": "conf/cvpr/KhwanmuangPSS23",
                    "ArXiv": "2304.02744",
                    "DOI": "10.1109/CVPR52729.2023.00832",
                    "CorpusId": 257985314
                },
                "corpusId": 257985314,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b142d943734dce9db452ccf279418f67b1e0cc95",
                "title": "StyleGAN Salon: Multi-View Latent Optimization for Pose-Invariant Hairstyle Transfer",
                "abstract": "Our paper seeks to transfer the hairstyle of a reference image to an input photo for virtual hair tryon. We target a variety of challenges scenarios, such as transforming a long hairstyle with bangs to a pixie cut, which requires removing the existing hair and inferring how the forehead would look, or transferring partially visible hair from a hat-wearing person in a different pose. Past solutions leverage StyleGAN for hallucinating any missing parts and producing a seamless face-hair composite through so-called GAN inversion or projection. However, there remains a challenge in controlling the hallucinations to accurately transfer hairstyle and preserve the face shape and identity of the input. To overcome this, we propose a multi-view optimization framework that uses two different views of reference composites to semantically guide occluded or ambiguous regions. Our optimization shares information between two poses, which allows us to produce high fidelity and realistic results from incomplete references. Our framework produces high-quality results and outperforms prior work in a user study that consists of significantly more challenging hair transfer scenarios than previously studied. Project page: https://stylegan-salon.github.io/.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2213740747",
                        "name": "Sasikarn Khwanmuang"
                    },
                    {
                        "authorId": "2052302131",
                        "name": "Pakkapon Phongthawee"
                    },
                    {
                        "authorId": "3430745",
                        "name": "Patsorn Sangkloy"
                    },
                    {
                        "authorId": "37016781",
                        "name": "Supasorn Suwajanakorn"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "1a6444fc0dd3c42b5058d635a502b7c869395159",
                "externalIds": {
                    "DBLP": "journals/chinaf/LiuWWZZ23",
                    "DOI": "10.1007/s11432-022-3679-0",
                    "CorpusId": 250918946
                },
                "corpusId": 250918946,
                "publicationVenue": {
                    "id": "0534c8a0-1226-4f5b-bcf6-a13a8dd1825e",
                    "name": "Science China Information Sciences",
                    "alternate_names": [
                        "Sci China Inf Sci"
                    ],
                    "issn": "1869-1919",
                    "url": "http://info.scichina.com/"
                },
                "url": "https://www.semanticscholar.org/paper/1a6444fc0dd3c42b5058d635a502b7c869395159",
                "title": "Survey on leveraging pre-trained generative adversarial networks for image editing and restoration",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "144474429",
                        "name": "Ming Liu"
                    },
                    {
                        "authorId": "2156252416",
                        "name": "Yuxiang Wei"
                    },
                    {
                        "authorId": "39637222",
                        "name": "Xiaohe Wu"
                    },
                    {
                        "authorId": "1724520",
                        "name": "W. Zuo"
                    },
                    {
                        "authorId": "1720539",
                        "name": "L. Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Since the well-designed StyleGAN [25] contains a semantically rich latent space, a commonly-used approach [43, 15, 44] is to explore and find \u201cwalking\u201d directions that control a specific attribute of interest."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "7f932a1c1ed097218c95925aa6afd1bf6cd248ac",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-00838",
                    "ArXiv": "2304.00838",
                    "DOI": "10.48550/arXiv.2304.00838",
                    "CorpusId": 257912544
                },
                "corpusId": 257912544,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7f932a1c1ed097218c95925aa6afd1bf6cd248ac",
                "title": "MetaHead: An Engine to Create Realistic Digital Head",
                "abstract": "Collecting and labeling training data is one important step for learning-based methods because the process is time-consuming and biased. For face analysis tasks, although some generative models can be used to generate face data, they can only achieve a subset of generation diversity, reconstruction accuracy, 3D consistency, high-fidelity visual quality, and easy editability. One recent related work is the graphics-based generative method, but it can only render low realism head with high computation cost. In this paper, we propose MetaHead, a unified and full-featured controllable digital head engine, which consists of a controllable head radiance field(MetaHead-F) to super-realistically generate or reconstruct view-consistent 3D controllable digital heads and a generic top-down image generation framework LabelHead to generate digital heads consistent with the given customizable feature labels. Experiments validate that our controllable digital head engine achieves the state-of-the-art generation visual quality and reconstruction accuracy. Moreover, the generated labeled data can assist real training data and significantly surpass the labeled data generated by graphics-based methods in terms of training effect.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2213358253",
                        "name": "Dingyun Zhang"
                    },
                    {
                        "authorId": "2187576723",
                        "name": "Chenglai Zhong"
                    },
                    {
                        "authorId": "8280113",
                        "name": "Yudong Guo"
                    },
                    {
                        "authorId": "2115399883",
                        "name": "Yang Hong"
                    },
                    {
                        "authorId": "2170672655",
                        "name": "Ju-yong Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The pioneering work StyleCLIP [18] adds a vector to the original image representation in the StyleGAN feature space, which is known to be well disentangled [3,7,26,30]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "0d234bdd8aa5365be04032f5d440f9d7a9e526b5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-00964",
                    "ArXiv": "2304.00964",
                    "DOI": "10.48550/arXiv.2304.00964",
                    "CorpusId": 257913931
                },
                "corpusId": 257913931,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0d234bdd8aa5365be04032f5d440f9d7a9e526b5",
                "title": "Robust Text-driven Image Editing Method that Adaptively Explores Directions in Latent Spaces of StyleGAN and CLIP",
                "abstract": "Automatic image editing has great demands because of its numerous applications, and the use of natural language instructions is essential to achieving flexible and intuitive editing as the user imagines. A pioneering work in text-driven image editing, StyleCLIP, finds an edit direction in the CLIP space and then edits the image by mapping the direction to the StyleGAN space. At the same time, it is difficult to tune appropriate inputs other than the original image and text instructions for image editing. In this study, we propose a method to construct the edit direction adaptively in the StyleGAN and CLIP spaces with SVM. Our model represents the edit direction as a normal vector in the CLIP space obtained by training a SVM to classify positive and negative images. The images are retrieved from a large-scale image corpus, originally used for pre-training StyleGAN, according to the CLIP similarity between the images and the text instruction. We confirmed that our model performed as well as the StyleCLIP baseline, whereas it allows simple inputs without increasing the computational time.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2212979940",
                        "name": "Tsuyoshi Baba"
                    },
                    {
                        "authorId": "2054143697",
                        "name": "Kosuke Nishida"
                    },
                    {
                        "authorId": "2006479562",
                        "name": "Kyosuke Nishida"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While researchers have found ways to make sense of latent space\u2019s underlying properties in generative models [4, 8], its hidden layers are not a part of the interaction in co-creation [7]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "fd9f52d6a1e0da81a12c6934f4be2daacf849583",
                "externalIds": {
                    "ArXiv": "2304.00266",
                    "DBLP": "journals/corr/abs-2304-00266",
                    "DOI": "10.48550/arXiv.2304.00266",
                    "CorpusId": 257913217
                },
                "corpusId": 257913217,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fd9f52d6a1e0da81a12c6934f4be2daacf849583",
                "title": "Hidden Layer Interaction: A Co-Creative Design Fiction for Generative Models",
                "abstract": "This paper presents a speculation on a fictive co-creation scenario that extends classical interaction patterns with generative models. While existing interfaces are restricted to the input and output layers, we suggest hidden layer interaction to extend the horizonal relation at play when co-creating with a generative model's design space. We speculate on applying feature visualization to manipulate neurons corresponding to features ranging from edges over textures to objects. By integrating visual representations of a neural network's hidden layers into co-creation, we aim to provide humans with a new means of interaction, contributing to a phenomenological account of the model's inner workings during generation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2161242023",
                        "name": "Imke Grabe"
                    },
                    {
                        "authorId": "35187206",
                        "name": "Jichen Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some of these techniques allow a user to edit a set of visual attributes found through unsupervised learning [13, 33], while others leverage supervised learning or pre-trained classifiers",
                "This provides the potential for the proposed framework to control a generative model\u2019s output using implicit feedback from users, rather than requiring them to explicitly quantify their preferences [13, 33, 1, 22, 34].",
                "To expand their capabilities, there is substantial interest in improving the controllability of these models, providing the end user the ability to modify the continuous attributes of output images [13, 33, 1, 22, 34]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "1f03c82f41224006c0c544f30fabf066f8705b21",
                "externalIds": {
                    "ArXiv": "2304.00185",
                    "DBLP": "journals/corr/abs-2304-00185",
                    "DOI": "10.48550/arXiv.2304.00185",
                    "CorpusId": 257913839
                },
                "corpusId": 257913839,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1f03c82f41224006c0c544f30fabf066f8705b21",
                "title": "PrefGen: Preference Guided Image Generation with Relative Attributes",
                "abstract": "Deep generative models have the capacity to render high fidelity images of content like human faces. Recently, there has been substantial progress in conditionally generating images with specific quantitative attributes, like the emotion conveyed by one's face. These methods typically require a user to explicitly quantify the desired intensity of a visual attribute. A limitation of this method is that many attributes, like how\"angry\"a human face looks, are difficult for a user to precisely quantify. However, a user would be able to reliably say which of two faces seems\"angrier\". Following this premise, we develop the $\\textit{PrefGen}$ system, which allows users to control the relative attributes of generated images by presenting them with simple paired comparison queries of the form\"do you prefer image $a$ or image $b$?\"Using information from a sequence of query responses, we can estimate user preferences over a set of image attributes and perform preference-guided image editing and generation. Furthermore, to make preference localization feasible and efficient, we apply an active query selection strategy. We demonstrate the success of this approach using a StyleGAN2 generator on the task of human face editing. Additionally, we demonstrate how our approach can be combined with CLIP, allowing a user to edit the relative intensity of attributes specified by text prompts. Code at https://github.com/helblazer811/PrefGen.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "153223021",
                        "name": "Alec Helbling"
                    },
                    {
                        "authorId": "1690427",
                        "name": "C. Rozell"
                    },
                    {
                        "authorId": "103548014",
                        "name": "Matthew R. O\u2019Shaughnessy"
                    },
                    {
                        "authorId": "2007290564",
                        "name": "Kion Fallah"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Unsupervised methods [18], [19], [20], [21], [22] adopt",
                "Unsupervised approaches [18], [19], [20], [21], [22] typically use classical unsupervised machine learning techniques, e.",
                "Unsupervised approaches [18], [19], [20] are inappropriate for this task as they focus on discovering interpretable latent semantics, instead of solving for the latent direction for the target attribute."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "fb363da9e128f45fec5b5bf6a73e2fabe04c1029",
                "externalIds": {
                    "DBLP": "journals/tcsv/LiuLDS23",
                    "DOI": "10.1109/TCSVT.2022.3213662",
                    "CorpusId": 252829781
                },
                "corpusId": 252829781,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fb363da9e128f45fec5b5bf6a73e2fabe04c1029",
                "title": "Towards Spatially Disentangled Manipulation of Face Images With Pre-Trained StyleGANs",
                "abstract": "Generative Adversarial Networks with style-based generators could successfully synthesize realistic images from input latent code. Moreover, recent studies have revealed that interpretable translations of generated images could be obtained by linearly traversing in the latent space. However, in most existing latent spaces, linear interpolation often leads to \u2018spatially entangled modification\u2019 in the manipulation result, which is undesirable in many real-world applications where local editing is required. To solve this problem, we propose to manipulate the latent code in the \u2018style space\u2019 and analyze its advantage in achieving spatial disentanglement. Furthermore, we point out the weakness of simply interpolating in the style space and propose \u2018Style Intervention\u2019, a lightweight optimization-based algorithm, to further improve the visual fidelity of manipulation results. The performance of our method is verified with the task of attribute editing on high-resolution face images. Both qualitative and quantitative results demonstrate the advantage of image translation in the style space and the effectiveness of our method on both real and synthetic images.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1860829",
                        "name": "Yunfan Liu"
                    },
                    {
                        "authorId": "2118912249",
                        "name": "Qi Li"
                    },
                    {
                        "authorId": "51162255",
                        "name": "Qiyao Deng"
                    },
                    {
                        "authorId": "1757186",
                        "name": "Zhenan Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In the latent space W defined asw \u2208W,Gmapping(z) = w, significant semantic directions have already been found [16], [17]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "20e92fff3b6f76300a266dda6c9e2174286b074e",
                "externalIds": {
                    "ArXiv": "2303.17222",
                    "DBLP": "journals/corr/abs-2303-17222",
                    "DOI": "10.48550/arXiv.2303.17222",
                    "CorpusId": 257834012
                },
                "corpusId": 257834012,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/20e92fff3b6f76300a266dda6c9e2174286b074e",
                "title": "LatentForensics: Towards lighter deepfake detection in the StyleGAN latent space",
                "abstract": "The classification of forged videos has been a challenge for the past few years. Deepfake classifiers can now reliably predict whether or not video frames have been tampered with. However, their performance is tied to both the dataset used for training and the analyst's computational power. We propose a deepfake classification method that operates in the latent space of a state-of-the-art generative adversarial network (GAN) trained on high-quality face images. The proposed method leverages the structure of the latent space of StyleGAN to learn a lightweight classification model. Experimental results on a standard dataset reveal that the proposed approach outperforms other state-of-the-art deepfake classification methods. To the best of our knowledge, this is the first study showing the interest of the latent space of StyleGAN for deepfake classification. Combined with other recent studies on the interpretation and manipulation of this latent space, we believe that the proposed approach can help in developing robust deepfake classification methods based on interpretable high-level properties of face images.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2212982393",
                        "name": "Matthieu Delmas"
                    },
                    {
                        "authorId": "3407502",
                        "name": "Amine Kacete"
                    },
                    {
                        "authorId": "51924600",
                        "name": "S. Paquelet"
                    },
                    {
                        "authorId": "1996023",
                        "name": "Simon Leglaive"
                    },
                    {
                        "authorId": "1697632",
                        "name": "R. S\u00e9guier"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Unsupervised techniques attempt to find interesting edits without labeled data [22, 24, 41, 49]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "350ba2d07cfe9112cbe35f8e7da1a7567b5ddf1f",
                "externalIds": {
                    "DBLP": "conf/cvpr/FruhstuckSXWT23",
                    "ArXiv": "2303.15893",
                    "DOI": "10.1109/CVPR52729.2023.00432",
                    "CorpusId": 257771697
                },
                "corpusId": 257771697,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/350ba2d07cfe9112cbe35f8e7da1a7567b5ddf1f",
                "title": "VIVE3D: Viewpoint-Independent Video Editing using 3D-Aware GANs",
                "abstract": "We introduce VIVE3D, a novel approach that extends the capabilities of image-based 3D GANs to video editing and is able to represent the input video in an identity-preserving and temporally consistent way. We propose two new building blocks. First, we introduce a novel GAN inversion technique specifically tailored to 3D GANs by jointly embedding multiple frames and optimizing for the camera parameters. Second, besides traditional semantic face edits (e.g. for age and expression), we are the first to demonstrate edits that show novel views of the head enabled by the inherent prop-erties of 3D GANs and our optical flow-guided compositing technique to combine the head with the background video. Our experiments demonstrate that VIVE3D generates high-fidelity face edits at consistent quality from a range of camera viewpoints which are composited with the original video in a temporally and spatially consistent manner.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2151790120",
                        "name": "Anna Fr\u00fchst\u00fcck"
                    },
                    {
                        "authorId": "91995183",
                        "name": "N. Sarafianos"
                    },
                    {
                        "authorId": "2762640",
                        "name": "Yuanlu Xu"
                    },
                    {
                        "authorId": "1798011",
                        "name": "Peter Wonka"
                    },
                    {
                        "authorId": "144139707",
                        "name": "Tony Tung"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "28ac6312b46c8341df9f8406d2417d16468346fe",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-15441",
                    "ArXiv": "2303.15441",
                    "DOI": "10.1109/CVPR52729.2023.01119",
                    "CorpusId": 257766385
                },
                "corpusId": 257766385,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/28ac6312b46c8341df9f8406d2417d16468346fe",
                "title": "Zero-Shot Model Diagnosis",
                "abstract": "When it comes to deploying deep vision models, the behavior of these systems must be explicable to ensure confidence in their reliability and fairness. A common approach to evaluate deep learning models is to build a labeled test set with attributes of interest and assess how well it performs. However, creating a balanced test set (i.e., one that is uniformly sampled over all the important traits) is often time-consuming, expensive, and prone to mistakes. The question we try to address is: can we evaluate the sensitivity of deep learning models to arbitrary visual attributes without an annotated test set? This paper argues the case that Zero-shot Model Diagnosis (ZOOM) is possible without the need for a test set nor labeling. To avoid the need for test sets, our system relies on a generative model and CLIP. The key idea is enabling the user to select a set of prompts (relevant to the problem) and our system will automatically search for semantic counterfactual images (i.e., synthesized images that flip the prediction in the case of a binary classifier) using the generative model. We evaluate several visual tasks (classification, key-point detection, and segmentation) in multiple visual domains to demonstrate the viability of our methodology. Extensive experiments demonstrate that our method is capable of producing counterfactual images and offering sensitivity analysis for model diagnosis without the need for a test set.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "32020492",
                        "name": "Jinqi Luo"
                    },
                    {
                        "authorId": "2156070663",
                        "name": "Zhaoning Wang"
                    },
                    {
                        "authorId": "114621402",
                        "name": "Chen Henry Wu"
                    },
                    {
                        "authorId": "145252513",
                        "name": "Dong Huang"
                    },
                    {
                        "authorId": "143867160",
                        "name": "F. D. L. Torre"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[27, 50, 28, 8, 68, 81, 63, 24, 17] There have been a number of works that renovate GANs focusing on styles."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ae165a3c5d31f7024e017b3902d97ac9959de2b8",
                "externalIds": {
                    "ArXiv": "2303.15403",
                    "DBLP": "journals/corr/abs-2303-15403",
                    "DOI": "10.48550/arXiv.2303.15403",
                    "CorpusId": 257766537
                },
                "corpusId": 257766537,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ae165a3c5d31f7024e017b3902d97ac9959de2b8",
                "title": "Training-free Style Transfer Emerges from h-space in Diffusion models",
                "abstract": "Diffusion models (DMs) synthesize high-quality images in various domains. However, controlling their generative process is still hazy because the intermediate variables in the process are not rigorously studied. Recently, StyleCLIP-like editing of DMs is found in the bottleneck of the U-Net, named $h$-space. In this paper, we discover that DMs inherently have disentangled representations for content and style of the resulting images: $h$-space contains the content and the skip connections convey the style. Furthermore, we introduce a principled way to inject content of one image to another considering progressive nature of the generative process. Briefly, given the original generative process, 1) the feature of the source content should be gradually blended, 2) the blended feature should be normalized to preserve the distribution, 3) the change of skip connections due to content injection should be calibrated. Then, the resulting image has the source content with the style of the original image just like image-to-image translation. Interestingly, injecting contents to styles of unseen domains produces harmonization-like style transfer. To the best of our knowledge, our method introduces the first training-free feed-forward style transfer only with an unconditional pretrained frozen generative network. The code is available at https://curryjung.github.io/DiffStyle/.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "72286913",
                        "name": "Jaeseok Jeong"
                    },
                    {
                        "authorId": "2182293854",
                        "name": "Mingi Kwon"
                    },
                    {
                        "authorId": "2847986",
                        "name": "Youngjung Uh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As demonstrated in [20, 29, 1, 56], StyleGAN2 can disentangle viewpoints in the early layers."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "19db96f8cdeaebbe3cb226d1e010d57f100274a9",
                "externalIds": {
                    "ArXiv": "2303.14706",
                    "DBLP": "journals/corr/abs-2303-14706",
                    "DOI": "10.48550/arXiv.2303.14706",
                    "CorpusId": 257767400
                },
                "corpusId": 257767400,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/19db96f8cdeaebbe3cb226d1e010d57f100274a9",
                "title": "BlobGAN-3D: A Spatially-Disentangled 3D-Aware Generative Model for Indoor Scenes",
                "abstract": "3D-aware image synthesis has attracted increasing interest as it models the 3D nature of our real world. However, performing realistic object-level editing of the generated images in the multi-object scenario still remains a challenge. Recently, a 2D GAN termed BlobGAN has demonstrated great multi-object editing capabilities on real-world indoor scene datasets. In this work, we propose BlobGAN-3D, which is a 3D-aware improvement of the original 2D BlobGAN. We enable explicit camera pose control while maintaining the disentanglement for individual objects in the scene by extending the 2D blobs into 3D blobs. We keep the object-level editing capabilities of BlobGAN and in addition allow flexible control over the 3D location of the objects in the scene. We test our method on real-world indoor datasets and show that our method can achieve comparable image quality compared to the 2D BlobGAN and other 3D-aware GAN baselines while being able to enable camera pose control and object-level editing in the challenging multi-object real-world scenarios.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2155616805",
                        "name": "Qian Wang"
                    },
                    {
                        "authorId": "2155345360",
                        "name": "Yiqun Wang"
                    },
                    {
                        "authorId": "2143447",
                        "name": "Michael Birsak"
                    },
                    {
                        "authorId": "1798011",
                        "name": "Peter Wonka"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "cea2eed901c2f6915fc0739bbff406a8b24bcdc7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-13515",
                    "ArXiv": "2303.13515",
                    "DOI": "10.1109/CVPR52729.2023.01999",
                    "CorpusId": 257687856
                },
                "corpusId": 257687856,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/cea2eed901c2f6915fc0739bbff406a8b24bcdc7",
                "title": "Persistent Nature: A Generative Model of Unbounded 3D Worlds",
                "abstract": "Despite increasingly realistic image quality, recent 3D image generative models often operate on 3D volumes of fixed extent with limited camera motions. We investigate the task of unconditionally synthesizing unbounded nature scenes, enabling arbitrarily large camera motion while maintaining a persistent 3D world model. Our scene representation consists of an extendable, planar scene layout grid, which can be rendered from arbitrary camera poses via a 3D decoder and volume rendering, and a panoramic sky-dome. Based on this representation, we learn a generative world model solely from single-view internet photos. Our method enables simulating long flights through 3D landscapes, while maintaining global scene consistency-for instance, returning to the starting point yields the same view of the scene. Our approach enables scene extrapolation beyond the fixed bounds of current 3D generative models, while also supporting a persistent, camera-independent world representation that stands in contrast to auto-regressive 3D prediction models. Our project page: https://chail.github.io/persistent-nature/.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51322829",
                        "name": "Lucy Chai"
                    },
                    {
                        "authorId": "2061556403",
                        "name": "Richard Tucker"
                    },
                    {
                        "authorId": "2145369560",
                        "name": "Zhengqi Li"
                    },
                    {
                        "authorId": "2094770",
                        "name": "Phillip Isola"
                    },
                    {
                        "authorId": "1830653",
                        "name": "Noah Snavely"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANs are known to encode the semantics of the training data in their latent space [1, 2, 3].",
                "Simply translating a latent code in a given direction can lead to the variation of a semantic attribute in the corresponding generated image [1, 2, 3, 12].",
                "Latent semantic directions can be extracted from the latent space without supervision by performing PCA [2] or by singular value decomposition on the weights of the pretrained GAN [3, 12]."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3bf5545e0541842ff02ff30fd165703664c35528",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-10508",
                    "ArXiv": "2304.10508",
                    "DOI": "10.48550/arXiv.2304.10508",
                    "CorpusId": 258236532
                },
                "corpusId": 258236532,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3bf5545e0541842ff02ff30fd165703664c35528",
                "title": "Wasserstein Loss for Semantic Editing in the Latent Space of GANs",
                "abstract": "The latent space of GANs contains rich semantics reflecting the training data. Different methods propose to learn edits in latent space corresponding to semantic attributes, thus allowing to modify generated images. Most supervised methods rely on the guidance of classifiers to produce such edits. However, classifiers can lead to out-of-distribution regions and be fooled by adversarial samples. We propose an alternative formulation based on the Wasserstein loss that avoids such problems, while maintaining performance on-par with classifier-based approaches. We demonstrate the effectiveness of our method on two datasets (digits and faces) using StyleGAN2.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2136376223",
                        "name": "Perla Doubinsky"
                    },
                    {
                        "authorId": "3468294",
                        "name": "N. Audebert"
                    },
                    {
                        "authorId": "1719698",
                        "name": "M. Crucianu"
                    },
                    {
                        "authorId": "2138418",
                        "name": "H. Borgne"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "78fa62f199bdb37f9856e0c64166d478672c504c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-11545",
                    "ArXiv": "2303.11545",
                    "DOI": "10.1109/CVPR52729.2023.01367",
                    "CorpusId": 257636852
                },
                "corpusId": 257636852,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/78fa62f199bdb37f9856e0c64166d478672c504c",
                "title": "Fix the Noise: Disentangling Source Feature for Controllable Domain Translation",
                "abstract": "Recent studies show strong generative performance in domain translation especially by using transfer learning techniques on the unconditional generator. However, the control between different domain features using a single model is still challenging. Existing methods often require additional models, which is computationally demanding and leads to unsatisfactory visual quality. In addition, they have restricted control steps, which prevents a smooth transition. In this paper, we propose a new approach for high-quality domain translation with better controllability. The key idea is to preserve source features within a disentangled subspace of a target feature space. This allows our method to smoothly control the degree to which it preserves source features while generating images from an entirely new domain using only a single model. Our extensive experiments show that the proposed method can produce more consistent and realistic images than previous works and maintain precise controllability over different levels of transformation. The code is available at LeeDongYeun/FixNoise.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2212116745",
                        "name": "Dongyeun Lee"
                    },
                    {
                        "authorId": "2163967832",
                        "name": "Jae Young Lee"
                    },
                    {
                        "authorId": "2129311595",
                        "name": "Doyeon Kim"
                    },
                    {
                        "authorId": "2149220512",
                        "name": "Jaehyun Choi"
                    },
                    {
                        "authorId": "8351571",
                        "name": "Jaejun Yoo"
                    },
                    {
                        "authorId": "1769295",
                        "name": "Junmo Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In the context of GANs [10], it was shown that the principal components of a collection of randomly sampled latent codes results in semantically interpretable editing direction.",
                "[10] found interpretable control directions in pretrained GANs by applying principal components of latent codes to appropriate layers of the generator.",
                "While there has been extensive research on finding disentangled editing directions in the latent space of unconditional GANs [1, 33, 10, 6, 35, 38, 24], comparatively little work has been done on this topic for unconditional DDMs.",
                "Semantic editing has been widely explored in GANs [33, 10, 6, 35, 38, 20, 24, 39, 43]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a64e9fe44051d93202853a43656def4b44f84883",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-11073",
                    "ArXiv": "2303.11073",
                    "DOI": "10.48550/arXiv.2303.11073",
                    "CorpusId": 257631803
                },
                "corpusId": 257631803,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a64e9fe44051d93202853a43656def4b44f84883",
                "title": "Discovering Interpretable Directions in the Semantic Latent Space of Diffusion Models",
                "abstract": "Denoising Diffusion Models (DDMs) have emerged as a strong competitor to Generative Adversarial Networks (GANs). However, despite their widespread use in image synthesis and editing applications, their latent space is still not as well understood. Recently, a semantic latent space for DDMs, coined `$h$-space', was shown to facilitate semantic image editing in a way reminiscent of GANs. The $h$-space is comprised of the bottleneck activations in the DDM's denoiser across all timesteps of the diffusion process. In this paper, we explore the properties of h-space and propose several novel methods for finding meaningful semantic directions within it. We start by studying unsupervised methods for revealing interpretable semantic directions in pretrained DDMs. Specifically, we show that global latent directions emerge as the principal components in the latent space. Additionally, we provide a novel method for discovering image-specific semantic directions by spectral analysis of the Jacobian of the denoiser w.r.t. the latent code. Next, we extend the analysis by finding directions in a supervised fashion in unconditional DDMs. We demonstrate how such directions can be found by relying on either a labeled data set of real images or by annotating generated samples with a domain-specific attribute classifier. We further show how to semantically disentangle the found direction by simple linear projection. Our approaches are applicable without requiring any architectural modifications, text-based guidance, CLIP-based optimization, or model fine-tuning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2140280426",
                        "name": "Ren\u00e9 Haas"
                    },
                    {
                        "authorId": "2136361448",
                        "name": "Inbar Huberman-Spiegelglas"
                    },
                    {
                        "authorId": "30415770",
                        "name": "Rotem Mulayoff"
                    },
                    {
                        "authorId": "1880407",
                        "name": "T. Michaeli"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "7b548ffcc2098c90af26e8a3f3130f6869cdcf8e",
                "externalIds": {
                    "DBLP": "conf/cvpr/SinghST23a",
                    "ArXiv": "2303.11424",
                    "DOI": "10.1109/CVPR52729.2023.00203",
                    "CorpusId": 257636751
                },
                "corpusId": 257636751,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7b548ffcc2098c90af26e8a3f3130f6869cdcf8e",
                "title": "Polynomial Implicit Neural Representations For Large Diverse Datasets",
                "abstract": "Implicit neural representations (INR) have gained significant popularity for signal and image representation for many end-tasks, such as superresolution, 3D modeling, and more. Most INR architectures rely on sinusoidal positional encoding, which accounts for high-frequency information in data. However, the finite encoding size restricts the model's representational power. Higher representational power is needed to go from representing a single given image to representing large and diverse datasets. Our approach addresses this gap by representing an image with a polynomial function and eliminates the need for positional encodings. Therefore, to achieve a progressively higher degree of polynomial representation, we use element-wise multiplications between features and affine-transformed coordinate locations after every ReLU layer. The proposed method is evaluated qualitatively and quantitatively on large datasets like ImageNet. The proposed Poly-INR model performs comparably to state-of-the-art generative models without any convolution, normalization, or self-attention layers, and with far fewer trainable parameters. With much fewer training parameters and higher representative power, our approach paves the way for broader adoption of INR models for generative modeling tasks in complex domains. The code is available at https://github.com/Rajhans0/Poly_INR",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "26248419",
                        "name": "Rajhans Singh"
                    },
                    {
                        "authorId": "144132989",
                        "name": "Ankita Shukla"
                    },
                    {
                        "authorId": "143655174",
                        "name": "P. Turaga"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "5690d0e664c78ad3eced65f60a0a7f04926a3425",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-10774",
                    "ArXiv": "2303.10774",
                    "DOI": "10.1109/CVPR52729.2023.00771",
                    "CorpusId": 257632446
                },
                "corpusId": 257632446,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5690d0e664c78ad3eced65f60a0a7f04926a3425",
                "title": "Cross-GAN Auditing: Unsupervised Identification of Attribute Level Similarities and Differences Between Pretrained Generative Models",
                "abstract": "Generative Adversarial Networks (GANs) are notoriously difficult to train especially for complex distributions and with limited data. This has driven the need for tools to audit trained networks in human intelligible format, for example, to identify biases or ensure fairness. Existing GAN audit tools are restricted to coarse-grained, modeldata comparisons based on summary statistics such as FID or recall. In this paper, we propose an alternative approach that compares a newly developed GAN against a prior baseline. To this end, we introduce Cross-GAN Auditing (xGA) that, given an established \u201creference\u201d GAN and a newly proposed \u201cclient\u201d GAN, jointly identifies intelligible attributes that are either common across both GANs, novel to the client GAN, or missing from the client GAN. This provides both users and model developers an intuitive assessment of similarity and differences between GANs. We introduce novel metrics to evaluate attribute-based GAN auditing approaches and use these metrics to demonstrate quantitatively that xGA outperforms baseline approaches. We also include qualitative results that illustrate the common, novel and missing attributes identified by xGA from GANs trained on a variety of image datasets 1",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2058026244",
                        "name": "Matthew Lyle Olson"
                    },
                    {
                        "authorId": "47130096",
                        "name": "Shusen Liu"
                    },
                    {
                        "authorId": "2860488",
                        "name": "Rushil Anirudh"
                    },
                    {
                        "authorId": "2064767378",
                        "name": "J. Thiagarajan"
                    },
                    {
                        "authorId": "145466013",
                        "name": "P. Bremer"
                    },
                    {
                        "authorId": "37535697",
                        "name": "Weng-Keen Wong"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "7eb2afee6823eebdf37f379a7eb1a2d1fb0a98a5",
                "externalIds": {
                    "ArXiv": "2303.09036",
                    "DBLP": "journals/corr/abs-2303-09036",
                    "DOI": "10.48550/arXiv.2303.09036",
                    "CorpusId": 257557618
                },
                "corpusId": 257557618,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7eb2afee6823eebdf37f379a7eb1a2d1fb0a98a5",
                "title": "Mimic3D: Thriving 3D-Aware GANs via 3D-to-2D Imitation",
                "abstract": "Generating images with both photorealism and multiview 3D consistency is crucial for 3D-aware GANs, yet existing methods struggle to achieve them simultaneously. Improving the photorealism via CNN-based 2D super-resolution can break the strict 3D consistency, while keeping the 3D consistency by learning high-resolution 3D representations for direct rendering often compromises image quality. In this paper, we propose a novel learning strategy, namely 3D-to-2D imitation, which enables a 3D-aware GAN to generate high-quality images while maintaining their strict 3D consistency, by letting the images synthesized by the generator's 3D rendering branch to mimic those generated by its 2D super-resolution branch. We also introduce 3D-aware convolutions into the generator for better 3D representation learning, which further improves the image generation quality. With the above strategies, our method reaches FID scores of 5.4 and 4.3 on FFHQ and AFHQ-v2 Cats, respectively, at 512x512 resolution, largely outperforming existing 3D-aware GANs using direct 3D rendering and coming very close to the previous state-of-the-art method that leverages 2D super-resolution. Project website: https://seanchenxy.github.io/Mimic3DWeb.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2143792314",
                        "name": "Xingyu Chen"
                    },
                    {
                        "authorId": "152710186",
                        "name": "Yu Deng"
                    },
                    {
                        "authorId": "2450889",
                        "name": "Baoyuan Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026explored the use of instance-specific augmentations obtained via GAN inversion (Xia et al., 2022; Huh et al., 2020; Zhu et al., 2016), which map original images into latent vectors that can be subsequently transformed to generate augmented images (Jahanian et al., 2020; H\u00e4rk\u00f6nen et al., 2020).",
                ", 2016), which map original images into latent vectors that can be subsequently transformed to generate augmented images (Jahanian et al., 2020; H\u00e4rk\u00f6nen et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "db738620b512a27f3b3f55964b53b68eaf7660f7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-09677",
                    "ArXiv": "2303.09677",
                    "DOI": "10.48550/arXiv.2303.09677",
                    "CorpusId": 257622945
                },
                "corpusId": 257622945,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/db738620b512a27f3b3f55964b53b68eaf7660f7",
                "title": "Instance-Conditioned GAN Data Augmentation for Representation Learning",
                "abstract": "Data augmentation has become a crucial component to train state-of-the-art visual representation models. However, handcrafting combinations of transformations that lead to improved performances is a laborious task, which can result in visually unrealistic samples. To overcome these limitations, recent works have explored the use of generative models as learnable data augmentation tools, showing promising results in narrow application domains, e.g., few-shot learning and low-data medical imaging. In this paper, we introduce a data augmentation module, called DA_IC-GAN, which leverages instance-conditioned GAN generations and can be used off-the-shelf in conjunction with most state-of-the-art training recipes. We showcase the benefits of DA_IC-GAN by plugging it out-of-the-box into the supervised training of ResNets and DeiT models on the ImageNet dataset, and achieving accuracy boosts up to between 1%p and 2%p with the highest capacity models. Moreover, the learnt representations are shown to be more robust than the baselines when transferred to a handful of out-of-distribution datasets, and exhibit increased invariance to variations of instance and viewpoints. We additionally couple DA_IC-GAN with a self-supervised training recipe and show that we can also achieve an improvement of 1%p in accuracy in some settings. With this work, we strengthen the evidence on the potential of learnable data augmentations to improve visual representation learning, paving the road towards non-handcrafted augmentations in model training.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "28898388",
                        "name": "Pietro Astolfi"
                    },
                    {
                        "authorId": "8742492",
                        "name": "Arantxa Casanova"
                    },
                    {
                        "authorId": "34602236",
                        "name": "Jakob Verbeek"
                    },
                    {
                        "authorId": "145467703",
                        "name": "Pascal Vincent"
                    },
                    {
                        "authorId": "1456285042",
                        "name": "Adriana Romero-Soriano"
                    },
                    {
                        "authorId": "3325894",
                        "name": "M. Drozdzal"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For text-guided image synthesis using GANs, text was usually used as condition.",
                "With image-caption data pairs, GANs were trained to generate samples by utilizing attention mechanisms or contrastive approaches [21, 22].",
                "Early works on image manipulation utilized GANs [18, 5, 19]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "756b38ef5db820de59d33297c2257222ad4fb722",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-08767",
                    "ArXiv": "2303.08767",
                    "DOI": "10.48550/arXiv.2303.08767",
                    "CorpusId": 257532912
                },
                "corpusId": 257532912,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/756b38ef5db820de59d33297c2257222ad4fb722",
                "title": "Highly Personalized Text Embedding for Image Manipulation by Stable Diffusion",
                "abstract": "Diffusion models have shown superior performance in image generation and manipulation, but the inherent stochasticity presents challenges in preserving and manipulating image content and identity. While previous approaches like DreamBooth and Textual Inversion have proposed model or latent representation personalization to maintain the content, their reliance on multiple reference images and complex training limits their practicality. In this paper, we present a simple yet highly effective approach to personalization using highly personalized (HiPer) text embedding by decomposing the CLIP embedding space for personalization and content manipulation. Our method does not require model fine-tuning or identifiers, yet still enables manipulation of background, texture, and motion with just a single image and target text. Through experiments on diverse target texts, we demonstrate that our approach produces highly personalized and complex semantic image edits across a wide range of tasks. We believe that the novel understanding of the text embedding space presented in this work has the potential to inspire further research across various tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2067994375",
                        "name": "Inhwa Han"
                    },
                    {
                        "authorId": "1418385702",
                        "name": "Serin Yang"
                    },
                    {
                        "authorId": "2078502528",
                        "name": "Taesung Kwon"
                    },
                    {
                        "authorId": "30547794",
                        "name": "Jong-Chul Ye"
                    }
                ]
            }
        },
        {
            "contexts": [
                "tions by principal component analysis [10], low-rank factorization [41] and closed-form factorization [28].",
                "This means that the StyleGAN editing vectors found in previous studies [26, 10, 28] can be directly applied to StyleGANEX for normal FoV face editing, e.",
                "Building upon StyleGAN, researchers have developed a range of face manipulation models [1, 23, 29, 26, 10, 28, 41, 37]."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "06aa2aa2a29f25437545478e7361744aa35a0419",
                "externalIds": {
                    "ArXiv": "2303.06146",
                    "DBLP": "journals/corr/abs-2303-06146",
                    "DOI": "10.48550/arXiv.2303.06146",
                    "CorpusId": 257482257
                },
                "corpusId": 257482257,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/06aa2aa2a29f25437545478e7361744aa35a0419",
                "title": "StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces",
                "abstract": "Recent advances in face manipulation using StyleGAN have produced impressive results. However, StyleGAN is inherently limited to cropped aligned faces at a fixed image resolution it is pre-trained on. In this paper, we propose a simple and effective solution to this limitation by using dilated convolutions to rescale the receptive fields of shallow layers in StyleGAN, without altering any model parameters. This allows fixed-size small features at shallow layers to be extended into larger ones that can accommodate variable resolutions, making them more robust in characterizing unaligned faces. To enable real face inversion and manipulation, we introduce a corresponding encoder that provides the first-layer feature of the extended StyleGAN in addition to the latent style code. We validate the effectiveness of our method using unaligned face inputs of various resolutions in a diverse set of face manipulation tasks, including facial attribute editing, super-resolution, sketch/mask-to-face translation, and face toonification.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2159711748",
                        "name": "Shuai Yang"
                    },
                    {
                        "authorId": "94106850",
                        "name": "Liming Jiang"
                    },
                    {
                        "authorId": "2117940996",
                        "name": "Ziwei Liu"
                    },
                    {
                        "authorId": "1717179",
                        "name": "Chen Change Loy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Unsupervised methods for finding interpretable axes in the generator have also been proposed (H\u00e4rk\u00f6nen et al. 2020; Voynov and Babenko 2020; Tzelepis, Tzimiropoulos, and Patras 2021; Shen and Zhou 2021; Wang and Ponce 2021)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "043d9c216755d36587f609838fb7498e7b1049de",
                "externalIds": {
                    "ArXiv": "2303.05699",
                    "CorpusId": 257482628
                },
                "corpusId": 257482628,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/043d9c216755d36587f609838fb7498e7b1049de",
                "title": "Feature Unlearning for Pre-trained GANs and VAEs",
                "abstract": "We tackle the problem of feature unlearning from a pre-trained image generative model: GANs and VAEs. Unlike a common unlearning task where an unlearning target is a subset of the training set, we aim to unlearn a specific feature, such as hairstyle from facial images, from the pre-trained generative models. As the target feature is only presented in a local region of an image, unlearning the entire image from the pre-trained model may result in losing other details in the remaining region of the image. To specify which features to unlearn, we collect randomly generated images that contain the target features. We then identify a latent representation corresponding to the target feature and then use the representation to fine-tune the pre-trained model. Through experiments on MNIST and CelebA datasets, we show that target features are successfully removed while keeping the fidelity of the original models. Further experiments with an adversarial attack show that the unlearned model is more robust under the presence of malicious parties.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2166791053",
                        "name": "Saemi Moon"
                    },
                    {
                        "authorId": "2047594257",
                        "name": "Seunghyuk Cho"
                    },
                    {
                        "authorId": "2145138660",
                        "name": "Dongwoo Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As first demonstrated by StyleCLIP [34], the requirements for large amounts of annotated data [25] and manual efforts [14, 44] were considerably alleviated.",
                "Desirable changes to attributes of interest were previously brought out by discovering the relevant channels [44] and curating principal components [14] either through manual inspection or otherwise driven by data-hungry attribute predictors."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ff47f8b31fc19dd1e46440271d3e205c65ae7070",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-05031",
                    "ArXiv": "2303.05031",
                    "DOI": "10.1109/CVPR52729.2023.01221",
                    "CorpusId": 257427187
                },
                "corpusId": 257427187,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ff47f8b31fc19dd1e46440271d3e205c65ae7070",
                "title": "CoralStyleCLIP: Co-optimized Region and Layer Selection for Image Editing",
                "abstract": "Edit fidelity is a significant issue in open-world controllable generative image editing. Recently, CLIP-based approaches have traded off simplicity to alleviate these problems by introducing spatial attention in a handpicked layer of a StyleGAN. In this paper, we propose CoralStyleCLIP, which incorporates a multi-layer attention-guided blending strategy in the feature space of StyleGAN2 for obtaining high-fidelity edits. We propose multiple forms of our co-optimized region and layer selection strategy to demonstrate the variation of time complexity with the quality of edits over different architectural intricacies while preserving simplicity. We conduct extensive experimental analysis and benchmark our method against state-of-the-art CLIP-based methods. Our findings suggest that CoralStyleCLIP results in high-quality edits while preserving the ease of use.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1631728549",
                        "name": "Ambareesh Revanur"
                    },
                    {
                        "authorId": "3186608",
                        "name": "D. Basu"
                    },
                    {
                        "authorId": "8811200",
                        "name": "Shradha Agrawal"
                    },
                    {
                        "authorId": "2057234427",
                        "name": "Dhwanit Agarwal"
                    },
                    {
                        "authorId": "33893997",
                        "name": "Deepak Pai"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Notably, StyleGAN-family models [32,34] have shown impressive ability in image synthesis tasks for single-category domains [1, 25, 49, 70, 84]."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0930fe943d7d9b5bb943613d87c4ca92850dd43a",
                "externalIds": {
                    "DBLP": "conf/cvpr/KangZ0PSPP23",
                    "ArXiv": "2303.05511",
                    "DOI": "10.1109/CVPR52729.2023.00976",
                    "CorpusId": 257427461
                },
                "corpusId": 257427461,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0930fe943d7d9b5bb943613d87c4ca92850dd43a",
                "title": "Scaling up GANs for Text-to-Image Synthesis",
                "abstract": "The recent success of text-to-image synthesis has taken the world by storm and captured the general public's imagination. From a technical standpoint, it also marked a drastic change in the favored architecture to design generative image models. GANs used to be the de facto choice, with techniques like StyleGAN. With DALL.E 2, autoregressive and diffusion models became the new standard for large-scale generative models overnight. This rapid shift raises a fundamental question: can we scale up GANs to benefit from large datasets like LAION? We find that na\u00efvely increasing the capacity of the StyleGan architecture quickly becomes unstable. We introduce GigaGAN, a new GAN architecture that far exceeds this limit, demonstrating GANs as a viable option for text-to-image synthesis. GigaGAN offers three major advantages. First, it is orders of magnitude faster at inference time, taking only 0.13 seconds to synthesize a 512px image. Second, it can synthesize high-resolution images, for example, 16-megapixel images in 3.66 seconds. Finally, GigaGAN supports various latent space editing applications such as latent interpolation, style mixing, and vector arithmetic operations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2153110145",
                        "name": "Minguk Kang"
                    },
                    {
                        "authorId": "1922024303",
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "authorId": "2109976035",
                        "name": "Richard Zhang"
                    },
                    {
                        "authorId": "2109193504",
                        "name": "Jaesik Park"
                    },
                    {
                        "authorId": "2177801",
                        "name": "Eli Shechtman"
                    },
                    {
                        "authorId": "145799132",
                        "name": "Sylvain Paris"
                    },
                    {
                        "authorId": "2071929129",
                        "name": "Taesung Park"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "8cb08ec85ab454f9564b7f906e4d84e665ec2443",
                "externalIds": {
                    "ArXiv": "2303.05102",
                    "DBLP": "journals/corr/abs-2303-05102",
                    "DOI": "10.1016/j.imavis.2023.104808",
                    "CorpusId": 257427459
                },
                "corpusId": 257427459,
                "publicationVenue": {
                    "id": "6cc36eeb-d056-42c4-a306-7bcb239cc442",
                    "name": "Image and Vision Computing",
                    "type": "journal",
                    "alternate_names": [
                        "Image Vis Comput"
                    ],
                    "issn": "0262-8856",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525443/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/02628856",
                        "https://www.journals.elsevier.com/image-and-vision-computing"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8cb08ec85ab454f9564b7f906e4d84e665ec2443",
                "title": "StyleDiff: Attribute Comparison Between Unlabeled Datasets in Latent Disentangled Space",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1677093614",
                        "name": "Keisuke Kawano"
                    },
                    {
                        "authorId": "34687538",
                        "name": "Takuro Kutsuna"
                    },
                    {
                        "authorId": "3450697",
                        "name": "Ryoko Tokuhisa"
                    },
                    {
                        "authorId": "2187455748",
                        "name": "Akihiro Nakamura"
                    },
                    {
                        "authorId": "101368663",
                        "name": "Yasushi Esaki"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e460a44ca1c85187590daa2bfd04bf7f268993e8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-09454",
                    "ArXiv": "2305.09454",
                    "DOI": "10.48550/arXiv.2305.09454",
                    "CorpusId": 258714970
                },
                "corpusId": 258714970,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e460a44ca1c85187590daa2bfd04bf7f268993e8",
                "title": "Rethinking the editing of generative adversarial networks: a method to estimate editing vectors based on dimension reduction",
                "abstract": "While Generative Adversarial Networks (GANs) have recently found applications in image editing, most previous GAN-based image editing methods require largescale datasets with semantic segmentation annotations for training, only provide high level control, or merely interpolate between different images. Previous researchers have proposed EditGAN for high-quality, high-precision semantic image editing with limited semantic annotations by finding `editing vectors'. However, it is noticed that there are many features that are not highly associated with semantics, and EditGAN may fail on them. Based on the orthogonality of latent space observed by EditGAN, we propose a method to estimate editing vectors that do not rely on semantic segmentation nor differentiable feature estimation network. Our method assumes that there is a correlation between the intensity distribution of features and the distribution of hidden vectors, and estimates the relationship between the above distributions by sampling the feature intensity of the image corresponding to several hidden vectors. We modified Linear Discriminant Analysis (LDA) to deal with both binary feature editing and continuous feature editing. We then found that this method has a good effect in processing features such as clothing type and texture, skin color and hair.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "144149886",
                        "name": "Yu Cao"
                    },
                    {
                        "authorId": "48579571",
                        "name": "Haoran Jiang"
                    },
                    {
                        "authorId": "47055450",
                        "name": "Zhenghong Yu"
                    },
                    {
                        "authorId": "2118912812",
                        "name": "Qi Li"
                    },
                    {
                        "authorId": "2108236172",
                        "name": "Xuyang Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "4139a9d059def4f4595964b93ab92cef8dee9b34",
                "externalIds": {
                    "DOI": "10.1016/j.commatsci.2023.112074",
                    "CorpusId": 256890768
                },
                "corpusId": 256890768,
                "publicationVenue": {
                    "id": "3ec21075-eb50-4e2c-bdcb-ac69565537af",
                    "name": "Computational materials science",
                    "type": "journal",
                    "alternate_names": [
                        "Comput mater sci",
                        "Computational Materials Science",
                        "Comput Mater Sci"
                    ],
                    "issn": "0927-0256",
                    "url": "https://www.journals.elsevier.com/computational-materials-science/",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09270256"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4139a9d059def4f4595964b93ab92cef8dee9b34",
                "title": "Quantification of similarity and physical awareness of microstructures generated via generative models",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "102260994",
                        "name": "S. Thakre"
                    },
                    {
                        "authorId": "2191685688",
                        "name": "Vir Karan"
                    },
                    {
                        "authorId": "98209667",
                        "name": "A. Kanjarla"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "dba385c368aa8b67f8727e3652a9ffb9c2f45273",
                "externalIds": {
                    "DBLP": "journals/prl/MuhammadLC23",
                    "DOI": "10.1016/j.patrec.2023.03.019",
                    "CorpusId": 257890221
                },
                "corpusId": 257890221,
                "publicationVenue": {
                    "id": "f35e3e87-9df4-497b-aa0d-bb8584197290",
                    "name": "Pattern Recognition Letters",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit Lett"
                    ],
                    "issn": "0167-8655",
                    "url": "https://www.journals.elsevier.com/pattern-recognition-letters/",
                    "alternate_urls": [
                        "http://www.journals.elsevier.com/pattern-recognition-letters/",
                        "http://www.sciencedirect.com/science/journal/01678655"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/dba385c368aa8b67f8727e3652a9ffb9c2f45273",
                "title": "Exploiting mixing regularization for truly unsupervised font synthesis",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2055824138",
                        "name": "A. Muhammad"
                    },
                    {
                        "authorId": "2110055311",
                        "name": "Hyunsoo Lee"
                    },
                    {
                        "authorId": "2149219619",
                        "name": "Jaeyoung Choi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Though these techniques achieve impressive results for translation, they suffer the same limitations as GANs such as mode coverage and difficulty in training.",
                "Improving editability and controllability in various other forms of generative models (e.g., GANs [14, 15, 50], VAE [2, 27] and Flow-based Models [10, 11]) has been one of the most prominent research topics in the past few years.",
                "To overcome the limitations, we use similar techniques and build on top of diffusion models, that has shown to have better mode coverage and higher quality generations [9] compared to GANs.",
                "GANs such as StyleGAN-v2 [22] have been shown to inherently learn smooth and regular latent spaces [15,50] that enable meaningful edits and manipulations on a real or generated image.",
                ", GANs [14, 15, 50], VAE [2, 27] and Flow-based Models [10, 11]) has been one of the most prominent research topics in the past few years.",
                "DDGAN [51] combined the best of GANs and diffusion models to retain the mode coverage and quality of diffusion models while making it faster like GANs.",
                "Inspired by DiffAE [40] and similar approaches in GANs [29], we introduce a content encoder Ec( \u00b7 ;\u03c8) and a style encoderEs( \u00b7 ;\u03c6) in our framework as shown in Fig.",
                "An alternative to using the inherent latent space of GANs for manipulation is to learn multiple external disentangled latent spaces to condition the generation [21, 29, 32, 39].",
                "We apply PCA on the style and content latent spaces and identify meaningful attribute specific manipulation directions similar to [15] as shown in Fig.",
                "However, the extent of controllability and editability with diffusion models is underexplored relative to GANs.",
                "Additionally, the learned latent spaces are observed to have desirable properties similar to GANs."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "729919e6e764112a66bbe2fcea0cc1224d45d145",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-14368",
                    "ArXiv": "2302.14368",
                    "DOI": "10.48550/arXiv.2302.14368",
                    "CorpusId": 257232777
                },
                "corpusId": 257232777,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/729919e6e764112a66bbe2fcea0cc1224d45d145",
                "title": "Towards Enhanced Controllability of Diffusion Models",
                "abstract": "Denoising Diffusion models have shown remarkable capabilities in generating realistic, high-quality and diverse images. However, the extent of controllability during generation is underexplored. Inspired by techniques based on GAN latent space for image manipulation, we train a diffusion model conditioned on two latent codes, a spatial content mask and a flattened style embedding. We rely on the inductive bias of the progressive denoising process of diffusion models to encode pose/layout information in the spatial structure mask and semantic/style information in the style code. We propose two generic sampling techniques for improving controllability. We extend composable diffusion models to allow for some dependence between conditional inputs, to improve the quality of generations while also providing control over the amount of guidance from each latent code and their joint distribution. We also propose timestep dependent weight scheduling for content and style latents to further improve the translations. We observe better controllability compared to existing methods and show that without explicit training objectives, diffusion models can be used for effective image manipulation and image translation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "41021457",
                        "name": "Wonwoong Cho"
                    },
                    {
                        "authorId": "39936314",
                        "name": "Hareesh Ravi"
                    },
                    {
                        "authorId": "2206112624",
                        "name": "Midhun Harikumar"
                    },
                    {
                        "authorId": "2204801",
                        "name": "V. Khuc"
                    },
                    {
                        "authorId": "50339742",
                        "name": "Krishna Kumar Singh"
                    },
                    {
                        "authorId": "2054975",
                        "name": "Jingwan Lu"
                    },
                    {
                        "authorId": "2055998168",
                        "name": "David I. Inouye"
                    },
                    {
                        "authorId": "37493415",
                        "name": "Ajinkya Kale"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "147e7ba65a20a6f68e237675bc06d60f731f5e04",
                "externalIds": {
                    "DBLP": "conf/wsdm/YangTNPF23",
                    "DOI": "10.1145/3539597.3573031",
                    "CorpusId": 257079699
                },
                "corpusId": 257079699,
                "publicationVenue": {
                    "id": "ea38228f-6ed3-4222-a3ce-d963d8cc9516",
                    "name": "Web Search and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Web Search Data Min",
                        "WSDM"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=3158"
                },
                "url": "https://www.semanticscholar.org/paper/147e7ba65a20a6f68e237675bc06d60f731f5e04",
                "title": "\"Just To See You Smile\": SMILEY, a Voice-Guided GUY GAN",
                "abstract": "In this technical demonstration, we present SMILEY, a voice-guided virtual assistant. The system utilizes a deep neural architecture ContraCLIP to manipulate facial attributes using voice instructions, allowing for deeper speaker engagement and smoother customer experience when being used in the \"virtual concierge\" scenario. We validate the effectiveness of SMILEY and ContraCLIP via a successful real-world case study in Singapore and a large-scale quantitative evaluation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2109122521",
                        "name": "Qi Yang"
                    },
                    {
                        "authorId": "1694090",
                        "name": "Christos Tzelepis"
                    },
                    {
                        "authorId": "2147037111",
                        "name": "Sergey I. Nikolenko"
                    },
                    {
                        "authorId": "50058816",
                        "name": "I. Patras"
                    },
                    {
                        "authorId": "2229491",
                        "name": "Aleksandr Farseev"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2019), unsupervised (Shen & Zhou, 2021; Wang & Ponce, 2021; H\u00e4rk\u00f6nen et al., 2020; Voynov & Babenko, 2020), or text-guided methods (Global Mapper & GlobalDirection1 of StyleCLIP (Patashnik et al.",
                "The difference is that unlike GlobalDirection which relies on a single channel manipulation, Multi2One encode change of image caused by image-agnostic direction found by unsupervised methods (H\u00e4rk\u00f6nen et al., 2020; Shen & Zhou, 2021).",
                "1(a), we can observe that applying the 70-th GANspace direction manipulates the source image to become a man with wide smile, showing that pre-trained StyleGAN itself is capable of such manipulation.",
                "Finally, we compute the similarity score for 30 instances of source images and 1024 directions from SeFa and GANspace whose average is reported in Tab.",
                "To avoid this unrealistic assumption, we substitute s\u0302t with the known directions \u03b1 \u2208 Rn derived from unsupervised methods (H\u00e4rk\u00f6nen et al., 2020; Shen & Zhou, 2021).",
                "The dictionary learning process of Multi2One employs the directions \u03b1 \u2208 Rn from unsupervised methods (Shen & Zhou, 2021; H\u00e4rk\u00f6nen et al., 2020).",
                "1 to show that GlobalDirection (Patashnik et al., 2021) cannot effectively recover the directions found by unsupervised methods (H\u00e4rk\u00f6nen et al., 2020; Shen & Zhou, 2021).",
                "(4) by using all 512 directions found by GANspace and directions with top 80 eigenvalues out of 512 from SeFa4 (H\u00e4rk\u00f6nen et al., 2020; Shen & Zhou, 2021).",
                "We emphasize that despite the dictionary of our method is learned from the known directions in unsupervised approaches (Shen & Zhou, 2021; H\u00e4rk\u00f6nen et al., 2020), the manipulation results show that our learned dictionary could adapt to previously unseen combination of semantics such as red hair,\u2026",
                ", 2021) cannot effectively recover the directions found by unsupervised methods (H\u00e4rk\u00f6nen et al., 2020; Shen & Zhou, 2021).",
                "On the other hand, GANspace (H\u00e4rk\u00f6nen et al., 2020) relies on the randomly sampled latent codes in W and 2",
                "First, we show that many edits using unsupervised methods (H\u00e4rk\u00f6nen et al., 2020; Shen & Zhou, 2021) cannot be recovered by GlobalDirection.",
                "\u2026contrary to this common belief on text-guidance, the standard method (Patashnik et al., 2021) for text-based StyleGAN manipulation surprisingly fails to even find the manipulation directions that are known to be found in unsupervised approaches (H\u00e4rk\u00f6nen et al., 2020; Shen & Zhou, 2021) (see Fig.",
                "We use unsupervised directions from SeFa and GANspace, both of which are found in intermediate space W limiting the maximum number of directions to 512, which is the dimension of the intermediate latent space.",
                "To avoid this unrealistic assumption, we substitute \u015dt with the known directions \u03b1 \u2208 R derived from unsupervised methods (H\u00e4rk\u00f6nen et al., 2020; Shen & Zhou, 2021).",
                "\u2026fast inference and is applicable to any images once found using supervised (Jahanian et al., 2019), unsupervised (Shen & Zhou, 2021; Wang & Ponce, 2021; H\u00e4rk\u00f6nen et al., 2020; Voynov & Babenko, 2020), or text-guided methods (Global Mapper & GlobalDirection1 of StyleCLIP (Patashnik et al., 2021)).",
                "On the other hand, GANspace (H\u00e4rk\u00f6nen et al., 2020) relies on the randomly sampled latent codes in W and\nthe eigenvectors from the latent codes proved to be global directions that share an image-agnostic modification ability.",
                "Therefore, we conduct an ablation study on the effect of using unsupervised directions by comparing the two cases where directions \u03b1 come from supervised method (Shen et al., 2020) and unsupervised methods (Shen & Zhou, 2021; H\u00e4rk\u00f6nen et al., 2020).",
                "We emphasize that despite the dictionary of our method is learned from the known directions in unsupervised approaches (Shen & Zhou, 2021; H\u00e4rk\u00f6nen et al., 2020), the manipulation results show that our learned dictionary could adapt to previously unseen combination of semantics such as red hair, pale skin, and big eyes to represent \u2018Little Mermaid\u2019 and unnatural smiles with red lipstick and pale face to represent \u2018Joker smile\u2019.",
                ", 2021) for text-based StyleGAN manipulation surprisingly fails to even find the manipulation directions that are known to be found in unsupervised approaches (H\u00e4rk\u00f6nen et al., 2020; Shen & Zhou, 2021) (see Fig."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "2e5dc2bbfd04bd965635218d10716b4fb996ca7c",
                "externalIds": {
                    "ArXiv": "2302.13331",
                    "DBLP": "conf/iclr/KimKKCY23",
                    "DOI": "10.48550/arXiv.2302.13331",
                    "CorpusId": 257219541
                },
                "corpusId": 257219541,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2e5dc2bbfd04bd965635218d10716b4fb996ca7c",
                "title": "Learning Input-agnostic Manipulation Directions in StyleGAN with Text Guidance",
                "abstract": "With the advantages of fast inference and human-friendly flexible manipulation, image-agnostic style manipulation via text guidance enables new applications that were not previously available. The state-of-the-art text-guided image-agnostic manipulation method embeds the representation of each channel of StyleGAN independently in the Contrastive Language-Image Pre-training (CLIP) space, and provides it in the form of a Dictionary to quickly find out the channel-wise manipulation direction during inference time. However, in this paper we argue that this dictionary which is constructed by controlling single channel individually is limited to accommodate the versatility of text guidance since the collective and interactive relation among multiple channels are not considered. Indeed, we show that it fails to discover a large portion of manipulation directions that can be found by existing methods, which manually manipulates latent space without texts. To alleviate this issue, we propose a novel method that learns a Dictionary, whose entry corresponds to the representation of a single channel, by taking into account the manipulation effect coming from the interaction with multiple other channels. We demonstrate that our strategy resolves the inability of previous methods in finding diverse known directions from unsupervised methods and unknown directions from random text while maintaining the real-time inference speed and disentanglement ability.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "5565862",
                        "name": "Yoonjeon Kim"
                    },
                    {
                        "authorId": "2118020280",
                        "name": "Hyunsung Kim"
                    },
                    {
                        "authorId": "2173728733",
                        "name": "Junho Kim"
                    },
                    {
                        "authorId": "30187096",
                        "name": "Yunjey Choi"
                    },
                    {
                        "authorId": "1720494",
                        "name": "Eunho Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It follows the course of generative adversarial networks: extending per-sample editing directions (Ramesh et al., 2018; Patashnik et al., 2021; Abdal et al., 2021; Shen & Zhou, 2021) to global editing directions (Ha\u0308rko\u0308nen et al., 2020; Shen & Zhou, 2021; Yu\u0308ksel et al., 2021).",
                "As we introduce the first unsupervised editing in DMs, we compare our method with GANSpace (Ha\u0308rko\u0308nen et al., 2020) considering the mapping from X to H instead of Z to W in GANs.",
                "Since unsupervised editing is not available for DMs, we consider GANSpace for image editing.",
                ", 2021; Shen & Zhou, 2021) to global editing directions (H\u00e4rk\u00f6nen et al., 2020; Shen & Zhou, 2021; Y\u00fcksel et al., 2021).",
                "In addition to what \u00a7 4.5 provides, the editing direction, extracted by the GANSpace, primarily alters colors in images.",
                "As we introduce the first unsupervised editing in DMs, we compare our method with GANSpace (H\u00e4rk\u00f6nen et al., 2020) considering the mapping from X to H instead of Z to W in GANs.",
                "Appendix C describes more details for GANSpace.",
                "We use 1k random images with DDIM generative process for GANSpace.",
                ", 2021) have been developed, as well as global manipulation techniques such as (H\u00e4rk\u00f6nen et al., 2020; Shen & Zhou, 2021; Y\u00fcksel et al., 2021).",
                "Note that the GANSpace method is obtaining directions inW thus we used GANSpace to add directions directly toH.",
                "For example, local latent space manipulation techniques such as (Ramesh et al., 2018; Patashnik et al., 2021; Abdal et al., 2021) have been developed, as well as global manipulation techniques such as (Ha\u0308rko\u0308nen et al., 2020; Shen & Zhou, 2021; Yu\u0308ksel et al., 2021)."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6d23c64e7feb217d53f01f532e8e8885e62f76b2",
                "externalIds": {
                    "ArXiv": "2302.12469",
                    "DBLP": "journals/corr/abs-2302-12469",
                    "DOI": "10.48550/arXiv.2302.12469",
                    "CorpusId": 257205694
                },
                "corpusId": 257205694,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6d23c64e7feb217d53f01f532e8e8885e62f76b2",
                "title": "Unsupervised Discovery of Semantic Latent Directions in Diffusion Models",
                "abstract": "Despite the success of diffusion models (DMs), we still lack a thorough understanding of their latent space. While image editing with GANs builds upon latent space, DMs rely on editing the conditions such as text prompts. We present an unsupervised method to discover interpretable editing directions for the latent variables $\\mathbf{x}_t \\in \\mathcal{X}$ of DMs. Our method adopts Riemannian geometry between $\\mathcal{X}$ and the intermediate feature maps $\\mathcal{H}$ of the U-Nets to provide a deep understanding over the geometrical structure of $\\mathcal{X}$. The discovered semantic latent directions mostly yield disentangled attribute changes, and they are globally consistent across different samples. Furthermore, editing in earlier timesteps edits coarse attributes, while ones in later timesteps focus on high-frequency details. We define the curvedness of a line segment between samples to show that $\\mathcal{X}$ is a curved manifold. Experiments on different baselines and datasets demonstrate the effectiveness of our method even on Stable Diffusion. Our source code will be publicly available for the future researchers.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2209948157",
                        "name": "Yong-Hyun Park"
                    },
                    {
                        "authorId": "2182293854",
                        "name": "Mingi Kwon"
                    },
                    {
                        "authorId": "1832935",
                        "name": "Junghyo Jo"
                    },
                    {
                        "authorId": "2847986",
                        "name": "Youngjung Uh"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0adc8fe26c89cc4e73c8fa91e1c78b552a536c63",
                "externalIds": {
                    "ArXiv": "2302.12798",
                    "DBLP": "journals/corr/abs-2302-12798",
                    "DOI": "10.1111/cgf.14793",
                    "CorpusId": 257206016
                },
                "corpusId": 257206016,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0adc8fe26c89cc4e73c8fa91e1c78b552a536c63",
                "title": "3D Generative Model Latent Disentanglement via Local Eigenprojection",
                "abstract": "Designing realistic digital humans is extremely complex. Most data\u2010driven generative models used to simplify the creation of their underlying geometric shape do not offer control over the generation of local shape attributes. In this paper, we overcome this limitation by introducing a novel loss function grounded in spectral geometry and applicable to different neural\u2010network\u2010based generative models of 3D head and body meshes. Encouraging the latent variables of mesh variational autoencoders (VAEs) or generative adversarial networks (GANs) to follow the local eigenprojections of identity attributes, we improve latent disentanglement and properly decouple the attribute creation. Experimental results show that our local eigenprojection disentangled (LED) models not only offer improved disentanglement with respect to the state\u2010of\u2010the\u2010art, but also maintain good generation capabilities with training times comparable to the vanilla implementations of the models. Our code and pre\u2010trained models are available at github.com/simofoti/LocalEigenprojDisentangled.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51105857",
                        "name": "Simone Foti"
                    },
                    {
                        "authorId": "3273408",
                        "name": "Bongjin Koo"
                    },
                    {
                        "authorId": "2825517",
                        "name": "D. Stoyanov"
                    },
                    {
                        "authorId": "145447328",
                        "name": "M. Clarkson"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There are numerous strategies that can be used to build GAN-based FG models, more details can be found Kammoun et al. (2022)Wang et al. (2022)Ning et al. (2020). In Kammoun et al.",
                "There are numerous strategies that can be used to build GAN-based FG models, more details can be found Kammoun et al. (2022)Wang et al. (2022)Ning et al. (2020). In Kammoun et al. (2022), models are organized in three main types: Conditional, Controllable, and Progressive. Uncontrolled approaches have shown the best performance due to the big amount of unannotated data available. One of the most important families of GAN models is the StyleGAN family Karras et al. (2019). StyleGAN2Karras et al. (2020), a SOTA model in facial generation which applies effective strategies to improve the model performance such as: generator normalization, revisiting progressive growing and regularizing the generator.",
                "There are numerous strategies that can be used to build GAN-based FG models, more details can be found Kammoun et al. (2022)Wang et al. (2022)Ning et al.",
                "There are numerous strategies that can be used to build GAN-based FG models, more details can be found Kammoun et al. (2022)Wang et al.",
                "There are numerous strategies that can be used to build GAN-based FG models, more details can be found Kammoun et al. (2022)Wang et al. (2022)Ning et al. (2020). In Kammoun et al. (2022), models are organized in three main types: Conditional, Controllable, and Progressive. Uncontrolled approaches have shown the best performance due to the big amount of unannotated data available. One of the most important families of GAN models is the StyleGAN family Karras et al. (2019). StyleGAN2Karras et al.",
                "There are numerous strategies that can be used to build GAN-based FG models, more details can be found Kammoun et al. (2022)Wang et al. (2022)Ning et al. (2020). In Kammoun et al. (2022), models are organized in three main types: Conditional, Controllable, and Progressive."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "93ac42d0bcaf7f61fe3ac77a235318acbead9805",
                "externalIds": {
                    "ArXiv": "2302.11562",
                    "DBLP": "journals/corr/abs-2302-11562",
                    "DOI": "10.48550/arXiv.2302.11562",
                    "CorpusId": 257078619
                },
                "corpusId": 257078619,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/93ac42d0bcaf7f61fe3ac77a235318acbead9805",
                "title": "Uncovering Bias in Face Generation Models",
                "abstract": "Recent advancements in GANs and diffusion models have enabled the creation of high-resolution, hyper-realistic images. However, these models may misrepresent certain social groups and present bias. Understanding bias in these models remains an important research question, especially for tasks that support critical decision-making and could affect minorities. The contribution of this work is a novel analysis covering architectures and embedding spaces for fine-grained understanding of bias over three approaches: generators, attribute modifier, and post-processing bias mitigators. This work shows that generators suffer from bias across all social groups with attribute preferences such as between 75%-85% for whiteness and 60%-80% for the female gender (for all trained CelebA models) and low probabilities of generating children and older men. Modifier and mitigators work as post-processor and change the generator performance. For instance, attribute channel perturbation strategies modify the embedding spaces. We quantify the influence of this change on group fairness by measuring the impact on image quality and group features. Specifically, we use the Fr\\'echet Inception Distance (FID), the Face Matching Error and the Self-Similarity score. For Interfacegan, we analyze one and two attribute channel perturbations and examine the effect on the fairness distribution and the quality of the image. Finally, we analyzed the post-processing bias mitigators, which are the fastest and most computationally efficient way to mitigate bias. We find that these mitigation techniques show similar results on KL divergence and FID score, however, self-similarity scores show a different feature concentration on the new groups of the data distribution. The weaknesses and ongoing challenges described in this work must be considered in the pursuit of creating fair and unbiased face generation models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "115359104",
                        "name": "Cristian Mu\u00f1oz"
                    },
                    {
                        "authorId": "19214542",
                        "name": "Sara Zannone"
                    },
                    {
                        "authorId": "2058367335",
                        "name": "U. Mohammed"
                    },
                    {
                        "authorId": "2127728158",
                        "name": "A. Koshiyama"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Further investigation into the latent space structure has been performed by applying unsupervised methods, such as Principal Component Analysis (PCA) [18] or eigenvalue decomposition [43], or by using semantic labels [42,2].",
                "6: Editing quality on LSUN Church using GANSpace [18]",
                "For these datasets, the editing directions were taken from GANSpace [18].",
                "The unsupervised path aims to unveil the domain\u2019s structure by applying PCA [18] or eigenvalue decomposition [43].",
                "7: Editing quality on the Stanford Cars using GANSpace [18]"
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "ab2017e6126d3780110bc5eb8e0ee875bf1e7ad1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-11413",
                    "ArXiv": "2302.11413",
                    "DOI": "10.48550/arXiv.2302.11413",
                    "CorpusId": 257078878
                },
                "corpusId": 257078878,
                "publicationVenue": {
                    "id": "34a8e4b6-33c0-41d2-a418-fb738851fb68",
                    "name": "Scandinavian Conference on Image Analysis",
                    "type": "conference",
                    "alternate_names": [
                        "SCIA",
                        "Scand Conf Image Anal"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ab2017e6126d3780110bc5eb8e0ee875bf1e7ad1",
                "title": "Gradient Adjusting Networks for Domain Inversion",
                "abstract": "StyleGAN2 was demonstrated to be a powerful image generation engine that supports semantic editing. However, in order to manipulate a real-world image, one first needs to be able to retrieve its corresponding latent representation in StyleGAN's latent space that is decoded to an image as close as possible to the desired image. For many real-world images, a latent representation does not exist, which necessitates the tuning of the generator network. We present a per-image optimization method that tunes a StyleGAN2 generator such that it achieves a local edit to the generator's weights, resulting in almost perfect inversion, while still allowing image editing, by keeping the rest of the mapping between an input latent representation tensor and an output image relatively intact. The method is based on a one-shot training of a set of shallow update networks (aka. Gradient Modification Modules) that modify the layers of the generator. After training the Gradient Modification Modules, a modified generator is obtained by a single application of these networks to the original parameters, and the previous editing capabilities of the generator are maintained. Our experiments show a sizable gap in performance over the current state of the art in this very active domain. Our code is available at \\url{https://github.com/sheffier/gani}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2209200555",
                        "name": "Erez Sheffi"
                    },
                    {
                        "authorId": "32576006",
                        "name": "Michael Rotman"
                    },
                    {
                        "authorId": "145128145",
                        "name": "Lior Wolf"
                    }
                ]
            }
        },
        {
            "contexts": [
                "2 Semantic-based Editing Except for the parsing-based and appearance-based editing methods, our fine-tuned anime generator also supports the latent semantic editing [H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020; Shen and Zhou 2021]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6d5a5f264836f8245a7622fdf73cf0710bb0ca65",
                "externalIds": {
                    "DBLP": "journals/tog/LiXZZLLH23",
                    "DOI": "10.1145/3585002",
                    "CorpusId": 257039600
                },
                "corpusId": 257039600,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6d5a5f264836f8245a7622fdf73cf0710bb0ca65",
                "title": "Parsing-Conditioned Anime Translation: A New Dataset and Method",
                "abstract": "Anime is an abstract art form that is substantially different from the human portrait, leading to a challenging misaligned image translation problem that is beyond the capability of existing methods. This can be boiled down to a highly ambiguous unconstrained translation between two domains. To this end, we design a new anime translation framework by deriving the prior knowledge of a pre-trained StyleGAN model. We introduce disentangled encoders to separately embed structure and appearance information into the same latent code, governed by four tailored losses. Moreover, we develop a FaceBank aggregation method that leverages the generated data of the StyleGAN, anchoring the prediction to produce in-domain animes. To empower our model and promote the research of anime translation, we propose the first anime portrait parsing dataset, Danbooru-Parsing, containing 4,921 densely labeled images across 17 classes. This dataset connects the face semantics with appearances, enabling our new constrained translation setting. We further show the editability of our results, and extend our method to manga images, by generating the first manga parsing pseudo data. Extensive experiments demonstrate the values of our new dataset and method, resulting in the first feasible solution on anime translation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2208716792",
                        "name": "Zhansheng Li"
                    },
                    {
                        "authorId": "2154894041",
                        "name": "Yangyang Xu"
                    },
                    {
                        "authorId": "51150125",
                        "name": "Nanxuan Zhao"
                    },
                    {
                        "authorId": "2173136638",
                        "name": "Yang Zhou"
                    },
                    {
                        "authorId": "1740718257",
                        "name": "Yongtuo Liu"
                    },
                    {
                        "authorId": "1807606",
                        "name": "Dahua Lin"
                    },
                    {
                        "authorId": "2115300590",
                        "name": "Shengfeng He"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also compare with some other SOTA methods in the experiments, i.e., StyleRig [7], InterfaceGAN [8], GANSpace [9], StyleFlow [10].",
                "The comparisons of the multi-view editing effects, using StyleRig [7], InterfaceGAN [8], GANSpace [9], StyleFlow [10] and ours.",
                ", StyleRig [7], InterfaceGAN [8], GANSpace [9], StyleFlow [10]."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "41fbd58779db0947b27051b8014b8a9d76baa9fa",
                "externalIds": {
                    "ArXiv": "2302.09467",
                    "DBLP": "journals/corr/abs-2302-09467",
                    "DOI": "10.48550/arXiv.2302.09467",
                    "CorpusId": 257038059
                },
                "corpusId": 257038059,
                "publicationVenue": {
                    "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
                    "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Acoust Speech Signal Process",
                        "IEEE Int Conf Acoust Speech Signal Process",
                        "ICASSP",
                        "International Conference on Acoustics, Speech, and Signal Processing"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
                },
                "url": "https://www.semanticscholar.org/paper/41fbd58779db0947b27051b8014b8a9d76baa9fa",
                "title": "Designing a 3D-Aware StyleNeRF Encoder for Face Editing",
                "abstract": "GAN inversion has been exploited in many face manipulation tasks, but 2D GANs often fail to generate multi-view 3D consistent images. The encoders designed for 2D GANs are not able to provide sufficient 3D information for the inversion and editing. Therefore, 3D-aware GAN inversion is proposed to increase the 3D editing capability of GANs. However, the 3D-aware GAN inversion remains under-explored. To tackle this problem, we propose a 3D-aware (3Da) encoder for GAN inversion and face editing based on the powerful StyleNeRF model. Our proposed 3Da encoder combines a parametric 3D face model with a learnable detail representation model to generate geometry, texture and view direction codes. For more flexible face manipulation, we then design a dual-branch StyleFlow module to transfer the StyleNeRF codes with disentangled geometry and texture flows. Extensive experiments demonstrate that we realize 3D consistent face manipulation in both facial attribute editing and texture transfer. Furthermore, for video editing, we make the sequence of frame codes share a common canonical manifold, which improves the temporal consistency of the edited attributes.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2143935738",
                        "name": "Songlin Yang"
                    },
                    {
                        "authorId": "40119691",
                        "name": "Wei Wang"
                    },
                    {
                        "authorId": "2054590303",
                        "name": "Bo Peng"
                    },
                    {
                        "authorId": "143863960",
                        "name": "Jing Dong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANSpace is unable to discover the editing direction for a specific semantic attribute.",
                "GANSpace [6] analyses the latent space W by PCA and identifies semantic editing directions manually.",
                "For unsupervised approaches, GANSpace[6] adopts PCA to analyse meaningful editing directions in the latent spaceW."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "d4a05ac079679055f70abdd9b45e11a76c179a93",
                "externalIds": {
                    "ArXiv": "2302.09260",
                    "DBLP": "journals/corr/abs-2302-09260",
                    "DOI": "10.48550/arXiv.2302.09260",
                    "CorpusId": 257038565
                },
                "corpusId": 257038565,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d4a05ac079679055f70abdd9b45e11a76c179a93",
                "title": "Attribute-Specific Manipulation Based on Layer-Wise Channels",
                "abstract": "Image manipulation on the latent space of the pre-trained StyleGAN can control the semantic attributes of the generated images. Recently, some studies have focused on detecting channels with specific properties to directly manipulate the latent code, which is limited by the entanglement of the latent space. To detect the attribute-specific channels, we propose a novel detection method in the context of pre-trained classifiers. We analyse the gradients layer by layer on the style space. The intensities of the gradients indicate the channel's responses to specific attributes. The latent style codes of channels control separate attributes in the layers. We choose channels with top-$k$ gradients to control specific attributes in the maximum response layer. We implement single-channel and multi-channel manipulations with a certain attribute. Our methods can accurately detect relevant channels for a large number of face attributes. Extensive qualitative and quantitative results demonstrate that the proposed methods outperform state-of-the-art methods in generalization and scalability.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "102365723",
                        "name": "Yuanjie Yan"
                    },
                    {
                        "authorId": "2179595489",
                        "name": "Jian Zhao"
                    },
                    {
                        "authorId": "1728090",
                        "name": "S. Furao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Semantic editing using latent spaces has employed supervised [2, 33, 47] and unsupervised approaches [13, 29, 39].",
                "through latent space edits proposed by [13, 33].",
                "Latent Space Manipulation of StyleGAN: Since the proposal of StyleGAN, there has been a plethora of research on the semantic interpretability of the intermediate latent spaces [13, 33, 34, 47].",
                "Considering the latent space manipulations in [2, 13, 28, 47] it is evident that the latent space of a pre-trained StyleGAN has implicit 3D information embedded within it.",
                "StyleGAN\u2019s [22] ability to produce high-resolution (1024(2)) photo-realistic faces, richness and semantic interpretability of its latent spaces [13, 28, 33, 47], and the improvements in inversion techniques contributed to improved re-enactment generations [19, 28, 49].",
                "applied to the source image (see [13,33]) could be generated as shown in Fig.",
                ", beard, age, make-up) accommodating latent manipulation techniques such as [13, 33]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b0d83182c8561d70526a2325c869908ad3c37f2c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-07848",
                    "ArXiv": "2302.07848",
                    "DOI": "10.48550/arXiv.2302.07848",
                    "CorpusId": 256868632
                },
                "corpusId": 256868632,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b0d83182c8561d70526a2325c869908ad3c37f2c",
                "title": "One-Shot Face Video Re-enactment using Hybrid Latent Spaces of StyleGAN2",
                "abstract": "While recent research has progressively overcome the low-resolution constraint of one-shot face video re-enactment with the help of StyleGAN's high-fidelity portrait generation, these approaches rely on at least one of the following: explicit 2D/3D priors, optical flow based warping as motion descriptors, off-the-shelf encoders, etc., which constrain their performance (e.g., inconsistent predictions, inability to capture fine facial details and accessories, poor generalization, artifacts). We propose an end-to-end framework for simultaneously supporting face attribute edits, facial motions and deformations, and facial identity control for video generation. It employs a hybrid latent-space that encodes a given frame into a pair of latents: Identity latent, $\\mathcal{W}_{ID}$, and Facial deformation latent, $\\mathcal{S}_F$, that respectively reside in the $W+$ and $SS$ spaces of StyleGAN2. Thereby, incorporating the impressive editability-distortion trade-off of $W+$ and the high disentanglement properties of $SS$. These hybrid latents employ the StyleGAN2 generator to achieve high-fidelity face video re-enactment at $1024^2$. Furthermore, the model supports the generation of realistic re-enactment videos with other latent-based semantic edits (e.g., beard, age, make-up, etc.). Qualitative and quantitative analyses performed against state-of-the-art methods demonstrate the superiority of the proposed approach.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "31668191",
                        "name": "T. Oorloff"
                    },
                    {
                        "authorId": "1964574",
                        "name": "Y. Yacoob"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Thanks to inversion, StyleGAN has found use in many image editing [23, 43, 51] and restoration [13\u201315, 39, 40] tasks."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "0bb811e12fb737d832178d5e6bd796baae27517c",
                "externalIds": {
                    "DBLP": "conf/cvpr/Poirier-GinterL23",
                    "ArXiv": "2302.06733",
                    "DOI": "10.1109/CVPR52729.2023.02135",
                    "CorpusId": 256846892
                },
                "corpusId": 256846892,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0bb811e12fb737d832178d5e6bd796baae27517c",
                "title": "Robust Unsupervised StyleGAN Image Restoration",
                "abstract": "GAN-based image restoration inverts the generative process to repair images corrupted by known degradations. Existing unsupervised methods must be carefully tuned for each task and degradation level. In this work, we make StyleGAN image restoration robust: a single set of hyperparameters works across a wide range of degradation levels. This makes it possible to handle combinations of several degradations, without the need to retune. Our proposed approach relies on a 3-phase progressive latent space extension and a conservative optimizer, which avoids the need for any additional regularization terms. Extensive experiments demonstrate robustness on inpainting, upsampling, denoising, and deartifacting at varying degradations levels, outperforming other StyleGAN-based inversion techniques. Our approach also favorably compares to diffusion-based restoration by yielding much more realistic inversion results. Code is available at the above URL.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2165304859",
                        "name": "Yohan Poirier-Ginter"
                    },
                    {
                        "authorId": "144430305",
                        "name": "Jean-Fran\u00e7ois Lalonde"
                    }
                ]
            }
        },
        {
            "contexts": [
                "StyleGANs [26\u201329] have achieved high-quality photorealistic 2D image generation and have been successfully applied to various image editing applications [18, 24, 40, 45].",
                "With our GAN inversion, we can modify the latent code to perform high-quality semantic image editing [18, 24, 40, 45] or video editing [53, 58, 60].",
                "By changing the latent code, one can achieve many creative semantic editing effects [18, 24, 40, 45] for images."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "73b155f5ff28845fb5eea919bd6ac1a02ce7acaf",
                "externalIds": {
                    "ArXiv": "2302.04871",
                    "DBLP": "journals/corr/abs-2302-04871",
                    "DOI": "10.48550/arXiv.2302.04871",
                    "CorpusId": 256697627
                },
                "corpusId": 256697627,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/73b155f5ff28845fb5eea919bd6ac1a02ce7acaf",
                "title": "In-N-Out: Face Video Inversion and Editing with Volumetric Decomposition",
                "abstract": "3D-aware GANs offer new capabilities for creative content editing, such as view synthesis, while preserving the editing capability of their 2D counterparts. These methods use GAN inversion to reconstruct images or videos by optimizing a latent code, allowing for semantic editing by manipulating the code. However, a model pre-trained on a face dataset (e.g., FFHQ) often has difficulty handling faces with out-of-distribution (OOD) objects, e.g., heavy make-up or occlusions. We address this issue by explicitly modeling OOD objects in face videos. Our core idea is to represent the face in a video using two neural radiance fields, one for the in-distribution and the other for the out-of-distribution object, and compose them together for reconstruction. Such explicit decomposition alleviates the inherent trade-off between reconstruction fidelity and editability. We evaluate our method's reconstruction accuracy and editability on challenging real videos and showcase favorable results against other baselines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2176060995",
                        "name": "Yi Xu"
                    },
                    {
                        "authorId": "2496409",
                        "name": "Zhixin Shu"
                    },
                    {
                        "authorId": "153551055",
                        "name": "Cameron Smith"
                    },
                    {
                        "authorId": "2238908897",
                        "name": "Jia-Bin Huang"
                    },
                    {
                        "authorId": "3451982",
                        "name": "Seoung Wug Oh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "2022], and utilize latent edit directions [Abdal et al. 2022; H\u00e4rk\u00f6nen et al. 2020; Patashnik et al. 2021; Shen et al. 2020]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "daf61010eee0fbf6f9bab7db71c395ffca6f3ff3",
                "externalIds": {
                    "DBLP": "conf/siggraph/ParmarS0LLZ23",
                    "ArXiv": "2302.03027",
                    "DOI": "10.1145/3588432.3591513",
                    "CorpusId": 256616002
                },
                "corpusId": 256616002,
                "publicationVenue": {
                    "id": "cf6b5e76-9274-46e6-a2dd-7c190ec2ec5f",
                    "name": "International Conference on Computer Graphics and Interactive Techniques",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Graph Interact Tech",
                        "SIGGRAPH"
                    ],
                    "url": "http://www.siggraph.org/"
                },
                "url": "https://www.semanticscholar.org/paper/daf61010eee0fbf6f9bab7db71c395ffca6f3ff3",
                "title": "Zero-shot Image-to-Image Translation",
                "abstract": "Large-scale text-to-image generative models have shown their remarkable ability to synthesize diverse, high-quality images. However, directly applying these models for real image editing remains challenging for two reasons. First, it is hard for users to craft a perfect text prompt depicting every visual detail in the input image. Second, while existing models can introduce desirable changes in certain regions, they often dramatically alter the input content and introduce unexpected changes in unwanted regions. In this work, we introduce pix2pix-zero, an image-to-image translation method that can preserve the original image\u2019s content without manual prompting. We first automatically discover editing directions that reflect desired edits in the text embedding space. To preserve the content structure, we propose cross-attention guidance, which aims to retain the cross-attention maps of the input image throughout the diffusion process. Finally, to enable interactive editing, we distill the diffusion model into a fast conditional GAN. We conduct extensive experiments and show that our method outperforms existing and concurrent works for both real and synthetic image editing. In addition, our method does not need additional training for these edits and can directly use the existing pre-trained text-to-image diffusion model.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2065083806",
                        "name": "Gaurav Parmar"
                    },
                    {
                        "authorId": "50339742",
                        "name": "Krishna Kumar Singh"
                    },
                    {
                        "authorId": "2109976035",
                        "name": "Richard Zhang"
                    },
                    {
                        "authorId": "152998391",
                        "name": "Yijun Li"
                    },
                    {
                        "authorId": "2054975",
                        "name": "Jingwan Lu"
                    },
                    {
                        "authorId": "1922024303",
                        "name": "Jun-Yan Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[47] applied Principal Component Analysis on the latent space and proposed to control the semantics by layer-wise perturbation along the principal directions."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "9507158000705c0997725e83fbfb7b5e7f6b5e16",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-00908",
                    "ArXiv": "2302.00908",
                    "DOI": "10.48550/arXiv.2302.00908",
                    "CorpusId": 256503567
                },
                "corpusId": 256503567,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9507158000705c0997725e83fbfb7b5e7f6b5e16",
                "title": "GANalyzer: Analysis and Manipulation of GANs Latent Space for Controllable Face Synthesis",
                "abstract": "Generative Adversarial Networks (GANs) are capable of synthesizing high-quality facial images. Despite their success, GANs do not provide any information about the relationship between the input vectors and the generated images. Currently, facial GANs are trained on imbalanced datasets, which generate less diverse images. For example, more than 77% of 100K images that we randomly synthesized using the StyleGAN3 are classified as Happy, and only around 3% are Angry. The problem even becomes worse when a mixture of facial attributes is desired: less than 1% of the generated samples are Angry Woman, and only around 2% are Happy Black. To address these problems, this paper proposes a framework, called GANalyzer, for the analysis, and manipulation of the latent space of well-trained GANs. GANalyzer consists of a set of transformation functions designed to manipulate latent vectors for a specific facial attribute such as facial Expression, Age, Gender, and Race. We analyze facial attribute entanglement in the latent space of GANs and apply the proposed transformation for editing the disentangled facial attributes. Our experimental results demonstrate the strength of GANalyzer in editing facial attributes and generating any desired faces. We also create and release a balanced photo-realistic human face dataset. Our code is publicly available on GitHub.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2051713558",
                        "name": "A. P. Fard"
                    },
                    {
                        "authorId": "145531712",
                        "name": "M. Mahoor"
                    },
                    {
                        "authorId": "23592180",
                        "name": "S. Lamer"
                    },
                    {
                        "authorId": "3519267",
                        "name": "Timothy D. Sweeny"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "06a2386a999653c0fd0af320962029d8e4a210fc",
                "externalIds": {
                    "ArXiv": "2302.00179",
                    "DBLP": "journals/corr/abs-2302-00179",
                    "DOI": "10.48550/arXiv.2302.00179",
                    "CorpusId": 247475856
                },
                "corpusId": 247475856,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/06a2386a999653c0fd0af320962029d8e4a210fc",
                "title": "Stable Attribute Group Editing for Reliable Few-shot Image Generation",
                "abstract": "Few-shot image generation aims to generate data of an unseen category based on only a few samples. Apart from basic content generation, a bunch of downstream applications hopefully benefit from this task, such as low-data detection and few-shot classification. To achieve this goal, the generated images should guarantee category retention for classification beyond the visual quality and diversity. In our preliminary work, we present an ``editing-based'' framework Attribute Group Editing (AGE) for reliable few-shot image generation, which largely improves the generation performance. Nevertheless, AGE's performance on downstream classification is not as satisfactory as expected. This paper investigates the class inconsistency problem and proposes Stable Attribute Group Editing (SAGE) for more stable class-relevant image generation. SAGE takes use of all given few-shot images and estimates a class center embedding based on the category-relevant attribute dictionary. Meanwhile, according to the projection weights on the category-relevant attribute dictionary, we can select category-irrelevant attributes from the similar seen categories. Consequently, SAGE injects the whole distribution of the novel class into StyleGAN's latent space, thus largely remains the category retention and stability of the generated images. Going one step further, we find that class inconsistency is a common problem in GAN-generated images for downstream classification. Even though the generated images look photo-realistic and requires no category-relevant editing, they are usually of limited help for downstream classification. We systematically discuss this issue from both the generative model and classification model perspectives, and propose to boost the downstream classification performance of SAGE by enhancing the pixel and frequency components.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2057943535",
                        "name": "Guanqi Ding"
                    },
                    {
                        "authorId": "144638992",
                        "name": "Xinzhe Han"
                    },
                    {
                        "authorId": "2119545962",
                        "name": "Shuhui Wang"
                    },
                    {
                        "authorId": "1485402830",
                        "name": "Xin Jin"
                    },
                    {
                        "authorId": "2929196",
                        "name": "Dandan Tu"
                    },
                    {
                        "authorId": "1689702",
                        "name": "Qingming Huang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "fa56c8b33d0795df62e2102f7bb98e9ba396de4a",
                "externalIds": {
                    "DBLP": "journals/sensors/ZhangYWC23",
                    "PubMedCentral": "9958588",
                    "DOI": "10.3390/s23041815",
                    "CorpusId": 256655188,
                    "PubMed": "36850416"
                },
                "corpusId": 256655188,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fa56c8b33d0795df62e2102f7bb98e9ba396de4a",
                "title": "Generated Image Editing Method Based on Global-Local Jacobi Disentanglement for Machine Learning",
                "abstract": "Accurate semantic editing of the generated images is extremely important for machine learning and sample enhancement of big data. Aiming at the problem of semantic entanglement in generated image latent space of the StyleGAN2 network, we proposed a generated image editing method based on global-local Jacobi disentanglement. In terms of global disentanglement, we extract the weight matrix of the style layer in the pre-trained StyleGAN2 network; obtain the semantic attribute direction vector by using the weight matrix eigen decomposition method; finally, utilize this direction vector as the initialization vector for the Jacobi orthogonal regularization search algorithm. Our method improves the speed of the Jacobi orthogonal regularization search algorithm with the proportion of effective semantic attribute editing directions. In terms of local disentanglement, we design a local contrast regularized loss function to relax the semantic association local area and non-local area and utilize the Jacobi orthogonal regularization search algorithm to obtain a more accurate semantic attribute editing direction based on the local area prior MASK. The experimental results show that the proposed method achieves SOTA in semantic attribute disentangled metrics and can discover more accurate editing directions compared with the mainstream unsupervised generated image editing methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "49050283",
                        "name": "Jianlong Zhang"
                    },
                    {
                        "authorId": "2174315125",
                        "name": "Xincheng Yu"
                    },
                    {
                        "authorId": "2152592987",
                        "name": "Bin Wang"
                    },
                    {
                        "authorId": "40262099",
                        "name": "Cheng Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Early unsupervised methods (Shen and Zhou 2021; H\u00e4rk\u00f6nen et al. 2020) apply Principal Component Analysis (PCA) on latent space or model weights, and interpretable control can be performed by layer-wise perturbation along the principal directions.",
                "Early unsupervised methods (Shen and Zhou 2021; Ha\u0308rko\u0308nen et al. 2020) apply Principal Component Analysis (PCA) on latent space or model weights, and interpretable control can be performed by layer-wise perturbation along the principal directions.",
                "\u2026(Wu, Lischinski, and Shechtman 2021). winit can be semantically manipulated by a pre-trained style editing models (Shen and Zhou 2021; Ha\u0308rko\u0308nen et al. 2020; Shen et al. 2020; Patashnik et al. 2021; Wang, Yu, and Fritz 2021; Li et al. 2021; et al. 2021a; Ling et al. 2021; Shi et al.\u2026",
                "winit can be semantically manipulated by a pre-trained style editing models (Shen and Zhou 2021; H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020; Patashnik et al. 2021; Wang, Yu, and Fritz 2021; Li et al. 2021; et al. 2021a; Ling et al. 2021; Shi et al. 2022; Chong, Lee, and Forsyth 2021; Hou et al. 2022)."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "ea5e945cde29b61254115d512e9d39fb57a7514c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-13402",
                    "ArXiv": "2301.13402",
                    "DOI": "10.48550/arXiv.2301.13402",
                    "CorpusId": 256416093
                },
                "corpusId": 256416093,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/ea5e945cde29b61254115d512e9d39fb57a7514c",
                "title": "ReGANIE: Rectifying GAN Inversion Errors for Accurate Real Image Editing",
                "abstract": "The StyleGAN family succeed in high-fidelity image generation and allow for flexible and plausible editing of generated images by manipulating the semantic-rich latent style space. However, projecting a real image into its latent space encounters an inherent trade-off between inversion quality and editability. Existing encoder-based or optimization-based StyleGAN inversion methods attempt to mitigate the trade-off but see limited performance. To fundamentally resolve this problem, we propose a novel two-phase framework by designating two separate networks to tackle editing and reconstruction respectively, instead of balancing the two. Specifically, in Phase I, a W-space-oriented StyleGAN inversion network is trained and used to perform image inversion and edit- ing, which assures the editability but sacrifices reconstruction quality. In Phase II, a carefully designed rectifying network is utilized to rectify the inversion errors and perform ideal reconstruction. Experimental results show that our approach yields near-perfect reconstructions without sacrificing the editability, thus allowing accurate manipulation of real images. Further, we evaluate the performance of our rectifying net- work, and see great generalizability towards unseen manipulation types and out-of-domain images.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2145726957",
                        "name": "Bingchuan Li"
                    },
                    {
                        "authorId": "2114185891",
                        "name": "Tianxiang Ma"
                    },
                    {
                        "authorId": "2151330326",
                        "name": "Peng Zhang"
                    },
                    {
                        "authorId": "2128319758",
                        "name": "Miao Hua"
                    },
                    {
                        "authorId": "2157221999",
                        "name": "Wei Liu"
                    },
                    {
                        "authorId": "2152880412",
                        "name": "Qian He"
                    },
                    {
                        "authorId": "39737792",
                        "name": "Zili Yi"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020), GANspace (GS) (H\u00e4rk\u00f6nen et al., 2020), LatentDiscovery (LD) (Voynov & Babenko, 2020) and DisCo (Ren et al.",
                "The GAN-based baselines include InfoGAN-CR (Lin et al., 2020), GANspace (GS) (Ha\u0308rko\u0308nen et al., 2020), LatentDiscovery (LD) (Voynov & Babenko, 2020) and DisCo (Ren et al., 2021)."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b2fa14be74c45d7c8c4d96330264860b074526c2",
                "externalIds": {
                    "ArXiv": "2301.13721",
                    "DBLP": "journals/corr/abs-2301-13721",
                    "DOI": "10.48550/arXiv.2301.13721",
                    "CorpusId": 256416173
                },
                "corpusId": 256416173,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b2fa14be74c45d7c8c4d96330264860b074526c2",
                "title": "DisDiff: Unsupervised Disentanglement of Diffusion Probabilistic Models",
                "abstract": "In this paper, targeting to understand the underlying explainable factors behind observations and modeling the conditional generation process on these factors, we propose a new task, disentanglement of diffusion probabilistic models (DPMs), to take advantage of the remarkable modeling ability of DPMs. To tackle this task, we further devise an unsupervised approach named DisDiff. For the \ufb01rst time, we achieve disentangled representation learning in the framework of diffusion probabilistic models. Given a pre-trained DPM, DisDiff can automatically discover the inherent factors behind the image data and disentangle the gradient \ufb01elds of DPM into sub-gradient \ufb01elds, each conditioned on the representation of each discovered factor. We propose a novel Disentangling Loss for DisDiff to facilitate the disentanglement of the representation and sub-gradients. The extensive experiments on synthetic and real-world datasets demonstrate the effectiveness of DisDiff.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "46853740",
                        "name": "T. Yang"
                    },
                    {
                        "authorId": "46393469",
                        "name": "Yuwang Wang"
                    },
                    {
                        "authorId": "2203932367",
                        "name": "Yan Lv"
                    },
                    {
                        "authorId": "2203426091",
                        "name": "Nanning Zh"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "3fdc679637bc83bb435ae9a31c249ba5f80acd06",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-00079",
                    "ArXiv": "2302.00079",
                    "DOI": "10.1145/3544548.3581226",
                    "CorpusId": 256459700
                },
                "corpusId": 256459700,
                "publicationVenue": {
                    "id": "b55b50b1-aae7-47a7-b042-8aecc930073d",
                    "name": "International Conference on Human Factors in Computing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "CHI",
                        "Int Conf Hum Factor Comput Syst",
                        "Human Factors in Computing Systems",
                        "Conference on Human Interface",
                        "Conf Hum Interface",
                        "Hum Factor Comput Syst"
                    ],
                    "url": "http://www.acm.org/sigchi/"
                },
                "url": "https://www.semanticscholar.org/paper/3fdc679637bc83bb435ae9a31c249ba5f80acd06",
                "title": "GANravel: User-Driven Direction Disentanglement in Generative Adversarial Networks",
                "abstract": "Generative adversarial networks (GANs) have many application areas including image editing, domain translation, missing data imputation, and support for creative work. However, GANs are considered \u2018black boxes\u2019. Specifically, the end-users have little control over how to improve editing directions through disentanglement. Prior work focused on new GAN architectures to disentangle editing directions. Alternatively, we propose GANravel\u2014a user-driven direction disentanglement tool that complements the existing GAN architectures and allows users to improve editing directions iteratively. In two user studies with 16 participants each, GANravel users were able to disentangle directions and outperformed the state-of-the-art direction discovery baselines in disentanglement performance. In the second user study, GANravel was used in a creative task of creating dog memes and was able to create high-quality edited images and GIFs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "28136682",
                        "name": "Noyan Evirgen"
                    },
                    {
                        "authorId": "2143733868",
                        "name": "Xiang Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Without label annotation, other methods explore the latent space by unsupervised [13, 29, 32, 33] or self-supervised ways [15, 24] to find more semantic directions way."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3ef86367fbe3f0123f5630358155eec0c212421c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-12141",
                    "ArXiv": "2301.12141",
                    "DOI": "10.48550/arXiv.2301.12141",
                    "CorpusId": 256390482
                },
                "corpusId": 256390482,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3ef86367fbe3f0123f5630358155eec0c212421c",
                "title": "What Decreases Editing Capability? Domain-Specific Hybrid Refinement for Improved GAN Inversion",
                "abstract": "Recently, inversion methods have focused on additional high-rate information in the generator (e.g., weights or intermediate features) to refine inversion and editing results from embedded latent codes. Although these techniques gain reasonable improvement in reconstruction, they decrease editing capability, especially on complex images (e.g., containing occlusions, detailed backgrounds, and artifacts). A vital crux is refining inversion results, avoiding editing capability degradation. To tackle this problem, we introduce Domain-Specific Hybrid Refinement (DHR), which draws on the advantages and disadvantages of two mainstream refinement techniques to maintain editing ability with fidelity improvement. Specifically, we first propose Domain-Specific Segmentation to segment images into two parts: in-domain and out-of-domain parts. The refinement process aims to maintain the editability for in-domain areas and improve two domains' fidelity. We refine these two parts by weight modulation and feature modulation, which we call Hybrid Modulation Refinement. Our proposed method is compatible with all latent code embedding methods. Extension experiments demonstrate that our approach achieves state-of-the-art in real image inversion and editing. Code is available at https://github.com/caopulan/GANInverter/tree/main/configs/dhr.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2203366502",
                        "name": "Pu Cao"
                    },
                    {
                        "authorId": "39702333",
                        "name": "Lu Yang"
                    },
                    {
                        "authorId": "2152507127",
                        "name": "Dongxu Liu"
                    },
                    {
                        "authorId": "2109079276",
                        "name": "Zhiwei Liu"
                    },
                    {
                        "authorId": "2186150892",
                        "name": "Shan Li"
                    },
                    {
                        "authorId": "34223028",
                        "name": "Q. Song"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "288e8a0caa643b39ef655ec2892b5fb28d80d8cf",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-10670",
                    "ArXiv": "2301.10670",
                    "DOI": "10.48550/arXiv.2301.10670",
                    "CorpusId": 256231257
                },
                "corpusId": 256231257,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/288e8a0caa643b39ef655ec2892b5fb28d80d8cf",
                "title": "Towards Arbitrary Text-driven Image Manipulation via Space Alignment",
                "abstract": "The recent GAN inversion methods have been able to successfully invert the real image input to the corresponding editable latent code in StyleGAN. By combining with the language-vision model (CLIP), some text-driven image manipulation methods are proposed. However, these methods require extra costs to perform optimization for a certain image or a new attribute editing mode. To achieve a more efficient editing method, we propose a new Text-driven image Manipulation framework via Space Alignment (TMSA). The Space Alignment module aims to align the same semantic regions in CLIP and StyleGAN spaces. Then, the text input can be directly accessed into the StyleGAN space and be used to find the semantic shift according to the text description. The framework can support arbitrary image editing mode without additional cost. Our work provides the user with an interface to control the attributes of a given image according to text input and get the result in real time. Ex tensive experiments demonstrate our superior performance over prior works.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "48442720",
                        "name": "Yun-Hao Bai"
                    },
                    {
                        "authorId": "2069513824",
                        "name": "Zi-Qi Zhong"
                    },
                    {
                        "authorId": "30459277",
                        "name": "Chao Dong"
                    },
                    {
                        "authorId": "2108043700",
                        "name": "Weichen Zhang"
                    },
                    {
                        "authorId": "2115724245",
                        "name": "Guowei Xu"
                    },
                    {
                        "authorId": "2175625059",
                        "name": "Chun Yuan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "StyleGAN (Karras et al., 2019; 2020; 2021) in particular has a thoroughly studied latent space, which allows principled control of generated images (Bermano et al., 2022; Ha\u0308rko\u0308nen et al., 2020; Shen et al., 2020; Abdal et al., 2021; Kafri et al., 2022).",
                ", 2019; 2020; 2021) in particular has a thoroughly studied latent space, which allows principled control of generated images (Bermano et al., 2022; H\u00e4rk\u00f6nen et al., 2020; Shen et al., 2020; Abdal et al., 2021; Kafri et al., 2022)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "d33be7c99b7265652edbb2b0f832b479e2162ae1",
                "externalIds": {
                    "DBLP": "conf/icml/SauerKL0A23",
                    "ArXiv": "2301.09515",
                    "DOI": "10.48550/arXiv.2301.09515",
                    "CorpusId": 256105441
                },
                "corpusId": 256105441,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d33be7c99b7265652edbb2b0f832b479e2162ae1",
                "title": "StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis",
                "abstract": "Text-to-image synthesis has recently seen significant progress thanks to large pretrained language models, large-scale training data, and the introduction of scalable model families such as diffusion and autoregressive models. However, the best-performing models require iterative evaluation to generate a single sample. In contrast, generative adversarial networks (GANs) only need a single forward pass. They are thus much faster, but they currently remain far behind the state-of-the-art in large-scale text-to-image synthesis. This paper aims to identify the necessary steps to regain competitiveness. Our proposed model, StyleGAN-T, addresses the specific requirements of large-scale text-to-image synthesis, such as large capacity, stable training on diverse datasets, strong text alignment, and controllable variation vs. text alignment tradeoff. StyleGAN-T significantly improves over previous GANs and outperforms distilled diffusion models - the previous state-of-the-art in fast text-to-image synthesis - in terms of sample quality and speed.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "40562186",
                        "name": "Axel Sauer"
                    },
                    {
                        "authorId": "2976930",
                        "name": "Tero Karras"
                    },
                    {
                        "authorId": "36436218",
                        "name": "S. Laine"
                    },
                    {
                        "authorId": "47237027",
                        "name": "Andreas Geiger"
                    },
                    {
                        "authorId": "1761103",
                        "name": "Timo Aila"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Although they provide some control over the camera poses [36, 37, 15, 38], they lack explicit 3D understanding of the scenes."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "3dfbb5e01fd25d3674ac33bca059af6199666b0d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-09091",
                    "ArXiv": "2301.09091",
                    "DOI": "10.48550/arXiv.2301.09091",
                    "CorpusId": 256105686
                },
                "corpusId": 256105686,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3dfbb5e01fd25d3674ac33bca059af6199666b0d",
                "title": "BallGAN: 3D-aware Image Synthesis with a Spherical Background",
                "abstract": "3D-aware GANs aim to synthesize realistic 3D scenes such that they can be rendered in arbitrary perspectives to produce images. Although previous methods produce realistic images, they suffer from unstable training or degenerate solutions where the 3D geometry is unnatural. We hypothesize that the 3D geometry is underdetermined due to the insufficient constraint, i.e., being classified as real image to the discriminator is not enough. To solve this problem, we propose to approximate the background as a spherical surface and represent a scene as a union of the foreground placed in the sphere and the thin spherical background. It reduces the degree of freedom in the background field. Accordingly, we modify the volume rendering equation and incorporate dedicated constraints to design a novel 3D-aware GAN framework named BallGAN. BallGAN has multiple advantages as follows. 1) It produces more reasonable 3D geometry; the images of a scene across different viewpoints have better photometric consistency and fidelity than the state-of-the-art methods. 2) The training becomes much more stable. 3) The foreground can be separately rendered on top of different arbitrary backgrounds.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2120172982",
                        "name": "Minjung Shin"
                    },
                    {
                        "authorId": "2146739052",
                        "name": "Yunji Seo"
                    },
                    {
                        "authorId": "2206482",
                        "name": "Jeongmin Bae"
                    },
                    {
                        "authorId": "2167590889",
                        "name": "Young Sun Choi"
                    },
                    {
                        "authorId": "2118020280",
                        "name": "Hyunsung Kim"
                    },
                    {
                        "authorId": "144036125",
                        "name": "H. Byun"
                    },
                    {
                        "authorId": "2847986",
                        "name": "Youngjung Uh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For a certain attribute, they search for a certain direction in the latent space, and then alter the target attribute via moving the latent code z along the searched direction [6], [9], [10], [11], [12].",
                "However, the existing methods [6], [8], [9], [10], [11], [12] mostly require additional information like extra annotation or human selection.",
                "Some recent works [8], [10], [12] identify the essential directions in the latent space via the techniques like",
                "Some recent works [8], [10], [12], [25] search for steerable directions using techniques like Principal Component Analysis (PCA) in an unsupervised manner."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6ff9ca21f329ce14cfad61c2e6d25ca8da1785e7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-08455",
                    "ArXiv": "2301.08455",
                    "DOI": "10.48550/arXiv.2301.08455",
                    "CorpusId": 256080455
                },
                "corpusId": 256080455,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6ff9ca21f329ce14cfad61c2e6d25ca8da1785e7",
                "title": "Spatial Steerability of GANs via Self-Supervision from Discriminator",
                "abstract": "\u2014Generative models make huge progress to the photorealistic image synthesis in recent years. To enable human to steer the image generation process and customize the output, many works explore the interpretable dimensions of the latent space in GANs. Existing methods edit the attributes of the output image such as orientation or color scheme by varying the latent code along certain directions. However, these methods usually require additional human annotations for each pretrained model, and they mostly focus on editing global attributes. In this work, we propose a self-supervised approach to improve the spatial steerability of GANs without searching for steerable directions in the latent space or requiring extra annotations. Speci\ufb01cally, we design randomly sampled Gaussian heatmaps to be encoded into the intermediate layers of generative models as spatial inductive bias. Along with training the GAN model from scratch, these heatmaps are being aligned with the emerging attention of the GAN\u2019s discriminator in a self-supervised learning manner. During inference, human users can intuitively interact with the spatial heatmaps to edit the output image, such as varying the scene layout or moving objects in the scene. Extensive experiments show that the proposed method not only enables spatial editing over human faces, animal faces, outdoor scenes, and complicated indoor scenes, but also brings improvement in synthesis quality. Code, models, and demo video are available at https://genforce.github.io/eqgan-sa.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1832343458",
                        "name": "Jianyuan Wang"
                    },
                    {
                        "authorId": "49984891",
                        "name": "Ceyuan Yang"
                    },
                    {
                        "authorId": "121983635",
                        "name": "Yinghao Xu"
                    },
                    {
                        "authorId": "2117687899",
                        "name": "Yujun Shen"
                    },
                    {
                        "authorId": "2171936720",
                        "name": "Hongdong Li"
                    },
                    {
                        "authorId": "145291669",
                        "name": "Bolei Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2021], GANspace [H\u00e4rk\u00f6nen et al., 2020], InterfaceGAN [Shen et al.",
                "Recent works such as Styleflow [Abdal et al., 2021], GANspace [Ha\u0308rko\u0308nen et al., 2020], InterfaceGAN [Shen et al., 2020], and StyleSpace [Wu et al., 2021] presented techniques to discover the concepts encoded in the GAN latent."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "a9cd05d3421801ba94f76fafc74573cbf81d9a50",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-06324",
                    "ArXiv": "2301.06324",
                    "DOI": "10.48550/arXiv.2301.06324",
                    "CorpusId": 255942098
                },
                "corpusId": 255942098,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a9cd05d3421801ba94f76fafc74573cbf81d9a50",
                "title": "Img2Tab: Automatic Class Relevant Concept Discovery from StyleGAN Features for Explainable Image Classification",
                "abstract": "Traditional tabular classifiers provide explainable decision-making with interpretable features(concepts). However, using their explainability in vision tasks has been limited due to the pixel representation of images. In this paper, we design Img2Tabs that classify images by concepts to harness the explainability of tabular classifiers. Img2Tabs encode image pixels into tabular features by StyleGAN inversion. Since not all of the resulting features are class-relevant or interpretable due to their generative nature, we expect Img2Tab classifiers to discover class-relevant concepts automatically from the StyleGAN features. Thus, we propose a novel method using the Wasserstein-1 metric to quantify class-relevancy and interpretability simultaneously. Using this method, we investigate whether important features extracted by tabular classifiers are class-relevant concepts. Consequently, we determine the most effective classifier for Img2Tabs in terms of discovering class-relevant concepts automatically from StyleGAN features. In evaluations, we demonstrate concept-based explanations through importance and visualization. Img2Tab achieves top-1 accuracy that is on par with CNN classifiers and deep feature learning baselines. Additionally, we show that users can easily debug Img2Tab classifiers at the concept level to ensure unbiased and fair decision-making without sacrificing accuracy.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2138148717",
                        "name": "Y. Song"
                    },
                    {
                        "authorId": "2125878653",
                        "name": "S. K. Shyn"
                    },
                    {
                        "authorId": "2201430050",
                        "name": "Kwang-su Kim"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "1f839ee1e9682e5acfd067309b46fe855851f4b1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-05225",
                    "ArXiv": "2301.05225",
                    "DOI": "10.1109/CVPR52729.2023.01529",
                    "CorpusId": 255749195
                },
                "corpusId": 255749195,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1f839ee1e9682e5acfd067309b46fe855851f4b1",
                "title": "Domain Expansion of Image Generators",
                "abstract": "Can one inject new concepts into an already trained generative model, while respecting its existing structure and knowledge? We propose a new task - domain expansion - to address this. Given a pretrained generator and novel (but related) domains, we expand the generator to jointly model all domains, old and new, harmoniously. First, we note the generator contains a meaningful, pretrained latent space. Is it possible to minimally perturb this hard-earned representation, while maximally representing the new domains? Interestingly, we find that the latent space offers unused, \u201cdormant\u201d directions, which do not affect the output. This provides an opportunity: By \u201crepurposing\u201d these directions, we can represent new domains without perturbing the original representation. In fact, we find that pretrained generators have the capacity to add several- even hundreds - of new domains! Using our expansion method, one \u201cexpanded\u201d model can supersede numerous domain-specific models, without expanding the model size. Additionally, a single expanded generator natively supports smooth transitions between domains, as well as composition of domains. Code and project page available here.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1702992975",
                        "name": "Yotam Nitzan"
                    },
                    {
                        "authorId": "3282136",
                        "name": "Micha\u00ebl Gharbi"
                    },
                    {
                        "authorId": "2109976035",
                        "name": "Richard Zhang"
                    },
                    {
                        "authorId": "2071929129",
                        "name": "Taesung Park"
                    },
                    {
                        "authorId": "1922024303",
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    },
                    {
                        "authorId": "2177801",
                        "name": "Eli Shechtman"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Editing with pretrained GANs (StyleGAN inversionbased models).",
                "When encoders are forced to encode images into GAN\u2019s natural latent distribution, the results suffer from the lack of faithful reconstruction.",
                "Embedding images in GAN\u2019s space and exploring interpretable directions in latent codes have emerged as important research endeavors on the fixed pretrained GANs [9], [27], [28], [30], [35].",
                "There have been various architectures [3], [31] and objectives proposed to project an image to GAN\u2019s embedding.",
                "These directions are explored in supervised [27], and unsupervised ways [9], [28], [30], [35], and many directions are found for face editing, e.",
                "Facial attribute editing is also shown to be possible with pretrained GANs.",
                "To edit a facial attribute of an input image, one needs to project the image to a latent code in GANs\u2019 latent space [1] such that the generator reconstructs the input image from the latent code.",
                "Facial attribute editing task has been a popular topic among the image translation tasks, and significant improvements have been achieved with generative adversarial networks (GANs) [4], [5], [20], [26], [36], [40].",
                "We\n3 also show this behavior in the Results section 4.3 when comparing our method with state-of-the-art editing with pretrained GANs methods.",
                "For example, with the GANSpace method [9], editing di-",
                "Those methods that employ pre-trained StyleGANs rely on StyleGAN\u2019s semantically rich feature organizations.",
                "That is if the image is faithfully reconstructed, it may not lie in the true distribution of GANs latent space, and therefore, the directions do not work as expected, which prevents editing the image."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "0269a2db6657a1658b8078e4bd651b594a53689c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-04628",
                    "ArXiv": "2301.04628",
                    "DOI": "10.48550/arXiv.2301.04628",
                    "CorpusId": 255595590
                },
                "corpusId": 255595590,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0269a2db6657a1658b8078e4bd651b594a53689c",
                "title": "Face Attribute Editing with Disentangled Latent Vectors",
                "abstract": "We propose an image-to-image translation framework for facial attribute editing with disentangled interpretable latent directions. Facial attribute editing task faces the challenges of targeted attribute editing with controllable strength and disentanglement in the representations of attributes to preserve the other attributes during edits. For this goal, inspired by the latent space factorization works of fixed pretrained GANs, we design the attribute editing by latent space factorization, and for each attribute, we learn a linear direction that is orthogonal to the others. We train these directions with orthogonality constraints and disentanglement losses. To project images to semantically organized latent spaces, we set an encoder-decoder architecture with attention-based skip connections. We extensively compare with previous image translation algorithms and editing with pretrained GAN works. Our extensive experiments show that our method significantly improves over the state-of-the-arts. Project page: https://yusufdalva.github.io/vecgan",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2125378742",
                        "name": "Yusuf Dalva"
                    },
                    {
                        "authorId": "2199012510",
                        "name": "Hamza Pehlivan"
                    },
                    {
                        "authorId": "2166413451",
                        "name": "Cansu Moran"
                    },
                    {
                        "authorId": "2202040524",
                        "name": "\u00d6yk\u00fc Irmak Hatipoglu"
                    },
                    {
                        "authorId": "2202033399",
                        "name": "Ayseg\u00fcl D\u00fcndar"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0fc8bc437bd741410a5688d475bf8b0687aef039",
                "externalIds": {
                    "ArXiv": "2301.04604",
                    "DBLP": "journals/corr/abs-2301-04604",
                    "DOI": "10.48550/arXiv.2301.04604",
                    "CorpusId": 255595751
                },
                "corpusId": 255595751,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0fc8bc437bd741410a5688d475bf8b0687aef039",
                "title": "LinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis",
                "abstract": "This work presents an easy-to-use regularizer for GAN training, which helps explicitly link some axes of the latent space to a set of pixels in the synthesized image. Establishing such a connection facilitates a more convenient local control of GAN generation, where users can alter the image content only within a spatial area simply by partially resampling the latent code. Experimental results confirm four appealing properties of our regularizer, which we call LinkGAN. (1) The latent-pixel linkage is applicable to either a fixed region (\\textit{i.e.}, same for all instances) or a particular semantic category (i.e., varying across instances), like the sky. (2) Two or multiple regions can be independently linked to different latent axes, which further supports joint control. (3) Our regularizer can improve the spatial controllability of both 2D and 3D-aware GAN models, barely sacrificing the synthesis performance. (4) The models trained with our regularizer are compatible with GAN inversion techniques and maintain editability on real images.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47054925",
                        "name": "Jiapeng Zhu"
                    },
                    {
                        "authorId": "49984891",
                        "name": "Ceyuan Yang"
                    },
                    {
                        "authorId": "2117687899",
                        "name": "Yujun Shen"
                    },
                    {
                        "authorId": "2125701522",
                        "name": "Zifan Shi"
                    },
                    {
                        "authorId": "1678783",
                        "name": "Deli Zhao"
                    },
                    {
                        "authorId": "2157737759",
                        "name": "Qifeng Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The disentangled latent space learned by StyleGAN has been shown to exhibit semantic properties conducive to semantic image editing [1, 3, 16, 22, 36, 44, 51, 56, 62].",
                "Since our 3D domain adaptation is designed to preserve the properties of W and S spaces, we can perform semantic edits via InterFaceGAN [51], GANSpace [22], StyleSpace [62] etc.",
                "Since our 3D domain adaptation is designed to preserve the properties of W and S spaces, we can perform semantic edits via InterFaceGAN [53], GANSpace [23], StyleSpace [63] etc., and geometric edits using TPS (Sec."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "12f99b597fd65c9eb730cfef498b47f3fb3a5ec8",
                "externalIds": {
                    "DBLP": "conf/cvpr/AbdalL0CSWT23",
                    "ArXiv": "2301.02700",
                    "DOI": "10.1109/CVPR52729.2023.00442",
                    "CorpusId": 255546292
                },
                "corpusId": 255546292,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/12f99b597fd65c9eb730cfef498b47f3fb3a5ec8",
                "title": "3DAvatarGAN: Bridging Domains for Personalized Editable Avatars",
                "abstract": "Modern 3D-GANs synthesize geometry and texture by training on large-scale datasets with a consistent structure. Training such models on stylized, artistic data, with often unknown, highly variable geometry, and camera information has not yet been shown possible. Can we train a 3D GAN on such artistic data, while maintaining multi-view consistency and texture quality? To this end, we propose an adaptation framework, where the source domain is a pre-trained 3D-GAN, while the target domain is a 2D-GAN trained on artistic datasets. We, then, distill the knowledge from a 2D generator to the source 3D generator. To do that, we first propose an optimization-based method to align the distributions of camera parameters across domains. Second, we propose regularizations necessary to learn high-quality texture, while avoiding degenerate geometric solutions, such as flat shapes. Third, we show a deformation-based technique for modeling exaggerated geometry of artistic domains, enabling-as a byproduct- personalized geometric editing. Finally, we propose a novel inversion method for 3D-GANs linking the latent spaces of the source and the target domains. Our contributions-for the first time-allow for the generation, editing, and animation of personalized artistic 3D avatars on artistic datasets. Project Page: https:/rameenabdal.github.io/3DAvatarGAN",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "94395014",
                        "name": "Rameen Abdal"
                    },
                    {
                        "authorId": "49923155",
                        "name": "Hsin-Ying Lee"
                    },
                    {
                        "authorId": "100516590",
                        "name": "Peihao Zhu"
                    },
                    {
                        "authorId": "1752091",
                        "name": "Menglei Chai"
                    },
                    {
                        "authorId": "10753214",
                        "name": "Aliaksandr Siarohin"
                    },
                    {
                        "authorId": "1798011",
                        "name": "Peter Wonka"
                    },
                    {
                        "authorId": "145582202",
                        "name": "S. Tulyakov"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Such solutions typically first embed the given input image into the latent space of a pretrained GAN model through a process referred to as GAN inversion [20], and then perform text\u2013 conditioned manipulations in the latent space that eventually lead to semantically meaningful changes in the corresponding output images [19], [21], [22], [23]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "8b555bd1edf9ab3d8437a7a668f8cd75bc22ec2f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-02110",
                    "ArXiv": "2301.02110",
                    "DOI": "10.48550/arXiv.2301.02110",
                    "CorpusId": 255440532
                },
                "corpusId": 255440532,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8b555bd1edf9ab3d8437a7a668f8cd75bc22ec2f",
                "title": "FICE: Text-Conditioned Fashion Image Editing With Guided GAN Inversion",
                "abstract": "Fashion-image editing represents a challenging computer vision task, where the goal is to incorporate selected apparel into a given input image. Most existing techniques, known as Virtual Try-On methods, deal with this task by first selecting an example image of the desired apparel and then transferring the clothing onto the target person. Conversely, in this paper, we consider editing fashion images with text descriptions. Such an approach has several advantages over example-based virtual try-on techniques, e.g.: (i) it does not require an image of the target fashion item, and (ii) it allows the expression of a wide variety of visual concepts through the use of natural language. Existing image-editing methods that work with language inputs are heavily constrained by their requirement for training sets with rich attribute annotations or they are only able to handle simple text descriptions. We address these constraints by proposing a novel text-conditioned editing model, called FICE (Fashion Image CLIP Editing), capable of handling a wide variety of diverse text descriptions to guide the editing procedure. Specifically with FICE, we augment the common GAN inversion process by including semantic, pose-related, and image-level constraints when generating images. We leverage the capabilities of the CLIP model to enforce the semantics, due to its impressive image-text association capabilities. We furthermore propose a latent-code regularization technique that provides the means to better control the fidelity of the synthesized images. We validate FICE through rigorous experiments on a combination of VITON images and Fashion-Gen text descriptions and in comparison with several state-of-the-art text-conditioned image editing approaches. Experimental results demonstrate FICE generates highly realistic fashion images and leads to stronger editing performance than existing competing approaches.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "115771744",
                        "name": "Martin Pernu\u0161"
                    },
                    {
                        "authorId": "3140440",
                        "name": "C. Fookes"
                    },
                    {
                        "authorId": "2011218",
                        "name": "V. \u0160truc"
                    },
                    {
                        "authorId": "1704880",
                        "name": "S. Dobri\u0161ek"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We run extensive experiments with directions explored with InterfaceGAN [30], GANSpace [15], StyleClip [26], and GradCtrl [7] methods.",
                "Many methods have been proposed for finding latent directions to edit images [15, 30, 31, 33, 37].",
                "For this purpose, different GAN inversion methods are proposed, aiming to project real images to pretrained GAN latent space [15, 30, 31, 33, 37].",
                "These directions are explored both in supervised [3, 30] and unsupervised ways [15, 31, 33, 37] resulting in exploration of attribute manipulations beyond the predefined attributes of labeled datasets.",
                "For each method, the first column shows inversion, and the second shows InterfaceGAN [30] and GANSpace [15] edits."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "df046f64b404b79e87a180dff1d239a17d7b86f5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-14359",
                    "ArXiv": "2212.14359",
                    "DOI": "10.1109/CVPR52729.2023.00182",
                    "CorpusId": 255340691
                },
                "corpusId": 255340691,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/df046f64b404b79e87a180dff1d239a17d7b86f5",
                "title": "StyleRes: Transforming the Residuals for Real Image Editing with StyleGAN",
                "abstract": "We present a novel image inversion framework and a training pipeline to achieve high-fidelity image inversion with high-quality attribute editing. Inverting real images into StyleGAN's latent space is an extensively studied problem, yet the trade-off between the image reconstruction fidelity and image editing quality remains an open challenge. The low-rate latent spaces are limited in their expressiveness power for high-fidelity reconstruction. On the other hand, high-rate latent spaces result in degradation in editing quality. In this work, to achieve high-fidelity inversion, we learn residual features in higher latent codes that lower latent codes were not able to encode. This enables preserving image details in reconstruction. To achieve high-quality editing, we learn how to transform the residual features for adapting to manipulations in latent codes. We train the framework to extract residual features and transform them via a novel architecture pipeline and cycle consistency losses. We run extensive experiments and compare our method with state-of-the-art inversion methods. Qualitative metrics and visual comparisons show significant improvements. Code: https://github.com/hamzapehlivanIStyleRes.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2199012510",
                        "name": "Hamza Pehlivan"
                    },
                    {
                        "authorId": "2125378742",
                        "name": "Yusuf Dalva"
                    },
                    {
                        "authorId": "2130620",
                        "name": "A. Dundar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Note that prior works that manipulate one feature at a time in latent space [1, 9, 31, 36, 40, 43] are not applicable in our context.",
                "Other works use pre-trained image generators such as StyleGAN [16, 17] to achieve high naturalness, but often only focus on the manipulation of attributes in the facial images [9, 31, 36, 43]; not de-identification.",
                "Identity disentanglement in latent space has not been addressed by prior work and is not possible with existing methods [9, 31, 43].",
                "Prior work has shown how such disentanglement can be leveraged to manipulate selected facial features [9, 31, 36, 43].",
                "The gender is corrected in the third row by using global direction of GANSpace (only for faces with incorrect gender).",
                "In the context of non-identity related features, prior research [9, 43] has found that more fine-grained control of facial features is possible when manipulating individual channels.",
                "To show this, note that there exist many tools for controlling non-identity related attributes, including StyleSpace [43], GANSpace [9], InterfaceGAN [36], StyleCLIP [31]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b5a332f584e7a728e3c7d366cd29920f03e4e789",
                "externalIds": {
                    "DBLP": "journals/popets/LeC23",
                    "ArXiv": "2212.13791",
                    "DOI": "10.48550/arXiv.2212.13791",
                    "CorpusId": 253177663
                },
                "corpusId": 253177663,
                "publicationVenue": {
                    "id": "d5dc4224-e4c3-43c9-918a-bd6326650b5b",
                    "name": "Proceedings on Privacy Enhancing Technologies",
                    "alternate_names": [
                        "Proc Priv Enhancing Technol"
                    ],
                    "issn": "2299-0984",
                    "url": "https://www.degruyter.com/view/j/popets"
                },
                "url": "https://www.semanticscholar.org/paper/b5a332f584e7a728e3c7d366cd29920f03e4e789",
                "title": "StyleID: Identity Disentanglement for Anonymizing Faces",
                "abstract": "Privacy of machine learning models is one of the remaining challenges that hinder the broad adoption of Artificial Intelligent (AI). This paper considers this problem in the context of image datasets containing faces. Anonymization of such datasets is becoming increasingly important due to their central role in the training of autonomous cars, for example, and the vast amount of data generated by surveillance systems. While most prior work de-identifies facial images by modifying identity features in pixel space, we instead project the image onto the latent space of a Generative Adversarial Network (GAN) model, find the features that provide the biggest identity disentanglement, and then manipulate these features in latent space, pixel space, or both. The main contribution of the paper is the design of a feature-preserving anonymization framework, StyleID, which protects the individuals\u2019 identity, while preserving as many characteristics of the original faces in the image dataset as possible. As part of the contribution, we present a novel disentanglement metric, three complementing disentanglement methods, and new insights into identity disentanglement. StyleID provides tunable privacy, has low computational complexity, and is shown to outperform current state-of-the-art solutions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2057524923",
                        "name": "Minh-Ha Le"
                    },
                    {
                        "authorId": "1739691",
                        "name": "Niklas Carlsson"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "03f6f90a6f5c23af75aa8074271024bfa39bdcb2",
                "externalIds": {
                    "DBLP": "conf/cvpr/XuVDS23",
                    "ArXiv": "2212.12645",
                    "DOI": "10.1109/CVPR52729.2023.00772",
                    "CorpusId": 253083378
                },
                "corpusId": 253083378,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/03f6f90a6f5c23af75aa8074271024bfa39bdcb2",
                "title": "HandsOff: Labeled Dataset Generation With No Additional Human Annotations",
                "abstract": "Recent work leverages the expressive power of generative adversarial networks (GANs) to generate labeled synthetic datasets. These dataset generation methods often require new annotations of synthetic images, which forces practitioners to seek out annotators, curate a set of synthetic images, and ensure the quality of generated labels. We introduce the HandsOff framework, a technique capable of producing an unlimited number of synthetic images and corresponding labels after being trained on less than 50 preexisting labeled images. Our framework avoids the practical drawbacks of prior work by unifying the field of GAN inversion with dataset generation. We generate datasets with rich pixel-wise labels in multiple challenging domains such as faces, cars, full-body human poses, and urban driving scenes. Our method achieves state-of-the-art performance in semantic segmentation, keypoint detection, and depth estimation compared to prior dataset generation approaches and transfer learning baselines. We additionally showcase its ability to address broad challenges in model development which stem from fixed, hand-annotated datasets, such as the long-tail problem in semantic segmentation. Project page: austinxu87.github.io/handsoff.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2064635172",
                        "name": "Austin Xu"
                    },
                    {
                        "authorId": "2066512744",
                        "name": "Mariya I. Vasileva"
                    },
                    {
                        "authorId": "2298523",
                        "name": "Achal Dave"
                    },
                    {
                        "authorId": "2069504401",
                        "name": "Arjun Seshadri"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "53a85ad75831fb0780c3a3a10ef78a833bf2dd58",
                "externalIds": {
                    "ArXiv": "2212.10229",
                    "CorpusId": 257900868
                },
                "corpusId": 257900868,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/53a85ad75831fb0780c3a3a10ef78a833bf2dd58",
                "title": "StyleDomain: Efficient and Lightweight Parameterizations of StyleGAN for One-shot and Few-shot Domain Adaptation",
                "abstract": "Domain adaptation of GANs is a problem of fine-tuning GAN models pretrained on a large dataset (e.g. StyleGAN) to a specific domain with few samples (e.g. painting faces, sketches, etc.). While there are many methods that tackle this problem in different ways, there are still many important questions that remain unanswered. In this paper, we provide a systematic and in-depth analysis of the domain adaptation problem of GANs, focusing on the StyleGAN model. We perform a detailed exploration of the most important parts of StyleGAN that are responsible for adapting the generator to a new domain depending on the similarity between the source and target domains. As a result of this study, we propose new efficient and lightweight parameterizations of StyleGAN for domain adaptation. Particularly, we show that there exist directions in StyleSpace (StyleDomain directions) that are sufficient for adapting to similar domains. For dissimilar domains, we propose Affine+ and AffineLight+ parameterizations that allows us to outperform existing baselines in few-shot adaptation while having significantly less training parameters. Finally, we examine StyleDomain directions and discover their many surprising properties that we apply for domain mixing and cross-domain image morphing. Source code can be found at https://github.com/AIRI-Institute/StyleDomain.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "82901572",
                        "name": "Aibek Alanov"
                    },
                    {
                        "authorId": "2165156333",
                        "name": "Vadim Titov"
                    },
                    {
                        "authorId": "2184296381",
                        "name": "M. Nakhodnov"
                    },
                    {
                        "authorId": "2492721",
                        "name": "D. Vetrov"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Also, we have compared our method with state-of-the-art methods [36, 4, 5, 24] on face attribute manipulation [17, 25].",
                "8) with offthe-shelf GAN manipulation approaches [30, 25, 17]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3942fe497cad43993acc2d9cffc7b4caba5b2d9e",
                "externalIds": {
                    "ArXiv": "2212.09262",
                    "CorpusId": 259108981
                },
                "corpusId": 259108981,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3942fe497cad43993acc2d9cffc7b4caba5b2d9e",
                "title": "Out-of-domain GAN inversion via Invertibility Decomposition for Photo-Realistic Human Face Manipulation",
                "abstract": "The fidelity of Generative Adversarial Networks (GAN) inversion is impeded by Out-Of-Domain (OOD) areas (e.g., background, accessories) in the image. Detecting the OOD areas beyond the generation ability of the pre-trained model and blending these regions with the input image can enhance fidelity. The\"invertibility mask\"figures out these OOD areas, and existing methods predict the mask with the reconstruction error. However, the estimated mask is usually inaccurate due to the influence of the reconstruction error in the In-Domain (ID) area. In this paper, we propose a novel framework that enhances the fidelity of human face inversion by designing a new module to decompose the input images to ID and OOD partitions with invertibility masks. Unlike previous works, our invertibility detector is simultaneously learned with a spatial alignment module. We iteratively align the generated features to the input geometry and reduce the reconstruction error in the ID regions. Thus, the OOD areas are more distinguishable and can be precisely predicted. Then, we improve the fidelity of our results by blending the OOD areas from the input image with the ID GAN inversion results. Our method produces photo-realistic results for real-world human face image inversion and manipulation. Extensive experiments demonstrate our method's superiority over existing methods in the quality of GAN inversion and attribute manipulation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2150440804",
                        "name": "Xin Yang"
                    },
                    {
                        "authorId": "2027354611",
                        "name": "Xiaogang Xu"
                    },
                    {
                        "authorId": "2109289860",
                        "name": "Ying-Cong Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In contrast to the previous, supervised method, GANSpace [81] describes an unsupervised approach for discovering semantic directions.",
                "In contrast to the previous, supervised method, GANSpace [72] describes an unsupervised approach for discovering semantic directions.",
                "To achieve a more disentangled editing it was proposed to orthogonalize a discovered set of semantic Figure 15: Moving along the 10th principal component in 7-8 layers changes hair color [72]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "c5e4571f52b6e36e41574b2035d3bbf31b6a0ad8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-09102",
                    "ArXiv": "2212.09102",
                    "DOI": "10.48550/arXiv.2212.09102",
                    "CorpusId": 254853703
                },
                "corpusId": 254853703,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c5e4571f52b6e36e41574b2035d3bbf31b6a0ad8",
                "title": "Face Generation and Editing with StyleGAN: A Survey",
                "abstract": "Our goal with this survey is to provide an overview of the state of the art deep learning methods for face generation and editing using StyleGAN. The survey covers the evolution of StyleGAN, from PGGAN to StyleGAN3, and explores relevant topics such as suitable metrics for training, different latent representations, GAN inversion to latent spaces of StyleGAN, face image editing, cross-domain face stylization, face restoration, and even Deepfake applications. We aim to provide an entry point into the field for readers that have basic knowledge about the field of deep learning and are looking for an accessible introduction and overview.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "33016134",
                        "name": "Andrew Melnik"
                    },
                    {
                        "authorId": "2197479315",
                        "name": "Maksim Miasayedzenkau"
                    },
                    {
                        "authorId": "2197478199",
                        "name": "Dzianis Makarovets"
                    },
                    {
                        "authorId": "2197480088",
                        "name": "Dzianis Pirshtuk"
                    },
                    {
                        "authorId": "146572560",
                        "name": "Eren Akbulut"
                    },
                    {
                        "authorId": "2197463256",
                        "name": "Dennis Holzmann"
                    },
                    {
                        "authorId": "2197479310",
                        "name": "Tarek Renusch"
                    },
                    {
                        "authorId": "2197476140",
                        "name": "Gustav Reichert"
                    },
                    {
                        "authorId": "30258243",
                        "name": "H. Ritter"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b916302feec5be76fc4aa935202008f6ae638efa",
                "externalIds": {
                    "DBLP": "conf/cvpr/WuLZKB0L0C23",
                    "ArXiv": "2212.08698",
                    "DOI": "10.1109/CVPR52729.2023.00189",
                    "CorpusId": 254854155
                },
                "corpusId": 254854155,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b916302feec5be76fc4aa935202008f6ae638efa",
                "title": "Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models",
                "abstract": "Generative models have been widely studied in computer vision. Recently, diffusion models have drawn substantial attention due to the high quality of their generated images. A key desired property of image generative models is the ability to disentangle different attributes, which should enable modification towards a style without changing the semantic content, and the modification parameters should generalize to different images. Previous studies have found that generative adversarial networks (GANs) are inherently endowed with such disentanglement capability, so they can perform disentangled image editing without re-training or fine-tuning the network. In this work, we explore whether diffusion models are also inherently equipped with such a capability. Our finding is that for stable diffusion models, by partially changing the input text embedding from a neutral description (e.g., \u201ca photo of person\u201d) to one with style (e.g., \u201ca photo of person with smile\u201d) while fixing all the Gaussian random noises introduced during the denoising process, the generated images can be modified towards the target style without changing the semantic content. Based on this finding, we further propose a simple, light-weight image editing algorithm where the mixing weights of the two text embeddings are optimized for style matching and content preservation. This entire process only involves optimizing over around 50 parameters and does not fine-tune the diffusion model itself. Experiments show that the proposed method can modify a wide range of attributes, with the performance outperforming diffusion-model-based image-editing algorithms that require fine-tuning. The optimized weights generalize well to different images. Our code is publicly available at https://github.com/UCSB-NLP-Chang/DiffusionDisentanglement.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2112250365",
                        "name": "Qiucheng Wu"
                    },
                    {
                        "authorId": "2136367839",
                        "name": "Yujian Liu"
                    },
                    {
                        "authorId": "7574699",
                        "name": "Handong Zhao"
                    },
                    {
                        "authorId": "37493415",
                        "name": "Ajinkya Kale"
                    },
                    {
                        "authorId": "73607354",
                        "name": "T. Bui"
                    },
                    {
                        "authorId": "2148832584",
                        "name": "Tong Yu"
                    },
                    {
                        "authorId": "2152934157",
                        "name": "Zhe Lin"
                    },
                    {
                        "authorId": "37873860",
                        "name": "Yang Zhang"
                    },
                    {
                        "authorId": "2122374354",
                        "name": "Shiyu Chang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "7224248d35e0a877a1e5bae1d9664fd9e6cc0045",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-07277",
                    "ArXiv": "2212.07277",
                    "DOI": "10.48550/arXiv.2212.07277",
                    "CorpusId": 254636213
                },
                "corpusId": 254636213,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/7224248d35e0a877a1e5bae1d9664fd9e6cc0045",
                "title": "ContraFeat: Contrasting Deep Features for Semantic Discovery",
                "abstract": "StyleGAN has shown strong potential for disentangled semantic control, thanks to its special design of multi-layer intermediate latent variables. However, existing semantic discovery methods on StyleGAN rely on manual selection of modified latent layers to obtain satisfactory manipulation results, which is tedious and demanding. In this paper, we propose a model that automates this process and achieves state-of-the-art semantic discovery performance. The model consists of an attention-equipped navigator module and losses contrasting deep-feature changes. We propose two model variants, with one contrasting samples in a binary manner, and another one contrasting samples with learned prototype variation patterns. The proposed losses are computed with pretrained deep features, based on our assumption that the features implicitly possess the desired semantic variation structure including consistency and orthogonality. Additionally, we design two metrics to quantitatively evaluate the performance of semantic discovery methods on FFHQ dataset, and also show that disentangled representations can be derived via a simple training process. Experimentally, we show that our models achieve state-of-the-art semantic discovery results without relying on layer-wise manual selection, and these discovered semantics can be used to manipulate real-world images.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "26415158",
                        "name": "Xinqi Zhu"
                    },
                    {
                        "authorId": "93374657",
                        "name": "Chang Xu"
                    },
                    {
                        "authorId": "2140448089",
                        "name": "Dacheng Tao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Inspired by the potential of StyleGAN, several image editing methods based on StyleGAN have been proposed [11, 23, 27].",
                "The rapid development of Generative Adversarial Networks (GANs) in image generation and synthesis tasks [9, 13\u201316] has led to the appearance of image editing methods [11, 23, 26, 27, 30].",
                "For example, GANSpace [11] finds the directions which enable to edit a particular attribute by Principal Component Analysis (PCA) of latent codes.",
                "GANSpace (NeurIPS\u2019 2020) [11] gets the directions that can edit image attributes by PCA.",
                "For unsupervised approaches, GANSpace [11] finds editable directions for some attributes by performing PCA of latent codes.",
                "There are several image editing methods based on the disentangled latent space of StyleGAN [3, 11, 23, 25, 27, 28].",
                "Among GAN-based image editing methods, it has been reported that recently proposed StyleGAN [15] has a disentanglement latent space, and it can edit a particular attribute of the synthesized image by moving the latent code along the certain direction in the latent space [11]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6351bc0cb0d77b84cf6712bebec96e8996c312f2",
                "externalIds": {
                    "DBLP": "conf/mmasia/OhagaT0H22",
                    "DOI": "10.1145/3551626.3564949",
                    "CorpusId": 254294073
                },
                "corpusId": 254294073,
                "publicationVenue": {
                    "id": "94b02de5-862a-4b6b-b584-500c2d265667",
                    "name": "ACM Multimedia Asia",
                    "type": "conference",
                    "alternate_names": [
                        "MMAsia",
                        "ACM Multimedia Asia"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6351bc0cb0d77b84cf6712bebec96e8996c312f2",
                "title": "Disentangled Image Attribute Editing in Latent Space via Mask-Based Retention Loss",
                "abstract": "We propose an image attribute editing method with the mask-based retention loss. Although conventional image attribute editing methods can edit a particular attribute, they cannot retain non-editing attributes including unknown attributes before and after editing, which causes unexpected changes in the edited images. We solve this problem by dividing the pre- and post-edited images into the editing and non-editing regions and increasing the image similarity in the non-editing regions. In this paper, we introduce the novel mask-based retention loss to retain the non-editing regions. To compute the mask-based retention loss, we divide the images into the editing and non-editing regions by using a binary mask generated from the difference between the pre- and post-edited images. Experimental results show that our proposed method is qualitatively and quantitatively superior to state-of-the-art methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2142750140",
                        "name": "S. Ohaga"
                    },
                    {
                        "authorId": "3470264",
                        "name": "Ren Togo"
                    },
                    {
                        "authorId": "144392699",
                        "name": "Takahiro Ogawa"
                    },
                    {
                        "authorId": "144029207",
                        "name": "M. Haseyama"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b10ca22eeedafdc3f273a99cbda6fbf159354d18",
                "externalIds": {
                    "DBLP": "journals/ci/WangZHLL23",
                    "DOI": "10.1111/coin.12564",
                    "CorpusId": 254660121
                },
                "corpusId": 254660121,
                "publicationVenue": {
                    "id": "03e270f3-9983-44e1-9d91-754044085687",
                    "name": "International Conference on Climate Informatics",
                    "type": "conference",
                    "alternate_names": [
                        "CI",
                        "Comput Intell",
                        "Computational Intelligence",
                        "Int Conf Clim Informatics",
                        "International Workshop Climate Informatics",
                        "Computational intelligence",
                        "Int Workshop Clim Informatics",
                        "Comput intell"
                    ],
                    "issn": "0824-7935",
                    "url": "http://www.wiley.com/WileyCDA/WileyTitle/productCd-COIN.html",
                    "alternate_urls": [
                        "https://onlinelibrary.wiley.com/journal/14678640"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b10ca22eeedafdc3f273a99cbda6fbf159354d18",
                "title": "Current status, application, and challenges of the interpretability of generative adversarial network models",
                "abstract": "The generative adversarial network (GAN) is one of the most promising methods in the field of unsupervised learning. Model developers, users, and other interested people are highly concerned about the GAN mechanism where the generative model and the discriminative model learn from each other in a gameplay manner, which generates a causal relationship among output features, internal network structure, feature extraction process, and output results. Through the study of the interpretability of GANs, the validity, reliability, and robustness of the application of GANs can be verified, and the weaknesses of the GANs in specific applications can be diagnosed, which can provide support for designing better network structures. It can also improve security and reduce the decision\u2010making and prediction risks brought by GANs. In this article, the study of the interpretability of GANs is explored, and ways of the evaluation of the application effect of GAN interpretability techniques are analyzed. Besides, the effect of interpretable GANs in fields such as medical treatment and military is discussed, and current limitations and future challenges are demonstrated.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2196113926",
                        "name": "Sulin Wang"
                    },
                    {
                        "authorId": "2195744512",
                        "name": "Chengqiang Zhao"
                    },
                    {
                        "authorId": "2111400588",
                        "name": "Lingling Huang"
                    },
                    {
                        "authorId": "33232556",
                        "name": "Yuanwei Li"
                    },
                    {
                        "authorId": "2112079714",
                        "name": "Ruochen Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "80ef878f039003fe5f9338d6bcabb516563a8af6",
                "externalIds": {
                    "DBLP": "journals/pami/SongSW23b",
                    "ArXiv": "2212.05599",
                    "DOI": "10.1109/TPAMI.2022.3228979",
                    "CorpusId": 254564483,
                    "PubMed": "37015375"
                },
                "corpusId": 254564483,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/80ef878f039003fe5f9338d6bcabb516563a8af6",
                "title": "Orthogonal SVD Covariance Conditioning and Latent Disentanglement",
                "abstract": "Inserting an SVD meta-layer into neural networks is prone to make the covariance ill-conditioned, which could harm the model in the training stability and generalization abilities. In this article, we systematically study how to improve the covariance conditioning by enforcing orthogonality to the Pre-SVD layer. Existing orthogonal treatments on the weights are first investigated. However, these techniques can improve the conditioning but would hurt the performance. To avoid such a side effect, we propose the Nearest Orthogonal Gradient (NOG) and Optimal Learning Rate (OLR). The effectiveness of our methods is validated in two applications: decorrelated Batch Normalization (BN) and Global Covariance Pooling (GCP). Extensive experiments on visual recognition demonstrate that our methods can simultaneously improve covariance conditioning and generalization. The combinations with orthogonal weight can further boost the performance. Moreover, we show that our orthogonality techniques can benefit generative models for better latent disentanglement through a series of experiments on various benchmarks. Code is available at: https://github.com/KingJamesSong/OrthoImproveCond.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152601878",
                        "name": "Yue Song"
                    },
                    {
                        "authorId": "1429806753",
                        "name": "N. Sebe"
                    },
                    {
                        "authorId": "40397893",
                        "name": "Wei Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a6ad30123bef4b19ee40c3d63cfabf00d211f0ef",
                "externalIds": {
                    "ArXiv": "2212.04489",
                    "DBLP": "conf/cvpr/ZhangHGMR23",
                    "DOI": "10.1109/CVPR52729.2023.00584",
                    "CorpusId": 254408758
                },
                "corpusId": 254408758,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a6ad30123bef4b19ee40c3d63cfabf00d211f0ef",
                "title": "SINE: SINgle Image Editing with Text-to-Image Diffusion Models",
                "abstract": "Recent works on diffusion models have demonstrated a strong capability for conditioning image generation, e.g., text-guided image synthesis. Such success inspires many efforts trying to use large-scale pre-trained diffusion models for tackling a challenging problem-real image editing. Works conducted in this area learn a unique textual token corresponding to several images containing the same object. However, under many circumstances, only one image is available, such as the painting of the Girl with a Pearl Earring. Using existing works on fine-tuning the pre-trained diffusion models with a single image causes severe overfitting issues. The information leakage from the pre-trained diffusion models makes editing can not keep the same content as the given image while creating new features depicted by the language guidance. This work aims to address the problem of single-image editing. We propose a novel model-based guidance built upon the classifier-free guidance so that the knowledge from the model trained on a single image can be distilled into the pre-trained diffusion model, enabling content creation even with one given image. Additionally, we propose a patch-based fine-tuning that can effectively help the model generate images of arbitrary resolution. We provide extensive experiments to validate the design choices of our approach and show promising editing capabilities, including changing style, content addition, and object manipulation. Our code is made publicly available here.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2128662401",
                        "name": "Zhixing Zhang"
                    },
                    {
                        "authorId": "3471102",
                        "name": "Ligong Han"
                    },
                    {
                        "authorId": "2461629",
                        "name": "Arna Ghosh"
                    },
                    {
                        "authorId": "1711560",
                        "name": "Dimitris N. Metaxas"
                    },
                    {
                        "authorId": "2111473627",
                        "name": "Jian Ren"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a68ff71d338bf6f892c0731952ddbfdc2cfbf343",
                "externalIds": {
                    "DBLP": "conf/cvpr/BaiYXLYS23",
                    "ArXiv": "2212.03752",
                    "DOI": "10.1109/CVPR52729.2023.01164",
                    "CorpusId": 254366636
                },
                "corpusId": 254366636,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a68ff71d338bf6f892c0731952ddbfdc2cfbf343",
                "title": "GLeaD: Improving GANs with A Generator-Leading Task",
                "abstract": "Generative adversarial network (GAN) is formulated as a two-player game between a generator (G) and a discriminator (D), where D is asked to differentiate whether an image comes from real data or is produced by G. Under such a formulation, D plays as the rule maker and hence tends to dominate the competition. Towards a fairer game in GANs, we propose a new paradigm for adversarial training, which makes G assign a task to D as well. Specifically, given an image, we expect D to extract representative features that can be adequately decoded by G to reconstruct the input. That way, instead of learning freely, D is urged to align with the view of G for domain classification. Experimental results on various datasets demonstrate the substantial superiority of our approach over the baselines. For instance, we improve the FID of StyleGAN2 from 4.30 to 2.55 on LSUN Bedroom and from 4.04 to 2.82 on LSUN Church. We believe that the pioneering attempt present in this work could inspire the community with better designed generator-leading tasks for GAN improvement. Project page is at https://ezioby.github.io/glead/.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2083548117",
                        "name": "Qingyan Bai"
                    },
                    {
                        "authorId": "49984891",
                        "name": "Ceyuan Yang"
                    },
                    {
                        "authorId": "121983635",
                        "name": "Yinghao Xu"
                    },
                    {
                        "authorId": "46522599",
                        "name": "Xihui Liu"
                    },
                    {
                        "authorId": "2108585311",
                        "name": "Yujiu Yang"
                    },
                    {
                        "authorId": "2117687899",
                        "name": "Yujun Shen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "48484c24356e1473480375d65796bf5dc3dffe63",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-05056",
                    "ArXiv": "2212.05056",
                    "DOI": "10.48550/arXiv.2212.05056",
                    "CorpusId": 254564376
                },
                "corpusId": 254564376,
                "publicationVenue": {
                    "id": "91a636b2-7d67-48f5-b326-712bc1218671",
                    "name": "Journal of Cybersecurity",
                    "type": "journal",
                    "alternate_names": [
                        "J Cybersecur"
                    ],
                    "issn": "2057-2093",
                    "alternate_issns": [
                        "2057-2085"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/48484c24356e1473480375d65796bf5dc3dffe63",
                "title": "Testing Human Ability To Detect Deepfake Images of Human Faces",
                "abstract": "\n \u2018Deepfakes\u2019 are computationally created entities that falsely represent reality. They can take image, video, and audio modalities, and pose a threat to many areas of systems and societies, comprising a topic of interest to various aspects of cybersecurity and cybersafety. In 2020, a workshop consulting AI experts from academia, policing, government, the private sector, and state security agencies ranked deepfakes as the most serious AI threat. These experts noted that since fake material can propagate through many uncontrolled routes, changes in citizen behaviour may be the only effective defence. This study aims to assess human ability to identify image deepfakes of human faces (these being uncurated output from the StyleGAN2 algorithm as trained on the FFHQ dataset) from a pool of non-deepfake images (these being random selection of images from the FFHQ dataset), and to assess the effectiveness of some simple interventions intended to improve detection accuracy. Using an online survey, participants (N\u00a0=\u00a0280) were randomly allocated to one of four groups: a control group, and three assistance interventions. Each participant was shown a sequence of 20 images randomly selected from a pool of 50 deepfake images of human faces and 50 images of real human faces. Participants were asked whether each image was AI-generated or not, to report their confidence, and to describe the reasoning behind each response. Overall detection accuracy was only just above chance and none of the interventions significantly improved this. Of equal concern was the fact that participants\u2019 confidence in their answers was high and unrelated to accuracy. Assessing the results on a per-image basis reveals that participants consistently found certain images easy to label correctly and certain images difficult, but reported similarly high confidence regardless of the image. Thus, although participant accuracy was 62% overall, this accuracy across images ranged quite evenly between 85 and 30%, with an accuracy of below 50% for one in every five images. We interpret the findings as suggesting that there is a need for an urgent call to action to address this threat.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2195341674",
                        "name": "Sergi D. Bray"
                    },
                    {
                        "authorId": "31673555",
                        "name": "Shane D. Johnson"
                    },
                    {
                        "authorId": "6032930",
                        "name": "Bennett Kleinberg"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For StyleNeRF and EG3D, we apply 2D editing method [7] on the frontal image and get an inverted latent code.",
                "We can apply any existing latent code editing methods [7, 21] on the frontal latent code w\u0304 to get wstyle.",
                "Method 360\u25e6 Editing w/o 3D Auxiliary data Real image Editing 3D Consistent Real-time Editing 2D GANs [7, 10, 21] 7 3 3 7 3 3D-aware [1, 2, 6, 16] 7 3 7 3 7 NeRF editing [12, 31] 3 7 7 3 7 Ours 3 3 3 3 3",
                "To ensure the high-fidelity, we restrict the camera pose range to lie in StyleGAN\u2019s training pose distribution [1,16,26].",
                "Using existing latent space manipulation techniques, 2D GANs [7,10,21] can produce stylized images from multiple camera poses.",
                "Recent works [7, 21] found that the pre-trained StyleGAN has a well-behaved latent space, which involves interpretable styles such as pose, !! !\" D A ! .",
                "Furthermore, when we vary the pose, the baselines degrade quickly: GANSpace incurs obvious background; InterFaceGAN has a large shift; EG3D obtains blurry results.",
                "Previous 2D GAN manipulation works [7, 30, 32] show that the latent space of pre-trained GANs can be decomposed to control the image",
                "However, since the training dataset cannot cover a diverse and continuous range of viewing directions, both the supervised [11, 21, 23, 29] and unsupervised [7, 22, 30, 32] manipulation methods struggle to make accurate and outof-domain control of viewing directions.",
                "For GANSpace and InterFaceGAN, we use their own stylization\nmethod for editing.",
                "w/o Editing w Editing ID\u2191 PSNR\u2191 SSIM\u2191 LPIPS\u2193 APS\u2191 Pose \u2193 GANSpace [7] 44.",
                "For 2D manipulation baselines, we choose GANSpace [7] and InterFaceGAN [21], both of which are able to control the pose direction.",
                "To realize this, we restrict the pose range to StyleGAN\u2019s training pose domain and align the images on FaceScape.",
                "Previous 2D GAN manipulation works [7, 30, 32] show that the latent space of pre-trained GANs can be decomposed to control the image\n1Code and dataset will be released.\ngeneration process for attribute editing."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "d799f9323b65a13794ebe85670f5bd49058e73b0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-03848",
                    "ArXiv": "2212.03848",
                    "DOI": "10.48550/arXiv.2212.03848",
                    "CorpusId": 254366407
                },
                "corpusId": 254366407,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d799f9323b65a13794ebe85670f5bd49058e73b0",
                "title": "NeRFEditor: Differentiable Style Decomposition for Full 3D Scene Editing",
                "abstract": "We present NeRFEditor, an efficient learning framework for 3D scene editing, which takes a video captured over 360{\\deg} as input and outputs a high-quality, identity-preserving stylized 3D scene. Our method supports diverse types of editing such as guided by reference images, text prompts, and user interactions. We achieve this by encouraging a pre-trained StyleGAN model and a NeRF model to learn from each other mutually. Specifically, we use a NeRF model to generate numerous image-angle pairs to train an adjustor, which can adjust the StyleGAN latent code to generate high-fidelity stylized images for any given angle. To extrapolate editing to GAN out-of-domain views, we devise another module that is trained in a self-supervised learning manner. This module maps novel-view images to the hidden space of StyleGAN that allows StyleGAN to generate stylized images on novel views. These two modules together produce guided images in 360{\\deg}views to finetune a NeRF to make stylization effects, where a stable fine-tuning strategy is proposed to achieve this. Experiments show that NeRFEditor outperforms prior work on benchmark and real-world scenes with better editability, fidelity, and identity preservation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118330577",
                        "name": "Chun-Yu Sun"
                    },
                    {
                        "authorId": "2108078235",
                        "name": "Yanbing Liu"
                    },
                    {
                        "authorId": "2109488495",
                        "name": "Junlin Han"
                    },
                    {
                        "authorId": "145273587",
                        "name": "Stephen Gould"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In particular, with the improvement of analysis and manipulation techniques for recent Generative Adversarial Network (GAN) models [8, 12, 21, 27, 28], we simply can do this task by manipulating a given image\u2019s latent feature."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "8240048df28571e36e12aaab9f0ce249d4e7cd37",
                "externalIds": {
                    "DBLP": "conf/cvpr/KimSKCKY23",
                    "ArXiv": "2212.02802",
                    "DOI": "10.1109/CVPR52729.2023.00590",
                    "CorpusId": 254275011
                },
                "corpusId": 254275011,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8240048df28571e36e12aaab9f0ce249d4e7cd37",
                "title": "Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding",
                "abstract": "Inspired by the impressive performance of recent face image editing methods, several studies have been naturally proposed to extend these methods to the face video editing task. One of the main challenges here is temporal consistency among edited frames, which is still unresolved. To this end, we propose a novel face video editing framework based on diffusion autoencoders that can successfully extract the decomposed features - for the first time as a face video editing model - of identity and motion from a given video. This modeling allows us to edit the video by simply manipulating the temporally invariant feature to the desired direction for the consistency. Another unique strength of our model is that, since our model is based on diffusion models, it can satisfy both reconstruction and edit capabilities at the same time, and is robust to corner cases in wild face videos (e.g. occluded faces) unlike the existing GAN-based methods.11Project page: https://diff-video-ae.github.io",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2135792196",
                        "name": "Gyeongman Kim"
                    },
                    {
                        "authorId": "25066587",
                        "name": "Hajin Shim"
                    },
                    {
                        "authorId": "2118020280",
                        "name": "Hyunsung Kim"
                    },
                    {
                        "authorId": "30187096",
                        "name": "Yunjey Choi"
                    },
                    {
                        "authorId": "2173728733",
                        "name": "Junho Kim"
                    },
                    {
                        "authorId": "1720494",
                        "name": "Eunho Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Additionally, some recent research [52, 120] manipulated two or more face attributes and controlled the variation intensity via interpreting the separation boundaries between different facial styles."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "5710bacf983eb10ace5fe038ae6af73b5fb8f12a",
                "externalIds": {
                    "DBLP": "journals/csur/KammounSTOA23",
                    "DOI": "10.1145/3527850",
                    "CorpusId": 247795062
                },
                "corpusId": 247795062,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5710bacf983eb10ace5fe038ae6af73b5fb8f12a",
                "title": "Generative Adversarial Networks for Face Generation: A Survey",
                "abstract": "Recently, generative adversarial networks (GANs) have progressed enormously, which makes them able to learn complex data distributions in particular faces. More and more efficient GAN architectures have been designed and proposed to learn the different variations of faces, such as cross pose, age, expression, and style. These GAN-based approaches need to be reviewed, discussed, and categorized in terms of architectures, applications, and metrics. Several reviews that focus on the use and advances of GAN in general have been proposed. However, to the best of our knowledge, the GAN models applied to the face, which we call facial GANs, have never been addressed. In this article, we review facial GANs and their different applications. We mainly focus on architectures, problems, and performance evaluation with respect to each application and used datasets. More precisely, we review the progress of architectures and discuss the contributions and limits of each. Then, we expose the encountered problems of facial GANs and propose solutions to handle them. Additionally, as GAN evaluation has become a notable current defiance, we investigate the state-of-the-art quantitative and qualitative evaluation metrics and their applications. We conclude this work with a discussion on the face generation challenges and propose open research issues.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2129299138",
                        "name": "Amina Kammoun"
                    },
                    {
                        "authorId": "3086366",
                        "name": "Rim Slama"
                    },
                    {
                        "authorId": "2397984",
                        "name": "Hedi Tabia"
                    },
                    {
                        "authorId": "2784229",
                        "name": "T. Ouni"
                    },
                    {
                        "authorId": "2160731650",
                        "name": "Mohmed Abid"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our experiments show that latent directions found by prior methods adapted to SIS [10, 29] lead to weaker class edits, comparable to random directions (see Sec.",
                "GANSpace [10] performed PCA on the intermediate generator features, discovering useful directions in the latent space resulting from layerwise perturbations along the principal directions.",
                "As the dependence on supervision limits the practical use of these methods, [10, 29, 30, 35, 36, 41] investigated unsupervised discovery of GAN controls.",
                "Prior GAN control methods were mostly evaluated by subjective visual inspection [10, 29, 41].",
                "Ctrl-SIS is compared against the two related latent discovery methods GANSpace [10] and SeFa [29], using the authors\u2019 code 2.",
                "On the other hand, prior work has extensively studied the latent space of unconditional GANs [8, 10, 25, 29, 35, 41], finding interpretable latent directions which activate distinctive factors of variations in the generation process in an unsupervised fashion, without exploiting reference images.",
                "Following GANSpaceStyleGAN2 [10] and SeFA-StyleGAN2 [29], we train all latent direction methods on features extracted from the normalization layers of each ResNet block in the generator."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "73ac1507bef087d1a2222d06180718e4db9d34e3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-01455",
                    "ArXiv": "2212.01455",
                    "DOI": "10.1109/CVPRW59228.2023.00076",
                    "CorpusId": 260917900
                },
                "corpusId": 260917900,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/73ac1507bef087d1a2222d06180718e4db9d34e3",
                "title": "Discovering Class-Specific GAN Controls for Semantic Image Synthesis",
                "abstract": "Prior work has extensively studied the latent space structure of GANs for unconditional image synthesis, enabling global editing of generated images by the unsupervised discovery of interpretable latent directions. However, the discovery of latent directions for conditional GANs for semantic image synthesis (SIS) has remained unexplored. In this work, we specifically focus on addressing this gap. We propose a novel optimization method for finding spatially disentangled class-specific directions in the latent space of pretrained SIS models. We show that the latent directions found by our method can effectively control the local appearance of semantic classes, e.g., changing their internal structure, texture or color independently from each other. Visual inspection and quantitative evaluation of the discovered GAN controls on various datasets demonstrate that our method discovers a diverse set of unique and semantically meaningful latent directions for class-specific edits.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "52165905",
                        "name": "Edgar Sch\u00f6nfeld"
                    },
                    {
                        "authorId": "2193553751",
                        "name": "Julio Borges"
                    },
                    {
                        "authorId": "2034002419",
                        "name": "V. Sushko"
                    },
                    {
                        "authorId": "48920094",
                        "name": "B. Schiele"
                    },
                    {
                        "authorId": "145327993",
                        "name": "A. Khoreva"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANSpace [20] applies PCA over latent codes to obtain semantically meaningful edits, such as zoom, rotation, hair color, and gender."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "08f5c64207edc6cc810dfefdf8dc518707b4afe9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-01381",
                    "ArXiv": "2212.01381",
                    "DOI": "10.48550/arXiv.2212.01381",
                    "CorpusId": 254220722
                },
                "corpusId": 254220722,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/08f5c64207edc6cc810dfefdf8dc518707b4afe9",
                "title": "LatentSwap3D: Semantic Edits on 3D Image GANs",
                "abstract": "3D GANs have the ability to generate latent codes for entire 3D volumes rather than only 2D images. These models offer desirable features like high-quality geometry and multi-view consistency, but, unlike their 2D counterparts, complex semantic image editing tasks for 3D GANs have only been partially explored. To address this problem, we propose LatentSwap3D, a semantic edit approach based on latent space discovery that can be used with any off-the-shelf 3D or 2D GAN model and on any dataset. LatentSwap3D relies on identifying the latent code dimensions corresponding to specific attributes by feature ranking using a random forest classifier. It then performs the edit by swapping the selected dimensions of the image being edited with the ones from an automatically selected reference image. Compared to other latent space control-based edit methods, which were mainly designed for 2D GANs, our method on 3D GANs provides remarkably consistent semantic edits in a disentangled manner and outperforms others both qualitatively and quantitatively. We show results on seven 3D GANs (pi-GAN, GIRAFFE, StyleSDF, MVCGAN, EG3D, StyleNeRF, and VolumeGAN) and on five datasets (FFHQ, AFHQ, Cats, MetFaces, and CompCars).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1395808197",
                        "name": "Enis Simsar"
                    },
                    {
                        "authorId": "20406113",
                        "name": "A. Tonioni"
                    },
                    {
                        "authorId": "1491550942",
                        "name": "Evin Pinar Ornek"
                    },
                    {
                        "authorId": "50516802",
                        "name": "F. Tombari"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "360c0ac9b110af60a7afc123910478b5b6f72a6c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-00981",
                    "ArXiv": "2212.00981",
                    "DOI": "10.48550/arXiv.2212.00981",
                    "CorpusId": 254221091
                },
                "corpusId": 254221091,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/360c0ac9b110af60a7afc123910478b5b6f72a6c",
                "title": "QC-StyleGAN - Quality Controllable Image Generation and Manipulation",
                "abstract": "The introduction of high-quality image generation models, particularly the StyleGAN family, provides a powerful tool to synthesize and manipulate images. However, existing models are built upon high-quality (HQ) data as desired outputs, making them unfit for in-the-wild low-quality (LQ) images, which are common inputs for manipulation. In this work, we bridge this gap by proposing a novel GAN structure that allows for generating images with controllable quality. The network can synthesize various image degradation and restore the sharp image via a quality control code. Our proposed QC-StyleGAN can directly edit LQ images without altering their quality by applying GAN inversion and manipulation techniques. It also provides for free an image restoration solution that can handle various degradations, including noise, blur, compression artifacts, and their mixtures. Finally, we demonstrate numerous other applications such as image degradation synthesis, transfer, and interpolation. The code is available at https://github.com/VinAIResearch/QC-StyleGAN.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2087064988",
                        "name": "D. Nguyen"
                    },
                    {
                        "authorId": "2193387252",
                        "name": "Phong Tran The"
                    },
                    {
                        "authorId": "2142664005",
                        "name": "Tan M. Dinh"
                    },
                    {
                        "authorId": "2237780305",
                        "name": "Cuong Pham"
                    },
                    {
                        "authorId": "145830668",
                        "name": "A. Tran"
                    }
                ]
            }
        },
        {
            "contexts": [
                "H\u00e4rk\u00f6nen et al.25 summarized the previous work and realized unsupervised latent space.",
                "H\u00e4rk\u00f6nen et al.(25) summarized the previous work and realized unsupervised latent space."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "4ad2faa1c5ed9cd5a78c9f9a52670aea127bb0ce",
                "externalIds": {
                    "DBLP": "journals/jei/YangZXLL22",
                    "DOI": "10.1117/1.JEI.32.4.042103",
                    "CorpusId": 254355718
                },
                "corpusId": 254355718,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4ad2faa1c5ed9cd5a78c9f9a52670aea127bb0ce",
                "title": "Improving multimedia information security by enriching face antispoofing dataset with a facial forgery method",
                "abstract": "Abstract. The development of artificial intelligence, especially the application and development of deep learning, brings not only great convenience to people but also some challenges. Some face forgery techniques can confuse the false with the true in deep learning. With the widespread popularity of social applications and streaming media, people pay more and more attention to portrait privacy and security. To solve the great threat to personal privacy caused by deep face forgery technology, we propose an effective face forgery method to enrich the face antispoofing datasets. We first explore the representation of identity information in latent space. Based on this foundation, we use the identity feature replacement module to edit the latent codes in the latent space and finally generate the images with photo-realistic results with the help of the powerful generation ability of StyleGAN. The face forgery method can further expand the face antispoofing dataset, provide rich data for the learning-based forgery detection methods to improve their generalization and to ensure the security in preserving portrait privacy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3021550",
                        "name": "Jiachen Yang"
                    },
                    {
                        "authorId": "48270268",
                        "name": "Yong Zhu"
                    },
                    {
                        "authorId": "2112869738",
                        "name": "Shuai Xiao"
                    },
                    {
                        "authorId": "2124702432",
                        "name": "Guipeng Lan"
                    },
                    {
                        "authorId": "2154900466",
                        "name": "Yang Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Even unsupervised methods like [H\u00e4rk\u00f6nen et al. 2020] rely on intuition about semantics to demonstrate meaningful edit directions.",
                "0730-0301/2022/12-ART269 $15.00 https://doi.org/10.1145/3550454.3555472\nAdditional Key Words and Phrases: GANs, example-based media, digital brushes\nACM Reference Format: Maria Shugrina, Chin-Ying Li, and Sanja Fidler.",
                "\u2026many methods allow control and exploration of high-level attributes by leveraging the latent space [Abdal et al. 2020, 2021a,c; Alaluf et al. 2021; H\u00e4rk\u00f6nen et al. 2020; Kim et al. 2021; Richardson et al. 2021; Tov et al. 2021], or by modifying GAN architectures with built-in control using\u2026",
                "Generative Adversarial Networks (GANs) [Goodfellow et al. 2014] continue to show impressive results in image generation [Brock et al. 2018; Karras et al. 2021, 2019, 2020b], including scenarios with limited data [Karras et al. 2020a].",
                "Generative Adversarial Networks (GANs) [Goodfellow et al. 2014] have shown a remarkable range of applications by approximating a continuous distribution of natural images from unlabeled data.",
                "Many methods have been developed for semantic editing of images using GANs [H\u00e4rk\u00f6nen et al. 2020; Ling et al. 2021; Shen et al. 2020; Zhu et al. 2021] for domains such as human faces and cars, where semantics have a well-defined meaning.",
                "For example, many methods allow control and exploration of high-level attributes by leveraging the latent space [Abdal et al. 2020, 2021a,c; Alaluf et al. 2021; H\u00e4rk\u00f6nen et al. 2020; Kim et al. 2021; Richardson et al. 2021; Tov et al. 2021], or by modifying GAN architectures with built-in control using available labels [Choi et al.",
                "A wide range of approaches for manipulating images using GANs have been developed.",
                "We follow the same methodology and instead apply deep convolutional GANs to model a distribution of interactive drawing tools, showing a range of novel and expressive applications in control and discovery of digital brushes.",
                "Like other GANs, our model is challenging to evaluate."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "572b9c4c650793efbf0ba1ff43ed0e7020937027",
                "externalIds": {
                    "DBLP": "journals/tog/ShugrinaLF22",
                    "DOI": "10.1145/3550454.3555472",
                    "CorpusId": 254097123
                },
                "corpusId": 254097123,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/572b9c4c650793efbf0ba1ff43ed0e7020937027",
                "title": "Neural Brushstroke Engine",
                "abstract": "We propose Neural Brushstroke Engine, the first method to apply deep generative models to learn a distribution of interactive drawing tools. Our conditional GAN model learns the latent space of drawing styles from a small set (about 200) of unlabeled images in different media. Once trained, a single model can texturize stroke patches drawn by the artist, emulating a diverse collection of brush styles in the latent space. In order to enable interactive painting on a canvas of arbitrary size, we design a painting engine able to support real-time seamless patch-based generation, while allowing artists direct control of stroke shape, color and thickness. We show that the latent space learned by our model generalizes to unseen drawing and more experimental styles (e.g. beads) by embedding real styles into the latent space. We explore other applications of the continuous latent space, such as optimizing brushes to enable painting in the style of an existing artwork, automatic line drawing stylization, brush interpolation, and even natural language search over a continuous space of drawing tools. Our prototype received positive feedback from a small group of digital artists.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2854827",
                        "name": "Maria Shugrina"
                    },
                    {
                        "authorId": "2193071836",
                        "name": "Chin-Ying Li"
                    },
                    {
                        "authorId": "37895334",
                        "name": "S. Fidler"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026an age code explicitly or by traversing the latent space along a linear or non-linear path (a \u201csemantic dimension\u201d) as steered by a pre-trained age classifier [Abdal et al. 2021; Alaluf et al. 2021; Antipov et al. 2017; H\u00e4rk\u00f6nen et al. 2020; Or-El et al. 2020; Shen et al. 2020; Yang et al. 2021].",
                "We thus seek to achieve this goal using photorealisc synthetic faces, taking inspiration in recent work that leverages semantic manipulations within the latent space of powerful neural face models pre-trained on thousands of real faces [Abdal et al. 2021; Alaluf et al. 2021; H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020].",
                "\u2026this goal using photorealisc synthetic faces, taking inspiration in recent work that leverages semantic manipulations within the latent space of powerful neural face models pre-trained on thousands of real faces [Abdal et al. 2021; Alaluf et al. 2021; H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020].",
                "Leveraging the semantics learned by the neural model, this body of research work seeks to re-age a face, represented as a particular latent point, either by interpolating an age code explicitly or by traversing the latent space along a linear or non-linear path (a \u201csemantic dimension\u201d) as steered by a pre-trained age classifier [Abdal et al. 2021; Alaluf et al. 2021; Antipov et al. 2017; H\u00e4rk\u00f6nen et al. 2020; Or-El et al. 2020; Shen et al. 2020; Yang et al. 2021].",
                "The parameter space of the model is then traversed along highly elaborate \u201csemantic dimensions\u201d to provide realistic edits such as re-aging [Abdal et al. 2021; Alaluf et al. 2021; H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "453dff42e3f657014621b979304811bc210458a5",
                "externalIds": {
                    "DBLP": "journals/tog/ZossCSGGB22",
                    "DOI": "10.1145/3550454.3555520",
                    "CorpusId": 254097080
                },
                "corpusId": 254097080,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/453dff42e3f657014621b979304811bc210458a5",
                "title": "Production-Ready Face Re-Aging for Visual Effects",
                "abstract": "Photorealistic digital re-aging of faces in video is becoming increasingly common in entertainment and advertising. But the predominant 2D painting workflow often requires frame-by-frame manual work that can take days to accomplish, even by skilled artists. Although research on facial image re-aging has attempted to automate and solve this problem, current techniques are of little practical use as they typically suffer from facial identity loss, poor resolution, and unstable results across subsequent video frames. In this paper, we present the first practical, fully-automatic and production-ready method for re-aging faces in video images. Our first key insight is in addressing the problem of collecting longitudinal training data for learning to re-age faces over extended periods of time, a task that is nearly impossible to accomplish for a large number of real people. We show how such a longitudinal dataset can be constructed by leveraging the current state-of-the-art in facial re-aging that, although failing on real images, does provide photoreal re-aging results on synthetic faces. Our second key insight is then to leverage such synthetic data and formulate facial re-aging as a practical image-to-image translation task that can be performed by training a well-understood U-Net architecture, without the need for more complex network designs. We demonstrate how the simple U-Net, surprisingly, allows us to advance the state of the art for re-aging real faces on video, with unprecedented temporal stability and preservation of facial identity across variable expressions, viewpoints, and lighting conditions. Finally, our new face re-aging network (FRAN) incorporates simple and intuitive mechanisms that provides artists with localized control and creative freedom to direct and fine-tune the re-aging effect, a feature that is largely important in real production pipelines and often overlooked in related research work.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51152514",
                        "name": "G. Zoss"
                    },
                    {
                        "authorId": "2066173302",
                        "name": "P. Chandran"
                    },
                    {
                        "authorId": "1748270",
                        "name": "Eftychios Sifakis"
                    },
                    {
                        "authorId": "2246459104",
                        "name": "M. Gross"
                    },
                    {
                        "authorId": "2741258",
                        "name": "P. Gotardo"
                    },
                    {
                        "authorId": "143929823",
                        "name": "D. Bradley"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[10] proposes to identify important latent directions based on the Principal Components Analysis (PCA) of the latent space vectors.",
                "But different from [10], the PCA components are computed from the weight parameters rather that the sampled vectors."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "d067c282800846da385d155e167e286578ea2bf9",
                "externalIds": {
                    "DOI": "10.1109/DICTA56598.2022.10034574",
                    "CorpusId": 256744089
                },
                "corpusId": 256744089,
                "publicationVenue": {
                    "id": "375cb686-e96e-4b79-825c-1589c99aca1d",
                    "name": "International Conference on Digital Image Computing: Techniques and Applications",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Digit Image Comput Tech Appl",
                        "DICTA",
                        "Digital Image Computing: Techniques and Applications",
                        "Digit Image Comput Tech Appl"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=710"
                },
                "url": "https://www.semanticscholar.org/paper/d067c282800846da385d155e167e286578ea2bf9",
                "title": "FaceCook: Attribute-Controllable Face Generation Based on Linear Scaling Factors",
                "abstract": "With the excellent disentanglement properties of state-of-the-art generative models, image editing has been the dominant approach to controlling the attributes of synthesized face images. However, these edited results often suffer from artifacts or incorrect feature rendering, especially when there is a large discrepancy between the image to be edited and the desired feature set. Therefore, we propose a new approach to mapping the latent vectors of the generative model to the scaling factors through solving a set of multivariate linear equations. The coefficients of the equations are the eigenvectors of the weight parameters of the pre-trained model, which form the basis of a hyper coordinate system. The qualitative and quantitative results both show that the proposed method outperforms the baseline in terms of image diversity. In addition, the method is much more time-efficient since the synthesized images with desirable features can be obtained directly from the latent vectors, rather than the former process of editing randomly generated images with redundant steps.",
                "year": 2022,
                "authors": []
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "45908d868e0176b53cce33eb8b3d07d800055666",
                "externalIds": {
                    "ArXiv": "2211.14573",
                    "DBLP": "conf/cvpr/Aoshima023",
                    "DOI": "10.1109/CVPR52729.2023.00577",
                    "CorpusId": 254043592
                },
                "corpusId": 254043592,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/45908d868e0176b53cce33eb8b3d07d800055666",
                "title": "Deep Curvilinear Editing: Commutative and Nonlinear Image Manipulation for Pretrained Deep Generative Model",
                "abstract": "Semantic editing of images is the fundamental goal of computer vision. Although deep learning methods, such as generative adversarial networks (GANs), are capable of producing high-quality images, they often do not have an inherent way of editing generated images semantically. Recent studies have investigated a way of manipulating the latent variable to determine the images to be generated. However, methods that assume linear semantic arithmetic have certain limitations in terms of the quality of image editing, whereas methods that discover nonlinear semantic pathways provide non-commutative editing, which is inconsistent when applied in different orders. This study proposes a novel method called deep curvilinear editing (DeCurvEd) to determine semantic commuting vector fields on the latent space. We theoretically demonstrate that owing to commutativity, the editing of multiple attributes depends only on the quantities and not on the order. Furthermore, we experimentally demonstrate that compared to previous methods, the nonlinear and commutative nature of DeCurvEdfacilitates the disentanglement of image attributes and provides higher-quality editing.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2022212481",
                        "name": "Takehiro Aoshima"
                    },
                    {
                        "authorId": "144872058",
                        "name": "Takashi Matsubara"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "408248c1614254cd8a6967e7faaabf18754fead6",
                "externalIds": {
                    "ArXiv": "2211.13901",
                    "DBLP": "journals/corr/abs-2211-13901",
                    "DOI": "10.1109/CVPR52729.2023.00430",
                    "CorpusId": 254017882
                },
                "corpusId": 254017882,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/408248c1614254cd8a6967e7faaabf18754fead6",
                "title": "Learning Detailed Radiance Manifolds for High-Fidelity and 3D-Consistent Portrait Synthesis from Monocular Image",
                "abstract": "A key challenge for novel view synthesis of monocular portrait images is 3D consistency under continuous pose variations. Most existing methods rely on 2D generative models which often leads to obvious 3D inconsistency artifacts. We present a 3D-consistent novel view synthesis approach for monocular portrait images based on a recent proposed 3D-aware GAN, namely Generative Radiance Manifolds (GRAM) [13], which has shown strong 3D consistency at multiview image generation of virtual subjects via the radiance manifolds representation. However, simply learning an encoder to map a real image into the latent space of GRAM can only reconstruct coarse radiance manifolds without faithful fine details, while improving the reconstruction fidelity via instance-specific optimization is time-consuming. We introduce a novel detail manifolds reconstructor to learn 3D-consistent fine details on the radiance manifolds from monocular images, and combine them with the coarse radiance manifolds for high-fidelity reconstruction. The 3D priors derived from the coarse radiance manifolds are used to regulate the learned details to ensure reasonable synthesized results at novel views. Trained on in-the-wild 2D images, our method achieves high-fidelity and 3D-consistent portrait synthesis largely outperforming the prior art. Project page: https://yudeng.github.io/GRAMInverter/",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152710186",
                        "name": "Yu Deng"
                    },
                    {
                        "authorId": "2450889",
                        "name": "Baoyuan Wang"
                    },
                    {
                        "authorId": "93596028",
                        "name": "H. Shum"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e2e34dc10482795a94e401c343a78cb333960996",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-13874",
                    "ArXiv": "2211.13874",
                    "DOI": "10.1109/CVPR52729.2023.00043",
                    "CorpusId": 254018271
                },
                "corpusId": 254018271,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e2e34dc10482795a94e401c343a78cb333960996",
                "title": "FFHQ-UV: Normalized Facial UV-Texture Dataset for 3D Face Reconstruction",
                "abstract": "We present a large-scale facial UV-texture dataset that contains over 50,000 high-quality texture UV-maps with even illuminations, neutral expressions, and cleaned facial regions, which are desired characteristics for rendering realistic 3D face models under different lighting conditions. The dataset is derived from a large-scale face image dataset namely FFHQ, with the help of our fully automatic and robust UV-texture production pipeline. Our pipeline utilizes the recent advances in StyleGAN-based facial image editing approaches to generate multi-view normalized face images from single-image inputs. An elaborated UV-texture extraction, correction, and completion procedure is then applied to produce high-quality UV-maps from the normalized face images. Compared with existing UV-texture datasets, our dataset has more diverse and higher-quality texture maps. We further train a GAN-based texture decoder as the nonlinear texture basis for parametric fitting based 3D face reconstruction. Experiments show that our method improves the reconstruction accuracy over state-of-the-art approaches, and more importantly, produces high-quality texture maps that are ready for realistic renderings. The dataset, code, and pre-trained texture decoder are publicly available at https://github.com/csbhr/FFHQ-UV.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2067571063",
                        "name": "Haoran Bai"
                    },
                    {
                        "authorId": "2151879170",
                        "name": "Di Kang"
                    },
                    {
                        "authorId": "2143781255",
                        "name": "Haoxian Zhang"
                    },
                    {
                        "authorId": "9416881",
                        "name": "Jin-shan Pan"
                    },
                    {
                        "authorId": "2780029",
                        "name": "Linchao Bao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For instance, PCA is applied in the latent space to create interpretable controls for synthesizing images [8, 26].",
                "Others [7,8,26,35,52] try to find semantic directions in an unsupervised manner."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4c391cb0d150773454b2f134064e6ddb2499f641",
                "externalIds": {
                    "ArXiv": "2211.12347",
                    "DBLP": "journals/corr/abs-2211-12347",
                    "DOI": "10.48550/arXiv.2211.12347",
                    "CorpusId": 253760983
                },
                "corpusId": 253760983,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4c391cb0d150773454b2f134064e6ddb2499f641",
                "title": "The Euclidean Space is Evil: Hyperbolic Attribute Editing for Few-shot Image Generation",
                "abstract": "Few-shot image generation is a challenging task since it aims to generate diverse new images for an unseen category with only a few images. Existing methods suffer from the trade-off between the quality and diversity of generated images. To tackle this problem, we propose Hyperbolic Attribute Editing~(HAE), a simple yet effective method. Unlike other methods that work in Euclidean space, HAE captures the hierarchy among images using data from seen categories in hyperbolic space. Given a well-trained HAE, images of unseen categories can be generated by moving the latent code of a given image toward any meaningful directions in the Poincar\\'e disk with a fixing radius. Most importantly, the hyperbolic space allows us to control the semantic diversity of the generated images by setting different radii in the disk. Extensive experiments and visualizations demonstrate that HAE is capable of not only generating images with promising quality and diversity using limited data but achieving a highly controllable and interpretable editing process.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146625611",
                        "name": "Ling Li"
                    },
                    {
                        "authorId": "46867018",
                        "name": "Yi Zhang"
                    },
                    {
                        "authorId": "2161455710",
                        "name": "Shuhui Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Broadly speaking, we can divide image editing with GANs into two subgroups: (i) Unconditional GAN-based methods [16, 54], which find editing vectors using unsupervised learning methods like PCA [16] or activation maps [54].",
                "With the advent of models based on StyleGAN [24], there has been a plethora of work focusing on controllable manipulation of the latent code for the task of image editing [9, 16, 48].",
                "On the other hand, methods like [9, 16, 47, 53, 54] use unsupervised approaches for finding editing directions."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a2b110958a89bedd3ddf40a57b1079f08275525b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-12209",
                    "ArXiv": "2211.12209",
                    "DOI": "10.48550/arXiv.2211.12209",
                    "CorpusId": 253761167
                },
                "corpusId": 253761167,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/a2b110958a89bedd3ddf40a57b1079f08275525b",
                "title": "S2-Flow: Joint Semantic and Style Editing of Facial Images",
                "abstract": "The high-quality images yielded by generative adversarial networks (GANs) have motivated investigations into their application for image editing. However, GANs are often limited in the control they provide for performing specific edits. One of the principal challenges is the entangled latent space of GANs, which is not directly suitable for performing independent and detailed edits. Recent editing methods allow for either controlled style edits or controlled semantic edits. In addition, methods that use semantic masks to edit images have difficulty preserving the identity and are unable to perform controlled style edits. We propose a method to disentangle a GAN$\\text{'}$s latent space into semantic and style spaces, enabling controlled semantic and style edits for face images independently within the same framework. To achieve this, we design an encoder-decoder based network architecture ($S^2$-Flow), which incorporates two proposed inductive biases. We show the suitability of $S^2$-Flow quantitatively and qualitatively by performing various semantic and style edits.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "116811112",
                        "name": "Krishnakant Singh"
                    },
                    {
                        "authorId": "1412432168",
                        "name": "Simone Schaub-Meyer"
                    },
                    {
                        "authorId": "145920814",
                        "name": "S. Roth"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Two popular editing methods are selected for semantic editing: GANSpace [7] and InterfaceGAN [6] to manipulate inverted images.",
                "[7] utilize typical unsupervised learning strategies, i."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "48d0a943bbd0c8ea06cce15c50a21cd97c9fcce4",
                "externalIds": {
                    "ArXiv": "2211.12123",
                    "DBLP": "journals/corr/abs-2211-12123",
                    "DOI": "10.48550/arXiv.2211.12123",
                    "CorpusId": 253761340
                },
                "corpusId": 253761340,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/48d0a943bbd0c8ea06cce15c50a21cd97c9fcce4",
                "title": "Unsupervised Domain Adaptation GAN Inversion for Image Editing",
                "abstract": "Existing GAN inversion methods work brilliantly for high-quality image reconstruction and editing while struggling with finding the corresponding high-quality images for low-quality inputs. Therefore, recent works are directed toward leveraging the supervision of paired high-quality and low-quality images for inversion. However, these methods are infeasible in real-world scenarios and further hinder performance improvement. In this paper, we resolve this problem by introducing Unsupervised Domain Adaptation (UDA) into the Inversion process, namely UDA-Inversion, for both high-quality and low-quality image inversion and editing. Particularly, UDA-Inversion first regards the high-quality and low-quality images as the source domain and unlabeled target domain, respectively. Then, a discrepancy function is presented to measure the difference between two domains, after which we minimize the source error and the discrepancy between the distributions of two domains in the latent space to obtain accurate latent codes for low-quality images. Without direct supervision, constructive representations of high-quality images can be spontaneously learned and transformed into low-quality images based on unsupervised domain adaptation. Experimental results indicate that UDA-inversion is the first that achieves a comparable level of performance with supervised methods in low-quality images across multiple domain datasets. We hope this work provides a unique inspiration for latent embedding distributions in image process tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "41125982",
                        "name": "Siyu Xing"
                    },
                    {
                        "authorId": "2057185016",
                        "name": "Chen Gong"
                    },
                    {
                        "authorId": "2158017251",
                        "name": "Hewei Guo"
                    },
                    {
                        "authorId": "2155169677",
                        "name": "Xiaoyi Zhang"
                    },
                    {
                        "authorId": "1761961",
                        "name": "Xinwen Hou"
                    },
                    {
                        "authorId": "2167664198",
                        "name": "Yu Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "5071e230e8868e2a0388dd93e39f32aad6ea02e0",
                "externalIds": {
                    "DBLP": "conf/cvpr/LiuSC23",
                    "ArXiv": "2211.11448",
                    "DOI": "10.1109/CVPR52729.2023.00971",
                    "CorpusId": 253734815
                },
                "corpusId": 253734815,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5071e230e8868e2a0388dd93e39f32aad6ea02e0",
                "title": "Delving StyleGAN Inversion for Image Editing: A Foundation Latent Space Viewpoint",
                "abstract": "GAN inversion and editing via StyleGAN maps an input image into the embedding spaces (W, W+, and F) to simultaneously maintain image fidelity and meaningful manipulation. From latent space W to extended latent space W+ to feature space F in StyleGAN, the editability of GAN inversion decreases while its reconstruction quality increases. Recent GAN inversion methods typically explore W+ and F rather than W to improve reconstruction fidelity while maintaining editability. As W+ and F are derived from W that is essentially the foundation latent space of StyleGAN, these GAN inversion methods focusing on W+ and F spaces could be improved by stepping back to W. In this work, we propose to first obtain the proper latent code in foundation latent space W. We introduce contrastive learning to align W and the image space for proper latent code discovery. Then, we leverage a cross-attention encoder to transform the obtained latent code in W into W+ and F, accordingly. Our experiments show that our exploration of the foundation latent space W improves the representation ability of latent codes in W+ and features in F, which yields state-of-the-art reconstruction fidelity and editability results on the standard benchmarks. Project page: https://kumapowerliu.github.io/CLCAE.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115669461",
                        "name": "Hongyu Liu"
                    },
                    {
                        "authorId": "2255687",
                        "name": "Yibing Song"
                    },
                    {
                        "authorId": "2157737759",
                        "name": "Qifeng Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "1173e12bbdbf20ed085f390d74fc1fb86eec0cb6",
                "externalIds": {
                    "ArXiv": "2211.11825",
                    "CorpusId": 253761178
                },
                "corpusId": 253761178,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1173e12bbdbf20ed085f390d74fc1fb86eec0cb6",
                "title": "Multi-Directional Subspace Editing in Style-Space",
                "abstract": "This paper describes a new technique for finding disentangled semantic directions in the latent space of StyleGAN. Our method identifies meaningful orthogonal subspaces that allow editing of one human face attribute, while minimizing undesired changes in other attributes. Our model is capable of editing a single attribute in multiple directions, resulting in a range of possible generated images. We compare our scheme with three state-of-the-art models and show that our method outperforms them in terms of face editing and disentanglement capabilities. Additionally, we suggest quantitative measures for evaluating attribute separation and disentanglement, and exhibit the superiority of our model with respect to those measures.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2191691627",
                        "name": "Chen Naveh"
                    },
                    {
                        "authorId": "1398184731",
                        "name": "Y. Hel-Or"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some works try to discover interpretable directions in the GAN latent space in an unsupervised manner [17, 39, 46].",
                "Several works [17, 38] empirically showed thatW supports linear latent code manipulation as they were able to find semantic directions inW corresponding to meaningful disentangled attributes such as color change, zoom, pose, gender, etc."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "2045c34106c4ce2fada725ee023203b5d6900b42",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-10812",
                    "ArXiv": "2211.10812",
                    "DOI": "10.48550/arXiv.2211.10812",
                    "CorpusId": 253734260
                },
                "corpusId": 253734260,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2045c34106c4ce2fada725ee023203b5d6900b42",
                "title": "Face Swapping as A Simple Arithmetic Operation",
                "abstract": "We propose a novel high-fidelity face swapping method called\"Arithmetic Face Swapping\"(AFS) that explicitly disentangles the intermediate latent space W+ of a pretrained StyleGAN into the\"identity\"and\"style\"subspaces so that a latent code in W+ is the sum of an\"identity\"code and a\"style\"code in the corresponding subspaces. Via our disentanglement, face swapping (FS) can be regarded as a simple arithmetic operation in W+, i.e., the summation of a source\"identity\"code and a target\"style\"code. This makes AFS more intuitive and elegant than other FS methods. In addition, our method can generalize over the standard face swapping to support other interesting operations, e.g., combining the identity of one source with styles of multiple targets and vice versa. We implement our identity-style disentanglement by learning a neural network that maps a latent code to a\"style\"code. We provide a condition for this network which theoretically guarantees identity preservation of the source face even after a sequence of face swapping operations. Extensive experiments demonstrate the advantage of our method over state-of-the-art FS methods in producing high-quality swapped faces. Our source code was made public at https://github.com/truongvu2000nd/AFS",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2184053082",
                        "name": "T. Vu"
                    },
                    {
                        "authorId": "2184054470",
                        "name": "K. Do"
                    },
                    {
                        "authorId": "2055541132",
                        "name": "Khang Nguyen"
                    },
                    {
                        "authorId": "38685176",
                        "name": "Khoat Than"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANSpace [13] proposes a technique to analyze GANs and create interpretable controls in terms of latent directions based on a Principal Component Analysis (PCA).",
                "(E.g- Curved backrest and Stuffed seat in GANSpace adds changes legs, Connected armrest in GANSpace adds changes to seat, Removing armrest in Closed-form adds changes to backrest, etc.) Although GANSpace has been more successful than Closed-form in identifying directions for common semantics like swivel legs, specific semantics like cantilever legs were not present/clearly distinguishable among directions yielded by either GANSpace or Closed-form.",
                "Fig.4 shows results of part level semantic manipulation using 3DLatNav, GANSpace [13] and Closed-form [42].",
                "As shown in Table 2, 3DLatNav outperforms both GANSpace and Closed-form in restricting the semantic changes to a required part, confirming its superior performance in disentangling part-level semantic controls.",
                "3DLatNav consistently outperforms the previous other latent disentanglement and navigation approaches [13,42] in most semantic manipulations.",
                "4 shows results of part level semantic manipulation using 3DLatNav, GANSpace [13] and Closed-form [42].",
                "Comparison of part-level object manipulation results of 3DLatNav with unsupervised latent disentanglement methods; Closed-form [42] and GANSpace [13]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "19e62f26b9f28b06de78090ce8f693a4eb4a969e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-09770",
                    "ArXiv": "2211.09770",
                    "DOI": "10.48550/arXiv.2211.09770",
                    "CorpusId": 253581810
                },
                "corpusId": 253581810,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/19e62f26b9f28b06de78090ce8f693a4eb4a969e",
                "title": "3DLatNav: Navigating Generative Latent Spaces for Semantic-Aware 3D Object Manipulation",
                "abstract": "3D generative models have been recently successful in generating realistic 3D objects in the form of point clouds. However, most models do not offer controllability to manipulate the shape semantics of component object parts without extensive semantic attribute labels or other reference point clouds. Moreover, beyond the ability to perform simple latent vector arithmetic or interpolations, there is a lack of understanding of how part-level semantics of 3D shapes are encoded in their corresponding generative latent spaces. In this paper, we propose 3DLatNav; a novel approach to navigating pretrained generative latent spaces to enable controlled part-level semantic manipulation of 3D objects. First, we propose a part-level weakly-supervised shape semantics identification mechanism using latent representations of 3D shapes. Then, we transfer that knowledge to a pretrained 3D object generative latent space to unravel disentangled embeddings to represent different shape semantics of component parts of an object in the form of linear subspaces, despite the unavailability of part-level labels during the training. Finally, we utilize those identified subspaces to show that controllable 3D object part manipulation can be achieved by applying the proposed framework to any pretrained 3D generative model. With two novel quantitative metrics to evaluate the consistency and localization accuracy of part-level manipulations, we show that 3DLatNav outperforms existing unsupervised latent disentanglement methods in identifying latent directions that encode part-level shape semantics of 3D objects. With multiple ablation studies and testing on state-of-the-art generative models, we show that 3DLatNav can implement controlled part-level semantic manipulations on an input point cloud while preserving other features and the realistic nature of the object.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2088382219",
                        "name": "A. Dharmasiri"
                    },
                    {
                        "authorId": "2156789747",
                        "name": "Dinithi Dissanayake"
                    },
                    {
                        "authorId": "2084548724",
                        "name": "Mohamed Afham"
                    },
                    {
                        "authorId": "1825640352",
                        "name": "Isuru Dissanayake"
                    },
                    {
                        "authorId": "144952844",
                        "name": "R. Rodrigo"
                    },
                    {
                        "authorId": "3153007",
                        "name": "Kanchana Thilakarathna"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Once the image has been inverted, edits can be made in the latent space to re-create the adjusted image [1] [9] [19] [25]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "a4be3489d7dba5d286cf00b698f267b65ebd7aaf",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-07825",
                    "ArXiv": "2211.07825",
                    "DOI": "10.48550/arXiv.2211.07825",
                    "CorpusId": 253523448
                },
                "corpusId": 253523448,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a4be3489d7dba5d286cf00b698f267b65ebd7aaf",
                "title": "Direct Inversion: Optimization-Free Text-Driven Real Image Editing with Diffusion Models",
                "abstract": "With the rise of large, publicly-available text-to-image diffusion models, text-guided real image editing has garnered much research attention recently. Existing methods tend to either rely on some form of per-instance or per-task fine-tuning and optimization, require multiple novel views, or they inherently entangle preservation of real image identity, semantic coherence, and faithfulness to text guidance. In this paper, we propose an optimization-free and zero fine-tuning framework that applies complex and non-rigid edits to a single real image via a text prompt, avoiding all the pitfalls described above. Using widely-available generic pre-trained text-to-image diffusion models, we demonstrate the ability to modulate pose, scene, background, style, color, and even racial identity in an extremely flexible manner through a single target text detailing the desired edit. Furthermore, our method, which we name $\\textit{Direct Inversion}$, proposes multiple intuitively configurable hyperparameters to allow for a wide range of types and extents of real image edits. We prove our method's efficacy in producing high-quality, diverse, semantically coherent, and faithful real image edits through applying it on a variety of inputs for a multitude of tasks. We also formalize our method in well-established theory, detail future experiments for further improvement, and compare against state-of-the-art attempts.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2190819274",
                        "name": "Adham Elarabawy"
                    },
                    {
                        "authorId": "2074872694",
                        "name": "Harish Kamath"
                    },
                    {
                        "authorId": "1721047453",
                        "name": "Samuel Denton"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recall that prior works such as [5, 21] explored similar properties in GAN latent spaces, but their domain of study was restricted to well-aligned data such as faces or churches."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "78550c60dcdcdd8ca30f5838bcdbace1e8c9ba15",
                "externalIds": {
                    "ArXiv": "2211.08332",
                    "DBLP": "journals/corr/abs-2211-08332",
                    "DOI": "10.48550/arXiv.2211.08332",
                    "CorpusId": 253523371
                },
                "corpusId": 253523371,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/78550c60dcdcdd8ca30f5838bcdbace1e8c9ba15",
                "title": "Versatile Diffusion: Text, Images and Variations All in One Diffusion Model",
                "abstract": "Recent advances in diffusion models have set an impressive milestone in many generation tasks, and trending works such as DALL-E2, Imagen, and Stable Diffusion have attracted great interest. Despite the rapid landscape changes, recent new approaches focus on extensions and performance rather than capacity, thus requiring separate models for separate tasks. In this work, we expand the existing single-flow diffusion pipeline into a multi-task multimodal network, dubbed Versatile Diffusion (VD), that handles multiple flows of text-to-image, image-to-text, and variations in one unified model. The pipeline design of VD instantiates a unified multi-flow diffusion framework, consisting of sharable and swappable layer modules that enable the crossmodal generality beyond images and text. Through extensive experiments, we demonstrate that VD successfully achieves the following: a) VD outperforms the baseline approaches and handles all its base tasks with competitive quality; b) VD enables novel extensions such as disentanglement of style and semantics, dual- and multi-context blending, etc.; c) The success of our multi-flow multimodal framework over images and text may inspire further diffusion-based universal AI research. Our code and models are open-sourced at https://github.com/SHI-Labs/Versatile-Diffusion.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "101246501",
                        "name": "Xingqian Xu"
                    },
                    {
                        "authorId": "2108404505",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "2176400796",
                        "name": "Eric Zhang"
                    },
                    {
                        "authorId": "37833805",
                        "name": "Kai Wang"
                    },
                    {
                        "authorId": "48667025",
                        "name": "Humphrey Shi"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "f5e76bbd00e910c9d5698efd01bd63ea6f9a85c7",
                "externalIds": {
                    "DBLP": "conf/cvpr/HaasGB23",
                    "ArXiv": "2211.07195",
                    "DOI": "10.1109/CVPRW59228.2023.00075",
                    "CorpusId": 253511090
                },
                "corpusId": 253511090,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f5e76bbd00e910c9d5698efd01bd63ea6f9a85c7",
                "title": "Controllable GAN Synthesis Using Non-Rigid Structure-from-Motion",
                "abstract": "In this paper, we present an approach for combining non-rigid structure-from-motion (NRSfM) with deep generative models, and propose an efficient framework for discovering trajectories in the latent space of 2D GANs corresponding to changes in 3D geometry. Our approach uses recent advances in NRSfM and enables editing of the camera and non-rigid shape information associated with the latent codes without needing to retrain the generator. This formulation provides an implicit dense 3D reconstruction as it enables the image synthesis of novel shapes from arbitrary view angles and non-rigid structure. The method is built upon a sparse backbone, where a neural regressor is first trained to regress parameters describing the cameras and sparse non-rigid structure directly from the latent codes. The latent trajectories associated with changes in the camera and structure parameters are then identified by estimating the local inverse of the regressor in the neighborhood of a given latent code. The experiments show that our approach provides a versatile, systematic way to model, analyze, and edit the geometry and non-rigid structures of faces.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2140280426",
                        "name": "Ren\u00e9 Haas"
                    },
                    {
                        "authorId": "23620370",
                        "name": "Stella Grasshof"
                    },
                    {
                        "authorId": "120414984",
                        "name": "Sami S. Brandt"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some methods [2,18,39,41] also leverage disentangled properties in the latent space to enable 3D controls.",
                ", W space) [18, 20, 38, 41] or extended latent space (i."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "17e059974ed3200c4ad77adaf2beeb224fb8497a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-06583",
                    "ArXiv": "2211.06583",
                    "DOI": "10.48550/arXiv.2211.06583",
                    "CorpusId": 253510676
                },
                "corpusId": 253510676,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/17e059974ed3200c4ad77adaf2beeb224fb8497a",
                "title": "3D-Aware Encoding for Style-based Neural Radiance Fields",
                "abstract": "We tackle the task of NeRF inversion for style-based neural radiance fields, (e.g., StyleNeRF). In the task, we aim to learn an inversion function to project an input image to the latent space of a NeRF generator and then synthesize novel views of the original image based on the latent code. Compared with GAN inversion for 2D generative models, NeRF inversion not only needs to 1) preserve the identity of the input image, but also 2) ensure 3D consistency in generated novel views. This requires the latent code obtained from the single-view image to be invariant across multiple views. To address this new challenge, we propose a two-stage encoder for style-based NeRF inversion. In the first stage, we introduce a base encoder that converts the input image to a latent code. To ensure the latent code is view-invariant and is able to synthesize 3D consistent novel view images, we utilize identity contrastive learning to train the base encoder. Second, to better preserve the identity of the input image, we introduce a refining encoder to refine the latent code and add finer details to the output image. Importantly note that the novelty of this model lies in the design of its first-stage encoder which produces the closest latent code lying on the latent manifold and thus the refinement in the second stage would be close to the NeRF manifold. Through extensive experiments, we demonstrate that our proposed two-stage encoder qualitatively and quantitatively exhibits superiority over the existing encoders for inversion in both image reconstruction and novel-view rendering.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3312576",
                        "name": "Yu-Jhe Li"
                    },
                    {
                        "authorId": "2118716788",
                        "name": "Tao Xu"
                    },
                    {
                        "authorId": "3130257",
                        "name": "Bichen Wu"
                    },
                    {
                        "authorId": "2065869523",
                        "name": "N. Zheng"
                    },
                    {
                        "authorId": "4527324",
                        "name": "Xiaoliang Dai"
                    },
                    {
                        "authorId": "49107901",
                        "name": "Albert Pumarola"
                    },
                    {
                        "authorId": "2918780",
                        "name": "Peizhao Zhang"
                    },
                    {
                        "authorId": "48682997",
                        "name": "P\u00e9ter Vajda"
                    },
                    {
                        "authorId": "144040368",
                        "name": "Kris Kitani"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Control in Image Synthesis Methods In GANSpace: Discovering Interpretable GAN Controls [2], H\u00e4rk\u00f6nen\u00a0et al. propose a method for analyzing the latent space in GANbased methods.",
                "Control in Image Synthesis Methods In GANSpace: Discovering Interpretable GAN Controls [2], H\u00e4rk\u00f6nen et al."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "2de7067e9b02e15ec00537fa5530d9bf466f250f",
                "externalIds": {
                    "DBLP": "journals/sncs/BrehmBL23",
                    "DOI": "10.1007/s42979-022-01462-w",
                    "CorpusId": 253450389
                },
                "corpusId": 253450389,
                "publicationVenue": {
                    "id": "7a7dc89b-e1a6-44df-a496-46c330a87840",
                    "name": "SN Computer Science",
                    "type": "journal",
                    "alternate_names": [
                        "SN Comput Sci"
                    ],
                    "issn": "2661-8907",
                    "alternate_issns": [
                        "2662-995X"
                    ],
                    "url": "https://link.springer.com/journal/42979"
                },
                "url": "https://www.semanticscholar.org/paper/2de7067e9b02e15ec00537fa5530d9bf466f250f",
                "title": "Controlling 3D Objects in 2D Image Synthesis",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40090845",
                        "name": "Stephan Brehm"
                    },
                    {
                        "authorId": "2190446251",
                        "name": "Florian Barthel"
                    },
                    {
                        "authorId": "144739319",
                        "name": "R. Lienhart"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Meanwhile, the success of the StyleGAN series [24, 25, 26, 27] on generation tasks has established a robust baseline for many downstream tasks such as style transfer [1, 16, 39], GAN-inverse [37], latent space editing [17, 45, 49], and inpainting [63]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "06d58ba23575d8906dd066c046c3c0f49a62326b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-03700",
                    "ArXiv": "2211.03700",
                    "DOI": "10.1109/WACV56688.2023.00457",
                    "CorpusId": 253384254
                },
                "corpusId": 253384254,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/06d58ba23575d8906dd066c046c3c0f49a62326b",
                "title": "Image Completion with Heterogeneously Filtered Spectral Hints",
                "abstract": "Image completion with large-scale free-form missing regions is one of the most challenging tasks for the computer vision community. While researchers pursue better solutions, drawbacks such as pattern unawareness, blurry textures, and structure distortion remain noticeable, and thus leave space for improvement. To overcome these challenges, we propose a new StyleGAN-based image completion network, Spectral Hint GAN (SH-GAN), inside which a carefully designed spectral processing module, Spectral Hint Unit, is introduced. We also propose two novel 2D spectral processing strategies, Heterogeneous Filtering and Gaussian Split that well-fit modern deep learning models and may further be extended to other tasks. From our inclusive experiments, we demonstrate that our model can reach FID scores of 3.4134 and 7.0277 on the benchmark datasets FFHQ and Places2, and therefore outperforms prior works and reaches a new state-of-the-art. We also prove the effectiveness of our design via ablation studies, from which one may notice that the aforementioned challenges, i.e. pattern unawareness, blurry textures, and structure distortion, can be noticeably resolved. Our code will be open-sourced at: https://github.com/SHI-Labs/SH-GAN.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "101246501",
                        "name": "Xingqian Xu"
                    },
                    {
                        "authorId": "2042485075",
                        "name": "Shant Navasardyan"
                    },
                    {
                        "authorId": "2190105341",
                        "name": "Vahram Tadevosyan"
                    },
                    {
                        "authorId": "2154037819",
                        "name": "Andranik Sargsyan"
                    },
                    {
                        "authorId": "145353089",
                        "name": "Yadong Mu"
                    },
                    {
                        "authorId": "48667025",
                        "name": "Humphrey Shi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANs synthesize not only realistic images but also steerable ones towards specific content or styles [22, 54, 50, 33, 59, 55, 32].",
                "This understanding is also evidenced by image editing works [22, 54, 50, 55, 32] which show that interfering with low-resolution feature maps leads to a structural and high-level change of an image, and altering high-resolution feature maps only induces subtle appearance changes."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e2653bf58e00a24eceae1c47dfb311f7d91bc124",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-03000",
                    "ArXiv": "2211.03000",
                    "DOI": "10.48550/arXiv.2211.03000",
                    "CorpusId": 253384053
                },
                "corpusId": 253384053,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e2653bf58e00a24eceae1c47dfb311f7d91bc124",
                "title": "Distilling Representations from GAN Generator via Squeeze and Span",
                "abstract": "In recent years, generative adversarial networks (GANs) have been an actively studied topic and shown to successfully produce high-quality realistic images in various domains. The controllable synthesis ability of GAN generators suggests that they maintain informative, disentangled, and explainable image representations, but leveraging and transferring their representations to downstream tasks is largely unexplored. In this paper, we propose to distill knowledge from GAN generators by squeezing and spanning their representations. We squeeze the generator features into representations that are invariant to semantic-preserving transformations through a network before they are distilled into the student network. We span the distilled representation of the synthetic domain to the real domain by also using real training data to remedy the mode collapse of GANs and boost the student network performance in a real domain. Experiments justify the efficacy of our method and reveal its great significance in self-supervised representation learning. Code is available at https://github.com/yangyu12/squeeze-and-span.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118809263",
                        "name": "Yu Yang"
                    },
                    {
                        "authorId": "15652287",
                        "name": "Xiaotian Cheng"
                    },
                    {
                        "authorId": "2118484320",
                        "name": "Chang Liu"
                    },
                    {
                        "authorId": "2518212",
                        "name": "Hakan Bilen"
                    },
                    {
                        "authorId": "2117709282",
                        "name": "Xiang Ji"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent methods (Goetschalckx et al., 2019; Shen et al., 2020; Karras et al., 2019; Plumerault et al., 2020; Jahanian et al., 2020; Voynov & Babenko, 2020; Spingarn-Eliezer et al., 2021; H\u00e4rk\u00f6nen et al., 2020) show that certain factors such as object shape and position in the images synthesized by generative adversarial networks (GANs) (Goodfellow et al.",
                "\u20262019; Shen et al., 2020; Karras et al., 2019; Plumerault et al., 2020; Jahanian et al., 2020; Voynov & Babenko, 2020; Spingarn-Eliezer et al., 2021; Ha\u0308rko\u0308nen et al., 2020) show that certain factors such as object shape and position in the images synthesized by generative adversarial networks\u2026"
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b18072fa828f4d19319ed76d97cb2b356a29f67c",
                "externalIds": {
                    "DBLP": "conf/iclr/YangCBJ22",
                    "ArXiv": "2211.03003",
                    "DOI": "10.48550/arXiv.2211.03003",
                    "CorpusId": 251647801
                },
                "corpusId": 251647801,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b18072fa828f4d19319ed76d97cb2b356a29f67c",
                "title": "Learning to Annotate Part Segmentation with Gradient Matching",
                "abstract": "The success of state-of-the-art deep neural networks heavily relies on the presence of large-scale labelled datasets, which are extremely expensive and time-consuming to annotate. This paper focuses on tackling semi-supervised part segmentation tasks by generating high-quality images with a pre-trained GAN and labelling the generated images with an automatic annotator. In particular, we formulate the annotator learning as a learning-to-learn problem. Given a pre-trained GAN, the annotator learns to label object parts in a set of randomly generated images such that a part segmentation model trained on these synthetic images with their predicted labels obtains low segmentation error on a small validation set of manually labelled images. We further reduce this nested-loop optimization problem to a simple gradient matching problem and efficiently solve it with an iterative algorithm. We show that our method can learn annotators from a broad range of labelled images including real images, generated images, and even analytically rendered images. Our method is evaluated with semi-supervised part segmentation tasks and significantly outperforms other semi-supervised competitors when the amount of labelled examples is extremely limited.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118809263",
                        "name": "Yu Yang"
                    },
                    {
                        "authorId": "15652287",
                        "name": "Xiaotian Cheng"
                    },
                    {
                        "authorId": "2518212",
                        "name": "Hakan Bilen"
                    },
                    {
                        "authorId": "7807689",
                        "name": "Xiangyang Ji"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another line of works (Voynov and Babenko 2020; Ha\u0308rko\u0308nen et al. 2020; Shen and Zhou 2021; SpingarnEliezer, Banner, and Michaeli 2021; Ramesh, Choi, and LeCun 2018; Zhu et al. 2021; Esser, Rombach, and Ommer 2020; Choi et al. 2022) search latent directions without external human supervision.",
                "Another line of works (Voynov and Babenko 2020; H\u00e4rk\u00f6nen et al. 2020; Shen and Zhou 2021; SpingarnEliezer, Banner, and Michaeli 2021; Ramesh, Choi, and LeCun 2018; Zhu et al. 2021; Esser, Rombach, and Ommer 2020; Choi et al. 2022) search latent directions without external human supervision."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e079490ecf14f0afd422d1aeaf76e67ec8d0ac90",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-02798",
                    "ArXiv": "2211.02798",
                    "DOI": "10.48550/arXiv.2211.02798",
                    "CorpusId": 253383654
                },
                "corpusId": 253383654,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e079490ecf14f0afd422d1aeaf76e67ec8d0ac90",
                "title": "Local Manifold Augmentation for Multiview Semantic Consistency",
                "abstract": "Multiview self-supervised representation learning roots in exploring semantic consistency across data of complex intra-class variation. Such variation is not directly accessible and therefore simulated by data augmentations. However, commonly adopted augmentations are handcrafted and limited to simple geometrical and color changes, which are unable to cover the abundant intra-class variation. In this paper, we propose to extract the underlying data variation from datasets and construct a novel augmentation operator, named local manifold augmentation (LMA). LMA is achieved by training an instance-conditioned generator to fit the distribution on the local manifold of data and sampling multiview data using it. LMA shows the ability to create an infinite number of data views, preserve semantics, and simulate complicated variations in object pose, viewpoint, lighting condition, background etc. Experiments show that with LMA integrated, self-supervised learning methods such as MoCov2 and SimSiam gain consistent improvement on prevalent benchmarks including CIFAR10, CIFAR100, STL10, ImageNet100, and ImageNet. Furthermore, LMA leads to representations that obtain more significant invariance to the viewpoint, object pose, and illumination changes and stronger robustness to various real distribution shifts reflected by ImageNet-V2, ImageNet-R, ImageNet Sketch etc.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118809263",
                        "name": "Yu Yang"
                    },
                    {
                        "authorId": "153776540",
                        "name": "Wing Yin Cheung"
                    },
                    {
                        "authorId": "2118484407",
                        "name": "Chang Liu"
                    },
                    {
                        "authorId": "2117709282",
                        "name": "Xiang Ji"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This method is getting popular recently and both unsupervised methods [43, 17, 36] and supervised methods [19, 35, 52] are heavily explored.",
                "Recent works [43, 17, 36] show that postprocessing can be applied to find disentangled latent directions in a pretrained GAN space."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "2e138f40afb0e1ce131dfe13538376dc91c087a8",
                "externalIds": {
                    "DBLP": "conf/eccv/LiLLSLS22",
                    "ArXiv": "2211.02707",
                    "DOI": "10.1007/978-3-031-19787-1_19",
                    "CorpusId": 253119991
                },
                "corpusId": 253119991,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/2e138f40afb0e1ce131dfe13538376dc91c087a8",
                "title": "Contrastive Learning for Diverse Disentangled Foreground Generation",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1527091339",
                        "name": "Yuheng Li"
                    },
                    {
                        "authorId": "152998391",
                        "name": "Yijun Li"
                    },
                    {
                        "authorId": "2054975",
                        "name": "Jingwan Lu"
                    },
                    {
                        "authorId": "2177801",
                        "name": "Eli Shechtman"
                    },
                    {
                        "authorId": "144756076",
                        "name": "Yong Jae Lee"
                    },
                    {
                        "authorId": "50339742",
                        "name": "Krishna Kumar Singh"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", StyleGAN) and the semantic control they exhibit [10, 16, 17, 18]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "62f1fa19be63b66fc4bd59b05b6aca98bcd1a99f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-16742",
                    "ArXiv": "2210.16742",
                    "DOI": "10.48550/arXiv.2210.16742",
                    "CorpusId": 253237985
                },
                "corpusId": 253237985,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/62f1fa19be63b66fc4bd59b05b6aca98bcd1a99f",
                "title": "On-the-fly Object Detection using StyleGAN with CLIP Guidance",
                "abstract": "We present a fully automated framework for building object detectors on satellite imagery without requiring any human annotation or intervention. We achieve this by leveraging the combined power of modern generative models (e.g., StyleGAN) and recent advances in multi-modal learning (e.g., CLIP). While deep generative models effectively encode the key semantics pertinent to a data distribution, this information is not immediately accessible for downstream tasks, such as object detection. In this work, we exploit CLIP's ability to associate image features with text descriptions to identify neurons in the generator network, which are subsequently used to build detectors on-the-fly.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2140026650",
                        "name": "Yu-Ta Lu"
                    },
                    {
                        "authorId": "47130096",
                        "name": "Shusen Liu"
                    },
                    {
                        "authorId": "2064767378",
                        "name": "J. Thiagarajan"
                    },
                    {
                        "authorId": "3178630",
                        "name": "W. Sakla"
                    },
                    {
                        "authorId": "2860488",
                        "name": "Rushil Anirudh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, GANSpace [11] is an approach that applies PCA on the latent space, and uses the dominant eigenvectors for the image manipulation.",
                "The authors showed that SeFa enables more disentangled manipulation and is thus better for controlling a single semantic attribute compared to GANSpace.",
                "Next, Shen et al. [32] proposed the closed-form factorization (SeFa), which is similar to GANSpace but the PCA was applied on the weight matrices of the afne transformation."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "f32aa2b0b2894e6ba5f142b52e044d920bc15815",
                "externalIds": {
                    "DBLP": "conf/uist/KoAPKKKJS22",
                    "DOI": "10.1145/3526113.3545612",
                    "CorpusId": 253205327
                },
                "corpusId": 253205327,
                "publicationVenue": {
                    "id": "c62b1316-0733-4b4c-8017-c07e18afa954",
                    "name": "ACM Symposium on User Interface Software and Technology",
                    "type": "conference",
                    "alternate_names": [
                        "User Interface Software and Technology",
                        "ACM Symp User Interface Softw Technol",
                        "User Interface Softw Technol",
                        "UIST"
                    ],
                    "url": "http://www.acm.org/uist/"
                },
                "url": "https://www.semanticscholar.org/paper/f32aa2b0b2894e6ba5f142b52e044d920bc15815",
                "title": "We-toon: A Communication Support System between Writers and Artists in Collaborative Webtoon Sketch Revision",
                "abstract": "We present a communication support system, namely We-toon, that can bridge the webtoon writers and artists during sketch revision (i.e., character design and draft revision). In the highly iterative design process between the webtoon writers and artists, writers often have difficulties in precisely articulating their feedback on sketches owing to their lack of drawing proficiency. This drawback makes the writers rely on textual descriptions and reference images found using search engines, leading to indirect and inefficient communications. Inspired by a formative study, we designed We-toon to help writers revise webtoon sketches and effectively communicate with artists. Through a GAN-based image synthesis and manipulation, We-toon can interactively generate diverse reference images and synthesize them locally on any user-provided image. Our user study with 24 professional webtoon authors demonstrated that We-toon outperforms the traditional methods in terms of communication effectiveness and the writers\u2019 satisfaction level related to the revised image.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2065492498",
                        "name": "Hyung-Kwon Ko"
                    },
                    {
                        "authorId": "11057270",
                        "name": "Subin An"
                    },
                    {
                        "authorId": "1519971579",
                        "name": "Gwanmo Park"
                    },
                    {
                        "authorId": "2166514336",
                        "name": "Seungkwon Kim"
                    },
                    {
                        "authorId": "2142541313",
                        "name": "Daesik Kim"
                    },
                    {
                        "authorId": "2713391",
                        "name": "Bo Hyoung Kim"
                    },
                    {
                        "authorId": "1939362",
                        "name": "Jaemin Jo"
                    },
                    {
                        "authorId": "2016076",
                        "name": "Jinwook Seo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[27] (Apr 2020) create interpretable controls for image synthesis by identifying important latent directions based on PCA applied in the latent or feature space.",
                "Unsupervised Manner Some methods [27, 123] aim to discover interpretable directions in the latent space in an unsupervised manner, i.",
                "InterFaceGAN [97] CVPR 2020 Face N/A \u2717 \u2713 synthetic image & label GANSpace [27] NeurIPS 2020 Face, ImageNet N/A \u2717 \u2713 Unsup.",
                "CONFIG [51] GANSpace [27]) InterFaceGAN [97]) NGP [16] SeFa [98]"
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "bbe393d68defa7cb03b41d6a751726374020bb12",
                "externalIds": {
                    "ArXiv": "2210.14267",
                    "DOI": "10.1145/3626193",
                    "CorpusId": 253116603
                },
                "corpusId": 253116603,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bbe393d68defa7cb03b41d6a751726374020bb12",
                "title": "A Survey on Deep Generative 3D-aware Image Synthesis",
                "abstract": "Recent years have seen remarkable progress in deep learning powered visual content creation. This includes deep generative 3D-aware image synthesis, which produces high-fidelity images in a 3D-consistent manner while simultaneously capturing compact surfaces of objects from pure image collections without the need for any 3D supervision, thus bridging the gap between 2D imagery and 3D reality. The field of computer vision has been recently captivated by the task of deep generative 3D-aware image synthesis, with hundreds of papers appearing in top-tier journals and conferences over the past few years (mainly the past two years), but there lacks a comprehensive survey of this remarkable and swift progress. Our survey aims to introduce new researchers to this topic, provide a useful reference for related works, and stimulate future research directions through our discussion section. Apart from the presented papers, we aim to constantly update the latest relevant papers along with corresponding implementations at https://weihaox.github.io/3D-aware-Gen.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50875615",
                        "name": "Weihao Xia"
                    },
                    {
                        "authorId": "2067730921",
                        "name": "Jing Xue"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "1aeaf0fdc2ffa9e57b1bef5fde11512ae978167d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-14145",
                    "ArXiv": "2210.14145",
                    "DOI": "10.1109/CVPR52729.2023.01616",
                    "CorpusId": 253107418
                },
                "corpusId": 253107418,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1aeaf0fdc2ffa9e57b1bef5fde11512ae978167d",
                "title": "GlassesGAN: Eyewear Personalization Using Synthetic Appearance Discovery and Targeted Subspace Modeling",
                "abstract": "We present GlassesGAN, a novel image editing frame-work for custom design of glasses, that sets a new standard in terms of output-image quality, edit realism, and continuous multi-style edit capability. To facilitate the editing process with GlassesGAN, we propose a Targeted Subspace Modelling (TSM) procedure that, based on a novel mechanism for (synthetic) appearance discovery in the latent space of a pre-trained GAN generator, constructs an eyeglasses-specific (latent) subspace that the editing framework can utilize. Additionally, we also introduce an appearance-constrained subspace initialization (SI) technique that centers the latent representation of the given input image in the well-defined part of the constructed sub-space to improve the reliability of the learned edits. We test GlassesGAN on two (diverse) high-resolution datasets (CelebA-HQ and SiblingsDB-HQf) and compare it to three state-of-the-art baselines, i.e., InterfaceGAN, GANSpace, and MaskGAN. The reported results show that GlassesGAN convincingly outperforms all competing techniques, while offering functionality (e.g., fine-grained multi-style editing) not available with any of the competitors. The source code for GlassesGAN is made publicly available.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1492128685",
                        "name": "Richard Plesh"
                    },
                    {
                        "authorId": "34862665",
                        "name": "P. Peer"
                    },
                    {
                        "authorId": "51301592",
                        "name": "Vitomir \u0160truc"
                    }
                ]
            }
        },
        {
            "contexts": [
                "WarpedGANSpace [175] ICCV 2021 Linear Interpolation ZSN , ZPG, ZBig , Z 1024\u00d7 1024 TARR, L1-normalized Correlation of Attribute Distributions GANSpace [176] NeurIPS 2020 Linear Interpolation ZBig , W 1024\u00d7 1024 -",
                "On the other hand, GANSpace [176] models interpretable traversal directions as the principal components of feature tensors in the W space, which capture the major semantic variations of training data (i.",
                "WarpedGANSpace [175] adopts a framework similar to UDID with nZ estimated by a set of Radial basis functions (RBFs).",
                "On the other hand, GANSpace [176] models interpretable traversal directions as the principal components of feature tensors in the W space, which capture the major semantic variations of training data (i.e., facial attributes) and can be solved by Principal Components Analysis.",
                "The recent success of style-based GANs [28]\u2013[30] in synthesizing HR images and learning disentangled semantic representations has enabled FAM based on latent space navigation [33], [86], [176].",
                "Method Name Publication Navigation Type Latent Space Resolution Quantitative Metrics StyleGAN2Distillation [167] ECCV 2020 Linear Interpolation W , W+ 1024\u00d7 1024 FID, US\nStyleSpaceAnalysis [92] CVPR 2021 Linear Interpolation S 1024\u00d7 1024 FID, TARR, DCI [168], Attribute Dependency (AD) InterFaceGAN [33] CVPR 2020 Linear Interpolation ZPG, Z, W 1024\u00d7 1024 Correlation of Attribute Distributions\nACU [169] ACM MM 2021 Linear Interpolation S 1024\u00d7 1024 FID, AD [92], Success Rate of Local Editing, Region Purity AdvStyle [83] CVPR 2021 Linear Interpolation W 1024\u00d7 1024 Correlation of Attribute Distributions\nEditGAN [170] NeurIPS 2021 Linear Interpolation W+ 1024\u00d7 1024 FID, KID, TARR, CSIM EnjoyEditingGAN [171] ICLR 2021 Linear Interpolation ZPG, W 1024\u00d7 1024 NAPR, CSIM, US Latent-Transformer [40] ICCV 2021 Linear Interpolation W+ 1024\u00d7 1024 The Relation between NAPR/CSIM and Attribute Change Style-Transformer [172] CVPR 2022 Linear Interpolation W+ 1024\u00d7 1024 FID, LPIPS, AD [92], SWD [173], Cost Analysis\nUDID [174] ICML 2020 Linear Interpolation ZSN , ZPG, ZBig 1024\u00d7 1024 TARR, US WarpedGANSpace [175] ICCV 2021 Linear Interpolation ZSN , ZPG, ZBig , Z 1024\u00d7 1024 TARR, L1-normalized Correlation of Attribute Distributions\nGANSpace [176] NeurIPS 2020 Linear Interpolation ZBig , W 1024\u00d7 1024 - SeFa [86] CVPR 2021 Linear Interpolation ZPG, ZBig , Z 1024\u00d7 1024 FID, US, Attribute Re-scoring Analysis\nLowRankGAN [177] NeurIPS 2021 Linear Interpolation ZBig , Z 1024\u00d7 1024 FID, Masked L2 Error of Pixel Value, US, SWD [173] LatentCLR [178] ICCV 2021 Linear Interpolation ZBig , Z 512\u00d7 512 US, Attribute Re-scoring Analysis NeuralODE [179] ICCV 2021 Non-linear Traversal W 256\u00d7 256 US, Control-Disentanglement Curve\nSGF [180] CVPR 2021 Non-linear Traversal ZPG, W -"
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "190a8932bba8807617868a706abe34e6b815b7e7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-12683",
                    "ArXiv": "2210.12683",
                    "DOI": "10.48550/arXiv.2210.12683",
                    "CorpusId": 253098683,
                    "PubMed": "37494159"
                },
                "corpusId": 253098683,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/190a8932bba8807617868a706abe34e6b815b7e7",
                "title": "GAN-based Facial Attribute Manipulation",
                "abstract": "Facial Attribute Manipulation (FAM) aims to aesthetically modify a given face image to render desired attributes, which has received significant attention due to its broad practical applications ranging from digital entertainment to biometric forensics. In the last decade, with the remarkable success of Generative Adversarial Networks (GANs) in synthesizing realistic images, numerous GAN-based models have been proposed to solve FAM with various problem formulation approaches and guiding information representations. This paper presents a comprehensive survey of GAN-based FAM methods with a focus on summarizing their principal motivations and technical details. The main contents of this survey include: (i) an introduction to the research background and basic concepts related to FAM, (ii) a systematic review of GAN-based FAM methods in three main categories, and (iii) an in-depth discussion of important properties of FAM methods, open issues, and future research directions. This survey not only builds a good starting point for researchers new to this field but also serves as a reference for the vision community.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1860829",
                        "name": "Yunfan Liu"
                    },
                    {
                        "authorId": "2118912249",
                        "name": "Qi Li"
                    },
                    {
                        "authorId": "51162255",
                        "name": "Qiyao Deng"
                    },
                    {
                        "authorId": "2118238164",
                        "name": "Zhen Sun"
                    },
                    {
                        "authorId": "2038518365",
                        "name": "Mingcong Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Meanwhile, generative adversarial network Goodfellow et al. (2020) address their latent space for image editing (Ling et al. (2021), Ha\u0308rko\u0308nen et al. (2020), Chefer et al. (2021), Shen et al. (2020),\nYu\u0308ksel et al. (2021), Patashnik et al. (2021), Gal et al. (2021), Dai et al. (2019), Xu et al.\u2026"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "a02313d56a6f71be9aafe43628e0f3a1d0cb858e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-10960",
                    "ArXiv": "2210.10960",
                    "DOI": "10.48550/arXiv.2210.10960",
                    "CorpusId": 253018703
                },
                "corpusId": 253018703,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a02313d56a6f71be9aafe43628e0f3a1d0cb858e",
                "title": "Diffusion Models already have a Semantic Latent Space",
                "abstract": "Diffusion models achieve outstanding generative performance in various domains. Despite their great success, they lack semantic latent space which is essential for controlling the generative process. To address the problem, we propose asymmetric reverse process (Asyrp) which discovers the semantic latent space in frozen pretrained diffusion models. Our semantic latent space, named h-space, has nice properties for accommodating semantic image manipulation: homogeneity, linearity, robustness, and consistency across timesteps. In addition, we introduce a principled design of the generative process for versatile editing and quality boost ing by quantifiable measures: editing strength of an interval and quality deficiency at a timestep. Our method is applicable to various architectures (DDPM++, iD- DPM, and ADM) and datasets (CelebA-HQ, AFHQ-dog, LSUN-church, LSUN- bedroom, and METFACES). Project page: https://kwonminki.github.io/Asyrp/",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2182293854",
                        "name": "Mingi Kwon"
                    },
                    {
                        "authorId": "72286913",
                        "name": "Jaeseok Jeong"
                    },
                    {
                        "authorId": "2847986",
                        "name": "Youngjung Uh"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "064ccebc03d3afabaae30fe29a457c1cfcdff7e3",
                "externalIds": {
                    "ArXiv": "2210.11427",
                    "DBLP": "journals/corr/abs-2210-11427",
                    "DOI": "10.48550/arXiv.2210.11427",
                    "CorpusId": 253018768
                },
                "corpusId": 253018768,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/064ccebc03d3afabaae30fe29a457c1cfcdff7e3",
                "title": "DiffEdit: Diffusion-based semantic image editing with mask guidance",
                "abstract": "Image generation has recently seen tremendous advances, with diffusion models allowing to synthesize convincing images for a large variety of text prompts. In this article, we propose DiffEdit, a method to take advantage of text-conditioned diffusion models for the task of semantic image editing, where the goal is to edit an image based on a text query. Semantic image editing is an extension of image generation, with the additional constraint that the generated image should be as similar as possible to a given input image. Current editing methods based on diffusion models usually require to provide a mask, making the task much easier by treating it as a conditional inpainting task. In contrast, our main contribution is able to automatically generate a mask highlighting regions of the input image that need to be edited, by contrasting predictions of a diffusion model conditioned on different text prompts. Moreover, we rely on latent inference to preserve content in those regions of interest and show excellent synergies with mask-based diffusion. DiffEdit achieves state-of-the-art editing performance on ImageNet. In addition, we evaluate semantic image editing in more challenging settings, using images from the COCO dataset as well as text-based generated images.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1637414390",
                        "name": "Guillaume Couairon"
                    },
                    {
                        "authorId": "34602236",
                        "name": "Jakob Verbeek"
                    },
                    {
                        "authorId": "144518416",
                        "name": "Holger Schwenk"
                    },
                    {
                        "authorId": "51021910",
                        "name": "M. Cord"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, [3], [4] show that the latent space of GAN has rich semantic information and that image manipulation is possible by modifying these latent code w."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "251da7523d4b5ac412ca52f627229999fe88dd60",
                "externalIds": {
                    "DBLP": "conf/ictc/ShinH22",
                    "DOI": "10.1109/ICTC55196.2022.9952548",
                    "CorpusId": 253881925
                },
                "corpusId": 253881925,
                "publicationVenue": {
                    "id": "9aa57e01-2dc2-4422-93b7-2b628326f78c",
                    "name": "Information and Communication Technology Convergence",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Information and Communication Technology Convergence",
                        "Inf Commun Technol Converg",
                        "Int Conf Inf Commun Technol Converg",
                        "ICTC"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/251da7523d4b5ac412ca52f627229999fe88dd60",
                "title": "GAN Inversion with Semantic Segmentation Map for Image Editing",
                "abstract": "In this paper, we propose a framework to perform Generative Adversarial Network (GAN) inversion using semantic segmentation map to invert input image into the GAN latent space. Generally, it is still difficult to invert semantic information of input image into GAN latent space. In particular, conventional GAN inversion methods usually suffer from inverting accurate semantic information such as shape of glasses and hairstyle. To this end, we propose a framework that uses the semantic segmentation map of the real image to guide the latent space corresponding to feature map with coarse resolution in the Style-GANv2. Experimental results show that our proposed method generates more accurate images and is possible of detailed editing of input images with a variety of semantic information compared with previous GAN inversion methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "122935236",
                        "name": "Changyeop Shin"
                    },
                    {
                        "authorId": "48418149",
                        "name": "Y. S. Heo"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "aa6058c93c3959d98bf5b483b553851d6a7e5cf2",
                "externalIds": {
                    "DOI": "10.3390/math10203860",
                    "CorpusId": 253025676
                },
                "corpusId": 253025676,
                "publicationVenue": {
                    "id": "6175efe8-6f8e-4cbe-8cee-d154f4e78627",
                    "name": "Mathematics",
                    "issn": "2227-7390",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-283014",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-283014",
                        "https://www.mdpi.com/journal/mathematics"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/aa6058c93c3959d98bf5b483b553851d6a7e5cf2",
                "title": "A GAN-Based Face Rotation for Artistic Portraits",
                "abstract": "We present a GAN-based model that rotates the faces in artistic portraits to various angles. We build a dataset of artistic portraits for training our GAN-based model by applying a 3D face model to the artistic portraits. We also devise proper loss functions to preserve the styles in the artistic portraits as well as to rotate the faces in the portraits to proper angles. These approaches enable us to construct a GAN-based face rotation model. We apply this model to various artistic portraits, including photorealistic oil paint portraits, watercolor portraits, well-known portrait artworks and banknote portraits, and produce convincing rotated faces in the artistic portraits. Finally, we prove that our model can produce improved results compared with the existing models by evaluating the similarity and the angles of the rotated faces through evaluation schemes including FID estimation, recognition ratio estimation, pose estimation and user study.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2171417905",
                        "name": "Han Kyeol Kim"
                    },
                    {
                        "authorId": "2117055057",
                        "name": "Junho Kim"
                    },
                    {
                        "authorId": "34231520",
                        "name": "Heekyung Yang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "23e261a20a315059b4de5492ed071c97a20c12e7",
                "externalIds": {
                    "ArXiv": "2210.09276",
                    "DBLP": "conf/cvpr/KawarZLTCDMI23",
                    "DOI": "10.1109/CVPR52729.2023.00582",
                    "CorpusId": 252918469
                },
                "corpusId": 252918469,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/23e261a20a315059b4de5492ed071c97a20c12e7",
                "title": "Imagic: Text-Based Real Image Editing with Diffusion Models",
                "abstract": "Text-conditioned image editing has recently attracted considerable interest. However, most methods are currently limited to one of the following: specific editing types (e.g., object overlay, style transfer), synthetically generated images, or requiring multiple input images of a common object. In this paper we demonstrate, for the very first time, the ability to apply complex (e.g., non-rigid) text-based semantic edits to a single real image. For example, we can change the posture and composition of one or multiple objects inside an image, while preserving its original characteristics. Our method can make a standing dog sit down, cause a bird to spread its wings, etc. \u2013 each within its single high-resolution user-provided natural image. Contrary to previous work, our proposed method requires only a single input image and a target text (the desired edit). It operates on real images, and does not require any additional inputs (such as image masks or additional views of the object). Our method, called Imagic, leverages a pre-trained text-to-image diffusion model for this task. It produces a text embedding that aligns with both the input image and the target text, while fine-tuning the diffusion model to capture the image-specific appearance. We demonstrate the quality and versatility of Imagic on numerous inputs from various domains, showcasing a plethora of high quality complex semantic image edits, all within a single unified framework. To better assess performance, we introduce TEdBench, a highly challenging image editing benchmark. We conduct a user study, whose findings show that human raters prefer Imagic to previous leading editing methods on TEdBench.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2047309422",
                        "name": "Bahjat Kawar"
                    },
                    {
                        "authorId": "2145761298",
                        "name": "Shiran Zada"
                    },
                    {
                        "authorId": "49618488",
                        "name": "Oran Lang"
                    },
                    {
                        "authorId": "2047833213",
                        "name": "Omer Tov"
                    },
                    {
                        "authorId": "2146380",
                        "name": "Hui-Tang Chang"
                    },
                    {
                        "authorId": "2112779",
                        "name": "Tali Dekel"
                    },
                    {
                        "authorId": "2138834",
                        "name": "Inbar Mosseri"
                    },
                    {
                        "authorId": "144611617",
                        "name": "M. Irani"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[9] Erik H\u00e4rk\u00f6nen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris.",
                "1 Introduction Contemporary generative adversarial networks (GANs) [8, 14, 15, 13, 3] show remarkable performance in modeling image distributions and have applications in a wide range of computer vision tasks (image enhancement [18, 42], editing [9, 31], image-to-image translation [12, 46, 47], etc."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "666fff7b3570978b67b49b3100a111c6d51ea556",
                "externalIds": {
                    "ArXiv": "2210.08884",
                    "DBLP": "journals/corr/abs-2210-08884",
                    "DOI": "10.48550/arXiv.2210.08884",
                    "CorpusId": 252917819
                },
                "corpusId": 252917819,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/666fff7b3570978b67b49b3100a111c6d51ea556",
                "title": "HyperDomainNet: Universal Domain Adaptation for Generative Adversarial Networks",
                "abstract": "Domain adaptation framework of GANs has achieved great progress in recent years as a main successful approach of training contemporary GANs in the case of very limited training data. In this work, we significantly improve this framework by proposing an extremely compact parameter space for fine-tuning the generator. We introduce a novel domain-modulation technique that allows to optimize only 6 thousand-dimensional vector instead of 30 million weights of StyleGAN2 to adapt to a target domain. We apply this parameterization to the state-of-art domain adaptation methods and show that it has almost the same expressiveness as the full parameter space. Additionally, we propose a new regularization loss that considerably enhances the diversity of the fine-tuned generator. Inspired by the reduction in the size of the optimizing parameter space we consider the problem of multi-domain adaptation of GANs, i.e. setting when the same model can adapt to several domains depending on the input query. We propose the HyperDomainNet that is a hypernetwork that predicts our parameterization given the target domain. We empirically confirm that it can successfully learn a number of domains at once and may even generalize to unseen domains. Source code can be found at https://github.com/MACderRu/HyperDomainNet",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "82901572",
                        "name": "Aibek Alanov"
                    },
                    {
                        "authorId": "2165156333",
                        "name": "Vadim Titov"
                    },
                    {
                        "authorId": "2492721",
                        "name": "D. Vetrov"
                    }
                ]
            }
        },
        {
            "contexts": [
                "SeFa [14] or GANSpace [15] uncover relevant directions in the latent space of pre-trained StyleGAN that affect the semantic properties of the decoded image in an unsupervised manner."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "9adfd1875b67a8520b0538c8f61d08ce683b0c79",
                "externalIds": {
                    "ArXiv": "2301.08443",
                    "DBLP": "journals/corr/abs-2301-08443",
                    "DOI": "10.1109/ICIP46576.2022.9898012",
                    "CorpusId": 253329246
                },
                "corpusId": 253329246,
                "publicationVenue": {
                    "id": "b6369c33-5d70-463c-8e82-95a54efa3cc8",
                    "name": "International Conference on Information Photonics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Image Process",
                        "ICIP",
                        "Int Conf Inf Photonics",
                        "International Conference on Image Processing"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9adfd1875b67a8520b0538c8f61d08ce683b0c79",
                "title": "DIFAI: Diverse Facial Inpainting using StyleGAN Inversion",
                "abstract": "Image inpainting is an old problem in computer vision that restores occluded regions and completes damaged images. In the case of facial image inpainting, most of the methods generate only one result for each masked image, even though there are other reasonable possibilities. To prevent any potential biases and unnatural constraints stemming from generating only one image, we propose a novel framework for diverse facial inpainting exploiting the embedding space of StyleGAN. Our framework employs pSp encoder and SeFa algorithm to identify semantic components of the StyleGAN embeddings and feed them into our proposed SPARN decoder that adopts region normalization for plausible inpainting. We demonstrate that our proposed method outperforms several state-of-the-art methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146861723",
                        "name": "Dongsik Yoon"
                    },
                    {
                        "authorId": "2016481042",
                        "name": "Jeong-gi Kwak"
                    },
                    {
                        "authorId": "2111164052",
                        "name": "Yuanming Li"
                    },
                    {
                        "authorId": "2757702",
                        "name": "D. Han"
                    },
                    {
                        "authorId": "144878703",
                        "name": "Hanseok Ko"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Specifically, investigations [3, 4, 18, 40, 45] predict the meaningful offsets or directions in the latent space given image annotations as supervision, while studies [16, 41, 48, 49] disentangle the latent space in an unsupervised manner to find the semantic directions."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "841fbc1a767bfdf689109748aa82bb5de56fd92b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-07883",
                    "ArXiv": "2210.07883",
                    "DOI": "10.48550/arXiv.2210.07883",
                    "CorpusId": 252907861
                },
                "corpusId": 252907861,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/841fbc1a767bfdf689109748aa82bb5de56fd92b",
                "title": "One Model to Edit Them All: Free-Form Text-Driven Image Manipulation with Semantic Modulations",
                "abstract": "Free-form text prompts allow users to describe their intentions during image manipulation conveniently. Based on the visual latent space of StyleGAN[21] and text embedding space of CLIP[34], studies focus on how to map these two latent spaces for text-driven attribute manipulations. Currently, the latent mapping between these two spaces is empirically designed and confines that each manipulation model can only handle one fixed text prompt. In this paper, we propose a method named Free-Form CLIP (FFCLIP), aiming to establish an automatic latent mapping so that one manipulation model handles free-form text prompts. Our FFCLIP has a cross-modality semantic modulation module containing semantic alignment and injection. The semantic alignment performs the automatic latent mapping via linear transformations with a cross attention mechanism. After alignment, we inject semantics from text prompt embeddings to the StyleGAN latent space. For one type of image (e.g., `human portrait'), one FFCLIP model can be learned to handle free-form text prompts. Meanwhile, we observe that although each training text prompt only contains a single semantic meaning, FFCLIP can leverage text prompts with multiple semantic meanings for image manipulation. In the experiments, we evaluate FFCLIP on three types of images (i.e., `human portraits', `cars', and `churches'). Both visual and numerical results show that FFCLIP effectively produces semantically accurate and visually realistic images. Project page: https://github.com/KumapowerLIU/FFCLIP.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118953468",
                        "name": "Yi-Chun Zhu"
                    },
                    {
                        "authorId": "2115669461",
                        "name": "Hongyu Liu"
                    },
                    {
                        "authorId": "2255687",
                        "name": "Yibing Song"
                    },
                    {
                        "authorId": "8591253",
                        "name": "Ziyang Yuan"
                    },
                    {
                        "authorId": "2257769",
                        "name": "Xintong Han"
                    },
                    {
                        "authorId": "2175625059",
                        "name": "Chun Yuan"
                    },
                    {
                        "authorId": "2157737759",
                        "name": "Qifeng Chen"
                    },
                    {
                        "authorId": "2167482071",
                        "name": "Jue Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "According to these PCA results on vector-based texture codes and shape codes, our TSD-GAN clearly exhibits the capability of controlling the useful directions of textures and shapes, suggesting the potential to assist designers to accomplish \u201cintelligent\u201d designs to some extent.",
                "Then, each PCA result on the texture code of a fashion item is formulated as\npt = G (Tt (xt + \u03f1 \u00d7 tk ) ,Ts (xs )) , (15) where \u03f1 \u2208 \u03a9 and \u03f1\u00d7tk denotes moving along the component tk in the \u03f1 edit direction.",
                "Here, sk (k \u2208 [1, 5]) denotes the five principal shape components decomposed by PCA; and each PCA result on the shape code of\nACM Trans.",
                "For texture codes, we use PCA important directions in the latent space.",
                "Likewise, for shape codes, we also use PCA to learn the first five critical directions in the latent space.",
                "Figure 9 illustrates certain samples generated by PCA.",
                "In line with this, we use principal component analysis (PCA) [13] to calculate the vector-based texture and shape in the orthogonal direction in order to obtain texture and shape principal components.",
                "Here, tk (k \u2208 [1, 5]) denotes the five principal texture components decomposed by PCA; \u03a9 = [\u22121, 1] indicates the edit directions of the texture codes.",
                "In line with this, we use principal component analysis (PCA) [13] to calculate the vector-based texture and shape in the orthogonal direction to obtain texture and shape principal components.",
                "Figure 10 illustrates some samples by utilizing the PCA."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "d5224b86970032a522b01f870553c82be99aa80e",
                "externalIds": {
                    "DBLP": "journals/tomccap/Yan0SMX23",
                    "DOI": "10.1145/3567596",
                    "CorpusId": 252847273
                },
                "corpusId": 252847273,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d5224b86970032a522b01f870553c82be99aa80e",
                "title": "Toward Intelligent Fashion Design: A Texture and Shape Disentangled Generative Adversarial Network",
                "abstract": "Texture and shape in fashion, constituting essential elements of garments, characterize the body and surface of the fabric and outline the silhouette of clothing, respectively. The selection of texture and shape plays a critical role in the design process, as they largely determine the success of a new design for fashion items. In this research, we propose a texture and shape disentangled generative adversarial network (TSD-GAN) to perform \u201cintelligent\u201d design with the transformation of texture and shape in fashion items. Our TSD-GAN aims to learn how to disentangle the features of texture and shape of different fashion items in an unsupervised manner. Specifically, a fashion attribute encoder is developed to decompose the input fashion items into independent representations of texture and shape. Then, to learn the coarse or fine styles hidden in the features of texture and shape, a texture mapping network and a shape mapping network are proposed to disentangle the features into different hierarchical representations. The different hierarchical representations of texture and shape are then fed into a multi-factor-based generator to generate mixed-style fashion items. In addition, a multi-discriminator framework is developed to distinguish the authenticity and texture similarity between the generated images and the real images. Experimental results on different fashion categories demonstrate that our proposed TSD-GAN may be useful for assisting designers to accomplish the design process by transforming the texture and shape of fashion items.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152208647",
                        "name": "Han Yan"
                    },
                    {
                        "authorId": "2187723675",
                        "name": "Haijun Zhang"
                    },
                    {
                        "authorId": "2153118892",
                        "name": "Jianyang Shi"
                    },
                    {
                        "authorId": "150152641",
                        "name": "Jianghong Ma"
                    },
                    {
                        "authorId": "2173137660",
                        "name": "Xiaofei Xu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We employ GANSpace [17] method to quantitatively evaluate the manipulation capability of the acquired latent code.",
                "In this work, we leverage GANSpace [17], which performs principal component analysis in the latent space, to demonstrate latent-based manipulation of 3D shape.",
                "We perform various edits [17] over latent codes and camera pose acquired by each method.",
                "Thus, other researchers resorted to using an unsupervised approach [17] or contrastive learning based methods [48, 35] to find meaningful directions."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6703fc21d6801c1ab3f755269ddd14e23c47bd8d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-07301",
                    "ArXiv": "2210.07301",
                    "DOI": "10.1109/WACV56688.2023.00298",
                    "CorpusId": 252907567
                },
                "corpusId": 252907567,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/6703fc21d6801c1ab3f755269ddd14e23c47bd8d",
                "title": "3D GAN Inversion with Pose Optimization",
                "abstract": "With the recent advances in NeRF-based 3D aware GANs quality, projecting an image into the latent space of these 3D-aware GANs has a natural advantage over 2D GAN inversion: not only does it allow multi-view consistent editing of the projected image, but it also enables 3D reconstruction and novel view synthesis when given only a single image. However, the explicit viewpoint control acts as a main hindrance in the 3D GAN inversion process, as both camera pose and latent code have to be optimized simultaneously to reconstruct the given image. Most works that explore the latent space of the 3D-aware GANs rely on ground-truth camera viewpoint or deformable 3D model, thus limiting their applicability. In this work, we introduce a generalizable 3D GAN inversion method that infers camera viewpoint and latent code simultaneously to enable multi-view consistent semantic image editing. The key to our approach is to leverage pre-trained estimators for better initialization and utilize the pixel-wise depth calculated from NeRF parameters to better reconstruct the given image. We conduct extensive experiments on image reconstruction and editing both quantitatively and qualitatively, and further compare our results with 2D GAN-based editing to demonstrate the advantages of utilizing the latent space of 3D GANs. Additional results and visualizations are available at https://3dgan-inversion.github.io/.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146964336",
                        "name": "Jae-Sub Ko"
                    },
                    {
                        "authorId": "2163750694",
                        "name": "Kyusun Cho"
                    },
                    {
                        "authorId": "2114950520",
                        "name": "Daewon Choi"
                    },
                    {
                        "authorId": "108517481",
                        "name": "Kwang-seok Ryoo"
                    },
                    {
                        "authorId": "2596437",
                        "name": "Seung Wook Kim"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "2f0205be8c5ca92eeeaf0634bb8d828c49118bc5",
                "externalIds": {
                    "ArXiv": "2210.06642",
                    "DBLP": "journals/corr/abs-2210-06642",
                    "DOI": "10.1111/cgf.14761",
                    "CorpusId": 252873228
                },
                "corpusId": 252873228,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2f0205be8c5ca92eeeaf0634bb8d828c49118bc5",
                "title": "What's in a Decade? Transforming Faces Through Time",
                "abstract": "How can one visually characterize photographs of people over time? In this work, we describe the Faces Through Time dataset, which contains over a thousand portrait images per decade from the 1880s to the present day. Using our new dataset, we devise a framework for resynthesizing portrait images across time, imagining how a portrait taken during a particular decade might have looked like had it been taken in other decades. Our framework optimizes a family of per\u2010decade generators that reveal subtle changes that differentiate decades\u2014such as different hairstyles or makeup\u2014while maintaining the identity of the input portrait. Experiments show that our method can more effectively resynthesizing portraits across time compared to state\u2010of\u2010the\u2010art image\u2010to\u2010image translation methods, as well as attribute\u2010based and language\u2010guided portrait editing models. Our code and data will be available at facesthroughtime.github.io.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2190443251",
                        "name": "Eric Chen"
                    },
                    {
                        "authorId": "2157277451",
                        "name": "Jin Sun"
                    },
                    {
                        "authorId": "3139858",
                        "name": "Apoorv Khandelwal"
                    },
                    {
                        "authorId": "70018371",
                        "name": "D. Lischinski"
                    },
                    {
                        "authorId": "1830653",
                        "name": "Noah Snavely"
                    },
                    {
                        "authorId": "1388323535",
                        "name": "Hadar Averbuch-Elor"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Fr\u00e9chet basis is compared with GANSpace [12] and SeFa [35] because these two methods are also unsupervised global basis (Sec 2).",
                "Among them, one approach is to find meaningful latent perturbations that induce the disentangled semantic variation on generated images [7, 12, 33, 35].",
                ", Global directions in StyleCLIP [29], GANSpace [12], and SeFa [35].",
                "For a fair comparison, we took the annotated basis in GANSpace [12] and compared those with Fr\u00e9chet basis onW-space of three StyleGAN models.",
                "GANSpace [12] suggested the principal components of latent space obtained by performing PCA as global meaningful perturbations.",
                "Following the experiments in [6], we assessed the global-basis-compatibility by the FID [13] Gap between Local Basis and GANSpace [12] under the same perturbation intensity.",
                "In this work, we focus on the global methods [12, 35].",
                "We ran this basis refinement on the subspace generated by the existing global methods, GANSpace [12] and SeFa [35]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "9534ee7d40043ead143246503cd9d9e175b4fc25",
                "externalIds": {
                    "DBLP": "conf/iclr/ChoiHCK23",
                    "ArXiv": "2210.05509",
                    "DOI": "10.48550/arXiv.2210.05509",
                    "CorpusId": 252815793
                },
                "corpusId": 252815793,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9534ee7d40043ead143246503cd9d9e175b4fc25",
                "title": "Finding the global semantic representation in GAN through Frechet Mean",
                "abstract": "The ideally disentangled latent space in GAN involves the global representation of latent space with semantic attribute coordinates. In other words, considering that this disentangled latent space is a vector space, there exists the global semantic basis where each basis component describes one attribute of generated images. In this paper, we propose an unsupervised method for finding this global semantic basis in the intermediate latent space in GANs. This semantic basis represents sample-independent meaningful perturbations that change the same semantic attribute of an image on the entire latent space. The proposed global basis, called Fr\\'echet basis, is derived by introducing Fr\\'echet mean to the local semantic perturbations in a latent space. Fr\\'echet basis is discovered in two stages. First, the global semantic subspace is discovered by the Fr\\'echet mean in the Grassmannian manifold of the local semantic subspaces. Second, Fr\\'echet basis is found by optimizing a basis of the semantic subspace via the Fr\\'echet mean in the Special Orthogonal Group. Experimental results demonstrate that Fr\\'echet basis provides better semantic factorization and robustness compared to the previous methods. Moreover, we suggest the basis refinement scheme for the previous methods. The quantitative experiments show that the refined basis achieves better semantic factorization while constrained on the same semantic subspace given by the previous method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2921953",
                        "name": "Jaewoong Choi"
                    },
                    {
                        "authorId": "1947131861",
                        "name": "Geonho Hwang"
                    },
                    {
                        "authorId": "2111237576",
                        "name": "Hyunsoo Cho"
                    },
                    {
                        "authorId": "2259103",
                        "name": "Myung-joo Kang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These methods free users from drawing on face masks and shows superiority over other latent space manipulation methods [17, 21, 35, 47] in component transfer and disentangled attribute manipulation.",
                "GANSpace [17] carries out Principal Component Analysis (PCA) in the latent space of generative networks and explores interpretable controls in an unsupervised manner."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "abac6e242ece348dd8a98d827187f483199d3b2a",
                "externalIds": {
                    "DBLP": "conf/mm/HuangT022",
                    "DOI": "10.1145/3503161.3548392",
                    "CorpusId": 252782128
                },
                "corpusId": 252782128,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/abac6e242ece348dd8a98d827187f483199d3b2a",
                "title": "Box-FaceS: A Bidirectional Method for Box-Guided Face Component Editing",
                "abstract": "While the quality of face manipulation has been improved tremendously, the ability to control face components, e.g., eyebrows, is still limited. Although existing methods have realized component editing with user-provided geometry guidance, such as masks or sketches, their performance is largely dependent on the user's painting efforts. To address these issues, we propose Box-FaceS, a bidirectional method that can edit face components by simply translating and zooming the bounding boxes. This framework learns representations for every face component, independently, as well as a high-dimensional tensor capturing face outlines. To enable box-guided face editing, we develop a novel Box Adaptive Modulation (BAM) module for the generator, which first transforms component embeddings to style parameters and then modulates visual features inside a given box-like region on the face outlines. A cooperative learning scheme is proposed to impose independence between face outlines and component embeddings. As a result, it is flexible to determine the component style by its embedding, and to control its position and size by the provided bounding box. Box-FaceS also learns to transfer components between two faces while maintaining the consistency of image content. In particular, Box-FaceS can generate creative faces with reasonable exaggerations, requiring neither supervision nor complex spatial morphing operations. Through the comparisons with state-of-the-art methods, Box-FaceS shows its superiority in component editing, both qualitatively and quantitatively. To the best of our knowledge, Box-FaceS is the first approach that can freely edit the position and shape of the face components without editing the face masks or sketches. Our implementation is available at https://github.com/CMACH508/Box-FaceS.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "151492345",
                        "name": "Wenjing Huang"
                    },
                    {
                        "authorId": "1701972",
                        "name": "Shikui Tu"
                    },
                    {
                        "authorId": "2109329726",
                        "name": "Lei Xu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANSpace [16] adopts PCA to find facial semantic representation in the latent space of the GANmodel.",
                "Recently, learning facial semantics via manipulating latent code in the latent space has achieved great success in high-fidelity face image synthesis [16, 41, 43].",
                "[16] adopt PCA to find the principle face attribute representation in the latent space of GAN model."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "186e4a7ab8676d1bd95682cd7617a6e65675104c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-05300",
                    "DOI": "10.1145/3503161.3547791",
                    "CorpusId": 250533889
                },
                "corpusId": 250533889,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/186e4a7ab8676d1bd95682cd7617a6e65675104c",
                "title": "SD-GAN: Semantic Decomposition for Face Image Synthesis with Discrete Attribute",
                "abstract": "Manipulating latent code in generative adversarial networks (GANs) for facial image synthesis mainly focuses on continuous attribute synthesis (e.g., age, pose and emotion), while discrete attribute synthesis (like face mask and eyeglasses) receives less attention. Directly applying existing works to facial discrete attributes may cause inaccurate results. In this work, we propose an innovative framework to tackle challenging facial discrete attribute synthesis via semantic decomposing, dubbed SD-GAN. To be concrete, we explicitly decompose the discrete attribute representation into two components, i.e. the semantic prior basis and offset latent representation. The semantic prior basis shows an initializing direction for manipulating face representation in the latent space. The offset latent presentation obtained by 3D-aware semantic fusion network is proposed to adjust prior basis. In addition, the fusion network integrates 3D embedding for better identity preservation and discrete attribute synthesis. The combination of prior basis and offset latent representation enable our method to synthesize photo-realistic face images with discrete attributes. Notably, we construct a large and valuable dataset MEGN (Face Mask and Eyeglasses images crawled from Google and Naver) for completing the lack of discrete attributes in the existing dataset. Extensive qualitative and quantitative experiments demonstrate the state-of-the-art performance of our method. Our code is available at an anonymous website: https://github.com/MontaEllis/SD-GAN.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1491232168",
                        "name": "Kangneng Zhou"
                    },
                    {
                        "authorId": "2159182559",
                        "name": "Xiaobin Zhu"
                    },
                    {
                        "authorId": "1380181436",
                        "name": "Daiheng Gao"
                    },
                    {
                        "authorId": "2176278333",
                        "name": "Kai Lee"
                    },
                    {
                        "authorId": "2108191762",
                        "name": "Xinjie Li"
                    },
                    {
                        "authorId": "1682664",
                        "name": "Xu-Cheng Yin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANSpace [20] identify important manipulation vectors as the principal components (PCA)"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "7ee02a0b3a0b39d3f44704eab82a405a44d84c3a",
                "externalIds": {
                    "DBLP": "conf/mm/ChenWZYL22",
                    "DOI": "10.1145/3503161.3548002",
                    "CorpusId": 252782473
                },
                "corpusId": 252782473,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7ee02a0b3a0b39d3f44704eab82a405a44d84c3a",
                "title": "D2Animator: Dual Distillation of StyleGAN For High-Resolution Face Animation",
                "abstract": "The style-based generator architectures (e.g. StyleGAN v1, v2) largely promote the controllability and explainability of Generative Adversarial Networks (GANs). Many researchers have applied the pretrained style-based generators to image manipulation and video editing by exploring the correlation between linear interpolation in the latent space and semantic transformation in the synthesized image manifold. However, most previous studies focused on manipulating separate discrete attributes, which is insufficient to animate a still image to generate videos with complex and diverse poses and expressions. In this work, we devise a dual distillation strategy (D2Animator) for generating animated high-resolution face videos conditioned on identities and poses from different images. Specifically, we first introduce a Clustering-based Distiller (CluDistiller) to distill diverse interpolation directions in the latent space, and synthesize identity-consistent faces with various poses and expressions, such as blinking, frowning, looking up/down, etc. Then we propose an Augmentation-based Distiller (AugDistiller) that learns to encode arbitrary face deformation into a combination of interpolation directions via training on augmentation samples synthesized by CluDistiller. Through assembling the two distillation methods, D2Animator can generate high-resolution face animation videos without training on video sequences. Extensive experiments on self-driving, cross-identity and sequence-driving tasks demonstrate the superiority of the proposed D2Animator over existing StyleGAN manipulation and face animation methods in both generation quality and animation fidelity.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2111499679",
                        "name": "Zhuo Chen"
                    },
                    {
                        "authorId": "2518211",
                        "name": "Chaoyue Wang"
                    },
                    {
                        "authorId": "11359740",
                        "name": "Haimei Zhao"
                    },
                    {
                        "authorId": "2055907880",
                        "name": "Bo Yuan"
                    },
                    {
                        "authorId": "2164225707",
                        "name": "Xiu Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "9fb5064f30b581a8997d035ad08a3d01a75c1863",
                "externalIds": {
                    "DBLP": "conf/mm/IsrarZ22",
                    "DOI": "10.1145/3503161.3548415",
                    "CorpusId": 252782630
                },
                "corpusId": 252782630,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9fb5064f30b581a8997d035ad08a3d01a75c1863",
                "title": "Customizing GAN Using Few-shot Sketches",
                "abstract": "Generative adversarial networks (GANs) have demonstrated remarkable success in image synthesis applications, but their performance deteriorates under limited data regimes. The fundamental challenge is that it is extremely difficult to synthesize photo-realistic and highly diversified images while capturing meaningful attributes of the targets under minimum supervision. Previous methods either fine-tune or rewrite the model weights to adapt to few-shot datasets. However, this either overfits or requires access to large-scale data on which they are trained. To tackle the problem, we propose a framework that repurposes the existing pre-trained generative models using only a few samples (e.g., <30) of sketches. Unlike previous works, we transfer the sample diversity and quality without accessing the source data using inter-domain distance consistency. By employing cross-domain adversarial learning, we encourage the model output to closely resemble the input sketches in both shape and pose. Extensive experiments show that our method significantly outperforms the existing approaches in terms of sample quality and diversity. The qualitative and quantitative results on various standard datasets also demonstrate its efficacy. On the most popularly used dataset, Gabled church, we achieve a Fr\u00e9chet inception distance (FID) score of 15.63.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "6694630",
                        "name": "S. M. Israr"
                    },
                    {
                        "authorId": "47861039",
                        "name": "Feng Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[2] identified important latent directions based on Principal Component Analysis (PCA) and showed a large number of interpretable controls can be defined by layer-wise perturbation along the principal directions."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "9239da44d076700faec0a68328756e6676b2b264",
                "externalIds": {
                    "DBLP": "conf/icb/DongPLZWG22",
                    "DOI": "10.1109/IJCB54206.2022.10007972",
                    "CorpusId": 255994872
                },
                "corpusId": 255994872,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9239da44d076700faec0a68328756e6676b2b264",
                "title": "SketchCLIP: Text-based Attribute Manipulation for Face Sketch Synthesis",
                "abstract": "This paper proposes a method of modifying the face sketch with text descriptions. Face sketch is widely used in the criminal field and digital entertainment field. Forensic painters usually draw face sketches based on descriptions provided by witnesses or clients. However, drawing a face sketch often takes lots of time and effort. Existing face sketch synthesis studies have not considered text-based sketch manipulation, and we find that applying text-driven editing methods on natural images directly to face sketches causes severe distortion of generated results. Therefore, this paper proposes a novel text-based attribute manipulation method for face sketch synthesis, named SketchCLIP. Our approach adopts text-driven attribute manipulation by using the powerful Contrastive Language-Image Pre-Training (CLIP) model, which not only conforms to the current drawing process of face sketches but also does not require tedious manual operations and allows for more diverse modifications. Besides, we design an intra-modality fine-tuning module to eliminate distortion and improve the quality of the modified face sketch. Through extensive comparison experiments on public face sketch datasets, our method is demonstrated to be very excellent in the effectiveness of the face sketch processing and the quality of modified results.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2201678307",
                        "name": "Mengdi Dong"
                    },
                    {
                        "authorId": "2299758",
                        "name": "Chunlei Peng"
                    },
                    {
                        "authorId": "4308702",
                        "name": "Decheng Liu"
                    },
                    {
                        "authorId": "145473096",
                        "name": "Yu Zheng"
                    },
                    {
                        "authorId": "144050305",
                        "name": "N. Wang"
                    },
                    {
                        "authorId": "2164214077",
                        "name": "Xinbo Gao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other works show that it is possible to find meaningful directions in latent space in an unsupervised way [20, 21, 49]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "b56a1aa75f99f42823b3702b546a81837d9d6242",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-03007",
                    "ArXiv": "2210.03007",
                    "DOI": "10.48550/arXiv.2210.03007",
                    "CorpusId": 252734885
                },
                "corpusId": 252734885,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/b56a1aa75f99f42823b3702b546a81837d9d6242",
                "title": "XDGAN: Multi-Modal 3D Shape Generation in 2D Space",
                "abstract": "Generative models for 2D images has recently seen tremendous progress in quality, resolution and speed as a result of the efficiency of 2D convolutional architectures. However it is difficult to extend this progress into the 3D domain since most current 3D representations rely on custom network components. This paper addresses a central question: Is it possible to directly leverage 2D image generative models to generate 3D shapes instead? To answer this, we propose XDGAN, an effective and fast method for applying 2D image GAN architectures to the generation of 3D object geometry combined with additional surface attributes, like color textures and normals. Specifically, we propose a novel method to convert 3D shapes into compact 1-channel geometry images and leverage StyleGAN3 and image-to-image translation networks to generate 3D objects in 2D space. The generated geometry images are quick to convert to 3D meshes, enabling real-time 3D object synthesis, visualization and interactive editing. Moreover, the use of standard 2D architectures can help bring more 2D advances into the 3D realm. We show both quantitatively and qualitatively that our method is highly effective at various tasks such as 3D shape generation, single view reconstruction and shape manipulation, while being significantly faster and more flexible compared to recent 3D generative models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3278836",
                        "name": "Hassan Abu Alhaija"
                    },
                    {
                        "authorId": "2145260636",
                        "name": "Alara Dirik"
                    },
                    {
                        "authorId": "2187057614",
                        "name": "Andr'e Knorig"
                    },
                    {
                        "authorId": "37895334",
                        "name": "S. Fidler"
                    },
                    {
                        "authorId": "2854827",
                        "name": "Maria Shugrina"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In the image domain, it has been demonstrated that augmentations in an embedded space learned via a GAN [8, 26, 30] or an encoder-decoder model [12] can be used to improve classification performance."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1dc68762b4e2fb146349bdfb401f2ffc2e25158d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-02995",
                    "ArXiv": "2210.02995",
                    "DOI": "10.48550/arXiv.2210.02995",
                    "CorpusId": 252735173
                },
                "corpusId": 252735173,
                "publicationVenue": {
                    "id": "a8f26d13-e373-4e48-b57b-ef89bf48f4db",
                    "name": "Asian Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Asian Conf Comput Vis",
                        "ACCV"
                    ],
                    "url": "http://www.cvl.iis.u-tokyo.ac.jp/afcv/"
                },
                "url": "https://www.semanticscholar.org/paper/1dc68762b4e2fb146349bdfb401f2ffc2e25158d",
                "title": "Compressed Vision for Efficient Video Understanding",
                "abstract": "Experience and reasoning occur across multiple temporal scales: milliseconds, seconds, hours or days. The vast majority of computer vision research, however, still focuses on individual images or short videos lasting only a few seconds. This is because handling longer videos require more scalable approaches even to process them. In this work, we propose a framework enabling research on hour-long videos with the same hardware that can now process second-long videos. We replace standard video compression, e.g. JPEG, with neural compression and show that we can directly feed compressed videos as inputs to regular video networks. Operating on compressed videos improves efficiency at all pipeline levels -- data transfer, speed and memory -- making it possible to train models faster and on much longer videos. Processing compressed signals has, however, the downside of precluding standard augmentation techniques if done naively. We address that by introducing a small network that can apply transformations to latent codes corresponding to commonly used augmentations in the original video space. We demonstrate that with our compressed vision pipeline, we can train video models more efficiently on popular benchmarks such as Kinetics600 and COIN. We also perform proof-of-concept experiments with new tasks defined over hour-long videos at standard frame rates. Processing such long videos is impossible without using compressed representation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "8792285",
                        "name": "Olivia Wiles"
                    },
                    {
                        "authorId": "144601991",
                        "name": "J. Carreira"
                    },
                    {
                        "authorId": "2159207795",
                        "name": "Iain Barr"
                    },
                    {
                        "authorId": "1688869",
                        "name": "Andrew Zisserman"
                    },
                    {
                        "authorId": "145478807",
                        "name": "Mateusz Malinowski"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, GAN-based editing methods invert an image to the latent space [1, 9, 14, 99, 121, 138], and edit the inverted image by modifying the latent code [49, 68, 72, 89, 109, 137].",
                "We now show that images inverted with top-ranked models can be further edited using existing GAN-based image editing techniques such as GANSpace [49].",
                "We use the model ranked first by our image-based model retrieval algorithm for inverting the real image, and then we perform editing using GANspace [49]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "9942f459461bb5d4e57f9829e5973ce999087c90",
                "externalIds": {
                    "ArXiv": "2210.03116",
                    "DBLP": "journals/corr/abs-2210-03116",
                    "DOI": "10.48550/arXiv.2210.03116",
                    "CorpusId": 252735129
                },
                "corpusId": 252735129,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9942f459461bb5d4e57f9829e5973ce999087c90",
                "title": "Content-Based Search for Deep Generative Models",
                "abstract": "The growing proliferation of pretrained generative models has made it infeasible for a user to be fully cognizant of every model in existence. To address this need, we introduce the task of content-based model search: given a query and a large set of generative models, find the models that best match the query. Because each generative model produces a distribution of images, we formulate the search problem as an optimization to maximize the probability of generating a query match given a model. We develop approximations to make this problem tractable when the query is an image, a sketch, a text description, another generative model, or a combination of the above. We benchmark our method in both accuracy and speed over a set of generative models. We demonstrate that our model search retrieves suitable models for image editing and reconstruction, few-shot transfer learning, and latent space interpolation. Finally, we deploy our search algorithm to our online generative model-sharing platform at https://modelverse.cs.cmu.edu.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1768846661",
                        "name": "Daohan Lu"
                    },
                    {
                        "authorId": "12782331",
                        "name": "Sheng-Yu Wang"
                    },
                    {
                        "authorId": "46373847",
                        "name": "Nupur Kumari"
                    },
                    {
                        "authorId": "41180706",
                        "name": "Rohan Agarwal"
                    },
                    {
                        "authorId": "144159726",
                        "name": "David Bau"
                    },
                    {
                        "authorId": "1922024303",
                        "name": "Jun-Yan Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There are multiple methods [9, 14, 17, 24, 36, 44, 45, 54, 57, 63] to manipulate the latent code, most of them are based on algebraic operations on the latent code.",
                "One major example of deepfakes is face manipulation with GANs, which has been an emerging topic in very recent years [9, 10, 12, 14, 17, 24, 36, 38, 44, 45, 47, 49, 54, 56, 57, 63].",
                "The second step is latent code manipulation [9, 14, 17, 24, 36, 44, 45, 54, 57, 63]"
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "70e9be708b1fae70c5340d33ca738c6c813da9ae",
                "externalIds": {
                    "DBLP": "conf/uss/LiY00F023",
                    "ArXiv": "2210.00957",
                    "DOI": "10.48550/arXiv.2210.00957",
                    "CorpusId": 252683525
                },
                "corpusId": 252683525,
                "publicationVenue": {
                    "id": "54649c1d-6bcc-4232-9cd1-aa446867b8d0",
                    "name": "USENIX Security Symposium",
                    "type": "conference",
                    "alternate_names": [
                        "USENIX Secur Symp"
                    ],
                    "url": "http://www.usenix.org/events/bytopic/security.html"
                },
                "url": "https://www.semanticscholar.org/paper/70e9be708b1fae70c5340d33ca738c6c813da9ae",
                "title": "UnGANable: Defending Against GAN-based Face Manipulation",
                "abstract": "Deepfakes pose severe threats of visual misinformation to our society. One representative deepfake application is face manipulation that modifies a victim's facial attributes in an image, e.g., changing her age or hair color. The state-of-the-art face manipulation techniques rely on Generative Adversarial Networks (GANs). In this paper, we propose the first defense system, namely UnGANable, against GAN-inversion-based face manipulation. In specific, UnGANable focuses on defending GAN inversion, an essential step for face manipulation. Its core technique is to search for alternative images (called cloaked images) around the original images (called target images) in image space. When posted online, these cloaked images can jeopardize the GAN inversion process. We consider two state-of-the-art inversion techniques including optimization-based inversion and hybrid inversion, and design five different defenses under five scenarios depending on the defender's background knowledge. Extensive experiments on four popular GAN models trained on two benchmark face datasets show that UnGANable achieves remarkable effectiveness and utility performance, and outperforms multiple baseline methods. We further investigate four adaptive adversaries to bypass UnGANable and show that some of them are slightly effective.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146247989",
                        "name": "Zheng Li"
                    },
                    {
                        "authorId": "2052212417",
                        "name": "Ning Yu"
                    },
                    {
                        "authorId": "66697271",
                        "name": "A. Salem"
                    },
                    {
                        "authorId": "144588806",
                        "name": "M. Backes"
                    },
                    {
                        "authorId": "1739548",
                        "name": "Mario Fritz"
                    },
                    {
                        "authorId": "1698138",
                        "name": "Yang Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "614f8de5b21d73da35dd897753969276e39678fd",
                "externalIds": {
                    "DBLP": "journals/cee/ChenZL22",
                    "DOI": "10.1016/j.compeleceng.2022.108282",
                    "CorpusId": 252341475
                },
                "corpusId": 252341475,
                "publicationVenue": {
                    "id": "4b85084f-20f0-4592-a3c6-2f38acb5c0b3",
                    "name": "Computers & electrical engineering",
                    "type": "journal",
                    "alternate_names": [
                        "Computers & Electrical Engineering",
                        "Comput  electr eng",
                        "Comput  Electr Eng"
                    ],
                    "issn": "0045-7906",
                    "url": "https://www.journals.elsevier.com/computers-and-electrical-engineering",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00457906"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/614f8de5b21d73da35dd897753969276e39678fd",
                "title": "Face image inpainting via latent features reconstruction and mask awareness",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144180429",
                        "name": "F. Chen"
                    },
                    {
                        "authorId": "2146325702",
                        "name": "Tongtong Zhang"
                    },
                    {
                        "authorId": "2109279387",
                        "name": "Heng Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c73a29a25b753bc5f6a102e67ba6cb5e4b7d4f3a",
                "externalIds": {
                    "DBLP": "journals/cgf/SeoOLLKN22",
                    "DOI": "10.1111/cgf.14666",
                    "CorpusId": 254185772
                },
                "corpusId": 254185772,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c73a29a25b753bc5f6a102e67ba6cb5e4b7d4f3a",
                "title": "StylePortraitVideo: Editing Portrait Videos with Expression Optimization",
                "abstract": "High\u2010quality portrait image editing has been made easier by recent advances in GANs (e.g., StyleGAN) and GAN inversion methods that project images onto a pre\u2010trained GAN's latent space. However, extending the existing image editing methods, it is hard to edit videos to produce temporally coherent and natural\u2010looking videos. We find challenges in reproducing diverse video frames and preserving the natural motion after editing. In this work, we propose solutions for these challenges. First, we propose a video adaptation method that enables the generator to reconstruct the original input identity, unusual poses, and expressions in the video. Second, we propose an expression dynamics optimization that tweaks the latent codes to maintain the meaningful motion in the original video. Based on these methods, we build a StyleGAN\u2010based high\u2010quality portrait video editing system that can edit videos in the wild in a temporally coherent way at up to 4K resolution.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1920243364",
                        "name": "Kwanggyoon Seo"
                    },
                    {
                        "authorId": "3451982",
                        "name": "Seoung Wug Oh"
                    },
                    {
                        "authorId": "2054975",
                        "name": "Jingwan Lu"
                    },
                    {
                        "authorId": "1576788264",
                        "name": "Joon-Young Lee"
                    },
                    {
                        "authorId": "2109603647",
                        "name": "Seonghyeon Kim"
                    },
                    {
                        "authorId": "2140244852",
                        "name": "Jun-yong Noh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "generative factors of GANs, the research community focuses on discovering interpretable and controllable directions in the latent space of pre-trained generators [6]\u2013[9], [17], [24], [28].",
                "Moreover, GANSpace [17] performs Principal Components Analysis (PCA) on deep features at the early layers of the generator and finds directions in the latent space that best map to those deep PCA vectors, arriving at a set of non-orthogonal directions in the latent space.",
                "space and the underlying generative factors of GANs, the research community has recently directed its efforts towards discovering interpretable/disentangled directions in the latent space of pre-trained generators [6]\u2013[9], [17]; that is, latent directions travelling across which gives rise to generations where only a single (or a very few) generative factors are",
                "[17] or on certain pre-trained detectors [8].",
                "lability of such methods is very limited, introducing severe changes in the identity characteristics, and their evaluation relies either on laborious manual annotation [7], [17] or on certain pre-trained detectors [8]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "9c6011caf103dd6bfd3c55c642286474a3a18341",
                "externalIds": {
                    "ArXiv": "2209.13375",
                    "DBLP": "journals/corr/abs-2209-13375",
                    "DOI": "10.1109/FG57933.2023.10042744",
                    "CorpusId": 252544841
                },
                "corpusId": 252544841,
                "publicationVenue": {
                    "id": "b0c05768-6345-45d7-b541-235edf6ead54",
                    "name": "IEEE International Conference on Automatic Face & Gesture Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Autom Face Gesture Recognit",
                        "FG",
                        "IEEE International Conference on Automatic Face and Gesture Recognition",
                        "IEEE Int Conf Autom Face  Gesture Recognit",
                        "FGR",
                        "Form Gramm",
                        "Formal Grammar"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1029"
                },
                "url": "https://www.semanticscholar.org/paper/9c6011caf103dd6bfd3c55c642286474a3a18341",
                "title": "StyleMask: Disentangling the Style Space of StyleGAN2 for Neural Face Reenactment",
                "abstract": "In this paper we address the problem of neural face reenactment, where, given a pair of a source and a target facial image, we need to transfer the target's pose (defined as the head pose and its facial expressions) to the source image, by preserving at the same time the source's identity characteristics (e.g., facial shape, hair style, etc), even in the challenging case where the source and the target faces belong to different identities. In doing so, we address some of the limitations of the state-of-the-art works, namely, a) that they depend on paired training data (i.e., source and target faces have the same identity), b) that they rely on labeled data during inference, and c) that they do not preserve identity in large head pose changes. More specifically, we propose a framework that, using unpaired randomly generated facial images, learns to disentangle the identity characteristics of the face from its pose by incorporating the recently introduced style space S [1] of StyleGAN2 [2], a latent representation space that exhibits remarkable disentanglement properties. By capitalizing on this, we learn to successfully mix a pair of source and target style codes using supervision from a 3D model. The resulting latent code, that is subsequently used for reenactment, consists of latent units corresponding to the facial pose of the target only and of units corresponding to the identity of the source only, leading to notable improvement in the reenactment performance compared to recent state-of-the-art methods. In comparison to state of the art, we quantitatively and qualitatively show that the proposed method produces higher quality results even on extreme pose variations. Finally, we report results on real images by first embedding them on the latent space of the pretrained generator. We make the code and the pretrained models publicly available at: https://github.com/StelaBou/StyleMask.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2004556863",
                        "name": "Stella Bounareli"
                    },
                    {
                        "authorId": "1694090",
                        "name": "Christos Tzelepis"
                    },
                    {
                        "authorId": "1689047",
                        "name": "V. Argyriou"
                    },
                    {
                        "authorId": "50058816",
                        "name": "I. Patras"
                    },
                    {
                        "authorId": "2137359565",
                        "name": "Georgios Tzimiropoulos"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other methods use latent space exploration thanks to backpropagation or principal component analysis [25, 26] and allow precise control of the generation based on the study of the GAN representation."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1c52cef367f9e6a0e0676ffb883cddf8b50bfb1b",
                "externalIds": {
                    "DBLP": "conf/bhi/Plateau-Holleville22",
                    "DOI": "10.1109/BHI56158.2022.9926846",
                    "CorpusId": 253270969
                },
                "corpusId": 253270969,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1c52cef367f9e6a0e0676ffb883cddf8b50bfb1b",
                "title": "Class-aware data augmentation by GAN specialisation to improve endoscopic images classification",
                "abstract": "An expert eye is often needed to correctly identify mucosal lesions within endoscopic images. Hence, computer-aided diagnosis systems could decrease the need for highly specialized senior endoscopists and the effect of medical desertification. Moreover, they can significantly impact the latest endoscopic classification challenges such as the Inflammatory Bowel Disease (IBD) gradation. Most of the existing methods are based on deep learning algorithms. However, it is well known that these techniques suffer from the lack of data and/or class imbalance which can be lowered by using augmentation strategies thanks to synthetic generations. Late GAN framework progress made available accurate and production-ready artificial image generation that can be harnessed to extend training sets. It requires, however, to deal with the unsupervised nature of those networks to produce class-aware artificial images. In this article, we present our work to extend two datasets through a class-aware GAN-based augmentation strategy with the help of the state-of-the-art framework StyleGAN2-ADA and fine-tuning. We especially focused our efforts on endoscopic and IBD datasets to improve the classification and gradation of these images.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2126517660",
                        "name": "Cyprien Plateau-Holleville"
                    },
                    {
                        "authorId": "2523684",
                        "name": "Y. Benezeth"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020) and analyzing(H\u00e4rk\u00f6nen et al., 2020) in synthetic latent space.",
                "Others find meaningful directions in an unsupervised (H\u00e4rk\u00f6nen et al., 2020; Shen & Zhou, 2021; Voynov & Babenko, 2020; Wang & Ponce, 2021) or self-supervised(Jahanian et al."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "2fdbff08442496e59f7c5189f113943e113c9bb8",
                "externalIds": {
                    "ArXiv": "2209.12746",
                    "DBLP": "journals/corr/abs-2209-12746",
                    "DOI": "10.48550/arXiv.2209.12746",
                    "CorpusId": 252531859
                },
                "corpusId": 252531859,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2fdbff08442496e59f7c5189f113943e113c9bb8",
                "title": "LSAP: Rethinking Inversion Fidelity, Perception and Editability in GAN Latent Space",
                "abstract": "As the methods evolve, inversion is mainly divided into two steps. The first step is Image Embedding, in which an encoder or optimization process embeds images to get the corresponding latent codes. Afterward, the second step aims to refine the inversion and editing results, which we named Result Refinement. Although the second step significantly improves fidelity, perception and editability are almost unchanged, deeply dependent on inverse latent codes attained in the first step. Therefore, a crucial problem is gaining the latent codes with better perception and editability while retaining the reconstruction fidelity. In this work, we first point out that these two characteristics are related to the degree of alignment (or disalignment) of the inverse codes with the synthetic distribution. Then, we propose Latent Space Alignment Inversion Paradigm (LSAP), which consists of evaluation metric and solution for this problem. Specifically, we introduce Normalized Style Space ($\\mathcal{S^N}$ space) and $\\mathcal{S^N}$ Cosine Distance (SNCD) to measure disalignment of inversion methods. Since our proposed SNCD is differentiable, it can be optimized in both encoder-based and optimization-based embedding methods to conduct a uniform solution. Extensive experiments in various domains demonstrate that SNCD effectively reflects perception and editability, and our alignment paradigm archives the state-of-the-art in both two steps. Code is available on https://github.com/caopulan/GANInverter/tree/main/configs/lsap.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2186113855",
                        "name": "Cao Pu"
                    },
                    {
                        "authorId": "39702333",
                        "name": "Lu Yang"
                    },
                    {
                        "authorId": "2185633904",
                        "name": "Dong-Rui Liu"
                    },
                    {
                        "authorId": "1844299386",
                        "name": "Zhiwei Liu"
                    },
                    {
                        "authorId": "2693875",
                        "name": "Wenguan Wang"
                    },
                    {
                        "authorId": "2186150892",
                        "name": "Shan Li"
                    },
                    {
                        "authorId": "34223028",
                        "name": "Q. Song"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Instead, GANspace (Ha\u0308rko\u0308nen\net al. 2020) performs PCA on the latent space and discover editable and controllable semantic directions directly."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "0f44890c6e4c6b6dac1396359143bac37657e811",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-12050",
                    "ArXiv": "2209.12050",
                    "DOI": "10.48550/arXiv.2209.12050",
                    "CorpusId": 252531206
                },
                "corpusId": 252531206,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0f44890c6e4c6b6dac1396359143bac37657e811",
                "title": "Controllable Face Manipulation and UV Map Generation by Self-supervised Learning",
                "abstract": "Although manipulating facial attributes by Generative Adversarial Networks (GANs) has been remarkably successful recently, there are still some challenges in explicit control of features such as pose, expression, lighting, etc. Recent methods achieve explicit control over 2D images by combining 2D generative model and 3DMM. However, due to the lack of realism and clarity in texture reconstruction by 3DMM, there is a domain gap between the synthetic image and the rendered image of 3DMM. Since rendered 3DMM images contain facial region only without the background, directly computing the loss between these two domains is not ideal and the resultant trained model will be biased. In this study, we propose to explicitly edit the latent space of the pretrained StyleGAN by controlling the parameters of the 3DMM. To address the domain gap problem, we propose a noval network called 'Map and edit' and a simple but effective attribute editing method to avoid direct loss computation between rendered and synthesized images. Furthermore, since our model can accurately generate multi-view face images while the identity remains unchanged. As a by-product, combined with visibility masks, our proposed model can also generate texture-rich and high-resolution UV facial textures. Our model relies on pretrained StyleGAN, and the proposed model is trained in a self-supervised manner without any manual annotations or datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2111164052",
                        "name": "Yuanming Li"
                    },
                    {
                        "authorId": "2016481042",
                        "name": "Jeong-gi Kwak"
                    },
                    {
                        "authorId": "2757702",
                        "name": "D. Han"
                    },
                    {
                        "authorId": "144878703",
                        "name": "Hanseok Ko"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Besides image editing [H\u00e4rk\u00f6nen et al. 2020; Jiang et al. 2021; Shen et al. 2020; Shen and Zhou 2021], StyleGAN has also gained attention in video editing."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "866e09b1b9fcca7371346fd225479485a2bae20b",
                "externalIds": {
                    "ArXiv": "2209.11224",
                    "DBLP": "journals/corr/abs-2209-11224",
                    "DOI": "10.48550/arXiv.2209.11224",
                    "CorpusId": 252438607
                },
                "corpusId": 252438607,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/866e09b1b9fcca7371346fd225479485a2bae20b",
                "title": "VToonify: Controllable High-Resolution Portrait Video Style Transfer",
                "abstract": "Generating high-quality artistic portrait videos is an important and desirable task in computer graphics and vision. Although a series of successful portrait image toonification models built upon the powerful StyleGAN have been proposed, these image-oriented methods have obvious limitations when applied to videos, such as the fixed frame size, the requirement of face alignment, missing non-facial details and temporal inconsistency. In this work, we investigate the challenging controllable high-resolution portrait video style transfer by introducing a novel VToonify framework. Specifically, VToonify leverages the mid- and high-resolution layers of StyleGAN to render high-quality artistic portraits based on the multi-scale content features extracted by an encoder to better preserve the frame details. The resulting fully convolutional architecture accepts non-aligned faces in videos of variable size as input, contributing to complete face regions with natural motions in the output. Our framework is compatible with existing StyleGAN-based image toonification models to extend them to video toonification, and inherits appealing features of these models for flexible style control on color and intensity. This work presents two instantiations of VToonify built upon Toonify and DualStyleGAN for collection-based and exemplar-based portrait video style transfer, respectively. Extensive experimental results demonstrate the effectiveness of our proposed VToonify framework over existing methods in generating high-quality and temporally-coherent artistic portrait videos with flexible style controls.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2159711748",
                        "name": "Shuai Yang"
                    },
                    {
                        "authorId": "94106850",
                        "name": "Liming Jiang"
                    },
                    {
                        "authorId": "2117940996",
                        "name": "Ziwei Liu"
                    },
                    {
                        "authorId": "1717179",
                        "name": "Chen Change Loy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, finding decoupled editing directions for different facial expressions is laborious and difficult because these expressions are found to couple with other attributes such as poses in the latent space of StyleGAN due to the bias of the training data [12], [31], [32]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6e5252b1c70c92b917e5c29dc0618c940a9d22fb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-08289",
                    "ArXiv": "2209.08289",
                    "DOI": "10.48550/arXiv.2209.08289",
                    "CorpusId": 252367831
                },
                "corpusId": 252367831,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6e5252b1c70c92b917e5c29dc0618c940a9d22fb",
                "title": "Continuously Controllable Facial Expression Editing in Talking Face Videos",
                "abstract": "Recently audio-driven talking face video generation has attracted considerable attention. However, very few researches address the issue of emotional editing of these talking face videos with continuously controllable expressions, which is a strong demand in the industry. The challenge is that speech-related expressions and emotion-related expressions are often highly coupled. Meanwhile, traditional image-to-image translation methods cannot work well in our application due to the coupling of expressions with other attributes such as poses, i.e., translating the expression of the character in each frame may simultaneously change the head pose due to the bias of the training data distribution. In this paper, we propose a high-quality facial expression editing method for talking face videos, allowing the user to control the target emotion in the edited video continuously. We present a new perspective for this task as a special case of motion information editing, where we use a 3DMM to capture major facial movements and an associated texture map modeled by a StyleGAN to capture appearance details. Both representations (3DMM and texture map) contain emotional information and can be continuously modified by neural networks and easily smoothed by averaging in coefficient/latent spaces, making our method simple yet effective. We also introduce a mouth shape preservation loss to control the trade-off between lip synchronization and the degree of exaggeration of the edited expression. Extensive experiments and a user study show that our method achieves state-of-the-art performance across various evaluation criteria.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2149519045",
                        "name": "Zhiyao Sun"
                    },
                    {
                        "authorId": "2114784556",
                        "name": "Yu-Hui Wen"
                    },
                    {
                        "authorId": "2162368971",
                        "name": "Tian Lv"
                    },
                    {
                        "authorId": "71732784",
                        "name": "Y. Sun"
                    },
                    {
                        "authorId": "2144371828",
                        "name": "Ziyang Zhang"
                    },
                    {
                        "authorId": "2119050152",
                        "name": "Yaoyuan Wang"
                    },
                    {
                        "authorId": "1679704",
                        "name": "Y. Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Previous works have studied these challenges separately, and typical methods include editing of \u201cstyle\u201d codes [22, 66, 30] and explicit conditions [41, 67]."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b96d595f32987150a4043e49bebe80b81ae278ee",
                "externalIds": {
                    "DBLP": "conf/nips/WuMST22",
                    "ArXiv": "2209.06970",
                    "DOI": "10.48550/arXiv.2209.06970",
                    "CorpusId": 252280431
                },
                "corpusId": 252280431,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b96d595f32987150a4043e49bebe80b81ae278ee",
                "title": "Generative Visual Prompt: Unifying Distributional Control of Pre-Trained Generative Models",
                "abstract": "Generative models (e.g., GANs, diffusion models) learn the underlying data distribution in an unsupervised manner. However, many applications of interest require sampling from a particular region of the output space or sampling evenly over a range of characteristics. For efficient sampling in these scenarios, we propose Generative Visual Prompt (PromptGen), a framework for distributional control over pre-trained generative models by incorporating knowledge of other off-the-shelf models. PromptGen defines control as energy-based models (EBMs) and samples images in a feed-forward manner by approximating the EBM with invertible neural networks, avoiding optimization at inference. Our experiments demonstrate how PromptGen can efficiently sample from several unconditional generative models (e.g., StyleGAN2, StyleNeRF, diffusion autoencoder, NVAE) in a controlled or/and de-biased manner using various off-the-shelf models: (1) with the CLIP model as control, PromptGen can sample images guided by text, (2) with image classifiers as control, PromptGen can de-bias generative models across a set of attributes or attribute combinations, and (3) with inverse graphics models as control, PromptGen can sample images of the same identity in different poses. (4) Finally, PromptGen reveals that the CLIP model shows a\"reporting bias\"when used as control, and PromptGen can further de-bias this controlled distribution in an iterative manner. The code is available at https://github.com/ChenWu98/Generative-Visual-Prompt.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "114621402",
                        "name": "Chen Henry Wu"
                    },
                    {
                        "authorId": "1387976878",
                        "name": "Saman Motamed"
                    },
                    {
                        "authorId": "2183356017",
                        "name": "Shaunak Srivastava"
                    },
                    {
                        "authorId": "143867160",
                        "name": "F. D. L. Torre"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Generative Adversarial Networks (GANs) [9] have brought revolutionary changes for image processing field, ranging from image synthesis [6, 29], image editing [10, 18], and even some downstream applications like classification and regression [27].",
                "Unsupervised methods [10, 22, 24] find meaningful latent directions which make interpretable and distinguishable changes to the image.",
                "In recent years, with the expressive power of StyleGAN, many researches utilize StyleGAN latent spaces for semantic image manipulation [2, 10, 21, 22, 24]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "7d29fbf7626e0ec285abde97ba9df674aa5be9e5",
                "externalIds": {
                    "DBLP": "conf/cbmi/QianYY22",
                    "DOI": "10.1145/3549555.3549556",
                    "CorpusId": 252755497
                },
                "corpusId": 252755497,
                "publicationVenue": {
                    "id": "2985a421-bf55-4da9-985c-c7e6294354fa",
                    "name": "International Conference on Content-Based Multimedia Indexing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Content-based Multimedia Index",
                        "Content-based Multimedia Index",
                        "CBMI",
                        "Content-Based Multimedia Indexing"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=376"
                },
                "url": "https://www.semanticscholar.org/paper/7d29fbf7626e0ec285abde97ba9df674aa5be9e5",
                "title": "StyleGAN-based CLIP-guided Image Shape Manipulation",
                "abstract": "In this paper, we propose a text-guided image manipulation method which focuses on editing shape attribute using text description. We combine an image generation model, StyleGAN2, and image-text matching model, CLIP, and we have achieved the goal of image shape attribute manipulation by modifying the parameters of the pretrained StyleGAN2 generator. Qualitative and quantitative evaluations are conducted to demonstrate the effectiveness of the proposed method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2187177571",
                        "name": "Yuchen Qian"
                    },
                    {
                        "authorId": "2110502399",
                        "name": "Kohei Yamamoto"
                    },
                    {
                        "authorId": "1681659",
                        "name": "Keiji Yanai"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While aforementioned 2D GANs [15], [17], [22], [23] allow explicit head pose control to some extent, they fail to guarantee appearance consistency, leading to inconsistent identity or facial attributes when viewed from vastly different angles.",
                "correlates with the manipulated attribute [12], [13], [14], [15], [16], [16], [17], [18]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "4d0b528bc9f7e7e373dbba62bea8d9df43cffcca",
                "externalIds": {
                    "ArXiv": "2209.05434",
                    "CorpusId": 252917593
                },
                "corpusId": 252917593,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4d0b528bc9f7e7e373dbba62bea8d9df43cffcca",
                "title": "3DFaceShop: Explicitly Controllable 3D-Aware Portrait Generation",
                "abstract": "In contrast to the traditional avatar creation pipeline which is a costly process, contemporary generative approaches directly learn the data distribution from photographs. While plenty of works extend unconditional generative models and achieve some levels of controllability, it is still challenging to ensure multi-view consistency, especially in large poses. In this work, we propose a network that generates 3D-aware portraits while being controllable according to semantic parameters regarding pose, identity, expression and illumination. Our network uses neural scene representation to model 3D-aware portraits, whose generation is guided by a parametric face model that supports explicit control. While the latent disentanglement can be further enhanced by contrasting images with partially different attributes, there still exists noticeable inconsistency in non-face areas, e.g., hair and background, when animating expressions. Wesolve this by proposing a volume blending strategy in which we form a composite output by blending dynamic and static areas, with two parts segmented from the jointly learned semantic field. Our method outperforms prior arts in extensive experiments, producing realistic portraits with vivid expression in natural lighting when viewed from free viewpoints. It also demonstrates generalization ability to real images as well as out-of-domain data, showing great promise in real applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152949334",
                        "name": "Junshu Tang"
                    },
                    {
                        "authorId": "145803569",
                        "name": "Bo Zhang"
                    },
                    {
                        "authorId": "2157857267",
                        "name": "Binxin Yang"
                    },
                    {
                        "authorId": "2146320438",
                        "name": "Ting Zhang"
                    },
                    {
                        "authorId": "47514557",
                        "name": "Dong Chen"
                    },
                    {
                        "authorId": "2149343311",
                        "name": "Lizhuang Ma"
                    },
                    {
                        "authorId": "1716835",
                        "name": "Fang Wen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some works have studied the latent space of StyleGAN [1, 9, 22, 24] and discovered meaningful semantics for manipulating images."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "5b6425f5f6be0c43807f1ef0568657b380f4eb2c",
                "externalIds": {
                    "DBLP": "conf/mm/Alghamdi0BH22",
                    "ArXiv": "2209.04252",
                    "DOI": "10.1145/3503161.3548101",
                    "CorpusId": 252185153
                },
                "corpusId": 252185153,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5b6425f5f6be0c43807f1ef0568657b380f4eb2c",
                "title": "Talking Head from Speech Audio using a Pre-trained Image Generator",
                "abstract": "We propose a novel method for generating high-resolution videos of talking-heads from speech audio and a single 'identity' image. Our method is based on a convolutional neural network model that incorporates a pre-trained StyleGAN generator. We model each frame as a point in the latent space of StyleGAN so that a video corresponds to a trajectory through the latent space. Training the network is in two stages. The first stage is to model trajectories in the latent space conditioned on speech utterances. To do this, we use an existing encoder to invert the generator, mapping from each video frame into the latent space. We train a recurrent neural network to map from speech utterances to displacements in the latent space of the image generator. These displacements are relative to the back-projection into the latent space of an identity image chosen from the individuals depicted in the training dataset. In the second stage, we improve the visual quality of the generated videos by tuning the image generator on a single image or a short video of any chosen identity. We evaluate our model on standard measures (PSNR, SSIM, FID and LMD) and show that it significantly outperforms recent state-of-the-art methods on one of two commonly used datasets and gives comparable performance on the other. Finally, we report on ablation experiments that validate the components of the model. The code and videos from experiments can be found at https://mohammedalghamdi.github.io/talking-heads-acm-mm/",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2105491089",
                        "name": "M. M. Alghamdi"
                    },
                    {
                        "authorId": "153469612",
                        "name": "He Wang"
                    },
                    {
                        "authorId": "2285244",
                        "name": "A. Bulpitt"
                    },
                    {
                        "authorId": "1967104",
                        "name": "David C. Hogg"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Modern GANs [8,31,33] are capable of producing high quality images and are increasingly leveraged for image manipulation tasks [26,1]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "8133bca074ac5ed1158bbed4ab1d68cc86a79bd5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-02836",
                    "ArXiv": "2209.02836",
                    "DOI": "10.48550/arXiv.2209.02836",
                    "CorpusId": 252111156
                },
                "corpusId": 252111156,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/8133bca074ac5ed1158bbed4ab1d68cc86a79bd5",
                "title": "Studying Bias in GANs through the Lens of Race",
                "abstract": "In this work, we study how the performance and evaluation of generative image models are impacted by the racial composition of their training datasets. By examining and controlling the racial distributions in various training datasets, we are able to observe the impacts of different training distributions on generated image quality and the racial distributions of the generated images. Our results show that the racial compositions of generated images successfully preserve that of the training data. However, we observe that truncation, a technique used to generate higher quality images during inference, exacerbates racial imbalances in the data. Lastly, when examining the relationship between image quality and race, we find that the highest perceived visual quality images of a given race come from a distribution where that race is well-represented, and that annotators consistently prefer generated images of white people over those of Black people.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "90137288",
                        "name": "V. Maluleke"
                    },
                    {
                        "authorId": "91489410",
                        "name": "Neerja Thakkar"
                    },
                    {
                        "authorId": "2679394",
                        "name": "Tim Brooks"
                    },
                    {
                        "authorId": "143605244",
                        "name": "Ethan Weber"
                    },
                    {
                        "authorId": "1753210",
                        "name": "Trevor Darrell"
                    },
                    {
                        "authorId": "1763086",
                        "name": "Alexei A. Efros"
                    },
                    {
                        "authorId": "20615377",
                        "name": "Angjoo Kanazawa"
                    },
                    {
                        "authorId": "3493957",
                        "name": "Devin Guillory"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent works [21,29] further studied the interpolation directions by learning a linear binary classifier, and illustrated the effects of linearly interpolating different channels in a supervised or unsupervised manner [10,28].",
                "For example, is there a correspondence between interpolation directions in the latent space and visual factors in the generated images? If yes, how can we generate images in a controllable way? Furthermore, are different interpolation directions independent of each other? If not, is it possible to disentangle them in an interpretable fashion? To answer the above important but challenging questions, existing works have studied the learned latent space by identifying and linearly interpolating along directions that correspond to certain semantic factors in the generated images [10,18,21,24,28].",
                "Another line of works aim to identify semantic controls in a self-supervised or unsupervised manner [6,10,25].",
                "For example, to modify the color of a car, existing approaches need to learn all the pairwise connections among different colors, which is nontrivial [10,28].",
                "We compare our method with the related works [10,21,28] by measuring the accuracy and the level of attribute entanglement.",
                "Many attempts have been made to understand and visualize the latent representations of GANs [1,2,10,18,21,24,28,29].",
                "For example, GANSpace [10] identifies important latent directions by applying principal component analysis (PCA) to vectors in GAN latent space or feature space."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "aabfe8b7faf2d07bf2c1f2c0a0b77c265f10624a",
                "externalIds": {
                    "ArXiv": "2209.00698",
                    "DBLP": "journals/corr/abs-2209-00698",
                    "DOI": "10.48550/arXiv.2209.00698",
                    "CorpusId": 252070774
                },
                "corpusId": 252070774,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/aabfe8b7faf2d07bf2c1f2c0a0b77c265f10624a",
                "title": "Exploring Gradient-based Multi-directional Controls in GANs",
                "abstract": "Generative Adversarial Networks (GANs) have been widely applied in modeling diverse image distributions. However, despite its impressive applications, the structure of the latent space in GANs largely remains as a black-box, leaving its controllable generation an open problem, especially when spurious correlations between different semantic attributes exist in the image distributions. To address this problem, previous methods typically learn linear directions or individual channels that control semantic attributes in the image space. However, they often suffer from imperfect disentanglement, or are unable to obtain multi-directional controls. In this work, in light of the above challenges, we propose a novel approach that discovers nonlinear controls, which enables multi-directional manipulation as well as effective disentanglement, based on gradient information in the learned GAN latent space. More specifically, we first learn interpolation directions by following the gradients from classification networks trained separately on the attributes, and then navigate the latent space by exclusively controlling channels activated for the target attribute in the learned directions. Empirically, with small training data, our approach is able to gain fine-grained controls over a diverse set of bi-directional and multi-directional attributes, and we showcase its ability to achieve disentanglement significantly better than state-of-the-art methods both qualitatively and quantitatively.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2131592099",
                        "name": "Zikun Chen"
                    },
                    {
                        "authorId": "2052890223",
                        "name": "R. Jiang"
                    },
                    {
                        "authorId": "40807486",
                        "name": "Brendan Duke"
                    },
                    {
                        "authorId": "2146233072",
                        "name": "Han Zhao"
                    },
                    {
                        "authorId": "3241876",
                        "name": "P. Aarabi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Furthermore, these models have been shown to contain high-level semantics in their latent space mappings, allowing powerful post-hoc image editing operations, such as changing the appearance and expression of a generated person [1, 56, 22]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ea0d4d2707f502f430ed7edaf054f145564733a7",
                "externalIds": {
                    "ArXiv": "2211.14902",
                    "DBLP": "journals/corr/abs-2211-14902",
                    "DOI": "10.1109/3DV57658.2022.00046",
                    "CorpusId": 252384946
                },
                "corpusId": 252384946,
                "publicationVenue": {
                    "id": "4b02e809-1c26-4203-b9ba-311a418f664b",
                    "name": "International Conference on 3D Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf 3D Vis",
                        "3DV"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ea0d4d2707f502f430ed7edaf054f145564733a7",
                "title": "3inGAN: Learning a 3D Generative Model from Images of a Self-similar Scene",
                "abstract": "We introduce 3INGAN, an unconditional 3D generative model trained from 2D images of a single self-similar 3D scene. Such a model can be used to produce 3D \u201cremixes\u201d of a given scene, by mapping spatial latent codes into a 3D volumetric representation, which can subsequently be rendered from arbitrary views using physically based volume rendering. By construction, the generated scenes remain view-consistent across arbitrary camera configurations, without any flickering or spatio-temporal artifacts. During training, we employ a combination of 2D, obtained through differentiable volume tracing, and 3D Generative Adversarial Network (GAN) losses, across multiple scales, enforcing realism on both its 2D renderings and its 3D structure. We show results on semi-stochastic scenes of varying scale and complexity, obtained from real and synthetic sources. We demonstrate, for the first time, the feasibility of learning plausible view-consistent 3D scene variations from a single exemplar scene and provide qualitative and quantitative comparisons against two recent related methods. Code and data for the paper are available at https://geometry.cs.ucl.ac.uk/group_website/projects/2022/3inGAN/.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "19319579",
                        "name": "Animesh Karnewar"
                    },
                    {
                        "authorId": "39231399",
                        "name": "Oliver Wang"
                    },
                    {
                        "authorId": "1759347",
                        "name": "Tobias Ritschel"
                    },
                    {
                        "authorId": "1710455",
                        "name": "N. Mitra"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The former learns to discover interpretable directions in latent space by leveraging Principal Component Analysis (PCA) [H\u00e4rk\u00f6nen et al. 2020] (e.",
                "In order to compute the PCA of the style codes, GANSpace samples multiple random vectors (i.e., \ud835\udc67 space) and computes the corresponding style codes (i.e., W space).",
                "Regarding the unsupervised methods, most of them search the interpretable directions using PCA [H\u00e4rk\u00f6nen et al. 2020], or introducing orthogonalization [He et al. 2021; Voynov and Babenko 2020] in the latent space.",
                "The former learns to discover interpretable directions in latent space by leveraging Principal Component Analysis (PCA) [H\u00e4rk\u00f6nen et al. 2020] (e.g., using closed-form factorization [Shen and Zhou 2021]) by utilizing a learnable orthogonal matrix [He et al. 2021; Voynov and Babenko 2020] or by\u2026",
                "GANSpace [H\u00e4rk\u00f6nen et al. 2020] shows that PCA in the latent space of StyleGAN can find important interpretable directions that can be utilized to control image generation.",
                "Regarding the unsupervised methods, most of them search the interpretable directions using PCA [H\u00e4rk\u00f6nen et al. 2020], or introducing orthogonalization [He et al.",
                "The former learns to discover interpretable directions in latent space by leveraging Principal Component Analysis (PCA) [H\u00e4rk\u00f6nen et al. 2020] (e.g., using closed-form factorization [Shen and Zhou 2021]) by utilizing a learnable orthogonal matrix [He et al. 2021; Voynov and Babenko 2020] or by applying the regularization losses [Peebles et al. 2020; Wei et al. 2021]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "c21709f3f6671c6b417e7d72778d998a29a575f9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-12550",
                    "ArXiv": "2208.12550",
                    "DOI": "10.48550/arXiv.2208.12550",
                    "CorpusId": 251881341
                },
                "corpusId": 251881341,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c21709f3f6671c6b417e7d72778d998a29a575f9",
                "title": "Training and Tuning Generative Neural Radiance Fields for Attribute-Conditional 3D-Aware Face Generation",
                "abstract": "(GNeRF) have achieved impressive high-quality image generation, while preserving strong 3D consistency. The most notable achievements are made in the face gen- eration domain. However, most of these models focus on improving view consistency but neglect a disentanglement aspect, thus these models cannot provide high-quality semantic/attribute control over generation. To this end, we introduce a conditional GNeRF model uses specific (DAEM), generation. Moreover, we propose a TRIOT (TRaining as Init, and Optimizing for Tuning) method to optimize the latent vector to improve the precision further. Extensive experiments on the widely used show that our model yields high-quality editing with better view consis- tency while preserving the non-target regions. The code is available at",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50560752",
                        "name": "Jichao Zhang"
                    },
                    {
                        "authorId": "10753214",
                        "name": "Aliaksandr Siarohin"
                    },
                    {
                        "authorId": "1646872838",
                        "name": "Yahui Liu"
                    },
                    {
                        "authorId": "2109238637",
                        "name": "Hao Tang"
                    },
                    {
                        "authorId": "1429806753",
                        "name": "N. Sebe"
                    },
                    {
                        "authorId": "40397893",
                        "name": "Wei Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A partition of these methods leverages the naturally disentangled latent space of pre-trained GAN models to develop latent manipulations that allow for specific semantic operations [10,34]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6beff7221953ea14216968f1c5eff24c6d3940e1",
                "externalIds": {
                    "ArXiv": "2208.12632",
                    "DBLP": "journals/corr/abs-2208-12632",
                    "DOI": "10.48550/arXiv.2208.12632",
                    "CorpusId": 251881547
                },
                "corpusId": 251881547,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6beff7221953ea14216968f1c5eff24c6d3940e1",
                "title": "Selective manipulation of disentangled representations for privacy-aware facial image processing",
                "abstract": "Camera sensors are increasingly being combined with machine learning to perform various tasks such as intelligent surveillance. Due to its computational complexity, most of these machine learning algorithms are offloaded to the cloud for processing. However, users are increasingly concerned about privacy issues such as function creep and malicious usage by third-party cloud providers. To alleviate this, we propose an edge-based filtering stage that removes privacy-sensitive attributes before the sensor data are transmitted to the cloud. We use state-of-the-art image manipulation techniques that leverage disentangled representations to achieve privacy filtering. We define opt-in and opt-out filter operations and evaluate their effectiveness for filtering private attributes from face images. Additionally, we examine the effect of naturally occurring correlations and residual information on filtering. We find the results promising and believe this elicits further research on how image manipulation can be used for privacy preservation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2135590035",
                        "name": "Sander De Coninck"
                    },
                    {
                        "authorId": "2108449279",
                        "name": "Wei-Cheng Wang"
                    },
                    {
                        "authorId": "3162144",
                        "name": "Sam Leroux"
                    },
                    {
                        "authorId": "34209448",
                        "name": "P. Simoens"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For the unsupervised approach [CDH*16; VB20; H\u00e4r*20; SZ21; HKS21; YSEY21; ZFS*21], for example, GANSpace [H\u00e4r*20] discovered that moving a latent code toward principal directions in a latent space leads to interpretable control.",
                "GAN images by manipulating latent codes, latent space exploration techniques have been actively studied [VB20; SGTZ20; JCI20; H\u00e4r*20; YCW*21; SZ21; YSEY21; AZMW21]."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "373684280c7655487c1255584bad39ee87bbc366",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-12408",
                    "ArXiv": "2208.12408",
                    "DOI": "10.1111/cgf.14686",
                    "CorpusId": 251881740
                },
                "corpusId": 251881740,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/373684280c7655487c1255584bad39ee87bbc366",
                "title": "User\u2010Controllable Latent Transformer for StyleGAN Image Layout Editing",
                "abstract": "Latent space exploration is a technique that discovers interpretable latent directions and manipulates latent codes to edit various attributes in images generated by generative adversarial networks (GANs). However, in previous work, spatial control is limited to simple transformations (e.g., translation and rotation), and it is laborious to identify appropriate latent directions and adjust their parameters. In this paper, we tackle the problem of editing the StyleGAN image layout by annotating the image directly. To do so, we propose an interactive framework for manipulating latent codes in accordance with the user inputs. In our framework, the user annotates a StyleGAN image with locations they want to move or not and specifies a movement direction by mouse dragging. From these user inputs and initial latent codes, our latent transformer based on a transformer encoder\u2010decoder architecture estimates the output latent codes, which are fed to the StyleGAN generator to obtain a result image. To train our latent transformer, we utilize synthetic data and pseudo\u2010user inputs generated by off\u2010the\u2010shelf StyleGAN and optical flow models, without manual supervision. Quantitative and qualitative evaluations demonstrate the effectiveness of our method over existing methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2420042",
                        "name": "Yuki Endo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While latent space exploration has been attempted [17, 1, 41], it requires a lot of human labor to discover meaningful directions, and the editings could still be entangled.",
                "While latent space exploration [17, 43, 41] has proved to be effective, it requires extensive human labors to obtain meaningful control for generation."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c35a3bacc1b4bb842cd1be853e1e456e5bd1feb3",
                "externalIds": {
                    "DBLP": "conf/eccv/LiuSL00K22",
                    "ArXiv": "2208.11257",
                    "DOI": "10.48550/arXiv.2208.11257",
                    "CorpusId": 251765502
                },
                "corpusId": 251765502,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/c35a3bacc1b4bb842cd1be853e1e456e5bd1feb3",
                "title": "3D-FM GAN: Towards 3D-Controllable Face Manipulation",
                "abstract": "3D-controllable portrait synthesis has significantly advanced, thanks to breakthroughs in generative adversarial networks (GANs). However, it is still challenging to manipulate existing face images with precise 3D control. While concatenating GAN inversion and a 3D-aware, noise-to-image GAN is a straight-forward solution, it is inefficient and may lead to noticeable drop in editing quality. To fill this gap, we propose 3D-FM GAN, a novel conditional GAN framework designed specifically for 3D-controllable face manipulation, and does not require any tuning after the end-to-end learning phase. By carefully encoding both the input face image and a physically-based rendering of 3D edits into a StyleGAN's latent spaces, our image generator provides high-quality, identity-preserved, 3D-controllable face manipulation. To effectively learn such novel framework, we develop two essential training strategies and a novel multiplicative co-modulation architecture that improves significantly upon naive schemes. With extensive evaluations, we show that our method outperforms the prior arts on various tasks, with better editability, stronger identity preservation, and higher photo-realism. In addition, we demonstrate a better generalizability of our design on large pose editing and out-of-domain images.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Yuchen Liu"
                    },
                    {
                        "authorId": "2496409",
                        "name": "Zhixin Shu"
                    },
                    {
                        "authorId": "152998391",
                        "name": "Yijun Li"
                    },
                    {
                        "authorId": "2112754968",
                        "name": "Zhe Lin"
                    },
                    {
                        "authorId": "2844849",
                        "name": "Richard Zhang"
                    },
                    {
                        "authorId": "153574814",
                        "name": "S. Kung"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Exploiting such benefits, some recent works [30,11] focus on the linearity and the interpretability of the pretrained latent space of StyleGAN."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "069db11f886a19f062b2ee6c6148bf0397f9fc01",
                "externalIds": {
                    "ArXiv": "2208.10922",
                    "DBLP": "journals/corr/abs-2208-10922",
                    "DOI": "10.48550/arXiv.2208.10922",
                    "CorpusId": 251741190
                },
                "corpusId": 251741190,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/069db11f886a19f062b2ee6c6148bf0397f9fc01",
                "title": "StyleTalker: One-shot Style-based Audio-driven Talking Head Video Generation",
                "abstract": "We propose StyleTalker, a novel audio-driven talking head generation model that can synthesize a video of a talking person from a single reference image with accurately audio-synced lip shapes, realistic head poses, and eye blinks. Specifically, by leveraging a pretrained image generator and an image encoder, we estimate the latent codes of the talking head video that faithfully reflects the given audio. This is made possible with several newly devised components: 1) A contrastive lip-sync discriminator for accurate lip synchronization, 2) A conditional sequential variational autoencoder that learns the latent motion space disentangled from the lip movements, such that we can independently manipulate the motions and lip movements while preserving the identity. 3) An auto-regressive prior augmented with normalizing flow to learn a complex audio-to-motion multi-modal latent space. Equipped with these components, StyleTalker can generate talking head videos not only in a motion-controllable way when another motion source video is given but also in a completely audio-driven manner by inferring realistic motions from the input audio. Through extensive experiments and user studies, we show that our model is able to synthesize talking head videos with impressive perceptual quality which are accurately lip-synced with the input audios, largely outperforming state-of-the-art baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2072376267",
                        "name": "Dong Min"
                    },
                    {
                        "authorId": "2165751535",
                        "name": "Min-Hwan Song"
                    },
                    {
                        "authorId": "2110796623",
                        "name": "S. Hwang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "3d672f9a06ba30a4f489e5913810d291a1e63a7a",
                "externalIds": {
                    "DBLP": "conf/icpr/LinC22",
                    "DOI": "10.1109/ICPR56361.2022.9956222",
                    "CorpusId": 254101945
                },
                "corpusId": 254101945,
                "publicationVenue": {
                    "id": "48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                    "name": "International Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Pattern Recognit (ICPR Proc Int Conf",
                        "Int Conf Pattern Recognit",
                        "ICPR",
                        "International conference on pattern recognition",
                        "Int conf pattern recognit",
                        "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                    ],
                    "issn": "1041-3278",
                    "alternate_issns": [
                        "1051-4651"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=4740202"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3d672f9a06ba30a4f489e5913810d291a1e63a7a",
                "title": "Extended StyleGAN Encoder for Image Restoration",
                "abstract": "Great success has been achieved in reusing Style-GAN, but there are still challenges in reusing StyleGAN to image restoration tasks. We believe that the limited expression of StyleGAN\u2019s latent code hinders its effective use in image restoration tasks. Therefore, we propose to extend StyleGAN\u2019s latent code to improve its expressiveness. Specifically, we propose a new model called Extended StyleGAN Encoder (ESE). ESE is based on Feature Pyramid Network, which can encode input images both in coarse-grain and fine-grain, and generate H \u00d7 W \u00d7 2C feature maps instead of 2C-dimensional style vectors. In this way, ESE can provide more information of the input images to StyleGAN, which can achieve a more accurate reconstruction. Therefore, ESE can be applied to different image restoration tasks. Experiments show that ESE can surpass some models specifically designed for regular and irregular mask inpainting tasks, while it is difficult for other GAN Inversion methods. And experiments also show that ESE achieved good performance on colorization and denoising tasks. Besides, we further analyze the impact of the extended latent code by visualizing the outputs of ESE and the intermediate outputs of StyleGAN, which demonstrates that ESE does really reuse the knowledge of StyleGAN.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "35453735",
                        "name": "Kai-En Lin"
                    },
                    {
                        "authorId": "2404897",
                        "name": "Qingling Cai"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, some works [14,24,27,31] have been conducted to find these meaningful directions."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "03c5710f582c80cfd1ec0b08744bbbce1d42b79d",
                "externalIds": {
                    "DBLP": "conf/eccv/XuH0L22",
                    "ArXiv": "2208.08840",
                    "DOI": "10.48550/arXiv.2208.08840",
                    "CorpusId": 251643370
                },
                "corpusId": 251643370,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/03c5710f582c80cfd1ec0b08744bbbce1d42b79d",
                "title": "Mind the Gap in Distilling StyleGANs",
                "abstract": "StyleGAN family is one of the most popular Generative Adversarial Networks (GANs) for unconditional generation. Despite its impressive performance, its high demand on storage and computation impedes their deployment on resource-constrained devices. This paper provides a comprehensive study of distilling from the popular StyleGAN-like architecture. Our key insight is that the main challenge of StyleGAN distillation lies in the output discrepancy issue, where the teacher and student model yield different outputs given the same input latent code. Standard knowledge distillation losses typically fail under this heterogeneous distillation scenario. We conduct thorough analysis about the reasons and effects of this discrepancy issue, and identify that the mapping network plays a vital role in determining semantic information of generated images. Based on this finding, we propose a novel initialization strategy for the student model, which can ensure the output consistency to the maximum extent. To further enhance the semantic consistency between the teacher and student model, we present a latent-direction-based distillation loss that preserves the semantic relations in latent space. Extensive experiments demonstrate the effectiveness of our approach in distilling StyleGAN2 and StyleGAN3, outperforming existing GAN distillation methods by a large margin.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46538811",
                        "name": "Guodong Xu"
                    },
                    {
                        "authorId": "29976076",
                        "name": "Yuenan Hou"
                    },
                    {
                        "authorId": "2117940996",
                        "name": "Ziwei Liu"
                    },
                    {
                        "authorId": "1717179",
                        "name": "Chen Change Loy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Much like other GAN-inversion and latent space manipulation methods [3,4,13,28,29,39], accurate real-image editing with paint2pix is highly dependent on the ability of used encoder architecture to invert the original real image into StyleGAN [18] latent space."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "b54df0ceea1cd7232e261faa56e5b7ee1f2fa822",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-08092",
                    "ArXiv": "2208.08092",
                    "DOI": "10.48550/arXiv.2208.08092",
                    "CorpusId": 251622403
                },
                "corpusId": 251622403,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/b54df0ceea1cd7232e261faa56e5b7ee1f2fa822",
                "title": "Paint2Pix: Interactive Painting based Progressive Image Synthesis and Editing",
                "abstract": "Controllable image synthesis with user scribbles is a topic of keen interest in the computer vision community. In this paper, for the first time we study the problem of photorealistic image synthesis from incomplete and primitive human paintings. In particular, we propose a novel approach paint2pix, which learns to predict (and adapt)\"what a user wants to draw\"from rudimentary brushstroke inputs, by learning a mapping from the manifold of incomplete human paintings to their realistic renderings. When used in conjunction with recent works in autonomous painting agents, we show that paint2pix can be used for progressive image synthesis from scratch. During this process, paint2pix allows a novice user to progressively synthesize the desired image output, while requiring just few coarse user scribbles to accurately steer the trajectory of the synthesis process. Furthermore, we find that our approach also forms a surprisingly convenient approach for real image editing, and allows the user to perform a diverse range of custom fine-grained edits through the addition of only a few well-placed brushstrokes. Supplemental video and demo are available at https://1jsingh.github.io/paint2pix",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2112713578",
                        "name": "Jaskirat Singh"
                    },
                    {
                        "authorId": "2149972487",
                        "name": "Liang Zheng"
                    },
                    {
                        "authorId": "153551055",
                        "name": "Cameron Smith"
                    },
                    {
                        "authorId": "2586368",
                        "name": "J. Echevarria"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Whereas studies in [17, 63, 64] find directions in an unsupervised manner, requiring manual identification of the determined directions later and [55] using a closed-form factorization algorithm for identifying top semantic latent directions by directly decomposing the pre-trained weights.",
                "By leveraging the disentangling properties of the latent space as shown in [68, 54, 17, 8, 61, 51], extensive image manipulations could be performed."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6600265edfceb143dcdf882dfbb134a5c740071f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-08382",
                    "ArXiv": "2208.08382",
                    "DOI": "10.48550/arXiv.2208.08382",
                    "CorpusId": 251623210
                },
                "corpusId": 251623210,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6600265edfceb143dcdf882dfbb134a5c740071f",
                "title": "Deep Generative Views to Mitigate Gender Classification Bias Across Gender-Race Groups",
                "abstract": "Published studies have suggested the bias of automated face-based gender classification algorithms across gender-race groups. Specifically, unequal accuracy rates were obtained for women and dark-skinned people. To mitigate the bias of gender classifiers, the vision community has developed several strategies. However, the efficacy of these mitigation strategies is demonstrated for a limited number of races mostly, Caucasian and African-American. Further, these strategies often offer a trade-off between bias and classification accuracy. To further advance the state-of-the-art, we leverage the power of generative views, structured learning, and evidential learning towards mitigating gender classification bias. We demonstrate the superiority of our bias mitigation strategy in improving classification accuracy and reducing bias across gender-racial groups through extensive experimental validation, resulting in state-of-the-art performance in intra- and cross dataset evaluations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2130630728",
                        "name": "Sreeraj Ramachandran"
                    },
                    {
                        "authorId": "3285189",
                        "name": "A. Rattani"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, GANSpace [10] and InterfaceGAN [23] modify facial attributes via manipulation in the latent space of StyleGAN [15].",
                "With the understanding of the latent space in GANs, recent approaches based on latent space manipulation [10,23,26] have shown promising results in image editing."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "e582c8e1d98dd5bc8bb67068fa25f57a77bfda25",
                "externalIds": {
                    "ArXiv": "2208.07765",
                    "DBLP": "journals/corr/abs-2208-07765",
                    "DOI": "10.48550/arXiv.2208.07765",
                    "CorpusId": 251594561
                },
                "corpusId": 251594561,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/e582c8e1d98dd5bc8bb67068fa25f57a77bfda25",
                "title": "Style Your Hair: Latent Optimization for Pose-Invariant Hairstyle Transfer via Local-Style-Aware Hair Alignment",
                "abstract": "Editing hairstyle is unique and challenging due to the complexity and delicacy of hairstyle. Although recent approaches significantly improved the hair details, these models often produce undesirable outputs when a pose of a source image is considerably different from that of a target hair image, limiting their real-world applications. HairFIT, a pose-invariant hairstyle transfer model, alleviates this limitation yet still shows unsatisfactory quality in preserving delicate hair textures. To solve these limitations, we propose a high-performing pose-invariant hairstyle transfer model equipped with latent optimization and a newly presented local-style-matching loss. In the StyleGAN2 latent space, we first explore a pose-aligned latent code of a target hair with the detailed textures preserved based on local style matching. Then, our model inpaints the occlusions of the source considering the aligned target hair and blends both images to produce a final output. The experimental results demonstrate that our model has strengths in transferring a hairstyle under larger pose differences and preserving local hairstyle textures.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2111182754",
                        "name": "Taewoo Kim"
                    },
                    {
                        "authorId": "2049404784",
                        "name": "Chaeyeon Chung"
                    },
                    {
                        "authorId": "2117905094",
                        "name": "Yoonseong Kim"
                    },
                    {
                        "authorId": "2115278402",
                        "name": "S. Park"
                    },
                    {
                        "authorId": "1438418837",
                        "name": "Kangyeol Kim"
                    },
                    {
                        "authorId": "1795455",
                        "name": "J. Choo"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "4b687bf4664c76478a48a60f7d8e96d42427faa0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-03914",
                    "ArXiv": "2208.03914",
                    "DOI": "10.48550/arXiv.2208.03914",
                    "CorpusId": 251402676
                },
                "corpusId": 251402676,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4b687bf4664c76478a48a60f7d8e96d42427faa0",
                "title": "Interpretable Disentangled Parametrization of Measured BRDF with \u03b2-VAE",
                "abstract": "Finding a low dimensional parametric representation of measured BRDF remains challenging. Currently available solutions are either not interpretable, or rely on limited analytical solutions, or require expensive test subject based investigations. In this work, we strive to establish a parametrization space that affords the data-driven representation variance of measured BRDF models while still offering the artistic control of parametric analytical BRDFs. We present a machine learning approach that generates an interpretable disentangled parameter space. A disentangled representation is one in which each parameter is responsible for a unique generative factor and is insensitive to the ones encoded by the other parameters. To that end, we resort to a $\\beta$-Variational AutoEncoder ($\\beta$-VAE), a specific architecture of Deep Neural Network (DNN). After training our network, we analyze the parametrization space and interpret the learned generative factors utilizing our visual perception. It should be noted that perceptual analysis is called upon downstream of the system for interpretation purposes compared to most other existing methods where it is used upfront to elaborate the parametrization. In addition to that, we do not need a test subject investigation. A novel feature of our interpretable disentangled parametrization is the post-processing capability to incorporate new parameters along with the learned ones, thus expanding the richness of producible appearances. Furthermore, our solution allows more flexible and controllable material editing possibilities than manifold exploration. Finally, we provide a rendering interface, for real-time material editing and interpolation based on the presented new parametrization system.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2000488060",
                        "name": "A. Benamira"
                    },
                    {
                        "authorId": "2182112660",
                        "name": "Sachin Shah"
                    },
                    {
                        "authorId": "5206263",
                        "name": "S. Pattanaik"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Although, multiple works [1,23,44,47,56] are built upon this property to generate desired image transformations, there is no established metric to evaluate the extent of this linear correlation in the latent space.",
                "Several methods [23,47,48] propose ways to find attribute-specific directions in W+ latent space.",
                "Latent space of pretrained StyleGAN models is highly structured [47] and is popularly used to perform realistic image edits in the generated images [1, 4, 23, 47, 48, 56, 60].",
                "of popular attributes {gender, smile, age, hair, bangs, beard} [23, 47, 48]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "cdf746f6abea68f341dd704d672f90b81e17cfee",
                "externalIds": {
                    "ArXiv": "2208.03764",
                    "DBLP": "journals/corr/abs-2208-03764",
                    "DOI": "10.48550/arXiv.2208.03764",
                    "CorpusId": 251402920
                },
                "corpusId": 251402920,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/cdf746f6abea68f341dd704d672f90b81e17cfee",
                "title": "Hierarchical Semantic Regularization of Latent Spaces in StyleGANs",
                "abstract": "Progress in GANs has enabled the generation of high-resolution photorealistic images of astonishing quality. StyleGANs allow for compelling attribute modification on such images via mathematical operations on the latent style vectors in the W/W+ space that effectively modulate the rich hierarchical representations of the generator. Such operations have recently been generalized beyond mere attribute swapping in the original StyleGAN paper to include interpolations. In spite of many significant improvements in StyleGANs, they are still seen to generate unnatural images. The quality of the generated images is predicated on two assumptions; (a) The richness of the hierarchical representations learnt by the generator, and, (b) The linearity and smoothness of the style spaces. In this work, we propose a Hierarchical Semantic Regularizer (HSR) which aligns the hierarchical representations learnt by the generator to corresponding powerful features learnt by pretrained networks on large amounts of data. HSR is shown to not only improve generator representations but also the linearity and smoothness of the latent style spaces, leading to the generation of more natural-looking style-edited images. To demonstrate improved linearity, we propose a novel metric - Attribute Linearity Score (ALS). A significant reduction in the generation of unnatural images is corroborated by improvement in the Perceptual Path Length (PPL) metric by 16.19% averaged across different standard datasets while simultaneously improving the linearity of attribute-change in the attribute editing tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "115373370",
                        "name": "Tejan Karmali"
                    },
                    {
                        "authorId": "1669572692",
                        "name": "Rishubh Parihar"
                    },
                    {
                        "authorId": "2008802066",
                        "name": "Susmit Agrawal"
                    },
                    {
                        "authorId": "46224589",
                        "name": "Harsh Rangwani"
                    },
                    {
                        "authorId": "2131639924",
                        "name": "Varun Jampani"
                    },
                    {
                        "authorId": "1491308857",
                        "name": "M. Singh"
                    },
                    {
                        "authorId": "144682140",
                        "name": "R. Venkatesh Babu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several unsupervised approaches opted to bypass that limitation [18, 51] albeit still partially entailing model training and data sampling; nevertheless, later, purely unsupervised approaches were proposed"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "4da40aa6c39cf00191a0d9e1e0420e97de6ea632",
                "externalIds": {
                    "ArXiv": "2208.02089",
                    "DBLP": "conf/setn/KostagiolasNP22",
                    "DOI": "10.1145/3549737.3549777",
                    "CorpusId": 251280043
                },
                "corpusId": 251280043,
                "publicationVenue": {
                    "id": "3a7352c9-9494-49e2-8b4a-ba997954a26f",
                    "name": "Hellenic Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Hell Conf Artif Intell",
                        "SETN"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4da40aa6c39cf00191a0d9e1e0420e97de6ea632",
                "title": "Unsupervised Discovery of Semantic Concepts in Satellite Imagery with Style-based Wavelet-driven Generative Models",
                "abstract": "In recent years, considerable advancements have been made in the area of Generative Adversarial Networks (GANs), particularly with the advent of style-based architectures that address many key shortcomings - both in terms of modeling capabilities and network interpretability. Despite these improvements, the adoption of such approaches in the domain of satellite imagery is not straightforward. Typical vision datasets used in generative tasks are well-aligned and annotated, and exhibit limited variability. In contrast, satellite imagery exhibits great spatial and spectral variability, wide presence of fine, high-frequency details, while the tedious nature of annotating satellite imagery leads to annotation scarcity - further motivating developments in unsupervised learning. In this light, we present the first pre-trained style- and wavelet-based GAN model1 that can readily synthesize a wide gamut of realistic satellite images in a variety of settings and conditions - while also preserving high-frequency information. Furthermore, we show that by analyzing the intermediate activations of our network, one can discover a multitude of interpretable semantic directions that facilitate the guided synthesis of satellite images in terms of high-level concepts (e.g., urbanization) without using any form of supervision. Via a set of qualitative and quantitative experiments we demonstrate the efficacy of our framework, in terms of suitability for downstream tasks (e.g., data augmentation), quality of synthetic imagery, as well as generalization capabilities to unseen datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "91073385",
                        "name": "Nikos Kostagiolas"
                    },
                    {
                        "authorId": "1752913",
                        "name": "M. Nicolaou"
                    },
                    {
                        "authorId": "1780393",
                        "name": "Yannis Panagakis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "PCAGAN[9] utilizes principal component analysis (PCA)[10] to determine the main semantic direction of controllable editing in the latent space of GAN, but the editing result depends on the number of sampling points in the latent space.",
                "2 shows the obvious entanglement between pose and coat color attribute in the generated image of cat when edited by PCAGAN method.",
                "The proposed method is compared with two state-of-the art unsupervised editing methods, SefaGAN and PCAGAN."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "25b442e25b614f2374d14afb6fbdb4992d8edd11",
                "externalIds": {
                    "DBLP": "conf/smartiot/ZhangYWC22",
                    "DOI": "10.1109/SmartIoT55134.2022.00038",
                    "CorpusId": 252165253
                },
                "corpusId": 252165253,
                "publicationVenue": {
                    "id": "96710c95-a12e-4ed5-902f-bd4503396533",
                    "name": "International Conferences on Smart Internet of Things",
                    "type": "conference",
                    "alternate_names": [
                        "SmartIoT",
                        "Int Conf Smart Internet Thing"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/25b442e25b614f2374d14afb6fbdb4992d8edd11",
                "title": "Unsupervised Generated Image Editing Method Based on Multi-Scale Hierarchical Disentanglement",
                "abstract": "In order to solve the problem of semantic entanglement in generated image latent space of the StyleGAN2 network, we propose an unsupervised generated image editing method based on a multi-scale hierarchical disentanglement network structure. In our method, we first combine the mapping layer with the style mapping layer of each resolution branch in the StyleGAN2 network, and utilize the weight matrix eigen decomposition method at each scale independently to achieve the first-level disentanglement of image attributes and obtain the semantic direction vector of the scale. Then, we use Schmidt orthogonal decomposition based on the adjacent scale eigen vector to achieve the second-level disentanglement of image attributes. The result show that, compared with other mainstream unsupervised image editing methods, our method can achieve precise image editing at multiple scales, and the measurement of disentanglement between each attribute has also reached the best.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49050283",
                        "name": "Jianlong Zhang"
                    },
                    {
                        "authorId": "2174315125",
                        "name": "Xincheng Yu"
                    },
                    {
                        "authorId": "2152592987",
                        "name": "Bin Wang"
                    },
                    {
                        "authorId": null,
                        "name": "Chen Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "4062482a724b67d4dc0a8784b7e21ec78e156d20",
                "externalIds": {
                    "DOI": "10.1109/ARACE56528.2022.00022",
                    "CorpusId": 253629971
                },
                "corpusId": 253629971,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4062482a724b67d4dc0a8784b7e21ec78e156d20",
                "title": "Image Generation Network Model based on Principal Component Analysis",
                "abstract": "In the field of Artificial Intelligence, a large and densely annotated dataset is required for training making it a time and resource-expensive task. In this paper, we propose an image generation network model that keeps the training examples at a minimal level. The proposed model gives additional feature maps to the input value (latent space) of the DCGAN model, which is an adversarial image generation model using a convolutional neural network. To solve the problem that the neural network model cannot generate clear images in case of lack of training data, one additional feature map was added to the input value of the generation model, latent space. The feature map was extracted from 2,000 images of the CelebA dataset consisting of human face images through principal component analysis. We used 3,838 Large-Age-Gap datasets and one feature image for training. Compare to the previous model which uses 200,000 images, the proposed model generates more natural facial images with only 3,829 examples and the error rate is significantly reduced than the previous model at the beginning of the model training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2141657343",
                        "name": "Gi Soon Cha"
                    },
                    {
                        "authorId": "2113758816",
                        "name": "Usman Asim"
                    },
                    {
                        "authorId": "2183600064",
                        "name": "Myungseo Song"
                    },
                    {
                        "authorId": "1626143598",
                        "name": "Asim Niaz"
                    },
                    {
                        "authorId": "34637674",
                        "name": "K. Choi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, Ganspace [7] used Principal Component Analysis (PCA) to identify important latent directions and create interpretable controls for image attributes, such as viewpoint, aging, lighting, and time of day.",
                "Image editing is a task that modifies a target attribution of a given image while preserving other details [28,7,15,21,20,33,26,22]."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "5aeb6d81c6983e6c45e043ef7e8aeb2543e8f2ac",
                "externalIds": {
                    "ArXiv": "2207.14811",
                    "DBLP": "conf/eccv/WangYL022",
                    "DOI": "10.48550/arXiv.2207.14811",
                    "CorpusId": 251196614
                },
                "corpusId": 251196614,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/5aeb6d81c6983e6c45e043ef7e8aeb2543e8f2ac",
                "title": "StyleLight: HDR Panorama Generation for Lighting Estimation and Editing",
                "abstract": "We present a new lighting estimation and editing framework to generate high-dynamic-range (HDR) indoor panorama lighting from a single limited field-of-view (LFOV) image captured by low-dynamic-range (LDR) cameras. Existing lighting estimation methods either directly regress lighting representation parameters or decompose this problem into LFOV-to-panorama and LDR-to-HDR lighting generation sub-tasks. However, due to the partial observation, the high-dynamic-range lighting, and the intrinsic ambiguity of a scene, lighting estimation remains a challenging task. To tackle this problem, we propose a coupled dual-StyleGAN panorama synthesis network (StyleLight) that integrates LDR and HDR panorama synthesis into a unified framework. The LDR and HDR panorama synthesis share a similar generator but have separate discriminators. During inference, given an LDR LFOV image, we propose a focal-masked GAN inversion method to find its latent code by the LDR panorama synthesis branch and then synthesize the HDR panorama by the HDR panorama synthesis branch. StyleLight takes LFOV-to-panorama and LDR-to-HDR lighting generation into a unified framework and thus greatly improves lighting estimation. Extensive experiments demonstrate that our framework achieves superior performance over state-of-the-art methods on indoor lighting estimation. Notably, StyleLight also enables intuitive lighting editing on indoor HDR panoramas, which is suitable for real-world applications. Code is available at https://style-light.github.io.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "32317186",
                        "name": "Guangcong Wang"
                    },
                    {
                        "authorId": "2118773030",
                        "name": "Yinuo Yang"
                    },
                    {
                        "authorId": "1717179",
                        "name": "Chen Change Loy"
                    },
                    {
                        "authorId": "2117940996",
                        "name": "Ziwei Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To unsupervisedly discover meaningful latent directions of a pretrained GAN model, GANSpace [19] and SeFa [10] use Principal Component Analysis (PCA) to analyse the latent space."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "161c7f7c41e73513ecb1a99bfc00a560a1b3c625",
                "externalIds": {
                    "ArXiv": "2207.14425",
                    "DBLP": "journals/corr/abs-2207-14425",
                    "DOI": "10.48550/arXiv.2207.14425",
                    "CorpusId": 251196845
                },
                "corpusId": 251196845,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/161c7f7c41e73513ecb1a99bfc00a560a1b3c625",
                "title": "3D Cartoon Face Generation with Controllable Expressions from a Single GAN Image",
                "abstract": "In this paper, we investigate an open research task of generating 3D cartoon face shapes from single 2D GAN generated human faces and without 3D supervision, where we can also manipulate the facial expressions of the 3D shapes. To this end, we discover the semantic meanings of StyleGAN latent space, such that we are able to produce face images of various expressions, poses, and lighting by controlling the latent codes. Specifically, we first finetune the pretrained StyleGAN face model on the cartoon datasets. By feeding the same latent codes to face and cartoon generation models, we aim to realize the translation from 2D human face images to cartoon styled avatars. We then discover semantic directions of the GAN latent space, in an attempt to change the facial expressions while preserving the original identity. As we do not have any 3D annotations for cartoon faces, we manipulate the latent codes to generate images with different poses and lighting, such that we can reconstruct the 3D cartoon face shapes. We validate the efficacy of our method on three cartoon datasets qualitatively and quantitatively.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2359832",
                        "name": "Hongya Wang"
                    },
                    {
                        "authorId": "2604251",
                        "name": "Guosheng Lin"
                    },
                    {
                        "authorId": "1741126",
                        "name": "S. Hoi"
                    },
                    {
                        "authorId": "2158509654",
                        "name": "Chun Miao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "And \u03c0 -GAN [Chan et al. 2021] also learns a multi-identity NeRF model using network layers that are modulated by a noise vector, in StyleGAN fashion [Karras et al. 2019].",
                "StyleGAN and its derivatives [Karras et al. 2021, 2019, 2020] are popular and powerful full-head models that generate synthetic images with a large variety of facial identities and photorealistic appearance.",
                "While it is possible to separate some semantic components of these models for edits [Abdal et al. 2020; H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020], the results are typically limited to nearly frontal portraits and lack precise consistency in 3D geometry and appearance when rendering from multiple\u2026",
                "A larger training dataset should also allow for semantic control to be \u201cdiscovered\u201d by traversing the latent space using facial attribute classifiers, as done for StyleGAN [H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020].",
                "We adopt hierarchical fitting similar to that used with StyleGAN [Abdal et al. 2019]: we first fit an id code to initialize idw and idc, which are then optimized further in their own subspaces.",
                "In fact, even powerful StyleGAN models that are trained on thousands of identities cannot represent identity features that are unique to an arbitrary person not seen during training.",
                "Data-driven photorealistic face modeling has been a topic of recent research, which has led to very powerful generative models like the StyleGAN variants [Karras et al. 2019]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "7e849cc7376f9460fc6792207c7e9e55c44bae99",
                "externalIds": {
                    "DBLP": "conf/siggraph/WangCZBG22",
                    "DOI": "10.1145/3528233.3530753",
                    "CorpusId": 250702251
                },
                "corpusId": 250702251,
                "publicationVenue": {
                    "id": "cf6b5e76-9274-46e6-a2dd-7c190ec2ec5f",
                    "name": "International Conference on Computer Graphics and Interactive Techniques",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Graph Interact Tech",
                        "SIGGRAPH"
                    ],
                    "url": "http://www.siggraph.org/"
                },
                "url": "https://www.semanticscholar.org/paper/7e849cc7376f9460fc6792207c7e9e55c44bae99",
                "title": "MoRF: Morphable Radiance Fields for Multiview Neural Head Modeling",
                "abstract": "Recent research work has developed powerful generative models (e.g., StyleGAN2) that can synthesize complete human head images with impressive photorealism, enabling applications such as photorealistically editing real photographs. While these models can be trained on large collections of unposed images, their lack of explicit 3D knowledge makes it difficult to achieve even basic control over 3D viewpoint without unintentionally altering identity. On the other hand, recent Neural Radiance Field (NeRF) methods have already achieved multiview-consistent, photorealistic renderings but they are so far limited to a single facial identity. In this paper, we propose a new Morphable Radiance Field (MoRF) method that extends a NeRF into a generative neural model that can realistically synthesize multiview-consistent images of complete human heads, with variable and controllable identity. MoRF allows for morphing between particular identities, synthesizing arbitrary new identities, or quickly generating a NeRF from few images of a new subject, all while providing realistic and consistent rendering under novel viewpoints. We train MoRF in a supervised fashion by leveraging a high-quality database of multiview portrait images of several people, captured in studio with polarization-based separation of diffuse and specular reflection. Here, we demonstrate how MoRF is a strong new step forwards towards generative NeRFs for 3D neural head modeling.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1390598513",
                        "name": "Daoye Wang"
                    },
                    {
                        "authorId": "2066173302",
                        "name": "P. Chandran"
                    },
                    {
                        "authorId": "51152514",
                        "name": "G. Zoss"
                    },
                    {
                        "authorId": "143929823",
                        "name": "D. Bradley"
                    },
                    {
                        "authorId": "2741258",
                        "name": "P. Gotardo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Others have proposed ways to identify such semantic directions in an entirely unsupervised manner [H\u00e4rk\u00f6nen et al. 2020; Shen and Zhou 2020] or in a zero-shot manner by leveraging models [Radford et al. 2021] that jointly encode image and text [Patashnik et al. 2021].",
                "The unprecedented ability of StyleGAN to encode semantic properties within its latent space has spawned an impressive array of image manipulation methods [H\u00e4rk\u00f6nen et al. 2020; Patashnik et al. 2021; Shen et al. 2020; Shen and Zhou 2020;Wu et al. 2021a].",
                "2021] or even in an unsupervised fashion [H\u00e4rk\u00f6nen et al. 2020; Shen and Zhou 2020].",
                "Such directions can be found with weak supervision [Shen et al. 2020], in a zero-shot manner [Patashnik et al. 2021] or even in an unsupervised fashion [H\u00e4rk\u00f6nen et al. 2020; Shen and Zhou 2020].",
                "Others have proposed ways to identify such semantic directions in an entirely unsupervised manner [H\u00e4rk\u00f6nen et al. 2020; Shen and Zhou 2020] or in a zero-shot manner by leveraging models [Radford et al."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "b46c2d9718876b183918b4bed507c32caf0a076f",
                "externalIds": {
                    "DBLP": "conf/siggraph/0002GBCC22",
                    "DOI": "10.1145/3528233.3530698",
                    "CorpusId": 250702213
                },
                "corpusId": 250702213,
                "publicationVenue": {
                    "id": "cf6b5e76-9274-46e6-a2dd-7c190ec2ec5f",
                    "name": "International Conference on Computer Graphics and Interactive Techniques",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Graph Interact Tech",
                        "SIGGRAPH"
                    ],
                    "url": "http://www.siggraph.org/"
                },
                "url": "https://www.semanticscholar.org/paper/b46c2d9718876b183918b4bed507c32caf0a076f",
                "title": "Self-Conditioned GANs for Image Editing",
                "abstract": "Generative Adversarial Networks (GANs) are susceptible to bias, learned from either the unbalanced data, or through mode collapse. The networks focus on the core of the data distribution, leaving the tails \u2014 or the edges of the distribution \u2014 behind. We argue that this bias is responsible not only for fairness concerns, but that it plays a key role in the collapse of latent-traversal editing methods when deviating away from the distribution\u2019s core. Building on this observation, we outline a method for mitigating generative bias through a self-conditioning process, where distances in the latent-space of a pre-trained generator are used to provide initial labels for the data. By fine-tuning the generator on a re-sampled distribution drawn from these self-labeled data, we force the generator to better contend with rare semantic attributes and enable more realistic generation of these properties. We compare our models to a wide range of latent editing methods, and show that by alleviating the bias they achieve finer semantic control and better identity preservation through a wider range of transformations. Our code and models will be available at https://github.com/yzliu567/sc-gan",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2117964384",
                        "name": "Yunzhe Liu"
                    },
                    {
                        "authorId": "134639223",
                        "name": "Rinon Gal"
                    },
                    {
                        "authorId": "2177429675",
                        "name": "Amit H. Bermano"
                    },
                    {
                        "authorId": "2028246830",
                        "name": "Baoquan Chen"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Using ideas introduced in [H\u00e4rk\u00f6nen et al. 2020] and [Shoshan et al. 2021], the hidden space is disentangled by using a 2 layer feed forward neural network with ReLU activations.",
                "Using ideas introduced in [H\u00e4rk\u00f6nen et al. 2020] and [Shoshan et al."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4b0b2c65779e0df284d5bf97d4a556fe502d0f68",
                "externalIds": {
                    "DOI": "10.1145/3532836.3536241",
                    "CorpusId": 250992553
                },
                "corpusId": 250992553,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4b0b2c65779e0df284d5bf97d4a556fe502d0f68",
                "title": "Artistically Directable Walk Generation",
                "abstract": "We present a framework for artistically directable walk generation. A generative network is trained using a motion capture dataset and a manually animated collection of walks. To accommodate an animator\u2019s workflow, each walk is presented as a sequence of key poses. The generative framework allows to specify a set of traits including gender, stride, velocity and weight. A generated walk is designed to be the starting point when blocking an animation: an animator can introduce new keys on the controls.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2178850060",
                        "name": "Vladimir Ivanov"
                    },
                    {
                        "authorId": "2257932",
                        "name": "Parag Havaldar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Generative adversarial networks(GANs) have achieved promising results in various computer vision tasks including image [27\u201329] or video generation [53, 59, 60, 65], translation [7,20,31,34,73], manipulation [3,15,22,30,36,50], and cross-domain translation [18, 35] for the past several years."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "a39a30a6a2893e6cb3299063b2fad014e120ac8d",
                "externalIds": {
                    "ArXiv": "2207.13320",
                    "DBLP": "journals/corr/abs-2207-13320",
                    "DOI": "10.48550/arXiv.2207.13320",
                    "CorpusId": 262427873
                },
                "corpusId": 262427873,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/a39a30a6a2893e6cb3299063b2fad014e120ac8d",
                "title": "Generator Knows What Discriminator Should Learn in Unconditional GANs",
                "abstract": "Recent methods for conditional image generation benefit from dense supervision such as segmentation label maps to achieve high-fidelity. However, it is rarely explored to employ dense supervision for unconditional image generation. Here we explore the efficacy of dense supervision in unconditional generation and find generator feature maps can be an alternative of cost-expensive semantic label maps. From our empirical evidences, we propose a new generator-guided discriminator regularization(GGDR) in which the generator feature maps supervise the discriminator to have rich semantic representations in unconditional generation. In specific, we employ an U-Net architecture for discriminator, which is trained to predict the generator feature maps given fake images as inputs. Extensive experiments on mulitple datasets show that our GGDR consistently improves the performance of baseline methods in terms of quantitative and qualitative aspects. Code is available at https://github.com/naver-ai/GGDR",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47022712",
                        "name": "Gayoung Lee"
                    },
                    {
                        "authorId": "2118020280",
                        "name": "Hyunsung Kim"
                    },
                    {
                        "authorId": "1738212",
                        "name": "Jae Hyun Kim"
                    },
                    {
                        "authorId": "2245802575",
                        "name": "Seonghyeon Kim"
                    },
                    {
                        "authorId": "2577039",
                        "name": "Jung-Woo Ha"
                    },
                    {
                        "authorId": "30187096",
                        "name": "Yunjey Choi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Unlike the previous methods [55,19,56] that allows implicit pose control, we make StyleGAN enable explicit control over pose.",
                "Of course, the discovery using SURF-GAN is one of many applicable approaches and we can also utilize the existing semantic analysis methods [55,19,56] because our model is flexibly compatible with well-studied StyleGAN-based techniques.",
                "Recent works [55,19,3,45,56,67,48] have demonstrated semantic manipulation, especially for facial attributes, by analyzing the manifold and finding meaningful direction or mapping."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1a47926f35211ba5aeeaff0e5d373d1f1e44f6d0",
                "externalIds": {
                    "DBLP": "conf/eccv/KwakLYKHK22",
                    "ArXiv": "2207.10257",
                    "DOI": "10.48550/arXiv.2207.10257",
                    "CorpusId": 250921216
                },
                "corpusId": 250921216,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/1a47926f35211ba5aeeaff0e5d373d1f1e44f6d0",
                "title": "Injecting 3D Perception of Controllable NeRF-GAN into StyleGAN for Editable Portrait Image Synthesis",
                "abstract": "Over the years, 2D GANs have achieved great successes in photorealistic portrait generation. However, they lack 3D understanding in the generation process, thus they suffer from multi-view inconsistency problem. To alleviate the issue, many 3D-aware GANs have been proposed and shown notable results, but 3D GANs struggle with editing semantic attributes. The controllability and interpretability of 3D GANs have not been much explored. In this work, we propose two solutions to overcome these weaknesses of 2D GANs and 3D-aware GANs. We first introduce a novel 3D-aware GAN, SURF-GAN, which is capable of discovering semantic attributes during training and controlling them in an unsupervised manner. After that, we inject the prior of SURF-GAN into StyleGAN to obtain a high-fidelity 3D-controllable generator. Unlike existing latent-based methods allowing implicit pose control, the proposed 3D-controllable StyleGAN enables explicit pose control over portrait generation. This distillation allows direct compatibility between 3D control and many StyleGAN-based techniques (e.g., inversion and stylization), and also brings an advantage in terms of computational resources. Our codes are available at https://github.com/jgkwak95/SURF-GAN.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2016481042",
                        "name": "Jeong-gi Kwak"
                    },
                    {
                        "authorId": "2111164052",
                        "name": "Yuanming Li"
                    },
                    {
                        "authorId": "2146861723",
                        "name": "Dongsik Yoon"
                    },
                    {
                        "authorId": "2145183568",
                        "name": "Donghyeon Kim"
                    },
                    {
                        "authorId": "2757702",
                        "name": "D. Han"
                    },
                    {
                        "authorId": "144878703",
                        "name": "Hanseok Ko"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "caf4dc36923e7447ec974ce647e3ff83fc7300e6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-10309",
                    "ArXiv": "2207.10309",
                    "DOI": "10.48550/arXiv.2207.10309",
                    "CorpusId": 263787709
                },
                "corpusId": 263787709,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/caf4dc36923e7447ec974ce647e3ff83fc7300e6",
                "title": "A Survey on Leveraging Pre-trained Generative Adversarial Networks for Image Editing and Restoration",
                "abstract": "Generative adversarial networks (GANs) have drawn enormous attention due to the simple yet effective training mechanism and superior image generation quality. With the ability to generate photo-realistic high-resolution (e.g., $1024\\times1024$) images, recent GAN models have greatly narrowed the gaps between the generated images and the real ones. Therefore, many recent works show emerging interest to take advantage of pre-trained GAN models by exploiting the well-disentangled latent space and the learned GAN priors. In this paper, we briefly review recent progress on leveraging pre-trained large-scale GAN models from three aspects, i.e., 1) the training of large-scale generative adversarial networks, 2) exploring and understanding the pre-trained GAN models, and 3) leveraging these models for subsequent tasks like image restoration and editing. More information about relevant methods and repositories can be found at https://github.com/csmliu/pretrained-GANs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Ming Liu"
                    },
                    {
                        "authorId": "2156252416",
                        "name": "Yuxiang Wei"
                    },
                    {
                        "authorId": "39637222",
                        "name": "Xiaohe Wu"
                    },
                    {
                        "authorId": "2243334363",
                        "name": "Wangmeng Zuo"
                    },
                    {
                        "authorId": "2256831865",
                        "name": "Lei Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In GANSpace [13], a PCA is performed on latent codes to obtain the directions of maximum variations followed bymanual filtering of directions.",
                "to estimate a linear direction of variation that controls any given attribute [13, 22, 27, 28] in a disentangled manner.",
                "We compare FLAME quantitatively and quantitatively with three recent face editing methods - InterFaceGAN [27], GANSpace [13] and StyleFlow [3].",
                "proaches: InterfaceGAN [27], GANSpace [13], StyleFlow [3]",
                "Multiple works [3, 7, 9, 13, 27, 31] perform fine-grained image editing by leveraging the rich structure present in the latent space of a pre-trainedGAN.",
                "This is not uncommon practice to alter only few layers for editing of any given attribute and all the state-of-the-art methods follow this approach [3, 13, 39].",
                "Our method performs at par with the best performing GANSpace [13] method on both CS and ED metrics.",
                "Some works [13, 22, 27, 28] estimate global linear edit directions to model the latent transformation.",
                "Methods such as InterFaceGAN [27] and GANSpace [13] demonstrate the existence of directions in latent space that controls the extent of attributes in the generated image.",
                "Prior works [3, 4, 13, 27, 30, 33, 35, 39] estimate linear or non-linear paths in the latent space, achieving realistic attribute editing in StyleGAN generated images."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "140557bb19854802847c23faad46805fd3b322dc",
                "externalIds": {
                    "DBLP": "conf/mm/PariharDKR22",
                    "ArXiv": "2207.09855",
                    "DOI": "10.1145/3503161.3547972",
                    "CorpusId": 250698807
                },
                "corpusId": 250698807,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/140557bb19854802847c23faad46805fd3b322dc",
                "title": "Everything is There in Latent Space: Attribute Editing and Attribute Style Manipulation by StyleGAN Latent Space Exploration",
                "abstract": "Unconstrained Image generation with high realism is now possible using recent Generative Adversarial Networks (GANs). However, it is quite challenging to generate images with a given set of attributes. Recent methods use style-based GAN models to perform image editing by leveraging the semantic hierarchy present in the layers of the generator. We present Few-shot Latent-based Attribute Manipulation and Editing (FLAME), a simple yet effective framework to perform highly controlled image editing by latent space manipulation. Specifically, we estimate linear directions in the latent space (of a pre-trained StyleGAN) that controls semantic attributes in the generated image. In contrast to previous methods that either rely on large-scale attribute labeled datasets or attribute classifiers, FLAME uses minimal supervision of a few curated image pairs to estimate disentangled edit directions. FLAME can perform both individual and sequential edits with high precision on a diverse set of images while preserving identity. Further, we propose a novel task of Attribute Style Manipulation to generate diverse styles for attributes such as eyeglass and hair. We first encode a set of synthetic images of the same identity but having different attribute styles in the latent space to estimate an attribute style manifold. Sampling a new latent from this manifold will result in a new attribute style in the generated image. We propose a novel sampling method to sample latent from the manifold, enabling us to generate a diverse set of attribute styles beyond the styles present in the training set. FLAME can generate diverse attribute styles in a disentangled manner. We illustrate the superior performance of FLAME against previous image editing methods by extensive qualitative and quantitative comparisons. FLAME generalizes well on out-of-distribution images from art domain as well as on other datasets such as cars and churches.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1669572692",
                        "name": "Rishubh Parihar"
                    },
                    {
                        "authorId": "81144558",
                        "name": "Ankit Dhiman"
                    },
                    {
                        "authorId": "115373370",
                        "name": "Tejan Karmali"
                    },
                    {
                        "authorId": "144682140",
                        "name": "R. Venkatesh Babu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The 2D GAN manifolds appear to learn 3D geometrical properties implicitly, where recent GAN interpretation methods [14, 51] have shown that manipulating the latent code of the pre-trained GAN models can produce images of the same object under different viewpoints.",
                "Another line of works, such as InterFaceGAN [50], SeFa [51], GANSpace [14], discover the latent semantic directions of a pre-trained GAN model that can manipulate object rotation unaware of its underlying 3D model.",
                "Although GAN interpretation methods have shown that manipulating the latent code of StyleGAN produces multi-view images of the same object [14, 51], no studies have quantitatively evaluated the unreliable/inconsistent object shape",
                "Such latent codes have been shown to learn various disentangled semantics [14,51]."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "762d9b045fd2e6fe521720de57d8fc441fff7746",
                "externalIds": {
                    "ArXiv": "2207.10183",
                    "DBLP": "journals/corr/abs-2207-10183",
                    "DOI": "10.48550/arXiv.2207.10183",
                    "CorpusId": 250920692
                },
                "corpusId": 250920692,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/762d9b045fd2e6fe521720de57d8fc441fff7746",
                "title": "2D GANs Meet Unsupervised Single-view 3D Reconstruction",
                "abstract": "Recent research has shown that controllable image generation based on pre-trained GANs can benefit a wide range of computer vision tasks. However, less attention has been devoted to 3D vision tasks. In light of this, we propose a novel image-conditioned neural implicit field, which can leverage 2D supervisions from GAN-generated multi-view images and perform the single-view reconstruction of generic objects. Firstly, a novel offline StyleGAN-based generator is presented to generate plausible pseudo images with full control over the viewpoint. Then, we propose to utilize a neural implicit function, along with a differentiable renderer to learn 3D geometry from pseudo images with object masks and rough pose initializations. To further detect the unreliable supervisions, we introduce a novel uncertainty module to predict uncertainty maps, which remedy the negative effect of uncertain regions in pseudo images, leading to a better reconstruction performance. The effectiveness of our approach is demonstrated through superior single-view 3D reconstruction results of generic objects.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152943473",
                        "name": "Feng Liu"
                    },
                    {
                        "authorId": "2111119747",
                        "name": "Xiaoming Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The widespread use of GANs as opposed to other generative models in the interpretability is done due to the availability of an disentangled latent space [18, 49], which is a property we utilize in our work."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "3c73369b35b92de8d1a7a87433f19ab3addb4973",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-10074",
                    "ArXiv": "2207.10074",
                    "DOI": "10.48550/arXiv.2207.10074",
                    "CorpusId": 250699201
                },
                "corpusId": 250699201,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3c73369b35b92de8d1a7a87433f19ab3addb4973",
                "title": "Semantic uncertainty intervals for disentangled latent spaces",
                "abstract": "Meaningful uncertainty quantification in computer vision requires reasoning about semantic information -- say, the hair color of the person in a photo or the location of a car on the street. To this end, recent breakthroughs in generative modeling allow us to represent semantic information in disentangled latent spaces, but providing uncertainties on the semantic latent variables has remained challenging. In this work, we provide principled uncertainty intervals that are guaranteed to contain the true semantic factors for any underlying generative model. The method does the following: (1) it uses quantile regression to output a heuristic uncertainty interval for each element in the latent space (2) calibrates these uncertainties such that they contain the true value of the latent for a new, unseen input. The endpoints of these calibrated intervals can then be propagated through the generator to produce interpretable uncertainty visualizations for each semantic factor. This technique reliably communicates semantically meaningful, principled, and instance-adaptive uncertainty in inverse problems like image super-resolution and image completion.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2716670",
                        "name": "S. Sankaranarayanan"
                    },
                    {
                        "authorId": "153331364",
                        "name": "Anastasios Nikolas Angelopoulos"
                    },
                    {
                        "authorId": "153079946",
                        "name": "Stephen Bates"
                    },
                    {
                        "authorId": "3295351",
                        "name": "Yaniv Romano"
                    },
                    {
                        "authorId": "2094770",
                        "name": "Phillip Isola"
                    }
                ]
            }
        },
        {
            "contexts": [
                "2 Discovery of Interpretable Directions in Latent Spaces Several unsupervised methods to find interpretable directions in GAN latent spaces have been proposed [8, 24, 26].",
                "Recently, several unsupervised methods for discovering interpretable directions in GAN latent spaces were proposed [8, 24, 26]."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "5d06fc7a9461eaa4e62e465f64ad79e34d10db57",
                "externalIds": {
                    "DBLP": "conf/miccai/SchonSP22",
                    "ArXiv": "2207.09740",
                    "DOI": "10.48550/arXiv.2207.09740",
                    "CorpusId": 250699057
                },
                "corpusId": 250699057,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5d06fc7a9461eaa4e62e465f64ad79e34d10db57",
                "title": "Interpreting Latent Spaces of Generative Models for Medical Images using Unsupervised Methods",
                "abstract": "Generative models such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) play an increasingly important role in medical image analysis. The latent spaces of these models often show semantically meaningful directions corresponding to human-interpretable image transformations. However, until now, their exploration for medical images has been limited due to the requirement of supervised data. Several methods for unsupervised discovery of interpretable directions in GAN latent spaces have shown interesting results on natural images. This work explores the potential of applying these techniques on medical images by training a GAN and a VAE on thoracic CT scans and using an unsupervised method to discover interpretable directions in the resulting latent space. We find several directions corresponding to non-trivial image transformations, such as rotation or breast size. Furthermore, the directions show that the generative models capture 3D structure despite being presented only with 2D data. The results show that unsupervised methods to discover interpretable directions in GANs generalize to VAEs and can be applied to medical images. This opens a wide array of future work using these methods in medical image analysis.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2177340072",
                        "name": "Julian Schon"
                    },
                    {
                        "authorId": "36481129",
                        "name": "Raghavendra Selvan"
                    },
                    {
                        "authorId": "152800798",
                        "name": "Jens Petersen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The FFHQ dataset was first employed to evaluate StyleGAN, which performs unsupervised separation of high-level attributes of face images [283].",
                "CelebA contains 10,177 number of identities, 202,599 number of face images, and 5 landmark locations along with 40 binary attributes annotations per image.",
                "[156] leverages a PCA to decompose the latent space learned by models to factorize controllable latent directions.",
                "Borrowing the FFHQ dataset, we evaluate the human face editing task for image synthesis by collecting and summarizing results from Abdal et al. [213] in Table 6.",
                "Datasets that have been employed for image editing cover a wide range of types of image, including digit image such as MNIST [6, 96, 113, 122, 130] and The Street View House Numbers (SVHN) [96, 147], and human character, such as CelebA [6, 121, 121, 132, 133, 156], Flickr-Faces-HQ (FFHQ) [120, 156], Helen [132], Labeled Faces in theWild (LFW) [113] and Sprites [130], shape images such as dSprites [153, 154], 3D shapes [154] and ShapeNet [104], fashion image such as DeepFashion [170], animals such as Caltech-UCSD Birds 200 [113] and general scenes such as ImageNet [122, 133, 147, 155] and LSUN [133, 147].",
                "CelebFaces Attributes Dataset (CelebA) is a large-scale face dataset with more than 200,000 celebrity images [282]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "cb782fe598a77baedc29e49896bfa0d561820791",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-09542",
                    "ArXiv": "2207.09542",
                    "DOI": "10.48550/arXiv.2207.09542",
                    "CorpusId": 250699308
                },
                "corpusId": 250699308,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cb782fe598a77baedc29e49896bfa0d561820791",
                "title": "Controllable Data Generation by Deep Learning: A Review",
                "abstract": "Designing and generating new data under targeted properties has been attracting various critical applications such as molecule design, image editing and speech synthesis. Traditional hand-crafted approaches heavily rely on expertise experience and intensive human efforts, yet still suffer from the insufficiency of scientific knowledge and low throughput to support effective and efficient data generation. Recently, the advancement of deep learning induces expressive methods that can learn the underlying representation and properties of data. Such capability provides new opportunities in figuring out the mutual relationship between the structural patterns and functional properties of the data and leveraging such relationship to generate structural data given the desired properties. This article provides a systematic review of this promising research area, commonly known as controllable deep data generation. Firstly, the potential challenges are raised and preliminaries are provided. Then the controllable deep data generation is formally defined, a taxonomy on various techniques is proposed and the evaluation metrics in this specific domain are summarized. After that, exciting applications of controllable deep data generation are introduced and existing works are experimentally analyzed and compared. Finally, the promising future directions of controllable deep data generation are highlighted and five potential challenges are identified.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2130352205",
                        "name": "Shiyu Wang"
                    },
                    {
                        "authorId": "93584228",
                        "name": "Yuanqi Du"
                    },
                    {
                        "authorId": "46909769",
                        "name": "Xiaojie Guo"
                    },
                    {
                        "authorId": "2136223194",
                        "name": "Bo Pan"
                    },
                    {
                        "authorId": "144000223",
                        "name": "Liang Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[20] show that using principal component analysis can identify meaningful semantic directions in an unsupervised manner.",
                "One is StyleGAN\u2019s native latent space W [3, 20, 23, 43, 47], where the style code is a 512dimensional vector, and the other is an extended latent space W+ [1, 2, 41, 49, 62], where the style code consists of 18 different 512dimensional vectors.",
                "Numerous methods have been proposed to find semantically meaningful directions in the latent space of GANs, where semantic directions can be determined through fully-supervised approaches [3, 17, 43, 50, 57, 65], self-supervised approaches [23, 39, 46, 47], or unsupervised approaches [11, 20, 29, 44, 51, 52, 54]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "a0b8c91b270283d4dbcadf64a146e6c2ab27e845",
                "externalIds": {
                    "ArXiv": "2207.09367",
                    "DBLP": "journals/corr/abs-2207-09367",
                    "DOI": "10.1145/3503161.3548134",
                    "CorpusId": 250643988
                },
                "corpusId": 250643988,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a0b8c91b270283d4dbcadf64a146e6c2ab27e845",
                "title": "Cycle Encoding of a StyleGAN Encoder for Improved Reconstruction and Editability",
                "abstract": "GAN inversion aims to invert an input image into the latent space of a pre-trained GAN. Despite the recent advances in GAN inversion, there remain challenges to mitigate the tradeoff between distortion and editability, i.e. reconstructing the input image accurately and editing the inverted image with a small visual quality drop. The recently proposed pivotal tuning model makes significant progress towards reconstruction and editability, by using a two-step approach that first inverts the input image into a latent code, called pivot code, and then alters the generator so that the input image can be accurately mapped into the pivot code. Here, we show that both reconstruction and editability can be improved by a proper design of the pivot code. We present a simple yet effective method, named cycle encoding, for a high-quality pivot code. The key idea of our method is to progressively train an encoder in varying spaces according to a cycle scheme: W->W+->W. This training methodology preserves the properties of both W and W+ spaces, i.e. high editability of W and low distortion of W+. To further decrease the distortion, we also propose to refine the pivot code with an optimization-based method, where a regularization term is introduced to reduce the degradation in editability. Qualitative and quantitative comparisons to several state-of-the-art methods demonstrate the superiority of our approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "34443348",
                        "name": "Xudong Mao"
                    },
                    {
                        "authorId": "40626740",
                        "name": "Liujuan Cao"
                    },
                    {
                        "authorId": "2088846172",
                        "name": "A. T. Gnanha"
                    },
                    {
                        "authorId": "48598633",
                        "name": "Zhenguo Yang"
                    },
                    {
                        "authorId": "2117895391",
                        "name": "Qing Li"
                    },
                    {
                        "authorId": "1572139630",
                        "name": "Rongrong Ji"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[11] perform PCA on the sampled data to find primary directions in the latent space."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "e62a410771071cf99ca95ce419ad5f0cb1b6a069",
                "externalIds": {
                    "DBLP": "conf/icmcs/WangWZ22",
                    "DOI": "10.1109/ICME52920.2022.9859692",
                    "CorpusId": 251847128
                },
                "corpusId": 251847128,
                "publicationVenue": {
                    "id": "975e1529-a14b-436f-ad7e-a09d8eabc50d",
                    "name": "IEEE International Conference on Multimedia and Expo",
                    "type": "conference",
                    "alternate_names": [
                        "ICME",
                        "Int Conf Multimedia Expo",
                        "IEEE Int Conf Multimedia Expo",
                        "International Conference on Multimedia and Expo"
                    ],
                    "issn": "1945-7871",
                    "alternate_issns": [
                        "2330-7927"
                    ],
                    "url": "http://csdl2.computer.org/persagen/ProceedingByFilter.jsp?filter=I",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conferences.jsp",
                        "http://www.wikicfp.com/cfp/program?id=1418"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e62a410771071cf99ca95ce419ad5f0cb1b6a069",
                "title": "Generalizing Factorization of Gans by Characterizing Convolutional Layers",
                "abstract": "Existing unsupervised disentanglement methods in latent space of the Generative Adversarial Networks (GANs) rely on the analysis and decomposition of pre-trained weight matrix. However, they only consider the weight matrix of the fully connected layers, ignoring the convolutional layers which are indispensable for image processing in modern generative models. This results in the learned latent semantics lack inter-pretability, which is unacceptable for image editing tasks. In this paper, we propose a more generalized closed-form factor-ization of latent semantics in GANs, which takes the convolutionallayers into consideration when searching for the under-lying variation factors. Our method can be applied to a wide range of deep generators with just a few lines of code. Exten-sive experiments on multiple GAN models trained on various datasets show that our approach is capable of not only finding semantically meaningful dimensions, but also maintaining the consistency and interpretability of image content.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2125059091",
                        "name": "Yuehui Wang"
                    },
                    {
                        "authorId": "153115480",
                        "name": "Qing Wang"
                    },
                    {
                        "authorId": "46334785",
                        "name": "Dongyu Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANSpace [8] detects interpretable controls based on Principal Components Analysis (PCA)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a4c43ff04b1f15d9c034265ee3926bde44169a36",
                "externalIds": {
                    "DBLP": "conf/icmcs/XieXWLF22",
                    "DOI": "10.1109/ICME52920.2022.9859605",
                    "CorpusId": 251846849
                },
                "corpusId": 251846849,
                "publicationVenue": {
                    "id": "975e1529-a14b-436f-ad7e-a09d8eabc50d",
                    "name": "IEEE International Conference on Multimedia and Expo",
                    "type": "conference",
                    "alternate_names": [
                        "ICME",
                        "Int Conf Multimedia Expo",
                        "IEEE Int Conf Multimedia Expo",
                        "International Conference on Multimedia and Expo"
                    ],
                    "issn": "1945-7871",
                    "alternate_issns": [
                        "2330-7927"
                    ],
                    "url": "http://csdl2.computer.org/persagen/ProceedingByFilter.jsp?filter=I",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conferences.jsp",
                        "http://www.wikicfp.com/cfp/program?id=1418"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a4c43ff04b1f15d9c034265ee3926bde44169a36",
                "title": "Spatial Attention Guided Local Facial Attribute Editing",
                "abstract": "Facial attribute manipulation has attracted great attention from the public due to its wide range of applications. Aiming to smoothly manipulate the attributes of real facial images, it is critical to search for a proper latent code that aligns with the domain of pre-trained GAN for faithful inversion and controls the transformation within the scope of the attribute for precise editing. Previous methods mainly focused on improving the quality of reconstruction but often ignored the editing effect. To address this issue, we first propose a mapping network to manipulate latent code which is effective for diverse situations, and design a spatial attention network to predict binary mask of the certain attribute which encourages to only alter the relevant region of images and suppress irrelevant changes. In addition, we introduce a novel latent space into the GAN inversion framework which achieves high reconstruction quality especially preserving identity features and retains the ability to edit face attributes. Our methods pave the way to semantically meaningful and disentangled manipulations on both generated images and real images. Ex-perimental results indicate a clear improvement over the cur-rent state-of-the-art methods in various metrics.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9448302",
                        "name": "Mingye Xie"
                    },
                    {
                        "authorId": "1500417721",
                        "name": "Suncheng Xiang"
                    },
                    {
                        "authorId": "2145757694",
                        "name": "Feng Wang"
                    },
                    {
                        "authorId": "145840791",
                        "name": "Ting Liu"
                    },
                    {
                        "authorId": "7727059",
                        "name": "Yuzhuo Fu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a6475e8d33eab672676aeae01e8f233e74540161",
                "externalIds": {
                    "DBLP": "conf/uist/EvirgenC22",
                    "ArXiv": "2207.08320",
                    "DOI": "10.1145/3526113.3545638",
                    "CorpusId": 250627557
                },
                "corpusId": 250627557,
                "publicationVenue": {
                    "id": "c62b1316-0733-4b4c-8017-c07e18afa954",
                    "name": "ACM Symposium on User Interface Software and Technology",
                    "type": "conference",
                    "alternate_names": [
                        "User Interface Software and Technology",
                        "ACM Symp User Interface Softw Technol",
                        "User Interface Softw Technol",
                        "UIST"
                    ],
                    "url": "http://www.acm.org/uist/"
                },
                "url": "https://www.semanticscholar.org/paper/a6475e8d33eab672676aeae01e8f233e74540161",
                "title": "GANzilla: User-Driven Direction Discovery in Generative Adversarial Networks",
                "abstract": "Generative Adversarial Network (GAN) is widely adopted in numerous application areas, such as data preprocessing, image editing, and creativity support. However, GAN\u2019s \u2018black box\u2019 nature prevents non-expert users from controlling what data a model generates, spawning a plethora of prior work that focused on algorithm-driven approaches to extract editing directions to control GAN. Complementarily, we propose a GANzilla\u2014a user-driven tool that empowers a user with the classic scatter/gather technique to iteratively discover directions to meet their editing goals. In a study with 12 participants, GANzilla users were able to discover directions that (i) edited images to match provided examples (closed-ended tasks) and that (ii) met a high-level goal, e.g., making the face happier, while showing diversity across individuals (open-ended tasks).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "28136682",
                        "name": "Noyan Evirgen"
                    },
                    {
                        "authorId": "2028468",
                        "name": "Xiang 'Anthony' Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "6d5f806f351d3813bb4d7aa883deff1530e3d529",
                "externalIds": {
                    "ArXiv": "2207.07710",
                    "DBLP": "journals/corr/abs-2207-07710",
                    "DOI": "10.48550/arXiv.2207.07710",
                    "CorpusId": 250627219
                },
                "corpusId": 250627219,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6d5f806f351d3813bb4d7aa883deff1530e3d529",
                "title": "Outcome-Guided Counterfactuals for Reinforcement Learning Agents from a Jointly Trained Generative Latent Space",
                "abstract": "We present a novel generative method for producing unseen and plausible counterfactual examples for reinforcement learning (RL) agents based upon outcome variables that characterize agent behavior. Our approach uses a variational autoencoder to train a latent space that jointly encodes information about the observations and outcome variables pertaining to an agent's behavior. Counterfactuals are generated using traversals in this latent space, via gradient-driven updates as well as latent interpolations against cases drawn from a pool of examples. These include updates to raise the likelihood of generated examples, which improves the plausibility of generated counterfactuals. From experiments in three RL environments, we show that these methods produce counterfactuals that are more plausible and proximal to their queries compared to purely outcome-driven or case-based baselines. Finally, we show that a latent jointly trained to reconstruct both the input observations and behavioral outcome variables produces higher-quality counterfactuals over latents trained solely to reconstruct the observation inputs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40950555",
                        "name": "Eric Yeh"
                    },
                    {
                        "authorId": "1744526",
                        "name": "P. Sequeira"
                    },
                    {
                        "authorId": "49791678",
                        "name": "Jesse Hostetler"
                    },
                    {
                        "authorId": "2720925",
                        "name": "M. Gervasio"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In recent GANs studies [26, 39, 66, 92], the additional mapping network has proven to provide more disentangled semantics for the generator than directly using input codes."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "faa766713689847b8f68199e0458702a766bdf80",
                "externalIds": {
                    "ArXiv": "2207.06626",
                    "DBLP": "journals/access/LeeHH22",
                    "DOI": "10.1109/ACCESS.2022.3190089",
                    "CorpusId": 250508607
                },
                "corpusId": 250508607,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/faa766713689847b8f68199e0458702a766bdf80",
                "title": "Continuous Facial Motion Deblurring",
                "abstract": "We introduce a novel framework for continuous facial motion deblurring that restores the continuous sharp moment latent in a single motion-blurred face image via a moment control factor. Although a motion-blurred image is the accumulated signal of continuous sharp moments during the exposure time, most existing single image deblurring approaches aim to restore a fixed number of frames using multiple networks and training stages. To address this problem, we propose a continuous facial motion deblurring network based on GAN (CFMD-GAN), which is a novel framework for restoring the continuous moment latent in a single motion-blurred face image with a single network and a single training stage. To stabilize the network training, we train the generator to restore continuous moments in the order determined by our facial motion-based reordering process (FMR) utilizing domain-specific knowledge of the face. Moreover, we propose an auxiliary regressor that helps our generator produce more accurate images by estimating continuous sharp moments. Furthermore, we introduce a control-adaptive (ContAda) block that performs spatially deformable convolution and channel-wise attention as a function of the control factor. Extensive experiments on the 300VW datasets demonstrate that the proposed framework generates a various number of continuous output frames by varying the moment control factor. Compared with the recent single-to-single image deblurring networks trained with the same 300VW training set, the proposed method show the superior performance in restoring the central sharp frame in terms of perceptual metrics, including LPIPS, FID and Arcface identity distance. The proposed method outperforms the existing single-to-video deblurring method for both qualitative and quantitative comparisons. In our experiments on the 300VW test set, the proposed framework reached 33.14 dB and 0.93 for recovery of 7 sharp frames in PSNR and SSIM, respectively.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152476008",
                        "name": "Tae Bok Lee"
                    },
                    {
                        "authorId": "2152933243",
                        "name": "Sujy Han"
                    },
                    {
                        "authorId": "48418149",
                        "name": "Y. S. Heo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several studies explored disentanglement in the latent space of GANs in an unsupervised manner [25,23,9,28,32].",
                "In [9], the authors found that the principle components of features on pretrained GANs represent high-level semantic concepts."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ce2a874772eade6518a1ea18563da8d6f1350c2f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-06555",
                    "ArXiv": "2207.06555",
                    "DOI": "10.48550/arXiv.2207.06555",
                    "CorpusId": 250526660
                },
                "corpusId": 250526660,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/ce2a874772eade6518a1ea18563da8d6f1350c2f",
                "title": "Supervised Attribute Information Removal and Reconstruction for Image Manipulation",
                "abstract": "The goal of attribute manipulation is to control specified attribute(s) in given images. Prior work approaches this problem by learning disentangled representations for each attribute that enables it to manipulate the encoded source attributes to the target attributes. However, encoded attributes are often correlated with relevant image content. Thus, the source attribute information can often be hidden in the disentangled features, leading to unwanted image editing effects. In this paper, we propose an Attribute Information Removal and Reconstruction (AIRR) network that prevents such information hiding by learning how to remove the attribute information entirely, creating attribute excluded features, and then learns to directly inject the desired attributes in a reconstructed image. We evaluate our approach on four diverse datasets with a variety of attributes including DeepFashion Synthesis, DeepFashion Fine-grained Attribute, CelebA and CelebA-HQ, where our model improves attribute manipulation accuracy and top-k retrieval rate by 10% on average over prior work. A user study also reports that AIRR manipulated images are preferred over prior work in up to 76% of cases.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2149457259",
                        "name": "Nannan Li"
                    },
                    {
                        "authorId": "2856622",
                        "name": "Bryan A. Plummer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANSpace [16] adopts PCA to find facial semantic representation in the latent space of the GANmodel.",
                "Recently, learning facial semantics via manipulating latent code in the latent space has achieved great success in high-fidelity face image synthesis [16, 41, 43].",
                "[16] adopt PCA to find the principle face attribute representation in the latent space of GAN model."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "9edc32aeb60b2c38450bc1eb29da829d1bc5f72c",
                "externalIds": {
                    "ArXiv": "2207.05300",
                    "CorpusId": 250451057
                },
                "corpusId": 250451057,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9edc32aeb60b2c38450bc1eb29da829d1bc5f72c",
                "title": "SD-GAN: Semantic Decomposition for Face Image Synthesis with Discrete Attribute",
                "abstract": "Manipulating latent code in generative adversarial networks (GANs) for facial image synthesis mainly focuses on continuous attribute synthesis (e.g., age, pose and emotion), while discrete attribute synthesis (like face mask and eyeglasses) receives less attention. Directly applying existing works to facial discrete attributes may cause inaccurate results. In this work, we propose an innovative framework to tackle challenging facial discrete attribute synthesis via semantic decomposing, dubbed SD-GAN. To be concrete, we explicitly decompose the discrete attribute representation into two components, i.e. the semantic prior basis and offset latent representation. The semantic prior basis shows an initializing direction for manipulating face representation in the latent space. The offset latent presentation obtained by 3D-aware semantic fusion network is proposed to adjust prior basis. In addition, the fusion network integrates 3D embedding for better identity preservation and discrete attribute synthesis. The combination of prior basis and offset latent representation enable our method to synthesize photo-realistic face images with discrete attributes. Notably, we construct a large and valuable dataset MEGN (Face Mask and Eyeglasses images crawled from Google and Naver) for completing the lack of discrete attributes in the existing dataset. Extensive qualitative and quantitative experiments demonstrate the state-of-the-art performance of our method. Our code is available at: https://github.com/MontaEllis/SD-GAN.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1491232168",
                        "name": "Kangneng Zhou"
                    },
                    {
                        "authorId": "2159182559",
                        "name": "Xiaobin Zhu"
                    },
                    {
                        "authorId": "1380181436",
                        "name": "Daiheng Gao"
                    },
                    {
                        "authorId": "2175782177",
                        "name": "Lee Kai"
                    },
                    {
                        "authorId": "2108191762",
                        "name": "Xinjie Li"
                    },
                    {
                        "authorId": "2146267053",
                        "name": "Xu-Cheng Yin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For postprocessing, we applied PCA to 1,024 randomly-sampled latent codes to obtain bases W.",
                "InterFaceGAN [29] can control the pose and expression of faces by finding semantic boundaries via the training of a linear SVM. Shen et al. and Ha\u0308rko\u0308nen et al. found interpretable paths in a latent space through closed-form analysis [30] and principal components analysis (PCA) [12].",
                "Inspired by GANSpace [12], we solve this problem by restricting the latent code exploration to certain principal directions.",
                "found interpretable paths in a latent space through closed-form analysis [30] and principal components analysis (PCA) [12]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "edb8378bc2a87563a016c7ae08bd9bdd6d84f0f0",
                "externalIds": {
                    "DBLP": "journals/jvca/EndoK22",
                    "DOI": "10.1002/cav.2102",
                    "CorpusId": 250411905
                },
                "corpusId": 250411905,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/edb8378bc2a87563a016c7ae08bd9bdd6d84f0f0",
                "title": "Controlling StyleGANs using rough scribbles via one\u2010shot learning",
                "abstract": "This paper tackles the challenging problem of one\u2010shot semantic image synthesis from rough sparse annotations, which we call \u201csemantic scribbles.\u201d Namely, from only a single training pair annotated with semantic scribbles, we generate realistic and diverse images with layout control over, for example, facial part layouts and body poses. We present a training strategy that performs pseudo labeling for semantic scribbles using the StyleGAN prior. Our key idea is to construct a simple mapping between StyleGAN features and each semantic class from a single example of semantic scribbles. With such mappings, we can generate an unlimited number of pseudo semantic scribbles from random noise to train an encoder for controlling a pretrained StyleGAN generator. Even with our rough pseudo semantic scribbles obtained via one\u2010shot supervision, our method can synthesize high\u2010quality images thanks to our GAN inversion framework. We further offer optimization\u2010based postprocessing to refine the pixel alignment of synthesized images. Qualitative and quantitative results on various datasets demonstrate improvement over previous approaches in one\u2010shot settings.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2420042",
                        "name": "Yuki Endo"
                    },
                    {
                        "authorId": "2504432",
                        "name": "Yoshihiro Kanamori"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These works show that images can be\nmapped to the GANs latent space and edits can be achieved by manipulations in the latent space.",
                "Exploring these interpretable directions in latent codes has emerged as an important research endeavor on the fixed pretrained GANs [28,31,11,29,36].",
                "There has been a significant progress in image-to-image translation methods [15,26,39,8,23,41,22,25] especially for facial attribute editing [7,27,37,42,21] powered with generative adversarial networks (GANs).",
                "These directions are explored in supervised [28] and unsupervised ways [31,11,29,36].",
                "In another line of research, it is shown that GANs that are trained to synthesize faces can also be used for face attribute manipulations [17,5,18].",
                "However, it is shown that one can embed existing images into the GAN\u2019s embedding space [1] and further one can find latent directions to edit those images [28,31,11,29,36]."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "8a2a4dca4e52be9a7245525022d2373e53252636",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-03411",
                    "ArXiv": "2207.03411",
                    "DOI": "10.48550/arXiv.2207.03411",
                    "CorpusId": 250334473
                },
                "corpusId": 250334473,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/8a2a4dca4e52be9a7245525022d2373e53252636",
                "title": "VecGAN: Image-to-Image Translation with Interpretable Latent Directions",
                "abstract": "We propose VecGAN, an image-to-image translation framework for facial attribute editing with interpretable latent directions. Facial attribute editing task faces the challenges of precise attribute editing with controllable strength and preservation of the other attributes of an image. For this goal, we design the attribute editing by latent space factorization and for each attribute, we learn a linear direction that is orthogonal to the others. The other component is the controllable strength of the change, a scalar value. In our framework, this scalar can be either sampled or encoded from a reference image by projection. Our work is inspired by the latent space factorization works of fixed pretrained GANs. However, while those models cannot be trained end-to-end and struggle to edit encoded images precisely, VecGAN is end-to-end trained for image translation task and successful at editing an attribute while preserving the others. Our extensive experiments show that VecGAN achieves significant improvements over state-of-the-arts for both local and global edits.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2125378742",
                        "name": "Yusuf Dalva"
                    },
                    {
                        "authorId": "2125373866",
                        "name": "Said Fahri Altindis"
                    },
                    {
                        "authorId": "2130620",
                        "name": "A. Dundar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Realistic semantic image edits can be made by steering latents in optimized directions [7, 12, 19], or by finding and activating neurons that encode semantic concepts [2]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "dadbcb368a71fcb7f6c8f84953a7d59fd29eaf79",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-02774",
                    "ArXiv": "2207.02774",
                    "DOI": "10.48550/arXiv.2207.02774",
                    "CorpusId": 250311519
                },
                "corpusId": 250311519,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/dadbcb368a71fcb7f6c8f84953a7d59fd29eaf79",
                "title": "Local Relighting of Real Scenes",
                "abstract": "We introduce the task of local relighting, which changes a photograph of a scene by switching on and off the light sources that are visible within the image. This new task differs from the traditional image relighting problem, as it introduces the challenge of detecting light sources and inferring the pattern of light that emanates from them. We propose an approach for local relighting that trains a model without supervision of any novel image dataset by using synthetically generated image pairs from another model. Concretely, we collect paired training images from a stylespace-manipulated GAN; then we use these images to train a conditional image-to-image model. To benchmark local relighting, we introduce Lonoff, a collection of 306 precisely aligned images taken in indoor spaces with different combinations of lights switched on. We show that our method significantly outperforms baseline methods based on GAN inversion. Finally, we demonstrate extensions of our method that control different light sources separately. We invite the community to tackle this new task of local relighting.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2056068167",
                        "name": "Audrey Cui"
                    },
                    {
                        "authorId": "19203468",
                        "name": "Ali Jahanian"
                    },
                    {
                        "authorId": "2677488",
                        "name": "\u00c0gata Lapedriza"
                    },
                    {
                        "authorId": "143805211",
                        "name": "A. Torralba"
                    },
                    {
                        "authorId": "3362896",
                        "name": "Shahin Mahdizadehaghdam"
                    },
                    {
                        "authorId": "2154359344",
                        "name": "Rohit Kumar"
                    },
                    {
                        "authorId": "144159726",
                        "name": "David Bau"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "9c7ec94901efcbc22656cb0d9924d1716578bfb1",
                "externalIds": {
                    "DBLP": "conf/mm/YuZWZLCX0M22",
                    "ArXiv": "2207.02812",
                    "DOI": "10.1145/3503161.3547935",
                    "CorpusId": 250311554
                },
                "corpusId": 250311554,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9c7ec94901efcbc22656cb0d9924d1716578bfb1",
                "title": "Towards Counterfactual Image Manipulation via CLIP",
                "abstract": "Leveraging StyleGAN's expressivity and its disentangled latent codes, existing methods can achieve realistic editing of different visual attributes such as age and gender of facial images. An intriguing yet challenging problem arises: Can generative models achieve counterfactual editing against their learnt priors? Due to the lack of counterfactual samples in natural datasets, we investigate this problem in a text-driven manner with Contrastive-Language-Image-Pretraining (CLIP), which can offer rich semantic knowledge even for various counterfactual concepts. Different from in-domain manipulation, counterfactual manipulation requires more comprehensive exploitation of semantic knowledge encapsulated in CLIP as well as more delicate handling of editing directions for avoiding being stuck in local minimum or undesired editing. To this end, we design a novel contrastive loss that exploits predefined CLIP-space directions to guide the editing toward desired directions from different perspectives. In addition, we design a simple yet effective scheme that explicitly maps CLIP embeddings (of target text) to the latent space and fuses them with latent codes for effective latent code optimization and accurate editing. Extensive experiments show that our design achieves accurate and realistic editing while driving by target texts with various counterfactual concepts.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "101206696",
                        "name": "Yingchen Yu"
                    },
                    {
                        "authorId": "51111483",
                        "name": "Fangneng Zhan"
                    },
                    {
                        "authorId": "153088941",
                        "name": "Rongliang Wu"
                    },
                    {
                        "authorId": "2121386031",
                        "name": "Jiahui Zhang"
                    },
                    {
                        "authorId": "1771189",
                        "name": "Shijian Lu"
                    },
                    {
                        "authorId": "2055099005",
                        "name": "Miaomiao Cui"
                    },
                    {
                        "authorId": "65863521",
                        "name": "Xuansong Xie"
                    },
                    {
                        "authorId": "143863244",
                        "name": "Xiansheng Hua"
                    },
                    {
                        "authorId": "1679209",
                        "name": "C. Miao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2022 Relevance feedback:We implement relevance feedback by combining unsupervised interpretable controls based on principal component analysis (PCA) [6] with Thompson sampling, a Bayesian contextual bandit algorithm based on probability matching [3].",
                "Interpretable controls have been created using supervised [20] and unsupervised [6] learning, as well as methods where users draw directly on to an image [24], or combine features from multiple images to achieve the desired output [18]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "ed1260f3137ca364f825a32ff2b3fbdf3e7e90fa",
                "externalIds": {
                    "DBLP": "conf/sigir/LiuMG22",
                    "DOI": "10.1145/3477495.3531675",
                    "CorpusId": 250340395
                },
                "corpusId": 250340395,
                "publicationVenue": {
                    "id": "8dce23a9-44e0-4381-a39e-2acc1edff700",
                    "name": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                    "type": "conference",
                    "alternate_names": [
                        "International ACM SIGIR Conference on Research and Development in Information Retrieval",
                        "Int ACM SIGIR Conf Res Dev Inf Retr",
                        "SIGIR",
                        "Annu Int ACM SIGIR Conf Res Dev Inf Retr"
                    ],
                    "url": "http://www.acm.org/sigir/"
                },
                "url": "https://www.semanticscholar.org/paper/ed1260f3137ca364f825a32ff2b3fbdf3e7e90fa",
                "title": "ROGUE: A System for Exploratory Search of GANs",
                "abstract": "Image retrieval from generative adversarial networks (GANs) is challenging for several reasons. First, there are no clear mappings between the GAN's latent space and useful semantic features, making it difficult for users to navigate. Second, the number of unique images that can be generated is exceptionally high, taxing the scaling properties of existing search algorithms. In this article, we present ROGUE, a system to support exploratory search of images generated from GANs. We demonstrate how to implement features that are commonly found in exploratory search interfaces, such as faceted search and relevance feedback, in the context of GAN search. We additionally use reinforcement learning to help users navigate the image space [8], trading off exploration (showing diverse images) and exploitation (showing images predicted to receive positive relevance feedback). Finally, we present a usability study where participants were situated in the role of a casting director who needs to explore actors' headshots for an upcoming movie. The system obtained an average SUS score of 72.8 and all participants reported being either satisfied or very satisfied with the images they identified with the system. The system is shown in this accompanying video: https://vimeo.com/680036160.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1614039034",
                        "name": "Yang Liu"
                    },
                    {
                        "authorId": "3222337",
                        "name": "A. Medlar"
                    },
                    {
                        "authorId": "2090521",
                        "name": "D. Glowacka"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In particular, we obtain the controls from the pre-trained sourcemodels using the latent discovery method GANSpace [H\u00e4rk\u00f6nen et al. 2020].",
                "In particular, we obtain the controls from the pre-trained source models using the latent discovery method GANSpace [26].",
                ", deform a cat ear into a curly shape) compared to using latent directions [26].",
                "We can also apply GANSpace edits [26] to our models to change the object attributes such as poses or colors.",
                "Second, we observe that it is easier to introduce out-of-thedistribution geometric changes (e.g., deform a cat ear into a curly shape) compared to using latent directions [H\u00e4rk\u00f6nen et al. 2020]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a70e1480a7afd9b730efa3a5df6f8c2474e6e7e6",
                "externalIds": {
                    "ArXiv": "2207.14288",
                    "DBLP": "journals/tog/WangBZ22",
                    "DOI": "10.1145/3528223.3530065",
                    "CorpusId": 250956766
                },
                "corpusId": 250956766,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a70e1480a7afd9b730efa3a5df6f8c2474e6e7e6",
                "title": "Rewriting geometric rules of a GAN",
                "abstract": "Deep generative models make visual content creation more accessible to novice users by automating the synthesis of diverse, realistic content based on a collected dataset. However, the current machine learning approaches miss a key element of the creative process - the ability to synthesize things that go far beyond the data distribution and everyday experience. To begin to address this issue, we enable a user to \"warp\" a given model by editing just a handful of original model outputs with desired geometric changes. Our method applies a low-rank update to a single model layer to reconstruct edited examples. Furthermore, to combat overfitting, we propose a latent space augmentation method based on style-mixing. Our method allows a user to create a model that synthesizes endless objects with defined geometric changes, enabling the creation of a new generative model without the burden of curating a large-scale dataset. We also demonstrate that edited models can be composed to achieve aggregated effects, and we present an interactive interface to enable users to create new models through composition. Empirical measurements on multiple test cases suggest the advantage of our method against recent GAN fine-tuning methods. Finally, we showcase several applications using the edited models, including latent space interpolation and image editing.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "12782331",
                        "name": "Sheng-Yu Wang"
                    },
                    {
                        "authorId": "144159726",
                        "name": "David Bau"
                    },
                    {
                        "authorId": "1922024303",
                        "name": "Jun-Yan Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We use the GANSpace approach [H\u00e4rk\u00f6nen et al. 2020] to discover interpretable directions in the intermediate latent spacew .",
                "We use the GANSpace approach [H\u00e4rk\u00f6nen et al. 2020] to discover interpretable directions in the intermediate latent space\ud835\udc98 ."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "bd5ab9f2b0b5c74d1edacc97d5ffe8c9354068eb",
                "externalIds": {
                    "ArXiv": "2207.01413",
                    "DBLP": "journals/tog/HarkonenAKLAL22",
                    "DOI": "10.1145/3528223.3530170",
                    "CorpusId": 250265125
                },
                "corpusId": 250265125,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bd5ab9f2b0b5c74d1edacc97d5ffe8c9354068eb",
                "title": "Disentangling random and cyclic effects in time-lapse sequences",
                "abstract": "Time-lapse image sequences offer visually compelling insights into dynamic processes that are too slow to observe in real time. However, playing a long time-lapse sequence back as a video often results in distracting flicker due to random effects, such as weather, as well as cyclic effects, such as the day-night cycle. We introduce the problem of disentangling time-lapse sequences in a way that allows separate, after-the-fact control of overall trends, cyclic effects, and random effects in the images, and describe a technique based on data-driven generative models that achieves this goal. This enables us to \"re-render\" the sequences in ways that would not be possible with the input images alone. For example, we can stabilize a long sequence to focus on plant growth over many months, under selectable, consistent weather. Our approach is based on Generative Adversarial Networks (GAN) that are conditioned with the time coordinate of the time-lapse sequence. Our architecture and training procedure are designed so that the networks learn to model random variations, such as weather, using the GAN's latent space, and to disentangle overall trends and cyclic variations by feeding the conditioning time label to the model using Fourier features with specific frequencies. We show that our models are robust to defects in the training data, enabling us to amend some of the practical difficulties in capturing long time-lapse sequences, such as temporary occlusions, uneven frame spacing, and missing frames.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "103642338",
                        "name": "Erik H\u00e4rk\u00f6nen"
                    },
                    {
                        "authorId": "1907688",
                        "name": "M. Aittala"
                    },
                    {
                        "authorId": "100563136",
                        "name": "T. Kynk\u00e4\u00e4nniemi"
                    },
                    {
                        "authorId": "36436218",
                        "name": "S. Laine"
                    },
                    {
                        "authorId": "1761103",
                        "name": "Timo Aila"
                    },
                    {
                        "authorId": "49244945",
                        "name": "J. Lehtinen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "7a1cf1b0ea8ccbc961639a75b982d693d62c9bf4",
                "externalIds": {
                    "DBLP": "journals/tog/GuerreroHSMBM22",
                    "ArXiv": "2207.01044",
                    "DOI": "10.1145/3528223.3530173",
                    "CorpusId": 250264897
                },
                "corpusId": 250264897,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7a1cf1b0ea8ccbc961639a75b982d693d62c9bf4",
                "title": "MatFormer",
                "abstract": "Procedural material graphs are a compact, parameteric, and resolution-independent representation that are a popular choice for material authoring. However, designing procedural materials requires significant expertise and publicly accessible libraries contain only a few thousand such graphs. We present MatFormer, a generative model that can produce a diverse set of high-quality procedural materials with complex spatial patterns and appearance. While procedural materials can be modeled as directed (operation) graphs, they contain arbitrary numbers of heterogeneous nodes with unstructured, often long-range node connections, and functional constraints on node parameters and connections. MatFormer addresses these challenges with a multi-stage transformer-based model that sequentially generates nodes, node parameters, and edges, while ensuring the semantic validity of the graph. In addition to generation, MatFormer can be used for the auto-completion and exploration of partial material graphs. We qualitatively and quantitatively demonstrate that our method outperforms alternative approaches, in both generated graph and material quality.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145630914",
                        "name": "Paul Guerrero"
                    },
                    {
                        "authorId": "1824181511",
                        "name": "Milovs Havsan"
                    },
                    {
                        "authorId": "2454127",
                        "name": "Kalyan Sunkavalli"
                    },
                    {
                        "authorId": "1596823247",
                        "name": "Radom'ir Mvech"
                    },
                    {
                        "authorId": "1747280",
                        "name": "T. Boubekeur"
                    },
                    {
                        "authorId": "1710455",
                        "name": "N. Mitra"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "af098907f17ecbe732780d6c0045a2bec25512a3",
                "externalIds": {
                    "DBLP": "journals/tog/LiuCLLJFG22",
                    "DOI": "10.1145/3528223.3530056",
                    "CorpusId": 250980361
                },
                "corpusId": 250980361,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/af098907f17ecbe732780d6c0045a2bec25512a3",
                "title": "DeepFaceVideoEditing",
                "abstract": "Sketches, which are simple and concise, have been used in recent deep image synthesis methods to allow intuitive generation and editing of facial images. However, it is nontrivial to extend such methods to video editing due to various challenges, ranging from appropriate manipulation propagation and fusion of multiple editing operations to ensure temporal coherence and visual quality. To address these issues, we propose a novel sketch-based facial video editing framework, in which we represent editing manipulations in latent space and propose specific propagation and fusion modules to generate high-quality video editing results based on StyleGAN3. Specifically, we first design an optimization approach to represent sketch editing manipulations by editing vectors, which are propagated to the whole video sequence using a proper strategy to cope with different editing needs. Specifically, input editing operations are classified into two categories: temporally consistent editing and temporally variant editing. The former (e.g., change of face shape) is applied to the whole video sequence directly, while the latter (e.g., change of facial expression or dynamics) is propagated with the guidance of expression or only affects adjacent frames in a given time window. Since users often perform different editing operations in multiple frames, we further present a region-aware fusion approach to fuse diverse editing effects. Our method supports video editing on facial structure and expression movement by sketch, which cannot be achieved by previous works. Both qualitative and quantitative evaluations show the superior editing ability of our system to existing and alternative solutions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152943384",
                        "name": "Feng-Lin Liu"
                    },
                    {
                        "authorId": "2999411",
                        "name": "Shu-Yu Chen"
                    },
                    {
                        "authorId": "7827503",
                        "name": "Yu-Kun Lai"
                    },
                    {
                        "authorId": "2525637",
                        "name": "Chunpeng Li"
                    },
                    {
                        "authorId": "2116749037",
                        "name": "Yue-Ren Jiang"
                    },
                    {
                        "authorId": "3169698",
                        "name": "Hongbo Fu"
                    },
                    {
                        "authorId": "144614914",
                        "name": "Lin Gao"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020], discovering interpretability [H\u00e4rk\u00f6nen et al., 2020], and fine-tuning [Mo et al."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "dc5ab34f46fd70be93e9264401bdab9643cc76fe",
                "externalIds": {
                    "ArXiv": "2206.14389",
                    "DOI": "10.1109/SaTML54575.2023.00048",
                    "CorpusId": 254185323
                },
                "corpusId": 254185323,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/dc5ab34f46fd70be93e9264401bdab9643cc76fe",
                "title": "Data Redaction from Pre-trained GANs",
                "abstract": "Large pre-trained generative models are known to occasionally output undesirable samples, which undermines their trustworthiness. The common way to mitigate this is to re-train them differently from scratch using different data or different regularization - which uses a lot of computational resources and does not always fully address the problem. In this work, we take a different, more compute- friendly approach and investigate how to post-edit a model after training so that it \u201credacts\u201d, or refrains from outputting certain kinds of samples. We show that redaction is a fundamentally different task from data deletion, and data deletion may not always lead to redaction. We then consider Generative Adversar-ial Networks (GANs), and provide three different algorithms for data redaction that differ on how the samples to be redacted are described. Extensive evaluations on real-world image datasets show that our algorithms out-perform data deletion baselines, and are capable of redacting data while retaining high generation quality at a fraction of the cost of full re- training,",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "33470361",
                        "name": "Zhifeng Kong"
                    },
                    {
                        "authorId": "38120884",
                        "name": "Kamalika Chaudhuri"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "25318b3cd1370a8614eaac8e81fbbe6606f35b23",
                "externalIds": {
                    "DBLP": "conf/icip/ShukorYDH22",
                    "ArXiv": "2206.14892",
                    "DOI": "10.1109/ICIP46576.2022.9897791",
                    "CorpusId": 250144645
                },
                "corpusId": 250144645,
                "publicationVenue": {
                    "id": "b6369c33-5d70-463c-8e82-95a54efa3cc8",
                    "name": "International Conference on Information Photonics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Image Process",
                        "ICIP",
                        "Int Conf Inf Photonics",
                        "International Conference on Image Processing"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/25318b3cd1370a8614eaac8e81fbbe6606f35b23",
                "title": "Semantic Unfolding of Stylegan Latent Space",
                "abstract": "Generative adversarial networks (GANs) have proven to be surprisingly efficient for image editing by inverting and manipulating the latent code corresponding to an input real image. This editing property emerges from the disentangled nature of the latent space. In this paper, we identify that the facial attribute disentanglement is not optimal, thus facial editing relying on linear attribute separation is flawed. We thus propose to improve semantic disentanglement with supervision. Our method consists in learning a proxy latent representation using normalizing flows, and we show that this leads to a more efficient space for face image editing.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2067292515",
                        "name": "Mustafa Shukor"
                    },
                    {
                        "authorId": "2115586564",
                        "name": "Xu Yao"
                    },
                    {
                        "authorId": "8162004",
                        "name": "B. Damodaran"
                    },
                    {
                        "authorId": "1806880",
                        "name": "P. Hellier"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several works (Radford, Metz, and Chintala 2016; Chen et al. 2016; Ha\u0308rko\u0308nen et al. 2020; Shen and Zhou 2021; Voynov and Babenko 2020; Wu, Lischinski, and Shechtman 2021) focused on disentangling the attributes of the generated images.",
                "Several works (Radford, Metz, and Chintala 2016; Chen et al. 2016; H\u00e4rk\u00f6nen et al. 2020; Shen and Zhou 2021; Voynov and Babenko 2020; Wu, Lischinski, and Shechtman 2021) focused on disentangling the attributes of the generated images."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e16f5b41141355051c60b1ce0c4ba06ac96241dd",
                "externalIds": {
                    "DBLP": "conf/aaai/0028YWDXZ22",
                    "DOI": "10.1609/aaai.v36i2.20015",
                    "CorpusId": 250301226
                },
                "corpusId": 250301226,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/e16f5b41141355051c60b1ce0c4ba06ac96241dd",
                "title": "Interpretable Generative Adversarial Networks",
                "abstract": "Learning a disentangled representation is still a challenge in the field of the interpretability of generative adversarial networks (GANs). This paper proposes a generic method to modify a traditional GAN into an interpretable GAN, which ensures that filters in an intermediate layer of the generator encode disentangled localized visual concepts. Each filter in the layer is supposed to consistently generate image regions corresponding to the same visual concept when generating different images. The interpretable GAN learns to automatically discover meaningful visual concepts without any annotations of visual concepts. The interpretable GAN enables people to modify a specific visual concept on generated images by manipulating feature maps of the corresponding filters in the layer. Our method can be broadly applied to different types of GANs. Experiments have demonstrated the effectiveness of our method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2150357919",
                        "name": "Chao Li"
                    },
                    {
                        "authorId": "24915708",
                        "name": "Kelu Yao"
                    },
                    {
                        "authorId": "2143718823",
                        "name": "Jin Wang"
                    },
                    {
                        "authorId": "3070516",
                        "name": "Boyu Diao"
                    },
                    {
                        "authorId": "2146648122",
                        "name": "Yongjun Xu"
                    },
                    {
                        "authorId": "22063226",
                        "name": "Quanshi Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "On the other hand, some recent works [25, 41, 48] try to identify the manipulation directions in an unsupervised way by leveraging eigenvector decomposition."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "5cba046669b323d9baeff8d9e9d6bd0ec5191f54",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-13078",
                    "ArXiv": "2206.13078",
                    "DOI": "10.48550/arXiv.2206.13078",
                    "CorpusId": 250072983
                },
                "corpusId": 250072983,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5cba046669b323d9baeff8d9e9d6bd0ec5191f54",
                "title": "Video2StyleGAN: Encoding Video in Latent Space for Manipulation",
                "abstract": "Many recent works have been proposed for face image editing by leveraging the latent space of pretrained GANs. However, few attempts have been made to directly apply them to videos, because 1) they do not guarantee temporal consistency, 2) their application is limited by their processing speed on videos, and 3) they cannot accurately encode details of face motion and expression. To this end, we propose a novel network to encode face videos into the latent space of StyleGAN for semantic face video manipulation. Based on the vision transformer, our network reuses the high-resolution portion of the latent vector to enforce temporal consistency. To capture subtle face motions and expressions, we design novel losses that involve sparse facial landmarks and dense 3D face mesh. We have thoroughly evaluated our approach and successfully demonstrated its application to various face video manipulations. Particularly, we propose a novel network for pose/expression control in a 3D coordinate system. Both qualitative and quantitative results have shown that our approach can significantly outperform existing single image methods, while achieving real-time (66 fps) speed.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49402727",
                        "name": "Ji-yang Yu"
                    },
                    {
                        "authorId": "1800425",
                        "name": "Jingen Liu"
                    },
                    {
                        "authorId": "73284414",
                        "name": "Jing-ling Huang"
                    },
                    {
                        "authorId": "48902313",
                        "name": "Wei Zhang"
                    },
                    {
                        "authorId": "2070183421",
                        "name": "Tao Mei"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[33] found the latent direction of attributes in the latent space based on principal component analysis."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "543c4795d3ed5a9a380d3115d837d8c2056eba8e",
                "externalIds": {
                    "DBLP": "journals/sensors/YangLXLWZ22",
                    "PubMedCentral": "9268752",
                    "DOI": "10.3390/s22134697",
                    "CorpusId": 249967908,
                    "PubMed": "35808193"
                },
                "corpusId": 249967908,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/543c4795d3ed5a9a380d3115d837d8c2056eba8e",
                "title": "Enriching Facial Anti-Spoofing Datasets via an Effective Face Swapping Framework",
                "abstract": "In the era of rapid development of the Internet of things, deep learning, and communication technologies, social media has become an indispensable element. However, while enjoying the convenience brought by technological innovation, people are also facing the negative impact brought by them. Taking the users\u2019 portraits of multimedia systems as examples, with the maturity of deep facial forgery technologies, personal portraits are facing malicious tampering and forgery, which pose a potential threat to personal privacy security and social impact. At present, the deep forgery detection methods are learning-based methods, which depend on the data to a certain extent. Enriching facial anti-spoofing datasets is an effective method to solve the above problem. Therefore, we propose an effective face swapping framework based on StyleGAN. We utilize the feature pyramid network to extract facial features and map them to the latent space of StyleGAN. In order to realize the transformation of identity, we explore the representation of identity information and propose an adaptive identity editing module. We design a simple and effective post-processing process to improve the authenticity of the images. Experiments show that our proposed method can effectively complete face swapping and provide high-quality data for deep forgery detection to ensure the security of multimedia systems.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109723412",
                        "name": "Jiachen Yang"
                    },
                    {
                        "authorId": "2124702432",
                        "name": "Guipeng Lan"
                    },
                    {
                        "authorId": "2112869738",
                        "name": "Shuai Xiao"
                    },
                    {
                        "authorId": "2154900466",
                        "name": "Yang Li"
                    },
                    {
                        "authorId": "7797839",
                        "name": "Jiabao Wen"
                    },
                    {
                        "authorId": "48270268",
                        "name": "Yong Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While other generative models with explicit modulation of resolution hierarchies have shown properties of disentanglement [35, 36, 22, 21], note that our method was not designed to exhibit this sort of behaviour, and instead seems to be an emergent feature."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "9933e54773afff557fd8f498ebca7da8327f4224",
                "externalIds": {
                    "ArXiv": "2206.13397",
                    "DBLP": "conf/iclr/RissanenHS23",
                    "DOI": "10.48550/arXiv.2206.13397",
                    "CorpusId": 250072949
                },
                "corpusId": 250072949,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9933e54773afff557fd8f498ebca7da8327f4224",
                "title": "Generative Modelling With Inverse Heat Dissipation",
                "abstract": "While diffusion models have shown great success in image generation, their noise-inverting generative process does not explicitly consider the structure of images, such as their inherent multi-scale nature. Inspired by diffusion models and the empirical success of coarse-to-fine modelling, we propose a new diffusion-like model that generates images through stochastically reversing the heat equation, a PDE that locally erases fine-scale information when run over the 2D plane of the image. We interpret the solution of the forward heat equation with constant additive noise as a variational approximation in the diffusion latent variable model. Our new model shows emergent qualitative properties not seen in standard diffusion models, such as disentanglement of overall colour and shape in images. Spectral analysis on natural images highlights connections to diffusion models and reveals an implicit coarse-to-fine inductive bias in them.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "104147693",
                        "name": "Severi Rissanen"
                    },
                    {
                        "authorId": "34066178",
                        "name": "Markus Heinonen"
                    },
                    {
                        "authorId": "145410662",
                        "name": "A. Solin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Targeting at finding semantic directions in the latent space of a pretrained generator, in-domain editing [51,22,52,60,63,37,44,3,5,59,4,50] manipulates the attributes of the object, but keeps the same style.",
                "We refer to an in-domain editing [51,22,52,60] as the editing that only manipulates the latent code, given a fixed pretrained generator.",
                "For image-level editing applications, several approaches [22,51,52] find specific semantic directions in the latent space, e."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "97314468e7189fcd5f3cad84b1f55b8664008756",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-10590",
                    "ArXiv": "2206.10590",
                    "DOI": "10.48550/arXiv.2206.10590",
                    "CorpusId": 249889696
                },
                "corpusId": 249889696,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/97314468e7189fcd5f3cad84b1f55b8664008756",
                "title": "Temporally Consistent Semantic Video Editing",
                "abstract": "Generative adversarial networks (GANs) have demonstrated impressive image generation quality and semantic editing capability of real images, e.g., changing object classes, modifying attributes, or transferring styles. However, applying these GAN-based editing to a video independently for each frame inevitably results in temporal flickering artifacts. We present a simple yet effective method to facilitate temporally coherent video editing. Our core idea is to minimize the temporal photometric inconsistency by optimizing both the latent code and the pre-trained generator. We evaluate the quality of our editing on different domains and GAN inversion techniques and show favorable results against the baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2176060995",
                        "name": "Yi Xu"
                    },
                    {
                        "authorId": "23982870",
                        "name": "Badour Albahar"
                    },
                    {
                        "authorId": "2238908897",
                        "name": "Jia-Bin Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In our work, we use a similar method to GANSpace [14], which applies PCA to latent vectors sampled",
                "Our direction finding method is inspired by GANSpace [14], which applies Principal Component Analysis (PCA) [41] to a set of randomly sampled latent vectors from the intermediate layers of BigGAN [4] and StyleGAN [19] models to find meaningful directions and perform controlled im-",
                "Unsupervised latent space manipulation methods find meaningful directions in latent space in an unsupervised manner [14, 17, 34, 42]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "9b2fa04a3c8edd1a2c0657cc0c4339ad6b1ae06b",
                "externalIds": {
                    "DBLP": "conf/candc/TurkerDY22",
                    "DOI": "10.1145/3527927.3532790",
                    "CorpusId": 249872717
                },
                "corpusId": 249872717,
                "publicationVenue": {
                    "id": "1e23489a-111a-45c8-b5f5-eb80fa1272b6",
                    "name": "Creativity & Cognition",
                    "type": "conference",
                    "alternate_names": [
                        "Creativity  Cogn",
                        "C&C",
                        "Creativity Cogn",
                        "Creativity and Cognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9b2fa04a3c8edd1a2c0657cc0c4339ad6b1ae06b",
                "title": "MIDISpace: Finding Linear Directions in Latent Space for Music Generation",
                "abstract": "While recent work has shown that it is possible to find disentangled directions in the latent space of image generative networks, finding directions in the latent space of sequential models for music generation remains a largely unexplored topic. In this work, we propose a method for discovering linear directions in the latent space of a musicgenerating Variational Auto-Encoder (VAE). We use PCA, a statistical method, to transform the input data such that the variation along the new axes is maximized. We apply PCA to the latent space activations of our model and find largely disentangled directions that change the style and characteristics of the input music. Our experiments show that the found directions are often monotonic, global and encode fundamental musical characteristics such as colorfulness, speed, and repetitiveness. Moreover, we propose a set of quantitative metrics to describe different musical styles and characteristics to evaluate our results. We show that the found directions decouple content and can be utilized for style transfer and conditional music generation tasks. Our project page can be found at http://catlab-team.github.io/midispace.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2171369173",
                        "name": "Meliksah Turker"
                    },
                    {
                        "authorId": "2145260636",
                        "name": "Alara Dirik"
                    },
                    {
                        "authorId": "3137679",
                        "name": "Pinar Yanardag"
                    }
                ]
            }
        },
        {
            "contexts": [
                "1 Introduction Generative Adversarial Networks (GANs) [8] have achieved significant advancement over the past several years, enabling many computer vision tasks such as image manipulation [17, 12, 28, 9, 2], domain translation [11, 34, 6, 7, 20, 18], and image or video generation [19, 21, 15, 16, 13, 14, 29, 32, 30, 31]."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "d181656b2e40f17c34b04d6ee69a3015e2e56479",
                "externalIds": {
                    "DBLP": "conf/iclr/HanCCK0C23",
                    "ArXiv": "2206.08549",
                    "DOI": "10.48550/arXiv.2206.08549",
                    "CorpusId": 249848252
                },
                "corpusId": 249848252,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d181656b2e40f17c34b04d6ee69a3015e2e56479",
                "title": "Rarity Score : A New Metric to Evaluate the Uncommonness of Synthesized Images",
                "abstract": "Evaluation metrics in image synthesis play a key role to measure performances of generative models. However, most metrics mainly focus on image fidelity. Existing diversity metrics are derived by comparing distributions, and thus they cannot quantify the diversity or rarity degree of each generated image. In this work, we propose a new evaluation metric, called `rarity score', to measure the individual rarity of each image synthesized by generative models. We first show empirical observation that common samples are close to each other and rare samples are far from each other in nearest-neighbor distances of feature space. We then use our metric to demonstrate that the extent to which different generative models produce rare images can be effectively compared. We also propose a method to compare rarities between datasets that share the same concept such as CelebA-HQ and FFHQ. Finally, we analyze the use of metrics in different designs of feature spaces to better understand the relationship between feature spaces and resulting sparse images. Code will be publicly available online for the research community.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "12211976",
                        "name": "Jiyeon Han"
                    },
                    {
                        "authorId": "2111346236",
                        "name": "Hwanil Choi"
                    },
                    {
                        "authorId": "30187096",
                        "name": "Yunjey Choi"
                    },
                    {
                        "authorId": "1738212",
                        "name": "Jae Hyun Kim"
                    },
                    {
                        "authorId": "2577039",
                        "name": "Jung-Woo Ha"
                    },
                    {
                        "authorId": "7629367",
                        "name": "Jaesik Choi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[12], [31], [32], [33], [34], [35] 2D L.",
                "Instead of training a conditional generative model, the other line of works [12], [31], [32], [33], [34], [35] manages to explore the latent space of pretrained generative models, which can randomly generate high-fidelity images of a specific category, to find directions"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "55fe382924fa31ca8405554ed5f3073f8277e300",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-13251",
                    "ArXiv": "2206.08361",
                    "DOI": "10.48550/arXiv.2206.08361",
                    "CorpusId": 249712197
                },
                "corpusId": 249712197,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/55fe382924fa31ca8405554ed5f3073f8277e300",
                "title": "Controllable 3D Face Synthesis with Conditional Generative Occupancy Fields",
                "abstract": "Capitalizing on the recent advances in image generation models, existing controllable face image synthesis methods are able to generate high-fidelity images with some levels of controllability, e.g., controlling the shapes, expressions, textures, and poses of the generated face images. However, previous methods focus on controllable 2D image generative models, which are prone to producing inconsistent face images under large expression and pose changes. In this paper, we propose a new NeRF-based conditional 3D face synthesis framework, which enables 3D controllability over the generated face images by imposing explicit 3D conditions from 3D face priors. At its core is a conditional Generative Occupancy Field (cGOF++) that effectively enforces the shape of the generated face to conform to a given 3D Morphable Model (3DMM) mesh, built on top of EG3D [1], a recent tri-plane-based generative model. To achieve accurate control over fine-grained 3D face shapes of the synthesized images, we additionally incorporate a 3D landmark loss as well as a volume warping loss into our synthesis framework. Experiments validate the effectiveness of the proposed method, which is able to generate high-fidelity face images and shows more precise 3D controllability than state-of-the-art 2D-based controllable face synthesis methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "11837959",
                        "name": "Keqiang Sun"
                    },
                    {
                        "authorId": "2146648",
                        "name": "Shangzhe Wu"
                    },
                    {
                        "authorId": "1830448350",
                        "name": "Zhaoyang Huang"
                    },
                    {
                        "authorId": null,
                        "name": "Ning Zhang"
                    },
                    {
                        "authorId": "2145348695",
                        "name": "Quan Wang"
                    },
                    {
                        "authorId": "47893312",
                        "name": "Hongsheng Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While the disentangled style-space of StyleGANs [20\u201322] allows for control over the viewpoint of the generated images to some extent [13, 26, 42, 51], gaining precise 3D-consistent control is still non-trivial due to its lack of physical interpretation and operation in 2D."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "05f31f8d11629bec66e99b72862da48bbcb03ff7",
                "externalIds": {
                    "DBLP": "conf/nips/SchwarzSNL022",
                    "ArXiv": "2206.07695",
                    "DOI": "10.48550/arXiv.2206.07695",
                    "CorpusId": 249674609
                },
                "corpusId": 249674609,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/05f31f8d11629bec66e99b72862da48bbcb03ff7",
                "title": "VoxGRAF: Fast 3D-Aware Image Synthesis with Sparse Voxel Grids",
                "abstract": "State-of-the-art 3D-aware generative models rely on coordinate-based MLPs to parameterize 3D radiance fields. While demonstrating impressive results, querying an MLP for every sample along each ray leads to slow rendering. Therefore, existing approaches often render low-resolution feature maps and process them with an upsampling network to obtain the final image. Albeit efficient, neural rendering often entangles viewpoint and content such that changing the camera pose results in unwanted changes of geometry or appearance. Motivated by recent results in voxel-based novel view synthesis, we investigate the utility of sparse voxel grid representations for fast and 3D-consistent generative modeling in this paper. Our results demonstrate that monolithic MLPs can indeed be replaced by 3D convolutions when combining sparse voxel grids with progressive growing, free space pruning and appropriate regularization. To obtain a compact representation of the scene and allow for scaling to higher voxel resolutions, our model disentangles the foreground object (modeled in 3D) from the background (modeled in 2D). In contrast to existing approaches, our method requires only a single forward pass to generate a full 3D scene. It hence allows for efficient rendering from arbitrary viewpoints while yielding 3D consistent results with high visual fidelity.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40502376",
                        "name": "Katja Schwarz"
                    },
                    {
                        "authorId": "40562186",
                        "name": "Axel Sauer"
                    },
                    {
                        "authorId": "145048708",
                        "name": "Michael Niemeyer"
                    },
                    {
                        "authorId": "2699340",
                        "name": "Yiyi Liao"
                    },
                    {
                        "authorId": "47237027",
                        "name": "Andreas Geiger"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some approaches use fundamental techniques such as projection [48], PCA [17], and orthogonal regularization [26], while others use self-supervised techniques to learn interpretable representations [36]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "da135c5294aca324ce7ff3e0d30d7a1e1ed74f83",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-05257",
                    "ArXiv": "2206.05257",
                    "DOI": "10.48550/arXiv.2206.05257",
                    "CorpusId": 249605335
                },
                "corpusId": 249605335,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/da135c5294aca324ce7ff3e0d30d7a1e1ed74f83",
                "title": "Explaining Image Classifiers Using Contrastive Counterfactuals in Generative Latent Spaces",
                "abstract": "Despite their high accuracies, modern complex image classifiers cannot be trusted for sensitive tasks due to their unknown decision-making process and potential biases. Counterfactual explanations are very effective in providing transparency for these black-box algorithms. Nevertheless, generating counterfactuals that can have a consistent impact on classifier outputs and yet expose interpretable feature changes is a very challenging task. We introduce a novel method to generate causal and yet interpretable counterfactual explanations for image classifiers using pretrained generative models without any re-training or conditioning. The generative models in this technique are not bound to be trained on the same data as the target classifier. We use this framework to obtain contrastive and causal sufficiency and necessity scores as global explanations for black-box classifiers. On the task of face attribute classification, we show how different attributes influence the classifier output by providing both causal and contrastive feature attributions, and the corresponding counterfactual images.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46650151",
                        "name": "Kamran Alipour"
                    },
                    {
                        "authorId": "2003163970",
                        "name": "Aditya Lahiri"
                    },
                    {
                        "authorId": "3419364",
                        "name": "E. Adeli"
                    },
                    {
                        "authorId": "2124624117",
                        "name": "Babak Salimi"
                    },
                    {
                        "authorId": "1694780",
                        "name": "M. Pazzani"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", [34, 11]) or by incorporating pre-trained detectors [32].",
                ", ProgGAN [14], and StyleGAN2 [15]) and compare with GANSpace [11], WGS [32], and StyleCLIP [23].",
                "Exploring the latent space of a pre-trained GAN in an interpretable manner has typically been approached by the research community by discovering a set of linear [34, 11] or non-linear [32] latent paths, in an unsupervised or (semi)supervised manner, and then trying to label them based either on laborious manual annotation (e.",
                "In this section, we first compare the proposed method, which discovers non-linear paths in the GAN\u2019s latent space and may adopt non-linear or linear paths in the text space, with GANSpace [11] and WGS [32], which discover linear or non-linear paths, respectively, in the GAN latent space in an unsupervised manner.",
                "Exploring the latent space of a pre-trained GAN in an interpretable manner has drawn significant attention from the research community during the recent years [11, 34, 31, 22, 32, 9, 29, 24, 12, 1, 35, 30].",
                ", [34, 11]) or by incorporating pre-trained detectors (e.",
                ", by preserving certain attributes such as hair style and facial expressions) and far less artifacts, without the need of labeling them using manual annotation [11] or certain pretrained detectors [32].",
                "Figure 9: Comparison of the proposed method with GANSpace [11] and WGS [32].",
                "ID (\u2191) Age (\u2193) Gender (\u2193) Skin (\u2193) Hair (\u2193) Beard (\u2193) GANSpace [11] .",
                "GANSpace [11] performs PCA on deep features at the early layers of the generator and finds directions in the latent space that best map to those deep PCA vectors, arriving at a set of non-orthogonal directions in the latent space."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "result",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "d97581ce498a2944e37007976cdc7350a4aff6fd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-02104",
                    "ArXiv": "2206.02104",
                    "DOI": "10.48550/arXiv.2206.02104",
                    "CorpusId": 249395347
                },
                "corpusId": 249395347,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d97581ce498a2944e37007976cdc7350a4aff6fd",
                "title": "ContraCLIP: Interpretable GAN generation driven by pairs of contrasting sentences",
                "abstract": "This work addresses the problem of discovering non-linear interpretable paths in the latent space of pre-trained GANs in a model-agnostic manner. In the proposed method, the discovery is driven by a set of pairs of natural language sentences with contrasting semantics, named semantic dipoles, that serve as the limits of the interpretation that we require by the trainable latent paths to encode. By using the pre-trained CLIP encoder, the sentences are projected into the vision-language space, where they serve as dipoles, and where RBF-based warping functions define a set of non-linear directional paths, one for each semantic dipole, allowing in this way traversals from one semantic pole to the other. By defining an objective that discovers paths in the latent space of GANs that generate changes along the desired paths in the vision-language embedding space, we provide an intuitive way of controlling the underlying generative factors and address some of the limitations of the state-of-the-art works, namely, that a) they are typically tailored to specific GAN architectures (i.e., StyleGAN), b) they disregard the relative position of the manipulated and the original image in the image embedding and the relative position of the image and the text embeddings, and c) they lead to abrupt image manipulations and quickly arrive at regions of low density and, thus, low image quality, providing limited control of the generative factors. We provide extensive qualitative and quantitative results that demonstrate our claims with two pre-trained GANs, and make the code and the pre-trained models publicly available at: https://github.com/chi0tzp/ContraCLIP",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1694090",
                        "name": "Christos Tzelepis"
                    },
                    {
                        "authorId": "2059960629",
                        "name": "James Oldfield"
                    },
                    {
                        "authorId": "2137359565",
                        "name": "Georgios Tzimiropoulos"
                    },
                    {
                        "authorId": "50058816",
                        "name": "I. Patras"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The general idea of optimizing latent representations or parameters of an image generator using a separately trained classifier has been widely used as a powerful framework for generating, editing and recovering images (Dhariwal & Nichol, 2021; Abdal et al., 2019; Ha\u0308rko\u0308nen et al., 2020).",
                "The general idea of optimizing latent representations or parameters of an image generator using a separately trained classifier has been widely used as a powerful framework for generating, editing and recovering images (Dhariwal & Nichol, 2021; Abdal et al., 2019; H\u00e4rk\u00f6nen et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3e6fb49d720f386b5d373a25be746a40db490732",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-01661",
                    "ArXiv": "2206.01661",
                    "DOI": "10.48550/arXiv.2206.01661",
                    "CorpusId": 249375664
                },
                "corpusId": 249375664,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3e6fb49d720f386b5d373a25be746a40db490732",
                "title": "Style-Content Disentanglement in Language-Image Pretraining Representations for Zero-Shot Sketch-to-Image Synthesis",
                "abstract": "In this work, we propose and validate a framework to leverage language-image pretraining representations for training-free zero-shot sketch-to-image synthesis. We show that disentangled content and style representations can be utilized to guide image generators to employ them as sketch-to-image generators without (re-)training any parameters. Our approach for disentangling style and content entails a simple method consisting of elementary arithmetic assuming compositionality of information in representations of input sketches. Our results demonstrate that this approach is competitive with state-of-the-art instance-level open-domain sketch-to-image models, while only depending on pretrained off-the-shelf models and a fraction of the data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2140578249",
                        "name": "Jan Zuiderveld"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Controllable GAN Generation Increasingly powerful image GAN models have sparked interest in steerable image generation methods that synthesize an image by guiding the generator towards some objective: GAN output can be steered by directly guiding generation towards target images [12]; or by optimizing loss of a classifier [8, 23]; or PCA, clustering or other methods can also be used to directly identify meaningful representation subspaces for manipulating a GAN [3, 11, 24].",
                "The state-of-theart DALL-E [21] uses CLIP; and CLIP has also been combined with StyleGAN [2, 14, 19], BigGAN [18], and VQGAN [4\u20136].",
                "Figure 1 presents samples of generated images: the first row shows images generated with the original VQGAN+CLIP setting, capturing the visual concepts of the target prompts, and in cases of \u201cpeas\u201d, \u201ctime\u201d, \u201cfocus\u201d, and \u201cpolice\u201d also showing the letters of the words.",
                "We use an open-\nsource implementation from [5] of a VQGAN generation model [6] which steers the image generation based on a text prompt.",
                "In case of nonsense strings, the VQGAN+CLIP method is more likely to produce image text, possibly because nonsense string text prompts do not have a visual meaning associated with them.",
                "Like these methods, we investigate the ability of CLIP to steer VQGAN, however instead of generating individual images, we ask whether the broad ability of CLIP to read and draw visual words can be controlled.",
                "We generate 1000 images conditioned on real English words from our validation set, and 1000 images conditioned on nonsense strings from the validation text string set using VQGAN+CLIP and both of our projection models."
            ],
            "isInfluential": true,
            "intents": [],
            "citingPaper": {
                "paperId": "32ef7df798ecef81f7abc9a48df276dd07fbc865",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-07835",
                    "ArXiv": "2206.07835",
                    "DOI": "10.1109/CVPR52688.2022.01592",
                    "CorpusId": 249711999
                },
                "corpusId": 249711999,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/32ef7df798ecef81f7abc9a48df276dd07fbc865",
                "title": "Disentangling visual and written concepts in CLIP",
                "abstract": "The CLIP network measures the similarity between natural text and images; in this work, we investigate the entanglement of the representation of word images and natural images in its image encoder. First, we find that the image encoder has an ability to match word images with natural images of scenes described by those words. This is consistent with previous research that suggests that the meaning and the spelling of a word might be entangled deep within the network. On the other hand, we also find that CLIP has a strong ability to match nonsense words, suggesting that processing of letters is separated from processing of their meaning. To explicitly determine whether the spelling capability of CLIP is separable, we devise a procedure for identifying representation subspaces that selectively isolate or eliminate spelling capabilities. We benchmark our methods against a range of retrieval tasks, and we also test them by measuring the appearance of text in CLIP-guided generated images. We find that our methods are able to cleanly separate spelling capabilities of CLIP from the visual processing of natural images.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "7654960",
                        "name": "Joanna Materzynska"
                    },
                    {
                        "authorId": "143805211",
                        "name": "A. Torralba"
                    },
                    {
                        "authorId": "144159726",
                        "name": "David Bau"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Alternative methods have involved specially designing the GAN architecture to facilitate modification [24, 39], while others involve utilizing various mathematical techniques such as principal component analysis [16] to identify important dimensions within the latent space or even building customized models that can learn to manipulate the latent space, such as in [41].",
                "Unsupervised approaches typically involve identifying components within the latent GAN space [7, 16].",
                "Recent work has demonstrated that Generative Adversarial Networks (GANs) [15] encode human-interpretable representations of semantic concepts [14, 42, 16, 7, 41], which partially explains their performance in semantic editing tasks.",
                "Due to their high performance in modeling highly complex features, the most popular techniques involve various approaches built on generative neural networks [28, 2, 11, 42, 41, 27, 32, 16, 7], although other neural architectures [4, 29] have also shown promise."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "24debcf003cb021f62b3fb5a687aa3e57f8e59f2",
                "externalIds": {
                    "DBLP": "conf/cvpr/DavisTR22",
                    "DOI": "10.1109/CVPR52688.2022.01793",
                    "CorpusId": 250616234
                },
                "corpusId": 250616234,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/24debcf003cb021f62b3fb5a687aa3e57f8e59f2",
                "title": "Brain-Supervised Image Editing",
                "abstract": "Despite recent advances in deep neural models for semantic image editing, present approaches are dependent on explicit human input. Previous work assumes the availability of manually curated datasets for supervised learning, while for unsupervised approaches the human inspection of discovered components is required to identify those which modify worthwhile semantic features. Here, we present a novel alternative: the utilization of brain responses as a supervision signal for learning semantic feature representations. Participants $(N=30)$ in a neurophysiological experiment were shown artificially generated faces and instructed to look for a particular semantic feature, such as \u201cold\u201d or \u201csmiling\u201d, while their brain responses were recorded via electroencephalography (EEG). Using supervision signals inferred from these responses, semantic features within the latent space of a generative adversarial network (GAN) were learned and then used to edit semantic features of new images. We show that implicit brain supervision achieves comparable semantic image editing performance to explicit manual labeling. This work demonstrates the feasibility of utilizing implicit human reactions recorded via brain-computer interfaces for semantic image editing and interpretation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2107086284",
                        "name": "Keith M. Davis"
                    },
                    {
                        "authorId": "1404464386",
                        "name": "Carlos de la Torre-Ortiz"
                    },
                    {
                        "authorId": "2084101",
                        "name": "Tuukka Ruotsalo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Moreover, GANs like StyleGAN [28, 29] have been shown to form a semantic understanding of the modeled images in their features [3, 4, 15, 22, 25, 35, 56, 61, 63, 68, 73], which has been leveraged in diverse applications, including image editing [10, 20, 31, 32, 37, 60, 76], inverse rendering [71], style transfer [1, 30], image-to-image translation [8,9,23,51], and semi-supervised learning [35,68,73]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "414d909a49f126b843553f31e4a13b2709cbeb61",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-02903",
                    "ArXiv": "2206.02903",
                    "DOI": "10.1109/CVPR52688.2022.01037",
                    "CorpusId": 249431832
                },
                "corpusId": 249431832,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/414d909a49f126b843553f31e4a13b2709cbeb61",
                "title": "Polymorphic-GAN: Generating Aligned Samples across Multiple Domains with Learned Morph Maps",
                "abstract": "Modern image generative models show remarkable sample quality when trained on a single domain or class of objects. In this work, we introduce a generative adversarial network that can simultaneously generate aligned image samples from multiple related domains. We leverage the fact that a variety of object classes share common attributes, with certain geometric differences. We propose Polymorphic-GAN which learns shared features across all domains and a per-domain morph layer to morph shared features according to each domain. In contrast to previous works, our framework allows simultaneous modelling of images with highly varying geometries, such as images of human faces, painted and artistic faces, as well as multiple different animal faces. We demonstrate that our model produces aligned samples for all domains and show how it can be used for applications such as segmentation transfer and cross-domain image editing, as well as training in low-data regimes. Additionally, we apply our Polymorphic-GAN on image-to-image translation tasks and show that we can greatly surpass previous approaches in cases where the geometric differences between domains are large.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2596437",
                        "name": "Seung Wook Kim"
                    },
                    {
                        "authorId": "32113848",
                        "name": "Karsten Kreis"
                    },
                    {
                        "authorId": "2108864987",
                        "name": "Daiqing Li"
                    },
                    {
                        "authorId": "143805211",
                        "name": "A. Torralba"
                    },
                    {
                        "authorId": "37895334",
                        "name": "S. Fidler"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We use GANSpace [21] and StyleCLIP [40] for finding an editing direction \u03b4w in the W latent space.",
                "After inversion, we can edit the inverted code by traversing semantically meaningful directions computed using supervised [9, 25, 47] or unsupervised approaches [17, 21, 41, 48, 52].",
                "We use GANSpace [21] and StyleCLIP [40] for finding an editing direction \u03b4w+ in the W+ latent space."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3e390944fc7e7882acc4278dfaefda17233fd0dc",
                "externalIds": {
                    "DBLP": "conf/cvpr/ParmarLL0ZS22",
                    "ArXiv": "2206.08357",
                    "DOI": "10.1109/CVPR52688.2022.01111",
                    "CorpusId": 249712468
                },
                "corpusId": 249712468,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3e390944fc7e7882acc4278dfaefda17233fd0dc",
                "title": "Spatially-Adaptive Multilayer Selection for GAN Inversion and Editing",
                "abstract": "Existing GAN inversion and editing methods work well for aligned objects with a clean background, such as portraits and animal faces, but often struggle for more difficult categories with complex scene layouts and object occlusions, such as cars, animals, and outdoor images. We propose a new method to invert and edit such complex images in the latent space of GANs, such as StyleGAN2. Our key idea is to explore inversion with a collection of layers, spatially adapting the inversion process to the difficulty of the image. We learn to predict the \u201cinvertibility\u201d of different image segments and project each segment into a latent layer. Easier regions can be inverted into an earlier layer in the generator's latent space, while more challenging regions can be inverted into a later feature space. Experiments show that our method obtains better inversion results compared to the recent approaches on complex categories, while maintaining downstream editability. Please refer to our project page at gauravparmar.com/sam_inversion.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2065083806",
                        "name": "Gaurav Parmar"
                    },
                    {
                        "authorId": "152998391",
                        "name": "Yijun Li"
                    },
                    {
                        "authorId": "2054975",
                        "name": "Jingwan Lu"
                    },
                    {
                        "authorId": "2109976035",
                        "name": "Richard Zhang"
                    },
                    {
                        "authorId": "1922024303",
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "authorId": "50339742",
                        "name": "Krishna Kumar Singh"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "11985271ac6099cf3b5302528cce4e66d6b60efb",
                "externalIds": {
                    "DBLP": "journals/cgf/JeongLB22",
                    "DOI": "10.1111/cgf.14524",
                    "CorpusId": 250521018
                },
                "corpusId": 250521018,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/11985271ac6099cf3b5302528cce4e66d6b60efb",
                "title": "Interactively Assessing Disentanglement in GANs",
                "abstract": "Generative adversarial networks (GAN) have witnessed tremendous growth in recent years, demonstrating wide applicability in many domains. However, GANs remain notoriously difficult for people to interpret, particularly for modern GANs capable of generating photo\u2010realistic imagery. In this work we contribute a visual analytics approach for GAN interpretability, where we focus on the analysis and visualization of GAN disentanglement. Disentanglement is concerned with the ability to control content produced by a GAN along a small number of distinct, yet semantic, factors of variation. The goal of our approach is to shed insight on GAN disentanglement, above and beyond coarse summaries, instead permitting a deeper analysis of the data distribution modeled by a GAN. Our visualization allows one to assess a single factor of variation in terms of groupings and trends in the data distribution, where our analysis seeks to relate the learned representation space of GANs with attribute\u2010based semantic scoring of images produced by GANs. Through use\u2010cases, we show that our visualization is effective in assessing disentanglement, allowing one to quickly recognize a factor of variation and its overall quality. In addition, we show how our approach can highlight potential dataset biases learned by GANs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1899673899",
                        "name": "S. Jeong"
                    },
                    {
                        "authorId": "47130096",
                        "name": "Shusen Liu"
                    },
                    {
                        "authorId": "2113458694",
                        "name": "Matthew Berger"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The paths can also be learned by Principal Component Analysis or self-supervised approaches without annotation [15, 16]."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "6df0722a2c915e685a4673bc80b1bd7212e48112",
                "externalIds": {
                    "DBLP": "conf/cvpr/WuY022",
                    "DOI": "10.1109/CVPR52688.2022.00419",
                    "CorpusId": 250581277
                },
                "corpusId": 250581277,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6df0722a2c915e685a4673bc80b1bd7212e48112",
                "title": "HairMapper: Removing Hair from Portraits Using GANs",
                "abstract": "Removing hair from portrait images is challenging due to the complex occlusions between hair and face, as well as the lack of paired portrait data with/without hair. To this end, we present a dataset and a baseline method for removing hair from portrait images using generative adversarial networks (GANs). Our core idea is to train a fully connected network HairMapper to find the direction of hair removal in the latent space of StyleGAN for the training stage. We develop a new separation boundary and diffuse method to generate paired training data for males, and a novel \u201cfemale-male-bald\u201d pipeline for paired data of females. Experiments show that our method can naturally deal with portrait images with variations on gender, age, etc. We validate the superior performance of our method by comparing it to state-of-the-art methods through extensive experiments and user studies. We also demonstrate its applications in hair design and 3D face reconstruction.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115290070",
                        "name": "Yiqian Wu"
                    },
                    {
                        "authorId": "6635795",
                        "name": "Yong-Liang Yang"
                    },
                    {
                        "authorId": "2176497942",
                        "name": "Xiaogang Jin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The latent space distiller [34] is used to achieve attribute disentanglement and GANSpace [11] performs PCA on the sampled data to find primary directions in the latent space."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "69f92135a5f4514030e56fd687d7dd69c174f1be",
                "externalIds": {
                    "DBLP": "conf/cvpr/Xie0HFWG22",
                    "DOI": "10.1109/CVPR52688.2022.01925",
                    "CorpusId": 250616355
                },
                "corpusId": 250616355,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/69f92135a5f4514030e56fd687d7dd69c174f1be",
                "title": "Artistic Style Discovery with Independent Components",
                "abstract": "Style transfer has been well studied in recent years with excellent performance processed. While existing methods usually choose CNNs as the powerful tool to accomplish superb stylization, less attention was paid to the latent style space. Rare exploration of underlying dimensions results in the poor style controllability and the limited practical application. In this work, we rethink the internal meaning of style features, further proposing a novel unsupervised algorithm for style discovery and achieving personalized manip-ulation. In particular, we take a closer look into the mechanism of style transfer and obtain different artistic style components from the latent space consisting of different style features. Then fresh styles can be generated by linear combination according to various style components. Experimental results have shown that our approach is superb in 1) restylizing the original output with the diverse artistic styles discovered from the latent space while keeping the content unchanged, and 2) being generic and compatible for various style transfer methods. Our code is available in this page: https://github.com/Shelsin/ArtIns.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1933284437",
                        "name": "Xinxiong Xie"
                    },
                    {
                        "authorId": "2153684600",
                        "name": "Yi Li"
                    },
                    {
                        "authorId": "32885778",
                        "name": "Huaibo Huang"
                    },
                    {
                        "authorId": "2113504912",
                        "name": "Haiyan Fu"
                    },
                    {
                        "authorId": "2177250443",
                        "name": "Wanwan Wang"
                    },
                    {
                        "authorId": "144152334",
                        "name": "Yanqing Guo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[8, 26, 28] attempt to identify useful directions in an unsupervised manner.",
                "Alternative methods [8, 9, 25, 28, 35] find meaningful latent directions using unsupervised approaches."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4695915d81977692c28e60ad5e71049e47a6b8d3",
                "externalIds": {
                    "DBLP": "conf/cvpr/DonerBBKTY22",
                    "DOI": "10.1109/CVPRW56347.2022.00254",
                    "CorpusId": 251044372
                },
                "corpusId": 251044372,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4695915d81977692c28e60ad5e71049e47a6b8d3",
                "title": "PaintInStyle: One-Shot Discovery of Interpretable Directions by Painting",
                "abstract": "The search for interpretable directions in latent spaces of pre-trained Generative Adversarial Networks (GANs) has become a topic of interest. These directions can be utilized to perform semantic manipulations on the GAN generated images. The discovery of such directions is performed either in a supervised way, which requires manual annotation or pre-trained classifiers, or in an unsupervised way, which requires the user to interpret what these directions represent. In this work, we propose a framework that finds a specific manipulation direction using only a single simple sketch drawn on an image. Our method finds directions consisting of channels in the style space of the StyleGAN2 architecture responsible for the desired edits and performs image manipulations comparable with state-of-the-art methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2179131205",
                        "name": "Berkay Doner"
                    },
                    {
                        "authorId": "2179136413",
                        "name": "Elif Sema Balcioglu"
                    },
                    {
                        "authorId": "2179131438",
                        "name": "Merve Rabia Barin"
                    },
                    {
                        "authorId": "34907481",
                        "name": "Mert Tiftikci"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c7c66b99d28e248731fe85a8539a1ff4f268fac2",
                "externalIds": {
                    "DBLP": "conf/cvpr/KocasariZTSY22",
                    "DOI": "10.1109/CVPRW56347.2022.00255",
                    "CorpusId": 251073436
                },
                "corpusId": 251073436,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c7c66b99d28e248731fe85a8539a1ff4f268fac2",
                "title": "Rank in Style: A Ranking-based Approach to Find Interpretable Directions",
                "abstract": "Recent work such as StyleCLIP aims to harness the power of CLIP embeddings for controlled manipulations. Although these models are capable of manipulating images based on a text prompt, the success of the manipulation often depends on careful selection of the appropriate text for the desired manipulation. This limitation makes it particularly difficult to perform text-based manipulations in do-mains where the user lacks expertise, such as fashion. To address this problem, we propose a method for automatically determining the most successful and relevant text-based edits using a pre-trained StyleGAN model. Our approach consists of a novel mechanism that uses CLIP to guide beam-search decoding, and a ranking method that identifies the most relevant and successful edits based on a list of keywords. We also demonstrate the capabilities of our framework in several domains, including fashion.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2145258020",
                        "name": "Umut Kocasari"
                    },
                    {
                        "authorId": "2162194848",
                        "name": "Kerem Zaman"
                    },
                    {
                        "authorId": "34907481",
                        "name": "Mert Tiftikci"
                    },
                    {
                        "authorId": "1395808197",
                        "name": "Enis Simsar"
                    },
                    {
                        "authorId": "3137679",
                        "name": "Pinar Yanardag"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, whilst several methods identify ways of manipulating the latent space of GANs to bring about global semantic changes\u2013either in a supervised [16, 44, 48, 47] or unsupervised [53, 49, 20, 52, 42] manner\u2013many of them struggle to apply local changes to regions of interest in the image.",
                "A popular line of GAN-based image editing research concerns itself with learning so-called \u201cinterpretable directions\u201d in the generator\u2019s latent space [20, 49, 48, 53, 52, 57, 21, 18, 19].",
                "Understanding the representations induced by these networks for interpretation [3, 47, 57] and control [48, 47, 49, 20, 53, 15, 52, 63, 55, 1] has subsequently received much attention.",
                "Developing a better understanding of the way in which high-level concepts are represented and composed to form synthetic images is important for a number of downstream tasks such as generative model interpretability [47, 3, 57] and image editing [20, 49, 48, 53, 52, 2]."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f5cbde920a432a20b99417778aa128d40481217c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-00048",
                    "ArXiv": "2206.00048",
                    "DOI": "10.48550/arXiv.2206.00048",
                    "CorpusId": 249240397
                },
                "corpusId": 249240397,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f5cbde920a432a20b99417778aa128d40481217c",
                "title": "PandA: Unsupervised Learning of Parts and Appearances in the Feature Maps of GANs",
                "abstract": "Recent advances in the understanding of Generative Adversarial Networks (GANs) have led to remarkable progress in visual editing and synthesis tasks, capitalizing on the rich semantics that are embedded in the latent spaces of pre-trained GANs. However, existing methods are often tailored to specific GAN architectures and are limited to either discovering global semantic directions that do not facilitate localized control, or require some form of supervision through manually provided regions or segmentation masks. In this light, we present an architecture-agnostic approach that jointly discovers factors representing spatial parts and their appearances in an entirely unsupervised fashion. These factors are obtained by applying a semi-nonnegative tensor factorization on the feature maps, which in turn enables context-aware local image editing with pixel-level control. In addition, we show that the discovered appearance factors correspond to saliency maps that localize concepts of interest, without using any labels. Experiments on a wide range of GAN architectures and datasets show that, in comparison to the state of the art, our method is far more efficient in terms of training time and, most importantly, provides much more accurate localized control. Our code is available at: https://github.com/james-oldfield/PandA.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2059960629",
                        "name": "James Oldfield"
                    },
                    {
                        "authorId": "1694090",
                        "name": "Christos Tzelepis"
                    },
                    {
                        "authorId": "1780393",
                        "name": "Yannis Panagakis"
                    },
                    {
                        "authorId": "1752913",
                        "name": "M. Nicolaou"
                    },
                    {
                        "authorId": "50058816",
                        "name": "I. Patras"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "d5697301420b29213100c00db626ddcc72c4b00d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-15517",
                    "ArXiv": "2205.15517",
                    "DOI": "10.1145/3550454.3555506",
                    "CorpusId": 249210075
                },
                "corpusId": 249210075,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d5697301420b29213100c00db626ddcc72c4b00d",
                "title": "IDE-3D",
                "abstract": "Existing 3D-aware facial generation methods face a dilemma in quality versus editability: they either generate editable results in low resolution, or high-quality ones with no editing flexibility. In this work, we propose a new approach that brings the best of both worlds together. Our system consists of three major components: (1) a 3D-semantics-aware generative model that produces view-consistent, disentangled face images and semantic masks; (2) a hybrid GAN inversion approach that initializes the latent codes from the semantic and texture encoder, and further optimizes them for faithful reconstruction; and (3) a canonical editor that enables efficient manipulation of semantic masks in canonical view and produces high-quality editing results. Our approach is competent for many applications, e.g. free-view face drawing, editing and style control. Both quantitative and qualitative results show that our method reaches the state-of-the-art in terms of photorealism, faithfulness and efficiency.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2157278510",
                        "name": "Jingxiang Sun"
                    },
                    {
                        "authorId": "39849136",
                        "name": "X. Wang"
                    },
                    {
                        "authorId": "9644181",
                        "name": "Yichun Shi"
                    },
                    {
                        "authorId": "2108594877",
                        "name": "Lizhen Wang"
                    },
                    {
                        "authorId": "2167482077",
                        "name": "Jue Wang"
                    },
                    {
                        "authorId": "1680777",
                        "name": "Yebin Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For the StyleGAN architecutre, InterFaceGAN [Shen et al. 2020], GANSpace [H\u00e4rk\u00f6nen et al. 2020], StyleFlow [Abdal et al. 2021b], and StyleRig [Tewari et al. 2020a] propose linear and non-linear edits of the underlying\ud835\udc4a and\ud835\udc4a + spaces.",
                "2020], GANSpace [H\u00e4rk\u00f6nen et al. 2020], StyleFlow [Abdal et al.",
                "Note that this method is widely used by Gan-based image editing methods like InterfaceGAN [Shen et al. 2020] and GANSpace [H\u00e4rk\u00f6nen et al. 2020]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "693dd51c8065d92c3fe13f59729ae790b85fab5f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-13996",
                    "ArXiv": "2205.13996",
                    "DOI": "10.48550/arXiv.2205.13996",
                    "CorpusId": 249152120
                },
                "corpusId": 249152120,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/693dd51c8065d92c3fe13f59729ae790b85fab5f",
                "title": "Video2StyleGAN: Disentangling Local and Global Variations in a Video",
                "abstract": "Image editing using a pretrained StyleGAN generator has emerged as a powerful paradigm for facial editing, providing disentangled controls over age, expression, illumination, etc. However, the approach cannot be directly adopted for video manipulations. We hypothesize that the main missing ingredient is the lack of fine-grained and disentangled control over face location, face pose, and local facial expressions. In this work, we demonstrate that such a fine-grained control is indeed achievable using pretrained StyleGAN by working across multiple (latent) spaces (namely, the positional space, the W+ space, and the S space) and combining the optimization results across the multiple spaces. Building on this enabling component, we introduce Video2StyleGAN that takes a target image and driving video(s) to reenact the local and global locations and expressions from the driving video in the identity of the target image. We evaluate the effectiveness of our method over multiple challenging scenarios and demonstrate clear improvements over alternative approaches.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "94395014",
                        "name": "Rameen Abdal"
                    },
                    {
                        "authorId": "100516590",
                        "name": "Peihao Zhu"
                    },
                    {
                        "authorId": "1710455",
                        "name": "N. Mitra"
                    },
                    {
                        "authorId": "1798011",
                        "name": "Peter Wonka"
                    }
                ]
            }
        },
        {
            "contexts": [
                "On the other hand, the global methods search layer-wise perturbation directions that perform the same semantic manipulation on the entire latent space [14, 35, 37].",
                "We chose GANSpace [14] as a global basis because of its broad applicability.",
                "The global basis proposed in GANSpace is PCA components of latent variable samples.",
                "The results provide an explanation for the superior disentanglement of W-space observed in many literatures [14, 21], and suggest that the layers 6, 7 can serve as a similar-or-better alternative.",
                "We chose the Fr\u00e9chet Inception Distance (FID) [15] gap between Local Basis and GANSpace as a measure of global-basis-compatibility.",
                "Local Basis Several previous works suggested unsupervised methods to find semantic-factorizing directions based on the geometry of the target latent space in a pre-trained GAN model [8, 14].",
                "GANSpace [14] showed that the principal components obtained by PCA can serve as globally meaningful traversal directions.",
                "Here, global basis refers to the sample-independent semantically meaningful perturbations on a latent space [8], for example, GANSpace [14] and SeFa [35].",
                "FID gap represents the difference between FID score of Local Basis [8] and the global basis [14].",
                "FID is measured for 50k samples of perturbed images along the 1st component of Local Basis and GANSpace, respectively."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "651dbdb6f484babb552868d2975ab55e28c01d0c",
                "externalIds": {
                    "ArXiv": "2205.13182",
                    "DBLP": "journals/corr/abs-2205-13182",
                    "DOI": "10.48550/arXiv.2205.13182",
                    "CorpusId": 249097582
                },
                "corpusId": 249097582,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/651dbdb6f484babb552868d2975ab55e28c01d0c",
                "title": "Analyzing the Latent Space of GAN through Local Dimension Estimation",
                "abstract": "The impressive success of style-based GANs (StyleGANs) in high-fidelity image synthesis has motivated research to understand the semantic properties of their latent spaces. In this paper, we approach this problem through a geometric analysis of latent spaces as a manifold. In particular, we propose a local dimension estimation algorithm for arbitrary intermediate layers in a pre-trained GAN model. The estimated local dimension is interpreted as the number of possible semantic variations from this latent variable. Moreover, this intrinsic dimension estimation enables unsupervised evaluation of disentanglement for a latent space. Our proposed metric, called Distortion, measures an inconsistency of intrinsic tangent space on the learned latent space. Distortion is purely geometric and does not require any additional attribute information. Nevertheless, Distortion shows a high correlation with the global-basis-compatibility and supervised disentanglement score. Our work is the first step towards selecting the most disentangled latent space among various latent spaces in a GAN without attribute labels.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2921953",
                        "name": "Jaewoong Choi"
                    },
                    {
                        "authorId": "1947131861",
                        "name": "Geonho Hwang"
                    },
                    {
                        "authorId": "2111237576",
                        "name": "Hyunsoo Cho"
                    },
                    {
                        "authorId": "2259103",
                        "name": "Myung-joo Kang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "On the other hand, unsupervised methods [20,39] were proposed to find semantic direction without using attribute classifier."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "457753504fa5cd2798496297dce714daae6c58ed",
                "externalIds": {
                    "ArXiv": "2205.13368",
                    "DBLP": "journals/corr/abs-2205-13368",
                    "DOI": "10.48550/arXiv.2205.13368",
                    "CorpusId": 249097429
                },
                "corpusId": 249097429,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/457753504fa5cd2798496297dce714daae6c58ed",
                "title": "One-Shot Face Reenactment on Megapixels",
                "abstract": "The goal of face reenactment is to transfer a target expression and head pose to a source face while preserving the source identity. With the popularity of face-related applications, there has been much research on this topic. However, the results of existing methods are still limited to low-resolution and lack photorealism. In this work, we present a one-shot and high-resolution face reenactment method called MegaFR. To be precise, we leverage StyleGAN by using 3DMM-based rendering images and overcome the lack of high-quality video datasets by designing a loss function that works without high-quality videos. Also, we apply iterative refinement to deal with extreme poses and/or expressions. Since the proposed method controls source images through 3DMM parameters, we can explicitly manipulate source images. We apply MegaFR to various applications such as face frontalization, eye in-painting, and talking head generation. Experimental results show that our method successfully disentangles identity from expression and head pose, and outperforms conventional methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2113751282",
                        "name": "Wonjun Kang"
                    },
                    {
                        "authorId": "2152841248",
                        "name": "Geonsu Lee"
                    },
                    {
                        "authorId": "2463454",
                        "name": "H. Koo"
                    },
                    {
                        "authorId": "1707645",
                        "name": "N. Cho"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The typical knowledge, like the expression, age, or gender of facial images, may only contain information of low dimensions (Penev & Sirovich, 2000; Ha\u0308rko\u0308nen et al., 2020; Shen et al., 2020).",
                "The typical knowledge, like the expression, age, or gender of facial images, may only contain information of low dimensions (Penev & Sirovich, 2000; H\u00e4rk\u00f6nen et al., 2020; Shen et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "b5f24d66a147ac7e0534567d55fe386831d24082",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-13444",
                    "ArXiv": "2205.13444",
                    "DOI": "10.48550/arXiv.2205.13444",
                    "CorpusId": 249097850
                },
                "corpusId": 249097850,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b5f24d66a147ac7e0534567d55fe386831d24082",
                "title": "Principled Knowledge Extrapolation with GANs",
                "abstract": "Human can extrapolate well, generalize daily knowledge into unseen scenarios, raise and answer counterfactual questions. To imitate this ability via generative models, previous works have extensively studied explicitly encoding Structural Causal Models (SCMs) into architectures of generator networks. This methodology, however, lim-its the flexibility of the generator as they must be carefully crafted to follow the causal graph, and demands a ground truth SCM with strong ignorability assumption as prior, which is a nontrivial assumption in many real scenarios. Thus, many current causal GAN methods fail to generate high fidelity counterfactual results as they cannot easily leverage state-of-the-art generative models. In this paper, we propose to study counterfactual synthesis from a new perspective of knowledge extrapolation, where a given knowledge dimension of the data distribution is extrapolated, but the remaining knowledge is kept indistinguishable from the original distribution. We show that an adversarial game with a closed-form discriminator can be used to address the knowledge extrapolation problem, and a novel principal knowledge descent method can efficiently estimate the extrapolated distribution through the adversarial game. Our method enjoys both elegant theoretical guarantees and superior performance in many scenarios.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2119237546",
                        "name": "Ruili Feng"
                    },
                    {
                        "authorId": "2166974008",
                        "name": "Jie Xiao"
                    },
                    {
                        "authorId": "84005711",
                        "name": "Kecheng Zheng"
                    },
                    {
                        "authorId": "1678783",
                        "name": "Deli Zhao"
                    },
                    {
                        "authorId": "1709595",
                        "name": "Jingren Zhou"
                    },
                    {
                        "authorId": "2133377336",
                        "name": "Qibin Sun"
                    },
                    {
                        "authorId": "143962510",
                        "name": "Zhengjun Zha"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For pre-trained GAN-based baselines, we adopt GANspace (GS) [20], LatentDiscovery (LD) [39], ClosedForm (CF) [37], DeepSpectral (DS) [26] and DisCo [36]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "053e8b157bb14f30884f525615f52361366f23b2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-10093",
                    "ArXiv": "2205.10093",
                    "DOI": "10.48550/arXiv.2205.10093",
                    "CorpusId": 248965124
                },
                "corpusId": 248965124,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/053e8b157bb14f30884f525615f52361366f23b2",
                "title": "Visual Concepts Tokenization",
                "abstract": "Obtaining the human-like perception ability of abstracting visual concepts from concrete pixels has always been a fundamental and important target in machine learning research fields such as disentangled representation learning and scene decomposition. Towards this goal, we propose an unsupervised transformer-based Visual Concepts Tokenization framework, dubbed VCT, to perceive an image into a set of disentangled visual concept tokens, with each concept token responding to one type of independent visual concept. Particularly, to obtain these concept tokens, we only use cross-attention to extract visual information from the image tokens layer by layer without self-attention between concept tokens, preventing information leakage across concept tokens. We further propose a Concept Disentangling Loss to facilitate that different concept tokens represent independent visual concepts. The cross-attention and disentangling loss play the role of induction and mutual exclusion for the concept tokens, respectively. Extensive experiments on several popular datasets verify the effectiveness of VCT on the tasks of disentangled representation learning and scene decomposition. VCT achieves the state of the art results by a large margin.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1491069648",
                        "name": "Tao Yang"
                    },
                    {
                        "authorId": "46393469",
                        "name": "Yuwang Wang"
                    },
                    {
                        "authorId": "2165997451",
                        "name": "Yan Lu"
                    },
                    {
                        "authorId": "2144620206",
                        "name": "Nanning Zheng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We compared the rotation and smile directions found by our approach to those previously found by InterFaceGAN [33] and GANSpace [17].",
                "Visual comparison of editing a randomly sampled latent code in the smiling directions found in GANSpace [17] and InterFaceGAN [33] with the happiness direction found in this work.",
                "GANSpace [17] finds interpretable directions in an unsupervised fashion with PCA while manual examination of the found directions is required.",
                "Comparison of rotations produced by GANSpace [17] (top 2 rows), InterFaceGAN [33] (third row) and our approach (bottom).",
                "The quality of the edits performed with these directions is on par with the corresponding edits using GANSpace [17] and InterFaceGAN [33].",
                "To perform rotations with GANSpace [17], we initially used the 2nd principal component applied to the first three style vectors.",
                "Qualitative comparison of the found rotation direction with the equivalent edits from InterFaceGAN [33] and GANSpace [17] applied on the FEI face database [36]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "result",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "8c54cf03ec72e1cf855ef09b497250e1b59f3728",
                "externalIds": {
                    "ArXiv": "2205.06102",
                    "DBLP": "journals/corr/abs-2205-06102",
                    "DOI": "10.48550/arXiv.2205.06102",
                    "CorpusId": 248722159
                },
                "corpusId": 248722159,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8c54cf03ec72e1cf855ef09b497250e1b59f3728",
                "title": "Tensor-based Emotion Editing in the StyleGAN Latent Space",
                "abstract": "In this paper, we use a tensor model based on the Higher-Order Singular Value Decomposition (HOSVD) to discover semantic directions in Generative Adversarial Networks. This is achieved by \ufb01rst embedding a structured facial expression database into the latent space using the e4e encoder. Speci\ufb01cally, we discover directions in latent space corresponding to the six prototypical emotions: anger, disgust, fear, happiness, sadness, and surprise, as well as a direction for yaw rotation. These latent space directions are employed to change the expression or yaw rotation of real face images. We compare our found directions to similar directions found by two other methods. The results show that the visual quality of the resultant edits are on par with State-of-the-Art. It can also be concluded that the tensor-based model is well suited for emotion and yaw editing, i.e., that the emotion or yaw rotation of a novel face image can be robustly changed without a signi\ufb01cant effect on identity or other attributes in the images.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2140280426",
                        "name": "Ren\u00e9 Haas"
                    },
                    {
                        "authorId": "23620370",
                        "name": "Stella Grasshof"
                    },
                    {
                        "authorId": "120414984",
                        "name": "Sami S. Brandt"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Hence, our approach is still compatible with unsupervised editing techniques such as GANSpace [22] (fig."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "cf0c8ace93a9d204c02db6683e24f48bd4d2677c",
                "externalIds": {
                    "ArXiv": "2205.06304",
                    "DBLP": "journals/corr/abs-2205-06304",
                    "DOI": "10.48550/arXiv.2205.06304",
                    "CorpusId": 248798459
                },
                "corpusId": 248798459,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cf0c8ace93a9d204c02db6683e24f48bd4d2677c",
                "title": "Overparameterization Improves StyleGAN Inversion",
                "abstract": "Deep generative models like StyleGAN hold the promise of semantic image editing: modifying images by their content, rather than their pixel values. Unfortunately, working with arbitrary images requires inverting the StyleGAN generator, which has remained challenging so far. Existing inversion approaches obtain promising yet imperfect results, having to trade-off between reconstruction quality and downstream editability. To improve quality, these approaches must resort to various techniques that extend the model latent space after training. Taking a step back, we observe that these methods essentially all propose, in one way or another, to increase the number of free parameters. This suggests that inversion might be difficult because it is underconstrained. In this work, we address this directly and dramatically overparameterize the latent space, before training, with simple changes to the original StyleGAN architecture. Our overparameterization increases the available degrees of freedom, which in turn facilitates inversion. We show that this allows us to obtain near-perfect image reconstruction without the need for encoders nor for altering the latent space after training. Our approach also retains editability, which we demonstrate by realistically interpolating between images.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2165304859",
                        "name": "Yohan Poirier-Ginter"
                    },
                    {
                        "authorId": "2165312722",
                        "name": "Alexandre Lessard"
                    },
                    {
                        "authorId": "2165361707",
                        "name": "Ryan Smith"
                    },
                    {
                        "authorId": "144430305",
                        "name": "Jean-Fran\u00e7ois Lalonde"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "576ac4dd08ee354b727099de63ce5e4568ad6c0e",
                "externalIds": {
                    "ArXiv": "2205.05764",
                    "DBLP": "journals/corr/abs-2205-05764",
                    "DOI": "10.1007/s11229-022-03739-2",
                    "CorpusId": 248721934
                },
                "corpusId": 248721934,
                "publicationVenue": {
                    "id": "cfb7bc3b-4dad-4d1f-aea6-f5d1f2499ce8",
                    "name": "Synthese",
                    "type": "journal",
                    "issn": "0039-7857",
                    "url": "http://www.springer.com/11229",
                    "alternate_urls": [
                        "http://www.jstor.org/journals/00397857.html",
                        "https://www.jstor.org/journal/synthese"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/576ac4dd08ee354b727099de63ce5e4568ad6c0e",
                "title": "Deep learning and synthetic media",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "16273823",
                        "name": "Rapha\u00ebl Milli\u00e8re"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Although several methods [10, 23, 24] have shown the successful pose control by discovering a direction related to pose in the latent space of StyleGAN, they haven\u2019t found accurate mapping for frontalizing an arbitrary image in an unsupervised manner.",
                "Several techniques of StyleGAN latent manipulation [10, 23, 24] have been combined with GAN inversion [2, 22] for real image editing."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1f75e419ded2d2030f377497f433ee480c52a7a9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-02974",
                    "ArXiv": "2205.02974",
                    "DOI": "10.48550/arXiv.2205.02974",
                    "CorpusId": 248562951
                },
                "corpusId": 248562951,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1f75e419ded2d2030f377497f433ee480c52a7a9",
                "title": "Generate and Edit Your Own Character in a Canonical View",
                "abstract": "Recently, synthesizing personalized characters from a single user-given portrait has received remarkable attention as a drastic popularization of social media and the metaverse. The input image is not always in frontal view, thus it is important to acquire or predict canonical view for 3D modeling or other applications. Although the progress of generative models enables the stylization of a portrait, obtaining the stylized image in canonical view is still a challenging task. There have been several studies on face frontalization but their performance significantly decreases when input is not in the real image domain, e.g., cartoon or painting. Stylizing after frontalization also results in degenerated output. In this paper, we propose a novel and unified framework which generates stylized portraits in canonical view. With a proposed latent mapper, we analyze and discover frontalization mapping in a latent space of StyleGAN to stylize and frontalize at once. In addition, our model can be trained with unlabelled 2D image sets, without any 3D supervision. The effectiveness of our method is demonstrated by experimental results.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2016481042",
                        "name": "Jeong-gi Kwak"
                    },
                    {
                        "authorId": "2111164052",
                        "name": "Yuanming Li"
                    },
                    {
                        "authorId": "2146861723",
                        "name": "Dongsik Yoon"
                    },
                    {
                        "authorId": "2757702",
                        "name": "D. Han"
                    },
                    {
                        "authorId": "144878703",
                        "name": "Hanseok Ko"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Directions of variation naturally emerge in the latent space and can be discovered when guided by geometry/color changes [31], language or attributes [59, 63, 75, 2, 88], cognitive signals [17], or in an unsupervised manner [21, 76, 61]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "52edb6d5234c4eb545cfe4d499ab4e52795f1de6",
                "externalIds": {
                    "DBLP": "conf/eccv/EpsteinP0SE22",
                    "ArXiv": "2205.02837",
                    "DOI": "10.48550/arXiv.2205.02837",
                    "CorpusId": 248524853
                },
                "corpusId": 248524853,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/52edb6d5234c4eb545cfe4d499ab4e52795f1de6",
                "title": "BlobGAN: Spatially Disentangled Scene Representations",
                "abstract": ". We propose an unsupervised, mid-level representation for a generative model of scenes. The representation is mid-level in that it is neither per-pixel nor per-image; rather, scenes are modeled as a collection of spatial, depth-ordered \u201cblobs\u201d of features. Blobs are differentiably placed onto a feature grid that is decoded into an image by a generative adversarial network. Due to the spatial uniformity of blobs and the locality inherent to convolution, our network learns to associate different blobs with different entities in a scene and to arrange these blobs to capture scene layout. We demonstrate this emergent behavior by showing that, despite training without any supervision, our method enables applications such as easy manipulation of objects within a scene ( e.g. moving, removing, and restyling furniture), creation of feasible scenes given constraints ( e.g. plausible rooms with drawers at a particular location), and parsing of real-world images into constituent parts. On a challenging multi-category dataset of indoor scenes, BlobGAN outperforms StyleGAN2 in image quality as measured by FID. See our project page for video results and interactive demo: http://www.dave.ml/blobgan. objects that comprise them. Here, we demonstrate through various image manipulation applications that this ability emerges in our model. Our unsupervised representation allows effortless rearrangement, removal, cloning, and restyling of objects in scenes. We also measure correlation between blob presence and semantic categories as predicted by an off-the-shelf network and thus empirically verify the associations discovered by our model.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "32486555",
                        "name": "Dave Epstein"
                    },
                    {
                        "authorId": "2071929129",
                        "name": "Taesung Park"
                    },
                    {
                        "authorId": "2109976035",
                        "name": "Richard Zhang"
                    },
                    {
                        "authorId": "2177801",
                        "name": "Eli Shechtman"
                    },
                    {
                        "authorId": "1763086",
                        "name": "Alexei A. Efros"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In global manipulation works, interpolations in the latent space are located which correspond to edits over the entire image, either via visual attribute classifiers [10, 33, 57, 71], unsupervised disentanglement [26, 47, 56, 63, 67], or via image-text similarity [3, 11, 22, 40, 46, 55, 68]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "92df2194b2ddc44b137d5ae34aaa644b77f87847",
                "externalIds": {
                    "ArXiv": "2205.01668",
                    "DBLP": "journals/corr/abs-2205-01668",
                    "DOI": "10.48550/arXiv.2205.01668",
                    "CorpusId": 248505860
                },
                "corpusId": 248505860,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/92df2194b2ddc44b137d5ae34aaa644b77f87847",
                "title": "End-to-End Visual Editing with a Generatively Pre-Trained Artist",
                "abstract": "We consider the targeted image editing problem: blending a region in a source image with a driver image that specifies the desired change. Differently from prior works, we solve this problem by learning a conditional probability distribution of the edits, end-to-end. Training such a model requires addressing a fundamental technical challenge: the lack of example edits for training. To this end, we propose a self-supervised approach that simulates edits by augmenting off-the-shelf images in a target domain. The benefits are remarkable: implemented as a state-of-the-art auto-regressive transformer, our approach is simple, sidesteps difficulties with previous methods based on GAN-like priors, obtains significantly better edits, and is efficient. Furthermore, we show that different blending effects can be learned by an intuitive control of the augmentation process, with no other changes required to the model architecture. We demonstrate the superiority of this approach across several datasets in extensive quantitative and qualitative experiments, including human studies, significantly outperforming prior work.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152853748",
                        "name": "A. Brown"
                    },
                    {
                        "authorId": "2084646762",
                        "name": "Cheng-Yang Fu"
                    },
                    {
                        "authorId": "3188342",
                        "name": "Omkar M. Parkhi"
                    },
                    {
                        "authorId": "1685538",
                        "name": "Tamara L. Berg"
                    },
                    {
                        "authorId": "1687524",
                        "name": "A. Vedaldi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Many studies have utilized pre-trained StyleGAN [9, 10] in various fields, such as image encoding [20, 25, 28], discovering latent semantics [4, 21, 23], image editing [1, 22] and transfer learning [7, 16, 17]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "35770989601f6f0d54452b4b46e64018bd468e87",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-14079",
                    "ArXiv": "2204.14079",
                    "DOI": "10.48550/arXiv.2204.14079",
                    "CorpusId": 248476247
                },
                "corpusId": 248476247,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/35770989601f6f0d54452b4b46e64018bd468e87",
                "title": "Fix the Noise: Disentangling Source Feature for Transfer Learning of StyleGAN",
                "abstract": "Transfer learning of StyleGAN has recently shown great potential to solve diverse tasks, especially in domain translation. Previous methods utilized a source model by swapping or freezing weights during transfer learning, however, they have limitations on visual quality and controlling source features. In other words, they require additional models that are computationally demanding and have restricted control steps that prevent a smooth transition. In this paper, we propose a new approach to overcome these limitations. Instead of swapping or freezing, we introduce a simple feature matching loss to improve generation quality. In addition, to control the degree of source features, we train a target model with the proposed strategy, FixNoise, to preserve the source features only in a disentangled subspace of a target feature space. Owing to the disentangled feature space, our method can smoothly control the degree of the source features in a single model. Extensive experiments demonstrate that the proposed method can generate more consistent and realistic images than previous works.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115276299",
                        "name": "Dong-Yeon Lee"
                    },
                    {
                        "authorId": "2163967832",
                        "name": "Jae Young Lee"
                    },
                    {
                        "authorId": "2129311595",
                        "name": "Doyeon Kim"
                    },
                    {
                        "authorId": "2149220512",
                        "name": "Jaehyun Choi"
                    },
                    {
                        "authorId": "1769295",
                        "name": "Junmo Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "On the other hand, the unsupervised approach (Shen & Zhou, 2021; H\u00e4rk\u00f6nen et al., 2020) explores the principal components of the sampled latent codes and observes if these codes correspond to semantically meaningful editing directions.",
                "Smile Age Gender Glass InterfaceGAN 0.0515 0.1294 0.1225 0.0916 GANspace 0.1081 0.0975 0.0507 0.1420 Ours 0.0047 0.0660 0.0491 0.0279\nIdentity-agnostic Analysis It is essential to preserve the identity in face editing.",
                "We consider the following two methods: InterfaceGAN (Shen et al., 2020b) and GANspace (H\u00e4rk\u00f6nen et al., 2020).",
                "InterfaceGAN is a supervised disentangle method obtaining edit directions from trained SVMs, while GANspace is an unsupervised disentangle method that discovers edit directions from the principal components of sampled latent codes."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "9586167c81cfd2615c74375512ca9684029631e7",
                "externalIds": {
                    "ArXiv": "2204.12696",
                    "DBLP": "journals/corr/abs-2204-12696",
                    "DOI": "10.48550/arXiv.2204.12696",
                    "CorpusId": 248405964
                },
                "corpusId": 248405964,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9586167c81cfd2615c74375512ca9684029631e7",
                "title": "Grasping the Arrow of Time from the Singularity: Decoding Micromotion in Low-dimensional Latent Spaces from StyleGAN",
                "abstract": "The disentanglement of StyleGAN latent space has paved the way for realistic and controllable image editing, but does StyleGAN know anything about temporal motion, as it was only trained on static images? To study the motion features in the latent space of StyleGAN, in this paper, we hypothesize and demonstrate that a series of meaningful, natural, and versatile small, local movements (referred to as\"micromotion\", such as expression, head movement, and aging effect) can be represented in low-rank spaces extracted from the latent space of a conventionally pre-trained StyleGAN-v2 model for face generation, with the guidance of proper\"anchors\"in the form of either short text or video clips. Starting from one target face image, with the editing direction decoded from the low-rank space, its micromotion features can be represented as simple as an affine transformation over its latent feature. Perhaps more surprisingly, such micromotion subspace, even learned from just single target face, can be painlessly transferred to other unseen face images, even those from vastly different domains (such as oil painting, cartoon, and sculpture faces). It demonstrates that the local feature geometry corresponding to one type of micromotion is aligned across different face subjects, and hence that StyleGAN-v2 is indeed\"secretly\"aware of the subject-disentangled feature variations caused by that micromotion. We present various successful examples of applying our low-dimensional micromotion subspace technique to directly and effortlessly manipulate faces, showing high robustness, low computational overhead, and impressive domain transferability. Our codes are available at https://github.com/wuqiuche/micromotion-StyleGAN.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2112250365",
                        "name": "Qiucheng Wu"
                    },
                    {
                        "authorId": null,
                        "name": "Yifan Jiang"
                    },
                    {
                        "authorId": "14737712",
                        "name": "Junru Wu"
                    },
                    {
                        "authorId": "37833805",
                        "name": "Kai Wang"
                    },
                    {
                        "authorId": "2149338333",
                        "name": "Gong Zhang"
                    },
                    {
                        "authorId": "48667025",
                        "name": "Humphrey Shi"
                    },
                    {
                        "authorId": "2969311",
                        "name": "Zhangyang Wang"
                    },
                    {
                        "authorId": "2122374354",
                        "name": "Shiyu Chang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[19] presented GANSpace which collected a set of latent codes and conducted PCA [9] on them to obtain principal components as primary directions in the latent space."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "53d639bf0d2f04db4d399c3cd9e173d3283a7097",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-12678",
                    "ArXiv": "2204.12678",
                    "DOI": "10.1109/IJCNN55064.2022.9892738",
                    "CorpusId": 248405606
                },
                "corpusId": 248405606,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/53d639bf0d2f04db4d399c3cd9e173d3283a7097",
                "title": "Optimized latent-code selection for explainable conditional text-to-image GANs",
                "abstract": "The task of text-to-image generation has achieved remarkable progress due to the advances in conditional generative adversarial networks (GANs). However, existing conditional text-to-image GANs approaches mostly concentrate on improving both image quality and semantic relevance but ignore the explainability of the model which plays a vital role in real-world applications. In this paper, we present a variety of techniques to take a deep look into the latent space and semantic space of a conditional text-to-image GANs model. We introduce pairwise linear interpolation of latent codes and \u2018linguistic\u2019 linear interpolation to study what the model has learned within the latent space and \u2018linguistic\u2019 embeddings. Subsequently, we extend linear interpolation to triangular interpolation conditioned on three corners to further analyze the model. After that, we build a Good/Bad data set containing unsuccessfully and successfully synthesized samples and corresponding latent codes for the image-quality research. Based on this data set, we propose a framework for finding good latent codes by utilizing a linear SVM. Experimental results on the recent DiverGAN generator trained on two benchmark data sets qualitatively prove the effectiveness of our presented techniques, with a better than 94% accuracy in predicting Good/Bad classes for latent vectors. The Good/Bad data set is publicly available at https://zenodo.org/record/5850224#.YeGMwP7MKUk.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48806403",
                        "name": "Zhenxing Zhang"
                    },
                    {
                        "authorId": "1799278",
                        "name": "Lambert Schomaker"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Note that unsupervised methods (e.g., GANSpace, SeFa) might fail to edit unnatural attributes (i.e., glass) due to the lack of supervision from attribute labels.",
                "Both supervised [24,3,14] and unsupervised [12,27,25] approaches are employed to search for meaningful editing directions.",
                ", GANSpace[12], and SeFa [25]) on the estimated latent codes from both encoders.",
                "We show comparison with different editing methods including InterfaceGAN [24], AdvStyle [30], GANSpace[12], and SeFa [25].",
                "For instance, GANSpace [12] and SeFa [25] adopt principal components analysis (PCA) and eigenvector decomposition to search for editing directions respectively."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1e097b60e2a538179fde884ce0055492da53aefd",
                "externalIds": {
                    "ArXiv": "2204.12530",
                    "DBLP": "journals/corr/abs-2204-12530",
                    "DOI": "10.48550/arXiv.2204.12530",
                    "CorpusId": 248405696
                },
                "corpusId": 248405696,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1e097b60e2a538179fde884ce0055492da53aefd",
                "title": "Expanding the Latent Space of StyleGAN for Real Face Editing",
                "abstract": "Recently, a surge of face editing techniques have been proposed to employ the pretrained StyleGAN for semantic manipulation. To successfully edit a real image, one must first convert the input image into StyleGAN's latent variables. However, it is still challenging to find latent variables, which have the capacity for preserving the appearance of the input subject (e.g., identity, lighting, hairstyles) as well as enabling meaningful manipulations. In this paper, we present a method to expand the latent space of StyleGAN with additional content features to break down the trade-off between low-distortion and high-editability. Specifically, we proposed a two-branch model, where the style branch first tackles the entanglement issue by the sparse manipulation of latent codes, and the content branch then mitigates the distortion issue by leveraging the content and appearance details from the input image. We confirm the effectiveness of our method using extensive qualitative and quantitative experiments on real face editing and reconstruction tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2119042498",
                        "name": "Yindong Yu"
                    },
                    {
                        "authorId": "2182496",
                        "name": "Kamran Ghasedi"
                    },
                    {
                        "authorId": "1775992",
                        "name": "Hsiang-Tao Wu"
                    },
                    {
                        "authorId": "2109732576",
                        "name": "Jiaolong Yang"
                    },
                    {
                        "authorId": "2072474095",
                        "name": "Tong Xi"
                    },
                    {
                        "authorId": "2163522916",
                        "name": "Fu Yun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As for image manipulation, studies explore the capability of attribute disentanglement in the latent space with supervised [2, 33, 79] and unsupervised [29, 80, 87, 94] networks."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e4d66b15fce00531b96af6330238301ebbb76291",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-11823",
                    "ArXiv": "2204.11823",
                    "DOI": "10.48550/arXiv.2204.11823",
                    "CorpusId": 248377018
                },
                "corpusId": 248377018,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/e4d66b15fce00531b96af6330238301ebbb76291",
                "title": "StyleGAN-Human: A Data-Centric Odyssey of Human Generation",
                "abstract": "Unconditional human image generation is an important task in vision and graphics, which enables various applications in the creative industry. Existing studies in this field mainly focus on\"network engineering\"such as designing new components and objective functions. This work takes a data-centric perspective and investigates multiple critical aspects in\"data engineering\", which we believe would complement the current practice. To facilitate a comprehensive study, we collect and annotate a large-scale human image dataset with over 230K samples capturing diverse poses and textures. Equipped with this large dataset, we rigorously investigate three essential factors in data engineering for StyleGAN-based human generation, namely data size, data distribution, and data alignment. Extensive experiments reveal several valuable observations w.r.t. these aspects: 1) Large-scale data, more than 40K images, are needed to train a high-fidelity unconditional human generation model with vanilla StyleGAN. 2) A balanced training set helps improve the generation quality with rare face poses compared to the long-tailed counterpart, whereas simply balancing the clothing texture distribution does not effectively bring an improvement. 3) Human GAN models with body centers for alignment outperform models trained using face centers or pelvis points as alignment anchors. In addition, a model zoo and human editing applications are demonstrated to facilitate future research in the community.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145299345",
                        "name": "Jianglin Fu"
                    },
                    {
                        "authorId": "1509385629",
                        "name": "Shikai Li"
                    },
                    {
                        "authorId": "2127773416",
                        "name": "Yuming Jiang"
                    },
                    {
                        "authorId": "41016969",
                        "name": "Kwan-Yee Lin"
                    },
                    {
                        "authorId": "144461220",
                        "name": "Chen Qian"
                    },
                    {
                        "authorId": "1717179",
                        "name": "Chen Change Loy"
                    },
                    {
                        "authorId": "2110050420",
                        "name": "Wayne Wu"
                    },
                    {
                        "authorId": "2117940996",
                        "name": "Ziwei Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Work in this field has focused on uncovering semantic directions, either in a supervised [22, 31, 52] or unsupervised way [29], or on spatially editing images using GANs [76, 7, 44, 9]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "88f045ad6e19e31330bdf66d0af2ae65940df73e",
                "externalIds": {
                    "DBLP": "conf/3dim/DastjerdiHEKL22",
                    "ArXiv": "2204.07286",
                    "DOI": "10.1109/3DV57658.2022.00059",
                    "CorpusId": 248218630
                },
                "corpusId": 248218630,
                "publicationVenue": {
                    "id": "4b02e809-1c26-4203-b9ba-311a418f664b",
                    "name": "International Conference on 3D Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf 3D Vis",
                        "3DV"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/88f045ad6e19e31330bdf66d0af2ae65940df73e",
                "title": "Guided Co-Modulated GAN for 360\u00b0 Field of View Extrapolation",
                "abstract": "We propose a method to extrapolate a 360\u00b0 field of view from a single image that allows for user-controlled synthesis of the out-painted content. To do so, we propose improvements to an existing GAN-based in-painting architecture for out-painting panoramic image representation. Our method obtains state-of-the-art results and outperforms previous methods on standard image quality metrics. To allow controlled synthesis of out-painting, we introduce a novel guided co-modulation framework, which drives the image generation process with a common pretrained discriminative model. Doing so maintains the high visual quality of generated panoramas while enabling user-controlled semantic content in the extrapolated field of view. We demonstrate the state-of-the-art results of our method on field of view extrapolation both qualitatively and quantitatively, providing thorough analysis of our novel editing capabilities. Finally, we demonstrate that our approach benefits the photorealistic virtual insertion of highly glossy objects in photographs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2093581271",
                        "name": "Mohammad Reza Karimi Dastjerdi"
                    },
                    {
                        "authorId": "1403980495",
                        "name": "Yannick Hold-Geoffroy"
                    },
                    {
                        "authorId": "2813054",
                        "name": "Jonathan Eisenmann"
                    },
                    {
                        "authorId": "52115952",
                        "name": "Siavash Khodadadeh"
                    },
                    {
                        "authorId": "144430305",
                        "name": "Jean-Fran\u00e7ois Lalonde"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "91355f13e8be9e4494c5e6f43dfb6457dcacb8cf",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-07075",
                    "ArXiv": "2204.07075",
                    "DOI": "10.1016/j.specom.2023.02.005",
                    "CorpusId": 248178016
                },
                "corpusId": 248178016,
                "publicationVenue": {
                    "id": "e819bc36-457b-4018-82c7-cd1c01850557",
                    "name": "Speech Communication",
                    "type": "journal",
                    "alternate_names": [
                        "Speech Commun"
                    ],
                    "issn": "0167-6393",
                    "url": "https://www.journals.elsevier.com/speech-communication",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/01676393"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/91355f13e8be9e4494c5e6f43dfb6457dcacb8cf",
                "title": "Learning and controlling the source-filter representation of speech with a variational autoencoder",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2162466285",
                        "name": "Samir Sadok"
                    },
                    {
                        "authorId": "1996023",
                        "name": "Simon Leglaive"
                    },
                    {
                        "authorId": "1780746",
                        "name": "Laurent Girin"
                    },
                    {
                        "authorId": "1382657362",
                        "name": "Xavier Alameda-Pineda"
                    },
                    {
                        "authorId": "2162473370",
                        "name": "Renaud S'eguier"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our framework is motivated by the following observation [1, 13] \u2013 objects generated by BigGAN, despite originating from different domains, share high content correspondences when generated from the same latent code."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "2095aaa8ea71fc47bd4cfe0500b1c47cb157443d",
                "externalIds": {
                    "DBLP": "conf/cvpr/YangJ0L22a",
                    "ArXiv": "2204.03641",
                    "DOI": "10.1109/CVPR52688.2022.01779",
                    "CorpusId": 248006260
                },
                "corpusId": 248006260,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2095aaa8ea71fc47bd4cfe0500b1c47cb157443d",
                "title": "Unsupervised Image-to-Image Translation with Generative Prior",
                "abstract": "Unsupervised image-to-image translation aims to learn the translation between two visual domains without paired data. Despite the recent progress in image translation models, it remains challenging to build mappings between complex domains with drastic visual discrepancies. In this work, we present a novel framework, Generative Priorguided UNsupervised Image-to-image Translation (GP-UNIT), to improve the overall quality and applicability of the translation algorithm. Our key insight is to leverage the generative prior from pre-trained class-conditional GANs (e.g., BigGAN) to learn rich content correspondences across various domains. We propose a novel coarse-to-fine scheme: we first distill the generative prior to capture a robust coarse-level content representation that can link objects at an abstract semantic level, based on which finelevel content features are adaptively learned for more accurate multi-level content correspondences. Extensive experiments demonstrate the superiority of our versatile framework over state-of-the-art methods in robust, high-quality and diversified translations, even for challenging and distant domains. Code is available at https://github.com/williamyang1991/GP-UNIT.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2159711748",
                        "name": "Shuai Yang"
                    },
                    {
                        "authorId": "94106850",
                        "name": "Liming Jiang"
                    },
                    {
                        "authorId": "2117940996",
                        "name": "Ziwei Liu"
                    },
                    {
                        "authorId": "1717179",
                        "name": "Chen Change Loy"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Most commonly, recent works perform a latent space traversal of the GAN\u2019s learned manifold for controlling a speciic attribute of interest such as age, gender, and expression [Abdal et al. 2020b; Goetschalckx et al. 2019; H\u00e4rk\u00f6nen et al. 2020; Jahanian et al. 2020; Shen et al. 2020; Shen and Zhou 2020; Voynov and Babenko 2020; Wu et al. 2020].",
                "For example, as shall be shown, pairing StyleFusion with GANSpace [H\u00e4rk\u00f6nen et al. 2020] or StyleCLIP [Patashnik et al.",
                "2020], GANSpace [H\u00e4rk\u00f6nen et al. 2020], and StyleCLIP [Patashnik et al.",
                "For example, as shall be shown, pairing StyleFusion with GANSpace [H\u00e4rk\u00f6nen et al. 2020] or StyleCLIP [Patashnik et al. 2021] leverages their diverse manipulations while ensuring that the resulting edits alter only the desired semantic regions.",
                "\u2026traversal of the GAN\u2019s learned manifold for controlling a specific attribute of interest such as age, gender, and expression [Abdal et al. 2020b; Goetschalckx et al. 2019; H\u00e4rk\u00f6nen et al. 2020; Jahanian et al. 2020; Shen et al. 2020; Shen and Zhou 2020; Voynov and Babenko 2020; Wu et al. 2020].",
                "For InterFaceGAN and GANSpace we use their official implementation and latent directions, while for StyleCLIP we adapt the official implementation to train a latent mapper which manipulates only the desired image region.",
                "In Figure 14 we show the advantage of using StyleFusion\u2019s disentangled representation when editing images using three latent traversal editing methods: InterFaceGAN [Shen et al. 2020], GANSpace [H\u00e4rk\u00f6nen et al. 2020], and StyleCLIP [Patashnik et al. 2021].",
                "We then manipulate the resulting code to obtain the edited representation wedit (e.g., via a traversal along a latent path learned by InterFaceGAN or GANSpace)."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "281415c25b16031a314e4e7a0cc7e1dd53db6e0c",
                "externalIds": {
                    "DBLP": "journals/tog/KafriPAC22",
                    "DOI": "10.1145/3527168",
                    "CorpusId": 247843679
                },
                "corpusId": 247843679,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/281415c25b16031a314e4e7a0cc7e1dd53db6e0c",
                "title": "StyleFusion: Disentangling Spatial Segments in StyleGAN-Generated Images",
                "abstract": "We present StyleFusion, a new mapping architecture for StyleGAN, which takes as input a number of latent codes and fuses them into a single style code. Inserting the resulting style code into a pre-trained StyleGAN generator results in a single harmonized image in which each semantic region is controlled by one of the input latent codes. Effectively, StyleFusion yields a disentangled representation of the image, providing fine-grained control over each region of the generated image. Moreover, to help facilitate global control over the generated image, a special input latent code is incorporated into the fused representation. StyleFusion operates in a hierarchical manner, where each level is tasked with learning to disentangle a pair of image regions (e.g., the car body and wheels). The resulting learned disentanglement allows one to modify both local, fine-grained semantics (e.g., facial features) as well as more global features (e.g., pose and background), providing improved flexibility in the synthesis process. As a natural extension, StyleFusion allows one to perform semantically-aware cross-image mixing of regions that are not necessarily aligned. Finally, we demonstrate how StyleFusion can be paired with existing editing techniques to more faithfully constrain the edit to the user\u2019s region of interest. Code is available at: https://github.com/OmerKafri/StyleFusion.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2119539655",
                        "name": "Omer Kafri"
                    },
                    {
                        "authorId": "2819477",
                        "name": "Or Patashnik"
                    },
                    {
                        "authorId": "1850630812",
                        "name": "Yuval Alaluf"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "901f91006e113db7df64e155c40ed207f0056980",
                "externalIds": {
                    "DOI": "10.1016/j.cirp.2022.03.016",
                    "CorpusId": 248194174
                },
                "corpusId": 248194174,
                "publicationVenue": {
                    "id": "64fd23a1-3c84-4991-afdb-a731ad1abf24",
                    "name": "CIRP annals",
                    "type": "journal",
                    "alternate_names": [
                        "CIRP Annals",
                        "CIRP Ann",
                        "CIRP ann"
                    ],
                    "issn": "0007-8506",
                    "url": "https://www.journals.elsevier.com/cirp-annals",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00078506"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/901f91006e113db7df64e155c40ed207f0056980",
                "title": "Metrologically interpretable feature extraction for industrial machine vision using generative deep learning",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146132206",
                        "name": "R. H. Schmitt"
                    },
                    {
                        "authorId": "103763981",
                        "name": "Dominik Wolfschl\u00e4ger"
                    },
                    {
                        "authorId": "1397267970",
                        "name": "Evelina Masliankova"
                    },
                    {
                        "authorId": "31198773",
                        "name": "B. Montavon"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Previous works aiming at finding semantic directions can typically be divided into the supervised ones [12, 32], the self-supervised ones [19, 35], and the unsupervised ones [15, 34]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "bc6c2e432840e1ac17738eda4745e9cce3753030",
                "externalIds": {
                    "DBLP": "conf/cvpr/XuYJWZLDW22",
                    "ArXiv": "2203.17266",
                    "DOI": "10.1109/CVPR52688.2022.00753",
                    "CorpusId": 247839345
                },
                "corpusId": 247839345,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bc6c2e432840e1ac17738eda4745e9cce3753030",
                "title": "TransEditor: Transformer-Based Dual-Space GAN for Highly Controllable Facial Editing",
                "abstract": "Recent advances like StyleGAN have promoted the growth of controllable facial editing. To address its core challenge of attribute decoupling in a single latent space, attempts have been made to adopt dual-space GAN for better disentanglement of style and content representations. Nonetheless, these methods are still incompetent to obtain plausible editing results with high controllability, especially for complicated attributes. In this study, we highlight the importance of interaction in a dual-space GAN for more controllable editing. We propose TransEditor, a novel Transformer-based framework to enhance such interaction. Besides, we develop a new dual-space editing and inversion strategy to provide additional editing flexibility. Extensive experiments demonstrate the superiority of the proposed framework in image quality and editing capability, suggesting the effectiveness of TransEditor for highly controllable facial editing. Code and models are publicly available at https://github.com/BillyXYB/TransEditor.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2143558044",
                        "name": "Yanbo Xu"
                    },
                    {
                        "authorId": "120286258",
                        "name": "Yueqin Yin"
                    },
                    {
                        "authorId": "94106850",
                        "name": "Liming Jiang"
                    },
                    {
                        "authorId": "2115911378",
                        "name": "Qianyi Wu"
                    },
                    {
                        "authorId": null,
                        "name": "Chengyao Zheng"
                    },
                    {
                        "authorId": "1717179",
                        "name": "Chen Change Loy"
                    },
                    {
                        "authorId": "144445937",
                        "name": "Bo Dai"
                    },
                    {
                        "authorId": "2110050420",
                        "name": "Wayne Wu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Next, the resulting latent code can be semantically edited using a wide range of methods [2, 21, 36, 43, 49]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "3b732504d03ae58e955d11d3aae97406431ad41e",
                "externalIds": {
                    "DBLP": "journals/tog/NitzanAHLYGMPC22",
                    "ArXiv": "2203.17272",
                    "DOI": "10.1145/3550454.3555436",
                    "CorpusId": 254097227
                },
                "corpusId": 254097227,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3b732504d03ae58e955d11d3aae97406431ad41e",
                "title": "MyStyle",
                "abstract": "We introduce MyStyle, a personalized deep generative prior trained with a few shots of an individual. MyStyle allows to reconstruct, enhance and edit images of a specific person, such that the output is faithful to the person's key facial characteristics. Given a small reference set of portrait images of a person (~ 100), we tune the weights of a pretrained StyleGAN face generator to form a local, low-dimensional, personalized manifold in the latent space. We show that this manifold constitutes a personalized region that spans latent codes associated with diverse portrait images of the individual. Moreover, we demonstrate that we obtain a personalized generative prior, and propose a unified approach to apply it to various ill-posed image enhancement problems, such as inpainting and super-resolution, as well as semantic editing. Using the personalized generative prior we obtain outputs that exhibit high-fidelity to the input images and are also faithful to the key facial characteristics of the individual in the reference set. We demonstrate our method with fair-use images of numerous widely recognizable individuals for whom we have the prior knowledge for a qualitative evaluation of the expected outcome. We evaluate our approach against few-shots baselines and show that our personalized prior, quantitatively and qualitatively, outperforms state-of-the-art alternatives.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1702992975",
                        "name": "Yotam Nitzan"
                    },
                    {
                        "authorId": "3451442",
                        "name": "Kfir Aberman"
                    },
                    {
                        "authorId": "8159641",
                        "name": "Qiurui He"
                    },
                    {
                        "authorId": "2207191",
                        "name": "O. Liba"
                    },
                    {
                        "authorId": "2065978700",
                        "name": "Michal Yarom"
                    },
                    {
                        "authorId": "52164591",
                        "name": "Yossi Gandelsman"
                    },
                    {
                        "authorId": "2138834",
                        "name": "Inbar Mosseri"
                    },
                    {
                        "authorId": "1782328",
                        "name": "Y. Pritch"
                    },
                    {
                        "authorId": "1401985474",
                        "name": "Daniel Cohen-Or"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Studies show that disentanglement of semantic attributes can be achieved by carefully searching for latent directions learned by GANs [12,41,50], but all attributes being factorized have to be identified by humans.",
                "Recent studies [12, 41, 50] show that rich semantically meaningful directions (e."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "cddf135b9f0be392b647deff9d4ab0f0fd25ff4b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-16521",
                    "ArXiv": "2203.16521",
                    "DOI": "10.1109/CVPR52688.2022.00977",
                    "CorpusId": 247793754
                },
                "corpusId": 247793754,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/cddf135b9f0be392b647deff9d4ab0f0fd25ff4b",
                "title": "CoordGAN: Self-Supervised Dense Correspondences Emerge from GANs",
                "abstract": "Recent advances show that Generative Adversarial Networks (GANs) can synthesize images with smooth variations along semantically meaningful latent directions, such as pose, expression, layout, etc. While this indicates that GANs implicitly learn pixel-level correspondences across images, few studies explored how to extract them explicitly. In this work, we introduce Coordinate GAN (CoordGAN), a structure-texture disentangled GAN that learns a dense correspondence map for each generated image. We represent the correspondence maps of different images as warped coordinate frames transformed from a canonical coordinate frame, i.e., the correspondence map, which describes the structure (e.g., the shape of a face), is controlled via a transformation. Hence, finding correspondences boils down to locating the same coordinate in different correspondence maps. In CoordGAN, we sample a transformation to represent the structure of a synthesized instance, while an independent texture branch is responsible for rendering appearance details orthogonal to the structure. Our approach can also extract dense correspondence maps for real images by adding an encoder on top of the generator. We quantitatively demonstrate the quality of the learned dense correspondences through segmentation mask transfer on multiple datasets. We also show that the proposed generator achieves better structure and texture disentanglement compared to existing approaches. Project page: https://jitengmu.github.io/CoordGAN/",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9075977",
                        "name": "Jiteng Mu"
                    },
                    {
                        "authorId": "24817039",
                        "name": "Shalini De Mello"
                    },
                    {
                        "authorId": "1751019",
                        "name": "Zhiding Yu"
                    },
                    {
                        "authorId": "1699559",
                        "name": "N. Vasconcelos"
                    },
                    {
                        "authorId": "39849136",
                        "name": "X. Wang"
                    },
                    {
                        "authorId": "1690538",
                        "name": "J. Kautz"
                    },
                    {
                        "authorId": "2391885",
                        "name": "Sifei Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Research such as [20, 31] explore the latent-space of StyleGAN to identify the interpretable semantic directions that control attributes such as aging, smile, gender, pose, etc."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "7aeb8b24b2aa2565830470b0cfe57bfd2c1bb440",
                "externalIds": {
                    "ArXiv": "2203.14512",
                    "CorpusId": 256846532
                },
                "corpusId": 256846532,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7aeb8b24b2aa2565830470b0cfe57bfd2c1bb440",
                "title": "Expressive Talking Head Video Encoding in StyleGAN2 Latent-Space",
                "abstract": "While the recent advances in research on video reenactment have yielded promising results, the approaches fall short in capturing the fine, detailed, and expressive facial features (e.g., lip-pressing, mouth puckering, mouth gaping, and wrinkles) which are crucial in generating realistic animated face videos. To this end, we propose an end-to-end expressive face video encoding approach that facilitates data-efficient high-quality video re-synthesis by optimizing low-dimensional edits of a single Identity-latent. The approach builds on StyleGAN2 image inversion and multi-stage non-linear latent-space editing to generate videos that are nearly comparable to input videos. While existing StyleGAN latent-based editing techniques focus on simply generating plausible edits of static images, we automate the latent-space editing to capture the fine expressive facial deformations in a sequence of frames using an encoding that resides in the Style-latent-space (StyleSpace) of StyleGAN2. The encoding thus obtained could be super-imposed on a single Identity-latent to facilitate re-enactment of face videos at $1024^2$. The proposed framework economically captures face identity, head-pose, and complex expressive facial motions at fine levels, and thereby bypasses training, person modeling, dependence on landmarks/ keypoints, and low-resolution synthesis which tend to hamper most re-enactment approaches. The approach is designed with maximum data efficiency, where a single $W+$ latent and 35 parameters per frame enable high-fidelity video rendering. This pipeline can also be used for puppeteering (i.e., motion transfer).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "31668191",
                        "name": "T. Oorloff"
                    },
                    {
                        "authorId": "1964574",
                        "name": "Y. Yacoob"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026has been studied extensively and enabled image editing using linear and non-linear latent space arithmetic [Abdal et al. 2019, 2020, 2021; H\u00e4rk\u00f6nen et al. 2020; Pumarola et al. 2019; Roich et al. 2021; Shen et al. 2020; Tov et al. 2021; Zhu et al. 2020]; local, semantically-aware edits\u2026",
                "\u2026on generative adversarial network (GAN) inversion manipulate the latent space of a pre-trained 2D GAN [Choi et al. 2020; Karras et al. 2018, 2021, 2019, 2020;Wang et al. 2018] to adjust expression, pose, lighting, or other attributes [H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020; Tewari et al. 2020a,b].",
                "While StyleFlow or other latent space interpolation techniques [H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020] can be used with 2D GANs to control pose via conditional attributes, the results are limited in pose range and suffer from view inconsistencies and changes to the subject likeness during pose\u2026",
                "These limitations could be alleviated by using PCA-based editing techniques [H\u00e4rk\u00f6nen et al. 2020] to synthesize blinking textures, more sophisticated face tracking models, or improving the GAN training procedure to disentangle the subject expression, allowing explicit control similar to what is\u2026"
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "138d01f3f699f99ea477c4192b8734b1bbff642b",
                "externalIds": {
                    "ArXiv": "2203.13441",
                    "DBLP": "journals/corr/abs-2203-13441",
                    "DOI": "10.48550/arXiv.2203.13441",
                    "CorpusId": 247748597
                },
                "corpusId": 247748597,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/138d01f3f699f99ea477c4192b8734b1bbff642b",
                "title": "3D GAN Inversion for Controllable Portrait Image Animation",
                "abstract": "Millions of images of human faces are captured every single day; but these photographs portray the likeness of an individual with a fixed pose, expression, and appearance. Portrait image animation enables the post-capture adjustment of these attributes from a single image while maintaining a photorealistic reconstruction of the subject's likeness or identity. Still, current methods for portrait image animation are typically based on 2D warping operations or manipulations of a 2D generative adversarial network (GAN) and lack explicit mechanisms to enforce multi-view consistency. Thus these methods may significantly alter the identity of the subject, especially when the viewpoint relative to the camera is changed. In this work, we leverage newly developed 3D GANs, which allow explicit control over the pose of the image subject with multi-view consistency. We propose a supervision strategy to flexibly manipulate expressions with 3D morphable models, and we show that the proposed method also supports editing appearance attributes, such as age or hairstyle, by interpolating within the latent space of the GAN. The proposed technique for portrait image animation outperforms previous methods in terms of image quality, identity preservation, and pose transfer while also supporting attribute editing.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108720546",
                        "name": "Connor Z. Lin"
                    },
                    {
                        "authorId": "2202838",
                        "name": "David B. Lindell"
                    },
                    {
                        "authorId": "121028414",
                        "name": "Eric Chan"
                    },
                    {
                        "authorId": "1731170",
                        "name": "Gordon Wetzstein"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Moreover, IA-FaceS can find interesting directions at the component level by implementing PCA in the latent space of a specific component, providing users more accurate editing of components and creating more facial expressions.",
                "There are also investigations on finding disentangled directions in GAN\u2019s latent space, including supervised methods [1, 17, 49, 50] and unsupervised methods [6, 8, 10, 18].",
                "Although some attributes can be edited individually [1, 6, 8, 15, 16], the whole-face single-embedding manner limits the flexibility of editing facial components.",
                "Here, we can find many interesting fine-grained directions for component editing by applying PCA [6] to the latent space of each component.",
                "The first stream usually leverages the captured semantics in GAN\u2019s latent space after the generator is trained [1, 6, 29].",
                ") We further apply unsupervised methods [6] to find interpretable directions in the latent space of each component."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "50885d43360eb1ec09a52242653ee1adc8071e73",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-13097",
                    "ArXiv": "2203.13097",
                    "DOI": "10.48550/arXiv.2203.13097",
                    "CorpusId": 247748918,
                    "PubMed": "36481459"
                },
                "corpusId": 247748918,
                "publicationVenue": {
                    "id": "a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                    "name": "Neural Networks",
                    "type": "journal",
                    "alternate_names": [
                        "Neural Netw"
                    ],
                    "issn": "0893-6080",
                    "url": "http://www.elsevier.com/locate/neunet",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/841/description",
                        "http://www.sciencedirect.com/science/journal/08936080"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/50885d43360eb1ec09a52242653ee1adc8071e73",
                "title": "IA-FaceS: A Bidirectional Method for Semantic Face Editing",
                "abstract": "Semantic face editing has achieved substantial progress in recent years. However, existing face editing methods, which often encode the entire image into a single code, still have difficulty in enabling flexible editing while keeping high-fidelity reconstruction. The one-code scheme also brings entangled face manipulations and limited flexibility in editing face components. In this paper, we present IA-FaceS, a bidirectional method for disentangled face attribute manipulation as well as flexible, controllable component editing. We propose to embed images onto two branches: one branch computes high-dimensional component-invariant content embedding for capturing face details, and the other provides low-dimensional component-specific embeddings for component manipulations. The two-branch scheme naturally enables high-quality facial component-level editing while keeping faithful reconstruction with details. Moreover, we devise a component adaptive modulation (CAM) module, which integrates component-specific guidance into the decoder and successfully disentangles highly-correlated face components. The single-eye editing is developed for the first time without editing face masks or sketches. According to the experimental results, IA-FaceS establishes a good balance between maintaining image details and performing flexible face manipulation. Both quantitative and qualitative results indicate that the proposed method outperforms the existing methods in reconstruction, face attribute manipulation, and component transfer. We release the code and weights at: https://github.com/CMACH508/IA-FaceS.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "151492345",
                        "name": "Wenjing Huang"
                    },
                    {
                        "authorId": "1701972",
                        "name": "Shikui Tu"
                    },
                    {
                        "authorId": "2109329726",
                        "name": "Lei Xu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Existing studies on GAN interpretation have affirmed that, pre-trained GAN models own great potential in a range of downstream applications, such as object classification [9, 45], semantic segmentation [49], video generation [23], and image editing [4, 11, 13, 20, 28, 31, 38, 39, 42, 46, 50]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "69a8f7a27a56c32df453988938c09f1dbcd4245b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-11105",
                    "ArXiv": "2203.11105",
                    "DOI": "10.48550/arXiv.2203.11105",
                    "CorpusId": 247594338
                },
                "corpusId": 247594338,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/69a8f7a27a56c32df453988938c09f1dbcd4245b",
                "title": "High-fidelity GAN Inversion with Padding Space",
                "abstract": "Inverting a Generative Adversarial Network (GAN) facilitates a wide range of image editing tasks using pre-trained generators. Existing methods typically employ the latent space of GANs as the inversion space yet observe the insufficient recovery of spatial details. In this work, we propose to involve the padding space of the generator to complement the latent space with spatial information. Concretely, we replace the constant padding (e.g., usually zeros) used in convolution layers with some instance-aware coefficients. In this way, the inductive bias assumed in the pre-trained model can be appropriately adapted to fit each individual image. Through learning a carefully designed encoder, we manage to improve the inversion quality both qualitatively and quantitatively, outperforming existing alternatives. We then demonstrate that such a space extension barely affects the native GAN manifold, hence we can still reuse the prior knowledge learned by GANs for various downstream applications. Beyond the editing tasks explored in prior arts, our approach allows a more flexible image manipulation, such as the separate control of face contour and facial details, and enables a novel editing manner where users can customize their own manipulations highly efficiently.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2083548117",
                        "name": "Qingyan Bai"
                    },
                    {
                        "authorId": "121983635",
                        "name": "Yinghao Xu"
                    },
                    {
                        "authorId": "47054925",
                        "name": "Jiapeng Zhu"
                    },
                    {
                        "authorId": "50875615",
                        "name": "Weihao Xia"
                    },
                    {
                        "authorId": "2108585311",
                        "name": "Yujiu Yang"
                    },
                    {
                        "authorId": "2117687899",
                        "name": "Yujun Shen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Compared with prior works which present manipulation results on generated images of BigGAN via manipulating attribute vectors in the latent space [13, 20, 52, 56], our method demonstrates that simply altering only one feature channel could achieve this.",
                "Some attempts are made to also analyze class conditional models [13,20,45,52,56], but they still target the latent space, leaving it unclear how the generator leverages the categorical information."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "result"
            ],
            "citingPaper": {
                "paperId": "903834933012cc5f7e73853b2a917fed76db2b06",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-11173",
                    "ArXiv": "2203.11173",
                    "DOI": "10.48550/arXiv.2203.11173",
                    "CorpusId": 247594513
                },
                "corpusId": 247594513,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/903834933012cc5f7e73853b2a917fed76db2b06",
                "title": "Interpreting Class Conditional GANs with Channel Awareness",
                "abstract": "Understanding the mechanism of generative adversarial networks (GANs) helps us better use GANs for downstream applications. Existing efforts mainly target interpreting unconditional models, leaving it less explored how a conditional GAN learns to render images regarding various categories. This work fills in this gap by investigating how a class conditional generator unifies the synthesis of multiple classes. For this purpose, we dive into the widely used class-conditional batch normalization (CCBN), and observe that each feature channel is activated at varying degrees given different categorical embeddings. To describe such a phenomenon, we propose channel awareness, which quantitatively characterizes how a single channel contributes to the final synthesis. Extensive evaluations and analyses on the BigGAN model pre-trained on ImageNet reveal that only a subset of channels is primarily responsible for the generation of a particular category, similar categories (e.g., cat and dog) usually get related to some same channels, and some channels turn out to share information across all classes. For good measure, our algorithm enables several novel applications with conditional GANs. Concretely, we achieve (1) versatile image editing via simply altering a single channel and manage to (2) harmoniously hybridize two different classes. We further verify that the proposed channel awareness shows promising potential in (3) segmenting the synthesized image and (4) evaluating the category-wise synthesis performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2154314501",
                        "name": "Yin-Yin He"
                    },
                    {
                        "authorId": "2117993039",
                        "name": "Zhiyi Zhang"
                    },
                    {
                        "authorId": "47054925",
                        "name": "Jiapeng Zhu"
                    },
                    {
                        "authorId": "2117687899",
                        "name": "Yujun Shen"
                    },
                    {
                        "authorId": "2157737759",
                        "name": "Qifeng Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "component of the latent space [31], or to find a latent space that can edit a meaningful area of generated images by additionally using label information [32] or semantic map [33]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "007257c7a8caf7260f756324ba37e47f363ca3c0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-09301",
                    "ArXiv": "2203.09301",
                    "DOI": "10.1109/TPAMI.2023.3283551",
                    "CorpusId": 247519189,
                    "PubMed": "37352089"
                },
                "corpusId": 247519189,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/007257c7a8caf7260f756324ba37e47f363ca3c0",
                "title": "One-Shot Adaptation of GAN in Just One CLIP",
                "abstract": "There are many recent research efforts to fine-tune a pre-trained generator with a few target images to generate images of a novel domain. Unfortunately, these methods often suffer from overfitting or under-fitting when fine-tuned with a single target image. To address this, here we present a novel single-shot GAN adaptation method through unified CLIP space manipulations. Specifically, our model employs a two-step training strategy: reference image search in the source generator using a CLIP-guided latent optimization, followed by generator fine-tuning with a novel loss function that imposes CLIP space consistency between the source and adapted generators. To further improve the adapted model to produce spatially consistent samples with respect to the source generator, we also propose contrastive regularization for patchwise relationships in the CLIP space. Experimental results show that our model generates diverse outputs with the target texture and outperforms the baseline models both qualitatively and quantitatively. Furthermore, we show that our CLIP space manipulation strategy allows more effective attribute editing.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "116153377",
                        "name": "Gihyun Kwon"
                    },
                    {
                        "authorId": "30547794",
                        "name": "Jong-Chul Ye"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some of these works aim to discover specific directions such as expression or gender using supervision [24], while others propose unsupervised approaches to identify semantically meaningful directions [8, 31].",
                "We used the official implementations for both methods2 and obtained the top 10 principal components for Ganspace\n2http://github.com/harskish/ganspace, http:// github.com/genforce/sefa\nand the top 10 eigenvectors for SeFa methods using the default parameters.",
                "ture of the latent space of StyleGAN2 in a more principled way [8, 24].",
                "As can be seen from Figure 6, our method yields more disentangled and diverse directions compared to Ganspace and SeFa.",
                "For example, while both Ganspace and SeFA change semantics in the input, such as gender, age, eyeglasses, while also changing other semantics such as background, position, highlight at the same time.",
                "Comparison of the top 10 directions for Ganspace [8], SeFa [25] and our method.",
                "We conduct several qualitative experiments to demonstrate the effectiveness of the submodular framework and compare our method to supervised [34] and unsupervised methods [8, 25].",
                "Ganspace [8] uses principal component analysis (PCA) [33] on randomly",
                "On the other hand, unsupervised methods such as [8, 31] find a certain number of directions, but the user has to manually explore what these directions are capable of.",
                "Recent research has shown that the latent space of GANs contains semantically meaningful directions that can be used for editing images in a variety of ways [8, 9, 31].",
                "Ganspace applies PCA to randomly sampled w vectors of StyleGAN2 and uses the resulting principal components as directions.",
                "Ganspace [8] uses principal component analysis (PCA) [33] on randomly\nsampled latent vectors from the intermediate layers of BigGAN and StyleGAN2 and treats the generated principal components as latent directions.",
                "Next, we compare our results with the state-of-theart unsupervised methods Ganspace [8] and SeFa [25]."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "898626c116d3b3af14397168dfd45b196da6f94e",
                "externalIds": {
                    "DBLP": "conf/wacv/SimsarKEY23",
                    "ArXiv": "2203.08516",
                    "DOI": "10.1109/WACV56688.2023.00471",
                    "CorpusId": 247476285
                },
                "corpusId": 247476285,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/898626c116d3b3af14397168dfd45b196da6f94e",
                "title": "Fantastic Style Channels and Where to Find Them: A Submodular Framework for Discovering Diverse Directions in GANs",
                "abstract": "The discovery of interpretable directions in the latent spaces of pre-trained GAN models has recently become a popular topic. In particular, StyleGAN2 has enabled various image generation and manipulation tasks due to its rich and disentangled latent spaces. However, the discovery of such directions is typically made either in a supervised manner, which requires annotated data for each desired manipulation, or in an unsupervised manner, which requires a manual effort to identify the directions. As a result, existing work typically finds only a handful of directions in which controllable edits can be made. In this study, we design a novel submodular framework that finds the most representative and diverse subset of directions in the latent space of StyleGAN2. Our approach takes advantage of the latent space of channel-wise style parameters, so-called stylespace, in which we cluster channels that perform similar manipulations into groups. Our framework promotes diversity by using the notion of clusters and can be efficiently solved with a greedy optimization scheme. We evaluate our framework with qualitative and quantitative experiments and show that our method finds more diverse and disentangled directions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1395808197",
                        "name": "Enis Simsar"
                    },
                    {
                        "authorId": "2145258020",
                        "name": "Umut Kocasari"
                    },
                    {
                        "authorId": "2063799990",
                        "name": "Ezgi Gulperi Er"
                    },
                    {
                        "authorId": "3137679",
                        "name": "Pinar Yanardag"
                    }
                ]
            }
        },
        {
            "contexts": [
                "identified important latent directions based on PCA applied either in latent space or feature space.(27) They showed that important directions in GAN latent spaces can be found by applying PCA in latent space for StyleGAN2 and layer\u2010 wise decomposition of PCA edit directions leads to many interpretable controls."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "9689217e30b9fae1eb4267a7c9202c08d2999de1",
                "externalIds": {
                    "DOI": "10.1002/jor.25325",
                    "CorpusId": 247474601,
                    "PubMed": "35293648"
                },
                "corpusId": 247474601,
                "publicationVenue": {
                    "id": "2f3ee9ce-e8b1-4644-8f19-23c301283578",
                    "name": "Journal of Orthopaedic Research",
                    "type": "journal",
                    "alternate_names": [
                        "J Orthop Res"
                    ],
                    "issn": "0736-0266",
                    "url": "http://www.sciencedirect.com/science/journal/07360266",
                    "alternate_urls": [
                        "https://onlinelibrary.wiley.com/journal/1554527X",
                        "http://onlinelibrary.wiley.com/journal/10.1002/(ISSN)1554-527X"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9689217e30b9fae1eb4267a7c9202c08d2999de1",
                "title": "High\u2010resolution knee plain radiography image synthesis using style generative adversarial network adaptive discriminator augmentation",
                "abstract": "In this retrospective study, 10,000 anteroposterior (AP) radiography of the knee from a single institution was used to create medical data set that are more balanced and cheaper to create. Two types of convolutional networks were used, deep convolutional GAN (DCGAN) and Style GAN Adaptive Discriminator Augmentation (StyleGAN2\u2010ADA). To verify the quality of generated images from StyleGAN2\u2010ADA compared to real ones, the Visual Turing test was conducted by two computer vision experts, two orthopedic surgeons, and a musculoskeletal radiologist. For quantitative analysis, the Fr\u00e9chet inception distance (FID), and principal component analysis (PCA) were used. Generated images reproduced the features of osteophytes, joint space narrowing, and sclerosis. Classification accuracy of the experts was 34%, 43%, 44%, 57%, and 50%. FID between the generated images and real ones was 2.96, which is significantly smaller than another medical data set (BreCaHAD\u2009=\u200915.1). PCA showed that no significant difference existed between the PCs of the real and generated images (p\u2009>\u20090.05). At least 2000 images were required to make reliable images optimally. By performing PCA in latent space, we were able to control the desired PC that show a progression of arthritis. Using a GAN, we were able to generate knee X\u2010ray images that accurately reflected the characteristics of the arthritis progression stage, which neither human experts nor artificial intelligence could discern apart from the real images. In summary, our research opens up the potential to adopt a generative model to synthesize realistic anonymous images that can also solve data scarcity and class inequalities.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144700799",
                        "name": "Gun-Sang Ahn"
                    },
                    {
                        "authorId": "2159078848",
                        "name": "Byung Sun Choi"
                    },
                    {
                        "authorId": "10743891",
                        "name": "Sunho Ko"
                    },
                    {
                        "authorId": "146381734",
                        "name": "C. Jo"
                    },
                    {
                        "authorId": "144033848",
                        "name": "Hyuk-Soo Han"
                    },
                    {
                        "authorId": "2108889987",
                        "name": "M. Lee"
                    },
                    {
                        "authorId": "4057575",
                        "name": "D. Ro"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The meaningful dimensions can be identified by using segmentation-based networks [5], linear subspace models [12], Principal Components Analysis in the activation space [17], or carefully designed disentanglement constraints [28, 33, 38]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6eab96ac818947afba3e8be00aa69d65772df5e0",
                "externalIds": {
                    "ArXiv": "2203.08422",
                    "DBLP": "conf/cvpr/DingHWWJTH22",
                    "DOI": "10.1109/CVPR52688.2022.01091",
                    "CorpusId": 263792502
                },
                "corpusId": 263792502,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6eab96ac818947afba3e8be00aa69d65772df5e0",
                "title": "Attribute Group Editing for Reliable Few-shot Image Generation",
                "abstract": "Few-shot image generation is a challenging task even using the state-of-the-art Generative Adversarial Networks (GANs). Due to the unstable GAN training process and the limited training data, the generated images are often of low quality and low diversity. In this work, we propose a new \u201cediting-based\u201d method, i.e., Attribute Group Editing (AGE), for few-shot image generation. The basic assumption is that any image is a collection of attributes and the editing direction for a specific attribute is shared across all categories. AGE examines the internal representation learned in GANs and identifies semantically meaningful directions. Specifically, the class embedding, i.e., the mean vector of the latent codes from a specific category, is used to represent the category-relevant attributes, and the category-irrelevant attributes are learned globally by Sparse Dictionary Learning on the difference between the sample embedding and the class embedding. Given a GAN well trained on seen categories, diverse images of unseen categories can be synthesized through editing category-irrelevant attributes while keeping category-relevant attributes unchanged. Without re-training the GAN, AGE is capable of not only producing more realistic and diverse images for downstream visual applications with limited data but achieving controllable image editing with interpretable category-irrelevant directions. Code is available at https://github.com/UniBester/AGE.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2256692284",
                        "name": "Guanqi Ding"
                    },
                    {
                        "authorId": null,
                        "name": "Xinzhe Han"
                    },
                    {
                        "authorId": "2119545962",
                        "name": "Shuhui Wang"
                    },
                    {
                        "authorId": "2256802682",
                        "name": "Shuzhe Wu"
                    },
                    {
                        "authorId": null,
                        "name": "Xin Jin"
                    },
                    {
                        "authorId": "2256721330",
                        "name": "Dandan Tu"
                    },
                    {
                        "authorId": "2256640224",
                        "name": "Qingming Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANSpace [11] and Sefa [30] adopt PCA to find the principal directions in W space.",
                "Despite of the great efforts, inverting [1, 2, 28, 31, 40] or editing [3, 11, 29, 30, 35] images for StyleGAN is still challenging due to following reasons.",
                "For cars, we use the directions provided in GANSpace [11] for editing.",
                "For car domain, we apply GANSpace [11] to find the semantic directions.",
                "the pretrained and fixed StyleGAN for downstream tasks becomes a hot research topic, especially in the editing task of image-to-image (I2I) translation [3, 11, 29, 30, 35, 36].",
                "Traditional approaches [11,29,35] assume the linear separations in the latent space for a binary attribute, so inverted code from different images are edited by the same direction."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "da3b3377b2655e331ed8e5eac389aa3e194c8389",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-07932",
                    "ArXiv": "2203.07932",
                    "DOI": "10.1109/CVPR52688.2022.01105",
                    "CorpusId": 247450902
                },
                "corpusId": 247450902,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/da3b3377b2655e331ed8e5eac389aa3e194c8389",
                "title": "Style Transformer for Image Inversion and Editing",
                "abstract": "Existing GAN inversion methods fail to provide latent codes for reliable reconstruction and flexible editing simultaneously. This paper presents a transformer-based image inversion and editing model for pretrained StyleGAN which is not only with less distortions, but also of high quality and flexibility for editing. The proposed model employs a CNN encoder to provide multi-scale image features as keys and values. Meanwhile it regards the style code to be determined for different layers of the generator as queries. It first initializes query tokens as learnable parameters and maps them into W+ space. Then the multi-stage alternate self-and cross-attention are utilized, updating queries with the purpose of inverting the input by the generator. Moreover, based on the inverted code, we investigate the reference-and label-based attribute editing through a pretrained latent classifier, and achieve flexible image-to-image translation with high quality results. Extensive experiments are carried out, showing better performances on both inversion and editing tasks within StyleGAN. Codes are available at https://github.com/sapphire497/style-transformer.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109906523",
                        "name": "Xueqi Hu"
                    },
                    {
                        "authorId": "46299093",
                        "name": "Qiusheng Huang"
                    },
                    {
                        "authorId": "30561862",
                        "name": "Zhengyi Shi"
                    },
                    {
                        "authorId": "48831152",
                        "name": "Siyuan Li"
                    },
                    {
                        "authorId": "40115662",
                        "name": "Changxin Gao"
                    },
                    {
                        "authorId": "30135277",
                        "name": "Li Sun"
                    },
                    {
                        "authorId": "48934067",
                        "name": "Qingli Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "limited to the semantics identified in the latent space via a pre-trained classifier [1,41,57] or through a semi-automatic manner [22, 48].",
                "Through specific latent-space manipulation, high-level attributes such as age or gender can be identified and edited in a realistic manner [1,22,41,57].",
                "Other methods [22,48] operate without a pre-trained classifier and find the transformations in an unsupervised manner, requiring a manual labelling process to interpret and annotate the \u201cdiscovered\u201d transformations."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "dad410e66d26f5be1b7eea93a98d68aaf1e0f6e9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-04705",
                    "ArXiv": "2203.04705",
                    "DOI": "10.1109/CVPR52688.2022.01773",
                    "CorpusId": 247318772
                },
                "corpusId": 247318772,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/dad410e66d26f5be1b7eea93a98d68aaf1e0f6e9",
                "title": "FlexIT: Towards Flexible Semantic Image Translation",
                "abstract": "Deep generative models, like GANs, have considerably improved the state of the art in image synthesis, and are able to generate near photo-realistic images in structured domains such as human faces. Based on this success, recent work on image editing proceeds by projecting images to the GAN latent space and manipulating the latent vector. However, these approaches are limited in that only images from a narrow domain can be transformed, and with only a limited number of editing operations. We propose FlexIT, a novel method which can take any input image and a user-defined text instruction for editing. Our method achieves flexible and natural editing, pushing the limits of semantic image translation. First, FlexIT combines the input image and text into a single target point in the CLIP multimodal embedding space. Via the latent space of an autoencoder, we iteratively transform the input image toward the target point, ensuring coherence and quality with a variety of novel regularization terms. We propose an evaluation protocol for semantic image translation, and thoroughly evaluate our method on ImageNet. Code will be available at https://github.com/facebookresearch/SemanticImageTranslation/.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1637414390",
                        "name": "Guillaume Couairon"
                    },
                    {
                        "authorId": "2152663254",
                        "name": "Asya Grechka"
                    },
                    {
                        "authorId": "34602236",
                        "name": "Jakob Verbeek"
                    },
                    {
                        "authorId": "144518416",
                        "name": "Holger Schwenk"
                    },
                    {
                        "authorId": "51021910",
                        "name": "M. Cord"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The manipulations are achieved by latent space analysis [27], [28], utilizing pre-trained classifiers [29], [30], controlling a 3D morphable model [31], etc."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a1a3cae9b026869f18fcc55a9472f53be17291c0",
                "externalIds": {
                    "ArXiv": "2203.02762",
                    "DBLP": "journals/corr/abs-2203-02762",
                    "DOI": "10.48550/arXiv.2203.02762",
                    "CorpusId": 247292402,
                    "PubMed": "35635812"
                },
                "corpusId": 247292402,
                "publicationVenue": {
                    "id": "5e1f6444-5d03-48c7-b202-7f47d492aeae",
                    "name": "IEEE Transactions on Visualization and Computer Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Vis Comput Graph"
                    ],
                    "issn": "1077-2626",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=2945"
                },
                "url": "https://www.semanticscholar.org/paper/a1a3cae9b026869f18fcc55a9472f53be17291c0",
                "title": "DrawingInStyles: Portrait Image Generation and Editing with Spatially Conditioned StyleGAN",
                "abstract": "The research topic of sketch-to-portrait generation has witnessed a boost of progress with deep learning techniques. The recently proposed StyleGAN architectures achieve state-of-the-art generation ability but the original StyleGAN is not friendly for sketch-based creation due to its unconditional generation nature. To address this issue, we propose a direct conditioning strategy to better preserve the spatial information under the StyleGAN framework. Specifically, we introduce Spatially Conditioned StyleGAN (SC-StyleGAN for short), which explicitly injects spatial constraints to the original StyleGAN generation process. We explore two input modalities, sketches and semantic maps, which together allow users to express desired generation results more precisely and easily. Based on SC-StyleGAN, we present DrawingInStyles, a novel drawing interface for non-professional users to easily produce high-quality, photo-realistic face images with precise control, either from scratch or editing existing ones. Qualitative and quantitative evaluations show the superior generation ability of our method to existing and alternative solutions. The usability and expressiveness of our system are confirmed by a user study.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2761441",
                        "name": "Wanchao Su"
                    },
                    {
                        "authorId": "2111973001",
                        "name": "Hui Ye"
                    },
                    {
                        "authorId": "2999411",
                        "name": "Shu-Yu Chen"
                    },
                    {
                        "authorId": "51190170",
                        "name": "Lin Gao"
                    },
                    {
                        "authorId": "3169698",
                        "name": "Hongbo Fu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "36714983e9e70eaafe8d0f77d591e39e0c49eb0b",
                "externalIds": {
                    "DBLP": "journals/cgf/BermanoGAMNTPC22",
                    "ArXiv": "2202.14020",
                    "DOI": "10.1111/cgf.14503",
                    "CorpusId": 247158728
                },
                "corpusId": 247158728,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/36714983e9e70eaafe8d0f77d591e39e0c49eb0b",
                "title": "State\u2010of\u2010the\u2010Art in the Architecture, Methods and Applications of StyleGAN",
                "abstract": "Generative Adversarial Networks (GANs) have established themselves as a prevalent approach to image synthesis. Of these, StyleGAN offers a fascinating case study, owing to its remarkable visual quality and an ability to support a large array of downstream tasks. This state\u2010of\u2010the\u2010art report covers the StyleGAN architecture, and the ways it has been employed since its conception, while also analyzing its severe limitations. It aims to be of use for both newcomers, who wish to get a grasp of the field, and for more experienced readers that might benefit from seeing current research trends and existing tools laid out. Among StyleGAN's most interesting aspects is its learned latent space. Despite being learned with no supervision, it is surprisingly well\u2010behaved and remarkably disentangled. Combined with StyleGAN's visual quality, these properties gave rise to unparalleled editing capabilities. However, the control offered by StyleGAN is inherently limited to the generator's learned distribution, and can only be applied to images generated by StyleGAN itself. Seeking to bring StyleGAN's latent control to real\u2010world scenarios, the study of GAN inversion and latent space embedding has quickly gained in popularity. Meanwhile, this same study has helped shed light on the inner workings and limitations of StyleGAN. We map out StyleGAN's impressive story through these investigations, and discuss the details that have made StyleGAN the go\u2010to generator. We further elaborate on the visual priors StyleGAN constructs, and discuss their use in downstream discriminative tasks. Looking forward, we point out StyleGAN's limitations and speculate on current trends and promising directions for future research, such as task and target specific fine\u2010tuning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1755628",
                        "name": "Amit H. Bermano"
                    },
                    {
                        "authorId": "134639223",
                        "name": "Rinon Gal"
                    },
                    {
                        "authorId": "1850630812",
                        "name": "Yuval Alaluf"
                    },
                    {
                        "authorId": "147940380",
                        "name": "Ron Mokady"
                    },
                    {
                        "authorId": "1702992975",
                        "name": "Yotam Nitzan"
                    },
                    {
                        "authorId": "2047833213",
                        "name": "Omer Tov"
                    },
                    {
                        "authorId": "2819477",
                        "name": "Or Patashnik"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In addition, we mathematically show that Semantic Factorization (SeFa) [6], GANSpace [7] and regular PCA [11] typically achieve almost the identical results when sampling enough data for GANSpace.",
                "Study on the Latent Space of a GAN Recent studies [8, 7, 6] on a GAN reveal that a latent space possess a range of semantically-understandable information (e.",
                "Recent works [6, 7, 8] reveal that there exists a wide range of meaningful semantic factors in the latent space of a GAN, such as facial attributes and head poses for face synthesis [8] and layout for scene generation [9].",
                "Figure 9: Visualization of individual components within the latent codes, for (1) SeFa [6], (2) GANSpace [7] and (3) regular PCA.",
                "Based on recent studies [6, 7], we assume that the pre-trained weights of a conditional text-toimage GAN architecture contain a set of useful directions.",
                "9 plots the latent-code manipulation results of SeFa [6], GANSpace [7] and regular PCA on the CUB bird and COCO data sets.",
                "The scalar parameter for GANSpace [7] is set to 20 on the CUB bird data set and 9 on the COCO data set, respectively.",
                "Analyzing the correspondences between SeFa, GANSpace and PCA We attempt to discuss the relationship between SeFa [6] and GANSpace [7], since they both introduce an algorithmically simple but surprisingly effective technique to derive semantically-understandable directions.",
                "[7] designed a novel pipeline named GANSpace, which performed PCA [11] on a series of collected latent vectors and employed obtained principal components as the meaningful directions in the latent space."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "result",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "5095a26dbae1d03cb3fda32ac797aa230b9b64fa",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-12929",
                    "ArXiv": "2202.12929",
                    "CorpusId": 247158905
                },
                "corpusId": 247158905,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5095a26dbae1d03cb3fda32ac797aa230b9b64fa",
                "title": "OptGAN: Optimizing and Interpreting the Latent Space of the Conditional Text-to-Image GANs",
                "abstract": "Text-to-image generation intends to automatically produce a photo-realistic image, conditioned on a textual description. It can be potentially employed in the field of art creation, data augmentation, photo-editing, etc. Although many efforts have been dedicated to this task, it remains particularly challenging to generate believable, natural scenes. To facilitate the real-world applications of text-to-image synthesis, we focus on studying the following three issues: 1) How to ensure that generated samples are believable, realistic or natural? 2) How to exploit the latent space of the generator to edit a synthesized image? 3) How to improve the explainability of a text-to-image generation framework? In this work, we constructed two novel data sets (i.e., the Good&Bad bird and face data sets) consisting of successful as well as unsuccessful generated samples, according to strict criteria. To effectively and efficiently acquire high-quality images by increasing the probability of generating Good latent codes, we use a dedicated Good/Bad classifier for generated images. It is based on a pre-trained front end and fine-tuned on the basis of the proposed Good&Bad data set. After that, we present a novel algorithm which identifies semantically-understandable directions in the latent space of a conditional text-to-image GAN architecture by performing independent component analysis on the pre-trained weight values of the generator. Furthermore, we develop a background-flattening loss (BFL), to improve the background appearance in the edited image. Subsequently, we introduce linear interpolation analysis between pairs of keywords. This is extended into a similar triangular `linguistic' interpolation in order to take a deep look into what a text-to-image synthesis model has learned within the linguistic embeddings. Our data set is available at https://zenodo.org/record/6283798#.YhkN_ujMI2w.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48806403",
                        "name": "Zhenxing Zhang"
                    },
                    {
                        "authorId": "1799278",
                        "name": "Lambert Schomaker"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Since different hyperplanes for different facial attributes in StyleGAN latent space (Shen et al., 2020; Ha\u0308rko\u0308nen et al., 2020) can be found, our method can be used to modify the memorability of the images conditionally.",
                "Since different hyperplanes for different facial attributes in StyleGAN latent space (Shen et al., 2020; H\u00e4rk\u00f6nen et al., 2020) can be found, our method can be used to modify the memorability of the images conditionally."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "9d2b0565bd7550d7c529abaf26e8e43eedee3a72",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-11896",
                    "ArXiv": "2202.11896",
                    "CorpusId": 247084090
                },
                "corpusId": 247084090,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9d2b0565bd7550d7c529abaf26e8e43eedee3a72",
                "title": "Controlling Memorability of Face Images",
                "abstract": "Everyday, we are bombarded with many photographs of faces, whether on social media, television, or smartphones. From an evolutionary perspective, faces are intended to be remembered, mainly due to survival and personal relevance. However, all these faces do not have the equal opportunity to stick in our minds. It has been shown that memorability is an intrinsic feature of an image but yet, it is largely unknown what attributes make an image more memorable. In this work, we aimed to address this question by proposing a fast approach to modify and control the memorability of face images. In our proposed method, we first found a hyperplane in the latent space of StyleGAN to separate high and low memorable images. We then modified the image memorability (while maintaining the identity and other facial features such as age, emotion, etc.) by moving in the positive or negative direction of this hyperplane normal vector. We further analyzed how different layers of the StyleGAN augmented latent space contribute to face memorability. These analyses showed how each individual face attribute makes an image more or less memorable. Most importantly, we evaluated our proposed method for both real and synthesized face images. The proposed method successfully modifies and controls the memorability of real human faces as well as unreal synthesized faces. Our proposed method can be employed in photograph editing applications for social media, learning aids, or advertisement purposes.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1381185527",
                        "name": "M. Younesi"
                    },
                    {
                        "authorId": "49318589",
                        "name": "Y. Mohsenzadeh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "2020], most recent works are completely unsupervised [Gal et al. 2021; H\u00e4rk\u00f6nen et al. 2020; Patashnik et al. 2021; Shen and Zhou 2020; Voynov and Babenko 2020; Wang and Ponce 2021; Wu et al. 2020; Xia et al. 2021], potentially enabling the semantic editing of any domain, even for real images [Roich et al."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "56fd6fdc46e76cae7441e3abc6171d2af3b58e0c",
                "externalIds": {
                    "ArXiv": "2202.12211",
                    "DBLP": "journals/corr/abs-2202-12211",
                    "DOI": "10.1145/3528233.3530708",
                    "CorpusId": 247084382
                },
                "corpusId": 247084382,
                "publicationVenue": {
                    "id": "cf6b5e76-9274-46e6-a2dd-7c190ec2ec5f",
                    "name": "International Conference on Computer Graphics and Interactive Techniques",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Graph Interact Tech",
                        "SIGGRAPH"
                    ],
                    "url": "http://www.siggraph.org/"
                },
                "url": "https://www.semanticscholar.org/paper/56fd6fdc46e76cae7441e3abc6171d2af3b58e0c",
                "title": "Self-Distilled StyleGAN: Towards Generation from Internet Photos",
                "abstract": "StyleGAN is known to produce high-fidelity images, while also offering unprecedented semantic editing. However, these fascinating abilities have been demonstrated only on a limited set of datasets, which are usually structurally aligned and well curated. In this paper, we show how StyleGAN can be adapted to work on raw uncurated images collected from the Internet. Such image collections impose two main challenges to StyleGAN: they contain many outlier images, and are characterized by a multi-modal distribution. Training StyleGAN on such raw image collections results in degraded image synthesis quality. To meet these challenges, we proposed a StyleGAN-based self-distillation approach, which consists of two main components: (i) A generative-based self-filtering of the dataset to eliminate outlier images, in order to generate an adequate training set, and (ii) Perceptual clustering of the generated images to detect the inherent data modalities, which are then employed to improve StyleGAN\u2019s \u201ctruncation trick\u201d in the image synthesis process. The presented technique enables the generation of high-quality images, while minimizing the loss in diversity of the data. Through qualitative and quantitative evaluation, we demonstrate the power of our approach to new challenging and diverse domains collected from the Internet. New datasets and pre-trained models are provided in our project website https://self-distilled-stylegan.github.io/.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "147940380",
                        "name": "Ron Mokady"
                    },
                    {
                        "authorId": "2065978700",
                        "name": "Michal Yarom"
                    },
                    {
                        "authorId": "2047833213",
                        "name": "Omer Tov"
                    },
                    {
                        "authorId": "49618488",
                        "name": "Oran Lang"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    },
                    {
                        "authorId": "2112779",
                        "name": "Tali Dekel"
                    },
                    {
                        "authorId": "144611617",
                        "name": "M. Irani"
                    },
                    {
                        "authorId": "2138834",
                        "name": "Inbar Mosseri"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Moreover, with respect to natural scene objects, the generated images can be edited by enabling the interpretation of the GANs\u2019 latent space [34], [37], [38]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "889dc4ab2313f40eb7e61ae5e0db5b820a04c349",
                "externalIds": {
                    "ArXiv": "2202.11456",
                    "DBLP": "journals/corr/abs-2202-11456",
                    "DOI": "10.1109/TNNLS.2022.3151477",
                    "CorpusId": 247058493,
                    "PubMed": "35226609"
                },
                "corpusId": 247058493,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/889dc4ab2313f40eb7e61ae5e0db5b820a04c349",
                "title": "SLOGAN: Handwriting Style Synthesis for Arbitrary-Length and Out-of-Vocabulary Text",
                "abstract": "Large amounts of labeled data are urgently required for the training of robust text recognizers. However, collecting handwriting data of diverse styles, along with an immense lexicon, is considerably expensive. Although data synthesis is a promising way to relieve data hunger, two key issues of handwriting synthesis, namely, style representation and content embedding, remain unsolved. To this end, we propose a novel method that can synthesize parameterized and controllable handwriting Styles for arbitrary-Length and Out-of-vocabulary text based on a Generative Adversarial Network (GAN), termed SLOGAN. Specifically, we propose a style bank to parameterize specific handwriting styles as latent vectors, which are input to a generator as style priors to achieve the corresponding handwritten styles. The training of the style bank requires only writer identification of the source images, rather than attribute annotations. Moreover, we embed the text content by providing an easily obtainable printed style image, so that the diversity of the content can be flexibly achieved by changing the input printed image. Finally, the generator is guided by dual discriminators to handle both the handwriting characteristics that appear as separated characters and in a series of cursive joins. Our method can synthesize words that are not included in the training vocabulary and with various new styles. Extensive experiments have shown that high-quality text images with great style diversity and rich vocabulary can be synthesized using our method, thereby enhancing the robustness of the recognizer.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "30099960",
                        "name": "Canjie Luo"
                    },
                    {
                        "authorId": "49780704",
                        "name": "Yuanzhi Zhu"
                    },
                    {
                        "authorId": "144838978",
                        "name": "Lianwen Jin"
                    },
                    {
                        "authorId": "2109738992",
                        "name": "Zhe Li"
                    },
                    {
                        "authorId": "50705920",
                        "name": "Dezhi Peng"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "560bd1924fcfcfe4d26359da594b7c885ba30c35",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-11772",
                    "ArXiv": "2202.11772",
                    "CorpusId": 247084201
                },
                "corpusId": 247084201,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/560bd1924fcfcfe4d26359da594b7c885ba30c35",
                "title": "Discovering Multiple and Diverse Directions for Cognitive Image Properties",
                "abstract": "Recent research has shown that it is possible to find interpretable directions in the latent spaces of pre-trained GANs. These directions enable controllable generation and support a variety of semantic editing operations. While previous work has focused on discovering a single direction that performs a desired editing operation such as zoom-in, limited work has been done on the discovery of multiple and diverse directions that can achieve the desired edit. In this work, we propose a novel framework that discovers multiple and diverse directions for a given property of interest. In particular, we focus on the manipulation of cognitive properties such as Memorability, Emotional Valence and Aesthetics. We show with extensive experiments that our method successfully manipulates these properties while producing diverse outputs. Our project page and source code can be found at http://catlab-team.github.io/latentcognitive.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2145258020",
                        "name": "Umut Kocasari"
                    },
                    {
                        "authorId": "2120300123",
                        "name": "Alperen Bag"
                    },
                    {
                        "authorId": "2046873472",
                        "name": "O\u011fuz Kaan Y\u00fcksel"
                    },
                    {
                        "authorId": "3137679",
                        "name": "Pinar Yanardag"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Most prior arts, however, target at finding global attributes [15, 35, 40, 45] such that altering the latent code with these attributes will manipulate the output image as a whole.",
                "To interpret the latent space learned by GANs, many attempts have been made, including both supervised ones [20, 35, 45] and unsupervised ones [15, 36, 40]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "571a237019c3f796eef20a1252e76cf7beea9a9d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-09649",
                    "ArXiv": "2202.09649",
                    "CorpusId": 247011343
                },
                "corpusId": 247011343,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/571a237019c3f796eef20a1252e76cf7beea9a9d",
                "title": "Region-Based Semantic Factorization in GANs",
                "abstract": "Despite the rapid advancement of semantic discovery in the latent space of Generative Adversarial Networks (GANs), existing approaches either are limited to finding global attributes or rely on a number of segmentation masks to identify local attributes. In this work, we present a highly efficient algorithm to factorize the latent semantics learned by GANs concerning an arbitrary image region. Concretely, we revisit the task of local manipulation with pre-trained GANs and formulate region-based semantic discovery as a dual optimization problem. Through an appropriately defined generalized Rayleigh quotient, we manage to solve such a problem without any annotations or training. Experimental results on various state-of-the-art GAN models demonstrate the effectiveness of our approach, as well as its superiority over prior arts regarding precise control, region robustness, speed of implementation, and simplicity of use.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47054925",
                        "name": "Jiapeng Zhu"
                    },
                    {
                        "authorId": "2117687899",
                        "name": "Yujun Shen"
                    },
                    {
                        "authorId": "121983635",
                        "name": "Yinghao Xu"
                    },
                    {
                        "authorId": "1678783",
                        "name": "Deli Zhao"
                    },
                    {
                        "authorId": "143832240",
                        "name": "Qifeng Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Unsupervised methods [12, 35, 44] learn the control by analyzing the statistics or the model weights."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6bd005fe98c332d924ad983b2dc2808a440180a6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-08553",
                    "ArXiv": "2202.08553",
                    "DOI": "10.1007/978-3-031-19787-1_23",
                    "CorpusId": 246904392
                },
                "corpusId": 246904392,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/6bd005fe98c332d924ad983b2dc2808a440180a6",
                "title": "3D-Aware Indoor Scene Synthesis with Depth Priors",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2125701522",
                        "name": "Zifan Shi"
                    },
                    {
                        "authorId": "2117687899",
                        "name": "Yujun Shen"
                    },
                    {
                        "authorId": "47054925",
                        "name": "Jiapeng Zhu"
                    },
                    {
                        "authorId": "66427434",
                        "name": "Dit-Yan Yeung"
                    },
                    {
                        "authorId": "143832240",
                        "name": "Qifeng Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "StyleGAN and others investigate how to enhance desirable properties in GAN latent spaces to influence the characteristics of generated images selectively (Karras et al., 2019, 2020; H\u00e4rk\u00f6nen et al., 2020).",
                "StyleGAN and others investigate how to enhance desirable properties in GAN latent spaces to influence the characteristics of generated images selectively (Karras et al., 2019, 2020; Ha\u0308rko\u0308nen et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "d2026c6347d794ddf8adfc4c34637765167f995c",
                "externalIds": {
                    "ArXiv": "2202.08526",
                    "DBLP": "conf/aistats/TriessBPF022",
                    "CorpusId": 246904420
                },
                "corpusId": 246904420,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d2026c6347d794ddf8adfc4c34637765167f995c",
                "title": "Point Cloud Generation with Continuous Conditioning",
                "abstract": "Generative models can be used to synthesize 3D objects of high quality and diversity. However, there is typically no control over the properties of the generated object.This paper proposes a novel generative adversarial network (GAN) setup that generates 3D point cloud shapes conditioned on a continuous parameter. In an exemplary application, we use this to guide the generative process to create a 3D object with a custom-fit shape. We formulate this generation process in a multi-task setting by using the concept of auxiliary classifier GANs. Further, we propose to sample the generator label input for training from a kernel density estimation (KDE) of the dataset. Our ablations show that this leads to significant performance increase in regions with few samples. Extensive quantitative and qualitative experiments show that we gain explicit control over the object dimensions while maintaining good generation quality and diversity.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "150195521",
                        "name": "Larissa T. Triess"
                    },
                    {
                        "authorId": "2064721089",
                        "name": "Andreas B\u00fchler"
                    },
                    {
                        "authorId": "2053549195",
                        "name": "David Peter"
                    },
                    {
                        "authorId": "2869660",
                        "name": "F. Flohr"
                    },
                    {
                        "authorId": "32244386",
                        "name": "J. M. Z\u00f6llner"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANSpace [12]) proposes to apply principal component analysis (PCA, [35]) to randomly select the latent vectors of the intermediate layers of the BigGAN and StyleGAN models."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6ec7a6c0cd8679e64792f38c61f79a624a1268d4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-06240",
                    "ArXiv": "2202.06240",
                    "DOI": "10.1007/978-3-031-19778-9_33",
                    "CorpusId": 246823309
                },
                "corpusId": 246823309,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/6ec7a6c0cd8679e64792f38c61f79a624a1268d4",
                "title": "FairStyle: Debiasing StyleGAN2 with Style Channel Manipulations",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2154417321",
                        "name": "Cemre Karakas"
                    },
                    {
                        "authorId": "2145260636",
                        "name": "Alara Dirik"
                    },
                    {
                        "authorId": "2134272652",
                        "name": "Eyl\u00fcl Yal\u00e7\u0131nkaya"
                    },
                    {
                        "authorId": "3137679",
                        "name": "Pinar Yanardag"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For instance, GANSpace [22] proposes applying principal component analysis (PCA) [57] on a set of randomly sampled latent vectors extracted from the intermediate layers of BigGAN and StyleGAN.",
                "Since there are no existing approaches for this task, we propose a simple PCA-based baseline inspired by GANSpace [22]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "5aeade1f8bd33c6b7c93c034648297704450553a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-06079",
                    "ArXiv": "2202.06079",
                    "DOI": "10.1109/WACV56688.2023.00440",
                    "CorpusId": 246823836
                },
                "corpusId": 246823836,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/5aeade1f8bd33c6b7c93c034648297704450553a",
                "title": "Text and Image Guided 3D Avatar Generation and Manipulation",
                "abstract": "The manipulation of latent space has recently become an interesting topic in the field of generative models. Recent research shows that latent directions can be used to manipulate images towards certain attributes. However, controlling the generation process of 3D generative models remains a challenge. In this work, we propose a novel 3D manipulation method that can manipulate both the shape and texture of the model using text or image-based prompts such as \u2019a young face\u2019 or \u2019a surprised face\u2019. We leverage the power of Contrastive Language-Image Pre-training (CLIP) model and a pre-trained 3D GAN model designed to generate face avatars and create a fully differentiable rendering pipeline to manipulate meshes. More specifically, our method takes an input latent code and modifies it such that the target attribute specified by a text or image prompt is present or enhanced while leaving other attributes largely unaffected. Our method requires only 5 minutes per manipulation, and we demonstrate the effectiveness of our approach with extensive results and comparisons.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2154429014",
                        "name": "Zehranaz Canfes"
                    },
                    {
                        "authorId": "101535589",
                        "name": "M. Atasoy"
                    },
                    {
                        "authorId": "2145260636",
                        "name": "Alara Dirik"
                    },
                    {
                        "authorId": "3137679",
                        "name": "Pinar Yanardag"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b21b4f1f8ff320775a334526c7b0ef278856102e",
                "externalIds": {
                    "ArXiv": "2202.04978",
                    "DBLP": "journals/corr/abs-2202-04978",
                    "CorpusId": 246706243
                },
                "corpusId": 246706243,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b21b4f1f8ff320775a334526c7b0ef278856102e",
                "title": "Towards Assessing and Characterizing the Semantic Robustness of Face Recognition",
                "abstract": "Deep Neural Networks (DNNs) lack robustness against imperceptible perturbations to their input. Face Recognition Models (FRMs) based on DNNs inherit this vulnerability. We propose a methodology for assessing and characterizing the robustness of FRMs against semantic perturbations to their input. Our methodology causes FRMs to malfunction by designing adversarial attacks that search for identity-preserving modifications to faces. In particular, given a face, our attacks find identity-preserving variants of the face such that an FRM fails to recognize the images belonging to the same identity. We model these identity-preserving semantic modifications via direction- and magnitude-constrained perturbations in the latent space of StyleGAN. We further propose to characterize the semantic robustness of an FRM by statistically describing the perturbations that induce the FRM to malfunction. Finally, we combine our methodology with a certification technique, thus providing (i) theoretical guarantees on the performance of an FRM, and (ii) a formal description of how an FRM may model the notion of face identity.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152978592",
                        "name": "Juan C. P'erez"
                    },
                    {
                        "authorId": "92400655",
                        "name": "Motasem Alfarra"
                    },
                    {
                        "authorId": "35869086",
                        "name": "Ali K. Thabet"
                    },
                    {
                        "authorId": "2065698095",
                        "name": "P. Arbel'aez"
                    },
                    {
                        "authorId": "2931652",
                        "name": "Bernard Ghanem"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Latent-Space Editing The unprecedented ability of StyleGAN to encode semantic properties within its latent space has spawned an impressive array of image manipulation methods [16, 24, 32, 33, 40].",
                "Others have proposed ways to identify such semantic directions in an entirely unsupervised manner [16, 33] or in a zero-shot manner by leveraging models [26] that jointly encode image and text [24]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "4bed424f79afa84f65821b0988db49c2a0bc6fb1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-04040",
                    "ArXiv": "2202.04040",
                    "CorpusId": 246652560
                },
                "corpusId": 246652560,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4bed424f79afa84f65821b0988db49c2a0bc6fb1",
                "title": "Self-Conditioned Generative Adversarial Networks for Image Editing",
                "abstract": "Generative Adversarial Networks (GANs) are susceptible to bias, learned from either the unbalanced data, or through mode collapse. The networks focus on the core of the data distribution, leaving the tails - or the edges of the distribution - behind. We argue that this bias is responsible not only for fairness concerns, but that it plays a key role in the collapse of latent-traversal editing methods when deviating away from the distribution's core. Building on this observation, we outline a method for mitigating generative bias through a self-conditioning process, where distances in the latent-space of a pre-trained generator are used to provide initial labels for the data. By fine-tuning the generator on a re-sampled distribution drawn from these self-labeled data, we force the generator to better contend with rare semantic attributes and enable more realistic generation of these properties. We compare our models to a wide range of latent editing methods, and show that by alleviating the bias they achieve finer semantic control and better identity preservation through a wider range of transformations. Our code and models will be available at https://github.com/yzliu567/sc-gan",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2117964384",
                        "name": "Yunzhe Liu"
                    },
                    {
                        "authorId": "134639223",
                        "name": "Rinon Gal"
                    },
                    {
                        "authorId": "1755628",
                        "name": "Amit H. Bermano"
                    },
                    {
                        "authorId": "2028246830",
                        "name": "Baoquan Chen"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "ed3d0817ffd06bf4ef5bafd2d38c893e342a38fa",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-02713",
                    "ArXiv": "2202.02713",
                    "CorpusId": 246634191
                },
                "corpusId": 246634191,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ed3d0817ffd06bf4ef5bafd2d38c893e342a38fa",
                "title": "FEAT: Face Editing with Attention",
                "abstract": "Employing the latent space of pretrained generators has recently been shown to be an effective means for GAN-based face manipulation. The success of this approach heavily relies on the innate disentanglement of the latent space axes of the generator. However, face manipulation often intends to affect local regions only, while common generators do not tend to have the necessary spatial disentanglement. In this paper, we build on the StyleGAN generator, and present a method that explicitly encourages face manipulation to focus on the intended regions by incorporating learned attention maps. During the generation of the edited image, the attention map serves as a mask that guides a blending between the original features and the modified ones. The guidance for the latent space edits is achieved by employing CLIP, which has recently been shown to be effective for text-driven edits. We perform extensive experiments and show that our method can perform disentangled and controllable face manipulations based on text descriptions by attending to the relevant regions only. Both qualitative and quantitative experimental results demonstrate the superiority of our method for facial region editing over alternative methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3468964",
                        "name": "Xianxu Hou"
                    },
                    {
                        "authorId": "121640365",
                        "name": "Linlin Shen"
                    },
                    {
                        "authorId": "2819477",
                        "name": "Or Patashnik"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    },
                    {
                        "authorId": "40586368",
                        "name": "Hui Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Given an input latent code w, let us consider that we have a latent code w\u0303 corresponding to a desired editing, where w\u0303 is obtained from a latent space editing method [16, 33, 34].",
                "Recent studies [16, 33, 34, 43] have shown that it is possible to control semantic attributes of synthetic images by manipulating the latent space of a pre-trained GAN, however an efficient encoding method, necessary for real images, still remains an open problem, especially in the case of these editing tasks.",
                "These techniques include unsupervised exploration [38], learning linear SVM models [33], principle component analysis on the latent codes [16], and k-means clustering of the activation features [10]."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4a6e83f0eb4aa9acdac1fc4f77e4e1e1a9ea56b7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-02183",
                    "ArXiv": "2202.02183",
                    "CorpusId": 246608257
                },
                "corpusId": 246608257,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4a6e83f0eb4aa9acdac1fc4f77e4e1e1a9ea56b7",
                "title": "Feature-Style Encoder for Style-Based GAN Inversion",
                "abstract": "We propose a novel architecture for GAN inversion, which we call Feature-Style encoder. The style encoder is key for the manipulation of the obtained latent codes, while the feature encoder is crucial for optimal image reconstruction. Our model achieves accurate inversion of real images from the latent space of a pre-trained style-based GAN model, obtaining better perceptual quality and lower reconstruction error than existing methods. Thanks to its encoder structure, the model allows fast and accurate image editing. Additionally, we demonstrate that the proposed encoder is especially well-suited for inversion and editing on videos. We conduct extensive experiments for several style-based generators pre-trained on different data domains. Our proposed method yields state-of-the-art results for style-based GAN inversion, significantly outperforming competing approaches. Source codes are available at https://github.com/InterDigitalInc/FeatureStyleEncoder .",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115586564",
                        "name": "Xu Yao"
                    },
                    {
                        "authorId": "1902919",
                        "name": "A. Newson"
                    },
                    {
                        "authorId": "1796594",
                        "name": "Y. Gousseau"
                    },
                    {
                        "authorId": "1806880",
                        "name": "P. Hellier"
                    }
                ]
            }
        },
        {
            "contexts": [
                "see [12, 24, 30] and Figure 2), including work on co-creativity and computational creativity (e.",
                "However, there are techniques to facilitate the learning or extraction of disentangled dimensions [4, 12, 29], such that each one represents a (more) interpretable feature.",
                "[12]), (2) the impact of varying inputs and hyperparameters (e.",
                "[4]) or \u201cdiscover\u201d meaningful concepts in a model: For our study, we used the GANSpace approach [12] to automatically select the dimensions from StyleGAN to be controlled via sliders in our study (Section 4.",
                "Currently, there is no guidance on the number of control dimensions to use in various settings, with numbers reported in related work ranging from 5 to 80 dimensions [12, 22, 30].",
                "Beyond allowing for entering text labels [12, 30], our insights motivate exploring concepts from infovis and information retrieval (e.",
                "As a new way to handle this, researchers have worked on methods to explore and evaluate generative models via interaction [12, 30].",
                "Principal Component Analysis, PCA, on StyleGAN \u2019s w vectors, see [12] for details): Concretely, for a task with N sliders, we use this to extract the N top dimensions as ranked by PCA.",
                "Such UIs have been used to enable interaction for model exploration and evaluation [12, 22, 30].",
                "Figure 2: Examples of the many slider and image grid UIs used in related work to interact with generative image models: (a) UI for editing face photos with the \u201cSmart Portrait Filters\u201d in Adobe\u2019s Photoshop [33]; (b) UI used in the GANSpace paper [12] (sliders manipulate latent dimensions discovered via PCA); (c) UI used in the \u201cSwapping Autoencoders\u201d paper [22] (thumbnail grid; sliders on thumbnails change strength of the corresponding texture edits); (d) Automatically created image gallery for interactive GAN exploration (image grid without sliders) [43]; (e) slider UI used in a user study to evaluatemodel interpretability by interactive reconstruction [30]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b30ae7bcb8735240cfbc8a108348e7ee06db297a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-00965",
                    "ArXiv": "2202.00965",
                    "DOI": "10.1145/3491102.3502141",
                    "CorpusId": 246473029
                },
                "corpusId": 246473029,
                "publicationVenue": {
                    "id": "b55b50b1-aae7-47a7-b042-8aecc930073d",
                    "name": "International Conference on Human Factors in Computing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "CHI",
                        "Int Conf Hum Factor Comput Syst",
                        "Human Factors in Computing Systems",
                        "Conference on Human Interface",
                        "Conf Hum Interface",
                        "Hum Factor Comput Syst"
                    ],
                    "url": "http://www.acm.org/sigchi/"
                },
                "url": "https://www.semanticscholar.org/paper/b30ae7bcb8735240cfbc8a108348e7ee06db297a",
                "title": "GANSlider: How Users Control Generative Models for Images using Multiple Sliders with and without Feedforward Information",
                "abstract": "We investigate how multiple sliders with and without feedforward visualizations influence users\u2019 control of generative models. In an online study (N=138), we collected a dataset of people interacting with a generative adversarial network (StyleGAN2) in an image reconstruction task. We found that more control dimensions (sliders) significantly increase task difficulty and user actions. Visual feedforward partly mitigates this by enabling more goal-directed interaction. However, we found no evidence of faster or more accurate task performance. This indicates a tradeoff between feedforward detail and implied cognitive costs, such as attention. Moreover, we found that visualizations alone are not always sufficient for users to understand individual control dimensions. Our study quantifies fundamental UI design factors and resulting interaction behavior in this context, revealing opportunities for improvement in the UI design for interactive applications of generative models. We close by discussing design directions and further aspects.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2767073",
                        "name": "Hai Dang"
                    },
                    {
                        "authorId": "3410124",
                        "name": "Lukas Mecke"
                    },
                    {
                        "authorId": "1768653",
                        "name": "Daniel Buschek"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We then apply different manipulation directions obtained by GANspace [20] and StyleMC [31].",
                "They achieve high image fidelity [27, 28], fine-grained semantic control [20, 36, 61], and recently alias-free generation enabling realistic animation [26].",
                "inversion and edits the image with a direction discovered by GANspace [20]."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "82ba96443173da0b8b3e870c5ab8f41109a67203",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-00273",
                    "ArXiv": "2202.00273",
                    "DOI": "10.1145/3528233.3530738",
                    "CorpusId": 246441861
                },
                "corpusId": 246441861,
                "publicationVenue": {
                    "id": "cf6b5e76-9274-46e6-a2dd-7c190ec2ec5f",
                    "name": "International Conference on Computer Graphics and Interactive Techniques",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Graph Interact Tech",
                        "SIGGRAPH"
                    ],
                    "url": "http://www.siggraph.org/"
                },
                "url": "https://www.semanticscholar.org/paper/82ba96443173da0b8b3e870c5ab8f41109a67203",
                "title": "StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets",
                "abstract": "Computer graphics has experienced a recent surge of data-centric approaches for photorealistic and controllable content creation. StyleGAN in particular sets new standards for generative modeling regarding image quality and controllability. However, StyleGAN\u2019s performance severely degrades on large unstructured datasets such as ImageNet. StyleGAN was designed for controllability; hence, prior works suspect its restrictive design to be unsuitable for diverse datasets. In contrast, we find the main limiting factor to be the current training strategy. Following the recently introduced Projected GAN paradigm, we leverage powerful neural network priors and a progressive growing strategy to successfully train the latest StyleGAN3 generator on ImageNet. Our final model, StyleGAN-XL, sets a new state-of-the-art on large-scale image synthesis and is the first to generate images at a resolution of 10242 at such a dataset scale. We demonstrate that this model can invert and edit images beyond the narrow domain of portraits or specific object classes. Code, models, and supplementary videos can be found at https://sites.google.com/view/stylegan-xl/ .",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40562186",
                        "name": "Axel Sauer"
                    },
                    {
                        "authorId": "40502376",
                        "name": "Katja Schwarz"
                    },
                    {
                        "authorId": "47237027",
                        "name": "Andreas Geiger"
                    }
                ]
            }
        },
        {
            "contexts": [
                "StyleGAN2 [36] features several semantically-rich latent spaces, which have been heavily studied and exploited in the context of image manipulation [4, 15, 26, 60, 61, 65] and image inversion [1,2,10,25,51,85]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "9194bebb54e91bbac8a7c7c1e12be9ff148d6428",
                "externalIds": {
                    "DBLP": "conf/eccv/AlalufPWZSLC22",
                    "ArXiv": "2201.13433",
                    "DOI": "10.1007/978-3-031-25063-7_13",
                    "CorpusId": 246430668
                },
                "corpusId": 246430668,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9194bebb54e91bbac8a7c7c1e12be9ff148d6428",
                "title": "Third Time's the Charm? Image and Video Editing with StyleGAN3",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1850630812",
                        "name": "Yuval Alaluf"
                    },
                    {
                        "authorId": "2819477",
                        "name": "Or Patashnik"
                    },
                    {
                        "authorId": "34815981",
                        "name": "Zongze Wu"
                    },
                    {
                        "authorId": "2151793491",
                        "name": "Asif Zamir"
                    },
                    {
                        "authorId": "2177801",
                        "name": "Eli Shechtman"
                    },
                    {
                        "authorId": "70018371",
                        "name": "D. Lischinski"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Latent GAN space: There is a plethora of works that investigate the existence of interpretable directions in the GAN\u2019s latent space [13,23,26,34,35,38,39,46]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6661752197f093895709e5778d502160571888f4",
                "externalIds": {
                    "ArXiv": "2202.00046",
                    "DBLP": "journals/corr/abs-2202-00046",
                    "CorpusId": 246442040
                },
                "corpusId": 246442040,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/6661752197f093895709e5778d502160571888f4",
                "title": "Finding Directions in GAN's Latent Space for Neural Face Reenactment",
                "abstract": "This paper is on face/head reenactment where the goal is to transfer the facial pose (3D head orientation and expression) of a target face to a source face. Previous methods focus on learning embedding networks for identity and pose disentanglement which proves to be a rather hard task, degrading the quality of the generated images. We take a different approach, bypassing the training of such networks, by using (fine-tuned) pre-trained GANs which have been shown capable of producing high-quality facial images. Because GANs are characterized by weak controllability, the core of our approach is a method to discover which directions in latent GAN space are responsible for controlling facial pose and expression variations. We present a simple pipeline to learn such directions with the aid of a 3D shape model which, by construction, already captures disentangled directions for facial pose, identity and expression. Moreover, we show that by embedding real images in the GAN latent space, our method can be successfully used for the reenactment of real-world faces. Our method features several favorable properties including using a single source image (one-shot) and enabling cross-person reenactment. Our qualitative and quantitative results show that our approach often produces reenacted faces of significantly higher quality than those produced by state-of-the-art methods for the standard benchmarks of VoxCeleb1&2. Source code is available at: https://github.com/StelaBou/stylegan_directions_face_reenactment",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2004556863",
                        "name": "Stella Bounareli"
                    },
                    {
                        "authorId": "1689047",
                        "name": "V. Argyriou"
                    },
                    {
                        "authorId": "2137359565",
                        "name": "Georgios Tzimiropoulos"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This is in contrast to past traversal studies [4, 13, 15, 27, 30] that found global linear directions to suffice for simple scalar attributes.",
                "Most propose finding global linear directions correlated with scalar attributes of interest [4, 11, 13, 27, 30, 37, 41].",
                "Latent spaces of deep generative networks like generative adversarial networks (GANs) [12,16,17,28] and variational autoencoders (VAEs) [18] are known to organize semantic attributes into disentangled subspaces without supervision [13, 15, 28, 36, 38]."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "result"
            ],
            "citingPaper": {
                "paperId": "a292a68358e2b456b46770f3971f897d15acd066",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-10423",
                    "ArXiv": "2201.10423",
                    "CorpusId": 246275581
                },
                "corpusId": 246275581,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a292a68358e2b456b46770f3971f897d15acd066",
                "title": "Rayleigh EigenDirections (REDs): GAN latent space traversals for multidimensional features",
                "abstract": "We present a method for finding paths in a deep generative model's latent space that can maximally vary one set of image features while holding others constant. Crucially, unlike past traversal approaches, ours can manipulate multidimensional features of an image such as facial identity and pixels within a specified region. Our method is principled and conceptually simple: optimal traversal directions are chosen by maximizing differential changes to one feature set such that changes to another set are negligible. We show that this problem is nearly equivalent to one of Rayleigh quotient maximization, and provide a closed-form solution to it based on solving a generalized eigenvalue equation. We use repeated computations of the corresponding optimal directions, which we call Rayleigh EigenDirections (REDs), to generate appropriately curved paths in latent space. We empirically evaluate our method using StyleGAN2 on two image domains: faces and living rooms. We show that our method is capable of controlling various multidimensional features out of the scope of previous latent space traversal methods: face identity, spatial frequency bands, pixels within a region, and the appearance and position of an object. Our work suggests that a wealth of opportunities lies in the local analysis of the geometry and semantics of latent spaces.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47231927",
                        "name": "Guha Balakrishnan"
                    },
                    {
                        "authorId": "2176869",
                        "name": "Raghudeep Gadde"
                    },
                    {
                        "authorId": "1384255355",
                        "name": "Aleix M. Martinez"
                    },
                    {
                        "authorId": "1690922",
                        "name": "P. Perona"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANSpace [11] applies Principal Component Analysis (PCA) either in the latent space or feature space to identify useful control directions.",
                "This property has derived an active research area on exploring interpretable latent subspaces (also referred as latent controls) to modify visual attributes in synthesized images [1, 5, 11, 23, 27, 28, 40, 44].",
                "Unsupervised methods discover meaningful latent codes or directions by analyzing the space distribution under certain constraints [5, 11, 12, 27, 32, 44]."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "85b029777a4e87e8bdd9a692be1cb520bd4f3214",
                "externalIds": {
                    "ArXiv": "2201.09689",
                    "DBLP": "journals/corr/abs-2201-09689",
                    "CorpusId": 246240028
                },
                "corpusId": 246240028,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/85b029777a4e87e8bdd9a692be1cb520bd4f3214",
                "title": "Which Style Makes Me Attractive? Interpretable Control Discovery and Counterfactual Explanation on StyleGAN",
                "abstract": "The semantically disentangled latent subspace in GAN provides rich interpretable controls in image generation. This paper includes two contributions on semantic latent subspace analysis in the scenario of face generation using StyleGAN2. First, we propose a novel approach to disentangle latent subspace semantics by exploiting existing face analysis models, e.g., face parsers and face landmark detectors. These models provide the flexibility to construct various criterions with very concrete and interpretable semantic meanings (e.g., change face shape or change skin color) to restrict latent subspace disentanglement. Rich latent space controls unknown previously can be discovered using the constructed criterions. Second, we propose a new perspective to explain the behavior of a CNN classifier by generating counterfactuals in the interpretable latent subspaces we discovered. This explanation helps reveal whether the classifier learns semantics as intended. Experiments on various disentanglement criterions demonstrate the effectiveness of our approach. We believe this approach contributes to both areas of image manipulation and counterfactual explainability of CNNs. The code is available at \\url{https://github.com/prclibo/ice}.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48218911",
                        "name": "B. Li"
                    },
                    {
                        "authorId": "1684884626",
                        "name": "Qiuli Wang"
                    },
                    {
                        "authorId": "2143385094",
                        "name": "Jiquan Pei"
                    },
                    {
                        "authorId": "2118809263",
                        "name": "Yu Yang"
                    },
                    {
                        "authorId": "7807689",
                        "name": "Xiangyang Ji"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[28] use PCA to identify important latent space directions in order to modify the lightning, aging, and viewpoint of"
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "39aed453f1a182209841afcbdbdeecb53b7871db",
                "externalIds": {
                    "DOI": "10.3390/w14030333",
                    "CorpusId": 246342887
                },
                "corpusId": 246342887,
                "publicationVenue": {
                    "id": "2bff3ecd-cb0d-467a-bdff-036c3e4b3e2d",
                    "name": "Water",
                    "type": "journal",
                    "issn": "2073-4441",
                    "alternate_issns": [
                        "1462-897X",
                        "0310-0367"
                    ],
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-160269",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-160269",
                        "http://www.mdpi.com/journal/water/about/",
                        "https://www.mdpi.com/journal/water"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/39aed453f1a182209841afcbdbdeecb53b7871db",
                "title": "Autoencoders for Semi-Supervised Water Level Modeling in Sewer Pipes with Sparse Labeled Data",
                "abstract": "More frequent and thorough inspection of sewer pipes has the potential to save billions in utilities. However, the amount and quality of inspection are impeded by an imprecise and highly subjective manual process. It involves technicians judging stretches of sewer based on video from remote-controlled robots. Determining the state of sewer pipes based on these videos entails a great deal of ambiguity. Furthermore, the frequency with which the different defects occur differs a lot, leading to highly imbalanced datasets. Such datasets represent a poor basis for automating the labeling process using supervised learning. With this paper we explore the potential of self-supervision as a method for reducing the need for large numbers of well-balanced labels. First, our models learn to represent the data distribution using more than a million unlabeled images, then a small number of labeled examples are used to learn a mapping from the learned representations to a relevant target variable, in this case, water level. We choose a convolutional Autoencoder, a Variational Autoencoder and a Vector-Quantised Variational Autoencoder as the basis for our experiments. The best representations are shown to be learned by the classic Autoencoder with the Multi-Layer Perceptron achieving a Mean Absolute Error of 9.93. This is an improvement of 9.62 over the fully supervised baseline.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2151417316",
                        "name": "Ferran Plana Rius"
                    },
                    {
                        "authorId": "40601459",
                        "name": "M. P. Philipsen"
                    },
                    {
                        "authorId": "31010947",
                        "name": "J. M. Mirats Tur"
                    },
                    {
                        "authorId": "1700569",
                        "name": "T. Moeslund"
                    },
                    {
                        "authorId": "80852284",
                        "name": "C. Angulo Bah\u00f3n"
                    },
                    {
                        "authorId": "2151417205",
                        "name": "Marc Casas"
                    }
                ]
            }
        },
        {
            "contexts": [
                "On this basis, various works (Gu et al., 2020; H\u00e4rk\u00f6nen et al., 2020; Richardson et al., 2020; Zhu et al., 2020) explore in detail the StyleGAN potential vector space: some (Shen and Zhou, 2020; Shen et al.",
                ") (H\u00e4rk\u00f6nen et al., 2020; Shen and Zhou, 2020; Shen et al., 2020), establishing relationship between 3D semantic parameters and genuine facial expressions (Tewari et al."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "bf0a71a7ebe16ca0ac80774c2bcc520e058e616a",
                "externalIds": {
                    "DBLP": "journals/finr/YangQQXSLWCHY21",
                    "PubMedCentral": "8814752",
                    "DOI": "10.3389/fnbot.2021.785808",
                    "CorpusId": 246066749,
                    "PubMed": "35126081"
                },
                "corpusId": 246066749,
                "publicationVenue": {
                    "id": "de454aec-8c73-4737-bb1f-5231453ca8fa",
                    "name": "Frontiers in Neurorobotics",
                    "type": "journal",
                    "alternate_names": [
                        "Front Neurorobotics"
                    ],
                    "issn": "1662-5218",
                    "url": "https://www.frontiersin.org/journals/neurorobotics#articles",
                    "alternate_urls": [
                        "http://www.frontiersin.org/neurorobotics/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bf0a71a7ebe16ca0ac80774c2bcc520e058e616a",
                "title": "ShapeEditor: A StyleGAN Encoder for Stable and High Fidelity Face Swapping",
                "abstract": "With the continuous development of deep-learning technology, ever more advanced face-swapping methods are being proposed. Recently, face-swapping methods based on generative adversarial networks (GANs) have realized many-to-many face exchanges with few samples, which advances the development of this field. However, the images generated by previous GAN-based methods often show instability. The fundamental reason is that the GAN in these frameworks is difficult to converge to the distribution of face space in training completely. To solve this problem, we propose a novel face-swapping method based on pretrained StyleGAN generator with a stronger ability of high-quality face image generation. The critical issue is how to control StyleGAN to generate swapped images accurately. We design the control strategy of the generator based on the idea of encoding and decoding and propose an encoder called ShapeEditor to complete this task. ShapeEditor is a two-step encoder used to generate a set of coding vectors that integrate the identity and attribute of the input faces. In the first step, we extract the identity vector of the source image and the attribute vector of the target image; in the second step, we map the concatenation of the identity vector and attribute vector onto the potential internal space of StyleGAN. Extensive experiments on the test dataset show that the results of the proposed method are not only superior in clarity and authenticity than other state-of-the-art methods but also sufficiently integrate identity and attribute.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2145096712",
                        "name": "Shuai Yang"
                    },
                    {
                        "authorId": "48017396",
                        "name": "Kai Qiao"
                    },
                    {
                        "authorId": "9574521",
                        "name": "Ruoxi Qin"
                    },
                    {
                        "authorId": "50382557",
                        "name": "Pengfei Xie"
                    },
                    {
                        "authorId": "2106709150",
                        "name": "S. Shi"
                    },
                    {
                        "authorId": "2946255",
                        "name": "Ningning Liang"
                    },
                    {
                        "authorId": "82527663",
                        "name": "Linyuan Wang"
                    },
                    {
                        "authorId": "2118446044",
                        "name": "Jian Chen"
                    },
                    {
                        "authorId": "2114117405",
                        "name": "Guoen Hu"
                    },
                    {
                        "authorId": "101753330",
                        "name": "B. Yan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "2020a,b], such as attribute labels, to unsupervised and zero-shot approaches [Gal et al. 2021; H\u00e4rk\u00f6nen et al. 2020; Patashnik et al. 2021; Shen and Zhou 2020; Voynov and Babenko 2020; Wang and Ponce 2021; Xia et al. 2021a]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "03afe67228ee4ee2ed61a089f1d0569f9c94e4b6",
                "externalIds": {
                    "ArXiv": "2201.08361",
                    "DBLP": "conf/siggrapha/TzabanMGBC22",
                    "DOI": "10.1145/3550469.3555382",
                    "CorpusId": 246063490
                },
                "corpusId": 246063490,
                "publicationVenue": {
                    "id": "51fbe23f-0058-4484-b71c-186477a032db",
                    "name": "ACM SIGGRAPH Conference and Exhibition on Computer Graphics and Interactive Techniques in Asia",
                    "type": "conference",
                    "alternate_names": [
                        "ACM GR Conf Exhib Comput Graph Interact Tech Asia",
                        "GR Conf Exhib Comput Graph Interact Tech Asia",
                        "SIGGRAPH Asia",
                        "SIGGRAPH Conference and Exhibition on Computer Graphics and Interactive Techniques in Asia"
                    ],
                    "url": "http://www.siggraph.org/"
                },
                "url": "https://www.semanticscholar.org/paper/03afe67228ee4ee2ed61a089f1d0569f9c94e4b6",
                "title": "Stitch it in Time: GAN-Based Facial Editing of Real Videos",
                "abstract": "The ability of Generative Adversarial Networks to encode rich semantics within their latent space has been widely adopted for facial image editing. However, replicating their success with videos has proven challenging. Applying StyleGAN editing to real videos introduces two main challenges: (i) StyleGAN operates over aligned crops. When editing videos, these crops need to be pasted back into the frame, resulting in a spatial inconsistency. (ii) Videos introduce a fundamental barrier to overcome \u2014 temporal coherency. To address the first challenge, we propose a novel stitching-tuning procedure. The generator is carefully tuned to overcome the spatial artifacts at crop borders, resulting in smooth transitions even when difficult backgrounds are involved. Turning to temporal coherence, we propose that this challenge is largely artificial. The source video is already temporally coherent, and deviations arise in part due to careless treatment of individual components in the editing pipeline. We leverage the natural alignment of StyleGAN and the tendency of neural networks to learn low-frequency functions, and demonstrate that they provide a strongly consistent prior. These components are combined in an end-to-end framework for semantic editing of facial videos. We compare our pipeline to the current state-of-the-art and demonstrate significant improvements. Our method produces meaningful manipulations and maintains greater spatial and temporal consistency, even on challenging talking head videos which current methods struggle with. Our code and videos are available at https://stitch-time.github.io/.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2113246146",
                        "name": "Rotem Tzaban"
                    },
                    {
                        "authorId": "147940380",
                        "name": "Ron Mokady"
                    },
                    {
                        "authorId": "134639223",
                        "name": "Rinon Gal"
                    },
                    {
                        "authorId": "1755628",
                        "name": "Amit H. Bermano"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Such interpretable directions can found in both supervised [1, 20] and unsupervised [4, 8, 9, 18, 21, 24, 26] way.",
                "Recent methods [4, 8, 18] find such directions with the aid of multimodal representation of CLIP."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6ae2c34f22d73d2ac04e85c14b6614829b2d5851",
                "externalIds": {
                    "ArXiv": "2201.03809",
                    "DBLP": "journals/corr/abs-2201-03809",
                    "CorpusId": 245853828
                },
                "corpusId": 245853828,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6ae2c34f22d73d2ac04e85c14b6614829b2d5851",
                "title": "Music2Video: Automatic Generation of Music Video with fusion of audio and text",
                "abstract": "Creation of images using generative adversarial networks has been widely adapted into multi-modal regime with the advent of multi-modal representation models pre-trained on large corpus. Various modalities sharing a common representation space could be utilized to guide the generative models to create images from text or even from audio source. Departing from the previous methods that solely rely on either text or audio, we exploit the expressiveness of both modality. Based on the fusion of text and audio, we create video whose content is consistent with the distinct modalities that are provided. A simple approach to automatically segment the video into variable length intervals and maintain time consistency in generated video is part of our method. Our proposed framework for generating music video shows promising results in application level where users can interactively feed in music source and text source to create artistic music videos. Our code is available at https://github.com/joeljang/music2video.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2000091730",
                        "name": "Joel Jang"
                    },
                    {
                        "authorId": "2110848093",
                        "name": "Sumin Shin"
                    },
                    {
                        "authorId": "5565862",
                        "name": "Yoonjeon Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In this section, we qualitatively compare our approach with three state-of-the-art methods for face editing: InterFaceGAN [23], GANSpace [6], and StyleFlow [1].",
                "We find that our generation time is somewhat longer than the InterFaceGAN and GANSpace approaches, but more than four times faster than StyleFlow.",
                "IG, SF, GS, and L2L refer to InterFaceGAN, StyleFlow, GANSpace and our proposed method.",
                "[6] introduced GANSpace, an unsupervised method that uses principal component analysis to find directions for edits.",
                "For GANSpace, we used components provided by H\u00e4rk\u00f6nen [6] that best match these attributes.",
                "In this case, the GANSpace and InterFaceGAN approaches changed the eyebrows, illumination as well as the background of the figure.",
                "For GANSpace, we used components provided by Ha\u0308rko\u0308nen [6] that best match these attributes.",
                "In addition, GANSpace also significantly changed the background of the edited image.",
                "1StyleFlow: https://github.com/RameenAbdal/StyleFlow, GANSpace: https://github.com/harskish/ganspace , InterFaceGAN: https://github.com/a312863063/generators-with-stylegan2\nFigure 4 (top left) compares the approaches in adjusting the Age attribute.",
                "In this example, both StyleFlow and GANSpace added glasses to the face when the baldness attribute was moved in the negative direction, while InterfaceGAN radically changed the hair color.",
                "After finding these directions, GANSpace requires the user to observe the outputs and manually select meaningful directions based on the target attribute.",
                "A further technique that conserves the initial attributes of the image is to apply the changed latent vector only to some attribute dependent \u201cstyle\u201d layers of the generator - for instance to layer 6 and 7 of a StyleGAN for \u201cage\u201d [1, 6]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b23a205bd463e72ea9a3ade48c94825817b0b2c0",
                "externalIds": {
                    "DBLP": "conf/wacv/KhodadadehGMLBK22",
                    "DOI": "10.1109/WACV51458.2022.00373",
                    "CorpusId": 246872124
                },
                "corpusId": 246872124,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/b23a205bd463e72ea9a3ade48c94825817b0b2c0",
                "title": "Latent to Latent: A Learned Mapper for Identity Preserving Editing of Multiple Face Attributes in StyleGAN-generated Images",
                "abstract": "Several recent papers introduced techniques to adjust the attributes of human faces generated by unconditional GANs such as StyleGAN. Despite efforts to disentangle the attributes, a request to change one attribute often triggers unwanted changes to other attributes as well. More importantly, in some cases, a human observer would not recognize the edited face to belong to the same person. We propose an approach where a neural network takes as input the latent encoding of a face and the desired attribute changes and outputs the latent space encoding of the edited image. The network is trained offline using unsupervised data, with training labels generated by an off-the-shelf attribute classifier. The desired attribute changes and conservation laws, such as identity maintenance, are encoded in the training loss. The number of attributes the mapper can simultaneously modify is only limited by the attributes available to the classifier \u2013 we trained a network that handles 35 attributes, more than any previous approach. As no optimization is performed at deployment time, the computation time is negligible, allowing real-time attribute editing. Qualitative and quantitative comparisons with the current state-of-the-art show our method is better at conserving the identity of the face and restricting changes to the requested attributes.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "52115952",
                        "name": "Siavash Khodadadeh"
                    },
                    {
                        "authorId": "112843532",
                        "name": "S. Ghadar"
                    },
                    {
                        "authorId": "2897426",
                        "name": "Saeid Motiian"
                    },
                    {
                        "authorId": "3329881",
                        "name": "Wei-An Lin"
                    },
                    {
                        "authorId": "1701593",
                        "name": "Ladislau B\u00f6l\u00f6ni"
                    },
                    {
                        "authorId": "2563691",
                        "name": "R. Kalarot"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "13fab6dbb9d0f3eaac0b45a52c140165ae25b8b6",
                "externalIds": {
                    "DBLP": "journals/ijon/XiaPLHMZD22",
                    "DOI": "10.1016/j.neucom.2021.12.093",
                    "CorpusId": 245682770
                },
                "corpusId": 245682770,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/13fab6dbb9d0f3eaac0b45a52c140165ae25b8b6",
                "title": "GAN-based anomaly detection: A review",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1557264594",
                        "name": "X. Xia"
                    },
                    {
                        "authorId": "41052354",
                        "name": "Xizhou Pan"
                    },
                    {
                        "authorId": "2155791602",
                        "name": "Nan Li"
                    },
                    {
                        "authorId": "2154166161",
                        "name": "Xing He"
                    },
                    {
                        "authorId": "2152343688",
                        "name": "Lin Ma"
                    },
                    {
                        "authorId": "2135851366",
                        "name": "Xiaoguang Zhang"
                    },
                    {
                        "authorId": "2066767656",
                        "name": "N. Ding"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Consequently, pre-trained unconditional GANs have been incorporated as a building block in many image editing and manipulation tasks, enabling high flexibility while ensuring high quality performances [7, 11, 24, 9, 23, 28].",
                "A recent line of works [11, 24, 9] extend this to reveal steering directions corresponding to semantically meaningful image transformations in BigGAN\u2019s latent space."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "57761bdd27bc6f3dfc1f501dca9301172e526608",
                "externalIds": {
                    "DBLP": "conf/wacv/JakoelES22",
                    "DOI": "10.1109/WACV51458.2022.00011",
                    "CorpusId": 246872211
                },
                "corpusId": 246872211,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/57761bdd27bc6f3dfc1f501dca9301172e526608",
                "title": "GANs Spatial Control via Inference-Time Adaptive Normalization",
                "abstract": "We introduce a new approach for spatial control over the generation process of Generative Adversarial Networks (GANs). Our approach includes modifying the normalization scheme of a pre-trained GAN at test time, so as to act differently at different image regions, according to guidance from the user. This enables to achieve different generation effects at different locations across the image. In contrast to previous works that require either fine-tuning the model\u2019s parameters or training an additional network, our approach uses the pre-trained GAN as is, without any further modifications or training phase. Our method is thus completely generic and can be easily incorporated into common GAN models. We prove our technique to be useful for solving a line of image manipulation tasks, allowing different generation effects across the image, while preserving the GAN\u2019s high visual quality.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2154629419",
                        "name": "Karin Jakoel"
                    },
                    {
                        "authorId": "2154629408",
                        "name": "Liron Efraim"
                    },
                    {
                        "authorId": "3459255",
                        "name": "Tamar Rott Shaham"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This can be done by operating directly on the latent codes [17,18] or by analysing the activation space of latent codes to discover interpretable directions of manipulation in latent space [19]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "941bdad0e1c6a75d9e860c64bc6bb678a8f5653c",
                "externalIds": {
                    "PubMedCentral": "8774762",
                    "DBLP": "journals/entropy/BroadLG22",
                    "DOI": "10.3390/e24010028",
                    "CorpusId": 245489660,
                    "PubMed": "35052054"
                },
                "corpusId": 245489660,
                "publicationVenue": {
                    "id": "8270cfe1-3713-4325-a7bd-c6a87eed889e",
                    "name": "Entropy",
                    "type": "journal",
                    "issn": "1099-4300",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-155606",
                    "alternate_urls": [
                        "http://www.mdpi.com/journal/entropy/",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-155606",
                        "https://www.mdpi.com/journal/entropy"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/941bdad0e1c6a75d9e860c64bc6bb678a8f5653c",
                "title": "Network Bending: Expressive Manipulation of Generative Models in Multiple Domains",
                "abstract": "This paper presents the network bending framework, a new approach for manipulating and interacting with deep generative models. We present a comprehensive set of deterministic transformations that can be inserted as distinct layers into the computational graph of a trained generative neural network and applied during inference. In addition, we present a novel algorithm for analysing the deep generative model and clustering features based on their spatial activation maps. This allows features to be grouped together based on spatial similarity in an unsupervised fashion. This results in the meaningful manipulation of sets of features that correspond to the generation of a broad array of semantically significant features of the generated results. We outline this framework, demonstrating our results on deep generative models for both image and audio domains. We show how it allows for the direct manipulation of semantically meaningful aspects of the generative process as well as allowing for a broad range of expressive outcomes.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47214633",
                        "name": "Terence Broad"
                    },
                    {
                        "authorId": "1745250",
                        "name": "F. Leymarie"
                    },
                    {
                        "authorId": "1691146",
                        "name": "M. Grierson"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We compare against GANSpace [10] and SeFA [35] by identifying the attributes above in their basis of interpretable directions."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "508751d2beabf128ca2441f1f961163681fdb9cc",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-12911",
                    "ArXiv": "2112.12911",
                    "DOI": "10.1109/CVPR52688.2022.01125",
                    "CorpusId": 245501862
                },
                "corpusId": 245501862,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/508751d2beabf128ca2441f1f961163681fdb9cc",
                "title": "Cluster-guided Image Synthesis with Unconditional Models",
                "abstract": "Generative Adversarial Networks (GANs) are the driving force behind the state-of-the-art in image generation. Despite their ability to synthesize high-resolution photo-realistic images, generating content with on-demand conditioning of different granularity remains a challenge. This challenge is usually tackled by annotating massive datasets with the attributes of interest, a laborious task that is not always a viable option. Therefore, it is vital to introduce control into the generation process of unsupervised generative models. In this work, we focus on controllable image generation by leveraging GANs that are well-trained in an unsupervised fashion. To this end, we discover that the representation space of intermediate layers of the generator forms a number of clusters that separate the data according to semantically meaningful attributes (e.g., hair color and pose). By conditioning on the cluster assignments, the proposed method is able to control the semantic class of the generated image. Our approach enables sampling from each cluster by Implicit Maximum Likelihood Estimation (IMLE). We showcase the efficacy of our approach on faces (CelebA-HQ and FFHQ), animals (Imagenet) and objects (LSUN) using different pre-trained generative models. The results highlight the ability of our approach to condition image generation on attributes like gender, pose and hair style on faces, as well as a variety of features on different object classes.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "34291068",
                        "name": "Markos Georgopoulos"
                    },
                    {
                        "authorId": "2059960629",
                        "name": "James Oldfield"
                    },
                    {
                        "authorId": "34586458",
                        "name": "Grigorios G. Chrysos"
                    },
                    {
                        "authorId": "1780393",
                        "name": "Yannis Panagakis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[19] also sought to utilize the GAN latent space for image synthesis with specific properties."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "ad14f8b707cb77135cd99d055d3e69ecb4f98396",
                "externalIds": {
                    "ArXiv": "2112.12705",
                    "DBLP": "journals/corr/abs-2112-12705",
                    "DOI": "10.1109/RBME.2022.3185953",
                    "CorpusId": 245424950,
                    "PubMed": "35737637"
                },
                "corpusId": 245424950,
                "publicationVenue": {
                    "id": "042aa85f-6dca-421f-809b-e0ea76c1e58b",
                    "name": "IEEE Reviews in Biomedical Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE reviews in biomedical engineering",
                        "IEEE Rev Biomed Eng",
                        "IEEE rev biomed eng"
                    ],
                    "issn": "1941-1189",
                    "alternate_issns": [
                        "1937-3333"
                    ],
                    "url": "https://rbme.embs.org/",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=4664312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ad14f8b707cb77135cd99d055d3e69ecb4f98396",
                "title": "Explainable Artificial Intelligence Methods in Combating Pandemics: A Systematic Review",
                "abstract": "Despite the myriad peer-reviewed papers demonstrating novel Artificial Intelligence (AI)-based solutions to COVID-19 challenges during the pandemic, few have made a significant clinical impact, especially in diagnosis and disease precision staging. One major cause for such low impact is the lack of model transparency, significantly limiting the AI adoption in real clinical practice. To solve this problem, AI models need to be explained to users. Thus, we have conducted a comprehensive study of Explainable Artificial Intelligence (XAI) using PRISMA technology. Our findings suggest that XAI can improve model performance, instill trust in the users, and assist users in decision-making. In this systematic review, we introduce common XAI techniques and their utility with specific examples of their application. We discuss the evaluation of XAI results because it is an important step for maximizing the value of AI-based clinical decision support systems. Additionally, we present the traditional, modern, and advanced XAI models to demonstrate the evolution of novel techniques. Finally, we provide a best practice guideline that developers can refer to during the model experimentation. We also offer potential solutions with specific examples for common challenges in AI model experimentation. This comprehensive review, hopefully, can promote AI adoption in biomedicine and healthcare.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49406483",
                        "name": "F. Giuste"
                    },
                    {
                        "authorId": "2153422648",
                        "name": "Wenqi Shi"
                    },
                    {
                        "authorId": "1390861238",
                        "name": "Yuanda Zhu"
                    },
                    {
                        "authorId": "2003837297",
                        "name": "Tarun Naren"
                    },
                    {
                        "authorId": "14233831",
                        "name": "Monica Isgut"
                    },
                    {
                        "authorId": "2295696",
                        "name": "Ying Sha"
                    },
                    {
                        "authorId": "145464452",
                        "name": "L. Tong"
                    },
                    {
                        "authorId": "2123026233",
                        "name": "Mitali S. Gupte"
                    },
                    {
                        "authorId": "2109133022",
                        "name": "May D. Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "bbfebb10f41254ff88937feafcc653df1cfacb5d",
                "externalIds": {
                    "DBLP": "conf/cvpr/Or-ElLSSPK22",
                    "ArXiv": "2112.11427",
                    "DOI": "10.1109/CVPR52688.2022.01314",
                    "CorpusId": 245353683
                },
                "corpusId": 245353683,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bbfebb10f41254ff88937feafcc653df1cfacb5d",
                "title": "StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation",
                "abstract": "We introduce a high resolution, 3D-consistent image and shape generation technique which we call StyleSDF. Our method is trained on single-view RGB data only, and stands on the shoulders of StyleGAN2 for image generation, while solving two main challenges in 3D-aware GANs: 1) high-resolution, view-consistent generation of the RGB images, and 2) detailed 3D shape. We achieve this by merging a SDF-based 3D representation with a style-based 2D generator. Our 3D implicit network renders low-resolution feature maps, from which the style-based network generates view-consistent, 1024\u00d71024 images. Notably, our SDF-based 3D modeling defines detailed 3D surfaces, leading to consistent volume rendering. Our method shows higher quality results compared to state of the art in terms of visual and geometric quality.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1405516702",
                        "name": "Roy Or-El"
                    },
                    {
                        "authorId": "2115611109",
                        "name": "Xuan Luo"
                    },
                    {
                        "authorId": "153142877",
                        "name": "Mengyi Shan"
                    },
                    {
                        "authorId": "2177801",
                        "name": "Eli Shechtman"
                    },
                    {
                        "authorId": "2148838020",
                        "name": "Jeong Joon Park"
                    },
                    {
                        "authorId": "1397689071",
                        "name": "Ira Kemelmacher-Shlizerman"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[10] observed that applying PCA on the latent space of Style-GAN and BigGAN retrieves humaninterpretable directions.",
                "[27] and [22] attempts to learn directions that are easily distinguishable while [25], [26] and [10] finds directions of maximum variance.",
                "[27] performs unsupervised learning to identify distinguishable directions while [10], [26] and [25] obtains directions analytically."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "5d220f0fcfdb2c8f74996dfc0adeb27fdce453d5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-08835",
                    "ArXiv": "2112.08835",
                    "DOI": "10.1609/aaai.v36i7.20667",
                    "CorpusId": 245216804
                },
                "corpusId": 245216804,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/5d220f0fcfdb2c8f74996dfc0adeb27fdce453d5",
                "title": "Self-supervised Enhancement of Latent Discovery in GANs",
                "abstract": "Several methods for discovering interpretable directions in the latent space of pre-trained GANs have been proposed. Latent semantics discovered by unsupervised methods are less disentangled than supervised methods since they do not use pre-trained attribute classifiers. We propose Scale Ranking Estimator (SRE), which is trained using self-supervision. SRE enhances the disentanglement in directions obtained by existing unsupervised disentanglement techniques. These directions are updated to preserve the ordering of variation within each direction in latent space. Qualitative and quantitative evaluation of the discovered directions demonstrates that our proposed method significantly improves disentanglement in various datasets. We also show that the learned SRE can be used to perform Attribute-based image retrieval task without any training.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1754580394",
                        "name": "Silpa Vadakkeeveetil Sreelatha"
                    },
                    {
                        "authorId": "2145725770",
                        "name": "Adarsh Kappiyath"
                    },
                    {
                        "authorId": "25151823",
                        "name": "S. Sumitra"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Analyses for latent space of the generator were also performed to manipulate the semantic of the generation (Peebles et al. 2020; H\u00e4rk\u00f6nen et al. 2020).",
                "Analyses for latent space of the generator were also performed to manipulate the semantic of the generation (Peebles et al. 2020; Ha\u0308rko\u0308nen et al. 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a5f31f5f46f4eec87fde33a9b856d52ac1ea2018",
                "externalIds": {
                    "ArXiv": "2112.08814",
                    "DBLP": "conf/aaai/JeongHC22",
                    "DOI": "10.1609/aaai.v36i1.19989",
                    "CorpusId": 245219161
                },
                "corpusId": 245219161,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/a5f31f5f46f4eec87fde33a9b856d52ac1ea2018",
                "title": "An Unsupervised Way to Understand Artifact Generating Internal Units in Generative Neural Networks",
                "abstract": "Despite significant improvements on the image generation performance of Generative Adversarial Networks (GANs), generations with low visual fidelity still have been observed. As widely used metrics for GANs focus more on the overall performance of the model, evaluation on the quality of individual generations or detection of defective generations is challenging. While recent studies try to detect featuremap units that cause artifacts and evaluate individual samples, these approaches require additional resources such as external networks or a number of training data to approximate the real data manifold. \n In this work, we propose the concept of local activation, and devise a metric on the local activation to detect artifact generations without additional supervision.\n We empirically verify that our approach can detect and correct artifact generations from GANs with various datasets. Finally, we discuss a geometrical analysis to partially reveal the relation between the proposed concept and low visual fidelity.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2105663791",
                        "name": "Haedong Jeong"
                    },
                    {
                        "authorId": "12211976",
                        "name": "Jiyeon Han"
                    },
                    {
                        "authorId": "7629367",
                        "name": "Jaesik Choi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANSpace [10] proposes to apply Principal Component Analysis (PCA) [33] to randomly sampled latent vectors of the intermediate layers of BigGAN and StyleGAN models.",
                "Most of this work discovers domain-independent and interpretable directions such as zoom-in, rotation, and translation [21, 10, 30], while other frameworks propose to find a set of domain-specific directions such as hair color or gender on face images [25] or cognitive features [8]."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "958cd33eb354d7f3d117767c2ff981614c940c8f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-08493",
                    "ArXiv": "2112.08493",
                    "DOI": "10.1109/WACV51458.2022.00350",
                    "CorpusId": 245218520
                },
                "corpusId": 245218520,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/958cd33eb354d7f3d117767c2ff981614c940c8f",
                "title": "StyleMC: Multi-Channel Based Fast Text-Guided Image Generation and Manipulation",
                "abstract": "Discovering meaningful directions in the latent space of GANs to manipulate semantic attributes typically requires large amounts of labeled data. Recent work aims to overcome this limitation by leveraging the power of Contrastive Language-Image Pre-training (CLIP), a joint text-image model. While promising, these methods require several hours of preprocessing or training to achieve the desired manipulations. In this paper, we present StyleMC, a fast and efficient method for text-driven image generation and manipulation. StyleMC uses a CLIP-based loss and an identity loss to manipulate images via a single text prompt without significantly affecting other attributes. Unlike prior work, StyleMC requires only a few seconds of training per text prompt to find stable global directions, does not require prompt engineering and can be used with any pre-trained StyleGAN2 model. We demonstrate the effectiveness of our method and compare it to state-of-the-art methods. Our code can be found at http://catlab-team.github.io/stylemc.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2145258020",
                        "name": "Umut Kocasari"
                    },
                    {
                        "authorId": "2145260636",
                        "name": "Alara Dirik"
                    },
                    {
                        "authorId": "34907481",
                        "name": "Mert Tiftikci"
                    },
                    {
                        "authorId": "3137679",
                        "name": "Pinar Yanardag"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Most previous works find that the style space [23, 49, 51, 59, 60], a feature layer after the first 8-layer MLP of the StyleGAN generator, reveals fascinating semantic controlling over synthesis images."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "9e19ec84780a7809e3997a26881e0e5b8a131066",
                "externalIds": {
                    "ArXiv": "2112.07200",
                    "DBLP": "journals/corr/abs-2112-07200",
                    "DOI": "10.1109/CVPR52688.2022.00343",
                    "CorpusId": 245131380
                },
                "corpusId": 245131380,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9e19ec84780a7809e3997a26881e0e5b8a131066",
                "title": "Weakly Supervised High-Fidelity Clothing Model Generation",
                "abstract": "The development of online economics arouses the demand of generating images of models on product clothes, to display new clothes and promote sales. However, the expensive proprietary model images challenge the existing image virtual try-on methods in this scenario, as most of them need to be trained on considerable amounts of model images accompanied with paired clothes images. In this paper, we propose a cheap yet scalable weakly-supervised method called Deep Generative Projection (DGP) to address this specific scenario. Lying in the heart of the proposed method is to imitate the process of human predicting the wearing effect, which is an unsupervised imagination based on life experience rather than computation rules learned from supervisions. Here a pretrained StyleGAN is used to capture the practical experience of wearing. Experiments show that projecting the rough alignment of clothing and body onto the StyleGAN space can yield photo-realistic wearing results. Experiments on real scene proprietary model images demonstrate the superiority of DGP over several state-of-the-art supervised methods when generating clothing model images.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2119237546",
                        "name": "Ruili Feng"
                    },
                    {
                        "authorId": "2150968779",
                        "name": "Cheng Ma"
                    },
                    {
                        "authorId": "2152868014",
                        "name": "Chengji Shen"
                    },
                    {
                        "authorId": "2109103156",
                        "name": "Xin Gao"
                    },
                    {
                        "authorId": "2109106862",
                        "name": "Zhenjiang Liu"
                    },
                    {
                        "authorId": "2144427153",
                        "name": "Xiaobo Li"
                    },
                    {
                        "authorId": "2794841",
                        "name": "Kairi Ou"
                    },
                    {
                        "authorId": "143962510",
                        "name": "Zhengjun Zha"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This can be achieved with unsupervised and self-supervised methods [38,41,47,66], or an auxiliary signal [30, 56].",
                "Many disentanglement methods assume all images or image attributes can be combined (\u201cmixed-n-matched\u201d) with all others [30, 38, 41, 47, 56, 59, 65, 66]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ee39cfb34dd821844cda9836b5716551d9e618e1",
                "externalIds": {
                    "ArXiv": "2112.06909",
                    "DBLP": "conf/eccv/BrooksE22",
                    "DOI": "10.1007/978-3-031-19787-1_29",
                    "CorpusId": 245124211
                },
                "corpusId": 245124211,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/ee39cfb34dd821844cda9836b5716551d9e618e1",
                "title": "Hallucinating Pose-Compatible Scenes",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2679394",
                        "name": "Tim Brooks"
                    },
                    {
                        "authorId": "1763086",
                        "name": "Alexei A. Efros"
                    }
                ]
            }
        },
        {
            "contexts": [
                "InterfaceGAN [39] finds linear directions to edit latent codes in a supervised manner, while GANSpace [21] extracts unsupervised linear directions for editing using PCA in the W space.",
                "We use the Zero-Shot scores to compare three sets of images; (a) the positive images in X+ as a baseline, (b) the results of applying an edit using our approach in StyleGAN W+ space, and (c) the result of a GANSpace edit that was labeled with the same attribute by [21].",
                "Image editing frameworks in the StyleGAN domain [Abdal et al. 2021b; H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020; Tewari et al. 2020a] analyze the latent space to identify linear and non-linear paths for semantic editing.",
                "2020] finds linear directions to edit latent codes in a supervised manner, while GANSpace [H\u00e4rk\u00f6nen et al. 2020] extracts unsupervised linear directions for editing using PCA in theW space.",
                "Alternatively, GANSpace [H\u00e4rk\u00f6nen et al. 2020] finds latent space manipulations in an unsupervised manner but then the directions themselves are manually labeled and curated in a post process.",
                "We identify GANSpace [21] as the closest (unsupervised) method to compare with.",
                "Note that the \u201cGender/Male\u201d direction has a higher accuracy score for GANSpace but comparison with the Table 1 in the main paper indicates that the edit might retain some features or changes the attribute minimally which is not desirable.",
                "Note that in GANSpace the labels are assigned manually and all directions are curated.",
                "We identify GANSpace [H\u00e4rk\u00f6nen et al. 2020] as the closest (unsupervised) method to compare with.",
                "domain, various works [Bau et al. 2018, 2019; H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020; Tewari et al. 2020a; Wang et al. 2021] explore latent spaces to enable high quality image editing applications.",
                "2020b,a] and semantic control [Abdal et al. 2021b; H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020] together has enabled semantic editing of existing photographs.",
                "We use the Zero-Shot scores to compare three sets of images; (a) the positive images in X+ as a baseline, (b) the results of applying an edit using our approach in StyleGANW + space, and (c) the result of a GANSpace edit that was labeled with the same attribute by [H\u00e4rk\u00f6nen et al. 2020]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "fc7aa4067fd44dd7c83790b9f7c1029c78ea775f",
                "externalIds": {
                    "ArXiv": "2112.05219",
                    "DBLP": "journals/corr/abs-2112-05219",
                    "DOI": "10.1145/3528233.3530747",
                    "CorpusId": 245117814
                },
                "corpusId": 245117814,
                "publicationVenue": {
                    "id": "cf6b5e76-9274-46e6-a2dd-7c190ec2ec5f",
                    "name": "International Conference on Computer Graphics and Interactive Techniques",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Graph Interact Tech",
                        "SIGGRAPH"
                    ],
                    "url": "http://www.siggraph.org/"
                },
                "url": "https://www.semanticscholar.org/paper/fc7aa4067fd44dd7c83790b9f7c1029c78ea775f",
                "title": "CLIP2StyleGAN: Unsupervised Extraction of StyleGAN Edit Directions",
                "abstract": "The success of StyleGAN has enabled unprecedented semantic editing capabilities, on both synthesized and real images. However, such editing operations are either trained with semantic supervision or annotated manually by users. In another development, the CLIP architecture has been trained with internet-scale loose image and text pairings, and has been shown to be useful in several zero-shot learning settings. In this work, we investigate how to effectively link the pretrained latent spaces of StyleGAN and CLIP, which in turn allows us to automatically extract semantically-labeled edit directions from StyleGAN, finding and naming meaningful edit operations, in a fully unsupervised setup, without additional human guidance. Technically, we propose two novel building blocks; one for discovering interesting CLIP directions and one for semantically labeling arbitrary directions in CLIP latent space. The setup does not assume any pre-determined labels and hence we do not require any additional supervised text/attributes to build the editing framework. We evaluate the effectiveness of the proposed method and demonstrate that extraction of disentangled labeled StyleGAN edit directions is indeed possible, revealing interesting and non-trivial edit directions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "94395014",
                        "name": "Rameen Abdal"
                    },
                    {
                        "authorId": "100516590",
                        "name": "Peihao Zhu"
                    },
                    {
                        "authorId": "145685385",
                        "name": "John C. Femiani"
                    },
                    {
                        "authorId": "1710455",
                        "name": "N. Mitra"
                    },
                    {
                        "authorId": "1798011",
                        "name": "Peter Wonka"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Instead, we parameterize c as a linear combination of the top-N principal directions ofW space [28, 74]:"
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "03871045478e9a5062c336b16230e4a79d488052",
                "externalIds": {
                    "DBLP": "conf/cvpr/PeeblesZ00ES22",
                    "ArXiv": "2112.05143",
                    "DOI": "10.1109/CVPR52688.2022.01311",
                    "CorpusId": 245005830
                },
                "corpusId": 245005830,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/03871045478e9a5062c336b16230e4a79d488052",
                "title": "GAN-Supervised Dense Visual Alignment",
                "abstract": "We propose GAN-Supervised Learning, a framework for learning discriminative models and their GAN-generated training data jointly end-to-end. We apply our framework to the dense visual alignment problem. Inspired by the classic Congealing method, our GAN gealing algorithm trains a Spatial Transformer to map random samples from a GAN trained on unaligned data to a common, jointly-learned target mode. We show results on eight datasets, all of which demonstrate our method successfully aligns complex data and discovers dense correspondences. GANgealing significantly outperforms past self-supervised correspondence algorithms and performs on-par with (and sometimes exceeds) state-of-the-art supervised correspondence algorithms on several datasets-without making use of any correspondence supervision or data augmentation and despite being trained exclusively on GAN-generated data. For precise correspondence, we improve upon state-of-the-art supervised methods by as much as 3 \u00d7. We show applications of our method for augmented reality, image editing and automated pre-processing of image datasets for downstream GAN training.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35235273",
                        "name": "William S. Peebles"
                    },
                    {
                        "authorId": "1922024303",
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "authorId": "2109976035",
                        "name": "Richard Zhang"
                    },
                    {
                        "authorId": "143805211",
                        "name": "A. Torralba"
                    },
                    {
                        "authorId": "1763086",
                        "name": "Alexei A. Efros"
                    },
                    {
                        "authorId": "2177801",
                        "name": "Eli Shechtman"
                    }
                ]
            }
        },
        {
            "contexts": [
                "ral network in the latent space of StyleGAN, one could control the global attributes [4, 26, 59, 60] or 3D structure [64] of the generated images.",
                "For example, InterFaceGAN [59], GANSpace [26] and StyleFlow [4] trains a attribute model in the StyleGAN latent space to control binary attributes."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "2997e94ab3dccaa39eee8bd68f23b1ff30da4c80",
                "externalIds": {
                    "DBLP": "conf/cvpr/ShiYWS22",
                    "ArXiv": "2112.02236",
                    "DOI": "10.1109/CVPR52688.2022.01097",
                    "CorpusId": 244909015
                },
                "corpusId": 244909015,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2997e94ab3dccaa39eee8bd68f23b1ff30da4c80",
                "title": "SemanticStyleGAN: Learning Compositional Generative Priors for Controllable Image Synthesis and Editing",
                "abstract": "Recent studies have shown that StyleGANs provide promising prior models for downstream tasks on image synthesis and editing. However since the latent codes of StyleGANs are designed to control global styles it is hard to achieve a fine-grained control over synthesized images. We present SemanticStyleGAN where a generator is trained to model local semantic parts separately and synthesizes images in a compositional way. The structure and texture of different local parts are controlled by corresponding latent codes. Experimental results demonstrate that our model provides a strong disentanglement between different spatial areas. When combined with editing methods designed for StyleGANs it can achieve a more fine-grained control to edit synthesized or real images. The model can also be extended to other domains via transfer learning. Thus as a generic prior model with built-in disentanglement it could facilitate the development of GAN-based applications and enable more potential downstream tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9644181",
                        "name": "Yichun Shi"
                    },
                    {
                        "authorId": "2109457982",
                        "name": "Xiao Yang"
                    },
                    {
                        "authorId": "2015963130",
                        "name": "Yangyue Wan"
                    },
                    {
                        "authorId": "1720987",
                        "name": "Xiaohui Shen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Direct manipulations of latent representations inside generative models have been used to create human-understandable changes in synthesized images [Bau et al., 2019, Jahanian et al., 2019, Goetschalckx et al., 2019, Shen et al., 2020, H\u00e4rk\u00f6nen et al., 2020, Wu et al., 2020]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "ec4f3b701c6d97fc73d5afdede41a6510092320f",
                "externalIds": {
                    "ArXiv": "2112.01008",
                    "DBLP": "conf/nips/SanturkarTEBTM21",
                    "CorpusId": 244799629
                },
                "corpusId": 244799629,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/ec4f3b701c6d97fc73d5afdede41a6510092320f",
                "title": "Editing a classifier by rewriting its prediction rules",
                "abstract": "We present a methodology for modifying the behavior of a classifier by directly rewriting its prediction rules. Our approach requires virtually no additional data collection and can be applied to a variety of settings, including adapting a model to new environments, and modifying it to ignore spurious features. Our code is available at https://github.com/MadryLab/EditingClassifiers .",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2852106",
                        "name": "Shibani Santurkar"
                    },
                    {
                        "authorId": "2754804",
                        "name": "Dimitris Tsipras"
                    },
                    {
                        "authorId": "2142836269",
                        "name": "Mahalaxmi Elango"
                    },
                    {
                        "authorId": "144159726",
                        "name": "David Bau"
                    },
                    {
                        "authorId": "143805211",
                        "name": "A. Torralba"
                    },
                    {
                        "authorId": "143826246",
                        "name": "A. Madry"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Varying Textures, Backgrounds and More As shown in [14, 29], it is hard to control texture and background in standard GANs.",
                "[14] applied principal component analysis to the GAN space to create interpretable controls for image synthesis."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "98598d0c472694f3bdaf84997400c98d9ec7ffd6",
                "externalIds": {
                    "ArXiv": "2112.01573",
                    "DBLP": "journals/corr/abs-2112-01573",
                    "CorpusId": 244896415
                },
                "corpusId": 244896415,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/98598d0c472694f3bdaf84997400c98d9ec7ffd6",
                "title": "FuseDream: Training-Free Text-to-Image Generation with Improved CLIP+GAN Space Optimization",
                "abstract": "Generating images from natural language instructions is an intriguing yet highly challenging task. We approach text-to-image generation by combining the power of the retrained CLIP representation with an off-the-shelf image generator (GANs), optimizing in the latent space of GAN to find images that achieve maximum CLIP score with the given input text. Compared to traditional methods that train generative models from text to image starting from scratch, the CLIP+GAN approach is training-free, zero shot and can be easily customized with different generators. However, optimizing CLIP score in the GAN space casts a highly challenging optimization problem and off-the-shelf optimizers such as Adam fail to yield satisfying results. In this work, we propose a FuseDream pipeline, which improves the CLIP+GAN approach with three key techniques: 1) an AugCLIP score which robustifies the CLIP objective by introducing random augmentation on image. 2) a novel initialization and over-parameterization strategy for optimization which allows us to efficiently navigate the non-convex landscape in GAN space. 3) a composed generation technique which, by leveraging a novel bi-level optimization formulation, can compose multiple images to extend the GAN space and overcome the data-bias. When promoted by different input text, FuseDream can generate high-quality images with varying objects, backgrounds, artistic styles, even novel counterfactual concepts that do not appear in the training data of the GAN we use. Quantitatively, the images generated by FuseDream yield top-level Inception score and FID score on MS COCO dataset, without additional architecture design or training. Our code is publicly available at \\url{https://github.com/gnobitab/FuseDream}.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "46521757",
                        "name": "Xingchao Liu"
                    },
                    {
                        "authorId": "29777869",
                        "name": "Chengyue Gong"
                    },
                    {
                        "authorId": "8687492",
                        "name": "Lemeng Wu"
                    },
                    {
                        "authorId": "2107944048",
                        "name": "Shujian Zhang"
                    },
                    {
                        "authorId": "49466406",
                        "name": "Haoran Su"
                    },
                    {
                        "authorId": "47362268",
                        "name": "Qiang Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For a certain attribute, they search for a certain direction in the latent space, and then alter the target attribute via moving the latent code z along the searched direction [3,4,8,10,11]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "cea664d3ac312e9db549095bc0d1e35e18beb502",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-00718",
                    "ArXiv": "2112.00718",
                    "DOI": "10.1109/CVPR52688.2022.01100",
                    "CorpusId": 244772988
                },
                "corpusId": 244772988,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/cea664d3ac312e9db549095bc0d1e35e18beb502",
                "title": "Improving GAN Equilibrium by Raising Spatial Awareness",
                "abstract": "The success of Generative Adversarial Networks (GANs) is largely built upon the adversarial training between a generator (G) and a discriminator (D). They are expected to reach a certain equilibrium where D cannot distinguish the generated images from the real ones. However, such an equilibrium is rarely achieved in practical GAN training, instead, D almost always surpasses G. We attribute one of its sources to the information asymmetry between D and G. We observe that D learns its own visual attention when determining whether an image is real or fake, but G has no explicit clue on which regions to focus on for a particular synthesis. To alleviate the issue of D dominating the competition in GANs, we aim to raise the spatial awareness of G. Randomly sampled multi-level heatmaps are encoded into the intermediate layers of G as an inductive bias. Thus G can purposefully improve the synthesis of certain image regions. We further propose to align the spatial awareness of G with the attention map induced from D. Through this way we effectively lessen the information gap between D and G. Extensive results show that our method pushes the two-player game in GANs closer to the equilibrium, leading to a better synthesis performance. As a byproduct, the intro-duced spatial awareness facilitates interactive editing over the output synthesis. Demo video and code are available at https://genforce.github.io/eqgan-sa/",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1832343458",
                        "name": "Jianyuan Wang"
                    },
                    {
                        "authorId": "49984891",
                        "name": "Ceyuan Yang"
                    },
                    {
                        "authorId": "2118669478",
                        "name": "Yinghao Xu"
                    },
                    {
                        "authorId": "2117687899",
                        "name": "Yujun Shen"
                    },
                    {
                        "authorId": "46382489",
                        "name": "Hongdong Li"
                    },
                    {
                        "authorId": "145291669",
                        "name": "Bolei Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As a result, other studies [16, 40, 46, 47] propose unsupervised approaches to accomplish the same aim without the need for manual annotations.",
                "Several studies [7, 16, 40, 41, 50] have been conducted to examine the latent spaces learned by GANs,",
                "Editing directions on human facial domain are obtained from InterFaceGAN [41], while those for church domain are from GANSpace [16]."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "026841c383179ef509ff6e5c486dfcdf33ba71d0",
                "externalIds": {
                    "ArXiv": "2112.00719",
                    "DBLP": "conf/cvpr/Dinh0NH22",
                    "DOI": "10.1109/CVPR52688.2022.01110",
                    "CorpusId": 244772984
                },
                "corpusId": 244772984,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/026841c383179ef509ff6e5c486dfcdf33ba71d0",
                "title": "HyperInverter: Improving StyleGAN Inversion via Hypernetwork",
                "abstract": "Real-world image manipulation has achieved fantastic progress in recent years as a result of the exploration and utilization of GAN latent spaces. GAN inversion is the first step in this pipeline, which aims to map the real image to the latent code faithfully. Unfortunately, the majority of existing GAN inversion methods fail to meet at least one of the three requirements listed below: high reconstruction quality, editability, and fast inference. We present a novel two-phase strategy in this research that fits all requirements at the same time. In the first phase, we train an encoder to map the input image to StyleGAN2 W-space, which was proven to have excellent editability but lower reconstruction quality. In the second phase, we supplement the reconstruction ability in the initial phase by leveraging a series of hypernetworks to recover the missing information during inversion. These two steps complement each other to yield high reconstruction quality thanks to the hypernetwork branch and excellent editability due to the inversion done in the W-space. Our method is entirely encoder-based, resulting in extremely fast inference. Extensive experiments on two challenging datasets demonstrate the superiority of our method.11Project page: https://di-mi-ta.github.io/HyperInverter",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2142664005",
                        "name": "Tan M. Dinh"
                    },
                    {
                        "authorId": "145830668",
                        "name": "A. Tran"
                    },
                    {
                        "authorId": "2789311",
                        "name": "R. Nguyen"
                    },
                    {
                        "authorId": "143807806",
                        "name": "Binh-Son Hua"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Using the pre-trained model, several works discover directions in the latent space that correspond to spatial or semantic changes [H\u00e4rk\u00f6nen et al. 2020; Jahanian et al. 2020; Peebles et al. 2020; Shen and Zhou 2021; Shoshan et al. 2021]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "f4b5f2e5c2102192973e019b34285c9237227534",
                "externalIds": {
                    "DBLP": "journals/tog/AlBaharLYSSH21",
                    "DOI": "10.1145/3478513.3480559",
                    "CorpusId": 245015628
                },
                "corpusId": 245015628,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f4b5f2e5c2102192973e019b34285c9237227534",
                "title": "Pose with style",
                "abstract": "We present an algorithm for re-rendering a person from a single image under arbitrary poses. Existing methods often have difficulties in hallucinating occluded contents photo-realistically while preserving the identity and fine details in the source image. We first learn to inpaint the correspondence field between the body surface texture and the source image with a human body symmetry prior. The inpainted correspondence field allows us to transfer/warp local features extracted from the source to the target view even under large pose changes. Directly mapping the warped local features to an RGB image using a simple CNN decoder often leads to visible artifacts. Thus, we extend the StyleGAN generator so that it takes pose as input (for controlling poses) and introduces a spatially varying modulation for the latent space using the warped local features (for controlling appearances). We show that our method compares favorably against the state-of-the-art algorithms in both quantitative evaluation and visual comparison.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "23982870",
                        "name": "Badour Albahar"
                    },
                    {
                        "authorId": "2054975",
                        "name": "Jingwan Lu"
                    },
                    {
                        "authorId": "2109724839",
                        "name": "Jimei Yang"
                    },
                    {
                        "authorId": "2496409",
                        "name": "Zhixin Shu"
                    },
                    {
                        "authorId": "2177801",
                        "name": "Eli Shechtman"
                    },
                    {
                        "authorId": "2238908897",
                        "name": "Jia-Bin Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We perform various edits [24, 48, 56] over latent codes obtained by each inversion method.",
                "Thanks to their semantically rich latent representations, many works have utilized these models to facilitate diverse and expressive editing through latent space manipulations [4, 6, 9, 12, 24, 38, 44, 48, 56].",
                "These range from full-supervision in the form of semantic labels [3, 16, 19, 56] and facial priors [59,60] to unsupervised approaches [24,57,63,64]."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "0c6838f8b1728c8fa5f10d5a3e4a6000e84438e7",
                "externalIds": {
                    "ArXiv": "2111.15666",
                    "DBLP": "journals/corr/abs-2111-15666",
                    "DOI": "10.1109/CVPR52688.2022.01796",
                    "CorpusId": 244729249
                },
                "corpusId": 244729249,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0c6838f8b1728c8fa5f10d5a3e4a6000e84438e7",
                "title": "HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing",
                "abstract": "The inversion of real images into StyleGAN's latent space is a well-studied problem. Nevertheless, applying existing approaches to real-world scenarios remains an open challenge, due to an inherent trade-off between reconstruction and editability: latent space regions which can accurately represent real images typically suffer from degraded semantic control. Recent work proposes to mitigate this trade-off by fine-tuning the generator to add the target image to well-behaved, editable regions of the latent space. While promising, this fine-tuning scheme is impractical for prevalent use as it requires a lengthy training phase for each new image. In this work, we introduce this approach into the realm of encoder-based inversion. We propose HyperStyle, a hypernetwork that learns to modulate StyleGAN's weights to faithfully express a given image in editable regions of the latent space. A naive modulation approach would require training a hypernetwork with over three billion parameters. Through careful network design, we reduce this to be in line with existing encoders. HyperStyle yields reconstructions comparable to those of optimization techniques with the near real-time inference capabilities of encoders. Lastly, we demonstrate HyperStyle's effectiveness on several applications beyond the inversion task, including the editing of out-of-domain images which were never seen during training. Code is available on our project page: https://yuval-alaluf.github.io/hyperstyle/.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1850630812",
                        "name": "Yuval Alaluf"
                    },
                    {
                        "authorId": "2047833213",
                        "name": "Omer Tov"
                    },
                    {
                        "authorId": "147940380",
                        "name": "Ron Mokady"
                    },
                    {
                        "authorId": "134639223",
                        "name": "Rinon Gal"
                    },
                    {
                        "authorId": "1755628",
                        "name": "Amit H. Bermano"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANSpace [6] and StyleSpace [16] allow image manipulation with interpretable controls from a pre-trained GAN generator."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "ebed1b191de62aa4e595291062b39612cec5c578",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-00007",
                    "ArXiv": "2112.00007",
                    "DOI": "10.1109/CVPR52688.2022.00337",
                    "CorpusId": 244773603
                },
                "corpusId": 244773603,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ebed1b191de62aa4e595291062b39612cec5c578",
                "title": "Sound-Guided Semantic Image Manipulation",
                "abstract": "The recent success of the generative model shows that leveraging the multi-modal embedding space can manipu-late an image using text information. However, manipulating an image with other sources rather than text, such as sound, is not easy due to the dynamic characteristics of the sources. Especially, sound can convey vivid emotions and dynamic expressions of the real world. Here, we propose a framework that directly encodes sound into the multi-modal (image-text) embedding space and manipulates an image from the space. Our audio encoder is trained to pro-duce a latent representation from an audio input, which is forced to be aligned with image and text representations in the multi-modal embedding space. We use a direct latent op-timization method based on aligned embeddings for sound-guided image manipulation. We also show that our method can mix different modalities, i.e., text and audio, which en-rich the variety of the image modification. The experiments on zero-shot audio classification and semantic-level image classification show that our proposed model outperforms other text and sound-guided state-of-the-art methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2157283238",
                        "name": "Seung Hyun Lee"
                    },
                    {
                        "authorId": "95198164",
                        "name": "W. Roh"
                    },
                    {
                        "authorId": "145965455",
                        "name": "Wonmin Byeon"
                    },
                    {
                        "authorId": "2148390098",
                        "name": "Sang Ho Yoon"
                    },
                    {
                        "authorId": "2149054773",
                        "name": "Chan Young Kim"
                    },
                    {
                        "authorId": "2109217820",
                        "name": "Jinkyu Kim"
                    },
                    {
                        "authorId": "2142668751",
                        "name": "Sangpil Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Besides producing impressive image samples, GANs [15] have been shown to learn meaningful latent spaces [27] with extensive studies on multiple derived spaces [24, 57] and various knobs and controls for conditional human face generation [21, 37, 55]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "b582edb16f5425642767cb6c26839111f867f4dc",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-15640",
                    "ArXiv": "2111.15640",
                    "DOI": "10.1109/CVPR52688.2022.01036",
                    "CorpusId": 244729224
                },
                "corpusId": 244729224,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b582edb16f5425642767cb6c26839111f867f4dc",
                "title": "Diffusion Autoencoders: Toward a Meaningful and Decodable Representation",
                "abstract": "Diffusion probabilistic models (DPMs) have achieved remarkable quality in image generation that rivals GANs'. But unlike GANs, DPMs use a set of latent variables that lack semantic meaning and cannot serve as a useful representation for other tasks. This paper explores the possibility of using DPMs for representation learning and seeks to extract a meaningful and decodable representation of an input image via autoencoding. Our key idea is to use a learnable encoder for discovering the high-level semantics, and a DPM as the decoder for modeling the remaining stochastic variations. Our method can encode any image into a two-part latent code where the first part is semantically meaningful and linear, and the second part captures stochastic details, allowing near-exact reconstruction. This capability enables challenging applications that currently foil GAN-based methods, such as attribute manipulation on real images. We also show that this two-level encoding improves denoising efficiency and naturally facilitates various downstream tasks including few-shot conditional sampling. Please visit our page: https://Diff-AE.github.io/",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1471755827",
                        "name": "Konpat Preechakul"
                    },
                    {
                        "authorId": "2142548186",
                        "name": "Nattanat Chatthee"
                    },
                    {
                        "authorId": "2052302070",
                        "name": "Suttisak Wizadwongsa"
                    },
                    {
                        "authorId": "37016781",
                        "name": "Supasorn Suwajanakorn"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", on images [1, 37] and on unsupervisedly discovered directions in the latent space [12, 38, 43, 44].",
                "Recently, growing numbers of works focus on latent space image manipulation [12,13,37,41,45,48] because of the remarkable large scale GANs like StyleGAN [15, 16], which can generate high-resolution images with well disentangled latent space.",
                "Disentangled image manipulation [1,8,10,12,21,23,37, 38, 43, 44] aiming at changing the desired attributes of the image while keeping the others unchanged, has long been studied for its research significance and application value.",
                "Despite the convenience of directly using the pre-trained GANs to generate images, all these methods need human annotations [1, 12, 37, 38, 43, 44]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "38d4d2b8fc82246675fcb8b64db7756c2f80c330",
                "externalIds": {
                    "DBLP": "conf/cvpr/XuLTLHSTGD22",
                    "ArXiv": "2111.13333",
                    "DOI": "10.1109/CVPR52688.2022.01769",
                    "CorpusId": 244709276
                },
                "corpusId": 244709276,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/38d4d2b8fc82246675fcb8b64db7756c2f80c330",
                "title": "Predict, Prevent, and Evaluate: Disentangled Text-Driven Image Manipulation Empowered by Pre-Trained Vision-Language Model",
                "abstract": "To achieve disentangled image manipulation, previous works depend heavily on manual annotation. Meanwhile, the available manipulations are limited to a pre-defined set the models were trainedfor. We propose a novelframework, i.e., Predict, Prevent, and Evaluate (PPE), for disentangled text-driven image manipulation that requires little manual annotation while being applicable to a wide variety of ma-nipulations. Our method approaches the targets by deeply exploiting the power of the large-scale pre-trained vision-language model CLIP [32]. Concretely, we firstly Predict the possibly entangled attributes for a given text command. Then, based on the predicted attributes, we introduce an entanglement loss to Prevent entanglements during training. Finally, we propose a new evaluation metric to Evaluate the disentangled image manipulation. We verify the effectiveness of our method on the challenging face editing task. Extensive experiments show that the proposed PPE frame-work achieves much better quantitative and qualitative re-sults than the up-to-date StyleCLIP [31] baseline. Code is available at https://github.com/zipengxuc/PPE.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2117883527",
                        "name": "Zipeng Xu"
                    },
                    {
                        "authorId": "2115348355",
                        "name": "Tianwei Lin"
                    },
                    {
                        "authorId": "145462888",
                        "name": "Hao Tang"
                    },
                    {
                        "authorId": "2146340269",
                        "name": "Fu Li"
                    },
                    {
                        "authorId": "2192303",
                        "name": "Dongliang He"
                    },
                    {
                        "authorId": "1703601",
                        "name": "N. Sebe"
                    },
                    {
                        "authorId": "1732855",
                        "name": "R. Timofte"
                    },
                    {
                        "authorId": "1681236",
                        "name": "L. Gool"
                    },
                    {
                        "authorId": "12081764",
                        "name": "Errui Ding"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The latent space \u03a9 can be any of Z,W,W+,S for StyleGAN generator as in [16], and Z,Z+ for BigGAN generator as in [10].",
                "Several recent works control the semantics of the GANgenerated image by tweaking the latent code to perform global [9, 10, 11, 12, 13, 14] or localized [7, 15, 16, 17] editing.",
                "GAN-based Image Editing Algorithms Supervision A B C D E F G [10, 11, 12] Unsupervised 3 3 3 3 3 7 7",
                "Some works use an unsupervised approach to discover meaningful latent space directions, and then manually attribute a semantic to each of the found directions [10, 11, 12, 19].",
                "Meanwhile, we allow layer-wise (both coarse and fine-grained) semantic editing similar to [10] for both style-based GANs and BigGAN."
            ],
            "isInfluential": true,
            "intents": [],
            "citingPaper": {
                "paperId": "507640f2b70b1fcfdb1a35350c3fb6f05948602b",
                "externalIds": {
                    "ArXiv": "2111.12583",
                    "DBLP": "journals/corr/abs-2111-12583",
                    "DOI": "10.1109/ICASSP43922.2022.9747326",
                    "CorpusId": 244527329
                },
                "corpusId": 244527329,
                "publicationVenue": {
                    "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
                    "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Acoust Speech Signal Process",
                        "IEEE Int Conf Acoust Speech Signal Process",
                        "ICASSP",
                        "International Conference on Acoustics, Speech, and Signal Processing"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
                },
                "url": "https://www.semanticscholar.org/paper/507640f2b70b1fcfdb1a35350c3fb6f05948602b",
                "title": "Optimizing Latent Space Directions for Gan-Based Local Image Editing",
                "abstract": "Generative Adversarial Network (GAN) based localized image editing can suffer from ambiguity between semantic at-tributes. We thus present a novel objective function to evaluate the locality of an image edit. By introducing the super-vision from a pre-trained segmentation network and optimizing the objective function, our framework, called Locally Effective Latent Space Direction (LELSD), is applicable to any dataset and GAN architecture. Our method is also computationally fast and exhibits a high extent of disentanglement, which allows users to interactively perform a sequence of edits on an image. Our experiments on both GAN-generated and real images qualitatively demonstrate the high quality and advantages of our method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "52166067",
                        "name": "Ehsan Pajouheshgar"
                    },
                    {
                        "authorId": "2117882335",
                        "name": "Tong Zhang"
                    },
                    {
                        "authorId": "1735035",
                        "name": "S. S\u00fcsstrunk"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We see qualitatively in our GANSpace comparison in Fig.",
                "1 Mapping edits to latent space To use these directions found in the activation space to edit the original latent code, we transform this edit tensor back to the original latent space [12].",
                "The PCA-based method of GANSpace [12] finds interpretable directions in the traditional architectures by first vectorising these tensors vec(Zm) and then learning a PCA basis U that admits the following decomposition\nvec(Zm) = UU>vec(Zm) (1) = vec(Zm)\u00d71 UU>.",
                "In Section 3.3, we introduce the proposed multilinear approach (formulating GANSpace as a special case), and finally in Section 3.4 we detail how we use these learnt directions to modify the latent code.",
                "In this vein, recent methods use auxiliary networks to search for \u2018diverse\u2019 image transformations [34, 36], decompose the weights defining the mapping between layers [30], or decompose the intermediate generator\u2019s representations directly [12].",
                "The PCA-based method of GANSpace [12] finds interpretable directions in the traditional architectures by first vectorising these tensors vec(Zm) and then learning a PCA basis U that admits the following decomposition vec(Zm) = UUvec(Zm) (1) = vec(Zm)\u00d71 UU>.",
                "Since these works, a host of methods have been proposed to explore the latent structure in these generators by imposing structure at training-time [4, 24] or more recently in the pre-trained generators themselves [1, 12, 31, 32, 34, 36].",
                "We show how the linear approach of [12] can be framed as a special case.",
                "From this it is clear that the GANSpace [12] approach as we formulate it using the mode-1 product in Eq.",
                "To further shed light on the connection between our formulation and the linear approach of GANSpace in Eq.",
                "However, of the approaches that decompose the intermediate features directly (such as [12]), a linear decomposition is applied\u2013where we argue a multilinear one can be more suitable in providing an ability to locate different categories of transformation."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "f101247583f16f0d1a9a49603a97d437805879a8",
                "externalIds": {
                    "ArXiv": "2111.11736",
                    "DBLP": "conf/bmvc/0001GPNP21",
                    "CorpusId": 244488480
                },
                "corpusId": 244488480,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/f101247583f16f0d1a9a49603a97d437805879a8",
                "title": "Tensor Component Analysis for Interpreting the Latent Space of GANs",
                "abstract": "This paper addresses the problem of finding interpretable directions in the latent space of pre-trained Generative Adversarial Networks (GANs) to facilitate controllable image synthesis. Such interpretable directions correspond to transformations that can affect both the style and geometry of the synthetic images. However, existing approaches that utilise linear techniques to find these transformations often fail to provide an intuitive way to separate these two sources of variation. To address this, we propose to a) perform a multilinear decomposition of the tensor of intermediate representations, and b) use a tensor-based regression to map directions found using this decomposition to the latent space. Our scheme allows for both linear edits corresponding to the individual modes of the tensor, and non-linear ones that model the multiplicative interactions between them. We show experimentally that we can utilise the former to better separate style- from geometry-based transformations, and the latter to generate an extended set of possible transformations in comparison to prior works. We demonstrate our approach's efficacy both quantitatively and qualitatively compared to the current state-of-the-art.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2059960629",
                        "name": "James Oldfield"
                    },
                    {
                        "authorId": "34291068",
                        "name": "Markos Georgopoulos"
                    },
                    {
                        "authorId": "1780393",
                        "name": "Yannis Panagakis"
                    },
                    {
                        "authorId": "1752913",
                        "name": "M. Nicolaou"
                    },
                    {
                        "authorId": "50058816",
                        "name": "I. Patras"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A user can transform the generated outputs by tweaking the latent code [15, 20, 32, 33].",
                "Moreover, several recent studies have utilized unsupervised methods to obtain semantic directions [15,33,43]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "2e456338573481f42d064fa0ec9a47769f8bc75a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-10520",
                    "ArXiv": "2111.10520",
                    "CorpusId": 244478439
                },
                "corpusId": 244478439,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2e456338573481f42d064fa0ec9a47769f8bc75a",
                "title": "StylePart: Image-based Shape Part Manipulation",
                "abstract": "Due to a lack of image-based\"part controllers\", shape manipulation of man-made shape images, such as resizing the backrest of a chair or replacing a cup handle is not intuitive. To tackle this problem, we present StylePart, a framework that enables direct shape manipulation of an image by leveraging generative models of both images and 3D shapes. Our key contribution is a shape-consistent latent mapping function that connects the image generative latent space and the 3D man-made shape attribute latent space. Our method\"forwardly maps\"the image content to its corresponding 3D shape attributes, where the shape part can be easily manipulated. The attribute codes of the manipulated 3D shape are then\"backwardly mapped\"to the image latent code to obtain the final manipulated image. We demonstrate our approach through various manipulation tasks, including part replacement, part resizing, and viewpoint manipulation, and evaluate its effectiveness through extensive ablation studies.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "34023939",
                        "name": "I-Chao Shen"
                    },
                    {
                        "authorId": "2069859760",
                        "name": "Liwei Su"
                    },
                    {
                        "authorId": "2129436413",
                        "name": "Yu-Ting Wu"
                    },
                    {
                        "authorId": "1733344",
                        "name": "Bing-Yu Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Likewise, while there is some evidence for property disentanglement within their latent space to independent axes of variation at the global scale [28, 36, 64, 71], most existing GANs fail to provide controllability at the level of the individual object or local region."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "cf7d86145d81fd6e980023ab1a35e480f5038b55",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-08960",
                    "ArXiv": "2111.08960",
                    "CorpusId": 244270383
                },
                "corpusId": 244270383,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/cf7d86145d81fd6e980023ab1a35e480f5038b55",
                "title": "Compositional Transformers for Scene Generation",
                "abstract": "We introduce the GANformer2 model, an iterative object-oriented transformer, explored for the task of generative modeling. The network incorporates strong and explicit structural priors, to reflect the compositional nature of visual scenes, and synthesizes images through a sequential process. It operates in two stages: a fast and lightweight planning phase, where we draft a high-level scene layout, followed by an attention-based execution phase, where the layout is being refined, evolving into a rich and detailed picture. Our model moves away from conventional black-box GAN architectures that feature a flat and monolithic latent space towards a transparent design that encourages efficiency, controllability and interpretability. We demonstrate GANformer2's strengths and qualities through a careful evaluation over a range of datasets, from multi-object CLEVR scenes to the challenging COCO images, showing it successfully achieves state-of-the-art performance in terms of visual quality, diversity and consistency. Further experiments demonstrate the model's disentanglement and provide a deeper insight into its generative process, as it proceeds step-by-step from a rough initial sketch, to a detailed layout that accounts for objects' depths and dependencies, and up to the final high-resolution depiction of vibrant and intricate real-world scenes. See https://github.com/dorarad/gansformer for model implementation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "152951058",
                        "name": "Drew A. Hudson"
                    },
                    {
                        "authorId": "1699161",
                        "name": "C. L. Zitnick"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There is a growing number of studies investigating interpretable directions to manipulate the latent space of a GAN to synthesize images (Goetschalckx et al. 2019; Shen et al. 2019; H\u00e4rk\u00f6nen et al. 2020).",
                "Inspired by the latent space manipulation technique proposed by (Ha\u0308rko\u0308nen et al. 2020), important latent directions are identified by applying PCA to the latent representations of the patients.",
                "There is a growing number of studies investigating interpretable directions to manipulate the latent space of a GAN to synthesize images (Goetschalckx et al. 2019; Shen et al. 2019; Ha\u0308rko\u0308nen et al. 2020).",
                "Inspired by the latent space manipulation technique proposed by (H\u00e4rk\u00f6nen et al. 2020), important latent directions are identified by applying PCA to the latent representations of the patients.",
                "Principal Component Analysis (PCA) can also be applied to data points sampled from a latent space to find relevant directions (Ha\u0308rko\u0308nen et al. 2020).",
                "Principal Component Analysis (PCA) can also be applied to data points sampled from a latent space to find relevant directions (H\u00e4rk\u00f6nen et al. 2020)."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4644dc7912bec7eaa1371e240778f7aa1817d1fb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-08794",
                    "ArXiv": "2111.08794",
                    "CorpusId": 244270563
                },
                "corpusId": 244270563,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4644dc7912bec7eaa1371e240778f7aa1817d1fb",
                "title": "Investigating Conversion from Mild Cognitive Impairment to Alzheimer's Disease using Latent Space Manipulation",
                "abstract": "Alzheimer's disease is the most common cause of dementia that affects millions of lives worldwide. Investigating the underlying causes and risk factors of Alzheimer's disease is essential to prevent its progression. Mild Cognitive Impairment (MCI) is considered an intermediate stage before Alzheimer's disease. Early prediction of the conversion from the MCI to Alzheimer's is crucial to take necessary precautions for decelerating the progression and developing suitable treatments. In this study, we propose a deep learning framework to discover the variables which are identifiers of the conversion from MCI to Alzheimer's disease. In particular, the latent space of a variational auto-encoder network trained with the MCI and Alzheimer's patients is manipulated to obtain the significant attributes and decipher their behavior that leads to the conversion from MCI to Alzheimer's disease. By utilizing a generative decoder and the dimensions that lead to the Alzheimer's diagnosis, we generate synthetic dementia patients from MCI patients in the dataset. Experimental results show promising quantitative and qualitative results on one of the most extensive and commonly used Alzheimer's disease neuroimaging datasets in literature.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2141029768",
                        "name": "Deniz Sezin Ayvaz"
                    },
                    {
                        "authorId": "2844539",
                        "name": "Inci M. Baytas"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0b9779a06ddc4ebb5c9d77f53d86ad850e20aef9",
                "externalIds": {
                    "ArXiv": "2111.08419",
                    "DBLP": "journals/corr/abs-2111-08419",
                    "CorpusId": 244129979
                },
                "corpusId": 244129979,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0b9779a06ddc4ebb5c9d77f53d86ad850e20aef9",
                "title": "Delta-GAN-Encoder: Encoding Semantic Changes for Explicit Image Editing, using Few Synthetic Samples",
                "abstract": "Understating and controlling generative models' latent space is a complex task. In this paper, we propose a novel method for learning to control any desired attribute in a pre-trained GAN's latent space, for the purpose of editing synthesized and real-world data samples accordingly. We perform Sim2Real learning, relying on minimal samples to achieve an unlimited amount of continuous precise edits. We present an Autoencoder-based model that learns to encode the semantics of changes between images as a basis for editing new samples later on, achieving precise desired results - example shown in Fig. 1. While previous editing methods rely on a known structure of latent spaces (e.g., linearity of some semantics in StyleGAN), our method inherently does not require any structural constraints. We demonstrate our method in the domain of facial imagery: editing different expressions, poses, and lighting attributes, achieving state-of-the-art results.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2106064823",
                        "name": "Nir Diamant"
                    },
                    {
                        "authorId": "2141269793",
                        "name": "Nitsan Sandor"
                    },
                    {
                        "authorId": "49791556",
                        "name": "A. Bronstein"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Such an explicit model has the potential of making many subsequent tasks easier and better: better control of feature sampling for decoding and synthesis [70], designing more robust generators and classifiers for noise and corruptions based on the low-dimensional structures identified, or even extending to the settings of incremental and online learning [71,72]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "22ddb09aef3d05c2ba406d7fb3b1f5386d03a94d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-06636",
                    "PubMedCentral": "9031319",
                    "ArXiv": "2111.06636",
                    "DOI": "10.3390/e24040456",
                    "CorpusId": 244102768,
                    "PubMed": "35455120"
                },
                "corpusId": 244102768,
                "publicationVenue": {
                    "id": "8270cfe1-3713-4325-a7bd-c6a87eed889e",
                    "name": "Entropy",
                    "type": "journal",
                    "issn": "1099-4300",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-155606",
                    "alternate_urls": [
                        "http://www.mdpi.com/journal/entropy/",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-155606",
                        "https://www.mdpi.com/journal/entropy"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/22ddb09aef3d05c2ba406d7fb3b1f5386d03a94d",
                "title": "CTRL: Closed-Loop Transcription to an LDR via Minimaxing Rate Reduction",
                "abstract": "This work proposes a new computational framework for learning a structured generative model for real-world datasets. In particular, we propose to learn a Closed-loop Transcriptionbetween a multi-class, multi-dimensional data distribution and a Linear discriminative representation (CTRL) in the feature space that consists of multiple independent multi-dimensional linear subspaces. In particular, we argue that the optimal encoding and decoding mappings sought can be formulated as a two-player minimax game between the encoder and decoderfor the learned representation. A natural utility function for this game is the so-called rate reduction, a simple information-theoretic measure for distances between mixtures of subspace-like Gaussians in the feature space. Our formulation draws inspiration from closed-loop error feedback from control systems and avoids expensive evaluating and minimizing of approximated distances between arbitrary distributions in either the data space or the feature space. To a large extent, this new formulation unifies the concepts and benefits of Auto-Encoding and GAN and naturally extends them to the settings of learning a both discriminative and generative representation for multi-class and multi-dimensional real-world data. Our extensive experiments on many benchmark imagery datasets demonstrate tremendous potential of this new closed-loop formulation: under fair comparison, visual quality of the learned decoder and classification performance of the encoder is competitive and arguably better than existing methods based on GAN, VAE, or a combination of both. Unlike existing generative models, the so-learned features of the multiple classes are structured instead of hidden: different classes are explicitly mapped onto corresponding independent principal subspaces in the feature space, and diverse visual attributes within each class are modeled by the independent principal components within each subspace.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2978106",
                        "name": "Xili Dai"
                    },
                    {
                        "authorId": "2143202419",
                        "name": "Shengbang Tong"
                    },
                    {
                        "authorId": "152303292",
                        "name": "Mingyang Li"
                    },
                    {
                        "authorId": "50061431",
                        "name": "Ziyang Wu"
                    },
                    {
                        "authorId": "1664623023",
                        "name": "Kwan Ho Ryan Chan"
                    },
                    {
                        "authorId": "13430908",
                        "name": "Pengyuan Zhai"
                    },
                    {
                        "authorId": "2142173500",
                        "name": "Yaodong Yu"
                    },
                    {
                        "authorId": "1922956280",
                        "name": "Michael Psenka"
                    },
                    {
                        "authorId": "2115844675",
                        "name": "Xiaojun Yuan"
                    },
                    {
                        "authorId": "2069762399",
                        "name": "Harry Shum"
                    },
                    {
                        "authorId": "1596786698",
                        "name": "Yi Ma"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Additional unsupervised approaches for finding semantic directions in StyleGAN include Principal Component Analysis (PCA) on sampled latent codes [15] and the closed-form factorization suggested by [25].",
                "11: To find the optimal interpolation strength \u03b1 for rotation transfer for InterFaceGAN [24] and GANSpace [15] we compare the images generated by shifting the latent code corresponding to an image from the one rotation towards the other and compare the result with the ground truth.",
                "Finally, we compare \u03c4GAN to InterFaceGAN [24] and GANSpace [15] for the application of semantic face editing by using rotation transfer as one example."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "7a88bd6e5e7dcfa9615feb1f89f3c10ac81969aa",
                "externalIds": {
                    "DBLP": "conf/fgr/HaasGB21",
                    "ArXiv": "2111.04554",
                    "DOI": "10.1109/FG52635.2021.9666953",
                    "CorpusId": 243847402
                },
                "corpusId": 243847402,
                "publicationVenue": {
                    "id": "b0c05768-6345-45d7-b541-235edf6ead54",
                    "name": "IEEE International Conference on Automatic Face & Gesture Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Autom Face Gesture Recognit",
                        "FG",
                        "IEEE International Conference on Automatic Face and Gesture Recognition",
                        "IEEE Int Conf Autom Face  Gesture Recognit",
                        "FGR",
                        "Form Gramm",
                        "Formal Grammar"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1029"
                },
                "url": "https://www.semanticscholar.org/paper/7a88bd6e5e7dcfa9615feb1f89f3c10ac81969aa",
                "title": "Tensor-based Subspace Factorization for StyleGAN",
                "abstract": "In this paper, we propose TGAN a tensor-based method for modeling the latent space of generative models. The objective is to identify semantic directions in latent space. To this end, we propose to fit a multilinear tensor model on a structured facial expression database, which is initially embedded into latent space. We validate our approach on StyleGAN trained on FFHQ using BU-3DFE as a structured facial expression database. We show how the parameters of the multilinear tensor model can be approximated by Alternating Least Squares. Further, we introduce a stacked style-separated tensor model, defined as an ensemble of style-specific models to integrate our approach with the extended latent space of StyleGAN. We show that taking the individual styles of the extended latent space into account leads to higher model flexibility and lower reconstruction error. Finally, we do several experiments comparing our approach to former work on both GANs and multilinear models. Concretely, we analyze the expression subspace and find that the expression trajectories meet at an apathetic face that is consistent with earlier work. We also show that by changing the pose of a person, the generated image from our approach is closer to the ground truth than results from two competing approaches.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2140280426",
                        "name": "Ren\u00e9 Haas"
                    },
                    {
                        "authorId": "23620370",
                        "name": "Stella Grasshof"
                    },
                    {
                        "authorId": "120414984",
                        "name": "Sami S. Brandt"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(i) One line of work relies on the careful dissection of the GAN\u2019s latent space, aiming to find interpretable and disentangled latent variables, which can be leveraged for image editing, in a fully unsupervised manner [47, 24, 25, 12, 13, 14, 48, 49, 26, 27, 50, 51].",
                "much attention as a promising tool for efficient image editing, as it was found that latent space manipulations often lead to interpretable and predictable changes in output [47, 24, 48, 49, 26, 27, 50].",
                "Other approaches carefully analyze and dissect GANs\u2019 latent spaces, finding disentangled latent variables suitable for editing [24, 25, 12, 13, 14, 26, 27], or control the GANs\u2019 network parameters [25, 28, 16]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "d98f22eca403263a57018a160195950ebead3d33",
                "externalIds": {
                    "DBLP": "conf/nips/LingKLKTF21",
                    "ArXiv": "2111.03186",
                    "CorpusId": 243832879
                },
                "corpusId": 243832879,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d98f22eca403263a57018a160195950ebead3d33",
                "title": "EditGAN: High-Precision Semantic Image Editing",
                "abstract": "Generative adversarial networks (GANs) have recently found applications in image editing. However, most GAN based image editing methods often require large scale datasets with semantic segmentation annotations for training, only provide high level control, or merely interpolate between different images. Here, we propose EditGAN, a novel method for high quality, high precision semantic image editing, allowing users to edit images by modifying their highly detailed part segmentation masks, e.g., drawing a new mask for the headlight of a car. EditGAN builds on a GAN framework that jointly models images and their semantic segmentations, requiring only a handful of labeled examples, making it a scalable tool for editing. Specifically, we embed an image into the GAN latent space and perform conditional latent code optimization according to the segmentation edit, which effectively also modifies the image. To amortize optimization, we find editing vectors in latent space that realize the edits. The framework allows us to learn an arbitrary number of editing vectors, which can then be directly applied on other images at interactive rates. We experimentally show that EditGAN can manipulate images with an unprecedented level of detail and freedom, while preserving full image quality.We can also easily combine multiple edits and perform plausible edits beyond EditGAN training data. We demonstrate EditGAN on a wide variety of image types and quantitatively outperform several previous editing methods on standard editing benchmark tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "18900686",
                        "name": "Huan Ling"
                    },
                    {
                        "authorId": "32113848",
                        "name": "Karsten Kreis"
                    },
                    {
                        "authorId": "2108864987",
                        "name": "Daiqing Li"
                    },
                    {
                        "authorId": "2128102197",
                        "name": "Seung Wook Kim"
                    },
                    {
                        "authorId": "143805211",
                        "name": "A. Torralba"
                    },
                    {
                        "authorId": "37895334",
                        "name": "S. Fidler"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "68de1a5a391a7993d989929098f36f22db5f5c08",
                "externalIds": {
                    "ArXiv": "2111.01619",
                    "DBLP": "journals/corr/abs-2111-01619",
                    "CorpusId": 240419972
                },
                "corpusId": 240419972,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/68de1a5a391a7993d989929098f36f22db5f5c08",
                "title": "StyleGAN of All Trades: Image Manipulation with Only Pretrained StyleGAN",
                "abstract": "Recently, StyleGAN has enabled various image manipulation and editing tasks thanks to the high-quality generation and the disentangled latent space. However, additional architectures or task-specific training paradigms are usually required for different tasks. In this work, we take a deeper look at the spatial properties of StyleGAN. We show that with a pretrained StyleGAN along with some operations, without any additional architecture, we can perform comparably to the state-of-the-art methods on various tasks, including image blending, panorama generation, generation from a single image, controllable and local multimodal image to image translation, and attributes transfer. The proposed method is simple, effective, efficient, and applicable to any existing pretrained StyleGAN model.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50615993",
                        "name": "Min Jin Chong"
                    },
                    {
                        "authorId": "49923155",
                        "name": "Hsin-Ying Lee"
                    },
                    {
                        "authorId": "121068894",
                        "name": "David A. Forsyth"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In another line of work, the 3D scene information is extracted from 2D GANs such as StyleGAN2 to manipulate 2D images in 3D (Shen and Zhou 2020; Ha\u0308rko\u0308nen et al. 2020) and recover explicit 3D shapes from images (Pan et al. 2020; Zhang et al. 2020).",
                "In another line of work, the 3D scene information is extracted from 2D GANs such as StyleGAN2 to manipulate 2D images in 3D (Shen and Zhou 2020; H\u00e4rk\u00f6nen et al. 2020) and recover explicit 3D shapes from images (Pan et al."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "e3e133722951392bb3c26b397e723f892ea98a6e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-01048",
                    "ArXiv": "2111.01048",
                    "DOI": "10.1609/aaai.v36i2.20091",
                    "CorpusId": 240353691
                },
                "corpusId": 240353691,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/e3e133722951392bb3c26b397e723f892ea98a6e",
                "title": "MOST-GAN: 3D Morphable StyleGAN for Disentangled Face Image Manipulation",
                "abstract": "Recent advances in generative adversarial networks (GANs) have led to remarkable achievements in face image synthesis. While methods that use style-based GANs can generate strikingly photorealistic face images, it is often difficult to control the characteristics of the generated faces in a meaningful and disentangled way. Prior approaches aim to achieve such semantic control and disentanglement within the latent space of a previously trained GAN. In contrast, we propose a framework that a priori models physical attributes of the face such as 3D shape, albedo, pose, and lighting explicitly, thus providing disentanglement by design. Our method, MOST-GAN, integrates the expressive power and photorealism of style-based GANs with the physical disentanglement and flexibility of nonlinear 3D morphable models, which we couple with a state-of-the-art 2D hair manipulation network. MOST-GAN achieves photorealistic manipulation of portrait images with fully disentangled 3D control over their physical attributes, enabling extreme manipulation of lighting, facial expression, and pose variations up to full profile view.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51291503",
                        "name": "Safa C. Medin"
                    },
                    {
                        "authorId": "2053695764",
                        "name": "B. Egger"
                    },
                    {
                        "authorId": "2691929",
                        "name": "A. Cherian"
                    },
                    {
                        "authorId": "2115737963",
                        "name": "Ye Wang"
                    },
                    {
                        "authorId": "1763295",
                        "name": "J. Tenenbaum"
                    },
                    {
                        "authorId": "2111119747",
                        "name": "Xiaoming Liu"
                    },
                    {
                        "authorId": "34749896",
                        "name": "Tim K. Marks"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a6f0f67965a5a75e173e739a95a1075db1b1cef8",
                "externalIds": {
                    "DOI": "10.1016/j.precisioneng.2021.10.020",
                    "CorpusId": 243490723
                },
                "corpusId": 243490723,
                "publicationVenue": {
                    "id": "3f234649-8fcc-4df4-95c8-aba89ab48db1",
                    "name": "Precision engineering",
                    "type": "journal",
                    "alternate_names": [
                        "Precision Engineering-journal of The International Societies for Precision Engineering and Nanotechnology",
                        "Precis Eng Int Soc Precis Eng Nanotechnol",
                        "Precis eng"
                    ],
                    "issn": "0141-6359",
                    "url": "https://www.journals.elsevier.com/precision-engineering",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/01416359"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a6f0f67965a5a75e173e739a95a1075db1b1cef8",
                "title": "Generation and categorisation of surface texture data using a modified progressively growing adversarial network",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1657971903",
                        "name": "J. Eastwood"
                    },
                    {
                        "authorId": "66868048",
                        "name": "L. Newton"
                    },
                    {
                        "authorId": "3036666",
                        "name": "R. Leach"
                    },
                    {
                        "authorId": "1862545122",
                        "name": "S. Piano"
                    }
                ]
            }
        },
        {
            "contexts": [
                "position, scale) [4, 3, 15], memorability [16] to facial attributes [5, 17, 2, 9, 15, 6, 8].",
                "Some works attempt to find semantic directions with self-supervised learning [9], unsupervised approaches in latent space such as PCA [2], or by leveraging the internal representation of GANs to derive closed-form solutions [8, 15].",
                "In particular, some methods find linear directions that can be interpreted as variations of some semantic attributes across the latent space [2, 3, 4, 5, 6, 7, 8, 9].",
                "For example, GANSpace [2] applies PCA in theW space and the authors are able to assign semantic interpretations to the resulting directions (orthogonal by definition)."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "04424bb6e3051bc12566d8dcf7f95ce6178f5f7b",
                "externalIds": {
                    "DBLP": "journals/prl/DoubinskyACB22",
                    "ArXiv": "2111.00909",
                    "DOI": "10.1016/j.patrec.2022.08.012",
                    "CorpusId": 240353764
                },
                "corpusId": 240353764,
                "publicationVenue": {
                    "id": "f35e3e87-9df4-497b-aa0d-bb8584197290",
                    "name": "Pattern Recognition Letters",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit Lett"
                    ],
                    "issn": "0167-8655",
                    "url": "https://www.journals.elsevier.com/pattern-recognition-letters/",
                    "alternate_urls": [
                        "http://www.journals.elsevier.com/pattern-recognition-letters/",
                        "http://www.sciencedirect.com/science/journal/01678655"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/04424bb6e3051bc12566d8dcf7f95ce6178f5f7b",
                "title": "Multi-Attribute Balanced Sampling for Disentangled GAN Controls",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2136376223",
                        "name": "Perla Doubinsky"
                    },
                    {
                        "authorId": "3468294",
                        "name": "N. Audebert"
                    },
                    {
                        "authorId": "1719698",
                        "name": "M. Crucianu"
                    },
                    {
                        "authorId": "2138418",
                        "name": "H. Borgne"
                    }
                ]
            }
        },
        {
            "contexts": [
                "show that principal component analysis (PCA) can be used to create interpretable GAN controls to alter, for example, the gender, age or pose of a generated face [15].",
                "[15], where interpretable directions in latent space are found using PCA.",
                "create controls by sampling Z space and performing PCA on an intermediate representation of the GAN (W space in StyleGAN or feature space in BigGAN) [15].",
                "For example, DCGAN uses convolutions in both the generator and discriminator [34], PCGAN progressively grows its architecture, and therefore the resolution of images, during training [21] and BigGAN introduced the \u201ctruncation trick\u201d, where images are generated by sampling from a truncated normal distribution [6].",
                "H\u00e4rk\u00f6nen et al. create controls by sampling Z space and performing PCA on an intermediate representation of the GAN (W space in StyleGAN or feature space in BigGAN) [15]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1c764df1f9a99bc7532972617920e533f31ebefc",
                "externalIds": {
                    "DBLP": "conf/cikm/KropotovMG21",
                    "DOI": "10.1145/3459637.3482103",
                    "CorpusId": 240230750
                },
                "corpusId": 240230750,
                "publicationVenue": {
                    "id": "7431ff67-91dc-41fa-b322-1b1ca657025f",
                    "name": "International Conference on Information and Knowledge Management",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Information and Knowledge Management",
                        "Conf Inf Knowl Manag",
                        "Int Conf Inf Knowl Manag",
                        "CIKM"
                    ],
                    "url": "http://www.cikm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/1c764df1f9a99bc7532972617920e533f31ebefc",
                "title": "Exploratory Search of GANs with Contextual Bandits",
                "abstract": "Interactive image retrieval involves users searching a collection of images to satisfy their subjective information needs. However, even large image collections are finite and therefore may not be able to satisfy users. An alternate approach would be to explore a generative adversarial network (GAN) and model users' search intents directly in terms of the latent space used by the GAN to generate images. In this article, we present a simulation study exploring the performance of Gaussian Process bandits in the context of interactive GAN exploration. We used recent advances in interpretable GAN controls to investigate the scalability of different approaches in terms of image space dimensionality. While we present several experiments with promising results, none of the approaches tested scale sufficiently well to explore the entire GAN image space.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2135921224",
                        "name": "Ivan Kropotov"
                    },
                    {
                        "authorId": "3222337",
                        "name": "A. Medlar"
                    },
                    {
                        "authorId": "2090521",
                        "name": "D. Glowacka"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Additivity in the space of StyleGAN was demonstrated in [7, 8, 9], for linearly interpolating between different images along semantic directions as well as for the manipulation of semantic attributes ([10] for example).",
                "Image Manipulation Our work is also related to recent image manipulation works based on a pretrained generator or CLIP [18, 19, 9, 10]."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "61432c11c359f6abb38a62a674fa4fdbc8be94d3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-12427",
                    "ArXiv": "2110.12427",
                    "DOI": "10.1007/978-3-031-19778-9_40",
                    "CorpusId": 239768292
                },
                "corpusId": 239768292,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/61432c11c359f6abb38a62a674fa4fdbc8be94d3",
                "title": "Image-Based CLIP-Guided Essence Transfer",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2038268012",
                        "name": "Hila Chefer"
                    },
                    {
                        "authorId": "19310335",
                        "name": "Sagie Benaim"
                    },
                    {
                        "authorId": "2134839079",
                        "name": "Roni Paiss"
                    },
                    {
                        "authorId": "145128145",
                        "name": "Lior Wolf"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Many recent works have proposed methods to interpret the semantics encoded in that space and its extensions and apply them to image editing (Jahanian et al., 2019; Shen et al., 2020a; H\u00e4rk\u00f6nen et al., 2020; Tewari et al., 2020; Abdal et al., 2020; Wu et al., 2020; Patashnik et al., 2021).",
                "Many recent works have proposed methods to interpret the semantics encoded in that space and its extensions and apply them to image editing (Jahanian et al., 2019; Shen et al., 2020a; Ha\u0308rko\u0308nen et al., 2020; Tewari et al., 2020; Abdal et al., 2020; Wu et al., 2020; Patashnik et al., 2021)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "83bb34556b953a172bf5687131b24c17e015f476",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-11323",
                    "ArXiv": "2110.11323",
                    "CorpusId": 239049745
                },
                "corpusId": 239049745,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/83bb34556b953a172bf5687131b24c17e015f476",
                "title": "StyleAlign: Analysis and Applications of Aligned StyleGAN Models",
                "abstract": "In this paper, we perform an in-depth study of the properties and applications of aligned generative models. We refer to two models as aligned if they share the same architecture, and one of them (the child) is obtained from the other (the parent) via fine-tuning to another domain, a common practice in transfer learning. Several works already utilize some basic properties of aligned StyleGAN models to perform image-to-image translation. Here, we perform the first detailed exploration of model alignment, also focusing on StyleGAN. First, we empirically analyze aligned models and provide answers to important questions regarding their nature. In particular, we find that the child model's latent spaces are semantically aligned with those of the parent, inheriting incredibly rich semantics, even for distant data domains such as human faces and churches. Second, equipped with this better understanding, we leverage aligned models to solve a diverse set of tasks. In addition to image translation, we demonstrate fully automatic cross-domain image morphing. We further show that zero-shot vision tasks may be performed in the child domain, while relying exclusively on supervision in the parent domain. We demonstrate qualitatively and quantitatively that our approach yields state-of-the-art results, while requiring only simple fine-tuning and inversion.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "34815981",
                        "name": "Zongze Wu"
                    },
                    {
                        "authorId": "1702992975",
                        "name": "Yotam Nitzan"
                    },
                    {
                        "authorId": "2177801",
                        "name": "Eli Shechtman"
                    },
                    {
                        "authorId": "70018371",
                        "name": "D. Lischinski"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent works have attempted to overcome these issues by first training an unconditional generator, and then converting it to a conditional model with a small cost [26, 20, 1, 42].",
                "Some works explore linear manipulations of latent code [26, 20, 46], while others consider more complicated nonlinear manipulations [37, 16, 1, 42]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "a4727b807ae052c9dcddcf424a5d233bcc3c5a9e",
                "externalIds": {
                    "ArXiv": "2110.10873",
                    "DBLP": "conf/nips/NieVA21",
                    "CorpusId": 239050174
                },
                "corpusId": 239050174,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a4727b807ae052c9dcddcf424a5d233bcc3c5a9e",
                "title": "Controllable and Compositional Generation with Latent-Space Energy-Based Models",
                "abstract": "Controllable generation is one of the key requirements for successful adoption of deep generative models in real-world applications, but it still remains as a great challenge. In particular, the compositional ability to generate novel concept combinations is out of reach for most current models. In this work, we use energy-based models (EBMs) to handle compositional generation over a set of attributes. To make them scalable to high-resolution image generation, we introduce an EBM in the latent space of a pre-trained generative model such as StyleGAN. We propose a novel EBM formulation representing the joint distribution of data and attributes together, and we show how sampling from it is formulated as solving an ordinary differential equation (ODE). Given a pre-trained generator, all we need for controllable generation is to train an attribute classifier. Sampling with ODEs is done efficiently in the latent space and is robust to hyperparameters. Thus, our method is simple, fast to train, and efficient to sample. Experimental results show that our method outperforms the state-of-the-art in both conditional sampling and sequential editing. In compositional generation, our method excels at zero-shot generation of unseen attribute combinations. Also, by composing energy functions with logical operators, this work is the first to achieve such compositionality in generating photo-realistic images of resolution 1024x1024. Code is available at https://github.com/NVlabs/LACE.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2066304514",
                        "name": "Weili Nie"
                    },
                    {
                        "authorId": "3214848",
                        "name": "Arash Vahdat"
                    },
                    {
                        "authorId": "2047844",
                        "name": "Anima Anandkumar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Previous methods alleviate this problem by finding the latent semantic vectors existing in pre-trained 2D GAN models [22, 26, 52, 53]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6971b691bf5514b309822b9deda8f89fd13944ad",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-09788",
                    "ArXiv": "2110.09788",
                    "CorpusId": 239024427
                },
                "corpusId": 239024427,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6971b691bf5514b309822b9deda8f89fd13944ad",
                "title": "CIPS-3D: A 3D-Aware Generator of GANs Based on Conditionally-Independent Pixel Synthesis",
                "abstract": "The style-based GAN (StyleGAN) architecture achieved state-of-the-art results for generating high-quality images, but it lacks explicit and precise control over camera poses. The recently proposed NeRF-based GANs made great progress towards 3D-aware generators, but they are unable to generate high-quality images yet. This paper presents CIPS-3D, a style-based, 3D-aware generator that is composed of a shallow NeRF network and a deep implicit neural representation (INR) network. The generator synthesizes each pixel value independently without any spatial convolution or upsampling operation. In addition, we diagnose the problem of mirror symmetry that implies a suboptimal solution and solve it by introducing an auxiliary discriminator. Trained on raw, single-view images, CIPS-3D sets new records for 3D-aware image synthesis with an impressive FID of 6.97 for images at the $256\\times256$ resolution on FFHQ. We also demonstrate several interesting directions for CIPS-3D such as transfer learning and 3D-aware face stylization. The synthesis results are best viewed as videos, so we recommend the readers to check our github project at https://github.com/PeterouZh/CIPS-3D",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2113325955",
                        "name": "Peng Zhou"
                    },
                    {
                        "authorId": "3041937",
                        "name": "Lingxi Xie"
                    },
                    {
                        "authorId": "5796401",
                        "name": "Bingbing Ni"
                    },
                    {
                        "authorId": "2056267867",
                        "name": "Qi Tian"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Many approaches (Jahanian, Chai, and Isola 2019; Yang, Shen, and Zhou 2021; Shen et al. 2020; Balakrishnan et al. 2020; H\u00e4rk\u00f6nen et al. 2020) exploit the inherent disentanglement properties of GAN latent space to control the generated images.",
                "Many approaches (Jahanian, Chai, and Isola 2019; Yang, Shen, and Zhou 2021; Shen et al. 2020; Balakrishnan et al. 2020; Ha\u0308rko\u0308nen et al. 2020) exploit the inherent disentanglement properties of GAN latent space to control the generated images."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "33320c77a3c68b49a8aba775698147e0da0da124",
                "externalIds": {
                    "ArXiv": "2110.10278",
                    "DBLP": "journals/corr/abs-2110-10278",
                    "CorpusId": 239050102
                },
                "corpusId": 239050102,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/33320c77a3c68b49a8aba775698147e0da0da124",
                "title": "Fine-Grained Control of Artistic Styles in Image Generation",
                "abstract": "Recent advances in generative models and adversarial training have enabled artificially generating artworks in various artistic styles. It is highly desirable to gain more control over the generated style in practice. However, artistic styles are unlike object categories -- there are a continuous spectrum of styles distinguished by subtle differences. Few works have been explored to capture the continuous spectrum of styles and apply it to a style generation task. In this paper, we propose to achieve this by embedding original artwork examples into a continuous style space. The style vectors are fed to the generator and discriminator to achieve fine-grained control. Our method can be used with common generative adversarial networks (such as StyleGAN). Experiments show that our method not only precisely controls the fine-grained artistic style but also improves image quality over vanilla StyleGAN as measured by FID.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2067854663",
                        "name": "Xin Miao"
                    },
                    {
                        "authorId": "2109230200",
                        "name": "Huayan Wang"
                    },
                    {
                        "authorId": "2119276694",
                        "name": "Jun Fu"
                    },
                    {
                        "authorId": "2108357649",
                        "name": "Jiayi Liu"
                    },
                    {
                        "authorId": "2165680667",
                        "name": "Shen Wang"
                    },
                    {
                        "authorId": null,
                        "name": "Zhenyu Liao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some methods (H\u00e4rk\u00f6nen et al., 2020; Tewari et al., 2020a; Shen et al., 2020; Abdal et al., 2020; Tewari et al., 2020b; Leimk\u00fchler & Drettakis, 2021; Shoshan et al., 2021) leverage disentangled properties in the latent space to enable explicit controls, most of which focus on faces.",
                "Some methods (Ha\u0308rko\u0308nen et al., 2020; Tewari et al., 2020a; Shen et al., 2020; Abdal et al., 2020; Tewari et al., 2020b; Leimku\u0308hler & Drettakis, 2021; Shoshan et al., 2021) leverage disentangled properties in the latent space to enable explicit controls, most of which focus on faces."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3fdda879abf2462b09139fb1fd1c2c147c9a0ef0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-08985",
                    "ArXiv": "2110.08985",
                    "CorpusId": 239016913
                },
                "corpusId": 239016913,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3fdda879abf2462b09139fb1fd1c2c147c9a0ef0",
                "title": "StyleNeRF: A Style-based 3D-Aware Generator for High-resolution Image Synthesis",
                "abstract": "We propose StyleNeRF, a 3D-aware generative model for photo-realistic high-resolution image synthesis with high multi-view consistency, which can be trained on unstructured 2D images. Existing approaches either cannot synthesize high-resolution images with fine details or yield noticeable 3D-inconsistent artifacts. In addition, many of them lack control over style attributes and explicit 3D camera poses. StyleNeRF integrates the neural radiance field (NeRF) into a style-based generator to tackle the aforementioned challenges, i.e., improving rendering efficiency and 3D consistency for high-resolution image generation. We perform volume rendering only to produce a low-resolution feature map and progressively apply upsampling in 2D to address the first issue. To mitigate the inconsistencies caused by 2D upsampling, we propose multiple designs, including a better upsampler and a new regularization loss. With these designs, StyleNeRF can synthesize high-resolution images at interactive rates while preserving 3D consistency at high quality. StyleNeRF also enables control of camera poses and different levels of styles, which can generalize to unseen views. It also supports challenging tasks, including zoom-in and-out, style mixing, inversion, and semantic editing.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3016273",
                        "name": "Jiatao Gu"
                    },
                    {
                        "authorId": "46458089",
                        "name": "Lingjie Liu"
                    },
                    {
                        "authorId": "2155300848",
                        "name": "Peng Wang"
                    },
                    {
                        "authorId": "1680185",
                        "name": "C. Theobalt"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similarly, generative models can be used to create novel views of images [37, 39, 57] by manipulating them in latent space."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "9c8d46b59e871e18d8d2e1ec1aa9b96d2f3d7342",
                "externalIds": {
                    "DBLP": "conf/nips/GowalRWSCM21",
                    "ArXiv": "2110.09468",
                    "CorpusId": 239016201
                },
                "corpusId": 239016201,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9c8d46b59e871e18d8d2e1ec1aa9b96d2f3d7342",
                "title": "Improving Robustness using Generated Data",
                "abstract": "Recent work argues that robust training requires substantially larger datasets than those required for standard classification. On CIFAR-10 and CIFAR-100, this translates into a sizable robust-accuracy gap between models trained solely on data from the original training set and those trained with additional data extracted from the\"80 Million Tiny Images\"dataset (TI-80M). In this paper, we explore how generative models trained solely on the original training set can be leveraged to artificially increase the size of the original training set and improve adversarial robustness to $\\ell_p$ norm-bounded perturbations. We identify the sufficient conditions under which incorporating additional generated data can improve robustness, and demonstrate that it is possible to significantly reduce the robust-accuracy gap to models trained with additional real data. Surprisingly, we even show that even the addition of non-realistic random data (generated by Gaussian sampling) can improve robustness. We evaluate our approach on CIFAR-10, CIFAR-100, SVHN and TinyImageNet against $\\ell_\\infty$ and $\\ell_2$ norm-bounded perturbations of size $\\epsilon = 8/255$ and $\\epsilon = 128/255$, respectively. We show large absolute improvements in robust accuracy compared to previous state-of-the-art methods. Against $\\ell_\\infty$ norm-bounded perturbations of size $\\epsilon = 8/255$, our models achieve 66.10% and 33.49% robust accuracy on CIFAR-10 and CIFAR-100, respectively (improving upon the state-of-the-art by +8.96% and +3.29%). Against $\\ell_2$ norm-bounded perturbations of size $\\epsilon = 128/255$, our model achieves 78.31% on CIFAR-10 (+3.81%). These results beat most prior works that use external data.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2071666",
                        "name": "Sven Gowal"
                    },
                    {
                        "authorId": "8478422",
                        "name": "Sylvestre-Alvise Rebuffi"
                    },
                    {
                        "authorId": "8792285",
                        "name": "Olivia Wiles"
                    },
                    {
                        "authorId": "3205302",
                        "name": "Florian Stimberg"
                    },
                    {
                        "authorId": "2792016",
                        "name": "D. A. Calian"
                    },
                    {
                        "authorId": "144467964",
                        "name": "Timothy Mann"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Previous works [10, 25, 26] can identify a semantically meaningful direction vector in the W or Z space that can manipulates target attribute of r .",
                "Some previous works [9, 10, 14, 32] Poster Session 1 MM \u201921, October 20\u201324, 2021, Virtual Event, China",
                "The unsupervised approaches [6, 10, 21, 31] aim to discover as many directions as possible using unsupervised techniques.",
                "GANSpace [10] adopts PCA in latent space and identifies important latent directions.",
                "Note that, our proposed method can perform various local attribute editing tasks, which is much more than previous methods [10, 25, 26, 33]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "f7073896fd1a7ab92f64aab6550d0c079a7d6c46",
                "externalIds": {
                    "ArXiv": "2111.13010",
                    "DBLP": "journals/corr/abs-2111-13010",
                    "DOI": "10.1145/3474085.3475274",
                    "CorpusId": 239011757
                },
                "corpusId": 239011757,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f7073896fd1a7ab92f64aab6550d0c079a7d6c46",
                "title": "Attribute-specific Control Units in StyleGAN for Fine-grained Image Manipulation",
                "abstract": "Image manipulation with StyleGAN has been an increasing concern in recent years. Recent works have achieved tremendous success in analyzing several semantic latent spaces to edit the attributes of the generated images. However, due to the limited semantic and spatial manipulation precision in these latent spaces, the existing endeavors are defeated in fine-grained StyleGAN image manipulation, i.e., local attribute translation. To address this issue, we discover attribute-specific control units, which consist of multiple channels of feature maps and modulation styles. Specifically, we collaboratively manipulate the modulation style channels and feature maps in control units rather than individual ones to obtain the semantic and spatial disentangled controls. Furthermore, we propose a simple yet effective method to detect the attribute-specific control units. We move the modulation style along a specific sparse direction vector and replace the filter-wise styles used to compute the feature maps to manipulate these control units. We evaluate our proposed method in various face attribute manipulation tasks. Extensive qualitative and quantitative results demonstrate that our proposed method performs favorably against the state-of-the-art methods. The manipulation results of real images further show the effectiveness of our method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2151036605",
                        "name": "Rui Wang"
                    },
                    {
                        "authorId": "2118445731",
                        "name": "Jian Chen"
                    },
                    {
                        "authorId": "2116565951",
                        "name": "Gang Yu"
                    },
                    {
                        "authorId": "2110966343",
                        "name": "Li Sun"
                    },
                    {
                        "authorId": "2004605044",
                        "name": "Changqian Yu"
                    },
                    {
                        "authorId": "40115662",
                        "name": "Changxin Gao"
                    },
                    {
                        "authorId": "1707161",
                        "name": "N. Sang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANSpace [16] and SeFa [43] use unsupervised learning to achieve face attribute disentanglement, while StyleFlow [3] uses supervised learning to disentangle face attributes.",
                "In this section, we compare our methods against five concurrent works: Sefa [43], GANSpace [16], Image2StyleGAN [2], InterfaceGAN [42] and StyleFlow [3].",
                "The other methods such as SeFa [43] and GANSpace [16] are trained in unsupervised ways.",
                "Some approaches [2, 3, 16, 43, 44] have tried to extract semantic information from trained GANs [23, 24] and have shown promising performance on image editing.",
                "On the other hand, unsupervised methods for image editing based on a pretrained GAN [16, 43] have also been proposed recently."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4c4b655670a55eb2db03d19be9dd9b857ace97ef",
                "externalIds": {
                    "DBLP": "conf/mm/LiangHS21",
                    "DOI": "10.1145/3474085.3475454",
                    "CorpusId": 239012015
                },
                "corpusId": 239012015,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4c4b655670a55eb2db03d19be9dd9b857ace97ef",
                "title": "SSFlow: Style-guided Neural Spline Flows for Face Image Manipulation",
                "abstract": "Significant progress has been made in high-resolution and photo-realistic image generation by Generative Adversarial Networks (GANs). However, the generation process is still lack of control, which is crucial for semantic face editing. Furthermore, it remains challenging to edit target attributes and preserve the identity at the same time. In this paper, we propose SSFlow to achieve identity-preserved semantic face manipulation in StyleGAN latent space based on conditional Neural Spline Flows. To further improve the performance of Neural Spline Flows on such task, we also propose Constractive Squash component and Blockwise 1 x 1 Convolution layer. Moreover, unlike other conditional flow-based approaches that require facial attribute labels during inference, our method can achieve label-free manipulation in a more flexible way. As a result, our methods are able to perform well-disentangled edits along various attributes, and generalize well for both real and artistic face image manipulation. Qualitative and quantitative evaluations show the advantages of our method for semantic face manipulation over state-of-the-art approaches.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2111208513",
                        "name": "Hanbang Liang"
                    },
                    {
                        "authorId": "3468964",
                        "name": "Xianxu Hou"
                    },
                    {
                        "authorId": "121640365",
                        "name": "Linlin Shen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[15] applied principal component analysis over the intermediate features of random samples to determine the principal latent directions.",
                "In [15, 37], important latent directions are determined by performing principal component analysis and matrix factorization on the features and weights of an intermediate layer, respectively."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "99fcd6fc3254515829f821e51fa13811f5582c7c",
                "externalIds": {
                    "DBLP": "conf/mm/LiLWZ0XW21",
                    "DOI": "10.1145/3474085.3475293",
                    "CorpusId": 239011815
                },
                "corpusId": 239011815,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/99fcd6fc3254515829f821e51fa13811f5582c7c",
                "title": "Discovering Density-Preserving Latent Space Walks in GANs for Semantic Image Transformations",
                "abstract": "Generative adversarial network (GAN)-based models possess superior capability of high-fidelity image synthesis. There are a wide range of semantically meaningful directions in the latent representation space of well-trained GANs, and the corresponding latent space walks are meaningful for semantic controllability in the synthesized images. To explore the underlying organization of a latent space, we propose an unsupervised Density-Preserving Latent Semantics Exploration model (DP-LaSE). The important latent directions are determined by maximizing the variations in intermediate features, while the correlation between the directions is minimized. Considering that latent codes are sampled from a prior distribution, we adopt a density-preserving regularization approach to ensure latent space walks are maintained in iso-density regions, since moving to a higher/lower density region tends to cause unexpected transformations. To further refine semantics-specific transformations, we perform subspace learning over intermediate feature channels, such that the transformations are limited to the most relevant subspaces. Extensive experiments on a variety of benchmark datasets demonstrate that DP-LaSE is able to discover interpretable latent space walks, and specific properties of synthesized images can thus be precisely controlled.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "120045265",
                        "name": "Guanyue Li"
                    },
                    {
                        "authorId": "2153630228",
                        "name": "Yi Liu"
                    },
                    {
                        "authorId": "2123297243",
                        "name": "Xiwen Wei"
                    },
                    {
                        "authorId": "2145954976",
                        "name": "Yang Zhang"
                    },
                    {
                        "authorId": "98566535",
                        "name": "Si Wu"
                    },
                    {
                        "authorId": "2146647838",
                        "name": "Yong Xu"
                    },
                    {
                        "authorId": "145892864",
                        "name": "H. Wong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Z latent space[7, 11, 21, 23] and W+ latent space[3, 10, 29] are the most used optimization latent space in GAN inversion methods at present."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "ed149d284ac29f2b38ab151a9d1773fb11bd7841",
                "externalIds": {
                    "DBLP": "conf/mm/ZhangBG21",
                    "DOI": "10.1145/3474085.3475633",
                    "CorpusId": 239011739
                },
                "corpusId": 239011739,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ed149d284ac29f2b38ab151a9d1773fb11bd7841",
                "title": "SalS-GAN: Spatially-Adaptive Latent Space in StyleGAN for Real Image Embedding",
                "abstract": "Many GAN inversion methods have emerged to embed a given real image into the latent space of GAN for real image editing. These methods usually use a latent space composed of a series of one-dimensional vectors as an optimization space to reconstruct real images such as W+ latent space. However, the reconstructed image of these methods is usually difficult to maintain the rich detailed information in the real image. How to better preserve details in the real image is still a challenge. To solve this problem, we propose a spatially-adaptive latent space, called SA latent space, and adopt it as the optimization latent space in GAN inversion task. In particular, we use the affine transformation parameters of each convolutional layer in the generator to form the SA latent space and change affine transformation parameters from a one-dimensional vector to a spatially-adaptive three-dimensional tensor. With the more expressive latent space, we can better reconstruct the details of the real image. Extensive experiments suggest that the image reconstruction quality can be significantly improved while maintaining the semantic disentanglement ability of latent code. The code is available at https://github.com/zhang-lingyun/SalS-GAN.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2145404094",
                        "name": "Lingyun Zhang"
                    },
                    {
                        "authorId": "2151011226",
                        "name": "Xiuxiu Bai"
                    },
                    {
                        "authorId": "2154881143",
                        "name": "Yao Gao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "fb544e51f829c6b9ebac29e93947853c8d11ae12",
                "externalIds": {
                    "DBLP": "conf/smc/KatoSMF21",
                    "DOI": "10.1109/SMC52423.2021.9659205",
                    "CorpusId": 245803401
                },
                "corpusId": 245803401,
                "publicationVenue": {
                    "id": "e84bb5a1-8f79-42cc-8eb1-3a52f7c73d63",
                    "name": "IEEE International Conference on Systems, Man and Cybernetics",
                    "type": "conference",
                    "alternate_names": [
                        "Smoky Mountains Computational Sciences and Engineering Conference",
                        "Smoky Mt Comput Sci Eng Conf",
                        "IEEE Int Conf Syst Man Cybern",
                        "SMC",
                        "Syst Man Cybern",
                        "Systems, Man and Cybernetics"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fb544e51f829c6b9ebac29e93947853c8d11ae12",
                "title": "Face Image Generation with Features of Specific Group from Small Dataset *",
                "abstract": "The research on face image generation has gained widespread attention in the field of image generation. However, large datasets are required to train a generative model to produce images of high quality and resolution. It is especially difficult to collect multiple face images of a specific group. It may also be difficult to establish an efficient model by training with a small dataset.This paper presents a novel method for the generation of face images with the features of a specific group. The face images of a dataset are embedded into a latent space as sets of latent variables by Image2StyleGAN and are expressed as a distribution of the sets of latent variables in the latent space of a pre-trained StyleGAN2. Principal Component Analysis (PCA) is used to extract the features of the distribution. The generation of images for different groups does not require re-training of the model, as a pre-trained model is used instead. Furthermore, the proposed method can generate high-quality images with the features of the group from a dataset with only about 100 face images. However, the quality and the variety of the generated images can vary depending on a Cumulative Contribution Rate (CCR) of the PCA. Therefore, this study also proposes a metric called the Fr\u00e9chet Inception Distance in Principal Component Space (FID-PCS), which can evaluate the generated images even with a small dataset. The FID-PCS can be used to determine the CCR that generates images with a good balance between the quality and the variety. The face images of three groups were collected as datasets to evaluate the validity of the proposed method, which include male idols, female idols, and male mixed martial artists. It was observed that images with the features of the group are generated by the face distributions extracted by the PCA, and the images with high quality and wide variety are generated by determining the appropriate CCR by the FID-PCS.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2146958278",
                        "name": "Yuichi Kato"
                    },
                    {
                        "authorId": "2149083325",
                        "name": "Takuya Shuto"
                    },
                    {
                        "authorId": "1725869",
                        "name": "M. Mikawa"
                    },
                    {
                        "authorId": "2221677",
                        "name": "Makoto Fujisawa"
                    }
                ]
            }
        },
        {
            "contexts": [
                "On the other hand, GANSpace (Ha\u0308rko\u0308nen et al., 2020) extracts unsupervised linear directions for editing using PCA in the W space.",
                "In the StyleGAN domain, recent works (H\u00e4rk\u00f6nen et al., 2020; Shen et al., 2020; Tewari et al., 2020a; Abdal et al., 2021b) extract meaningful linear and non-linear paths in the latent space.",
                "In the StyleGAN domain, recent works (Ha\u0308rko\u0308nen et al., 2020; Shen et al., 2020; Tewari et al., 2020a; Abdal et al., 2021b) extract meaningful linear and non-linear paths in the latent space.",
                "Some notable works in this domain (Bau et al., 2018; 2019; Ha\u0308rko\u0308nen et al., 2020; Shen et al., 2020; Tewari et al., 2020a) have led to many GAN-based image editing applications.",
                "On the other hand, GANSpace (H\u00e4rk\u00f6nen et al., 2020) extracts unsupervised linear directions for editing using PCA in the W space.",
                "Some notable works in this domain (Bau et al., 2018; 2019; H\u00e4rk\u00f6nen et al., 2020; Shen et al., 2020; Tewari et al., 2020a) have led to many GAN-based image editing applications."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "c17cfe3add196c1d66a7981f7246363bfe4f31e4",
                "externalIds": {
                    "ArXiv": "2110.08398",
                    "DBLP": "journals/corr/abs-2110-08398",
                    "CorpusId": 239016426
                },
                "corpusId": 239016426,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c17cfe3add196c1d66a7981f7246363bfe4f31e4",
                "title": "Mind the Gap: Domain Gap Control for Single Shot Domain Adaptation for Generative Adversarial Networks",
                "abstract": "We present a new method for one shot domain adaptation. The input to our method is trained GAN that can produce images in domain A and a single reference image I_B from domain B. The proposed algorithm can translate any output of the trained GAN from domain A to domain B. There are two main advantages of our method compared to the current state of the art: First, our solution achieves higher visual quality, e.g. by noticeably reducing overfitting. Second, our solution allows for more degrees of freedom to control the domain gap, i.e. what aspects of image I_B are used to define the domain B. Technically, we realize the new method by building on a pre-trained StyleGAN generator as GAN and a pre-trained CLIP model for representing the domain gap. We propose several new regularizers for controlling the domain gap to optimize the weights of the pre-trained StyleGAN generator to output images in domain B instead of domain A. The regularizers prevent the optimization from taking on too many attributes of the single reference image. Our results show significant visual improvements over the state of the art as well as multiple applications that highlight improved control.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "100516590",
                        "name": "Peihao Zhu"
                    },
                    {
                        "authorId": "94395014",
                        "name": "Rameen Abdal"
                    },
                    {
                        "authorId": "145685385",
                        "name": "John C. Femiani"
                    },
                    {
                        "authorId": "1798011",
                        "name": "Peter Wonka"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In computer vision, discovering latent space manipulations for image style transfer has recently become a topic of increased interest, in both supervised (Jahanian et al., 2020; Zhuang et al., 2021) and unsupervised ways (H\u00e4rk\u00f6nen et al., 2020; Voynov and Babenko, 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6890a19a2cabcc76ece88e4face62944c085ec88",
                "externalIds": {
                    "ACL": "2022.aacl-main.36",
                    "DBLP": "journals/corr/abs-2110-07002",
                    "ArXiv": "2110.07002",
                    "CorpusId": 238856812
                },
                "corpusId": 238856812,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6890a19a2cabcc76ece88e4face62944c085ec88",
                "title": "Bag-of-Vectors Autoencoders for Unsupervised Conditional Text Generation",
                "abstract": "Text autoencoders are often used for unsupervised conditional text generation by applying mappings in the latent space to change attributes to the desired values. Recently, Mai et al. (2020) proposed Emb2Emb, a method to learn these mappings in the embedding space of an autoencoder. However, their method is restricted to autoencoders with a single-vector embedding, which limits how much information can be retained. We address this issue by extending their method to Bag-of-Vectors Autoencoders (BoV-AEs), which encode the text into a variable-size bag of vectors that grows with the size of the text, as in attention-based models. This allows to encode and reconstruct much longer texts than standard autoencoders. Analogous to conventional autoencoders, we propose regularization techniques that facilitate learning meaningful operations in the latent space. Finally, we adapt Emb2Emb for a training scheme that learns to map an input bag to an output bag, including a novel loss function and neural architecture. Our empirical evaluations on unsupervised sentiment transfer show that our method performs substantially better than a standard autoencoder.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "38777433",
                        "name": "Florian Mai"
                    },
                    {
                        "authorId": "46712043",
                        "name": "J. Henderson"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Controllable generative models have also been developed for images (H\u00e4rk\u00f6nen et al., 2020; Esser et al., 2019; Singh et al., 2019; Lample et al., 2017; Karras et al., 2020; Brock et al., 2019; Collins et al., 2020; Shen et al., 2020; Esser et al., 2020; Goetschalckx et al., 2019; Pavllo et al., 2020; Zhang et al., 2018; Chan et al., 2021; Kwon & Ye, 2021; Kazemi et al., 2019), which control the object class, pose, lighting, etc.",
                "Controllable generative models have also been developed for images (Ha\u0308rko\u0308nen et al., 2020; Esser et al., 2019; Singh et al., 2019; Lample et al., 2017; Karras et al., 2020; Brock et al., 2019; Collins et al., 2020; Shen et al., 2020; Esser et al., 2020; Goetschalckx et al., 2019; Pavllo et al.,\u2026"
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "e1f7746a6170d7cc3c8ce44e640e29317a862c9b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-02891",
                    "ArXiv": "2110.02891",
                    "CorpusId": 238408357
                },
                "corpusId": 238408357,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/e1f7746a6170d7cc3c8ce44e640e29317a862c9b",
                "title": "Style Equalization: Unsupervised Learning of Controllable Generative Sequence Models",
                "abstract": "Controllable generative sequence models with the capability to extract and replicate the style of speci\ufb01c examples enable many applications, including narrating audiobooks in different voices, auto-completing and auto-correcting written handwriting, and generating missing training samples for downstream recognition tasks. However, under an unsupervised-style setting, typical training algo-rithms for controllable sequence generative models suffer from the training-inference mismatch, where the same sample is used as content and style input during training but unpaired samples are given during inference. In this paper, we tackle the training-inference mismatch encountered during unsupervised learning of controllable generative sequence models. The proposed method is simple yet effective, where we use a style transformation module to transfer target style information into an unrelated style input. This method enables training using unpaired content and style samples and thereby mitigate the training-inference mismatch. We apply style equalization to text-to-speech and text-to-handwriting synthesis on three datasets. We conduct thorough evaluation, including both quantitative and qualitative user studies. Our results show that by mitigating the training-inference mismatch with the proposed style equalization, we achieve style replication scores comparable to real data in our user studies.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2148969927",
                        "name": "Jen-Hao Rick Chang"
                    },
                    {
                        "authorId": "1490900960",
                        "name": "A. Shrivastava"
                    },
                    {
                        "authorId": "1723948",
                        "name": "H. Koppula"
                    },
                    {
                        "authorId": "2108056752",
                        "name": "Xiaoshuai Zhang"
                    },
                    {
                        "authorId": "2577513",
                        "name": "Oncel Tuzel"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "7c32a4b50fa0081a85cdeceab1066086de883f48",
                "externalIds": {
                    "DBLP": "conf/iccv/SchwettmannHBKA21",
                    "ArXiv": "2110.04292",
                    "DOI": "10.1109/ICCV48922.2021.00673",
                    "CorpusId": 238531303
                },
                "corpusId": 238531303,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/7c32a4b50fa0081a85cdeceab1066086de883f48",
                "title": "Toward a Visual Concept Vocabulary for GAN Latent Space",
                "abstract": "A large body of recent work has identified transformations in the latent spaces of generative adversarial networks (GANs) that consistently and interpretably transform generated images. But existing techniques for identifying these transformations rely on either a fixed vocabulary of prespecified visual concepts, or on unsupervised disentanglement techniques whose alignment with human judgments about perceptual salience is unknown. This paper introduces a new method for building open-ended vocabularies of primitive visual concepts represented in a GAN\u2019s latent space. Our approach is built from three components: (1) automatic identification of perceptually salient directions based on their layer selectivity; (2) human annotation of these directions with free-form, compositional natural language descriptions; and (3) decomposition of these annotations into a visual concept vocabulary, consisting of distilled directions labeled with single words. Experiments show that concepts learned with our approach are reliable and composable\u2014generalizing across classes, contexts, and observers, and enabling fine-grained manipulation of image style and content.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "5478802",
                        "name": "Sarah Schwettmann"
                    },
                    {
                        "authorId": "51005902",
                        "name": "Evan Hernandez"
                    },
                    {
                        "authorId": "144159726",
                        "name": "David Bau"
                    },
                    {
                        "authorId": "2151234875",
                        "name": "Samuel J. Klein"
                    },
                    {
                        "authorId": "2112400",
                        "name": "Jacob Andreas"
                    },
                    {
                        "authorId": "143805211",
                        "name": "A. Torralba"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another approach, [14], demonstrates that interpretable directions often correspond to the principal components of the activations from hidden layers of generator networks.",
                "Moreover, since the GAN latent spaces are known to possess semantically meaningful vector space arithmetic, a plethora of recent works explore these spaces to discover the interpretable directions [27, 29, 11, 16, 26, 31, 14, 25].",
                "Finally, a bunch of recent methods [31, 14, 25] identify interpretable directions without any form of (self-)supervision."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "825f86037b5ccb8bc4599e593599cb2e0bd8c27a",
                "externalIds": {
                    "DBLP": "conf/iccv/KhrulkovMOB21",
                    "ArXiv": "2111.14825",
                    "DOI": "10.1109/ICCV48922.2021.01416",
                    "CorpusId": 244102299
                },
                "corpusId": 244102299,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/825f86037b5ccb8bc4599e593599cb2e0bd8c27a",
                "title": "Latent Transformations via NeuralODEs for GAN-based Image Editing",
                "abstract": "Recent advances in high-fidelity semantic image editing heavily rely on the presumably disentangled latent spaces of the state-of-the-art generative models, such as Style-GAN. Specifically, recent works show that it is possible to achieve decent controllability of attributes in face images via linear shifts along with latent directions. Several recent methods address the discovery of such directions, implicitly assuming that the state-of-the-art GANs learn the latent spaces with inherently linearly separable attribute distributions and semantic vector arithmetic properties.In our work, we show that nonlinear latent code manipulations realized as flows of a trainable Neural ODE are beneficial for many practical non-face image domains with more complex non-textured factors of variation. In particular, we investigate a large number of datasets with known attributes and demonstrate that certain attribute manipulations are challenging to obtain with linear shifts only.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "10662951",
                        "name": "Valentin Khrulkov"
                    },
                    {
                        "authorId": "66660622",
                        "name": "L. Mirvakhabova"
                    },
                    {
                        "authorId": "1738205",
                        "name": "I. Oseledets"
                    },
                    {
                        "authorId": "143743802",
                        "name": "Artem Babenko"
                    }
                ]
            }
        },
        {
            "contexts": [
                "ing the structure of GAN\u2019s latent space [28, 30, 18, 33, 7, 37, 1, 34, 35, 9, 14, 32, 26, 11].",
                ", [11]) or relies on laborious human labeling [34].",
                "GANSpace [11] is trained in an unsupervised manner in order to discover meaningful directions by using PCA on deep features of the generator.",
                "Finally, let us note that in contrast to the global linear directions discovered by [34, 11], in our case the directions along each warping are different for different latent codes.",
                "More specifically, for a given method that discovers a set of paths, that is, linear in the cases of [34, 11] or non-linear in our case, in the latent space of a pretrained GAN, we generate an image sequence for each path, starting from a random latent code and \u201cwalking\u201d towards the positive and the negative ways of the path for a certain amount of steps.",
                "Figure 8: Automatically discovered non-linear (ours \u2013 first row) and linear (Voynov and Babenko [34] \u2013 second row, GANSpace [11] \u2013 third row) interpretable paths in ProgGAN\u2019s [17] latent space.",
                "We compare with the corresponding linear directions obtained by [34, 11] and we note that our method both leads to greater variation in the respective generative factors (e.",
                "GANSpace [11] performs PCA on deep features at the early layers of the generator and finds directions in the latent space that best map to those deep PCA vectors, arriving at a set of nonorthogonal directions in the latent space.",
                ", SN-GAN [25], BigGAN [3], ProgGAN [17], and StyleGAN2 [19]) and compare our non-linear paths to linear ones [34, 11], both qualitatively and quantitatively.",
                "Finally, our method is closely related to those of [34, 11], since we are also learning a set of interpretable paths in an unsupervised and model-agnostic manner."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "2e41e73883154738a76f9b2da761fab73929f145",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-13357",
                    "ArXiv": "2109.13357",
                    "DOI": "10.1109/iccv48922.2021.00633",
                    "CorpusId": 238198279
                },
                "corpusId": 238198279,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/2e41e73883154738a76f9b2da761fab73929f145",
                "title": "WarpedGANSpace: Finding non-linear RBF paths in GAN latent space",
                "abstract": "This work addresses the problem of discovering, in an unsupervised manner, interpretable paths in the latent space of pretrained GANs, so as to provide an intuitive and easy way of controlling the underlying generative factors. In doing so, it addresses some of the limitations of the state-of-the-art works, namely, a) that they discover directions that are independent of the latent code, i.e., paths that are linear, and b) that their evaluation relies either on visual inspection or on laborious human labeling. More specifically, we propose to learn non-linear warpings on the latent space, each one parametrized by a set of RBF-based latent space warping functions, and where each warping gives rise to a family of non-linear paths via the gradient of the function. Building on the work of [34], that discovers linear paths, we optimize the trainable parameters of the set of RBFs, so as that images that are generated by codes along different paths, are easily distinguishable by a discriminator network. This leads to easily distinguishable image transformations, such as pose and facial expressions in facial images. We show that linear paths can be derived as a special case of our method, and show experimentally that non-linear paths in the latent space lead to steeper, more disentangled and interpretable changes in the image space than in state-of-the art methods, both qualitatively and quantitatively. We make the code and the pretrained models publicly available at: https://github.com/chi0tzp/WarpedGANSpace.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1694090",
                        "name": "Christos Tzelepis"
                    },
                    {
                        "authorId": "2610880",
                        "name": "Georgios Tzimiropoulos"
                    },
                    {
                        "authorId": "50058816",
                        "name": "I. Patras"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[21] identify important latent directions based on Principal Component Analysis (PCA) to control properties such as lighting, facial attributes, and landscape attributes."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "5cebb7310d74f8d7cc37d992b35121a6e410650c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-12492",
                    "ArXiv": "2109.12492",
                    "DOI": "10.1109/TMM.2022.3159115",
                    "CorpusId": 237940383
                },
                "corpusId": 237940383,
                "publicationVenue": {
                    "id": "10e76a35-58d6-443c-9683-fc16f2dd0a92",
                    "name": "IEEE transactions on multimedia",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Transactions on Multimedia",
                        "IEEE Trans Multimedia",
                        "IEEE trans multimedia"
                    ],
                    "issn": "1520-9210",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=6046"
                },
                "url": "https://www.semanticscholar.org/paper/5cebb7310d74f8d7cc37d992b35121a6e410650c",
                "title": "ISF-GAN: An Implicit Style Function for High-Resolution Image-to-Image Translation",
                "abstract": "Recently, there has been an increasing interest in image editing methods that employ pre-trained unconditional image generators (e.g., StyleGAN). However, applying these methods to translate images to multiple visual domains remains challenging. Existing works do not often preserve the domain-invariant part of the image (e.g., the identity in human face translations), or they do not usually handle multiple domains or allow for multi-modal translations. This work proposes an implicit style function (ISF) to straightforwardly achieve multi-modal and multi-domain image-to-image translation from pre-trained unconditional generators. The ISF manipulates the semantics of a latent code to ensure that the image generated from the manipulated code lies in the desired visual domain. Our human faces and animal image manipulations show significantly improved results over the baselines. Our model enables cost-effective multi-modal unsupervised image-to-image translations at high resolution using pre-trained unconditional GANs. The code and data are available at: https://github.com/yhlleo/stylegan-mmuit.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1646872838",
                        "name": "Yahui Liu"
                    },
                    {
                        "authorId": "2109270851",
                        "name": "Yajing Chen"
                    },
                    {
                        "authorId": "2780029",
                        "name": "Linchao Bao"
                    },
                    {
                        "authorId": "1703601",
                        "name": "N. Sebe"
                    },
                    {
                        "authorId": "49305855",
                        "name": "B. Lepri"
                    },
                    {
                        "authorId": "7405787",
                        "name": "Marco De Nadai"
                    }
                ]
            }
        },
        {
            "contexts": [
                "2020), GANSpace (H\u00e4rk\u00f6nen et al. 2020) and SeFa (Shen and Zhou 2020).",
                "Specifically, GANSpace (H\u00e4rk\u00f6nen et al. 2020) identifies latent directions based on applied PCA either in latent space or feature space, and interpretable controls can be defined by layer-wise perturbation along the principal directions.",
                "Methods of this stream (Shen et al. 2020; H\u00e4rk\u00f6nen et al. 2020; Shen and Zhou 2020; Hou et al. 2020; Tewari et al. 2020; Abdal et al. 2020; Wang, Yu, and Fritz 2021; Xia et al. 2021; Roich et al. 2021; Alaluf, Patashnik, and CohenOr 2021b; Ren et al. 2021; Lang et al. 2021; Wu, Lischinski, and Shechtman 2021; Patashnik et al. 2021) attempt to achieve controlled image synthesis by exploring the semantics in the latent space of well-trained GANs."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "764a7148ed00a5e202e20d7bebcefbfbd31fb0b7",
                "externalIds": {
                    "ArXiv": "2109.10737",
                    "DBLP": "conf/wacv/LiCLZHHY23",
                    "DOI": "10.1109/WACV56688.2023.00027",
                    "CorpusId": 237592637
                },
                "corpusId": 237592637,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/764a7148ed00a5e202e20d7bebcefbfbd31fb0b7",
                "title": "DyStyle: Dynamic Neural Network for Multi-Attribute-Conditioned Style Editings",
                "abstract": "The semantic controllability of StyleGAN is enhanced by unremitting research. Although the existing weak supervision methods work well in manipulating the style codes along one attribute, the accuracy of manipulating multiple attributes is neglected. Multi-attribute representations are prone to entanglement in the StyleGAN latent space, while sequential editing leads to error accumulation. To address these limitations, we design a Dynamic Style Manipulation Network (DyStyle) whose structure and parameters vary by input samples, to perform nonlinear and adaptive manipulation of latent codes for flexible and precise attribute control. In order to efficient and stable optimization of the DyStyle network, we propose a Dynamic Multi-Attribute Contrastive Learning (DmaCL) method: including dynamic multi-attribute contrastor and dynamic multi-attribute contrastive loss, which simultaneously disentangle a variety of attributes from the generative image and latent space of model. As a result, our approach demonstrates fine-grained disentangled edits along multiple numeric and binary attributes. Qualitative and quantitative comparisons with existing style manipulation methods verify the superiority of our method in terms of the multi-attribute control accuracy and identity preservation without compromising photorealism.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2145726957",
                        "name": "Bingchuan Li"
                    },
                    {
                        "authorId": "1993661033",
                        "name": "Shaofei Cai"
                    },
                    {
                        "authorId": null,
                        "name": "Wei Liu"
                    },
                    {
                        "authorId": "2151331126",
                        "name": "Peng Zhang"
                    },
                    {
                        "authorId": "2128319758",
                        "name": "Miao Hua"
                    },
                    {
                        "authorId": "2152880412",
                        "name": "Qian He"
                    },
                    {
                        "authorId": "39737792",
                        "name": "Zili Yi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "1 below, where the x is the scale parameter customed by user [3]:",
                "Specially, by utilizing the principal component analysis (PCA) on the intermediate latent space W of StyleGAN2 model [3], this paper achieved high-level properties control of generated building facade images.",
                "In addition, the features of latent space they have brings the possibility to achieve further model explanation and semantic edition of generated images [3, 11].",
                "Introduce GANSpace and image embedding method to visualize the correlation between the generated building fa\u00e7ade images and their corresponding latent vectors, which achieved unsupervised classification and high-level properties control of both generated and novel images.",
                "In this paper, by training the state-of-the-art GAN based image generation model, StyleGAN2 [2], with high-resolution building fa\u00e7ade image dataset, and exploring its latent space by applying PCA and GANSpace analysis, we could overcome above challenges in different extend [3].",
                "Above hypothesis has been proved by the research GANSpace [3]."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "0add20e847c2a0201fb660ad13ed2b30f4dee67c",
                "externalIds": {
                    "MAG": "3200070205",
                    "DOI": "10.1007/978-981-16-5983-6_6",
                    "CorpusId": 240514542
                },
                "corpusId": 240514542,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0add20e847c2a0201fb660ad13ed2b30f4dee67c",
                "title": "Exploring in the Latent Space of Design: A Method of Plausible Building Facades Images Generation, Properties Control and Model Explanation Base on StyleGAN2",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2144278970",
                        "name": "Sheng Meng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The manipulations can correspond to linear [H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020; Tewari et al. 2020a] or non-linear [Abdal et al. 2021; B R et al. 2021] paths in latent space, while disentangled controls can be jointly trained with the generator [Deng et al. 2020].",
                "Combined with recent methods to project a real photo to such a latent code [Abdal et al. 2019; Tewari et al. 2020b] they also allow semantic image editing, e.g., controlling facial expression or relighting [Abdal et al. 2021; Deng et al. 2020; H\u00e4rk\u00f6nen et al. 2020].",
                "The manipulations can correspond to linear [H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020; Tewari et al. 2020a] or non-linear [Abdal et al.",
                ", controlling facial expression or relighting [Abdal et al. 2021; Deng et al. 2020; H\u00e4rk\u00f6nen et al. 2020]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "8bf5ede037959c3bec9857288753c945e6f55143",
                "externalIds": {
                    "ArXiv": "2109.09378",
                    "DBLP": "journals/corr/abs-2109-09378",
                    "DOI": "10.1145/3478513.3480538",
                    "CorpusId": 237510393
                },
                "corpusId": 237510393,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8bf5ede037959c3bec9857288753c945e6f55143",
                "title": "FreeStyleGAN",
                "abstract": "Current Generative Adversarial Networks (GANs) produce photorealistic renderings of portrait images. Embedding real images into the latent space of such models enables high-level image editing. While recent methods provide considerable semantic control over the (re-)generated images, they can only generate a limited set of viewpoints and cannot explicitly control the camera. Such 3D camera control is required for 3D virtual and mixed reality applications. In our solution, we use a few images of a face to perform 3D reconstruction, and we introduce the notion of the GAN camera manifold, the key element allowing us to precisely define the range of images that the GAN can reproduce in a stable manner. We train a small face-specific neural implicit representation network to map a captured face to this manifold and complement it with a warping scheme to obtain free-viewpoint novel-view synthesis. We show how our approach - due to its precise camera control - enables the integration of a pre-trained StyleGAN into standard 3D rendering pipelines, allowing e.g., stereo rendering or consistent insertion of faces in synthetic 3D environments. Our solution proposes the first truly free-viewpoint rendering of realistic faces at interactive rates, using only a small number of casual photos as input, while simultaneously allowing semantic editing capabilities, such as facial expression or lighting changes.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2422386",
                        "name": "Thomas Leimk\u00fchler"
                    },
                    {
                        "authorId": "1721779",
                        "name": "G. Drettakis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANSpace explores latent directions using PCA and manually identifies the attributes corresponding to each direction.",
                "The 2nd column is the result of GANSpace, where the attributes are not well disentangled.",
                "(2) We realize controllable and disentangled facial attribute manipulations without any manual intervention, contrary to previous approaches [5, 6].",
                "Other approaches have attempted to imitate or directly carry out Principal Component Analysis (PCA) in the latent space of generative networks [6, 19].",
                "We compare our manipulation results with two recent state-of-the-art methods: GANSpace [6] and InterFaceGAN [5].",
                "Current approaches assume that a linear interpolation of latent codes lead to smooth variations of a visual attribute [5, 6].",
                "In each subfigure, from left to right are the original image, the manipulation result of GANSpace [6], that of InterFaceGAN [5] and ours."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "result",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "fa479e18bfb678181b19eb3e024106a7b800525b",
                "externalIds": {
                    "DBLP": "conf/icip/YaoNGH21",
                    "MAG": "3196204616",
                    "DOI": "10.1109/ICIP42928.2021.9506060",
                    "CorpusId": 238692296
                },
                "corpusId": 238692296,
                "publicationVenue": {
                    "id": "b6369c33-5d70-463c-8e82-95a54efa3cc8",
                    "name": "International Conference on Information Photonics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Image Process",
                        "ICIP",
                        "Int Conf Inf Photonics",
                        "International Conference on Image Processing"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fa479e18bfb678181b19eb3e024106a7b800525b",
                "title": "Learning Non-Linear Disentangled Editing For Stylegan",
                "abstract": "Recent work has demonstrated the great potential of image editing in the latent space of powerful deep generative models such as StyleGAN. However, the success of such methods relies on the assumption that a linear hyperplane may separate the latent space into two subspaces for a binary attribute. In this work, we show that this hypothesis is a significant limitation and propose to learn a non-linear, regularized and identity-preserving latent space transformation that leads to more accurate and disentangled manipulations of facial attributes.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2115586564",
                        "name": "Xu Yao"
                    },
                    {
                        "authorId": "1902919",
                        "name": "A. Newson"
                    },
                    {
                        "authorId": "1796594",
                        "name": "Y. Gousseau"
                    },
                    {
                        "authorId": "1806880",
                        "name": "P. Hellier"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Competitive approaches to StyleGAN appear in (Gao et al. 2021; Tewari et al. 2020; H\u00e4rk\u00f6nen et al. 2020; Nitzan et al. 2020).",
                "Competitive approaches to StyleGAN appear in (Gao et al. 2021; Tewari et al. 2020; Ha\u0308rko\u0308nen et al. 2020; Nitzan et al. 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "c6da67b676d4355b7ce6ac26e5d08bd49a397d5f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-09011",
                    "ArXiv": "2109.09011",
                    "DOI": "10.1609/aaai.v36i8.20843",
                    "CorpusId": 237572005
                },
                "corpusId": 237572005,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/c6da67b676d4355b7ce6ac26e5d08bd49a397d5f",
                "title": "PluGeN: Multi-Label Conditional Generation From Pre-Trained Models",
                "abstract": "Modern generative models achieve excellent quality in a variety of tasks including image or text generation and chemical molecule modeling. However, existing methods often lack the essential ability to generate examples with requested properties, such as the age of the person in the photo or the weight of the generated molecule. Incorporating such additional conditioning factors would require rebuilding the entire architecture and optimizing the parameters from scratch. Moreover, it is difficult to disentangle selected attributes so that to perform edits of only one attribute while leaving the others unchanged. To overcome these limitations we propose PluGeN (Plugin Generative Network), a simple yet effective generative technique that can be used as a plugin to pre-trained generative models. The idea behind our approach is to transform the entangled latent representation using a flow-based module into a multi-dimensional space where the values of each attribute are modeled as an independent one-dimensional distribution. In consequence, PluGeN can generate new samples with desired attributes as well as manipulate labeled attributes of existing examples. Due to the disentangling of the latent representation, we are even able to generate samples with rare or unseen combinations of attributes in the dataset, such as a young person with gray hair, men with make-up, or women with beards. We combined PluGeN with GAN and VAE models and applied it to conditional generation and manipulation of images and chemical molecule modeling. Experiments demonstrate that PluGeN preserves the quality of backbone models while adding the ability to control the values of labeled attributes. Implementation is available at https://github.com/gmum/plugen.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "73772332",
                        "name": "Maciej Wo\u0142czyk"
                    },
                    {
                        "authorId": "2243334689",
                        "name": "Magdalena Proszewska"
                    },
                    {
                        "authorId": "51455558",
                        "name": "Lukasz Maziarka"
                    },
                    {
                        "authorId": "3027512",
                        "name": "Maciej Zi\u0229ba"
                    },
                    {
                        "authorId": "2127473965",
                        "name": "Patryk Wielopolski"
                    },
                    {
                        "authorId": "7008299",
                        "name": "Rafa\u0142 Kurczab"
                    },
                    {
                        "authorId": "49688210",
                        "name": "Marek \u015amieja"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANspace [13] performed PCA on early feature layers.",
                "For attribute\nediting, we adopt InterfaceGAN [30] for face images and GANSpace [13] for car images.",
                "editing, we adopt InterfaceGAN [30] for face images and GANSpace [13] for car images."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "90cf38a7431d920843c25f4bc8ea8feca99e83ff",
                "externalIds": {
                    "DBLP": "conf/cvpr/WangZFWC22",
                    "ArXiv": "2109.06590",
                    "DOI": "10.1109/CVPR52688.2022.01109",
                    "CorpusId": 237503169
                },
                "corpusId": 237503169,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/90cf38a7431d920843c25f4bc8ea8feca99e83ff",
                "title": "High-Fidelity GAN Inversion for Image Attribute Editing",
                "abstract": "We present a novel highfidelity generative adversarial network (GAN) inversion framework that enables attribute editing with image-specific details well-preserved (e.g., background, appearance, and illumination). We first analyze the challenges of highfidelity GAN inversion from the perspective of lossy data compression. With a low bitrate latent code, previous works have difficulties in preserving highfidelity details in reconstructed and edited images. Increasing the size of a latent code can improve the accuracy of GAN inversion but at the cost of inferior editability. To improve image fidelity without compromising editability, we propose a distortion consultation approach that employs a distortion map as a reference for highfidelity reconstruction. In the distortion consultation inversion (DCI), the distortion map is first projected to a high-rate latent map, which then complements the basic low-rate latent code with more details via consultation fusion. To achieve high-fidelity editing, we propose an adaptive distortion alignment (ADA) module with a self-supervised training scheme, which bridges the gap between the edited and inversion images. Extensive experiments in the face and car domains show a clear improvement in both inversion and editing quality. The project page is https://tengfei-wang.github.io/HFGI/.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2116582092",
                        "name": "Tengfei Wang"
                    },
                    {
                        "authorId": "2144289260",
                        "name": "Yong Zhang"
                    },
                    {
                        "authorId": "2140245719",
                        "name": "Yanbo Fan"
                    },
                    {
                        "authorId": "2109763673",
                        "name": "Jue Wang"
                    },
                    {
                        "authorId": "143832240",
                        "name": "Qifeng Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Using the pre-trained model, several works discover directions in the latent space that correspond to spatial or semantic changes [H\u00e4rk\u00f6nen et al. 2020; Jahanian et al. 2020; Peebles et al. 2020; Shen and Zhou 2021; Shoshan et al. 2021]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6c84f4ebe78d8c161ff5362e5516a3469fb29560",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-06166",
                    "ArXiv": "2109.06166",
                    "CorpusId": 237491034
                },
                "corpusId": 237491034,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6c84f4ebe78d8c161ff5362e5516a3469fb29560",
                "title": "Pose with Style: Detail-Preserving Pose-Guided Image Synthesis with Conditional StyleGAN",
                "abstract": "We present an algorithm for re-rendering a person from a single image under arbitrary poses. Existing methods often have difficulties in hallucinating occluded contents photo-realistically while preserving the identity and fine details in the source image. We first learn to inpaint the correspondence field between the body surface texture and the source image with a human body symmetry prior. The inpainted correspondence field allows us to transfer/warp local features extracted from the source to the target view even under large pose changes. Directly mapping the warped local features to an RGB image using a simple CNN decoder often leads to visible artifacts. Thus, we extend the StyleGAN generator so that it takes pose as input (for controlling poses) and introduces a spatially varying modulation for the latent space using the warped local features (for controlling appearances). We show that our method compares favorably against the state-of-the-art algorithms in both quantitative evaluation and visual comparison.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "23982870",
                        "name": "Badour Albahar"
                    },
                    {
                        "authorId": "2054975",
                        "name": "Jingwan Lu"
                    },
                    {
                        "authorId": "2109724839",
                        "name": "Jimei Yang"
                    },
                    {
                        "authorId": "2496409",
                        "name": "Zhixin Shu"
                    },
                    {
                        "authorId": "2177801",
                        "name": "Eli Shechtman"
                    },
                    {
                        "authorId": "2238908897",
                        "name": "Jia-Bin Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Supervised methods find directions to edit the attributes of interest using attribute labels [40, 41, 59], while unsupervised methods exploit semantics learned by the pretrained GAN to discover the most important and distinguishable directions [45, 11, 42].",
                "While some approaches [40, 41, 45, 42, 11] could perform continuous editing to some extent by shifting the latent code of a pretrained GAN [17, 19, 16, 3], they typically make two assumptions: 1) the attribute change is achieved by traversing along a straight line in the latent space; 2) different identities share the same latent directions."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "842ad35b67d410b40432dbb6d5349103f3f33e53",
                "externalIds": {
                    "DBLP": "conf/iccv/0003HPL021",
                    "ArXiv": "2109.04425",
                    "DOI": "10.1109/ICCV48922.2021.01354",
                    "CorpusId": 237453495
                },
                "corpusId": 237453495,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/842ad35b67d410b40432dbb6d5349103f3f33e53",
                "title": "Talk-to-Edit: Fine-Grained Facial Editing via Dialog",
                "abstract": "Facial editing is an important task in vision and graphics with numerous applications. However, existing works are incapable to deliver a continuous and fine-grained editing mode (e.g., editing a slightly smiling face to a big laughing one) with natural interactions with users. In this work, we propose Talk-to-Edit, an interactive facial editing framework that performs fine-grained attribute manipulation through dialog between the user and the system. Our key insight is to model a continual \"semantic field\" in the GAN latent space. 1) Unlike previous works that regard the editing as traversing straight lines in the latent space, here the fine-grained editing is formulated as finding a curving trajectory that respects fine-grained attribute landscape on the semantic field. 2) The curvature at each step is location-specific and determined by the input image as well as the users\u2019 language requests. 3) To engage the users in a meaningful dialog, our system generates language feedback by considering both the user request and the current state of the semantic field.We also contribute CelebA-Dialog, a visual-language facial editing dataset to facilitate large-scale study. Specifically, each image has manually annotated fine-grained attribute annotations as well as template-based textual descriptions in natural language. Extensive quantitative and qualitative experiments demonstrate the superiority of our framework in terms of 1) the smoothness of fine-grained editing, 2) the identity/attribute preservation, and 3) the visual photorealism and dialog fluency. Notably, user study validates that our overall system is consistently favored by around 80% of the participants. Our project page is https://www.mmlab-ntu.com/project/talkedit/.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2127773416",
                        "name": "Yuming Jiang"
                    },
                    {
                        "authorId": "3150911",
                        "name": "Ziqi Huang"
                    },
                    {
                        "authorId": "14214933",
                        "name": "Xingang Pan"
                    },
                    {
                        "authorId": "1717179",
                        "name": "Chen Change Loy"
                    },
                    {
                        "authorId": "2117940996",
                        "name": "Ziwei Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "But different from [8], the PCA components are computed from the weight parameters rather that the sampled vectors.",
                "[8] proposes to identify important latent directions based on the Principal Components Analysis (PCA) of the latent space vectors."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "701c74702a42dbd4013fbe71e2b74a841708203e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-03492",
                    "ArXiv": "2109.03492",
                    "CorpusId": 237440258
                },
                "corpusId": 237440258,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/701c74702a42dbd4013fbe71e2b74a841708203e",
                "title": "FaceCook: Face Generation Based on Linear Scaling Factors",
                "abstract": "With the excellent disentanglement properties of state-of-the-art generative models, image editing has been the dominant approach to control the attributes of synthesised face images. However, these edited results often suffer from artifacts or incorrect feature rendering, especially when there is a large discrepancy between the image to be edited and the desired feature set. Therefore, we propose a new approach to mapping the latent vectors of the generative model to the scaling factors through solving a set of multivariate linear equations. The coefficients of the equations are the eigenvectors of the weight parameters of the pre-trained model, which form the basis of a hyper coordinate system. The qualitative and quantitative results both show that the proposed method outperforms the baseline in terms of image diversity. In addition, the method is much more time-efficient because you can obtain synthesised images with desirable features directly from the latent vectors, rather than the former process of editing randomly generated images requiring many processing steps.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "46958896",
                        "name": "Tianren Wang"
                    },
                    {
                        "authorId": "48406628",
                        "name": "Can Peng"
                    },
                    {
                        "authorId": "2110104958",
                        "name": "Teng Zhang"
                    },
                    {
                        "authorId": "144367279",
                        "name": "B. Lovell"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[14] applied Principal Component Analysis to the GAN input space."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1f0f6cdf4ae6799f3b96da0707fab28278175c23",
                "externalIds": {
                    "DBLP": "conf/audio/SchlagowskiMA21",
                    "DOI": "10.1145/3478384.3478411",
                    "CorpusId": 238992461
                },
                "corpusId": 238992461,
                "publicationVenue": {
                    "id": "27e5feaf-5e1e-4d4c-a13a-2058a0664051",
                    "name": "Audio Mostly Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Audio Most Conf",
                        "AM"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=144"
                },
                "url": "https://www.semanticscholar.org/paper/1f0f6cdf4ae6799f3b96da0707fab28278175c23",
                "title": "Taming the Chaos: Exploring Graphical Input Vector Manipulation User Interfaces for GANs in a Musical Context",
                "abstract": "Generative Adversarial Networks (GANs) are a widely used tool for generating highly realistic artificial data. As the output of these networks can show high diversity and novelty, GANs have the potential to be used as creative tools. However, using GANs in this context poses major challenges due to their unpredictability and lack of controllability, making it difficult for creative people to realize their artistic vision. To address this problem, we present two graphical user interfaces that visually order the (otherwise chaotic) latent input space of a GAN that was trained to generate drum samples. Further, these GUIs provide convergent search functions that allow users to fine-tune generated sounds. By doing so, we provide the ability to create sounds more purposefully to sound-affine users such as musicians or sound engineers. Additionally, we present the results of a user study that we conducted in order to explore our approach in accuracy-oriented and creative tasks. Our results indicate that usability and pragmatic qualities play a more important role for users than aesthetic-oriented aspects. Although not improving the accuracy within reproductive tasks, we observed that convergent search functions, if available, were used significantly more often than divergent/randomized search functions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35652187",
                        "name": "Ruben Schlagowski"
                    },
                    {
                        "authorId": "1456101466",
                        "name": "Silvan Mertes"
                    },
                    {
                        "authorId": "1742930",
                        "name": "E. Andr\u00e9"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Finally, interpretable directions can be discovered on w via supervised methods [21], [28], which also can be discovered in unsupervised methods [29], [30], i."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "fd60e8582353b2e3406ee04f6d875fab9627c67b",
                "externalIds": {
                    "ArXiv": "2108.10201",
                    "CorpusId": 250451258
                },
                "corpusId": 250451258,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fd60e8582353b2e3406ee04f6d875fab9627c67b",
                "title": "Diverse Similarity Encoder for Deep GAN Inversion",
                "abstract": "Current deep generative adversarial networks (GANs) can synthesize high-quality (HQ) images, so learning representation with GANs is favorable. GAN inversion is one of emerging approaches that study how to invert images into latent space. Existing GAN encoders can invert images on StyleGAN, but cannot adapt to other deep GANs. We propose a novel approach to address this issue. By evaluating diverse similarity in latent vectors and images, we design an adaptive encoder, named diverse similarity encoder (DSE), that can be expanded to a variety of state-of-the-art GANs. DSE makes GANs reconstruct higher fidelity images from HQ images, no matter whether they are synthesized or real images. DSE has unified convolutional blocks and adapts well to mainstream deep GANs, e.g., PGGAN, StyleGAN, and BigGAN.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2116580703",
                        "name": "Cheng Yu"
                    },
                    {
                        "authorId": "46315174",
                        "name": "Wenmin Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[10] identify semantic directions by applying the principal component analysis (PCA) on sampled latent codes.",
                "Recently, it has been shown that rich semantic information is encoded in the intermediate features and the latent space of GANs, and furthermore, that images can be effectively edited in a semantically meaningful way by modifying features or latent code [21, 4, 26, 23, 10]."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1f522186eb60db90405888adb6171e353b68bdd7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-08998",
                    "ArXiv": "2108.08998",
                    "DOI": "10.1109/ICCV48922.2021.01368",
                    "CorpusId": 237260139
                },
                "corpusId": 237260139,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/1f522186eb60db90405888adb6171e353b68bdd7",
                "title": "GAN Inversion for Out-of-Range Images with Geometric Transformations",
                "abstract": "For successful semantic editing of real images, it is critical for a GAN inversion method to find an in-domain latent code that aligns with the domain of a pre-trained GAN model. Unfortunately, such in-domain latent codes can be found only for in-range images that align with the training images of a GAN model. In this paper, we propose BDInvert, a novel GAN inversion approach to semantic editing of out- of-range images that are geometrically unaligned with the training images of a GAN model. To find a latent code that is semantically editable, BDInvert inverts an input out-of-range image into an alternative latent space than the original latent space. We also propose a regularized inversion method to find a solution that supports semantic editing in the alternative space. Our experiments show that BDInvert effectively supports semantic editing of out-of-range images with geometric transformations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "151075211",
                        "name": "Kyoungkook Kang"
                    },
                    {
                        "authorId": "2109710243",
                        "name": "S. Kim"
                    },
                    {
                        "authorId": "2149156222",
                        "name": "Sunghyun Cho"
                    }
                ]
            }
        },
        {
            "contexts": [
                "StyleGAN also possesses several desirable properties including projection (also known as GAN inversion) whereby the latent code of an existing image can be recovered [1, 21]; style-mixing whereby portions of the latent codes from different images can be mixed; and controlled synthesis by perturbing latent codes in important directions like the network weights\u2019 eigenvectors to obtain semantically meaningful changes in the output [13, 25].",
                "Recent work has analyzed the GAN latent space and network weights [13, 25]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "d5612daca3272ab9e9da076555f8e46c529d3b1e",
                "externalIds": {
                    "ArXiv": "2108.08922",
                    "DBLP": "journals/corr/abs-2108-08922",
                    "DOI": "10.1109/WACV51458.2022.00019",
                    "CorpusId": 237259774
                },
                "corpusId": 237259774,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/d5612daca3272ab9e9da076555f8e46c529d3b1e",
                "title": "Controlled GAN-Based Creature Synthesis via a Challenging Game Art Dataset - Addressing the Noise-Latent Trade-Off",
                "abstract": "The state-of-the-art StyleGAN2 network supports powerful methods to create and edit art, including generating random images, finding images \"like\" some query, and modifying content or style. Further, recent advancements enable training with small datasets. We apply these methods to synthesize card art, by training on a novel Yu-Gi-Oh dataset. While noise inputs to StyleGAN2 are essential for good synthesis, we find that coarse-scale noise interferes with latent variables on this dataset because both control long-scale image effects. We observe over-aggressive variation in art with changes in noise and weak content control via latent variable edits. Here, we demonstrate that training a modified StyleGAN2, where coarse-scale noise is suppressed, removes these unwanted effects. We obtain a superior FID; changes in noise result in local exploration of style; and identity control is markedly improved. These results and analysis lead towards a GAN-assisted art synthesis tool for digital artists of all skill levels, which can be used in film, games, or any creative industry for artistic ideation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3391147",
                        "name": "Vaibhav Vavilala"
                    },
                    {
                        "authorId": "121068894",
                        "name": "David A. Forsyth"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Thanks to the interpretable controls of GANs, our method could also attain controllable and smooth transitions by walking through GAN latent space.",
                "When we remove the pretrained GANs, our method degrades to a common automatic colorization method without guidance.",
                "Generative priors of pretrained GANs [3, 26, 27, 28] is previously exploited by GAN inversion [1, 13, 38, 41, 62, 63], which aims to find the closest latent codes given an input image.",
                "On the other hand, our method inherits the merits of interpretable controls of GANs [20, 24, 46, 49] and could attain controllable and smooth transitions by walking through GAN latent space.",
                "The prior features from pretrained GANs probably misalign with the input images.",
                "Our method also inherits the merit of interpretable controls of GANs and could attain controllable and smooth transitions by walking through GAN latent space.",
                "One could easily realize diverse colorization by simply adjusting the latent codes or conditions in GANs.",
                "In this work, we have developed a framework to produce vivid and diverse colorization results by leveraging generative color priors encapsulated in pretrained GANs."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "57c4e8877bde057f50601d1bf7ddd24364d5e048",
                "externalIds": {
                    "ArXiv": "2108.08826",
                    "DBLP": "journals/corr/abs-2108-08826",
                    "DOI": "10.1109/ICCV48922.2021.01411",
                    "CorpusId": 237213282
                },
                "corpusId": 237213282,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/57c4e8877bde057f50601d1bf7ddd24364d5e048",
                "title": "Towards Vivid and Diverse Image Colorization with Generative Color Prior",
                "abstract": "Colorization has attracted increasing interest in recent years. Classic reference-based methods usually rely on external color images for plausible results. A large image database or online search engine is inevitably required for retrieving such exemplars. Recent deep-learning-based methods could automatically colorize images at a low cost. However, unsatisfactory artifacts and incoherent colors are always accompanied. In this work, we aim at recovering vivid colors by leveraging the rich and diverse color priors encapsulated in a pretrained Generative Adversarial Networks (GAN). Specifically, we first \"retrieve\" matched features (similar to exemplars) via a GAN encoder and then incorporate these features into the colorization process with feature modulations. Thanks to the powerful generative color prior and delicate designs, our method could produce vivid colors with a single forward pass. Moreover, it is highly convenient to obtain diverse results by modifying GAN latent codes. Our method also inherits the merit of interpretable controls of GANs and could attain controllable and smooth transitions by walking through GAN latent space. Extensive experiments and user studies demonstrate that our method achieves superior performance than previous works.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1477959640",
                        "name": "Yanze Wu"
                    },
                    {
                        "authorId": null,
                        "name": "Xintao Wang"
                    },
                    {
                        "authorId": "2116609855",
                        "name": "Yu Li"
                    },
                    {
                        "authorId": "9184712",
                        "name": "Honglun Zhang"
                    },
                    {
                        "authorId": "2145742614",
                        "name": "Xun Zhao"
                    },
                    {
                        "authorId": "1387190008",
                        "name": "Ying Shan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There are several representative attempts aiming to gain more flexible and specific control upon the generation process by identifying editable latent or feature directions: some [11] performed Principal Components Analysis to the latent space or feature space for direction identification; Alharbi et al."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "93d0751539d77a4e6a98ea045105cb1037ce9b1e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-08674",
                    "ArXiv": "2108.08674",
                    "DOI": "10.1145/3474085.3475206",
                    "CorpusId": 237213543
                },
                "corpusId": 237213543,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/93d0751539d77a4e6a98ea045105cb1037ce9b1e",
                "title": "Towards Controllable and Photorealistic Region-wise Image Manipulation",
                "abstract": "Adaptive and flexible image editing is a desirable function of modern generative models. In this work, we present a generative model with auto-encoder architecture for per-region style manipulation. We apply a code consistency loss to enforce an explicit disentanglement between content and style latent representations, making the content and style of generated samples consistent with their corresponding content and style references. The model is also constrained by a content alignment loss to ensure the foreground editing will not interfere background contents. As a result, given interested region masks provided by users, our model supports foreground region-wise style transfer. Specially, our model receives no extra annotations such as semantic labels except for self-supervision. Extensive experiments show the effectiveness of the proposed method and exhibit the flexibility of the proposed model for various applications, including region-wise style editing, latent space interpolation, cross-domain style transfer.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "150928880",
                        "name": "Ansheng You"
                    },
                    {
                        "authorId": "2128235769",
                        "name": "Chenglin Zhou"
                    },
                    {
                        "authorId": "2107962491",
                        "name": "Qixuan Zhang"
                    },
                    {
                        "authorId": "2112162680",
                        "name": "Lan Xu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[13] performed PCA on the sampled data to find the important and meaningful directions in the style space of StyleGAN.",
                "Several unsupervised methods have been suggested for discovering interpretable directions in the latent space of a pre-trained GAN [2,13,37,38,41]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "79a4a5f673db0293effbb2d1ee17bbcd387854c9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-07668",
                    "ArXiv": "2108.07668",
                    "DOI": "10.1109/ICCV48922.2021.00665",
                    "CorpusId": 237142256
                },
                "corpusId": 237142256,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/79a4a5f673db0293effbb2d1ee17bbcd387854c9",
                "title": "Orthogonal Jacobian Regularization for Unsupervised Disentanglement in Image Generation",
                "abstract": "Unsupervised disentanglement learning is a crucial issue for understanding and exploiting deep generative models. Recently, SeFa tries to find latent disentangled directions by performing SVD on the first projection of a pretrained GAN. However, it is only applied to the first layer and works in a post-processing way. Hessian Penalty minimizes the off-diagonal entries of the output\u2019s Hessian matrix to facilitate disentanglement, and can be applied to multi-layers. However, it constrains each entry of output independently, making it not sufficient in disentangling the latent directions (e.g., shape, size, rotation, etc.) of spatially correlated variations. In this paper, we propose a simple Orthogonal Jacobian Regularization (OroJaR) to encourage deep generative model to learn disentangled representations. It simply encourages the variation of output caused by perturbations on different latent dimensions to be orthogonal, and the Jacobian with respect to the input is calculated to represent this variation. We show that our OroJaR also encourages the output\u2019s Hessian matrix to be diagonal in an indirect manner. In contrast to the Hessian Penalty, our OroJaR constrains the output in a holistic way, making it very effective in disentangling latent dimensions corresponding to spatially correlated variations. Quantitative and qualitative experimental results show that our method is effective in disentangled and controllable image generation, and performs favorably against the state-of-the-art methods. Our code is available at https://github.com/csyxwei/OroJaR.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2156252416",
                        "name": "Yuxiang Wei"
                    },
                    {
                        "authorId": "48081503",
                        "name": "Yupeng Shi"
                    },
                    {
                        "authorId": "2115591290",
                        "name": "Xiao Liu"
                    },
                    {
                        "authorId": "15383651",
                        "name": "Zhilong Ji"
                    },
                    {
                        "authorId": "2154877375",
                        "name": "Yuan Gao"
                    },
                    {
                        "authorId": "4574975",
                        "name": "Zhongqin Wu"
                    },
                    {
                        "authorId": "1724520",
                        "name": "W. Zuo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[7] perform PCA on the sampled data to find primary directions in the latent space."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "476654e048e002c7f70cbd450e09109ba8cecbef",
                "externalIds": {
                    "DBLP": "conf/icml/Zhou20",
                    "ArXiv": "2108.04896",
                    "DOI": "10.1007/978-3-031-04083-2_9",
                    "CorpusId": 236976067
                },
                "corpusId": 236976067,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/476654e048e002c7f70cbd450e09109ba8cecbef",
                "title": "Interpreting Generative Adversarial Networks for Interactive Image Generation",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145291669",
                        "name": "Bolei Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[12] manipulate semantic attributes of images by analyzing latent space of pre-trained generative models, while Huh et al.",
                "Generative models, such as generative adversarial networks (GAN) [3, 10, 19], normalizing flows [21], and variational autoencoders [42], have shown remarkable quality in image generation, and have been applied to numerous purposes such as image-to-image translation [7, 11, 31, 32, 35, 47] and image editing [1, 12, 36].",
                "Specifically, by projecting given images into the latent vectors [1, 2, 4, 49, 57] and manipulating them [6, 11, 12, 26, 36], images are easily edited."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "cda3fbbac6734b603bee363b0938e9baa924aa78",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-02938",
                    "ArXiv": "2108.02938",
                    "DOI": "10.1109/iccv48922.2021.01410",
                    "CorpusId": 236950721
                },
                "corpusId": 236950721,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/cda3fbbac6734b603bee363b0938e9baa924aa78",
                "title": "ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models",
                "abstract": "Denoising diffusion probabilistic models (DDPM) have shown remarkable performance in unconditional image generation. However, due to the stochasticity of the generative process in DDPM, it is challenging to generate images with the desired semantics. In this work, we propose Iterative Latent Variable Refinement (ILVR), a method to guide the generative process in DDPM to generate high-quality images based on a given reference image. Here, the refinement of the generative process in DDPM enables a single DDPM to sample images from various sets directed by the reference image. The proposed ILVR method generates high-quality images while controlling the generation. The controllability of our method allows adaptation of a single DDPM without any additional learning in various image generation tasks, such as generation from various downsampling factors, multi-domain image translation, paint-to-image, and editing with scribbles.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2155367203",
                        "name": "Jooyoung Choi"
                    },
                    {
                        "authorId": "2109687230",
                        "name": "Sungwon Kim"
                    },
                    {
                        "authorId": "112953122",
                        "name": "Yonghyun Jeong"
                    },
                    {
                        "authorId": "2163133",
                        "name": "Youngjune Gwon"
                    },
                    {
                        "authorId": "2999019",
                        "name": "Sungroh Yoon"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Furthermore, we showed the image can be further edited using GANSpace [22] (d).",
                "To investigate this, we apply the latent discovery method GANSpace [22] to the original models."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "64365e06b2f34b13b3d4e95342ec463e42c28dd5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-02774",
                    "ArXiv": "2108.02774",
                    "DOI": "10.1109/ICCV48922.2021.01379",
                    "CorpusId": 236924512
                },
                "corpusId": 236924512,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/64365e06b2f34b13b3d4e95342ec463e42c28dd5",
                "title": "Sketch Your Own GAN",
                "abstract": "Can a user create a deep generative model by sketching a single example? Traditionally, creating a GAN model has required the collection of a large-scale dataset of exemplars and specialized knowledge in deep learning. In contrast, sketching is possibly the most universally accessible way to convey a visual concept. In this work, we present a method, GAN Sketching, for rewriting GANs with one or more sketches, to make GANs training easier for novice users. In particular, we change the weights of an original GAN model according to user sketches. We encourage the model\u2019s output to match the user sketches through a crossdomain adversarial loss. Furthermore, we explore different regularization methods to preserve the original model\u2019s diversity and image quality. Experiments have shown that our method can mold GANs to match shapes and poses specified by sketches while maintaining realism and diversity. Finally, we demonstrate a few applications of the resulting GAN, including latent space interpolation and image editing.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "12782331",
                        "name": "Sheng-Yu Wang"
                    },
                    {
                        "authorId": "144159726",
                        "name": "David Bau"
                    },
                    {
                        "authorId": "1922024303",
                        "name": "Jun-Yan Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANSpace [9] discusses the use of PCA (Principle Component Analysis) to GAN (Generative Adversarial Network) to create inter-"
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "0e369ad5005acf536da391be57b87846d1331123",
                "externalIds": {
                    "DBLP": "conf/ic3/SainiRPG21",
                    "DOI": "10.1145/3474124.3474166",
                    "CorpusId": 243353872
                },
                "corpusId": 243353872,
                "publicationVenue": {
                    "id": "3bc59e53-8413-4f2a-b739-d2b155898833",
                    "name": "International Conference on Contemporary Computing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Contemp Comput",
                        "IC3"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1263"
                },
                "url": "https://www.semanticscholar.org/paper/0e369ad5005acf536da391be57b87846d1331123",
                "title": "An Encoder-decoder based approach for generating Faces using Facial Attributes using CNN",
                "abstract": "This paper addresses the challenge of generating faces using facial attributes. Although there are researches the address the problem of generating faces, they do so by using a facial image as a base and changing the required attributes. To solve this problem, we make CNN models to learn a classifier that can predict these features (1 feature per model) and output their labels. Labels are the enumerated value each attribute can take. Then these models are combined into one model to generate a dataset that maps the above 6 facial features to each image. This prepared dataset is then used to train the final CNN model that learns to generate a 200 \u00d7 200 \u00d7 3 matrix using a 6 \u00d7 1 matrix as input. The output matrix represents the resolution of the image with 3 channels namely, Red, Green and Blue. This 3D array when plotted gives the desired image. The 6 \u00d7 1 matrix represents the six labels. To improve the output, the final CNN model is changed and an Auto-encoder and decoder are used. Also, instead of 6 \u00d7 1 input array, 55 \u00d7 1 input array is used. This is first trained to regenerate images from an input image. The decoder from this trained model is then used for transfer learning. The decoder is retrained to learn the features specified by the 55 \u00d7 1 input matrix. Finally, this decoder is used to generate the desired images of size 150 \u00d7 150 \u00d7 3 using the 55 \u00d7 1 input matrix.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "38828311",
                        "name": "Anu Saini"
                    },
                    {
                        "authorId": "49508067",
                        "name": "Mukul Rawat"
                    },
                    {
                        "authorId": "2054368700",
                        "name": "Nikhil Pandey"
                    },
                    {
                        "authorId": "2148294805",
                        "name": "Puneet Gupta"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Such a latent space is conductive for tasks such as image editing and image-to-image translation [3, 13, 30, 33, 36].",
                "These range from image enhancement [18,50], editing [13,36] and recently even discriminative tasks such as classification and regression [27, 48].",
                "By traversing this intermediate latent space, W , or by mixing different w codes across different network layers, prior work demonstrated fine-grained control over semantic properties in generated images [2, 13, 30, 36]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "be697c79df8e4b280fec71751cb2d44667429f36",
                "externalIds": {
                    "DBLP": "journals/tog/GalPMBCC22",
                    "ArXiv": "2108.00946",
                    "DOI": "10.1145/3528223.3530164",
                    "CorpusId": 236772156
                },
                "corpusId": 236772156,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/be697c79df8e4b280fec71751cb2d44667429f36",
                "title": "StyleGAN-NADA",
                "abstract": "Can a generative model be trained to produce images from a specific domain, guided only by a text prompt, without seeing any image? In other words: can an image generator be trained \"blindly\"? Leveraging the semantic power of large scale Contrastive-Language-Image-Pre-training (CLIP) models, we present a text-driven method that allows shifting a generative model to new domains, without having to collect even a single image. We show that through natural language prompts and a few minutes of training, our method can adapt a generator across a multitude of domains characterized by diverse styles and shapes. Notably, many of these modifications would be difficult or infeasible to reach with existing methods. We conduct an extensive set of experiments across a wide range of domains. These demonstrate the effectiveness of our approach, and show that our models preserve the latent-space structure that makes generative models appealing for downstream tasks. Code and videos available at: stylegan-nada.github.io/",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "134639223",
                        "name": "Rinon Gal"
                    },
                    {
                        "authorId": "2819477",
                        "name": "Or Patashnik"
                    },
                    {
                        "authorId": "3416939",
                        "name": "Haggai Maron"
                    },
                    {
                        "authorId": "1755628",
                        "name": "Amit H. Bermano"
                    },
                    {
                        "authorId": "1732280",
                        "name": "Gal Chechik"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020), and discovering meaningful latent directions (Shen et al., 2020; Goetschalckx et al., 2019; Jahanian et al., 2020; H\u00e4rk\u00f6nen et al., 2020).",
                "\u2026designing better encoders (Richardson et al., 2021; Tov et al., 2021), modeling image corruption and transformations (Anirudh et al., 2020; Huh et al., 2020), and discovering meaningful latent directions (Shen et al., 2020; Goetschalckx et al., 2019; Jahanian et al., 2020; Ha\u0308rko\u0308nen et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f671a09e3e5922e6d38cb77dda8d76d5ceac2a27",
                "externalIds": {
                    "DBLP": "conf/iclr/MengHSSWZE22",
                    "ArXiv": "2108.01073",
                    "CorpusId": 245704504
                },
                "corpusId": 245704504,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f671a09e3e5922e6d38cb77dda8d76d5ceac2a27",
                "title": "SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations",
                "abstract": "Guided image synthesis enables everyday users to create and edit photo-realistic images with minimum effort. The key challenge is balancing faithfulness to the user input (e.g., hand-drawn colored strokes) and realism of the synthesized image. Existing GAN-based methods attempt to achieve such balance using either conditional GANs or GAN inversions, which are challenging and often require additional training data or loss functions for individual applications. To address these issues, we introduce a new image synthesis and editing method, Stochastic Differential Editing (SDEdit), based on a diffusion model generative prior, which synthesizes realistic images by iteratively denoising through a stochastic differential equation (SDE). Given an input image with user guide of any type, SDEdit first adds noise to the input, then subsequently denoises the resulting image through the SDE prior to increase its realism. SDEdit does not require task-specific training or inversions and can naturally achieve the balance between realism and faithfulness. SDEdit significantly outperforms state-of-the-art GAN-based methods by up to 98.09% on realism and 91.72% on overall satisfaction scores, according to a human perception study, on multiple tasks, including stroke-based image synthesis and editing as well as image compositing.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "83262128",
                        "name": "Chenlin Meng"
                    },
                    {
                        "authorId": "1490933536",
                        "name": "Yutong He"
                    },
                    {
                        "authorId": "2157995251",
                        "name": "Yang Song"
                    },
                    {
                        "authorId": "51453887",
                        "name": "Jiaming Song"
                    },
                    {
                        "authorId": "2110435874",
                        "name": "Jiajun Wu"
                    },
                    {
                        "authorId": "1922024303",
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "authorId": "2490652",
                        "name": "Stefano Ermon"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These normal vectors are commonly referred to as linear editing directions and many previous works proposed methods to identify them [20, 37, 45, 46].",
                "Many recent works have proposed methods to interpret the semantics encoded in this latent space and apply them to image editing [2, 20, 37, 45, 50, 57, 62]."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "f312db26053b39c90d5220a669d15ccea0635363",
                "externalIds": {
                    "DBLP": "conf/cvpr/NitzanGBC22",
                    "ArXiv": "2107.11186",
                    "DOI": "10.1109/CVPR52688.2022.01864",
                    "CorpusId": 236318260
                },
                "corpusId": 236318260,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f312db26053b39c90d5220a669d15ccea0635363",
                "title": "LARGE: Latent-Based Regression through GAN Semantics",
                "abstract": "We propose a novel method for solving regression tasks using few-shot or weak supervision. At the core of our method is the fundamental observation that GANs are incredibly successful at encoding semantic information within their latent space, even in a completely unsupervised setting. For modern generative frameworks, this semantic encoding manifests as smooth, linear directions which affect image attributes in a disentangled manner. These directions have been widely used in GAN-based image editing. In this work, we leverage them for few-shot regression. Specifically, we make the simple observation that distances traversed along such directions are good features for downstream tasks - reliably gauging the magnitude of a property in an image. In the absence of explicit supervision, we use these distances to solve tasks such as sorting a collection of images, and ordinal regression. With a few labels - as little as two - we calibrate these distances to real-world values and convert a pre-trained GAN into a state-of-the-art few-shot regression model. This enables solving regression tasks on datasets and attributes which are difficult to produce quality supervision for. Extensive experimental evaluations demonstrate that our method can be applied across a wide range of domains, leverage multiple latent direction discovery frame-works, and achieve state-of-the-art results in few-shot and low-supervision settings, even when compared to methods designed to tackle a single task. Code is available on our project website.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1702992975",
                        "name": "Yotam Nitzan"
                    },
                    {
                        "authorId": "134639223",
                        "name": "Rinon Gal"
                    },
                    {
                        "authorId": "2120835121",
                        "name": "Ofir Brenner"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The other part of the hybrid architecture, GANSpace, presents a method for finding significant latent space directions in a trained GAN model (Ha\u0308rko\u0308nen et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "83859ba4ffd3cf23aac976ea5da36d1ccae9da9c",
                "externalIds": {
                    "MAG": "3200026723",
                    "DOI": "10.5281/ZENODO.5137902",
                    "CorpusId": 240555881
                },
                "corpusId": 240555881,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/83859ba4ffd3cf23aac976ea5da36d1ccae9da9c",
                "title": "GANSpaceSynth: A Hybrid Generative Adversarial Network Architecture for Organising the Latent Space using a Dimensionality Reduction for Real-Time Audio Synthesis",
                "abstract": "Generative models enable possibilities in audio domain to present timbre as vectors in a high-dimensional latent space with Generative Adversarial Networks (GANs). It is a common method in GAN models in which the musician\u2019s control over timbre is mostly limited to sampling random points from the space and interpolating between them. In this paper, we present a novel hybrid GAN architecture that allows musicians to explore the GAN latent space in a more controlled manner, identifying the audio features in the trained checkpoints and giving an opportunity to specify particular audio features to be present or absent in the generated audio samples. We extend the paper with the detailed description of our GANSpaceSynth and present the Hallu composition tool as an application of this hybrid method in computer music practices.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2205587",
                        "name": "Koray Tahiroglu"
                    },
                    {
                        "authorId": "1729597455",
                        "name": "Miranda Kastemaa"
                    },
                    {
                        "authorId": "2132786794",
                        "name": "Oskar Koli"
                    }
                ]
            }
        },
        {
            "contexts": [
                "However, prior works either operate at the scene level [Granskog et al. 2020; H\u00e4rk\u00f6nen et al. 2020; Kulkarni et al. 2015; Nie et al. 2020], or rely on learned representations of standard graphics primitives, e.g. voxel grids [NguyenPhuoc et al. 2019, 2020, 2018; Olszewski et al. 2019; Rematas and\u2026"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "a788b829395c780ea5cb4fef6cc64c7bc331d739",
                "externalIds": {
                    "DOI": "10.1145/3476576.3476749",
                    "CorpusId": 235602180
                },
                "corpusId": 235602180,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a788b829395c780ea5cb4fef6cc64c7bc331d739",
                "title": "Neural scene graph rendering",
                "abstract": "We present a neural scene graph---a modular and controllable representation of scenes with elements that are learned from data. We focus on the forward rendering problem, where the scene graph is provided by the user and references learned elements. The elements correspond to geometry and material definitions of scene objects and constitute the leaves of the graph; we store them as high-dimensional vectors. The position and appearance of scene objects can be adjusted in an artist-friendly manner via familiar transformations, e.g. translation, bending, or color hue shift, which are stored in the inner nodes of the graph. In order to apply a (non-linear) transformation to a learned vector, we adopt the concept of linearizing a problem by lifting it into higher dimensions: we first encode the transformation into a high-dimensional matrix and then apply it by standard matrix-vector multiplication. The transformations are encoded using neural networks. We render the scene graph using a streaming neural renderer, which can handle graphs with a varying number of objects, and thereby facilitates scalability. Our results demonstrate a precise control over the learned object representations in a number of animated 2D and 3D scenes. Despite the limited visual complexity, our work presents a step towards marrying traditional editing mechanisms with learned representations, and towards high-quality, controllable neural rendering.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "104323907",
                        "name": "Jonathan Granskog"
                    },
                    {
                        "authorId": "2067062740",
                        "name": "Till N. Schnabel"
                    },
                    {
                        "authorId": "1801097",
                        "name": "Fabrice Rousselle"
                    },
                    {
                        "authorId": "2052127348",
                        "name": "Jan Nov\u00e1k"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANSpace [H\u00e4rk\u00f6nen et al. 2020] explores the linearity of the GAN space and its latent directions based on Principal Component Analysis."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "e30aaee173ca211e95c8f2fd38961336eefc798c",
                "externalIds": {
                    "DBLP": "journals/tog/WuYX021",
                    "DOI": "10.1145/3476576.3476591",
                    "CorpusId": 236094869
                },
                "corpusId": 236094869,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e30aaee173ca211e95c8f2fd38961336eefc798c",
                "title": "Coarse-to-fine",
                "abstract": "Facial structure editing of portrait images is challenging given the facial variety, the lack of ground-truth, the necessity of jointly adjusting color and shape, and the requirement of no visual artifacts. In this paper, we investigate how to perform chin editing as a case study of editing facial structures. We present a novel method that can automatically remove the double chin effect in portrait images. Our core idea is to train a fine classification boundary in the latent space of the portrait images. This can be used to edit the chin appearance by manipulating the latent code of the input portrait image while preserving the original portrait features. To achieve such a fine separation boundary, we employ a carefully designed training stage based on latent codes of paired synthetic images with and without a double chin. In the testing stage, our method can automatically handle portrait images with only a refinement to subtle misalignment before and after double chin editing. Our model enables alteration to the neck region of the input portrait image while keeping other regions unchanged, and guarantees the rationality of neck structure and the consistency of facial characteristics. To the best of our knowledge, this presents the first effort towards an effective application for editing double chins. We validate the efficacy and efficiency of our approach through extensive experiments and user studies.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2115290070",
                        "name": "Yiqian Wu"
                    },
                    {
                        "authorId": "6635795",
                        "name": "Yong-Liang Yang"
                    },
                    {
                        "authorId": "65827624",
                        "name": "Qinjie Xiao"
                    },
                    {
                        "authorId": "144240366",
                        "name": "Xiaogang Jin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "General decompositions such as PCA analysis could then be considered (H\u00e4rk\u00f6nen et al., 2020).",
                "General decompositions such as PCA analysis could then be considered (Ha\u0308rko\u0308nen et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "509e4d6282d1d76ef603d7186fe0504657aab253",
                "externalIds": {
                    "ArXiv": "2107.10657",
                    "DBLP": "journals/corr/abs-2107-10657",
                    "CorpusId": 236170951
                },
                "corpusId": 236170951,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/509e4d6282d1d76ef603d7186fe0504657aab253",
                "title": "Solving inverse problems with deep neural networks driven by sparse signal decomposition in a physics-based dictionary",
                "abstract": "Deep neural networks (DNN) have an impressive ability to invert very complex models, i.e. to learn the generative parameters from a model's output. Once trained, the forward pass of a DNN is often much faster than traditional, optimization-based methods used to solve inverse problems. This is however done at the cost of lower interpretability, a fundamental limitation in most medical applications. We propose an approach for solving general inverse problems which combines the efficiency of DNN and the interpretability of traditional analytical methods. The measurements are first projected onto a dense dictionary of model-based responses. The resulting sparse representation is then fed to a DNN with an architecture driven by the problem's physics for fast parameter learning. Our method can handle generative forward models that are costly to evaluate and exhibits similar performance in accuracy and computation time as a fully-learned DNN, while maintaining high interpretability and being easier to train. Concrete results are shown on an example of model-based brain parameter estimation from magnetic resonance imaging (MRI).",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3422955",
                        "name": "Ga\u00ebtan Rensonnet"
                    },
                    {
                        "authorId": "2120231730",
                        "name": "Louise Adam"
                    },
                    {
                        "authorId": "120876096",
                        "name": "B. Macq"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We achieve this by making use of the advantageous properties of StyleGAN\u2019sW+ space, that have been used for face editing before [9, 25, 26]: Our main assumption is that given a point inW+, the directions into which one would need to shift this point in order to change the identity of the actor that it depicts are mostly orthogonal to those directions that would change the pose/expression/articulation of the actor."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4f0622bb791b997582142cf007eea0e5b17285b7",
                "externalIds": {
                    "DBLP": "conf/bmvc/FoxTET21",
                    "ArXiv": "2107.07224",
                    "CorpusId": 235899129
                },
                "corpusId": 235899129,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/4f0622bb791b997582142cf007eea0e5b17285b7",
                "title": "StyleVideoGAN: A Temporal Generative Model using a Pretrained StyleGAN",
                "abstract": "Generative adversarial models (GANs) continue to produce advances in terms of the visual quality of still images, as well as the learning of temporal correlations. However, few works manage to combine these two interesting capabilities for the synthesis of video content: Most methods require an extensive training dataset to learn temporal correlations, while being rather limited in the resolution and visual quality of their output. We present a novel approach to the video synthesis problem that helps to greatly improve visual quality and drastically reduce the amount of training data and resources necessary for generating videos. Our formulation separates the spatial domain, in which individual frames are synthesized, from the temporal domain, in which motion is generated. For the spatial domain we use a pre-trained StyleGAN network, the latent space of which allows control over the appearance of the objects it was trained for. The expressive power of this model allows us to embed our training videos in the StyleGAN latent space. Our temporal architecture is then trained not on sequences of RGB frames, but on sequences of StyleGAN latent codes. The advantageous properties of the StyleGAN space simplify the discovery of temporal correlations. We demonstrate that it suffices to train our temporal architecture on only 10 minutes of footage of 1 subject for about 6 hours. After training, our model can not only generate new portrait videos for the training subject, but also for any random subject which can be embedded in the StyleGAN space.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "39745839",
                        "name": "Gereon Fox"
                    },
                    {
                        "authorId": "9102722",
                        "name": "A. Tewari"
                    },
                    {
                        "authorId": "1854465",
                        "name": "Mohamed A. Elgharib"
                    },
                    {
                        "authorId": "1680185",
                        "name": "C. Theobalt"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, GANSpace [13] requires manually choosing layer subsets; AttGAN [14] and InterFaceGAN [31] requires attribute labels, StyleRig [36] requires a 3D face model.",
                "GANSpace [13] uses PCA to identify meaningful latent directions.",
                "While other work based on StyleGAN, including EIS [10, 13], focus on manipulating generated images, we focus on the more relevant problem of manipulating real images.",
                "As an alternative, unsupervised discovery of latent directions in a pretrained GAN [13, 31, 39] allows for finding meaningful latent representations in a computationally efficient way.",
                "Unlike methods that manipulate the latent space via vector arithmetic [13, 16, 31, 32, 39], EIS formulates the semantic editing problem as copying style coefficients \u03c3 of StyleGAN [18] from a reference image to a source image, i."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6339d9370249daf0567bb56ad977c49bfbf8b28d",
                "externalIds": {
                    "DBLP": "conf/iccv/ChongCKF21",
                    "ArXiv": "2107.06256",
                    "DOI": "10.1109/iccv48922.2021.00386",
                    "CorpusId": 235829350
                },
                "corpusId": 235829350,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/6339d9370249daf0567bb56ad977c49bfbf8b28d",
                "title": "Retrieve in Style: Unsupervised Facial Feature Transfer and Retrieval",
                "abstract": "We present Retrieve in Style (RIS), an unsupervised framework for facial feature transfer and retrieval on real images. Recent work shows capabilities of transferring local facial features by capitalizing on the disentanglement property of the StyleGAN latent space. RIS improves existing art on the following: 1) Introducing more effective feature disentanglement to allow for challenging transfers (i.e., hair, pose) that were not shown possible in SoTA methods. 2) Eliminating the need for per-image hyperparameter tuning, and for computing a catalog over a large batch of images. 3) Enabling fine-grained face retrieval using disentangled facial features (e.g., eyes). To our best knowledge, this is the first work to retrieve face images at this fine level. 4) Demonstrating robust, natural editing on real images. Our qualitative and quantitative analyses show RIS achieves both high-fidelity feature transfers and accurate fine-grained retrievals on real images. We also discuss the responsible applications of RIS. Our code is available at https://github.com/mchong6/RetrieveInStyle.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50615993",
                        "name": "Min Jin Chong"
                    },
                    {
                        "authorId": "39336289",
                        "name": "Wen-Sheng Chu"
                    },
                    {
                        "authorId": "2109224633",
                        "name": "Abhishek Kumar"
                    },
                    {
                        "authorId": "121068894",
                        "name": "David A. Forsyth"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While several ways of controlling the generative process have been found [8, 28, 10, 41, 24, 2, 7, 46, 6], the foundations of the synthesis process remain only partially understood."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c1ff08b59f00c44f34dfdde55cd53370733a2c19",
                "externalIds": {
                    "MAG": "3174807077",
                    "DBLP": "conf/nips/KarrasALHHLA21",
                    "ArXiv": "2106.12423",
                    "CorpusId": 235606261
                },
                "corpusId": 235606261,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c1ff08b59f00c44f34dfdde55cd53370733a2c19",
                "title": "Alias-Free Generative Adversarial Networks",
                "abstract": "We observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to image coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2976930",
                        "name": "Tero Karras"
                    },
                    {
                        "authorId": "1907688",
                        "name": "M. Aittala"
                    },
                    {
                        "authorId": "36436218",
                        "name": "S. Laine"
                    },
                    {
                        "authorId": "103642338",
                        "name": "Erik H\u00e4rk\u00f6nen"
                    },
                    {
                        "authorId": "1454226629",
                        "name": "Janne Hellsten"
                    },
                    {
                        "authorId": "49244945",
                        "name": "J. Lehtinen"
                    },
                    {
                        "authorId": "1761103",
                        "name": "Timo Aila"
                    }
                ]
            }
        },
        {
            "contexts": [
                "From the 3rd column in each subfigure, from left to right are the manipulation result of GANSpace [18], that of InterFaceGAN [34] and ours.",
                "We compare our results with two state-of-the-art methods: InterFaceGAN [34] and GANSpace [18].",
                "GANSpace [18] performed PCA in the latent space of generative networks, explored the principal directions and discovered interpretable controls."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "83b154add7157c00c92274dd28cc1f224afd9289",
                "externalIds": {
                    "ArXiv": "2106.11895",
                    "DBLP": "conf/iccv/YaoNGH21",
                    "DOI": "10.1109/ICCV48922.2021.01353",
                    "CorpusId": 237154246
                },
                "corpusId": 237154246,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/83b154add7157c00c92274dd28cc1f224afd9289",
                "title": "A Latent Transformer for Disentangled Face Editing in Images and Videos",
                "abstract": "High quality facial image editing is a challenging problem in the movie post-production industry, requiring a high degree of control and identity preservation. Previous works that attempt to tackle this problem may suffer from the entanglement of facial attributes and the loss of the person\u2019s identity. Furthermore, many algorithms are limited to a certain task. To tackle these limitations, we propose to edit facial attributes via the latent space of a StyleGAN generator, by training a dedicated latent transformation network and incorporating explicit disentanglement and identity preservation terms in the loss function. We further introduce a pipeline to generalize our face editing to videos. Our model achieves a disentangled, controllable, and identity-preserving facial attribute editing, even in the challenging case of real (i.e., non-synthetic) images and videos. We conduct extensive experiments on image and video datasets and show that our model outperforms other state-of-the-art methods in visual quality and quantitative evaluation. Source codes are available at https://github.com/InterDigitalInc/latent-transformer.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2115586564",
                        "name": "Xu Yao"
                    },
                    {
                        "authorId": "1902919",
                        "name": "A. Newson"
                    },
                    {
                        "authorId": "1796594",
                        "name": "Y. Gousseau"
                    },
                    {
                        "authorId": "1806880",
                        "name": "P. Hellier"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These properties alleviate us to search for directions in the latent space as in [22] or to directly hardcode conditional features in the architecture as in [23]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "93667328d7f9998523f4a0b01ab6f72dea947612",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-07431",
                    "ArXiv": "2106.07431",
                    "CorpusId": 235422338
                },
                "corpusId": 235422338,
                "publicationVenue": {
                    "id": "cfc287e4-4c04-4848-ab16-633b33a61a09",
                    "name": "International Society for Music Information Retrieval Conference",
                    "type": "conference",
                    "alternate_names": [
                        "International Symposium/Conference on Music Information Retrieval",
                        "ISMIR",
                        "Int Soc Music Inf Retr Conf",
                        "Int Symp Music Inf Retr"
                    ],
                    "url": "http://www.ismir.net/"
                },
                "url": "https://www.semanticscholar.org/paper/93667328d7f9998523f4a0b01ab6f72dea947612",
                "title": "CRASH: Raw Audio Score-based Generative Modeling for Controllable High-resolution Drum Sound Synthesis",
                "abstract": "In this paper, we propose a novel score-base generative model for unconditional raw audio synthesis. Our proposal builds upon the latest developments on diffusion process modeling with stochastic differential equations, which already demonstrated promising results on image generation. We motivate novel heuristics for the choice of the diffusion processes better suited for audio generation, and consider the use of a conditional U-Net to approximate the score function. While previous approaches on diffusion models on audio were mainly designed as speech vocoders in medium resolution, our method termed CRASH (Controllable Raw Audio Synthesis with High-resolution) allows us to generate short percussive sounds in 44.1kHz in a controllable way. Through extensive experiments, we showcase on a drum sound generation task the numerous sampling schemes offered by our method (unconditional generation, deterministic generation, inpainting, interpolation, variations, class-conditional sampling) and propose the class-mixing sampling, a novel way to generate\"hybrid\"sounds. Our proposed method closes the gap with GAN-based methods on raw audio, while offering more flexible generation capabilities with lighter and easier-to-train models.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2111880204",
                        "name": "Simon Rouard"
                    },
                    {
                        "authorId": "3254356",
                        "name": "Ga\u00ebtan Hadjeres"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In Fig 5, we compare the semantic factorizations of Local Basis and GANSpace [15] for the particular semantics discovered by GANSpace.",
                "By contrast, the global methods, such as GANSpace [15] and SeFa [16], propose a global direction for the particular semantics (e.",
                "GANSpace [15] finds a global basis for W in StyleGAN using a PCA, enabling a fast image manipulation.",
                "(b) Comparison of Latent Traversal Methods (Global methods: GANSpace [15] and SeFa [16], Local methods: Ramesh et al.",
                "We compare the semantic-factorizing directions of GANSpace provided by the authors [15] with Local Basis of the highest cosine similarity.",
                "In Fig 3, the traversal image of Local Basis is compared with those of the global methods (GANSpace [15] and SeFa [16]) under the strong perturbation intensity of 12 along the 1st and 2nd direction of each method."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1e2770f58ba8b307d3048a4471cc42c489614484",
                "externalIds": {
                    "ArXiv": "2106.06959",
                    "DBLP": "conf/iclr/ChoiLYPHK22",
                    "CorpusId": 235422602
                },
                "corpusId": 235422602,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/1e2770f58ba8b307d3048a4471cc42c489614484",
                "title": "Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs",
                "abstract": "The discovery of the disentanglement properties of the latent space in GANs motivated a lot of research to find the semantically meaningful directions on it. In this paper, we suggest that the disentanglement property is closely related to the geometry of the latent space. In this regard, we propose an unsupervised method for finding the semantic-factorizing directions on the intermediate latent space of GANs based on the local geometry. Intuitively, our proposed method, called Local Basis, finds the principal variation of the latent space in the neighborhood of the base latent variable. Experimental results show that the local principal variation corresponds to the semantic factorization and traversing along it provides strong robustness to image traversal. Moreover, we suggest an explanation for the limited success in finding the global traversal directions in the latent space, especially W-space of StyleGAN2. We show that W-space is warped globally by comparing the local geometry, discovered from Local Basis, through the metric on Grassmannian Manifold. The global warpage implies that the latent space is not well-aligned globally and therefore the global traversal directions are bound to show limited success on it.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49331294",
                        "name": "Jaewoong Choi"
                    },
                    {
                        "authorId": "14122974",
                        "name": "Changyeon Yoon"
                    },
                    {
                        "authorId": "2108454988",
                        "name": "Junho Lee"
                    },
                    {
                        "authorId": "2118946034",
                        "name": "J. Park"
                    },
                    {
                        "authorId": "1947131861",
                        "name": "Geonho Hwang"
                    },
                    {
                        "authorId": "2259103",
                        "name": "Myung-joo Kang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We use the popular GANSpace [14] and InterfaceGAN [34] methods for latent-based editing.",
                "We use the popular GANSpace [H\u00e4rk\u00f6nen et al. 2020] and InterfaceGAN [Shen et al. 2020] methods for latent-based editing. hese approaches are orthogonal to ours, as they require the use of an inversion algorithm to edit real images.",
                "[14] identify latent directions based on Principal Component Analysis (PCA).",
                "Using this property, many methods demonstrate realistic editing abilities over StylGAN\u2019s latent space [3, 7, 14, 27, 34, 36, 42], such as changing facial orientations, expressions, or age, by traversing the learned manifold.",
                "Using this property, many methods demonstrate realistic editing abilities over StyleGAN\u2019s latent space [Abdal et al. 2021; Collins et al. 2020; H\u00e4rk\u00f6nen et al. 2020; Patashnik et al. 2021; Shen et al. 2020; Tewari et al. 2020a; Wu et al. 2021], such as changing facial orientations, expressions, or\u2026",
                "Some using fullsupervision in the form of semantic labels [10, 11, 34], others [15, 30, 35] find meaningful directions in a selfsupervised fashion, and finally recent works present unsupervised methods to achieve the same goal [14, 39, 40], requiring no manual annotations."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "e27b881192cd14a04018c372352b12e531870fc1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-05744",
                    "ArXiv": "2106.05744",
                    "DOI": "10.1145/3544777",
                    "CorpusId": 235390635
                },
                "corpusId": 235390635,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e27b881192cd14a04018c372352b12e531870fc1",
                "title": "Pivotal Tuning for Latent-based Editing of Real Images",
                "abstract": "Recently, numerous facial editing techniques have been proposed that leverage the generative power of a pretrained StyleGAN. To successfully edit an image this way, one must first project (or invert) the image into the pretrained generator\u2019s domain. As it turns out, StyleGAN\u2019s latent space induces an inherent tradeoff between distortion and editability, i.e., between maintaining the original appearance and convincingly altering its attributes. Hence, it remains challenging to apply ID-preserving edits to real facial images. In this article, we present an approach to bridge this gap. The idea is Pivotal Tuning\u2014a brief training process that preserves editing quality, while surgically changing the portrayed identity and appearance. In Pivotal Tuning Inversion, an initial inverted latent code serves as a pivot, around which the generator is fine-tuned. At the same time, a regularization term keeps nearby identities intact, to locally contain the effect. We further show that Pivotal Tuning also applies to accommodating for a multitude of faces, while introducing negligible distortion on the rest of the domain. We validate our technique through inversion and editing metrics and show preferable scores to state-of-the-art methods. Last, we present successful editing for harder cases, including elaborate make-up or headwear.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2110687042",
                        "name": "Daniel Roich"
                    },
                    {
                        "authorId": "147940380",
                        "name": "Ron Mokady"
                    },
                    {
                        "authorId": "1755628",
                        "name": "Amit H. Bermano"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Interestingly, generative models can also create multiple views of an image: by steering in their latent spaces they can achieve camera and color transformations [30] and more [24, 65]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "3743249bf829cbe0de72cc49371f51c40d7cf56c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-05258",
                    "ArXiv": "2106.05258",
                    "CorpusId": 235377428
                },
                "corpusId": 235377428,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3743249bf829cbe0de72cc49371f51c40d7cf56c",
                "title": "Generative Models as a Data Source for Multiview Representation Learning",
                "abstract": "Generative models are now capable of producing highly realistic images that look nearly indistinguishable from the data on which they are trained. This raises the question: if we have good enough generative models, do we still need datasets? We investigate this question in the setting of learning general-purpose visual representations from a black-box generative model rather than directly from data. Given an off-the-shelf image generator without any access to its training data, we train representations from the samples output by this generator. We compare several representation learning methods that can be applied to this setting, using the latent space of the generator to generate multiple \u201cviews\u201d of the same semantic content. We show that for contrastive methods, this multiview data can naturally be used to identify positive pairs (nearby in latent space) and negative pairs (far apart in latent space). We find that the resulting representations rival those learned directly from real data, but that good performance requires care in the sampling strategy applied and the training method. Generative models can be viewed as a compressed and organized copy of a dataset, and we envision a future where more and more \u201cmodel zoos\u201d proliferate while datasets become increasingly unwieldy, missing, or private. This paper suggests several techniques for dealing with visual representation learning in such a future. Code is released on our project page https://ali-design.github.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "19203468",
                        "name": "Ali Jahanian"
                    },
                    {
                        "authorId": "143872936",
                        "name": "Xavier Puig"
                    },
                    {
                        "authorId": "2476765",
                        "name": "Yonglong Tian"
                    },
                    {
                        "authorId": "2094770",
                        "name": "Phillip Isola"
                    }
                ]
            }
        },
        {
            "contexts": [
                "7, both GANSpace and SeFa have some degree of influence on the other region when editing a specific region.",
                "Methods FID\u2193 SWD\u2193 User Study GANSpace [15] 7.",
                "C V\n] 3\n0 N\nov 2\nbeyond well-defined annotations) and proposed to find steerable directions of the latent space in an unsupervised manner, such as using Principal Component Analysis (PCA) [15].",
                ", unable to identify semantics beyond well-defined annotations) and proposed to find steerable directions of the latent space in an unsupervised manner, such as using Principal Component Analysis (PCA) [15].",
                "For example, when adding the smile, the identity of GANSpace is changed as well as hairstyle.",
                "And then, we project the principal vectors into the null space of PCA.",
                "Prior work [33, 28, 15] has pointed out that the above pipeline could be limited by the labeling step (e.",
                "We compare our method with GANSpace [15] and SeFa [28] on StyleGAN2, which are the two state-of-the-art unsupervised approaches to image editing.",
                "Rigorously, simply conducting PCA onM in Eq.4 in the main paper does not possess a null space since all the eigenvalues are not equal to zeros.",
                "LowRankGAN (Ours) Figure 7: Comparison on image local editing with GANSpace [15] and SeFa [28].",
                ", supervised [13, 30, 18, 24] and unsupervised [33, 15, 28, 31].",
                "We find the most relevant vectors that can control the smile and hair in GANSpace and SeFa, according to their papers.",
                "And the new problem can be categorized to Robust PCA, which is formulated as follows: Problem 1.",
                "This implies that the null space of PCA does not contain any components that could affect the region of the eyes.",
                "When changing the hair color, GANSpace just has little activation on this attribute, while SeFa has an obvious change in the background.",
                "To be specific, the attribute vectors that could edit the regions of eyes vanishes after projecting them to the null space of PCA.",
                "Classical PCA seeks the best rank-r estimate of L0 [36] via Singular Value Decomposition (SVD).",
                "Table 1: Quantitative comparison results on (a) the effect of using null space projection, and (b) the image quality from various image editing approaches [15, 28].",
                "Previous works [15, 28, 24, 33] could edit some attributes in a specific region but without any control on the rest region, which actually results in a global change on the image when editing the region of interest.",
                "On the other hand, unsupervised approaches try to discover steerable latent dimensions by statistically analyzing the latent space [15], maximizing the mutual information between the latent space and the image space [33], or exploring model parameters [28, 31]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "cd0b08fc463526972b725c2d0b6d11db8efa6f11",
                "externalIds": {
                    "DBLP": "conf/nips/ZhuFSZZZC21",
                    "ArXiv": "2106.04488",
                    "CorpusId": 235367855
                },
                "corpusId": 235367855,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/cd0b08fc463526972b725c2d0b6d11db8efa6f11",
                "title": "Low-Rank Subspaces in GANs",
                "abstract": "The latent space of a Generative Adversarial Network (GAN) has been shown to encode rich semantics within some subspaces. To identify these subspaces, researchers typically analyze the statistical information from a collection of synthesized data, and the identified subspaces tend to control image attributes globally (i.e., manipulating an attribute causes the change of an entire image). By contrast, this work introduces low-rank subspaces that enable more precise control of GAN generation. Concretely, given an arbitrary image and a region of interest (e.g., eyes of face images), we manage to relate the latent space to the image region with the Jacobian matrix and then use low-rank factorization to discover steerable latent subspaces. There are three distinguishable strengths of our approach that can be aptly called LowRankGAN. First, compared to analytic algorithms in prior work, our low-rank factorization of Jacobians is able to find the low-dimensional representation of attribute manifold, making image editing more precise and controllable. Second, low-rank factorization naturally yields a null space of attributes such that moving the latent code within it only affects the outer region of interest. Therefore, local image editing can be simply achieved by projecting an attribute vector into the null space without relying on a spatial mask as existing methods do. Third, our method can robustly work with a local region from one image for analysis yet well generalize to other images, making it much easy to use in practice. Extensive experiments on state-of-the-art GAN models (including StyleGAN2 and BigGAN) trained on various datasets demonstrate the effectiveness of our LowRankGAN.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47054925",
                        "name": "Jiapeng Zhu"
                    },
                    {
                        "authorId": "2119237546",
                        "name": "Ruili Feng"
                    },
                    {
                        "authorId": "2117687899",
                        "name": "Yujun Shen"
                    },
                    {
                        "authorId": "1678783",
                        "name": "Deli Zhao"
                    },
                    {
                        "authorId": "143962510",
                        "name": "Zhengjun Zha"
                    },
                    {
                        "authorId": "1709595",
                        "name": "Jingren Zhou"
                    },
                    {
                        "authorId": "143832240",
                        "name": "Qifeng Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Semantic editing is the ability to perform image editing operations on images by manipulating their latent space [11, 34, 39].",
                "Images can be interpolated and transformed using semantic vectors in the embedding space [11, 34], effectively using it as a strong regularizer."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "814a86b5aea4e4224baf5460489549e51208cf1b",
                "externalIds": {
                    "ArXiv": "2106.03847",
                    "DBLP": "journals/corr/abs-2106-03847",
                    "DOI": "10.1007/978-3-031-20050-2_13",
                    "CorpusId": 235364033
                },
                "corpusId": 235364033,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/814a86b5aea4e4224baf5460489549e51208cf1b",
                "title": "GAN Cocktail: mixing GANs without dataset access",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2107086356",
                        "name": "Omri Avrahami"
                    },
                    {
                        "authorId": "70018371",
                        "name": "D. Lischinski"
                    },
                    {
                        "authorId": "2416503",
                        "name": "Ohad Fried"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For instance, GANspace [H\u00e4rk\u00f6nen et al. 2020] is able to extract linear directions from the StyleGAN latent space (W space) in an unsupervised fashion using Principal Component Analysis (PCA)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "127b814a0b9ab2831d2dee60cb8f979c401b1870",
                "externalIds": {
                    "DBLP": "journals/tog/ZhuAFW21",
                    "ArXiv": "2106.01505",
                    "DOI": "10.1145/3478513.3480537",
                    "CorpusId": 235313963
                },
                "corpusId": 235313963,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/127b814a0b9ab2831d2dee60cb8f979c401b1870",
                "title": "Barbershop",
                "abstract": "Seamlessly blending features from multiple images is extremely challenging because of complex relationships in lighting, geometry, and partial occlusion which cause coupling between different parts of the image. Even though recent work on GANs enables synthesis of realistic hair or faces, it remains difficult to combine them into a single, coherent, and plausible image rather than a disjointed set of image patches. We present a novel solution to image blending, particularly for the problem of hairstyle transfer, based on GAN-inversion. We propose a novel latent space for image blending which is better at preserving detail and encoding spatial information, and propose a new GAN-embedding algorithm which is able to slightly modify images to conform to a common segmentation mask. Our novel representation enables the transfer of the visual properties from multiple reference images including specific details such as moles and wrinkles, and because we do image blending in a latent-space we are able to synthesize images that are coherent. Our approach avoids blending artifacts present in other approaches and finds a globally consistent image. Our results demonstrate a significant improvement over the current state of the art in a user study, with users preferring our blending solution over 95 percent of the time. Source code for the new approach is available at https://zpdesu.github.io/Barbershop.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "100516590",
                        "name": "Peihao Zhu"
                    },
                    {
                        "authorId": "94395014",
                        "name": "Rameen Abdal"
                    },
                    {
                        "authorId": "145685385",
                        "name": "John C. Femiani"
                    },
                    {
                        "authorId": "1798011",
                        "name": "Peter Wonka"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We further compare with an unsupervised method [15] in Fig."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "59b8a2a1b88ce8dc15a6603f135f7b4d04820a55",
                "externalIds": {
                    "DBLP": "conf/cvpr/YangCWZSH21",
                    "DOI": "10.1109/CVPR46437.2021.01200",
                    "CorpusId": 235692142
                },
                "corpusId": 235692142,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/59b8a2a1b88ce8dc15a6603f135f7b4d04820a55",
                "title": "Discovering Interpretable Latent Space Directions of GANs Beyond Binary Attributes",
                "abstract": "Generative adversarial networks (GANs) learn to map noise latent vectors to high-fidelity image outputs. It is found that the input latent space shows semantic correlations with the output image space. Recent works aim to interpret the latent space and discover meaningful directions that correspond to human interpretable image transformations. However, these methods either rely on explicit scores of attributes (e.g., memorability) or are restricted to binary ones (e.g., gender), which largely limits the applicability of editing tasks, especially for free-form artistic tasks like style/anime editing. In this paper, we propose an adversarial method, AdvStyle, for discovering interpretable directions in the absence of well-labeled scores or binary attributes. In particular, the proposed adversarial method simultaneously optimizes the discovered directions and the attribute assessor using the target attribute data as positive samples, while the generated ones being negative. In this way, arbitrary attributes can be edited by collecting positive data only, and the proposed method learns a controllable representation enabling manipulation of non-binary attributes like anime styles and facial characteristics. Moreover, the proposed learning strategy attenuates the entanglement between attributes, such that multi-attribute manipulation can be easily achieved without any additional constraint. Furthermore, we reveal several interesting semantics with the involuntarily learned negative directions. Extensive experiments on 9 anime attributes and 7 human attributes demonstrate the effectiveness of our adversarial approach qualitatively and quantitatively. Code is available at https://github.com/BERYLSHEEP/AdvStyle.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2155586159",
                        "name": "Huiting Yang"
                    },
                    {
                        "authorId": "2034068458",
                        "name": "Liangyu Chai"
                    },
                    {
                        "authorId": "1491246783",
                        "name": "Q. Wen"
                    },
                    {
                        "authorId": "2111307626",
                        "name": "Shuang Zhao"
                    },
                    {
                        "authorId": "21072153",
                        "name": "Zixun Sun"
                    },
                    {
                        "authorId": "2115300590",
                        "name": "Shengfeng He"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "f26b6211d46a22907b1f292535378d1769d52a0a",
                "externalIds": {
                    "DBLP": "conf/cvpr/dApolitoPHRG21",
                    "DOI": "10.1109/CVPR46437.2021.00063",
                    "CorpusId": 235702555
                },
                "corpusId": 235702555,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f26b6211d46a22907b1f292535378d1769d52a0a",
                "title": "GANmut: Learning Interpretable Conditional Space for Gamut of Emotions",
                "abstract": "Humans can communicate emotions through a plethora of facial expressions, each with its own intensity, nuances and ambiguities. The generation of such variety by means of conditional GANs is limited to the expressions encoded in the used label system. These limitations are caused either due to burdensome labelling demand or the confounded label space. On the other hand, learning from inexpensive and intuitive basic categorical emotion labels leads to limited emotion variability. In this paper, we propose a novel GAN-based framework that learns an expressive and interpretable conditional space (usable as a label space) of emotions, instead of conditioning on handcrafted labels. Our framework only uses the categorical labels of basic emotions to learn jointly the conditional space as well as emotion manipulation. Such learning can benefit from the image variability within discrete labels, especially when the intrinsic labels reside beyond the discrete space of the defined. Our experiments demonstrate the effectiveness of the proposed framework, by allowing us to control and generate a gamut of complex and compound emotions while using only the basic categorical emotion labels during training. Our source code is available at https://github.com/stefanodapolito/GANmut.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2117335271",
                        "name": "Stefano d\u2019Apolito"
                    },
                    {
                        "authorId": "35268081",
                        "name": "D. Paudel"
                    },
                    {
                        "authorId": "145314891",
                        "name": "Zhiwu Huang"
                    },
                    {
                        "authorId": "145848547",
                        "name": "Andr\u00e9s Romero"
                    },
                    {
                        "authorId": "1681236",
                        "name": "L. Gool"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "3d62a84732d0f4d018dacb6e3d18a4cf8fbde174",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-14230",
                    "ArXiv": "2105.14230",
                    "DOI": "10.2139/ssrn.4170552",
                    "CorpusId": 235254637
                },
                "corpusId": 235254637,
                "publicationVenue": {
                    "id": "75d7a8c1-d871-42db-a8e4-7cf5146fdb62",
                    "name": "Social Science Research Network",
                    "type": "journal",
                    "alternate_names": [
                        "SSRN, Social Science Research Network (SSRN) home page",
                        "SSRN Electronic Journal",
                        "Soc Sci Res Netw",
                        "SSRN",
                        "SSRN Home Page",
                        "SSRN Electron J",
                        "Social Science Electronic Publishing presents Social Science Research Network"
                    ],
                    "issn": "1556-5068",
                    "url": "http://www.ssrn.com/",
                    "alternate_urls": [
                        "www.ssrn.com/",
                        "https://fatcat.wiki/container/tol7woxlqjeg5bmzadeg6qrg3e",
                        "https://www.wikidata.org/wiki/Q53949192",
                        "www.ssrn.com/en",
                        "http://www.ssrn.com/en/",
                        "http://umlib.nl/ssrn",
                        "umlib.nl/ssrn"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3d62a84732d0f4d018dacb6e3d18a4cf8fbde174",
                "title": "Transforming the Latent Space of StyleGAN for Real Face Editing",
                "abstract": "Despite recent advances in semantic manipulation using StyleGAN, semantic editing of real faces remains challenging. The gap between the $W$ space and the $W$+ space demands an undesirable trade-off between reconstruction quality and editing quality. To solve this problem, we propose to expand the latent space by replacing fully-connected layers in the StyleGAN's mapping network with attention-based transformers. This simple and effective technique integrates the aforementioned two spaces and transforms them into one new latent space called $W$++. Our modified StyleGAN maintains the state-of-the-art generation quality of the original StyleGAN with moderately better diversity. But more importantly, the proposed $W$++ space achieves superior performance in both reconstruction quality and editing quality. Despite these significant advantages, our $W$++ space supports existing inversion algorithms and editing methods with only negligible modifications thanks to its structural similarity with the $W/W$+ space. Extensive experiments on the FFHQ dataset prove that our proposed $W$++ space is evidently more preferable than the previous $W/W$+ space for real face editing. The code is publicly available for research purposes at https://github.com/AnonSubm2021/TransStyleGAN.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2153613845",
                        "name": "Heyi Li"
                    },
                    {
                        "authorId": "2108379899",
                        "name": "Jinlong Liu"
                    },
                    {
                        "authorId": "51193914",
                        "name": "Yunzhi Bai"
                    },
                    {
                        "authorId": "2109230200",
                        "name": "Huayan Wang"
                    },
                    {
                        "authorId": "3367666",
                        "name": "K. Mueller"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Each person was randomly assigned 18 groups of data and asked to choose all of the results they are satisfied with according to three criteria: the result looks natural, the primal attribute is\n1For InterfaceGAN, GANSpace and our method, we firstly embed the given images into the W+ latent space of StyleGAN using [Abdal et al., 2019] and then edit them.\nwell changed, and condition attribute is well preserved.",
                "Another set of works [Shen et al., 2020; Voynov and Babenko, 2020; Shen and Zhou, 2020; Plumerault et al., 2020; Ha\u0308rko\u0308nen et al., 2020; Goetschalckx et al., 2019] propose to edit face attribute by moving the latent code along a specific semantic direction in the latent space of well-trained\u2026",
                "We compare our IALS method with several state-of-the-art face attribute editing method proposed recently, including InterfaceGAN [Shen et al., 2020], GANSpace [Ha\u0308rko\u0308nen et al., 2020], and STGAN [Liu et al., 2019].",
                "Both InterfaceGAN and GANSpace are methods based on GAN latent space search.",
                "Another set of works [Shen et al., 2020; Voynov and Babenko, 2020; Shen and Zhou, 2020; Plumerault et al., 2020; H\u00e4rk\u00f6nen et al., 2020; Goetschalckx et al., 2019] propose to edit face attribute by moving the latent code along a specific semantic direction in the latent space of well-trained unconditional GAN model [Karras et al.",
                "[H\u00e4rk\u00f6nen et al., 2020] sample a collection of latent codes and perform PCA on them to find principle semantic directions.",
                ", 2020], GANSpace [H\u00e4rk\u00f6nen et al., 2020], and STGAN [Liu et al.",
                "InterfaceGAN is a supervised semantic direction search method as mentioned in the previous section, while GANSpace is an unsupervised one which performs PCA on the sampled latent codes to find principle semantic directions in the latent space.",
                "Another set of works [Shen et al., 2020; Voynov and Babenko, 2020; Shen and Zhou, 2020; Plumerault et al., 2020; Ha\u0308rko\u0308nen et al., 2020; Goetschalckx et al., 2019] propose to edit face attribute by moving the latent code along a specific semantic direction in the latent space of well-trained unconditional GAN model [Karras et al., 2018; Karras et al., 2019; Karras et al., 2020; Goodfellow et al., 2014].",
                "On the other hand, the condition attributes (i.e. eyeglasses) are changed by GANSpace and InterfaceGAN.",
                "Ha\u0308rko\u0308nen et al. [Ha\u0308rko\u0308nen et al., 2020] sample a collection of latent codes and perform PCA on them to find principle semantic directions.",
                "7 show that our method obtained the highest average satisfactory rate (69.66% for ours vs. 62.06% for InterfaceGAN, 35.44% for GANSpace and 6.48% for STGAN).",
                "This motivates several works [Shen et al., 2020; Voynov and Babenko, 2020; Shen and Zhou, 2020; H\u00e4rk\u00f6nen et al., 2020] to edit face attribute by reusing the knowledge learned by GAN models.",
                "This motivates several works [Shen et al., 2020; Voynov and Babenko, 2020; Shen and Zhou, 2020; Ha\u0308rko\u0308nen et al., 2020] to edit face attribute by reusing the knowledge learned by GAN models.",
                "InterfaceGAN and GANSpace only use attribute-level direction while ignoring the instance information when facial editing.",
                "We assign the directions found by GANSpace to interpretable meanings following [Shen and Zhou, 2020]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4cb1979846e6701f9b7a7f1bfed447ca3aaaae69",
                "externalIds": {
                    "ArXiv": "2105.12660",
                    "DBLP": "conf/ijcai/HanY021",
                    "DOI": "10.24963/ijcai.2021/99",
                    "CorpusId": 235195745
                },
                "corpusId": 235195745,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/4cb1979846e6701f9b7a7f1bfed447ca3aaaae69",
                "title": "Disentangled Face Attribute Editing via Instance-Aware Latent Space Search",
                "abstract": "Recent works have shown that a rich set of semantic directions exist in the latent space of Generative Adversarial Networks (GANs), which enables various facial attribute editing applications. However, existing methods may suffer poor attribute variation disentanglement, leading to unwanted change of other attributes when altering the desired one. The semantic directions used by existing methods are at attribute level, which are difficult to model complex attribute correlations, especially in the presence of attribute distribution bias in GAN's training set. In this paper, we propose a novel framework (IALS) that performs Instance-Aware Latent-Space Search to find semantic directions for disentangled attribute editing. The instance information is injected by leveraging the supervision from a set of attribute classifiers evaluated on the input images. We further propose a Disentanglement-Transformation (DT) metric to quantify the attribute transformation and disentanglement efficacy and find the optimal control factor between attribute-level and instance-specific directions based on it. Experimental results on both GAN-generated and real-world images collectively show that our method outperforms state-of-the-art methods proposed recently by a wide margin. Code is available at https://github.com/yxuhan/IALS.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Yuxuan Han"
                    },
                    {
                        "authorId": "2109732576",
                        "name": "Jiaolong Yang"
                    },
                    {
                        "authorId": "143728560",
                        "name": "Ying Fu"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", [Abdal et al. 2020; H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020; Tewari et al. 2020]) have tried to analyze and disentangle the latent code of some pretrained GAN space [Karras et al.",
                "Many other works (e.g., [Abdal et al. 2020; H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020; Tewari et al. 2020]) have tried to analyze and disentangle the latent code of some pretrained GAN space [Karras et al. 2019] also with labeled data of specific attributes."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e21801c576ab442aeeb1aa8e822c2e423b52d750",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-08935",
                    "ArXiv": "2105.08935",
                    "DOI": "10.1145/3476576.3476648",
                    "CorpusId": 234777805
                },
                "corpusId": 234777805,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e21801c576ab442aeeb1aa8e822c2e423b52d750",
                "title": "DeepFaceEditing",
                "abstract": "Recent facial image synthesis methods have been mainly based on conditional generative models. Sketch-based conditions can effectively describe the geometry of faces, including the contours of facial components, hair structures, as well as salient edges (e.g., wrinkles) on face surfaces but lack effective control of appearance, which is influenced by color, material, lighting condition, etc. To have more control of generated results, one possible approach is to apply existing disentangling works to disentangle face images into geometry and appearance representations. However, existing disentangling methods are not optimized for human face editing, and cannot achieve fine control of facial details such as wrinkles. To address this issue, we propose DeepFaceEditing, a structured disentanglement framework specifically designed for face images to support face generation and editing with disentangled control of geometry and appearance. We adopt a local-to-global approach to incorporate the face domain knowledge: local component images are decomposed into geometry and appearance representations, which are fused consistently using a global fusion module to improve generation quality. We exploit sketches to assist in extracting a better geometry representation, which also supports intuitive geometry editing via sketching. The resulting method can either extract the geometry and appearance representations from face images, or directly extract the geometry representation from face sketches. Such representations allow users to easily edit and synthesize face images, with decoupled control of their geometry and appearance. Both qualitative and quantitative evaluations show the superior detail and appearance control abilities of our method compared to state-of-the-art methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2999411",
                        "name": "Shu-Yu Chen"
                    },
                    {
                        "authorId": "2152943384",
                        "name": "Feng-Lin Liu"
                    },
                    {
                        "authorId": "7827503",
                        "name": "Yu-Kun Lai"
                    },
                    {
                        "authorId": "1734823",
                        "name": "Paul L. Rosin"
                    },
                    {
                        "authorId": "2525637",
                        "name": "Chunpeng Li"
                    },
                    {
                        "authorId": "3169698",
                        "name": "Hongbo Fu"
                    },
                    {
                        "authorId": "144614914",
                        "name": "Lin Gao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As for the nonlinear mapping network, it comprises repeated downsampling, convolution, batchnorm, and leakyReLU layers until the feature map can be pooled as a vector, i.e., l \u2208 R1\u00d7512.",
                "As for the latent space, early methods perform image inversion into W \u2208 R1\u00d7512 [42, 19, 17], while later works [1, 2, 11, 10] extend the latent space to W \u2208 R18\u00d7512, which proves to have better reconstruction results.",
                "Secondly, to modify face representations and resolve the problem of previous latent code manipulation methods [43, 17, 48, 51, 50, 7, 37, 3] that only one attribute can be modified once a time, a novel swapping module, Face Transfer Module (FTM), is proposed to control multiple attributes synchronously without explicit feature disentanglement.",
                "Finally, the transferred latent code ls2t \u2208 Ls2t can be predicted as\nls2t = \u03c3(\u03c9)l\u0302 high t + (1\u2212 \u03c3(\u03c9))l\u0302highs (3)\nwhere \u03c9 \u2208 R1\u00d7512 is a trainable weight vector, and \u03c3 stands for the sigmoid activation.",
                "As for the latent space, early methods perform image inversion into W \u2208 R1\u00d7512 [42, 19, 17], while later works [1, 2, 11, 10] extend the latent space to W+ \u2208 R18\u00d7512, which proves to have better reconstruction results.",
                "Previous methods [17, 51, 37, 43] have found good controllability of StyleGAN based on the assumption that semantic directions in StyleGAN latent space are linear."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1564ee7e4d0d17177b4c199a22ba9783e16a785e",
                "externalIds": {
                    "ArXiv": "2105.04932",
                    "DBLP": "journals/corr/abs-2105-04932",
                    "DOI": "10.1109/CVPR46437.2021.00480",
                    "CorpusId": 234357549
                },
                "corpusId": 234357549,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1564ee7e4d0d17177b4c199a22ba9783e16a785e",
                "title": "One Shot Face Swapping on Megapixels",
                "abstract": "Face swapping has both positive applications such as entertainment, human-computer interaction, etc., and negative applications such as DeepFake threats to politics, economics, etc. Nevertheless, it is necessary to understand the scheme of advanced methods for high-quality face swapping and generate enough and representative face swapping images to train DeepFake detection algorithms. This paper proposes the first Megapixel level method for one shot Face Swapping (or MegaFS for short). Firstly, MegaFS organizes face representation hierarchically by the proposed Hierarchical Representation Face Encoder (HieRFE) in an extended latent space to maintain more facial details, rather than compressed representation in previous face swapping methods. Secondly, a carefully designed Face Transfer Module (FTM) is proposed to transfer the identity from a source image to the target by a non-linear trajectory without explicit feature disentanglement. Finally, the swapped faces can be synthesized by StyleGAN2 with the benefits of its training stability and powerful generative capability. Each part of MegaFS can be trained separately so the requirement of our model for GPU memory can be satisfied for megapixel face swapping. In summary, complete face representation, stable training, and limited memory usage are the three novel contributions to the success of our method. Extensive experiments demonstrate the superiority of MegaFS and the first megapixel level face swapping database is released for research on DeepFake detection and face image editing in the public domain.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2140156511",
                        "name": "Yuhao Zhu"
                    },
                    {
                        "authorId": "2118912249",
                        "name": "Qi Li"
                    },
                    {
                        "authorId": "2152766452",
                        "name": "Jian Wang"
                    },
                    {
                        "authorId": "2153079122",
                        "name": "Chengzhong Xu"
                    },
                    {
                        "authorId": "1757186",
                        "name": "Zhenan Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Layerwise manipulation of the GANs are then performed to produce edits in the input image that are interpretable in terms of chosen semantic features [16].",
                "The principal components of activation tensors on the first few layers of the generator represent important factors of variation ([16])."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "6ca952211e518938cd415ace627ebbb436693941",
                "externalIds": {
                    "ArXiv": "2105.04070",
                    "CorpusId": 234336519
                },
                "corpusId": 234336519,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6ca952211e518938cd415ace627ebbb436693941",
                "title": "Robust Training Using Natural Transformation",
                "abstract": "Previous robustness approaches for deep learning models such as data augmentation techniques via data transformation or adversarial training cannot capture real-world variations that preserve the semantics of the input, such as a change in lighting conditions. To bridge this gap, we present NaTra, an adversarial training scheme that is designed to improve the robustness of image classification algorithms. We target attributes of the input images that are independent of the class identification, and manipulate those attributes to mimic real-world natural transformations (NaTra) of the inputs, which are then used to augment the training dataset of the image classifier. Specifically, we apply \\textit{Batch Inverse Encoding and Shifting} to map a batch of given images to corresponding disentangled latent codes of well-trained generative models. \\textit{Latent Codes Expansion} is used to boost image reconstruction quality through the incorporation of extended feature maps. \\textit{Unsupervised Attribute Directing and Manipulation} enables identification of the latent directions that correspond to specific attribute changes, and then produce interpretable manipulations of those attributes, thereby generating natural transformations to the input data. We demonstrate the efficacy of our scheme by utilizing the disentangled latent representations derived from well-trained GANs to mimic transformations of an image that are similar to real-world natural variations (such as lighting conditions or hairstyle), and train models to be invariant to these natural transformations. Extensive experiments show that our method improves generalization of classification models and increases its robustness to various real-world distortions",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2117010331",
                        "name": "Shuo Wang"
                    },
                    {
                        "authorId": "3366777",
                        "name": "L. Lyu"
                    },
                    {
                        "authorId": "1681657",
                        "name": "S. Nepal"
                    },
                    {
                        "authorId": "144031685",
                        "name": "C. Rudolph"
                    },
                    {
                        "authorId": "1803868",
                        "name": "M. Grobler"
                    },
                    {
                        "authorId": "2087142588",
                        "name": "Kristen Moore"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent research [3, 10, 13, 18, 30, 35] has proposed a number of techniques for interactive GAN image generation, which all could",
                "Techniques for interactive GAN image generation [3, 10, 13, 18, 30, 35] enable manual creation of specifc images, but not sampling of diverse, high-quality images from a GAN.",
                "Such algorithmic approaches [13, 18] often search specifc parts of latent space of a GAN using limited number"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "1c9b419b8d28aaca4f2923f673a0286ab0140ec7",
                "externalIds": {
                    "DBLP": "conf/chi/Zhang021",
                    "DOI": "10.1145/3411764.3445714",
                    "CorpusId": 233987602
                },
                "corpusId": 233987602,
                "publicationVenue": {
                    "id": "b55b50b1-aae7-47a7-b042-8aecc930073d",
                    "name": "International Conference on Human Factors in Computing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "CHI",
                        "Int Conf Hum Factor Comput Syst",
                        "Human Factors in Computing Systems",
                        "Conference on Human Interface",
                        "Conf Hum Interface",
                        "Hum Factor Comput Syst"
                    ],
                    "url": "http://www.acm.org/sigchi/"
                },
                "url": "https://www.semanticscholar.org/paper/1c9b419b8d28aaca4f2923f673a0286ab0140ec7",
                "title": "Method for Exploring Generative Adversarial Networks (GANs) via Automatically Generated Image Galleries",
                "abstract": "Generative Adversarial Networks (GANs) can automatically generate quality images from learned model parameters. However, it remains challenging to explore and objectively assess the quality of all possible images generated using a GAN. Currently, model creators evaluate their GANs via tedious visual examination of generated images sampled from narrow prior probability distributions on model parameters. Here, we introduce an interactive method to explore and sample quality images from GANs. Our first two user studies showed that participants can use the tool to explore a GAN and select quality images. Our third user study showed that images sampled from a posterior probability distribution using a Markov Chain Monte Carlo (MCMC) method on parameters of images collected in our first study resulted in on average higher quality and more diverse images than existing baselines. Our work enables principled qualitative GAN exploration and evaluation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2089557481",
                        "name": "Enhao Zhang"
                    },
                    {
                        "authorId": "2997335",
                        "name": "Nikola Banovic"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "908ef3ca32fd15d2f714ada0497e9f1556ca4b84",
                "externalIds": {
                    "DBLP": "journals/cgf/JahanGK21",
                    "DOI": "10.1111/cgf.142619",
                    "CorpusId": 232119449
                },
                "corpusId": 232119449,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/908ef3ca32fd15d2f714ada0497e9f1556ca4b84",
                "title": "Semantics\u2010Guided Latent Space Exploration for Shape Generation",
                "abstract": "We introduce an approach to incorporate user guidance into shape generation approaches based on deep networks. Generative networks such as autoencoders and generative adversarial networks are trained to encode shapes into latent vectors, effectively learning a latent shape space that can be sampled for generating new shapes. Our main idea is to enable users to explore the shape space with the use of high\u2010level semantic keywords. Specifically, the user inputs a set of keywords that describe the general attributes of the shape to be generated, e.g., \u201cfour legs\u201d for a chair. Then, our method maps the keywords to a subspace of the latent space, where the subspace captures the shapes possessing the specified attributes. The user then explores only this subspace to search for shapes that satisfy the design goal, in a process similar to using a parametric shape model. Our exploratory approach allows users to model shapes at a high level without the need for advanced artistic skills, in contrast to existing methods that allow to guide the generation with sketching or partial modeling of a shape. Our technical contribution to enable this exploration\u2010based approach is the introduction of a label regression neural network coupled with shape encoder/decoder networks. The label regression network takes the user\u2010provided keywords and maps them to distributions in the latent space. We show that our method allows users to explore the shape space and generate a variety of shapes with selected high\u2010level attributes.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1411098813",
                        "name": "Tansin Jahan"
                    },
                    {
                        "authorId": "1515650006",
                        "name": "Yanran Guan"
                    },
                    {
                        "authorId": "3276873",
                        "name": "O. V. Kaick"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "3618e503068e5f0e4f17ad1557a9bd6692daea79",
                "externalIds": {
                    "ArXiv": "2104.15069",
                    "MAG": "3158432584",
                    "DBLP": "journals/corr/abs-2104-15069",
                    "CorpusId": 232275342
                },
                "corpusId": 232275342,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3618e503068e5f0e4f17ad1557a9bd6692daea79",
                "title": "A Good Image Generator Is What You Need for High-Resolution Video Synthesis",
                "abstract": "Image and video synthesis are closely related areas aiming at generating content from noise. While rapid progress has been demonstrated in improving image-based models to handle large resolutions, high-quality renderings, and wide variations in image content, achieving comparable video generation results remains problematic. We present a framework that leverages contemporary image generators to render high-resolution videos. We frame the video synthesis problem as discovering a trajectory in the latent space of a pre-trained and fixed image generator. Not only does such a framework render high-resolution videos, but it also is an order of magnitude more computationally efficient. We introduce a motion generator that discovers the desired trajectory, in which content and motion are disentangled. With such a representation, our framework allows for a broad range of applications, including content and motion manipulation. Furthermore, we introduce a new task, which we call cross-domain video synthesis, in which the image and motion generators are trained on disjoint datasets belonging to different domains. This allows for generating moving objects for which the desired video data is not available. Extensive experiments on various datasets demonstrate the advantages of our methods over existing video generation techniques. Code will be released at https://github.com/snap-research/MoCoGAN-HD.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144978989",
                        "name": "Yu Tian"
                    },
                    {
                        "authorId": "144139198",
                        "name": "Jian Ren"
                    },
                    {
                        "authorId": "1752091",
                        "name": "Menglei Chai"
                    },
                    {
                        "authorId": "38376240",
                        "name": "Kyle Olszewski"
                    },
                    {
                        "authorId": "144152346",
                        "name": "Xi Peng"
                    },
                    {
                        "authorId": "1711560",
                        "name": "Dimitris N. Metaxas"
                    },
                    {
                        "authorId": "145582202",
                        "name": "S. Tulyakov"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent studies have shown that GANs naturally learn to encode rich semantics within the latent space, thus changing the latent code leads to manipulating the corresponding attributes of the output images [22, 42, 17, 15, 43, 3, 50, 5]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "85376095b9e1a6763192c288747b203996b8d427",
                "externalIds": {
                    "ArXiv": "2104.14754",
                    "DBLP": "conf/cvpr/KimCKYU21",
                    "DOI": "10.1109/CVPR46437.2021.00091",
                    "CorpusId": 233476599
                },
                "corpusId": 233476599,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/85376095b9e1a6763192c288747b203996b8d427",
                "title": "Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing",
                "abstract": "Generative adversarial networks (GANs) synthesize realistic images from random latent vectors. Although manipulating the latent vectors controls the synthesized outputs, editing real images with GANs suffers from i) time-consuming optimization for projecting real images to the latent vectors, ii) or inaccurate embedding through an encoder. We propose StyleMapGAN: the intermediate latent space has spatial dimensions, and a spatially variant modulation replaces AdaIN. It makes the embedding through an encoder more accurate than existing optimization-based methods while maintaining the properties of GANs. Experimental results demonstrate that our method significantly outperforms state-of-the-art models in various image manipulation tasks such as local editing and image interpolation. Last but not least, conventional editing methods on GANs are still valid on our StyleMapGAN. Source code is available at https://github.com/naver-ai/StyleMapGAN.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48206594",
                        "name": "Hyunsu Kim"
                    },
                    {
                        "authorId": "30187096",
                        "name": "Yunjey Choi"
                    },
                    {
                        "authorId": null,
                        "name": "Junho Kim"
                    },
                    {
                        "authorId": "1808405",
                        "name": "S. Yoo"
                    },
                    {
                        "authorId": "2847986",
                        "name": "Youngjung Uh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Alternatively, we also experiment with sampling according to principal directions; these directions were found to correspond well with interpretable controls in GANspace [21].",
                "eral interesting properties to emerge, where the generator learns meaningful variations in data without requiring an explicit training objective to do so [21, 53, 12].",
                "Previous work [1] finds that the intermediate w space is better able to represent images than the original code z, while moving in this space offers controllable and interesting effects [21].",
                "One set of approaches aims to uncover primary directions of variation in an intermediate latent space [21, 53, 12], while another enforces distinctness of optimized directions during training [58]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "8f9d1de9e1bd60783eb75fd80c42bf99ec67363a",
                "externalIds": {
                    "DBLP": "conf/cvpr/ChaiZSI021",
                    "ArXiv": "2104.14551",
                    "DOI": "10.1109/CVPR46437.2021.01475",
                    "CorpusId": 233444321
                },
                "corpusId": 233444321,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8f9d1de9e1bd60783eb75fd80c42bf99ec67363a",
                "title": "Ensembling with Deep Generative Views",
                "abstract": "Recent generative models can synthesize \"views\" of artificial images that mimic real-world variations, such as changes in color or pose, simply by learning from unlabeled image collections. Here, we investigate whether such views can be applied to real images to benefit downstream analysis tasks such as image classification. Using a pre-trained generator, we first find the latent code corresponding to a given real input image. Applying perturbations to the code creates natural variations of the image, which can then be ensembled together at test-time. We use StyleGAN2 as the source of generative augmentations and investigate this setup on classification tasks involving facial attributes, cat faces, and cars. Critically, we find that several design decisions are required towards making this process work; the perturbation procedure, weighting between the augmentations and original image, and training the classifier on synthesized images can all impact the result. Currently, we find that while test-time ensembling with GAN-based augmentations can offer some small improvements, the remaining bottlenecks are the efficiency and accuracy of the GAN reconstructions, coupled with classifier sensitivities to artifacts in GAN-generated images.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51322829",
                        "name": "Lucy Chai"
                    },
                    {
                        "authorId": "1922024303",
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "authorId": "2177801",
                        "name": "Eli Shechtman"
                    },
                    {
                        "authorId": "2094770",
                        "name": "Phillip Isola"
                    },
                    {
                        "authorId": "2109976035",
                        "name": "Richard Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Neural networks improve the fidelity and realism of generative models [14, 28] but limit control and interpretability [5, 6, 8, 16]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "15a7dfb7e17e534e106c4efddfd3287d9702f0dc",
                "externalIds": {
                    "ArXiv": "2104.14553",
                    "DBLP": "journals/corr/abs-2104-14553",
                    "CorpusId": 233443895
                },
                "corpusId": 233443895,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/15a7dfb7e17e534e106c4efddfd3287d9702f0dc",
                "title": "MarioNette: Self-Supervised Sprite Learning",
                "abstract": "Artists and video game designers often construct 2D animations using libraries of sprites -- textured patches of objects and characters. We propose a deep learning approach that decomposes sprite-based video animations into a disentangled representation of recurring graphic elements in a self-supervised manner. By jointly learning a dictionary of possibly transparent patches and training a network that places them onto a canvas, we deconstruct sprite-based content into a sparse, consistent, and explicit representation that can be easily used in downstream tasks, like editing or analysis. Our framework offers a promising approach for discovering recurring visual patterns in image collections without supervision.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144699082",
                        "name": "Dmitriy Smirnov"
                    },
                    {
                        "authorId": "3282136",
                        "name": "Micha\u00ebl Gharbi"
                    },
                    {
                        "authorId": "145002004",
                        "name": "Matthew Fisher"
                    },
                    {
                        "authorId": "9370721",
                        "name": "V. Guizilini"
                    },
                    {
                        "authorId": "1763086",
                        "name": "Alexei A. Efros"
                    },
                    {
                        "authorId": "1932072",
                        "name": "J. Solomon"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Principal component analysis (PCA) has been one of the effective tools for visualizing and analyzing embedded feature distribution (H\u00e4rk\u00f6nen et al. 2020).",
                "Principal component analysis (PCA) has been one of the effective tools for visualizing and analyzing embedded feature distribution (Ha\u0308rko\u0308nen et al. 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "48e02bfbb5d1b69deee0e324e824a1f310f86172",
                "externalIds": {
                    "DBLP": "conf/aaai/LeeS21",
                    "ArXiv": "2104.13561",
                    "DOI": "10.1609/aaai.v35i9.17009",
                    "CorpusId": 233423255
                },
                "corpusId": 233423255,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/48e02bfbb5d1b69deee0e324e824a1f310f86172",
                "title": "Interpretable Embedding Procedure Knowledge Transfer via Stacked Principal Component Analysis and Graph Neural Network",
                "abstract": "Knowledge distillation (KD) is one of the most useful techniques for light-weight neural networks. Although neural networks have a clear purpose of embedding datasets into the low-dimensional space, the existing knowledge was quite far\nfrom this purpose and provided only limited information. We argue that good knowledge should be able to interpret the embedding procedure. This paper proposes a method of generating interpretable embedding procedure (IEP) knowledge based on principal component analysis, and distilling it based on a message passing neural network. Experimental results show that the student network trained by the proposed KD method improves 2.28% in the CIFAR100 dataset, which is a higher performance than the state-of-the-art (SOTA) method. We also demonstrate that the embedding procedure knowledge is interpretable via visualization of the proposed KD process. The implemented code is available at https://github.com/sseung0703/IEPKT.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2117177611",
                        "name": "Seunghyun Lee"
                    },
                    {
                        "authorId": "10774886",
                        "name": "B. Song"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The above methods try to learn a GAN generator with explicit interpretable representations; in contrast, another class of methods, post-processing methods, try to reveal the interpretable factors from a well-trained GAN generator [9, 3, 35, 39, 32, 12, 38, 36].",
                "According to this property, one can identify semantic attributes from different layers of a well-trained generator by performing post-processing algorithms [3, 12, 36, 39], and then can manipulate these attributes on the synthesized images.",
                "[12] apply PCA to the feature space of the early layers, and the resulting principal components represent interpretable variations.",
                "Without introducing external supervision, several methods search interpretable factors in self-supervised [32] or unsupervised [12, 36] manners.",
                "Among these methods, [3, 35, 39, 12, 36] carefully investigate the semantics represented in different generator layers."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "23adffcc367ac585bbbaaa2fe430f881e79c756a",
                "externalIds": {
                    "DBLP": "conf/iccv/HeKS21",
                    "ArXiv": "2104.12476",
                    "DOI": "10.1109/ICCV48922.2021.01414",
                    "CorpusId": 233394581
                },
                "corpusId": 233394581,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/23adffcc367ac585bbbaaa2fe430f881e79c756a",
                "title": "EigenGAN: Layer-Wise Eigen-Learning for GANs",
                "abstract": "Recent studies on Generative Adversarial Network (GAN) reveal that different layers of a generative CNN hold different semantics of the synthesized images. However, few GAN models have explicit dimensions to control the semantic attributes represented in a specific layer. This paper proposes EigenGAN which is able to unsupervisedly mine interpretable and controllable dimensions from different generator layers. Specifically, EigenGAN embeds one linear subspace with orthogonal basis into each generator layer. Via generative adversarial training to learn a target distribution, these layer-wise subspaces automatically discover a set of \"eigen-dimensions\" at each layer corresponding to a set of semantic attributes or interpretable variations. By traversing the coefficient of a specific eigen-dimension, the generator can produce samples with continuous changes corresponding to a specific semantic attribute. Taking the human face for example, EigenGAN can discover controllable dimensions for high-level concepts such as pose and gender in the subspace of deep layers, as well as low-level concepts such as hue and color in the subspace of shallow layers. Moreover, in the linear case, we theoretically prove that our algorithm derives the principal components as PCA does. Codes can be found in https://github.com/LynnHo/EigenGAN-Tensorflow.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3469114",
                        "name": "Zhenliang He"
                    },
                    {
                        "authorId": "1693589",
                        "name": "Meina Kan"
                    },
                    {
                        "authorId": "145455919",
                        "name": "S. Shan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To overcome this limitation, some recent works [H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020; Shen and Zhou 2020] focus on the manipulation of the underlying learned latent space, using the vector arithmetic property observed in [Mikolov et al.",
                "Comparisons with Latent Space Manipulation Methods: We then compare our method with GANSpace and InterFaceGAN that perform semantic image control via latent space manipulation in Fig.",
                "TABLE 2 User Study Results\nMethod shape exp. illum. pose\nGANSpace 10.57 0.86 13.71 0.57 First Order - 4.29 - 2.86 InterFaceGAN - 2.86 - 2.00 MLS 0.57 0.57 - -",
                "Comparison results with GAN latent space manipulation methods: GANSpace (GANS.) and InterFaceGAN (Interface.).",
                "Since GANSpace is an unsupervised method without explicit supervision of the semantic attributes, it is difficult to locate the corresponding latent variable for a given attribute.",
                "In contrast, the noteworthy work GANSpace [H\u00e4rk\u00f6nen et al. 2020] can identify important latent directions for different attributes in an unsupervised fashion.",
                "Compared to GANSpace, InterFaceGAN allows explicit controls with supervision,\nFig.",
                "2019b] or latent space[H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020]), regarding both quality and controllability.",
                "In contrast, the noteworthy work GANSpace [15] can identify important latent directions for different attributes in an unsupervised fashion.",
                "To demonstrate our advantages in cross-domain face manipulation, we conduct comparisons with different types of representative baselines, including 2D warping based methods:Moving-Least-Squares (MLS) deformation [13] and First Order motion model [14], latent space manipulation based methods: GANSpace [15] and InterFaceGAN [16], and 3DMM guided methods: StyleRig [9] and PIE [10].",
                "2019b], latent space manipulation based methods: GANSpace [H\u00e4rk\u00f6nen et al. 2020] and InterFaceGAN [Shen et al."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "result",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3053f5fe0672197f241218b3208fdbff82257a5e",
                "externalIds": {
                    "ArXiv": "2104.11228",
                    "DBLP": "journals/corr/abs-2104-11228",
                    "DOI": "10.1109/TVCG.2021.3139913",
                    "CorpusId": 233346936,
                    "PubMed": "34982684"
                },
                "corpusId": 233346936,
                "publicationVenue": {
                    "id": "5e1f6444-5d03-48c7-b202-7f47d492aeae",
                    "name": "IEEE Transactions on Visualization and Computer Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Vis Comput Graph"
                    ],
                    "issn": "1077-2626",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=2945"
                },
                "url": "https://www.semanticscholar.org/paper/3053f5fe0672197f241218b3208fdbff82257a5e",
                "title": "Cross-Domain and Disentangled Face Manipulation With 3D Guidance",
                "abstract": "Face image manipulation via three-dimensional guidance has been widely applied in various interactive scenarios due to its semantically-meaningful understanding and user-friendly controllability. However, existing 3D-morphable-model-based manipulation methods are not directly applicable to out-of-domain faces, such as non-photorealistic paintings, cartoon portraits, or even animals, mainly due to the formidable difficulties in building the model for each specific face domain. To overcome this challenge, we propose, as far as we know, the first method to manipulate faces in arbitrary domains using human 3DMM. This is achieved through two major steps: 1) disentangled mapping from 3DMM parameters to the latent space embedding of a pre-trained StyleGAN2 [1] that guarantees disentangled and precise controls for each semantic attribute; and 2) cross-domain adaptation that bridges domain discrepancies and makes human 3DMM applicable to out-of-domain faces by enforcing a consistent latent space embedding. Experiments and comparisons demonstrate the superiority of our high-quality semantic manipulation method on a variety of face domains with all major 3D facial attributes controllable \u2013 pose, expression, shape, albedo, and illumination. Moreover, we develop an intuitive editing interface to support user-friendly control and instant feedback. Our project page is https://cassiepython.github.io/cddfm3d/index.html.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2144350104",
                        "name": "Can Wang"
                    },
                    {
                        "authorId": "1752091",
                        "name": "Menglei Chai"
                    },
                    {
                        "authorId": "4938340",
                        "name": "Mingming He"
                    },
                    {
                        "authorId": "49025801",
                        "name": "Dongdong Chen"
                    },
                    {
                        "authorId": "1510772128",
                        "name": "Jing Liao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We compare the proposed method SGF with two stateof-the-art latent space manipulation methods: InterfaceGAN [31] \u2020 and GANSpace [11] .",
                "On the other hand, unsupervised methods directly find semantically meaningful directions by PCA [11] or self-supervised learning [32].",
                "Existing methods can be categorized into two classes: supervised methods [31, 26, 8] and unsupervised methods [11, 32].",
                "To be specific, InterFaceGAN [31] and GANSpace [11] find meaningful directions in latent space, and vary latent codes along these directions to adjust the attributes of images."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "5f4c8a26d30b9a6a078c3f7433477f1c85cfb555",
                "externalIds": {
                    "DBLP": "conf/cvpr/LiJZ21",
                    "ArXiv": "2104.09065",
                    "DOI": "10.1109/CVPR46437.2021.00646",
                    "CorpusId": 233296707
                },
                "corpusId": 233296707,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5f4c8a26d30b9a6a078c3f7433477f1c85cfb555",
                "title": "Surrogate Gradient Field for Latent Space Manipulation",
                "abstract": "Generative adversarial networks (GANs) can generate high-quality images from sampled latent codes. Recent works attempt to edit an image by manipulating its under-lying latent code, but rarely go beyond the basic task of at-tribute adjustment. We propose the first method that enables manipulation with multidimensional condition such as key-points and captions. Specifically, we design an algorithm that searches for a new latent code that satisfies the target condition based on the Surrogate Gradient Field (SGF) induced by an auxiliary mapping network. For quantitative comparison, we propose a metric to evaluate the disentanglement of manipulation methods. Thorough experimental analysis on the facial attribute adjustment task shows that our method outperforms state-of-the-art methods in disentanglement. We further apply our method to tasks of various condition modalities to demonstrate that our method can alter complex image properties such as keypoints and captions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3700393",
                        "name": "Minjun Li"
                    },
                    {
                        "authorId": "2126155",
                        "name": "Yanghua Jin"
                    },
                    {
                        "authorId": "2115717844",
                        "name": "Huachun Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To control the GAN synthesis process, some works explore the semantic editing of latent codes by finding semantic directions on their latent space in a supervised [3] or unsupervised [39], [40] manner."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0ff34b0d8827dd0dd6b560dd9377371a94681a42",
                "externalIds": {
                    "ArXiv": "2104.07661",
                    "DBLP": "journals/tip/WeiCZLZYHY22",
                    "DOI": "10.1109/TIP.2022.3167305",
                    "CorpusId": 247763553,
                    "PubMed": "35439133"
                },
                "corpusId": 247763553,
                "publicationVenue": {
                    "id": "e117fa7f-05b7-4dd6-b64b-0a0a7c0393d8",
                    "name": "IEEE Transactions on Image Processing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Image Process"
                    ],
                    "issn": "1057-7149",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=83",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0ff34b0d8827dd0dd6b560dd9377371a94681a42",
                "title": "E2Style: Improve the Efficiency and Effectiveness of StyleGAN Inversion",
                "abstract": "This paper studies the problem of StyleGAN inversion, which plays an essential role in enabling the pretrained StyleGAN to be used for real image editing tasks. The goal of StyleGAN inversion is to find the exact latent code of the given image in the latent space of StyleGAN. This problem has a high demand for quality and efficiency. Existing optimization-based methods can produce high-quality results, but the optimization often takes a long time. On the contrary, forward-based methods are usually faster but the quality of their results is inferior. In this paper, we present a new feed-forward network \u201cE2Style\u201d for StyleGAN inversion, with significant improvement in terms of efficiency and effectiveness. In our inversion network, we introduce: 1) a shallower backbone with multiple efficient heads across scales; 2) multi-layer identity loss and multi-layer face parsing loss to the loss function; and 3) multi-stage refinement. Combining these designs together forms an effective and efficient method that exploits all benefits of optimization-based and forward-based methods. Quantitative and qualitative results show that our E2Style performs better than existing forward-based methods and comparably to state-of-the-art optimization-based methods while maintaining the high efficiency as well as forward-based methods. Moreover, a number of real image editing applications demonstrate the efficacy of our E2Style. Our code is available at https://github.com/wty-ustc/e2style",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3377943",
                        "name": "Tianyi Wei"
                    },
                    {
                        "authorId": "49025801",
                        "name": "Dongdong Chen"
                    },
                    {
                        "authorId": "47862527",
                        "name": "Wenbo Zhou"
                    },
                    {
                        "authorId": "1510772128",
                        "name": "Jing Liao"
                    },
                    {
                        "authorId": "51027868",
                        "name": "Weiming Zhang"
                    },
                    {
                        "authorId": "145347147",
                        "name": "Lu Yuan"
                    },
                    {
                        "authorId": "2058322032",
                        "name": "Gang Hua"
                    },
                    {
                        "authorId": "1708598",
                        "name": "Nenghai Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "entangling the latent space and adding more and more control [20, 11, 8, 27, 28, 29, 9, 13]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ad7fee29e84127e25a9b4cbc9fd8c9e371148c6d",
                "externalIds": {
                    "ArXiv": "2104.05988",
                    "DBLP": "conf/iccv/BuhlerMLBH21",
                    "DOI": "10.1109/ICCV48922.2021.01363",
                    "CorpusId": 233219749
                },
                "corpusId": 233219749,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/ad7fee29e84127e25a9b4cbc9fd8c9e371148c6d",
                "title": "VariTex: Variational Neural Face Textures",
                "abstract": "Deep generative models can synthesize photorealistic images of human faces with novel identities. However, a key challenge to the wide applicability of such techniques is to provide independent control over semantically meaningful parameters: appearance, head pose, face shape, and facial expressions. In this paper, we propose VariTex - to the best of our knowledge the first method that learns a variational latent feature space of neural face textures, which allows sampling of novel identities. We combine this generative model with a parametric face model and gain explicit control over head pose and facial expressions. To generate complete images of human heads, we propose an additive decoder that adds plausible details such as hair. A novel training scheme enforces a pose-independent latent space and in consequence, allows learning a one-to-many mapping between latent codes and pose-conditioned exterior regions. The resulting method can generate geometrically consistent images of novel identities under fine-grained control over head pose, face shape, and facial expressions. This facilitates a broad range of downstream tasks, like sampling novel identities, changing the head pose, expression transfer, and more.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2014999915",
                        "name": "Marcel C. B\u00fchler"
                    },
                    {
                        "authorId": "1953101",
                        "name": "Abhimitra Meka"
                    },
                    {
                        "authorId": "2108549588",
                        "name": "Gengyan Li"
                    },
                    {
                        "authorId": "2486770",
                        "name": "T. Beeler"
                    },
                    {
                        "authorId": "1466533438",
                        "name": "Otmar Hilliges"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[32] takes an unsupervised approach with a trainable reconstructor network and direction matrix which find directions that are easily distinguishable, and [13, 30] learn interpretable directions by performing principle component analysis (PCA) on samples in the latent space or on the learned weights that map the latent vector to the first convolutional layers."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a821751a6b77a6eca294b6373bfa69e72145698a",
                "externalIds": {
                    "ArXiv": "2104.05518",
                    "DBLP": "journals/corr/abs-2104-05518",
                    "CorpusId": 233210198
                },
                "corpusId": 233210198,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a821751a6b77a6eca294b6373bfa69e72145698a",
                "title": "Diamond in the rough: Improving image realism by traversing the GAN latent space",
                "abstract": "In just a few years, the photo-realism of images synthesized by Generative Adversarial Networks (GANs) has gone from somewhat reasonable to almost perfect largely by increasing the complexity of the networks, e.g., adding layers, intermediate latent spaces, style-transfer parameters, etc. This trajectory has led many of the state-of-the-art GANs to be inaccessibly large, disengaging many without large computational resources. Recognizing this, we explore a method for squeezing additional performance from existing, low-complexity GANs. Formally, we present an unsupervised method to find a direction in the latent space that aligns with improved photo-realism. Our approach leaves the network unchanged while enhancing the fidelity of the generated image. We use a simple generator inversion to find the direction in the latent space that results in the smallest change in the image space. Leveraging the learned structure of the latent space, we find moving in this direction corrects many image artifacts and brings the image into greater realism. We verify our findings qualitatively and quantitatively, showing an improvement in Frechet Inception Distance (FID) exists along our trajectory which surpasses the original GAN and other approaches including a supervised method. We expand further and provide an optimization method to automatically select latent vectors along the path that balance the variation and realism of samples. We apply our method to several diverse datasets and three architectures of varying complexity to illustrate the generalizability of our approach. By expanding the utility of low-complexity and existing networks, we hope to encourage the democratization of GANs.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2087094223",
                        "name": "Jeffrey Wen"
                    },
                    {
                        "authorId": "2072735201",
                        "name": "Fabian Benitez-Quiroz"
                    },
                    {
                        "authorId": "145972081",
                        "name": "Qianli Feng"
                    },
                    {
                        "authorId": "1384255355",
                        "name": "Aleix M. Martinez"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The direction is suggested for change of gender [18], yet the full-size model also changes the age significantly along the direction.",
                "Generative adversarial networks (GANs) [16] are the leading model for several crucial computer vision tasks like image generation [7, 30] and image editing [3, 4, 18, 47].",
                "GANSpace Editing.",
                "We further demonstrate the benefit of our content-aware compressed StyleGAN2 for editing tasks of style mixing, latent space image morphing, and a recent proposed tech-\nnique, GANSpace [18].",
                "We further deploy our compressed 1024px model for GANSpace [18] editing.",
                "We further deploy our compressed\n1024px model for GANSpace [18] editing.",
                "6, where we use the same latent code as in the original paper [18] and traverse it in the direction of the first principal component, u0."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "417a7050b7d025a2be2b0465aa0da27d9a6ca6c1",
                "externalIds": {
                    "ArXiv": "2104.02244",
                    "DBLP": "conf/cvpr/LiuSL0PK21",
                    "DOI": "10.1109/CVPR46437.2021.01198",
                    "CorpusId": 233033467
                },
                "corpusId": 233033467,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/417a7050b7d025a2be2b0465aa0da27d9a6ca6c1",
                "title": "Content-Aware GAN Compression",
                "abstract": "Generative adversarial networks (GANs), e.g., StyleGAN2, play a vital role in various image generation and synthesis tasks, yet their notoriously high computational cost hinders their efficient deployment on edge devices. Directly applying generic compression approaches yields poor results on GANs, which motivates a number of recent GAN compression works. While prior works mainly accelerate conditional GANs, e.g., pix2pix and Cycle-GAN, compressing state-of-the-art unconditional GANs has rarely been explored and is more challenging. In this paper, we propose novel approaches for unconditional GAN compression. We first introduce effective channel pruning and knowledge distillation schemes specialized for unconditional GANs. We then propose a novel content-aware method to guide the processes of both pruning and distillation. With content-awareness, we can effectively prune channels that are unimportant to the contents of interest, e.g., human faces, and focus our distillation on these regions, which significantly enhances the distillation quality. On StyleGAN2 and SN-GAN, we achieve a substantial improvement over the state-of-the-art compression method. Notably, we reduce the FLOPs of StyleGAN2 by 11\u00d7 with visually negligible image quality loss compared to the full-size model. More interestingly, when applied to various image manipulation tasks, our compressed model forms a smoother and better disentangled latent manifold, making it more effective for image editing.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Yuchen Liu"
                    },
                    {
                        "authorId": "2496409",
                        "name": "Zhixin Shu"
                    },
                    {
                        "authorId": "152998391",
                        "name": "Yijun Li"
                    },
                    {
                        "authorId": "145527707",
                        "name": "Zhe L. Lin"
                    },
                    {
                        "authorId": "2942259",
                        "name": "Federico Perazzi"
                    },
                    {
                        "authorId": "144410963",
                        "name": "S. Kung"
                    }
                ]
            }
        },
        {
            "contexts": [
                "On the other end of the supervision spectrum, several methods [16, 41, 42] find directions in a completely unsupervised manner.",
                ", StyleGAN [24, 25, 23], effectively encode semantic information in their latent spaces [16, 36, 21].",
                "To this end, we perform latent space manipulations [16, 36, 37] on the inverted latent codes to see if the embeddings are semantically meaningful.",
                "For performing the edits in the human facial domain we use InterFaceGAN [36], for the cars domain we use GANSpace [16], and for the horse domain we use SeFa [37]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "44c0446bb53e951cca8df07af91f1dea96045aea",
                "externalIds": {
                    "DBLP": "conf/iccv/AlalufPC21",
                    "ArXiv": "2104.02699",
                    "DOI": "10.1109/ICCV48922.2021.00664",
                    "CorpusId": 233033581
                },
                "corpusId": 233033581,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/44c0446bb53e951cca8df07af91f1dea96045aea",
                "title": "ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement",
                "abstract": "Recently, the power of unconditional image synthesis has significantly advanced through the use of Generative Adversarial Networks (GANs). The task of inverting an image into its corresponding latent code of the trained GAN is of utmost importance as it allows for the manipulation of real images, leveraging the rich semantics learned by the network. Recognizing the limitations of current inversion approaches, in this work we present a novel inversion scheme that extends current encoder-based inversion methods by introducing an iterative refinement mechanism. Instead of directly predicting the latent code of a given real image using a single pass, the encoder is tasked with predicting a residual with respect to the current estimate of the inverted latent code in a self-correcting manner. Our residual-based encoder, named ReStyle, attains improved accuracy compared to current state-of-the-art encoder-based methods with a negligible increase in inference time. We analyze the behavior of ReStyle to gain valuable insights into its iterative nature. We then evaluate the performance of our residual encoder and analyze its robustness compared to optimization-based inversion and state-of-the-art encoders. Code is available via our project page: https: //yuval-alaluf.github.io/restyle-encoder/",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1850630812",
                        "name": "Yuval Alaluf"
                    },
                    {
                        "authorId": "2819477",
                        "name": "Or Patashnik"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We believe that the closest methods to our work are Ganspace [7] and SeFa [22] methods which we extensively compare in Section 4.",
                "(b) A comparison of rotate, zoom and background change directions between our method and Ganspace [7].",
                "Ganspace [7] is a sampling-based unsupervised method that randomly samples latent vectors from the intermediate layers of BigGAN and StyleGAN models.",
                "Q4: How successful are the obtained directions comparing to other methods? We visually compare\u2020 the directions obtained by our method with Ganspace[7] using Husky class.",
                "As explained in Section 3, contrary to [7], our learning method can optionally consider only the effects of a selected subset of layers while finding directions.",
                "Similar to [7, 22, 26], we limit ourselves to the unsupervised setting, where we aim to identify such edit directions without any external supervision utilized in works such as [4, 21, 8].",
                "Utilizing Layer-wise Styles: The layer-wise structure of StyleGAN2 and BigGAN models can be used for finegrained editing, as pointed out by [7] where different semantics are controlled with different layer groups.",
                "Recently, several approaches are proposed to explore the structure of latent space in GANs in a more principled way [21, 7, 26, 8].",
                "We also compare our method to state-of-the-art unsupervised methods [7, 22]*, and run several qualitative and quantitative experiments to demonstrate the effectiveness of our approach.",
                "Figure 4: (a) Comparison of manipulation results on FFHQ dataset with Ganspace [7] and SeFa [22] methods."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "192fa7837ccf23957e0be306473c81f6959ac74b",
                "externalIds": {
                    "ArXiv": "2104.00820",
                    "DBLP": "journals/corr/abs-2104-00820",
                    "DOI": "10.1109/ICCV48922.2021.01400",
                    "CorpusId": 233004516
                },
                "corpusId": 233004516,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/192fa7837ccf23957e0be306473c81f6959ac74b",
                "title": "LatentCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions",
                "abstract": "Recent research has shown that it is possible to find interpretable directions in the latent spaces of pre-trained Generative Adversarial Networks (GANs). These directions enable controllable image generation and support a wide range of semantic editing operations, such as zoom or rotation. The discovery of such directions is often done in a supervised or semi-supervised manner and requires manual annotations which limits their use in practice. In comparison, unsupervised discovery allows finding subtle directions that are difficult to detect a priori. In this work, we propose a contrastive learning-based approach to discover semantic directions in the latent space of pre-trained GANs in a self-supervised manner. Our approach finds semantically meaningful dimensions compatible with state-of-the-art methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2046873472",
                        "name": "O\u011fuz Kaan Y\u00fcksel"
                    },
                    {
                        "authorId": "1395808197",
                        "name": "Enis Simsar"
                    },
                    {
                        "authorId": "2063799990",
                        "name": "Ezgi Gulperi Er"
                    },
                    {
                        "authorId": "3137679",
                        "name": "Pinar Yanardag"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Layered GANs Generative adversarial networks (GANs) [18] have been proposed and improved towards highly photorealistic image synthesis [26, 7, 27] and explored for disentangling factors of variation towards controllable generation [17, 46, 26, 41, 23, 49, 48, 22], efficient representation learning [12], and saving of human annotations [55, 54]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f8b090b895eb04e796382c8d8ed9a56c856b47d0",
                "externalIds": {
                    "DBLP": "conf/wacv/YangBZCJ22",
                    "ArXiv": "2104.00483",
                    "DOI": "10.1109/WACV51458.2022.00044",
                    "CorpusId": 244896601
                },
                "corpusId": 244896601,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/f8b090b895eb04e796382c8d8ed9a56c856b47d0",
                "title": "Learning Foreground-Background Segmentation from Improved Layered GANs",
                "abstract": "Deep learning approaches heavily rely on high-quality human supervision which is nonetheless expensive, time-consuming, and error-prone, especially for image segmentation task. In this paper, we propose a method to automatically synthesize paired photo-realistic images and segmentation masks for the use of training a foreground-background segmentation network. In particular, we learn a generative adversarial network that decomposes an image into foreground and background layers, and avoid trivial decompositions by maximizing mutual information between generated images and latent variables. The improved layered GANs can synthesize higher quality datasets from which segmentation networks of higher performance can be learned. Moreover, the segmentation networks are employed to stabilize the training of layered GANs in return, which are further alternately trained with Layered GANs. Experiments on a variety of single-object datasets show that our method achieves competitive generation quality and segmentation performance compared to related methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118809263",
                        "name": "Yu Yang"
                    },
                    {
                        "authorId": "2518212",
                        "name": "Hakan Bilen"
                    },
                    {
                        "authorId": "52230981",
                        "name": "Qiran Zou"
                    },
                    {
                        "authorId": "153776540",
                        "name": "Wing Yin Cheung"
                    },
                    {
                        "authorId": "3419565",
                        "name": "Xian-Wei Ji"
                    }
                ]
            }
        },
        {
            "contexts": [
                "many 1D inputs for a high D latent space) [22] vs."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ce020571af791987ba42b9be9943195c094e6850",
                "externalIds": {
                    "ArXiv": "2104.00358",
                    "DBLP": "journals/corr/abs-2104-00358",
                    "CorpusId": 232478535
                },
                "corpusId": 232478535,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ce020571af791987ba42b9be9943195c094e6850",
                "title": "Nine Potential Pitfalls when Designing Human-AI Co-Creative Systems",
                "abstract": "This position paper examines potential pitfalls on the way towards achieving human-AI co-creation with generative models in a way that is beneficial to the users' interests. In particular, we collected a set of nine potential pitfalls, based on the literature and our own experiences as researchers working at the intersection of HCI and AI. We illustrate each pitfall with examples and suggest ideas for addressing it. Reflecting on all pitfalls, we discuss and conclude with implications for future research directions. With this collection, we hope to contribute to a critical and constructive discussion on the roles of humans and AI in co-creative interactions, with an eye on related assumptions and potential side-effects for creative practices and beyond.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1768653",
                        "name": "Daniel Buschek"
                    },
                    {
                        "authorId": "3410124",
                        "name": "Lukas Mecke"
                    },
                    {
                        "authorId": "153451802",
                        "name": "Florian Lehmann"
                    },
                    {
                        "authorId": "2767073",
                        "name": "Hai Dang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "More recent work analyse the latent space of trained GANs by exploiting human-provided supervision [18, 51, 29] or selfsupervision [43, 26, 57], and mathematically analysing layer weights [54] or features [25], leading to discovery of substantial interpretable directions.",
                "Inspired by recent work on interpretable directions in GAN latent space [51, 26, 25, 57], we adopt the linear walk as the representation of T Z and model it with neural network \u03a6 as T Z z = z+\u03a6( ), where \u03a6 is implemented with Multiple Layer Perceptron (MLP) and jointly trained with G."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "c72126fe917c10e2bb19728575bddb027e59a4fa",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-00483",
                    "MAG": "3144337075",
                    "CorpusId": 232478554
                },
                "corpusId": 232478554,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c72126fe917c10e2bb19728575bddb027e59a4fa",
                "title": "Unsupervised Foreground-Background Segmentation with Equivariant Layered GANs",
                "abstract": "We propose an unsupervised foreground-background segmentation method via training a segmentation network on the synthetic pseudo segmentation dataset generated from GANs, which are trained from a collection of images without annotations to explicitly disentangle foreground and background. To efficiently generate foreground and background layers and overlay them to compose novel images, the construction of such GANs is fulfilled by our proposed Equivariant Layered GAN, whose improvement, compared to the precedented layered GAN, is embodied in the following two aspects. (1) The disentanglement of foreground and background is improved by extending the previous perturbation strategy and introducing private code recovery that reconstructs the private code of foreground from the composite image. (2) The latent space of the layered GANs is regularized by minimizing our proposed equivariance loss, resulting in interpretable latent codes and better disentanglement of foreground and background. Our methods are evaluated on unsupervised object segmentation datasets including Caltech-UCSD Birds and LSUN Car, achieving state-of-the-art performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118809263",
                        "name": "Yu Yang"
                    },
                    {
                        "authorId": "2518212",
                        "name": "Hakan Bilen"
                    },
                    {
                        "authorId": "52230981",
                        "name": "Qiran Zou"
                    },
                    {
                        "authorId": "153776540",
                        "name": "Wing Yin Cheung"
                    },
                    {
                        "authorId": "7807689",
                        "name": "Xiangyang Ji"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "aaa99de83292370a964fcaa51e6e866a96726bb2",
                "externalIds": {
                    "DBLP": "conf/iccv/PatashnikWSCL21",
                    "ArXiv": "2103.17249",
                    "DOI": "10.1109/ICCV48922.2021.00209",
                    "CorpusId": 232428282
                },
                "corpusId": 232428282,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/aaa99de83292370a964fcaa51e6e866a96726bb2",
                "title": "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery",
                "abstract": "Inspired by the ability of StyleGAN to generate highly realistic images in a variety of domains, much recent work has focused on understanding how to use the latent spaces of StyleGAN to manipulate generated and real images. However, discovering semantically meaningful latent manipulations typically involves painstaking human examination of the many degrees of freedom, or an annotated collection of images for each desired manipulation. In this work, we explore leveraging the power of recently introduced Contrastive Language-Image Pre-training (CLIP) models in order to develop a text-based interface for StyleGAN image manipulation that does not require such manual effort. We first introduce an optimization scheme that utilizes a CLIP-based loss to modify an input latent vector in response to a user-provided text prompt. Next, we describe a latent mapper that infers a text-guided latent manipulation step for a given input image, allowing faster and more stable text-based manipulation. Finally, we present a method for mapping text prompts to input-agnostic directions in StyleGAN\u2019s style space, enabling interactive text-driven image manipulation. Extensive results and comparisons demonstrate the effectiveness of our approaches.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2819477",
                        "name": "Or Patashnik"
                    },
                    {
                        "authorId": "34815981",
                        "name": "Zongze Wu"
                    },
                    {
                        "authorId": "2177801",
                        "name": "Eli Shechtman"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    },
                    {
                        "authorId": "70018371",
                        "name": "D. Lischinski"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", by modifying the training objective [7, 55] or network architecture [28, 29], or discovering factors of variation in latent spaces of pre-trained generative models [1, 12, 16, 19, 25, 60, 73]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "649895f9cda9fe48a9f2b44ef5fe10142b86fb03",
                "externalIds": {
                    "DBLP": "conf/3dim/NiemeyerG21",
                    "ArXiv": "2103.17269",
                    "DOI": "10.1109/3DV53792.2021.00103",
                    "CorpusId": 232428272
                },
                "corpusId": 232428272,
                "publicationVenue": {
                    "id": "4b02e809-1c26-4203-b9ba-311a418f664b",
                    "name": "International Conference on 3D Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf 3D Vis",
                        "3DV"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/649895f9cda9fe48a9f2b44ef5fe10142b86fb03",
                "title": "CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields",
                "abstract": "Tremendous progress in deep generative models has led to photorealistic image synthesis. While achieving compelling results, most approaches operate in the two-dimensional image domain, ignoring the three-dimensional nature of our world. Several recent works therefore propose generative models which are 3D-aware, i.e., scenes are modeled in 3D and then rendered differentiably to the image plane. While this leads to impressive 3D consistency, the camera needs to be modelled as well and we show in this work that these methods are sensitive to the choice of prior camera distributions. Current approaches assume fixed intrinsics and predefined priors over camera pose ranges, and parameter tuning is typically required for real-world data. If the data distribution is not matched, results degrade significantly. Our key hypothesis is that learning a camera generator jointly with the image generator leads to a more principled approach to 3D-aware image synthesis. Further, we propose to decompose the scene into a background and foreground model, leading to more efficient and disentangled scene representations. While training from raw, unposed image collections, we learn a 3D- and camera-aware generative model which faithfully recovers not only the image but also the camera data distribution. At test time, our model generates images with explicit control over the camera as well as the shape and appearance of the scene.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145048708",
                        "name": "Michael Niemeyer"
                    },
                    {
                        "authorId": "47237027",
                        "name": "Andreas Geiger"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Besides increasing the size of the dataset, recent methods to explore the model\u2019s latent space such as GANSpace [27] could be experimented with to achieve this, on top of fine-tuning or extending the base architecture."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "85010a8cabb9838dcb213fe57e2b8d8e5d53cb7f",
                "externalIds": {
                    "MAG": "3139697252",
                    "DOI": "10.3390/APP11073086",
                    "CorpusId": 233607940
                },
                "corpusId": 233607940,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/85010a8cabb9838dcb213fe57e2b8d8e5d53cb7f",
                "title": "Generative Adversarial Networks for Data Augmentation in Structural Adhesive Inspection",
                "abstract": "The technological advances brought forth by the Industry 4.0 paradigm have renewed the disruptive potential of artificial intelligence in the manufacturing sector, building the data-driven era on top of concepts such as Cyber\u2013Physical Systems and the Internet of Things. However, data availability remains a major challenge for the success of these solutions, particularly concerning those based on deep learning approaches. Specifically in the quality inspection of structural adhesive applications, found commonly in the automotive domain, defect data with sufficient variety, volume and quality is generally costly, time-consuming and inefficient to obtain, jeopardizing the viability of such approaches due to data scarcity. To mitigate this, we propose a novel approach to generate synthetic training data for this application, leveraging recent breakthroughs in training generative adversarial networks with limited data to improve the performance of automated inspection methods based on deep learning, especially for imbalanced datasets. Preliminary results in a real automotive pilot cell show promise in this direction, with the approach being able to generate realistic adhesive bead images and consequently object detection models showing improved mean average precision at different thresholds when trained on the augmented dataset. For reproducibility purposes, the model weights, configurations and data encompassed in this study are made publicly available.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2344643",
                        "name": "Ricardo Silva Peres"
                    },
                    {
                        "authorId": "2087468653",
                        "name": "Miguel Azevedo"
                    },
                    {
                        "authorId": "2087939909",
                        "name": "Sara Oleiro Ara\u00fajo"
                    },
                    {
                        "authorId": "49710422",
                        "name": "Magno Guedes"
                    },
                    {
                        "authorId": "2068995631",
                        "name": "F\u00e1bio Miranda"
                    },
                    {
                        "authorId": "2088082307",
                        "name": "Jos\u00e9 Barata"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In contrast to other style code based editing methods [31, 12, 6], our diagonal attention maps are shown to have a clear and intuitive relationship to different spatial regions."
            ],
            "isInfluential": false,
            "intents": [
                "result"
            ],
            "citingPaper": {
                "paperId": "817e6223546f13224c4dcb8670a9aefbf2daecb8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-16146",
                    "ArXiv": "2103.16146",
                    "DOI": "10.1109/ICCV48922.2021.01372",
                    "CorpusId": 232417395
                },
                "corpusId": 232417395,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/817e6223546f13224c4dcb8670a9aefbf2daecb8",
                "title": "Diagonal Attention and Style-based GAN for Content-Style Disentanglement in Image Generation and Translation",
                "abstract": "One of the important research topics in image generative models is to disentangle the spatial contents and styles for their separate control. Although StyleGAN can generate content feature vectors from random noises, the resulting spatial content control is primarily intended for minor spatial variations, and the disentanglement of global content and styles is by no means complete. Inspired by a mathematical understanding of normalization and attention, here we present a novel hierarchical adaptive Diagonal spatial ATtention (DAT) layers to separately manipulate the spatial contents from styles in a hierarchical manner. Using DAT and AdaIN, our method enables coarse-to-fine level disentanglement of spatial contents and styles. In addition, our generator can be easily integrated into the GAN inversion framework so that the content and style of translated images from multi-domain image translation tasks can be flexibly controlled. By using various datasets, we confirm that the proposed method not only outperforms the existing models in disentanglement scores, but also provides more flexible control over spatial features in the generated images.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "116153377",
                        "name": "Gihyun Kwon"
                    },
                    {
                        "authorId": "30547794",
                        "name": "Jong-Chul Ye"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We attempt to interpret the latent space of the model trained to synthesize all classes (setting B), following [13]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "5328d49fc8a222904a9491c408ba19504bb5f43f",
                "externalIds": {
                    "ArXiv": "2103.15627",
                    "DBLP": "conf/iccv/PavlloKHL21",
                    "DOI": "10.1109/iccv48922.2021.01362",
                    "CorpusId": 232404704
                },
                "corpusId": 232404704,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/5328d49fc8a222904a9491c408ba19504bb5f43f",
                "title": "Learning Generative Models of Textured 3D Meshes from Real-World Images",
                "abstract": "Recent advances in differentiable rendering have sparked an interest in learning generative models of textured 3D meshes from image collections. These models natively disentangle pose and appearance, enable downstream applications in computer graphics, and improve the ability of generative models to understand the concept of image formation. Although there has been prior work on learning such models from collections of 2D images, these approaches require a delicate pose estimation step that exploits annotated keypoints, thereby restricting their applicability to a few specific datasets. In this work, we propose a GAN framework for generating textured triangle meshes without relying on such annotations. We show that the performance of our approach is on par with prior work that relies on ground-truth keypoints, and more importantly, we demonstrate the generality of our method by setting new baselines on a larger set of categories from ImageNet\u2013for which keypoints are not available\u2013without any class-specific hyperparameter tuning. We release our code at https://github.com/dariopavllo/textured-3d-gan",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "41018093",
                        "name": "Dario Pavllo"
                    },
                    {
                        "authorId": "32602330",
                        "name": "Jonas K\u00f6hler"
                    },
                    {
                        "authorId": "153379696",
                        "name": "T. Hofmann"
                    },
                    {
                        "authorId": "40401747",
                        "name": "Aur\u00e9lien Lucchi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While state-of-the-art editing frameworks [3, 37, 18] achieve high quality fine grained edits using supervised and unsupervised approaches, background/ foreground aware embeddings and edits still pose a challenge.",
                "Some recent works in this domain [5, 6, 16, 18, 49, 3, 36, 2, 37] study the structure of the activation and latent space.",
                "While originally conjectured that GANs are merely great at memorizing the training data, recent work in GAN-based image editing [18, 3, 36, 42] demonstrates that GANs learn non-trivial semantic information about a class of objects, e."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "b68788a9ade569ba3b5a036e29ccbfc6fc7aba2d",
                "externalIds": {
                    "ArXiv": "2103.14968",
                    "DBLP": "conf/iccv/AbdalZMW21",
                    "DOI": "10.1109/ICCV48922.2021.01371",
                    "CorpusId": 232404685
                },
                "corpusId": 232404685,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/b68788a9ade569ba3b5a036e29ccbfc6fc7aba2d",
                "title": "Labels4Free: Unsupervised Segmentation using StyleGAN",
                "abstract": "We propose an unsupervised segmentation framework for StyleGAN generated objects. We build on two main observations. First, the features generated by StyleGAN hold valuable information that can be utilized towards training segmentation networks. Second, the foreground and background can often be treated to be largely independent and be swapped across images to produce plausible composited images. For our solution, we propose to augment the StyleGAN2 generator architecture with a segmentation branch and to split the generator into a foreground and background network. This enables us to generate soft segmentation masks for the foreground object in an unsupervised fashion. On multiple object classes, we report comparable results against state-of-the-art supervised segmentation networks, while against the best unsupervised segmentation approach we demonstrate a clear improvement, both in qualitative and quantitative metricsProject Page : https:/rameenabdal.github.io/Labels4Free",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "94395014",
                        "name": "Rameen Abdal"
                    },
                    {
                        "authorId": "100516590",
                        "name": "Peihao Zhu"
                    },
                    {
                        "authorId": "1710455",
                        "name": "N. Mitra"
                    },
                    {
                        "authorId": "1798011",
                        "name": "Peter Wonka"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There have been also many attempts to manipulate inverted codes in disentangled latent spaces [7, 15, 27, 28, 12]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "24dfd76a4de22dea0bbf780997b9756c83eadec9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-14877",
                    "ArXiv": "2103.14877",
                    "CorpusId": 232404541
                },
                "corpusId": 232404541,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/24dfd76a4de22dea0bbf780997b9756c83eadec9",
                "title": "Few-shot Semantic Image Synthesis Using StyleGAN Prior",
                "abstract": "This paper tackles a challenging problem of generating photorealistic images from semantic layouts in few-shot scenarios where annotated training pairs are hardly available but pixel-wise annotation is quite costly. We present a training strategy that performs pseudo labeling of semantic masks using the StyleGAN prior. Our key idea is to construct a simple mapping between the StyleGAN feature and each semantic class from a few examples of semantic masks. With such mappings, we can generate an unlimited number of pseudo semantic masks from random noise to train an encoder for controlling a pre-trained StyleGAN generator. Although the pseudo semantic masks might be too coarse for previous approaches that require pixel-aligned masks, our framework can synthesize high-quality images from not only dense semantic masks but also sparse inputs such as landmarks and scribbles. Qualitative and quantitative results with various datasets demonstrate improvement over previous approaches with respect to layout fidelity and visual quality in as few as one- or five-shot settings.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2420042",
                        "name": "Yuki Endo"
                    },
                    {
                        "authorId": "2504432",
                        "name": "Yoshihiro Kanamori"
                    }
                ]
            }
        },
        {
            "contexts": [
                "So recent work has developed unsupervised methods for identifying disentangled interpretable directions in these interior latents [17, 47]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c70e45892e40cb2b4587fd342fe735b0f16b6878",
                "externalIds": {
                    "ArXiv": "2103.10951",
                    "DBLP": "journals/corr/abs-2103-10951",
                    "CorpusId": 232290554
                },
                "corpusId": 232290554,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c70e45892e40cb2b4587fd342fe735b0f16b6878",
                "title": "Paint by Word",
                "abstract": "We investigate the problem of zero-shot semantic image painting. Instead of painting modifications into an image using only concrete colors or a finite set of semantic concepts, we ask how to create semantic paint based on open full-text descriptions: our goal is to be able to point to a location in a synthesized image and apply an arbitrary new concept such as\"rustic\"or\"opulent\"or\"happy dog.\"To do this, our method combines a state-of-the art generative model of realistic images with a state-of-the-art text-image semantic similarity network. We find that, to make large changes, it is important to use non-gradient methods to explore latent space, and it is important to relax the computations of the GAN to target changes to a specific region. We conduct user studies to compare our methods to several baselines.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144159726",
                        "name": "David Bau"
                    },
                    {
                        "authorId": "50112310",
                        "name": "A. Andonian"
                    },
                    {
                        "authorId": "2056068167",
                        "name": "Audrey Cui"
                    },
                    {
                        "authorId": "2110274184",
                        "name": "YeonHwan Park"
                    },
                    {
                        "authorId": "19203468",
                        "name": "Ali Jahanian"
                    },
                    {
                        "authorId": "143868587",
                        "name": "A. Oliva"
                    },
                    {
                        "authorId": "143805211",
                        "name": "A. Torralba"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While there are several other approaches which demonstrate transformations of StyleGAN latent vectors for semantic manipulation [Collins et al. 2020; H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020; Tewari et al. 2020a], these methods focus on StyleGAN generated images, and do not produce high-quality and high-resolution results for real existing",
                "While there are several other approaches which demonstrate transformations of StyleGAN latent vectors for semantic manipulation [Collins et al. 2020; H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020; Tewari et al. 2020a], these methods focus on StyleGAN generated images, and do not\nproduce high-quality and\u2026"
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "5761c76146be2e2fca2875445eb03d105846cfff",
                "externalIds": {
                    "ArXiv": "2103.07658",
                    "DBLP": "journals/tog/0001TDWBSPMCET21",
                    "DOI": "10.1145/3476576.3476589",
                    "CorpusId": 232233127
                },
                "corpusId": 232233127,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5761c76146be2e2fca2875445eb03d105846cfff",
                "title": "PhotoApp",
                "abstract": "Photorealistic editing of head portraits is a challenging task as humans are very sensitive to inconsistencies in faces. We present an approach for high-quality intuitive editing of the camera viewpoint and scene illumination (parameterised with an environment map) in a portrait image. This requires our method to capture and control the full reflectance field of the person in the image. Most editing approaches rely on supervised learning using training data captured with setups such as light and camera stages. Such datasets are expensive to acquire, not readily available and do not capture all the rich variations of in-the-wild portrait images. In addition, most supervised approaches only focus on relighting, and do not allow camera viewpoint editing. Thus, they only capture and control a subset of the reflectance field. Recently, portrait editing has been demonstrated by operating in the generative model space of StyleGAN. While such approaches do not require direct supervision, there is a significant loss of quality when compared to the supervised approaches. In this paper, we present a method which learns from limited supervised training data. The training images only include people in a fixed neutral expression with eyes closed, without much hair or background variations. Each person is captured under 150 one-light-at-a-time conditions and under 8 camera poses. Instead of training directly in the image space, we design a supervised problem which learns transformations in the latent space of StyleGAN. This combines the best of supervised learning and generative adversarial modeling. We show that the StyleGAN prior allows for generalisation to different expressions, hairstyles and backgrounds. This produces high-quality photorealistic results for in-the-wild images and significantly outperforms existing methods. Our approach can edit the illumination and pose simultaneously, and runs at interactive rates.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1413283221",
                        "name": "R. MallikarjunB."
                    },
                    {
                        "authorId": "9102722",
                        "name": "A. Tewari"
                    },
                    {
                        "authorId": "4303842",
                        "name": "Abdallah Dib"
                    },
                    {
                        "authorId": "1784306",
                        "name": "T. Weyrich"
                    },
                    {
                        "authorId": "3083909",
                        "name": "B. Bickel"
                    },
                    {
                        "authorId": "145156858",
                        "name": "H. Seidel"
                    },
                    {
                        "authorId": "143758236",
                        "name": "H. Pfister"
                    },
                    {
                        "authorId": "1752521",
                        "name": "W. Matusik"
                    },
                    {
                        "authorId": "39255836",
                        "name": "Louis Chevallier"
                    },
                    {
                        "authorId": "1854465",
                        "name": "Mohamed A. Elgharib"
                    },
                    {
                        "authorId": "1680185",
                        "name": "C. Theobalt"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several methods [65, 24] have been proposed to discover such latent directions that control certain aspects of the input (e.",
                "Several methods have been proposed, including choosing better or multiple layers to project and edit [1, 2, 19], fine-tuning network weights for each image [5], modeling image corruption and transformations [4, 35], and discovering meaningful latent directions [65, 16, 38, 24].",
                ", add smiling to a portrait) by tweaking the latent code [61, 38, 42, 24, 65]."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "43d9627fdf1436d827788cd95105112a57b53696",
                "externalIds": {
                    "ArXiv": "2103.03243",
                    "DBLP": "journals/corr/abs-2103-03243",
                    "DOI": "10.1109/CVPR46437.2021.01474",
                    "CorpusId": 232110729
                },
                "corpusId": 232110729,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/43d9627fdf1436d827788cd95105112a57b53696",
                "title": "Anycost GANs for Interactive Image Synthesis and Editing",
                "abstract": "Generative adversarial networks (GANs) have enabled photorealistic image synthesis and editing. However, due to the high computational cost of large-scale generators (e.g., StyleGAN2), it usually takes seconds to see the results of a single edit on edge devices, prohibiting interactive user experience. In this paper, inspired by quick preview features in modern rendering software, we propose Anycost GAN for interactive natural image editing. We train the Anycost GAN to support elastic resolutions and channels for faster image generation at versatile speeds. Running subsets of the full generator produce outputs that are perceptually similar to the full generator, making them a good proxy for quick preview. By using sampling-based multi-resolution training, adaptive-channel training, and a generator-conditioned discriminator, the anycost generator can be evaluated at various configurations while achieving better image quality compared to separately trained models. Furthermore, we develop new encoder training and latent code optimization techniques to encourage consistency between the different sub-generators during image projection. Anycost GAN can be executed at various cost budgets (up to 10\u00d7 computation reduction) and adapt to a wide range of hardware and la tency requirements. When deployed on desktop CPUs and edge devices, our model can provide perceptually similar previews at 6-12\u00d7 speedup, enabling interactive image editing. The ${\\color{RubineRed}{code}}$ and ${\\color{RubineRed}{demo}}$ are publicly available.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2110385919",
                        "name": "Ji Lin"
                    },
                    {
                        "authorId": "2844849",
                        "name": "Richard Zhang"
                    },
                    {
                        "authorId": "26385110",
                        "name": "F. Ganz"
                    },
                    {
                        "authorId": "143840275",
                        "name": "Song Han"
                    },
                    {
                        "authorId": "1922024303",
                        "name": "Jun-Yan Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similarly, generative models can be used to create novel views of images (Plumerault et al., 2020; Jahanian et al., 2019; H\u00e4rk\u00f6nen et al., 2020) by manipulating them in latent space.",
                "Similarly, generative models can be used to create novel views of images (Plumerault et al., 2020; Jahanian et al., 2019; Ha\u0308rko\u0308nen et al., 2020) by manipulating them in latent space."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "762752eb9a9a92b028026b17c46d50474ddf3f06",
                "externalIds": {
                    "ArXiv": "2103.01946",
                    "DBLP": "journals/corr/abs-2103-01946",
                    "CorpusId": 232092181
                },
                "corpusId": 232092181,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/762752eb9a9a92b028026b17c46d50474ddf3f06",
                "title": "Fixing Data Augmentation to Improve Adversarial Robustness",
                "abstract": "Adversarial training suffers from robust overfitting, a phenomenon where the robust test accuracy starts to decrease during training. In this paper, we focus on both heuristics-driven and data-driven augmentations as a means to reduce robust overfitting. First, we demonstrate that, contrary to previous findings, when combined with model weight averaging, data augmentation can significantly boost robust accuracy. Second, we explore how state-of-the-art generative models can be leveraged to artificially increase the size of the training set and further improve adversarial robustness. Finally, we evaluate our approach on CIFAR-10 against $\\ell_\\infty$ and $\\ell_2$ norm-bounded perturbations of size $\\epsilon = 8/255$ and $\\epsilon = 128/255$, respectively. We show large absolute improvements of +7.06% and +5.88% in robust accuracy compared to previous state-of-the-art methods. In particular, against $\\ell_\\infty$ norm-bounded perturbations of size $\\epsilon = 8/255$, our model reaches 64.20% robust accuracy without using any external data, beating most prior works that use external data.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "8478422",
                        "name": "Sylvestre-Alvise Rebuffi"
                    },
                    {
                        "authorId": "2071666",
                        "name": "Sven Gowal"
                    },
                    {
                        "authorId": "2792016",
                        "name": "D. A. Calian"
                    },
                    {
                        "authorId": "3205302",
                        "name": "Florian Stimberg"
                    },
                    {
                        "authorId": "8792285",
                        "name": "Olivia Wiles"
                    },
                    {
                        "authorId": "2554720",
                        "name": "Timothy A. Mann"
                    }
                ]
            }
        },
        {
            "contexts": [
                "H\u00e4rk\u00f6nen et al. (2020) search for important and meaningful directions by performing PCA in the style space of StyleGAN (Karras et al., 2019; 2020).",
                "For discovering-based methods, we consider serveral recent methods: GANspace (GS) (H\u00e4rk\u00f6nen et al., 2020), LatentDiscovery (LD) (Voynov & Babenko, 2020), ClosedForm (CF) (Shen & Zhou, 2021) and DeepSpectral (DS) (Khrulkov et al., 2021).",
                "For discovering-based methods, we consider serveral recent methods: GANspace (GS) (H\u00e4rk\u00f6nen et al., 2020), LatentDiscovery (LD) (Voynov & Babenko, 2020), ClosedForm (CF) (Shen & Zhou, 2021) and DeepSpectral (DS) (Khrulkov et al.",
                "Recent works (Shen & Zhou, 2021; Khrulkov et al., 2021; Karras et al., 2019; H\u00e4rk\u00f6nen et al., 2020; Voynov & Babenko, 2020) show that, for GANs purely trained for image generation, traversing along different directions in the latent space causes different variations of the generated image."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "77064160e80b260d044c91b44134df4de08e4c38",
                "externalIds": {
                    "ArXiv": "2102.10543",
                    "DBLP": "conf/iclr/RenYWZ22",
                    "CorpusId": 246823979
                },
                "corpusId": 246823979,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/77064160e80b260d044c91b44134df4de08e4c38",
                "title": "Learning Disentangled Representation by Exploiting Pretrained Generative Models: A Contrastive Learning View",
                "abstract": "From the intuitive notion of disentanglement, the image variations corresponding to different factors should be distinct from each other, and the disentangled representation should reflect those variations with separate dimensions. To discover the factors and learn disentangled representation, previous methods typically leverage an extra regularization term when learning to generate realistic images. However, the term usually results in a trade-off between disentanglement and generation quality. For the generative models pretrained without any disentanglement term, the generated images show semantically meaningful variations when traversing along different directions in the latent space. Based on this observation, we argue that it is possible to mitigate the trade-off by $(i)$ leveraging the pretrained generative models with high generation quality, $(ii)$ focusing on discovering the traversal directions as factors for disentangled representation learning. To achieve this, we propose Disentaglement via Contrast (DisCo) as a framework to model the variations based on the target disentangled representations, and contrast the variations to jointly discover disentangled directions and learn disentangled representations. DisCo achieves the state-of-the-art disentangled representation learning and distinct direction discovering, given pretrained non-disentangled generative models including GAN, VAE, and Flow. Source code is at https://github.com/xrenaa/DisCo.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1466503743",
                        "name": "Xuanchi Ren"
                    },
                    {
                        "authorId": "1958895984",
                        "name": "Tao Yang"
                    },
                    {
                        "authorId": "46393469",
                        "name": "Yuwang Wang"
                    },
                    {
                        "authorId": "152255579",
                        "name": "W. Zeng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Such learned latent spaces can contain subspaces with geometric properties, like viewpoint changes (H\u00e4rk\u00f6nen et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "46fe528531be22ab6913241dc27471caca547a51",
                "externalIds": {
                    "MAG": "3129710562",
                    "DBLP": "conf/icml/RematasMF21",
                    "ArXiv": "2102.08860",
                    "CorpusId": 231942605
                },
                "corpusId": 231942605,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/46fe528531be22ab6913241dc27471caca547a51",
                "title": "ShaRF: Shape-conditioned Radiance Fields from a Single View",
                "abstract": "We present a method for estimating neural scenes representations of objects given only a single image. The core of our method is the estimation of a geometric scaffold for the object and its use as a guide for the reconstruction of the underlying radiance field. Our formulation is based on a generative process that first maps a latent code to a voxelized shape, and then renders it to an image, with the object appearance being controlled by a second latent code. During inference, we optimize both the latent codes and the networks to fit a test image of a new object. The explicit disentanglement of shape and appearance allows our model to be fine-tuned given a single image. We can then render new views in a geometrically consistent manner and they represent faithfully the input object. Additionally, our method is able to generalize to images outside of the training domain (more realistic renderings and even real photographs). Finally, the inferred geometric scaffold is itself an accurate estimate of the object's 3D shape. We demonstrate in several experiments the effectiveness of our approach in both synthetic and real images.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3242262",
                        "name": "Konstantinos Rematas"
                    },
                    {
                        "authorId": "1401885873",
                        "name": "Ricardo Martin-Brualla"
                    },
                    {
                        "authorId": "143865718",
                        "name": "V. Ferrari"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our study is partially inspired by a very recent line of works on controllable generation (Voynov & Babenko, 2020; Shen & Zhou, 2020; H\u00e4rk\u00f6nen et al., 2020; Peebles et al., 2020), which explore the latent spaces of pretrained GANs and identify the latent directions useful for image editing.",
                "Our study is partially inspired by a very recent line of works on controllable generation (Voynov & Babenko, 2020; Shen & Zhou, 2020; Ha\u0308rko\u0308nen et al., 2020; Peebles et al., 2020), which explore the latent spaces of pretrained GANs and identify the latent directions useful for image editing.",
                "The discovery of directions that allow for interesting image manipulations is a nontrivial task, which, however, can be performed in an unsupervised manner surprisingly efficiently (Voynov & Babenko, 2020; Shen & Zhou, 2020;\nHa\u0308rko\u0308nen et al., 2020; Peebles et al., 2020).",
                "We consider several recently proposed methods: ClosedForm (Shen & Zhou, 2020), GANspace (Ha\u0308rko\u0308nen et al., 2020), LatentDiscovery (Voynov & Babenko, 2020).",
                "We consider several recently proposed methods: ClosedForm (Shen & Zhou, 2020), GANspace (H\u00e4rk\u00f6nen et al., 2020), LatentDiscovery (Voynov & Babenko, 2020).",
                "GANspace (GS)."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "5336fe502d440ee3c53c01f1fa46604154eba1a2",
                "externalIds": {
                    "ArXiv": "2102.06204",
                    "DBLP": "journals/corr/abs-2102-06204",
                    "CorpusId": 231879656
                },
                "corpusId": 231879656,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5336fe502d440ee3c53c01f1fa46604154eba1a2",
                "title": "Disentangled Representations from Non-Disentangled Models",
                "abstract": "Constructing disentangled representations is known to be a difficult task, especially in the unsupervised scenario. The dominating paradigm of unsupervised disentanglement is currently to train a generative model that separates different factors of variation in its latent space. This separation is typically enforced by training with specific regularization terms in the model's objective function. These terms, however, introduce additional hyperparameters responsible for the trade-off between disentanglement and generation quality. While tuning these hyperparameters is crucial for proper disentanglement, it is often unclear how to tune them without external supervision. This paper investigates an alternative route to disentangled representations. Namely, we propose to extract such representations from the state-of-the-art generative models trained without disentangling terms in their objectives. This paradigm of post hoc disentanglement employs little or no hyperparameters when learning representations while achieving results on par with existing state-of-the-art, as shown by comparison in terms of established disentanglement metrics, fairness, and the abstract reasoning task. All our code and models are publicly available.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "10662951",
                        "name": "Valentin Khrulkov"
                    },
                    {
                        "authorId": "66660622",
                        "name": "L. Mirvakhabova"
                    },
                    {
                        "authorId": "1738205",
                        "name": "I. Oseledets"
                    },
                    {
                        "authorId": "143743802",
                        "name": "Artem Babenko"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To this end, we follow our inversion method with several existing editing techniques: StyleFlow [3], InterFaceGAN [34], GANSpace [14], and SeFa [35].",
                "To this end, we follow our inversion with several existing editing techniques: StyleFlow [Abdal et al. 2020b], InterFaceGAN [Shen et al. 2020], GANSpace [H\u00e4rk\u00f6nen et al. 2020], and SeFa [Shen and Zhou 2020].",
                "For cars, we use directions obtained by GANSpace [14]; for faces we use StyleFlow [3]; and for horses, cats, and churches we use SeFa [35].",
                "H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020; Tewari et al. 2020a; Wu et al. 2020], which allow one to perform extensive image manipulations by leveraging a pretrained StyleGAN.",
                "In Figure 1, we show inversions obtained by our encoder in multiple domains, followed by several manipulations performed using various editing methods [Abdal et al. 2020b; H\u00e4rk\u00f6nen et al. 2020; Shen and Zhou 2020].",
                "For performing editing on the inversions, we use editing directions obtained by GANSpace [H\u00e4rk\u00f6nen et al. 2020].",
                "Finally, several methods [14, 38, 39] find latent directions in an unsupervised manner and require manual annotations to determine the semantic meaning of each direction post hoc.",
                "Finally, several methods [H\u00e4rk\u00f6nen et al. 2020; Voynov and Babenko 2020; Wang and Ponce 2021] find latent directions in an unsupervised manner and require manual annotations to determine the semantic meaning of each direction post hoc.",
                "Furthermore, numerous works have demonstrated thatW has intriguing disentangled properties [7, 14, 34, 37, 40], which allow one to perform extensive image manipulations by leveraging a pretrained StyleGAN.",
                "For performing editing on the inversions, we use editing directions obtained by GANSpace [14].",
                "In Figure 1, we show inversions obtained by our encoder in multiple domains, followed by several manipulations performed using various editing methods [3, 14, 34, 35].",
                "For cars, we use directions obtained by GANSpace [H\u00e4rk\u00f6nen et al. 2020]; for faces we use StyleFlow [Abdal et al. 2020b]; and for horses, cats, and churches we use SeFa [Shen and Zhou 2020]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1e7e1a7ed075edfb87440b6b98f2a94a48f74a57",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-02766",
                    "ArXiv": "2102.02766",
                    "DOI": "10.1145/3476576.3476706",
                    "CorpusId": 231802331
                },
                "corpusId": 231802331,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1e7e1a7ed075edfb87440b6b98f2a94a48f74a57",
                "title": "Designing an encoder for StyleGAN image manipulation",
                "abstract": "Recently, there has been a surge of diverse methods for performing image editing by employing pre-trained unconditional generators. Applying these methods on real images, however, remains a challenge, as it necessarily requires the inversion of the images into their latent space. To successfully invert a real image, one needs to find a latent code that reconstructs the input image accurately, and more importantly, allows for its meaningful manipulation. In this paper, we carefully study the latent space of StyleGAN, the state-of-the-art unconditional generator. We identify and analyze the existence of a distortion-editability tradeoff and a distortion-perception tradeoff within the StyleGAN latent space. We then suggest two principles for designing encoders in a manner that allows one to control the proximity of the inversions to regions that StyleGAN was originally trained on. We present an encoder based on our two principles that is specifically designed for facilitating editing on real images by balancing these tradeoffs. By evaluating its performance qualitatively and quantitatively on numerous challenging domains, including cars and horses, we show that our inversion method, followed by common editing techniques, achieves superior real-image editing quality, with only a small reconstruction accuracy drop.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2047833213",
                        "name": "Omer Tov"
                    },
                    {
                        "authorId": "1850630812",
                        "name": "Yuval Alaluf"
                    },
                    {
                        "authorId": "1702992975",
                        "name": "Yotam Nitzan"
                    },
                    {
                        "authorId": "2819477",
                        "name": "Or Patashnik"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other work considered unsupervised methods (H\u00e4rk\u00f6nen et al., 2020; Voynov & Babenko, 2020) to discover interpretable latent space directions.",
                "These global directions\nare commonly used (Ha\u0308rko\u0308nen et al., 2020; Jahanian et al., 2019; Shen et al., 2019; Viazovetskyi et al., 2020).",
                "Other work considered unsupervised methods (Ha\u0308rko\u0308nen et al., 2020; Voynov & Babenko, 2020) to discover interpretable latent space directions.",
                "Unsupervised latent-space editing methods (Ha\u0308rko\u0308nen et al., 2020; Voynov & Babenko, 2020) are often less effective at providing semantically meaningful directions and all too often change image identity during an edit.",
                "Unsupervised latent-space editing methods (H\u00e4rk\u00f6nen et al., 2020; Voynov & Babenko, 2020) are often less effective at providing semantically meaningful directions and all too often change image identity during an edit.",
                "\u2026evaluate the proposed approach on two types of datasets: (i) face datasets \u2013 FFHQ (Karras et al., 2019a), CelebA (Liu et al., 2018) and CelebA-HQ (Karras et al., 2017), commonly used in prior work (Ha\u0308rko\u0308nen et al., 2020; Karras et al., 2017; 2019a;b; Shen et al., 2019;\nViazovetskyi et al., 2020).",
                ", 2017a) and discovering semantically meaningful directions in a GAN latent space (H\u00e4rk\u00f6nen et al., 2020; Jahanian et al., 2019; Plumerault et al., 2020; Shen et al., 2019; Voynov & Babenko, 2020).",
                "\u20262018; 2020; Isola et al., 2017; Lee et al., 2020; Wang et al., 2018; Wu et al., 2019; Zhu et al., 2017a) and discovering semantically meaningful directions in a GAN latent space (Ha\u0308rko\u0308nen et al., 2020; Jahanian et al., 2019; Plumerault et al., 2020; Shen et al., 2019; Voynov & Babenko, 2020)."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3cdac7e1a3904a9458f55694d1dc6e6374659a02",
                "externalIds": {
                    "ArXiv": "2102.01187",
                    "DBLP": "conf/iclr/ZhuangKS21",
                    "CorpusId": 231749857
                },
                "corpusId": 231749857,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3cdac7e1a3904a9458f55694d1dc6e6374659a02",
                "title": "Enjoy Your Editing: Controllable GANs for Image Editing via Latent Space Navigation",
                "abstract": "Controllable semantic image editing enables a user to change entire image attributes with few clicks, e.g., gradually making a summer scene look like it was taken in winter. Classic approaches for this task use a Generative Adversarial Net (GAN) to learn a latent space and suitable latent-space transformations. However, current approaches often suffer from attribute edits that are entangled, global image identity changes, and diminished photo-realism. To address these concerns, we learn multiple attribute transformations simultaneously, we integrate attribute regression into the training of transformation functions, apply a content loss and an adversarial loss that encourage the maintenance of image identity and photo-realism. We propose quantitative evaluation strategies for measuring controllable editing performance, unlike prior work which primarily focuses on qualitative evaluation. Our model permits better control for both single- and multiple-attribute editing, while also preserving image identity and realism during transformation. We provide empirical results for both real and synthetic images, highlighting that our model achieves state-of-the-art performance for targeted image manipulation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "72612482",
                        "name": "Peiye Zhuang"
                    },
                    {
                        "authorId": "143812875",
                        "name": "O. Koyejo"
                    },
                    {
                        "authorId": "2068227",
                        "name": "A. Schwing"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "1f67ec9be3318b1d02e94291f4bb01a1f0f2d280",
                "externalIds": {
                    "MAG": "3152673717",
                    "DBLP": "journals/tai/LiTWLNYL21",
                    "DOI": "10.1109/TAI.2021.3071642",
                    "CorpusId": 234847784
                },
                "corpusId": 234847784,
                "publicationVenue": {
                    "id": "3c27e831-750f-45bc-9914-2148a5259eba",
                    "name": "IEEE Transactions on Artificial Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Artif Intell"
                    ],
                    "issn": "2691-4581",
                    "url": "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9078688"
                },
                "url": "https://www.semanticscholar.org/paper/1f67ec9be3318b1d02e94291f4bb01a1f0f2d280",
                "title": "Interpreting the Latent Space of GANs via Measuring Decoupling",
                "abstract": "With the success of generative adversarial networks (GANs) on various real-world applications, the controllability and security of GANs have raised more and more concerns from the community. Specifically, understanding the latent space of GANs, i.e., obtaining the completely decoupled latent space, is essential for applications in some secure scenarios. At present, there is no quantitative method to measure the decoupling of latent space, which is not conducive to the development of the community. In this article, we propose two methods to measure the sensitivity of latent dimensions: one is a sequential intervention method, and the other is an optimization-based method that measures the sensitivity in both the value and the direction. With these two methods, the decoupling of latent space can be measured by the sparsity of the sensitivity vector obtained. The effectiveness of the proposed methods has been verified by experiments on the representative GANs. Code will be available at https://github.com/iceli1007/latent-analysis-of.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1510709850",
                        "name": "Ziqiang Li"
                    },
                    {
                        "authorId": "47600048",
                        "name": "Rentuo Tao"
                    },
                    {
                        "authorId": "2146042461",
                        "name": "Jie Wang"
                    },
                    {
                        "authorId": "2157479891",
                        "name": "Fu Li"
                    },
                    {
                        "authorId": "6030884",
                        "name": "Hongjing Niu"
                    },
                    {
                        "authorId": "1912258801",
                        "name": "Mingdao Yue"
                    },
                    {
                        "authorId": "2156071890",
                        "name": "Bin Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "methods [89], [124], [127], [128] aim to discover interpretable",
                "[128] create interpretable controls for image"
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a3ef5a321876738a6b257de5e1eebc4a8aa5b907",
                "externalIds": {
                    "DBLP": "journals/pami/XiaZYXZY23",
                    "ArXiv": "2101.05278",
                    "DOI": "10.1109/TPAMI.2022.3181070",
                    "CorpusId": 231603119,
                    "PubMed": "37022469"
                },
                "corpusId": 231603119,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a3ef5a321876738a6b257de5e1eebc4a8aa5b907",
                "title": "GAN Inversion: A Survey",
                "abstract": "GAN inversion aims to invert a given image back into the latent space of a pretrained GAN model so that the image can be faithfully reconstructed from the inverted code by the generator. As an emerging technique to bridge the real and fake image domains, GAN inversion plays an essential role in enabling pretrained GAN models, such as StyleGAN and BigGAN, for applications of real image editing. Moreover, GAN inversion interprets GAN's latent space and examines how realistic images can be generated. In this paper, we provide a survey of GAN inversion with a focus on its representative algorithms and its applications in image restoration and image manipulation. We further discuss the trends and challenges for future research. A curated list of GAN inversion methods, datasets, and other related information can be found at https://github.com/weihaox/awesome-gan-inversion.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50875615",
                        "name": "Weihao Xia"
                    },
                    {
                        "authorId": "2129519081",
                        "name": "Yulun Zhang"
                    },
                    {
                        "authorId": "3001727",
                        "name": "Yujiu Yang"
                    },
                    {
                        "authorId": "1891766",
                        "name": "Jing-Hao Xue"
                    },
                    {
                        "authorId": "145291669",
                        "name": "Bolei Zhou"
                    },
                    {
                        "authorId": "1715634",
                        "name": "Ming-Hsuan Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Relative control over image generation: A widely studied approach for controlling the generated images of GANs is by exploiting the inherent disentanglement properties of their latent space [26, 53, 22, 44, 7].",
                "[22] use principal component analysis (PCA) in latent space to identify directions that correspond to image attributes."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1ad6da856f5d76b4df35d52fec6d8fb48a8b4462",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2101-02477",
                    "ArXiv": "2101.02477",
                    "DOI": "10.1109/ICCV48922.2021.01382",
                    "CorpusId": 230799563
                },
                "corpusId": 230799563,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/1ad6da856f5d76b4df35d52fec6d8fb48a8b4462",
                "title": "GAN-Control: Explicitly Controllable GANs",
                "abstract": "We present a framework for training GANs with explicit control over generated facial images. We are able to control the generated image by settings exact attributes such as age, pose, expression, etc. Most approaches for manipulating GAN-generated images achieve partial control by leveraging the latent space disentanglement properties, obtained implicitly after standard GAN training. Such methods are able to change the relative intensity of certain attributes, but not explicitly set their values. Recently proposed methods, designed for explicit control over human faces, harness morphable 3D face models (3DMM) to allow fine-grained control capabilities in GANs. Unlike these methods, our control is not constrained to 3DMM parameters and is extendable beyond the domain of human faces. Using contrastive learning, we obtain GANs with an explicitly disentangled latent space. This disentanglement is utilized to train control-encoders mapping human-interpretable inputs to suitable latent vectors, thus allowing explicit control. In the domain of human faces we demonstrate control over identity, age, pose, expression, hair color and illumination. We also demonstrate control capabilities of our framework in the domains of painted portraits and dog image generation. We demonstrate that our approach achieves state-of-the-art performance both qualitatively and quantitatively.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1403733421",
                        "name": "Alon Shoshan"
                    },
                    {
                        "authorId": "8523974",
                        "name": "Nadav Bhonker"
                    },
                    {
                        "authorId": "2866310",
                        "name": "Igor Kviatkovsky"
                    },
                    {
                        "authorId": "70874887",
                        "name": "G. Medioni"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "03db82055bf039503c8111e997757f122aa5438d",
                "externalIds": {
                    "ArXiv": "2012.14261",
                    "DBLP": "journals/tetci/ZhangTLT21",
                    "DOI": "10.1109/TETCI.2021.3100641",
                    "CorpusId": 229678413
                },
                "corpusId": 229678413,
                "publicationVenue": {
                    "id": "544cddb9-1149-469a-8377-d8c34f08d8b1",
                    "name": "IEEE Transactions on Emerging Topics in Computational Intelligence",
                    "alternate_names": [
                        "IEEE Trans Emerg Top Comput Intell"
                    ],
                    "issn": "2471-285X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=7433297",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7433297"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/03db82055bf039503c8111e997757f122aa5438d",
                "title": "A Survey on Neural Network Interpretability",
                "abstract": "Along with the great success of deep neural networks, there is also growing concern about their black-box nature. The interpretability issue affects people's trust on deep learning systems. It is also related to many ethical problems, e.g., algorithmic discrimination. Moreover, interpretability is a desired property for deep networks to become powerful tools in other research fields, e.g., drug discovery and genomics. In this survey, we conduct a comprehensive review of the neural network interpretability research. We first clarify the definition of interpretability as it has been used in many different contexts. Then we elaborate on the importance of interpretability and propose a novel taxonomy organized along three dimensions: type of engagement (passive vs. active interpretation approaches), the type of explanation, and the focus (from local to global interpretability). This taxonomy provides a meaningful 3D view of distribution of papers from the relevant literature as two of the dimensions are not simply categorical but allow ordinal subcategories. Finally, we summarize the existing interpretability evaluation methods and suggest possible research directions inspired by our new taxonomy.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2153634964",
                        "name": "Yu Zhang"
                    },
                    {
                        "authorId": "4023505",
                        "name": "P. Ti\u0148o"
                    },
                    {
                        "authorId": "1732672",
                        "name": "A. Leonardis"
                    },
                    {
                        "authorId": "144994537",
                        "name": "K. Tang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANSpace [16] considers unsupervised identification of interpretable controls over image synthesis, showing that semantically meaningful directions can be found by applying PCA in the latent space of StyleGAN.",
                "Different from previous works [42, 21, 16, 43] that adopt linear editing in latent space with a predefined editing direction, our method seeks to control face attributes via a non-linear editing conditioned on the starting latent code."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "2ad96488aa03242a8ef83e97a1c45632dd092867",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2012-11856",
                    "ArXiv": "2012.11856",
                    "DOI": "10.1016/j.neunet.2021.10.017",
                    "CorpusId": 229348791,
                    "PubMed": "34768091"
                },
                "corpusId": 229348791,
                "publicationVenue": {
                    "id": "a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                    "name": "Neural Networks",
                    "type": "journal",
                    "alternate_names": [
                        "Neural Netw"
                    ],
                    "issn": "0893-6080",
                    "url": "http://www.elsevier.com/locate/neunet",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/841/description",
                        "http://www.sciencedirect.com/science/journal/08936080"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2ad96488aa03242a8ef83e97a1c45632dd092867",
                "title": "GuidedStyle: Attribute Knowledge Guided Style Manipulation for Semantic Face Editing",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3468964",
                        "name": "Xianxu Hou"
                    },
                    {
                        "authorId": "2119704887",
                        "name": "Xiaokang Zhang"
                    },
                    {
                        "authorId": "121640365",
                        "name": "Linlin Shen"
                    },
                    {
                        "authorId": "50737981",
                        "name": "Zhihui Lai"
                    },
                    {
                        "authorId": "1785406293",
                        "name": "Jun Wan"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "945aa2eb4b7ceecebf0562dfc12fcadb8fd38970",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2012-12265",
                    "ArXiv": "2012.12265",
                    "DOI": "10.1109/CVPR46437.2021.00394",
                    "CorpusId": 229363762
                },
                "corpusId": 229363762,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/945aa2eb4b7ceecebf0562dfc12fcadb8fd38970",
                "title": "Generative Interventions for Causal Learning",
                "abstract": "We introduce a framework for learning robust visual representations that generalize to new viewpoints, backgrounds, and scene contexts. Discriminative models often learn naturally occurring spurious correlations, which cause them to fail on images outside of the training distribution. In this paper, we show that we can steer generative models to manufacture interventions on features caused by confounding factors. Experiments, visualizations, and theoretical results show this method learns robust representations more consistent with the underlying causal relationships. Our approach improves performance on multiple datasets demanding out-of-distribution generalization, and we demonstrate state-of-the-art performance generalizing from ImageNet to ObjectNet dataset.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "7700460",
                        "name": "Chengzhi Mao"
                    },
                    {
                        "authorId": "1813517446",
                        "name": "Amogh Gupta"
                    },
                    {
                        "authorId": "51380124",
                        "name": "Augustine Cha"
                    },
                    {
                        "authorId": "2359832",
                        "name": "Hongya Wang"
                    },
                    {
                        "authorId": "152211006",
                        "name": "Junfeng Yang"
                    },
                    {
                        "authorId": "1856025",
                        "name": "Carl Vondrick"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also test the gender change direction of GANSpace (See Table 3).",
                "For the pose change experiments, we use the right-left\npose edit from GANSpace.",
                "Unlike reconstruction, the editing quality of an embedding has not been studied, because competitive editing frameworks just became available very recently [22, 11, 25, 3].",
                "Specifically, for a latent code in the GANSpace coordinate system, we can set the coordinate corresponding to pose to five different values: -2\u03c3, -\u03c3, 0, \u03c3, and +2\u03c3, where \u03c3 is the eigenvalue for the direction.",
                "For its simplicity and interpretability, we choose GANSpace [11] as our main evaluation method for editing quality."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "13606304522066793272f1ea0607587031543304",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2012-09036",
                    "ArXiv": "2012.09036",
                    "MAG": "3111141257",
                    "CorpusId": 229212848
                },
                "corpusId": 229212848,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/13606304522066793272f1ea0607587031543304",
                "title": "Improved StyleGAN Embedding: Where are the Good Latents?",
                "abstract": "StyleGAN is able to produce photorealistic images almost indistinguishable from real ones. Embedding images into the StyleGAN latent space is not a trivial task due to the reconstruction quality and editing quality trade-off. In this paper, we first introduce a new normalized space to analyze the diversity and the quality of the reconstructed latent codes. This space can help answer the question of where good latent codes are located in latent space. Second, we propose a framework to analyze the quality of different embedding algorithms. Third, we propose an improved embedding algorithm based on our analysis. We compare our results with the current state-of-the-art methods and achieve a better trade-off between reconstruction quality and editing quality.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "100516590",
                        "name": "Peihao Zhu"
                    },
                    {
                        "authorId": "94395014",
                        "name": "Rameen Abdal"
                    },
                    {
                        "authorId": "2408885",
                        "name": "Yipeng Qin"
                    },
                    {
                        "authorId": "1798011",
                        "name": "Peter Wonka"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Most recently, the work of Ha\u0308rko\u0308nen et al. (2020) studied unsupervised discovery of meaningful directions by using PCA on deep features of the generator.",
                "Other works have recently presented unsupervised techniques for exposing meaningful directions (Voynov & Babenko, 2020; H\u00e4rk\u00f6nen et al., 2020; Peebles et al., 2020).",
                "It is known that the earlier scales in such models are responsible for generating the global composition of the image, while the deeper scales are responsible for more local attributes (Karras et al., 2019a; Yang et al., 2019; Ha\u0308rko\u0308nen et al., 2020).",
                "Other works have recently presented unsupervised techniques for exposing meaningful directions (Voynov & Babenko, 2020; Ha\u0308rko\u0308nen et al., 2020; Peebles et al., 2020).",
                "Our approach is seemingly similar to GANspace (H\u00e4rk\u00f6nen et al., 2020), which computes PCA of activations within the network.",
                "It is known that the earlier scales in such models are responsible for generating the global composition of the image, while the deeper scales are responsible for more local attributes (Karras et al., 2019a; Yang et al., 2019; H\u00e4rk\u00f6nen et al., 2020).",
                "Our approach is seemingly similar to GANspace (Ha\u0308rko\u0308nen et al., 2020), which computes PCA of activations within the network."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "bfbb573504d446d32a85c9182db4a9d2edbbcdd6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2012-05328",
                    "ArXiv": "2012.05328",
                    "MAG": "3110870971",
                    "CorpusId": 228083457
                },
                "corpusId": 228083457,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/bfbb573504d446d32a85c9182db4a9d2edbbcdd6",
                "title": "GAN Steerability without optimization",
                "abstract": "Recent research has shown remarkable success in revealing \"steering\" directions in the latent spaces of pre-trained GANs. These directions correspond to semantically meaningful image transformations e.g., shift, zoom, color manipulations), and have similar interpretable effects across all categories that the GAN can generate. Some methods focus on user-specified transformations, while others discover transformations in an unsupervised manner. However, all existing techniques rely on an optimization procedure to expose those directions, and offer no control over the degree of allowed interaction between different transformations. In this paper, we show that \"steering\" trajectories can be computed in closed form directly from the generator's weights without any form of training or optimization. This applies to user-prescribed geometric transformations, as well as to unsupervised discovery of more complex effects. Our approach allows determining both linear and nonlinear trajectories, and has many advantages over previous methods. In particular, we can control whether one transformation is allowed to come on the expense of another (e.g. zoom-in with or without allowing translation to keep the object centered). Moreover, we can determine the natural end-point of the trajectory, which corresponds to the largest extent to which a transformation can be applied without incurring degradation. Finally, we show how transferring attributes between images can be achieved without optimization, even across different categories.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2034151889",
                        "name": "Nurit Spingarn-Eliezer"
                    },
                    {
                        "authorId": "2607278",
                        "name": "Ron Banner"
                    },
                    {
                        "authorId": "1880407",
                        "name": "T. Michaeli"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "be023ae9db1550f0d2ff57f7d396bf20f9068ca9",
                "externalIds": {
                    "ArXiv": "2101.05069",
                    "DBLP": "journals/corr/abs-2101-05069",
                    "CorpusId": 231592899
                },
                "corpusId": 231592899,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/be023ae9db1550f0d2ff57f7d396bf20f9068ca9",
                "title": "Formatting the Landscape: Spatial conditional GAN for varying population in satellite imagery",
                "abstract": "Climate change is expected to reshuffle the settlement landscape: forcing people in affected areas to migrate, to change their lifeways, and continuing to affect demographic change throughout the world. Changes to the geographic distribution of population will have dramatic impacts on land use and land cover and thus constitute one of the major challenges of planning for climate change scenarios. In this paper, we explore a generative model framework for generating satellite imagery conditional on gridded population distributions. We make additions to the existing ALAE [30] architecture, creating a spatially conditional version: SCALAE. This method allows us to explicitly disentangle population from the model\u2019s latent space and thus input custom population forecasts into the generated imagery. We postulate that such imagery could then be directly used for land cover and land use change estimation using existing frameworks, as well as for realistic visualisation of expected local change. We evaluate the model by comparing pixel and semantic reconstructions, as well as calculate the standard FID metric. The results suggest the model captures population distributions accurately and delivers a controllable method to generate realistic satellite imagery.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "119180341",
                        "name": "T. Langer"
                    },
                    {
                        "authorId": "145617941",
                        "name": "N. Fedorova"
                    },
                    {
                        "authorId": "26350757",
                        "name": "R. Hagensieker"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some studies have attempted to identity semantically meaningful directions by self-supervised learning [29] or PCA on latent spaces [15]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "cbd3649fdf84c7b289d3534ed9375b5d2ef1fc15",
                "externalIds": {
                    "MAG": "3110321962",
                    "ArXiv": "2011.14107",
                    "DBLP": "journals/corr/abs-2011-14107",
                    "DOI": "10.1109/CVPR46437.2021.00778",
                    "CorpusId": 227228015
                },
                "corpusId": 227228015,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/cbd3649fdf84c7b289d3534ed9375b5d2ef1fc15",
                "title": "Hijack-GAN: Unintended-Use of Pretrained, Black-Box GANs",
                "abstract": "While Generative Adversarial Networks (GANs) show increasing performance and the level of realism is becoming indistinguishable from natural images, this also comes with high demands on data and computation. We show that state-of-the-art GAN models \u2013 such as they are being publicly released by researchers and industry \u2013 can be used for a range of applications beyond unconditional image generation. We achieve this by an iterative scheme that also allows gaining control over the image generation process despite the highly non-linear latent spaces of the latest GAN models. We demonstrate that this opens up the possibility to re-use state-of-the-art, difficult to train, pre-trained GANs with a high level of control even if only black-box access is granted. Our work also raises concerns and awareness that the use cases of a published GAN model may well reach beyond the creators\u2019 intention, which needs to be taken into account before a full public release. Code is available at https://github.com/a514514772/hijackgan.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2108911196",
                        "name": "Hui-Po Wang"
                    },
                    {
                        "authorId": "2052212417",
                        "name": "Ning Yu"
                    },
                    {
                        "authorId": "1739548",
                        "name": "Mario Fritz"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Finally, a bunch of recent methods [29, 10, 24] identify interpretable directions without any form of supervision.",
                "Another approach, [10], demonstrates that interpretable directions often correspond to the principal components of the activations from the first layer of the generator network.",
                "encode human-interpretable concepts [26, 28, 7, 14, 25, 29, 10, 24, 30], which makes GANs the dominant paradigm for controllable generation.",
                "Note that the problem statement above resembles the established problem of learning the interpretable latent controls addressed in [28, 14, 29, 10, 30].",
                "Since the seminal paper [26], which has demonstrated the semantic arithmetic of latent vectors in GANs, plenty of methods to discover interpretable directions in the GAN latent spaces have been developed [26, 28, 7, 14, 25, 29, 10, 24, 30]."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "7a14cf615f30feb0a650a7ffa1c0a3d650736899",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2011-13786",
                    "MAG": "3107867397",
                    "ArXiv": "2011.13786",
                    "DOI": "10.1109/CVPR46437.2021.00367",
                    "CorpusId": 227209339
                },
                "corpusId": 227209339,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7a14cf615f30feb0a650a7ffa1c0a3d650736899",
                "title": "Navigating the GAN Parameter Space for Semantic Image Editing",
                "abstract": "Generative Adversarial Networks (GANs) are currently an indispensable tool for visual editing, being a standard component of image-to-image translation and image restoration pipelines. Furthermore, GANs are especially advantageous for controllable generation since their latent spaces contain a wide range of interpretable directions, well suited for semantic editing operations. By gradually changing latent codes along these directions, one can produce impressive visual effects, unattainable without GANs.In this paper, we significantly expand the range of visual effects achievable with the state-of-the-art models, like StyleGAN2. In contrast to existing works, which mostly operate by latent codes, we discover interpretable directions in the space of the generator parameters. By several simple methods, we explore this space and demonstrate that it also contains a plethora of interpretable directions, which are an excellent source of non-trivial semantic manipulations. The discovered manipulations cannot be achieved by transforming the latent codes and can be used to edit both synthetic and real images. We release our code and models and hope they will serve as a handy tool for further efforts on GAN-based image editing.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2029237214",
                        "name": "A. Cherepkov"
                    },
                    {
                        "authorId": "74254200",
                        "name": "A. Voynov"
                    },
                    {
                        "authorId": "143743802",
                        "name": "Artem Babenko"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "021c4855bca16e221525dd2c1ace1f182b502d9c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2011-13611",
                    "MAG": "3109957618",
                    "ArXiv": "2011.13611",
                    "DOI": "10.1109/ICCV48922.2021.01367",
                    "CorpusId": 227208924
                },
                "corpusId": 227208924,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/021c4855bca16e221525dd2c1ace1f182b502d9c",
                "title": "Frequency Domain Image Translation: More Photo-realistic, Better Identity-preserving",
                "abstract": "Image-to-image translation has been revolutionized with GAN-based methods. However, existing methods lack the ability to preserve the identity of the source domain. As a result, synthesized images can often over-adapt to the reference domain, losing important structural characteristics and suffering from suboptimal visual quality. To solve these challenges, we propose a novel frequency domain image translation (FDIT) framework, exploiting frequency information for enhancing the image generation process. Our key idea is to decompose the image into low-frequency and high-frequency components, where the high-frequency feature captures object structure akin to the identity. Our training objective facilitates the preservation of frequency information in both pixel space and Fourier spectral space. We broadly evaluate FDIT across five large-scale datasets and multiple tasks including image translation and GAN inversion. Extensive experiments and ablations show that FDIT effectively preserves the identity of the source image, and produces photo-realistic images. FDIT establishes state-of-the-art performance, reducing the average FID score by 5.6% compared to the previous best method.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2053144019",
                        "name": "Mu Cai"
                    },
                    {
                        "authorId": "2146243452",
                        "name": "Hong Zhang"
                    },
                    {
                        "authorId": "48186233",
                        "name": "Huijuan Huang"
                    },
                    {
                        "authorId": "34973011",
                        "name": "Qichuan Geng"
                    },
                    {
                        "authorId": "143983679",
                        "name": "Gao Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In this section we compare the ability of our approach to achieve disentangled manipulation of visual attributes to that of two state-of-the-art methods, specifically GANSpace [12] and InterFaceGAN [29].",
                "However, current methods require either a pretrained classifier [10, 28, 29, 34], a large set of paired examples [15], or manual examination of many candidate control directions [12], which limits the versatility of these approaches.",
                "[12] detect interpretable controls based on PCA applied either to the latent space of StyleGAN [17] or to the feature space of BigGAN [6].",
                "In contrast, GANSpace [12] identifies manipulation controls via a manual examination of a large number of different manipulation directions, which typically involve all of the channels of one or several layers.",
                "Comparing manipulations performed in StyleSpace to those in W and W+ spaces [12, 29], shows that our controls exhibit significantly lower AD.",
                "GANSpace manipulations also exhibit some entanglement (Lipstick affects face lightness, Gray hair ages the rest of the face).",
                "Figure 8 plots the mean-AD of the three methods (GANSpace, InterFaceGAN, and ours) for a range of maniplations of the Gender, Gray hair, and Lipstick attributes."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "ecb34053270d0a1667f95dab682cce650871ab69",
                "externalIds": {
                    "MAG": "3107537120",
                    "DBLP": "conf/cvpr/WuLS21",
                    "ArXiv": "2011.12799",
                    "DOI": "10.1109/CVPR46437.2021.01267",
                    "CorpusId": 227162202
                },
                "corpusId": 227162202,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ecb34053270d0a1667f95dab682cce650871ab69",
                "title": "StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation",
                "abstract": "We explore and analyze the latent style space of Style-GAN2, a state-of-the-art architecture for image generation, using models pretrained on several different datasets. We first show that StyleSpace, the space of channel-wise style parameters, is significantly more disentangled than the other intermediate latent spaces explored by previous works. Next, we describe a method for discovering a large collection of style channels, each of which is shown to control a distinct visual attribute in a highly localized and dis-entangled manner. Third, we propose a simple method for identifying style channels that control a specific attribute, using a pretrained classifier or a small number of example images. Manipulation of visual attributes via these StyleSpace controls is shown to be better disentangled than via those proposed in previous works. To show this, we make use of a newly proposed Attribute Dependency metric. Finally, we demonstrate the applicability of StyleSpace controls to the manipulation of real images. Our findings pave the way to semantically meaningful and well-disentangled image manipulations via simple and intuitive interfaces.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "34815981",
                        "name": "Zongze Wu"
                    },
                    {
                        "authorId": "70018371",
                        "name": "D. Lischinski"
                    },
                    {
                        "authorId": "2177801",
                        "name": "Eli Shechtman"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANs [21] are arguably widely used generative model.",
                "During the past few years, GANs\u2019 performance has been remarkably improved and achieved to produce photo-realistic quality images.",
                "In particular, as demonstrated in the paper [28], the latent space of StyleGANs trained with FFHQ is well-defined and linearly separable.",
                "Training details for GANs.",
                "In response, we design a pipeline that combines GANs, the GAN manipulation techniques, and 3D reconstruction networks.",
                "Another research direction [46, 24, 54] towards the facial attribute control is to directly manipulate a latent vector on top of the pre-trained latent space.",
                "GAN models have been previously verified to enable manipulation of an attribute of an output image while maintaining the identity of the individual by navigating the GANs\u2019 latent space.",
                "Once the disentangling capability of the GANs\u2019 latent space is improved, we believe our pipeline can be enhanced as well.",
                "Facial attribute manipulation on 2D image has drawn significant attention in various computer vision and graphics research, such as GANs [46, 24, 54] and image translation [57, 13, 23, 12] due to its practical necessity and broad applicability.",
                "One feasible solution against this seemingly insurmountable obstacle is to make use of Generative Adversarial Networks (GANs).",
                "Briefly, StyleRig [51] introduced additional networks trained to map the 3D parametric space into the well-trained latent space of GANs.",
                "The core idea of GANs is to train a generator in a way that its output distribution matches the data distribution.",
                "Related works of GANs and 3D Morphable Model (3DMM) are described in subsection 2.1, 2.2.",
                "Boosted by the enhanced power of GANs, facial attribute manipulation on 2D image has been widely explored."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "2c4f05eabdf7fdf5c1137534c9929d8e9f97f043",
                "externalIds": {
                    "MAG": "3108736759",
                    "ArXiv": "2011.12833",
                    "DBLP": "journals/corr/abs-2011-12833",
                    "CorpusId": 227162177
                },
                "corpusId": 227162177,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2c4f05eabdf7fdf5c1137534c9929d8e9f97f043",
                "title": "Enhanced 3DMM Attribute Control via Synthetic Dataset Creation Pipeline",
                "abstract": "While facial attribute manipulation of 2D images via Generative Adversarial Networks (GANs) has become common in computer vision and graphics due to its many practical uses, research on 3D attribute manipulation is relatively undeveloped. Existing 3D attribute manipulation methods are limited because the same semantic changes are applied to every 3D face. The key challenge for developing better 3D attribute control methods is the lack of paired training data in which one attribute is changed while other attributes are held fixed -- e.g., a pair of 3D faces where one is male and the other is female but all other attributes, such as race and expression, are the same. To overcome this challenge, we design a novel pipeline for generating paired 3D faces by harnessing the power of GANs. On top of this pipeline, we then propose an enhanced non-linear 3D conditional attribute controller that increases the precision and diversity of 3D attribute control compared to existing methods. We demonstrate the validity of our dataset creation pipeline and the superior performance of our conditional attribute controller via quantitative and qualitative evaluations.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "41021457",
                        "name": "Wonwoong Cho"
                    },
                    {
                        "authorId": "2152634091",
                        "name": "Inyeop Lee"
                    },
                    {
                        "authorId": "2055998168",
                        "name": "David I. Inouye"
                    }
                ]
            }
        },
        {
            "contexts": [
                "They either modify the training objective [9, 40, 71] or network architecture [39], or investigate latent spaces of well-engineered and pre-trained generative models [1, 16, 23, 27, 34, 78, 96]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "69a1d72bac9dfb18940ff97ae91643d6c8158e6d",
                "externalIds": {
                    "MAG": "3109420014",
                    "DBLP": "conf/cvpr/Niemeyer021",
                    "ArXiv": "2011.12100",
                    "DOI": "10.1109/CVPR46437.2021.01129",
                    "CorpusId": 227151657
                },
                "corpusId": 227151657,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/69a1d72bac9dfb18940ff97ae91643d6c8158e6d",
                "title": "GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields",
                "abstract": "Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects\u2019 shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "145048708",
                        "name": "Michael Niemeyer"
                    },
                    {
                        "authorId": "47237027",
                        "name": "Andreas Geiger"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Unsupervised approaches [9, 11, 22, 26] use classical unsupervised machine learning techniques, e.",
                "Unsupervised approaches [9, 11, 22, 26] are inappropriate for this task as they focus on discovering interpretable latent semantics, instead of solving for the latent direction for the target attribute.",
                "ies [5, 9, 11, 21, 22, 25, 26], would inevitably cause spatial entanglement.",
                "In this paper, 12 attributes directions are considered for the qualitative experiment, which is a large extension comparing to previous studies [11, 21, 22, 26].",
                "Unsupervised approaches [9, 11, 22, 26] adopt classical unsupervised machine learning techniques, e."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "9770907daf8450c7bfbc8571fc77beec6763a5ae",
                "externalIds": {
                    "MAG": "3103951107",
                    "DBLP": "journals/corr/abs-2011-09699",
                    "ArXiv": "2011.09699",
                    "CorpusId": 227054489
                },
                "corpusId": 227054489,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9770907daf8450c7bfbc8571fc77beec6763a5ae",
                "title": "Style Intervention: How to Achieve Spatial Disentanglement with Style-based Generators?",
                "abstract": "Generative Adversarial Networks (GANs) with style-based generators (e.g. StyleGAN) successfully enable semantic control over image synthesis, and recent studies have also revealed that interpretable image translations could be obtained by modifying the latent code. However, in terms of the low-level image content, traveling in the latent space would lead to `spatially entangled changes' in corresponding images, which is undesirable in many real-world applications where local editing is required. To solve this problem, we analyze properties of the 'style space' and explore the possibility of controlling the local translation with pre-trained style-based generators. Concretely, we propose 'Style Intervention', a lightweight optimization-based algorithm which could adapt to arbitrary input images and render natural translation effects under flexible objectives. We verify the performance of the proposed framework in facial attribute editing on high-resolution images, where both photo-realism and consistency are required. Extensive qualitative results demonstrate the effectiveness of our method, and quantitative measurements also show that the proposed algorithm outperforms state-of-the-art benchmarks in various aspects.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1860829",
                        "name": "Yunfan Liu"
                    },
                    {
                        "authorId": "2118912249",
                        "name": "Qi Li"
                    },
                    {
                        "authorId": "1757186",
                        "name": "Zhenan Sun"
                    },
                    {
                        "authorId": "143874948",
                        "name": "T. Tan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This document is structured as follows: In Section II the GAN framework, some relevant variations and a literature review in image generating GANs are presented along with (a) StarGAN [12] (b) GANSpace [22] (c) StyleGAN-v2 [9]",
                "GANSpace v2 [22] 2020 Parameter/Secondary Image EM-GP Image-to-Image yes"
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "fc38e854c734e19578a0a9c9d56626d6dee7b1e6",
                "externalIds": {
                    "DBLP": "conf/sccc/MT20",
                    "MAG": "3111863396",
                    "DOI": "10.1109/SCCC51225.2020.9281213",
                    "CorpusId": 228092301
                },
                "corpusId": 228092301,
                "publicationVenue": {
                    "id": "1b8a2044-dac2-483f-b337-37bc0d8d4b47",
                    "name": "International Conference of the Chilean Computer Science Society",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Chil Comput Sci Soc",
                        "SCCC"
                    ],
                    "url": "http://www.sccc.cl/"
                },
                "url": "https://www.semanticscholar.org/paper/fc38e854c734e19578a0a9c9d56626d6dee7b1e6",
                "title": "Multi-OctConv: Reducing Memory Requirements in Image Generative Adversarial Networks",
                "abstract": "Generative Adversarial Networks (GANs) for image generation of human faces have provided excellent results in recent years. However, we were able to identify a common problem among them: high memory usage in their training phase due to the convolutional encoder architecture used in these models. We address this issue by replacing the traditional convolutional layers in a model by what we call a Multi-Octave Convolution (M-OctConv) without modifying its architecture. An advantage of this method is that it can be easily combined with traditional memory reduction techniques, such as pruning. We evaluate our proposition on StarGAN model achieving up to 40% of memory usage reduction without affecting the quality of the generated images.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2034989872",
                        "name": "M. FranciscoTobar"
                    },
                    {
                        "authorId": "144479869",
                        "name": "Claudio E. Torres"
                    }
                ]
            }
        },
        {
            "contexts": [
                "principal directions variation [14] in the latent/feature space or proposing a structured noise injection method, where the input noises are injected to GANs for controlling specific parts of the generated images [15]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6d7e7d7dd06e54da0f2971d16e51afae4d087b8d",
                "externalIds": {
                    "DBLP": "journals/tip/LiuCZLDQ22",
                    "ArXiv": "2011.02638",
                    "DOI": "10.1109/TIP.2022.3142527",
                    "CorpusId": 226254298,
                    "PubMed": "35044915"
                },
                "corpusId": 226254298,
                "publicationVenue": {
                    "id": "e117fa7f-05b7-4dd6-b64b-0a0a7c0393d8",
                    "name": "IEEE Transactions on Image Processing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Image Process"
                    ],
                    "issn": "1057-7149",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=83",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6d7e7d7dd06e54da0f2971d16e51afae4d087b8d",
                "title": "Towards Disentangling Latent Space for Unsupervised Semantic Face Editing",
                "abstract": "Facial attributes in StyleGAN generated images are entangled in the latent space which makes it very difficult to independently control a specific attribute without affecting the others. Supervised attribute editing requires annotated training data which is difficult to obtain and limits the editable attributes to those with labels. Therefore, unsupervised attribute editing in an disentangled latent space is key to performing neat and versatile semantic face editing. In this paper, we present a new technique termed Structure-Texture Independent Architecture with Weight Decomposition and Orthogonal Regularization (STIA-WO) to disentangle the latent space for unsupervised semantic face editing. By applying STIA-WO to GAN, we have developed a StyleGAN termed STGAN-WO which performs weight decomposition through utilizing the style vector to construct a fully controllable weight matrix to regulate image synthesis, and employs orthogonal regularization to ensure each entry of the style vector only controls one independent feature matrix. To further disentangle the facial attributes, STGAN-WO introduces a structure-texture independent architecture which utilizes two independently and identically distributed (i.i.d.) latent vectors to control the synthesis of the texture and structure components in a disentangled way. Unsupervised semantic editing is achieved by moving the latent code in the coarse layers along its orthogonal directions to change texture related attributes or changing the latent code in the fine layers to manipulate structure related ones. We present experimental results which show that our new STGAN-WO can achieve better attribute editing than state of the art methods.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "9152001",
                        "name": "Kanglin Liu"
                    },
                    {
                        "authorId": "10830369",
                        "name": "Gaofeng Cao"
                    },
                    {
                        "authorId": "143741775",
                        "name": "Fei Zhou"
                    },
                    {
                        "authorId": "3185721",
                        "name": "Bozhi Liu"
                    },
                    {
                        "authorId": "144408268",
                        "name": "Jiang Duan"
                    },
                    {
                        "authorId": "143740671",
                        "name": "G. Qiu"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2019), GANSpace (H\u00e4rk\u00f6nen et al., 2020), and SeFa (Shen & Zhou, 2020).",
                "We compare our method with other unsupervised methods that also achieve face rotation with GANs, including HoloGAN (Nguyen-Phuoc et al., 2019), GANSpace (Ha\u0308rko\u0308nen et al., 2020), and SeFa (Shen & Zhou, 2020).",
                "\u2193 HoloGAN 47.38 69.24 GANSpace 41.17 58.93 SeFa 41.79 60.73 Ours (3D) 28.93 43.02 Ours (GAN) 39.85 57.21\nIdentity-preserving Face Rotation.",
                "We compare with HoloGAN, GANSpace, and SeFa.",
                "In contrast, SeFa (Shen & Zhou, 2020) and GANSpace (H\u00e4rk\u00f6nen et al., 2020) discover meaningful latent directions without supervision, where some of them is coupled with content pose.",
                "In contrast, SeFa (Shen & Zhou, 2020) and GANSpace (Ha\u0308rko\u0308nen et al., 2020) discover meaningful latent directions without supervision, where some of them is coupled with content pose."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "7d7d189796efa8fbd3f516b183954bc36f262f3f",
                "externalIds": {
                    "ArXiv": "2011.00844",
                    "DBLP": "journals/corr/abs-2011-00844",
                    "MAG": "3095552265",
                    "CorpusId": 226226846
                },
                "corpusId": 226226846,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7d7d189796efa8fbd3f516b183954bc36f262f3f",
                "title": "Do 2D GANs Know 3D Shape? Unsupervised 3D shape reconstruction from 2D Image GANs",
                "abstract": "Natural images are projections of 3D objects on a 2D image plane. While state-of-the-art 2D generative models like GANs show unprecedented quality in modeling the natural image manifold, it is unclear whether they implicitly capture the underlying 3D object structures. And if so, how could we exploit such knowledge to recover the 3D shapes of objects in the images? To answer these questions, in this work, we present the first attempt to directly mine 3D geometric clues from an off-the-shelf 2D GAN that is trained on RGB images only. Through our investigation, we found that such a pre-trained GAN indeed contains rich 3D knowledge and thus can be used to recover 3D shape from a single 2D image in an unsupervised manner. The core of our framework is an iterative strategy that explores and exploits diverse viewpoint and lighting variations in the GAN image manifold. The framework does not require 2D keypoint or 3D annotations, or strong assumptions on object shapes (e.g. shapes are symmetric), yet it successfully recovers 3D shapes with high precision for human faces, cats, cars, and buildings. The recovered 3D shapes immediately allow high-quality image editing like relighting and object rotation. We quantitatively demonstrate the effectiveness of our approach compared to previous methods in both 3D shape reconstruction and face rotation. Our code and models will be released at this https URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "14214933",
                        "name": "Xingang Pan"
                    },
                    {
                        "authorId": "144445937",
                        "name": "Bo Dai"
                    },
                    {
                        "authorId": "2117940996",
                        "name": "Ziwei Liu"
                    },
                    {
                        "authorId": "1717179",
                        "name": "Chen Change Loy"
                    },
                    {
                        "authorId": "144389940",
                        "name": "P. Luo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Shen et al. (2020) aims to find the latent space vectors that correspond to meaningful edits, while Ha\u0308rko\u0308nen et al. (2020) exploits PCA to disentangle the latent space."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "288e170e5771200653b2f65d4837b74295b2c258",
                "externalIds": {
                    "MAG": "3093170800",
                    "DBLP": "journals/corr/abs-2010-09125",
                    "ArXiv": "2010.09125",
                    "CorpusId": 224706002
                },
                "corpusId": 224706002,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/288e170e5771200653b2f65d4837b74295b2c258",
                "title": "Image GANs meet Differentiable Rendering for Inverse Graphics and Interpretable 3D Neural Rendering",
                "abstract": "Differentiable rendering has paved the way to training neural networks to perform \"inverse graphics\" tasks such as predicting 3D geometry from monocular photographs. To train high performing models, most of the current approaches rely on multi-view imagery which are not readily available in practice. Recent Generative Adversarial Networks (GANs) that synthesize images, in contrast, seem to acquire 3D knowledge implicitly during training: object viewpoints can be manipulated by simply manipulating the latent codes. However, these latent codes often lack further physical interpretation and thus GANs cannot easily be inverted to perform explicit 3D reasoning. In this paper, we aim to extract and disentangle 3D knowledge learned by generative models by utilizing differentiable renderers. Key to our approach is to exploit GANs as a multi-view data generator to train an inverse graphics network using an off-the-shelf differentiable renderer, and the trained inverse graphics network as a teacher to disentangle the GAN's latent code into interpretable 3D properties. The entire architecture is trained iteratively using cycle consistency losses. We show that our approach significantly outperforms state-of-the-art inverse graphics networks trained on existing datasets, both quantitatively and via user studies. We further showcase the disentangled GAN as a controllable 3D \"neural renderer\", complementing traditional graphics renderers.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2108078171",
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "authorId": "47483054",
                        "name": "Wenzheng Chen"
                    },
                    {
                        "authorId": "18900686",
                        "name": "Huan Ling"
                    },
                    {
                        "authorId": "145200206",
                        "name": "Jun Gao"
                    },
                    {
                        "authorId": "1922259572",
                        "name": "Yinan Zhang"
                    },
                    {
                        "authorId": "143805211",
                        "name": "A. Torralba"
                    },
                    {
                        "authorId": "37895334",
                        "name": "S. Fidler"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[26] demonstrate the use of Principal Components Analysis (PCA) in the activation space of specific layers, allowing high-level control over image attributes without any supervision."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "c00364141b8bb62b62e7c366c877d041a2e7332d",
                "externalIds": {
                    "ArXiv": "2010.05177",
                    "MAG": "3091951446",
                    "DBLP": "journals/corr/abs-2010-05177",
                    "CorpusId": 222290501
                },
                "corpusId": 222290501,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c00364141b8bb62b62e7c366c877d041a2e7332d",
                "title": "MammoGANesis: Controlled Generation of High-Resolution Mammograms for Radiology Education",
                "abstract": "During their formative years, radiology trainees are required to interpret hundreds of mammograms per month, with the objective of becoming apt at discerning the subtle patterns differentiating benign from malignant lesions. Unfortunately, medico-legal and technical hurdles make it difficult to access and query medical images for training. \nIn this paper we train a generative adversarial network (GAN) to synthesize 512 x 512 high-resolution mammograms. The resulting model leads to the unsupervised separation of high-level features (e.g. the standard mammography views and the nature of the breast lesions), with stochastic variation in the generated images (e.g. breast adipose tissue, calcification), enabling user-controlled global and local attribute-editing of the synthesized images. \nWe demonstrate the model's ability to generate anatomically and medically relevant mammograms by achieving an average AUC of 0.54 in a double-blind study on four expert mammography radiologists to distinguish between generated and real images, ascribing to the high visual quality of the synthesized and edited mammograms, and to their potential use in advancing and facilitating medical education.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1994286801",
                        "name": "C. Zakka"
                    },
                    {
                        "authorId": "1994071803",
                        "name": "Ghida Saheb"
                    },
                    {
                        "authorId": "12765376",
                        "name": "Elie Najem"
                    },
                    {
                        "authorId": "6883559",
                        "name": "G. Berjawi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The work most closely related to ours is GANSpace (H\u00e4rk\u00f6nen et al., 2020) for image synthesis editing.",
                "GANSpace applies PCA within the latent feature space of a pretrained GAN to discover semantically-interpretable directions for image editing in the latent space.",
                "Performing SVD on the weight space enables two critical differences between our work and H\u00e4rk\u00f6nen et al. (2020): (i) we edit the entire output distribution rather than one image, and (ii) rather than manual editing, we adapt to a new domain."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4b8a11d7b2508d64a39e37968a3a00a2e80de187",
                "externalIds": {
                    "MAG": "3094087710",
                    "DBLP": "journals/corr/abs-2010-11943",
                    "ArXiv": "2010.11943",
                    "CorpusId": 225039998
                },
                "corpusId": 225039998,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4b8a11d7b2508d64a39e37968a3a00a2e80de187",
                "title": "Few-Shot Adaptation of Generative Adversarial Networks",
                "abstract": "Generative Adversarial Networks (GANs) have shown remarkable performance in image synthesis tasks, but typically require a large number of training samples to achieve high-quality synthesis. This paper proposes a simple and effective method, Few-Shot GAN (FSGAN), for adapting GANs in few-shot settings (less than 100 images). FSGAN repurposes component analysis techniques and learns to adapt the singular values of the pre-trained weights while freezing the corresponding singular vectors. This provides a highly expressive parameter space for adaptation while constraining changes to the pretrained weights. We validate our method in a challenging few-shot setting of 5-100 images in the target domain. We show that our method has significant visual quality gains compared with existing GAN adaptation methods. We report qualitative and quantitative results showing the effectiveness of our method. We additionally highlight a problem for few-shot synthesis in the standard quantitative metric used by data-efficient image synthesis works. Code and additional results are available at this http URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "40978165",
                        "name": "Esther Robb"
                    },
                    {
                        "authorId": "39336289",
                        "name": "Wen-Sheng Chu"
                    },
                    {
                        "authorId": "2109224633",
                        "name": "Abhishek Kumar"
                    },
                    {
                        "authorId": "2238908927",
                        "name": "Jia-Bin Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Training a standard GAN and retrospectively discovering semantically meaningful axes of variation in the latent space [25, 10]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6a347f0354bf7a126c4924953d9bbb542d4204ac",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2012-10553",
                    "ArXiv": "2012.10553",
                    "MAG": "3119477578",
                    "DOI": "10.1109/IJCB48548.2020.9304879",
                    "CorpusId": 229340598
                },
                "corpusId": 229340598,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6a347f0354bf7a126c4924953d9bbb542d4204ac",
                "title": "An Assessment of GANs for Identity-related Applications",
                "abstract": "Generative Adversarial Networks (GANs) are now capable of producing synthetic face images of exceptionally high visual quality. In parallel to the development of GANs themselves, efforts have been made to develop metrics to objectively assess the characteristics of the synthetic images, mainly focusing on visual quality and the variety of images. Little work has been done, however, to assess overfitting of GANs and their ability to generate new identities. In this paper we apply a state of the art biometric network to various datasets of synthetic images and perform a thorough assessment of their identity-related characteristics. We conclude that GANs can indeed be used to generate new, imagined identities meaning that applications such as anonymisation of image sets and augmentation of training datasets with distractor images are viable applications. We also assess the ability of GANs to disentangle identity from other image characteristics and propose a novel GAN triplet loss that we show to improve this disentanglement.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "4581098",
                        "name": "Richard T. Marriott"
                    },
                    {
                        "authorId": "2039961117",
                        "name": "Safa Madiouni"
                    },
                    {
                        "authorId": "3293655",
                        "name": "S. Romdhani"
                    },
                    {
                        "authorId": "1805495",
                        "name": "S. Gentric"
                    },
                    {
                        "authorId": "102252372",
                        "name": "Liming Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Most approaches linearly change the StyleGAN latent codes for editing [H\u00c3\u010frk\u00c3\u0171nen et al. 2020; Shen et al. 2020; Tewari et al. 2020]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "32346e82851ea5cf66c0898232403b025877757f",
                "externalIds": {
                    "MAG": "3087815362",
                    "DBLP": "journals/corr/abs-2009-09485",
                    "ArXiv": "2009.09485",
                    "DOI": "10.1145/3414685.3417803",
                    "CorpusId": 221818597
                },
                "corpusId": 221818597,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/32346e82851ea5cf66c0898232403b025877757f",
                "title": "PIE",
                "abstract": "Editing of portrait images is a very popular and important research topic with a large variety of applications. For ease of use, control should be provided via a semantically meaningful parameterization that is akin to computer animation controls. The vast majority of existing techniques do not provide such intuitive and fine-grained control, or only enable coarse editing of a single isolated control parameter. Very recently, high-quality semantically controlled editing has been demonstrated, however only on synthetically created StyleGAN images. We present the first approach for embedding real portrait images in the latent space of StyleGAN, which allows for intuitive editing of the head pose, facial expression, and scene illumination in the image. Semantic editing in parameter space is achieved based on StyleRig, a pretrained neural network that maps the control space of a 3D morphable face model to the latent space of the GAN. We design a novel hierarchical non-linear optimization problem to obtain the embedding. An identity preservation energy term allows spatially coherent edits while maintaining facial integrity. Our approach runs at interactive frame rates and thus allows the user to explore the space of possible edits. We evaluate our approach on a wide set of portrait photos, compare it to the current state of the art, and validate the effectiveness of its components in an ablation study.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "9102722",
                        "name": "A. Tewari"
                    },
                    {
                        "authorId": "1854465",
                        "name": "Mohamed A. Elgharib"
                    },
                    {
                        "authorId": "2228472909",
                        "name": "Mallikarjun B R"
                    },
                    {
                        "authorId": "39600032",
                        "name": "Florian Bernard"
                    },
                    {
                        "authorId": "145156858",
                        "name": "H. Seidel"
                    },
                    {
                        "authorId": "144565371",
                        "name": "P. P\u00e9rez"
                    },
                    {
                        "authorId": "1699058",
                        "name": "M. Zollh\u00f6fer"
                    },
                    {
                        "authorId": "1680185",
                        "name": "C. Theobalt"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some of the most recent methods perform face manipulation by generating the entire head [65], [66]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a7208735a38d53af9e3f0bf7652deddecaf92d61",
                "externalIds": {
                    "ArXiv": "2008.12262",
                    "DBLP": "journals/pami/NirkinWKH22",
                    "DOI": "10.1109/TPAMI.2021.3093446",
                    "CorpusId": 235685449,
                    "PubMed": "34185639"
                },
                "corpusId": 235685449,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a7208735a38d53af9e3f0bf7652deddecaf92d61",
                "title": "DeepFake Detection Based on Discrepancies Between Faces and Their Context",
                "abstract": "We propose a method for detecting face swapping and other identity manipulations in single images. Face swapping methods, such as DeepFake, manipulate the face region, aiming to adjust the face to the appearance of its context, while leaving the context unchanged. We show that this modus operandi produces discrepancies between the two regions (e.g., Fig. 1). These discrepancies offer exploitable telltale signs of manipulation. Our approach involves two networks: (i) a face identification network that considers the face region bounded by a tight semantic segmentation, and (ii) a context recognition network that considers the face context (e.g., hair, ears, neck). We describe a method which uses the recognition signals from our two networks to detect such discrepancies, providing a complementary detection signal that improves conventional real versus fake classifiers commonly used for detecting fake images. Our method achieves state of the art results on the FaceForensics++ and Celeb-DF-v2 benchmarks for face manipulation detection, and even generalizes to detect fakes produced by unseen methods.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "10795855",
                        "name": "Y. Nirkin"
                    },
                    {
                        "authorId": "145128145",
                        "name": "Lior Wolf"
                    },
                    {
                        "authorId": "1926432",
                        "name": "Y. Keller"
                    },
                    {
                        "authorId": "1756099",
                        "name": "Tal Hassner"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b2ceb85a8ffaec2a4496fa8ced03ce5c0c388819",
                "externalIds": {
                    "MAG": "3060988292",
                    "DBLP": "journals/corr/abs-2008-08930",
                    "CorpusId": 221186573
                },
                "corpusId": 221186573,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b2ceb85a8ffaec2a4496fa8ced03ce5c0c388819",
                "title": "Regularization And Normalization For Generative Adversarial Networks: A Review",
                "abstract": "Generative adversarial networks(GANs) is a popular generative model. With the development of the deep network, its application is more and more widely. By now, people think that the training of GANs is a two-person zero-sum game(discriminator and generator). The lack of strong supervision information makes the training very difficult, such as non-convergence, mode collapses, gradient disappearance, and the sensitivity of hyperparameters. As we all know, regularization and normalization are commonly used for stability training. This paper reviews and summarizes the research in the regularization and normalization for GAN. All the methods are classified into six groups: Gradient penalty, Norm normalization and regularization, Jacobian regularization, Layer normalization, Consistency regularization, and Self-supervision.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1510709850",
                        "name": "Ziqiang Li"
                    },
                    {
                        "authorId": "47600048",
                        "name": "Rentuo Tao"
                    },
                    {
                        "authorId": "2183101614",
                        "name": "Bin Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Unlike the previous methods (Abdal et al. 2019; Ha\u0308rko\u0308nen et al. 2020; Shen et al. 2019) the semantic edits performed on the latent vectors w forces the resultant vector to remain in the distribution of W space (p(w)). is enables us to do stable sequential edits which, to the best of our\u2026",
                "Unlike the previous methods (Abdal et al. 2019; H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2019) the semantic edits performed on the latent vectors w forces the resultant vector to remain in the distribution of W space (p(w)).",
                "(iii) GANSpace (H\u00e4rk\u00f6nen et al. 2020): We used the code provided by the authors and use the version using layer subsets.",
                "Nevertheless, we believe it will be useful for the reader to judge our work in competition with these recent papers (H\u00e4rk\u00f6nen et al. 2020; Nitzan et al. 2020; Tewari et al. 2020a), because they provide be\u008aer results than other work.",
                "We would like to reiterate that the three competing methods were only available on arXiv at the time of submission and were independently developed (GANSpace (Ha\u0308rko\u0308nen et al. 2020), StyleRig (Tewari et al. 2020a), InterfaceGAN (Shen et al. 2019))."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "8e9e5224bdb37bd454a5ff409824bc05d1e9a979",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2008-02401",
                    "MAG": "3047371217",
                    "ArXiv": "2008.02401",
                    "DOI": "10.1145/3447648",
                    "CorpusId": 221006041
                },
                "corpusId": 221006041,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8e9e5224bdb37bd454a5ff409824bc05d1e9a979",
                "title": "StyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated Images using Conditional Continuous Normalizing Flows",
                "abstract": "High-quality, diverse, and photorealistic images can now be generated by unconditional GANs (e.g., StyleGAN). However, limited options exist to control the generation process using (semantic) attributes while still preserving the quality of the output. Further, due to the entangled nature of the GAN latent space, performing edits along one attribute can easily result in unwanted changes along other attributes. In this article, in the context of conditional exploration of entangled latent spaces, we investigate the two sub-problems of attribute-conditioned sampling and attribute-controlled editing. We present StyleFlow as a simple, effective, and robust solution to both the sub-problems by formulating conditional exploration as an instance of conditional continuous normalizing flows in the GAN latent space conditioned by attribute features. We evaluate our method using the face and the car latent space of StyleGAN, and demonstrate fine-grained disentangled edits along various attributes on both real photographs and StyleGAN generated images. For example, for faces, we vary camera pose, illumination variation, expression, facial hair, gender, and age. Finally, via extensive qualitative and quantitative comparisons, we demonstrate the superiority of StyleFlow over prior and several concurrent works. Project Page and Video: https://rameenabdal.github.io/StyleFlow.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "94395014",
                        "name": "Rameen Abdal"
                    },
                    {
                        "authorId": "100516590",
                        "name": "Peihao Zhu"
                    },
                    {
                        "authorId": "1710455",
                        "name": "N. Mitra"
                    },
                    {
                        "authorId": "1798011",
                        "name": "Peter Wonka"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Here we relied on a simple PCA approach for creating a reduced basis of the generative space, but there are other promising approaches in the literature that could also be applied to this task (e.g., [27, 28]).",
                "We tested an alternative aggregation approach, where we summarized the five responses for each item with a KDE (Gaussian kernel, standard deviation of 0.5 in units of PCA standard deviations), and took the mode of the resulting distribution (Exp. 4c, Fig.",
                "As would be expected, the version of PCA with components 71\u201380 performs poorly; in practice, these components contribute very little perceptually speaking (see also [25]).",
                "We used the top 10 PCA components to parameterize our stimulus space, allowing these components to vary up to two standard deviations from the mean, and fixing the input latent code (z in the original papers) to the mean to control variability.",
                "0\n25\n50\n75\n100\nTraining Random Random PCA GSP Dataset\nC om\npo si\ntio n\n%\nPerceived gender Female Male Other\nFigure S25: Perceived gender for faces from different stages of the modeling pipeline, as collected in Exp. 4g.\n41\n0 10\n20\n30\n40\nTraining Random Random PCA GSP Dataset\nP er\nce iv\ned a\nge\nPerceived gender Female Male\nFigure S26: Perceived age split by gender for faces from different stages of the modeling pipeline, as collected in Exp. 4g.",
                "Training Random Random PCA GSP Training Random Random PCA GSP\nTraining Random Random PCA GSP 0\n25\n50\n75\n100\n0\n25\n50\n75\n100\nDataset\nP er\nce nt\nY es\nPerceived gender Female Male\nFigure S27: Evaluations of ethnicity, smiling, hats, formal clothes, and glasses, for faces from different stages of the modeling pipeline, split by gender (Exp. 4g).",
                "The results suggest an early advantage for the original PCA technique; however, the discrepancy with sparse PCA and ICA is small, and seems to disappear after more iterations.",
                "Figure S23: Final samples from the first four male and female chains in the dating preferences experiment (Exp. 4j).\nto a certain amount through the modeling pipeline, even before the PCA process; it seems as if the model is capturing this association and stereotyping it to a certain degree.",
                "On this basis, there is little evidence to dismiss any one of PCA, sparse PCA, or ICA.",
                "Following [50], we apply this approach to the generative adversarial network \u2018StyleGAN\u2019 [51, 52], pretrained on the FFHQ dataset of faces from Flickr [51], and applying PCA to the intermediate latent code (termed w in the original papers).",
                "In addition to the original PCA, we tested sparse PCA using a sparsity parameter of 1.0 (see the alpha parameter of SparsePCA from the scikit-learn package) and independent component analysis (ICA).",
                "We also tested the effect of retaining dimensions 71\u201380 instead of dimensions 1\u201310 of the PCA solution.",
                "71\u221280)\nSparse PCA\nICA\nFigure S24: Validation results for Exp. 4e (exploring different basis construction methods), as collected in Exp. 4f.",
                "S28, which shows that perceived intelligence is indeed associated\n40\n2.5\n3.0\n3.5\n0 10 20 30 Iteration\nR at\nin g\nOriginal PCA\nPCA (dims.",
                "The important prerequisite is finding a relatively low-dimensional basis for the network for GSP to parameterize; fortunately, it seems that relatively simple techniques such as PCA can sometimes suffice for this task [50].",
                "State-of-the-art image synthesis models typically still have high-dimensional parameter spaces, but here we build on recent work showing that the latent space of these models can be effectively navigated using principal component analysis (PCA) [50]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b7d8456790cbfccec23b06c354200ee79d8b374e",
                "externalIds": {
                    "DBLP": "conf/nips/HarrisonMARATLJ20",
                    "MAG": "3105925062",
                    "ArXiv": "2008.02595",
                    "DOI": "10.17605/OSF.IO/RZK4S",
                    "CorpusId": 221005871
                },
                "corpusId": 221005871,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b7d8456790cbfccec23b06c354200ee79d8b374e",
                "title": "Gibbs Sampling with People",
                "abstract": "A core problem in cognitive science and machine learning is to understand how humans derive semantic representations from perceptual objects, such as color from an apple, pleasantness from a musical chord, or trustworthiness from a face. Markov Chain Monte Carlo with People (MCMCP) is a prominent method for studying such representations, in which participants are presented with binary choice trials constructed such that the decisions follow a Markov Chain Monte Carlo acceptance rule. However, MCMCP's binary choice paradigm generates relatively little information per trial, and its local proposal function makes it slow to explore the parameter space and find the modes of the distribution. Here we therefore generalize MCMCP to a continuous-sampling paradigm, where in each iteration the participant uses a slider to continuously manipulate a single stimulus dimension to optimize a given criterion such as 'pleasantness'. We formulate both methods from a utility-theory perspective, and show that the new method can be interpreted as 'Gibbs Sampling with People' (GSP). Further, we introduce an aggregation parameter to the transition step, and show that this parameter can be manipulated to flexibly shift between Gibbs sampling and deterministic optimization. In an initial study, we show GSP clearly outperforming MCMCP; we then show that GSP provides novel and interpretable results in three other domains, namely musical chords, vocal emotions, and faces. We validate these results through large-scale perceptual rating experiments. The final experiments combine GSP with a state-of-the-art image synthesis network (StyleGAN) and a recent network interpretability technique (GANSpace), enabling GSP to efficiently explore high-dimensional perceptual spaces, and demonstrating how GSP can be a powerful tool for jointly characterizing semantic representations in humans and machines.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "38124977",
                        "name": "Peter M C Harrison"
                    },
                    {
                        "authorId": "2075275038",
                        "name": "Raja Marjieh"
                    },
                    {
                        "authorId": "6794215",
                        "name": "Federico Adolfi"
                    },
                    {
                        "authorId": "1856121554",
                        "name": "P. V. Rijn"
                    },
                    {
                        "authorId": "1405036163",
                        "name": "Manuel Anglada-Tort"
                    },
                    {
                        "authorId": "2989684",
                        "name": "O. Tchernichovski"
                    },
                    {
                        "authorId": "1403137882",
                        "name": "P. Larrouy-Maestri"
                    },
                    {
                        "authorId": "48771852",
                        "name": "Nori Jacoby"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, numerous methods have shown competence in controlling StyleGAN\u2019s latent space and performing meaningful manipulations in W [17, 35, 36, 13].",
                "[13] find useful paths in an unsupervised manner by using the principal component axes of an intermediate activation space."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4cc32db67ff82cf1aa160631c35bb315c5add749",
                "externalIds": {
                    "DBLP": "conf/cvpr/RichardsonAPNAS21",
                    "MAG": "3046411017",
                    "ArXiv": "2008.00951",
                    "DOI": "10.1109/CVPR46437.2021.00232",
                    "CorpusId": 220936362
                },
                "corpusId": 220936362,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4cc32db67ff82cf1aa160631c35bb315c5add749",
                "title": "Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation",
                "abstract": "We present a generic image-to-image translation framework, pixel2style2pixel (pSp). Our pSp framework is based on a novel encoder network that directly generates a series of style vectors which are fed into a pretrained StyleGAN generator, forming the extended $\\mathcal{W} + $ latent space. We first show that our encoder can directly embed real images into $\\mathcal{W} + $, with no additional optimization. Next, we propose utilizing our encoder to directly solve image-to-image translation tasks, defining them as encoding problems from some input domain into the latent domain. By deviating from the standard \"invert first, edit later\" methodology used with previous StyleGAN encoders, our approach can handle a variety of tasks even when the input image is not represented in the StyleGAN domain. We show that solving translation tasks through StyleGAN significantly simplifies the training process, as no adversary is required, has better support for solving tasks without pixel-to-pixel correspondence, and inherently supports multi-modal synthesis via the resampling of styles. Finally, we demonstrate the potential of our framework on a variety of facial image-to-image translation tasks, even when compared to state-of-the-art solutions designed specifically for a single task, and further show that it can be extended beyond the human facial domain. Code is available at https://github.com/eladrich/pixel2style2pixel.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2511847",
                        "name": "Elad Richardson"
                    },
                    {
                        "authorId": "1850630812",
                        "name": "Yuval Alaluf"
                    },
                    {
                        "authorId": "2819477",
                        "name": "Or Patashnik"
                    },
                    {
                        "authorId": "1702992975",
                        "name": "Yotam Nitzan"
                    },
                    {
                        "authorId": "1920175",
                        "name": "Yaniv Azar"
                    },
                    {
                        "authorId": "151221072",
                        "name": "Stav Shapiro"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    }
                ]
            }
        },
        {
            "contexts": [
                "which is commonly used in the existing approaches [7, 24, 27, 26, 10].",
                "GANSpace [10] proposes to perform PCA on a collection of sampled data to find principal directions in the latent space.",
                "More concretely, prior work [7, 24, 27, 26, 10] proposed to use a certain direction n \u2208 R in the latent space to represent a semantic concept.",
                "However, they still require model training [26] and data sampling [10].",
                "We compare our method with some unsupervised alternatives, including the sampling-based method [10] and the learning-based method [5].",
                "In this part, we compare SeFa with GANSpace on the StyleGAN model trained on FF-HQ dataset [17].",
                "SeFa and GANSpace show close FID score since this is mostly determined by the generator itself as well as the manipulation model in Eq.",
                "[10] perform PCA on the sampled data to find primary directions in the latent space.",
                "Qualitative comparison between (a) GANSpace [10] and (b) SeFa.",
                "Some very recent studies explore the unsupervised discovery of interpretable GAN semantics [26, 10], but they also require model training [26] or data sampling [10].",
                "Quantitative comparison with GANSpace [10].",
                "But SeFa outperforms GANSpace on attribute re-scoring and user study."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "dc0092d06ab76465431edfd51b08d823b7d1ff3f",
                "externalIds": {
                    "DBLP": "conf/cvpr/ShenZ21",
                    "MAG": "3043243633",
                    "ArXiv": "2007.06600",
                    "DOI": "10.1109/CVPR46437.2021.00158",
                    "CorpusId": 220514827
                },
                "corpusId": 220514827,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/dc0092d06ab76465431edfd51b08d823b7d1ff3f",
                "title": "Closed-Form Factorization of Latent Semantics in GANs",
                "abstract": "A rich set of interpretable dimensions has been shown to emerge in the latent space of the Generative Adversarial Networks (GANs) trained for synthesizing images. In order to identify such latent dimensions for image editing, previous methods typically annotate a collection of synthesized samples and train linear classifiers in the latent space. However, they require a clear definition of the target attribute as well as the corresponding manual annotations, limiting their applications in practice. In this work, we examine the internal representation learned by GANs to reveal the underlying variation factors in an unsupervised manner. In particular, we take a closer look into the generation mechanism of GANs and further propose a closedform factorization algorithm for latent semantic discovery by directly decomposing the pre-trained weights. With a lightning-fast implementation, our approach is capable of not only finding semantically meaningful dimensions comparably to the state-of-the-art supervised methods, but also resulting in far more versatile concepts across multiple GAN models trained on a wide range of datasets.1",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2117687899",
                        "name": "Yujun Shen"
                    },
                    {
                        "authorId": "145291669",
                        "name": "Bolei Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(H\u00e4rk\u00f6nen et al. 2020) showed that PCA directions of style latent vectors in StyleGAN (Karras, Laine, and Aila 2019) contain intuitive interpolation directions such as rotation."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "dc583c286637f6cf434c586b217286785594b574",
                "externalIds": {
                    "MAG": "3038900901",
                    "ArXiv": "2007.02171",
                    "DBLP": "conf/icccrea/AggarwalP20",
                    "CorpusId": 220363878
                },
                "corpusId": 220363878,
                "publicationVenue": {
                    "id": "5758d639-a450-4152-901d-7a78c8715aa7",
                    "name": "International Conference on Innovative Computing and Cloud Computing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Control Commun  Comput India",
                        "IEEE Int Conf Cogn Comput",
                        "IEEE International Conference Computer and Communications",
                        "Int Carpathian Control Conf",
                        "Int Conf Cogn Comput [services Soc",
                        "Int Conf Comput Cybern",
                        "IEEE International Conference on Cognitive Computing",
                        "IEEE Int Conf Comput Commun",
                        "International Conference on Computer Communication",
                        "Int Conf Innov Comput Cloud Comput",
                        "International Carpathian Control Conference",
                        "International Conference on Computational Creativity",
                        "Int Conf Comput Commun",
                        "International Conference on Control Communication & Computing India",
                        "ICCC",
                        "International Conference on Computational Cybernetics",
                        "Int Conf Comput Creativity",
                        "International Conference on Cognitive Computing [Services Society]",
                        "IEEE Int Conf Commun China",
                        "IEEE International Conference on Communications in China"
                    ],
                    "url": "http://computationalcreativity.net/",
                    "alternate_urls": [
                        "http://www.icccgovernors.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/dc583c286637f6cf434c586b217286785594b574",
                "title": "Neuro-Symbolic Generative Art: A Preliminary Study",
                "abstract": "There are two classes of generative art approaches: neural, where a deep model is trained to generate samples from a data distribution, and symbolic or algorithmic, where an artist designs the primary parameters and an autonomous system generates samples within these constraints. In this work, we propose a new hybrid genre: neuro-symbolic generative art. As a preliminary study, we train a generative deep neural network on samples from the symbolic approach. We demonstrate through human studies that subjects find the final artifacts and the creation process using our neuro-symbolic approach to be more creative than the symbolic approach 61% and 82% of the time respectively.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "48744869",
                        "name": "Gunjan Aggarwal"
                    },
                    {
                        "authorId": "153432684",
                        "name": "Devi Parikh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Like GANSpace [19], the user is provided with knobs to adjust the gain for each manipulation vector."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "7f91c91c817ee1488f70264ecc22cee0f6908260",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-00653",
                    "ArXiv": "2007.00653",
                    "MAG": "3101956722",
                    "CorpusId": 220280381
                },
                "corpusId": 220280381,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7f91c91c817ee1488f70264ecc22cee0f6908260",
                "title": "Swapping Autoencoder for Deep Image Manipulation",
                "abstract": "Deep generative models have become increasingly effective at producing realistic images from randomly sampled seeds, but using such models for controllable manipulation of existing images remains challenging. We propose the Swapping Autoencoder, a deep model designed specifically for image manipulation, rather than random sampling. The key idea is to encode an image with two independent components and enforce that any swapped combination maps to a realistic image. In particular, we encourage the components to represent structure and texture, by enforcing one component to encode co-occurrent patch statistics across different parts of an image. As our method is trained with an encoder, finding the latent codes for a new input image becomes trivial, rather than cumbersome. As a result, it can be used to manipulate real input images in various ways, including texture swapping, local and global editing, and latent code vector arithmetic. Experiments on multiple datasets show that our model produces better results and is substantially more efficient compared to recent generative models.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2071929129",
                        "name": "Taesung Park"
                    },
                    {
                        "authorId": "2436356",
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "authorId": "39231399",
                        "name": "Oliver Wang"
                    },
                    {
                        "authorId": "2054975",
                        "name": "Jingwan Lu"
                    },
                    {
                        "authorId": "2177801",
                        "name": "Eli Shechtman"
                    },
                    {
                        "authorId": "1763086",
                        "name": "Alexei A. Efros"
                    },
                    {
                        "authorId": "2844849",
                        "name": "Richard Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "StyleGAN latent codes exhibit disentangled properties, which enables extensive manipulation of images when using a trained StyleGAN model [15], [16], [17], [18], [19]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "26b9ec9f929497e89e9dab73ab3fd5e66609b893",
                "externalIds": {
                    "ArXiv": "2006.09701",
                    "DBLP": "journals/tifs/WangNARG22",
                    "DOI": "10.1109/TIFS.2022.3155975",
                    "CorpusId": 219720881
                },
                "corpusId": 219720881,
                "publicationVenue": {
                    "id": "d406a3f4-dc05-43be-b1f6-812f29de9c0e",
                    "name": "IEEE Transactions on Information Forensics and Security",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Inf Forensics Secur"
                    ],
                    "issn": "1556-6013",
                    "url": "http://www.ieee.org/organizations/society/sp/tifs.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=10206",
                        "http://www.signalprocessingsociety.org/publications/periodicals/forensics/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/26b9ec9f929497e89e9dab73ab3fd5e66609b893",
                "title": "Adversarial Detection by Latent Style Transformations",
                "abstract": "Detection-based defense approaches are effective against adversarial attacks without compromising the structure of the protected model. However, they could be bypassed by stronger adversarial attacks and are limited in their ability to handle high-fidelity images. In this paper, we explore an effective detection-based defense against adversarial attacks on images (including high-resolution images) by extending the investigation beyond a single-instance perspective to incorporate its transformations as well. Our intuition is that the essential characteristics of a valid image are generally not affected by non-essential style transformations, for example, a slight variation in the facial expression of a portrait would not alter its identification. In contrast, adversarial examples are designed to affect only a single instance at a time, with unpredictable effects on a set of transformations of the instance. Consequently, we leverage a controllable generative mechanism to conduct the non-essential style transformations for a given image via modification along the style axis in the latent space. Next, the consistency of prediction between the given input and its style transformations is used to distinguish adversarial instances. Based on experiments on three image datasets, including high-resolution images, we demonstrated that our defense could detect 90\u2013100 percent of adversarial examples produced by various state-of-the-art adversarial attacks, with a low false-positive rate.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2117010331",
                        "name": "Shuo Wang"
                    },
                    {
                        "authorId": "1681657",
                        "name": "S. Nepal"
                    },
                    {
                        "authorId": "1990239",
                        "name": "A. Abuadbba"
                    },
                    {
                        "authorId": "144031685",
                        "name": "C. Rudolph"
                    },
                    {
                        "authorId": "1803868",
                        "name": "M. Grobler"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, GANSpace [16] applies PCA to the latent space or feature space of a decoder to modify global attributes like the make of a car, background, or age."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4a632dc26ef619a2fa74e4d1bbb26d560646db30",
                "externalIds": {
                    "MAG": "3035450567",
                    "ArXiv": "2006.05394",
                    "DBLP": "conf/nips/HongABT20",
                    "CorpusId": 219559283
                },
                "corpusId": 219559283,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4a632dc26ef619a2fa74e4d1bbb26d560646db30",
                "title": "Low Distortion Block-Resampling with Spatially Stochastic Networks",
                "abstract": "We formalize and attack the problem of generating new images from old ones that are as diverse as possible, only allowing them to change without restrictions in certain parts of the image while remaining globally consistent. This encompasses the typical situation found in generative modelling, where we are happy with parts of the generated data, but would like to resample others (\"I like this generated castle overall, but this tower looks unrealistic, I would like a new one\"). In order to attack this problem we build from the best conditional and unconditional generative models to introduce a new network architecture, training procedure, and algorithm for resampling parts of the image as desired.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2111053934",
                        "name": "S. J. Hong"
                    },
                    {
                        "authorId": "2877311",
                        "name": "Mart\u00edn Arjovsky"
                    },
                    {
                        "authorId": "2054178670",
                        "name": "Ian Thompson"
                    },
                    {
                        "authorId": "2048052632",
                        "name": "Darryl Barnhart"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These often take the form of finding ways of navigating the latent space [23], or finding component vectors that represent key semantic properties [12, 27, 29], or trying to map out the latent space to find interpolations that map to semantic properties [15]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c08fefcd7abd5b4451ecef3dcfcf85aba57e8eae",
                "externalIds": {
                    "MAG": "3030072033",
                    "DBLP": "journals/corr/abs-2005-12420",
                    "ArXiv": "2005.12420",
                    "CorpusId": 218889525
                },
                "corpusId": 218889525,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c08fefcd7abd5b4451ecef3dcfcf85aba57e8eae",
                "title": "Network Bending: Manipulating The Inner Representations of Deep Generative Models",
                "abstract": "We introduce a new framework for interacting with and manipulating deep generative models that we call network bending. We present a comprehensive set of deterministic transformations that can be inserted as distinct layers into the computational graph of a trained generative neural network and applied during inference. In addition, we present a novel algorithm for clustering features based on their spatial activation maps. This allows features to be grouped together based on spatial similarity in an unsupervised fashion. This results in the meaningful manipulation of sets of features that correspond to the generation of a broad array of semantically significant aspects of the generated images. We demonstrate these transformations on the official pre-trained StyleGAN2 model trained on the FFHQ dataset. In doing so, we lay the groundwork for future interactive multimedia systems where the inner representation of deep generative models are manipulated for greater creative expression, whilst also increasing our understanding of how such \"black-box systems\" can be more meaningfully interpreted.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "47214633",
                        "name": "Terence Broad"
                    },
                    {
                        "authorId": "1745250",
                        "name": "F. Leymarie"
                    },
                    {
                        "authorId": "1691146",
                        "name": "M. Grierson"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[23] find useful paths in a completely unsupervised manner.",
                "As many works [51, 29, 64, 23] have shown, the latent space of GANs is well-behaved and allows great controlled editing opportunities."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ac057602f513f5abae0ddffbd49acc21f8592559",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2005-07728",
                    "ArXiv": "2005.07728",
                    "MAG": "3025437133",
                    "CorpusId": 218673538
                },
                "corpusId": 218673538,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ac057602f513f5abae0ddffbd49acc21f8592559",
                "title": "Disentangling in Latent Space by Harnessing a Pretrained Generator",
                "abstract": "Learning disentangled representations of data is a fundamental problem in artificial intelligence. Specifically, disentangled latent representations allow generative models to control and compose the disentangled factors in the synthesis process. Current methods, however, require extensive supervision and training, or instead, noticeably compromise quality. In this paper, we present a method that learn show to represent data in a disentangled way, with minimal supervision, manifested solely using available pre-trained networks. Our key insight is to decouple the processes of disentanglement and synthesis, by employing a leading pre-trained unconditional image generator, such as StyleGAN. By learning to map into its latent space, we leverage both its state-of-the-art quality generative power, and its rich and expressive latent space, without the burden of training it.We demonstrate our approach on the complex and high dimensional domain of human heads. We evaluate our method qualitatively and quantitatively, and exhibit its success with de-identification operations and with temporal identity coherency in image sequences. Through this extensive experimentation, we show that our method successfully disentangles identity from other facial attributes, surpassing existing methods, even though they require more training and supervision.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1702992975",
                        "name": "Yotam Nitzan"
                    },
                    {
                        "authorId": "1755628",
                        "name": "Amit H. Bermano"
                    },
                    {
                        "authorId": "1920864",
                        "name": "Yangyan Li"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As many works [H\u00e4rk\u00f6nen et al. 2020; Jahanian et al. 2019; Shen et al. 2019; Zhu et al. 2020] have shown, the latent space of GANs is well-behaved and allows great controlled editing opportunities."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b7b37f61acedbae95b5476b8e9f3992085e864a3",
                "externalIds": {
                    "MAG": "3109224353",
                    "DBLP": "journals/tog/NitzanBLC20",
                    "DOI": "10.1145/3414685.3417826",
                    "CorpusId": 221083302
                },
                "corpusId": 221083302,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b7b37f61acedbae95b5476b8e9f3992085e864a3",
                "title": "Face identity disentanglement via latent space mapping",
                "abstract": "Learning disentangled representations of data is a fundamental problem in artificial intelligence. Specifically, disentangled latent representations allow generative models to control and compose the disentangled factors in the synthesis process. Current methods, however, require extensive supervision and training, or instead, noticeably compromise quality. In this paper, we present a method that learns how to represent data in a disentangled way, with minimal supervision, manifested solely using available pre-trained networks. Our key insight is to decouple the processes of disentanglement and synthesis, by employing a leading pre-trained unconditional image generator, such as StyleGAN. By learning to map into its latent space, we leverage both its state-of-the-art quality, and its rich and expressive latent space, without the burden of training it. We demonstrate our approach on the complex and high dimensional domain of human heads. We evaluate our method qualitatively and quantitatively, and exhibit its success with de-identification operations and with temporal identity coherency in image sequences. Through extensive experimentation, we show that our method successfully disentangles identity from other facial attributes, surpassing existing methods, even though they require more training and supervision.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1702992975",
                        "name": "Yotam Nitzan"
                    },
                    {
                        "authorId": "1755628",
                        "name": "Amit H. Bermano"
                    },
                    {
                        "authorId": "1920864",
                        "name": "Yangyan Li"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "5361f372294d0d5405763fcf983b23919817055d",
                "externalIds": {
                    "MAG": "3022391944",
                    "ArXiv": "2005.01703",
                    "DBLP": "conf/eccv/HuhZZPH20",
                    "DOI": "10.1007/978-3-030-58536-5_2",
                    "CorpusId": 218487302
                },
                "corpusId": 218487302,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/5361f372294d0d5405763fcf983b23919817055d",
                "title": "Transforming and Projecting Images into Class-conditional Generative Networks",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "46213138",
                        "name": "Minyoung Huh"
                    },
                    {
                        "authorId": "2844849",
                        "name": "Richard Zhang"
                    },
                    {
                        "authorId": "2436356",
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "authorId": "46784982",
                        "name": "S. Paris"
                    },
                    {
                        "authorId": "1747779",
                        "name": "Aaron Hertzmann"
                    }
                ]
            }
        },
        {
            "contexts": [
                "manipulation [15], [39], [40], [41], [42], [43], but is restricted to synthetic images of the GAN itself or real images of limited complexity, e."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "7101bc1c316740d99cd87185586829291a983a1d",
                "externalIds": {
                    "ArXiv": "2003.13659",
                    "DBLP": "conf/eccv/PanZDLLL20",
                    "MAG": "3107096356",
                    "DOI": "10.1109/TPAMI.2021.3115428",
                    "CorpusId": 214713474,
                    "PubMed": "34559638"
                },
                "corpusId": 214713474,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7101bc1c316740d99cd87185586829291a983a1d",
                "title": "Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation",
                "abstract": "Learning a good image prior is a long-term goal for image restoration and manipulation. While existing methods like deep image prior (DIP) capture low-level image statistics, there are still gaps toward an image prior that captures rich image semantics including color, spatial coherence, textures, and high-level concepts. This work presents an effective way to exploit the image prior captured by a generative adversarial network (GAN) trained on large-scale natural images. As shown in Fig. 1, the deep generative prior (DGP) provides compelling results to restore missing semantics, e.g., color, patch, resolution, of various degraded images. It also enables diverse image manipulation including random jittering, image morphing, and category transfer. Such highly flexible restoration and manipulation are made possible through relaxing the assumption of existing GAN inversion methods, which tend to fix the generator. Notably, we allow the generator to be fine-tuned on-the-fly in a progressive manner regularized by feature distance obtained by the discriminator in GAN. We show that these easy-to-implement and practical changes help preserve the reconstruction to remain in the manifold of nature images, and thus lead to more precise and faithful reconstruction for real images. Code is available at https://github.com/XingangPan/deep-generative-prior.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "14214933",
                        "name": "Xingang Pan"
                    },
                    {
                        "authorId": "31818765",
                        "name": "Xiaohang Zhan"
                    },
                    {
                        "authorId": "144445937",
                        "name": "Bo Dai"
                    },
                    {
                        "authorId": "1807606",
                        "name": "Dahua Lin"
                    },
                    {
                        "authorId": "1717179",
                        "name": "Chen Change Loy"
                    },
                    {
                        "authorId": "144389940",
                        "name": "P. Luo"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "1bd136e223f32ecda855e46eaff4c8d4de478ec3",
                "externalIds": {
                    "ArXiv": "2002.11169",
                    "DBLP": "journals/neco/PaulWAB21",
                    "MAG": "3037248788",
                    "DOI": "10.1162/neco_a_01359",
                    "CorpusId": 220041599,
                    "PubMed": "33513320"
                },
                "corpusId": 220041599,
                "publicationVenue": {
                    "id": "69b9bcdd-8229-4a00-a6e0-00f0e99a2bf3",
                    "name": "Neural Computation",
                    "type": "journal",
                    "alternate_names": [
                        "Neural Comput"
                    ],
                    "issn": "0899-7667",
                    "url": "http://cognet.mit.edu/library/journals/journal?issn=08997667",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6720226",
                        "http://www.mitpressjournals.org/loi/neco",
                        "https://www.mitpressjournals.org/loi/neco"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1bd136e223f32ecda855e46eaff4c8d4de478ec3",
                "title": "Unsupervised Discovery, Control, and Disentanglement of Semantic Attributes With Applications to Anomaly Detection",
                "abstract": "Our work focuses on unsupervised and generative methods that address the following goals: (1) learning unsupervised generative representations that discover latent factors controlling image semantic attributes, (2) studying how this ability to control attributes formally relates to the issue of latent factor disentanglement, clarifying related but dissimilar concepts that had been confounded in the past, and (3) developing anomaly detection methods that leverage representations learned in the first goal. For goal 1, we propose a network architecture that exploits the combination of multiscale generative models with mutual information (MI) maximization. For goal 2, we derive an analytical result, lemma 1, that brings clarity to two related but distinct concepts: the ability of generative networks to control semantic attributes of images they generate, resulting from MI maximization, and the ability to disentangle latent space representations, obtained via total correlation minimization. More specifically, we demonstrate that maximizing semantic attribute control encourages disentanglement of latent factors. Using lemma 1 and adopting MI in our loss function, we then show empirically that for image generation tasks, the proposed approach exhibits superior performance as measured in the quality and disentanglement of the generated images when compared to other state-of-the-art methods, with quality assessed via the Fr\u00e9chet inception distance (FID) and disentanglement via mutual information gap. For goal 3, we design several systems for anomaly detection exploiting representations learned in goal 1 and demonstrate their performance benefits when compared to state-of-the-art generative and discriminative algorithms. Our contributions in representation learning have potential applications in addressing other important problems in computer vision, such as bias and privacy in AI.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2058356163",
                        "name": "W. Paul"
                    },
                    {
                        "authorId": "48812691",
                        "name": "I-J. Wang"
                    },
                    {
                        "authorId": "1741442",
                        "name": "F. Alajaji"
                    },
                    {
                        "authorId": "1765936",
                        "name": "P. Burlina"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2021) and control (Shen & Zhou, 2021; H\u00e4rk\u00f6nen et al., 2020; Voynov & Babenko, 2020; Georgopoulos et al., 2021; Tzelepis et al., 2021; Zhu et al., 2021a; Bounareli et al., 2022; Wu et al., 2021; Abdal et al., 2021) has subsequently received much attention.",
                "Eyes Nose Open mouth Smile GANSpace (H\u00e4rk\u00f6nen et al., 2020) 2.",
                "\u2026and qualitative results for the baseline methods, we use the following directions annotated from the pre-trained models by the authors, where available:\n\u2022 GANSpace (H\u00e4rk\u00f6nen et al., 2020): we use the following author-annotated directions: Eye_Openness, Nose_length, Screaming, and Smile.",
                "For both the quantitative and qualitative results for the baseline methods, we use the following directions annotated from the pre-trained models by the authors, where available:\n\u2022 GANSpace (H\u00e4rk\u00f6nen et al., 2020): we use the following author-annotated directions: Eye_Openness, Nose_length, Screaming, and Smile.",
                "\u2026tasks such as generative\n\u2217Corresponding author: j.a.oldfield@qmul.ac.uk\nmodel interpretability (Shen et al., 2020a; Bau et al., 2019; Yang et al., 2021) and image editing (H\u00e4rk\u00f6nen et al., 2020; Shen & Zhou, 2021; Shen et al., 2020c; Voynov & Babenko, 2020; Tzelepis et al., 2021; Bau et al., 2020).",
                "\u2026a supervised (Goetschalckx et al., 2019; Plumerault et al., 2020; Shen et al., 2020c;a) or unsupervised (Voynov & Babenko, 2020; Shen & Zhou, 2021; H\u00e4rk\u00f6nen et al., 2020; Tzelepis et al., 2021; Oldfield et al., 2021) manner\u2013many of them struggle to apply local changes to regions of interest in\u2026",
                "A popular line of GAN-based image editing research concerns itself with learning so-called \u201cinterpretable directions\u201d in the generator\u2019s latent space (H\u00e4rk\u00f6nen et al., 2020; Shen & Zhou, 2021; Shen et al., 2020c; Voynov & Babenko, 2020; Tzelepis et al., 2021; Yang et al., 2021; He et al., 2021; Haas et al., 2021; 2022).",
                "A popular line of GAN-based image editing research concerns itself with learning so-called \u201cinterpretable directions\u201d in the generator\u2019s latent space (H\u00e4rk\u00f6nen et al., 2020; Shen & Zhou, 2021; Shen et al., 2020c; Voynov & Babenko, 2020; Tzelepis et al., 2021; Yang et al., 2021; He et al., 2021;\u2026",
                "\u2026induced by these networks for interpretation (Bau et al., 2019; Shen et al., 2020a; Yang et al., 2021) and control (Shen & Zhou, 2021; H\u00e4rk\u00f6nen et al., 2020; Voynov & Babenko, 2020; Georgopoulos et al., 2021; Tzelepis et al., 2021; Zhu et al., 2021a; Bounareli et al., 2022; Wu et\u2026",
                ", 2020c;a) or unsupervised (Voynov & Babenko, 2020; Shen & Zhou, 2021; H\u00e4rk\u00f6nen et al., 2020; Tzelepis et al., 2021; Oldfield et al., 2021) manner\u2013many of them struggle to apply local changes to regions of interest in the image."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "92c8782c91eb3faab310e2957e9e86f8bd8779ce",
                "externalIds": {
                    "CorpusId": 259935257
                },
                "corpusId": 259935257,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/92c8782c91eb3faab310e2957e9e86f8bd8779ce",
                "title": "P AND A: U NSUPERVISED L EARNING OF P ARTS AND A PPEARANCES IN THE F EATURE M APS OF GAN S",
                "abstract": "Recent advances in the understanding of Generative Adversarial Networks (GANs) have led to remarkable progress in visual editing and synthesis tasks, capitalizing on the rich semantics that are embedded in the latent spaces of pre-trained GANs. However, existing methods are often tailored to specific GAN architectures and are limited to either discovering global semantic directions that do not facilitate localized control, or require some form of supervision through manually provided regions or segmentation masks. In this light, we present an architecture-agnostic approach that jointly discovers factors representing spatial parts and their appearances in an entirely unsupervised fashion. These factors are obtained by applying a semi-nonnegative tensor factorization on the feature maps, which in turn enables context-aware local image editing with pixel-level control. In addition, we show that the discovered appearance factors correspond to saliency maps that localize concepts of interest, without using any labels. Experiments on a wide range of GAN architectures and datasets show that, in comparison to the state of the art, our method is far more efficient in terms of training time and, most importantly, provides much more accurate localized control. Our code is available at https://github.com/james-oldfield/PandA.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2059960629",
                        "name": "James Oldfield"
                    },
                    {
                        "authorId": "1694090",
                        "name": "Christos Tzelepis"
                    },
                    {
                        "authorId": "1780393",
                        "name": "Yannis Panagakis"
                    },
                    {
                        "authorId": "1752913",
                        "name": "M. Nicolaou"
                    },
                    {
                        "authorId": "50058816",
                        "name": "I. Patras"
                    }
                ]
            }
        },
        {
            "contexts": [
                "directions in the latent space [26], [27]."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "f37b2574c4ea821f4b19e1620e0d90c9b9468c97",
                "externalIds": {
                    "DBLP": "journals/access/PernusBSSPSD23",
                    "DOI": "10.1109/ACCESS.2023.3276877",
                    "CorpusId": 258754534
                },
                "corpusId": 258754534,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f37b2574c4ea821f4b19e1620e0d90c9b9468c97",
                "title": "ChildNet: Structural Kinship Face Synthesis Model With Appearance Control Mechanisms",
                "abstract": "Kinship face synthesis is an increasingly popular topic within the computer vision community, particularly the task of predicting the child appearance using parental images. Previous work has been limited in terms of model capacity and inadequate training data, which comprised of low-resolution and tightly cropped images, leading to lower synthesis quality. In this paper, we propose ChildNet, a method for kinship face synthesis that leverages the facial image generation capabilities of a state-of-the-art Generative Adversarial Network (GAN), and resolves the aforementioned problems. ChildNet is designed within the GAN latent space and is able to predict a child appearance that bears high resemblance to real parents\u2019 children. To ensure fine-grained control, we propose an age and gender manipulation module that allows precise manipulation of the child synthesis result. ChildNet is capable of generating multiple child images per parent pair input, while providing a way to control the image generation variability. Additionally, we introduce a mechanism to control the dominant parent image. Finally, to facilitate the task of kinship face synthesis, we introduce a new kinship dataset, called Next of Kin. This dataset contains 3690 high-resolution face images with a diverse range of ethnicities and ages. We evaluate ChildNet in comprehensive experiments against three competing kinship face synthesis models, using two kinship datasets. The experiments demonstrate the superior performance of ChildNet in terms of identity similarity, while exhibiting high perceptual image quality. The source code for the model is publicly available at: https://github.com/MartinPernus/ChildNet.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "115771744",
                        "name": "Martin Pernu\u0161"
                    },
                    {
                        "authorId": "144477212",
                        "name": "Mansi Bhatnagar"
                    },
                    {
                        "authorId": "2217731085",
                        "name": "Badr Samad"
                    },
                    {
                        "authorId": "2112755896",
                        "name": "D. Singh"
                    },
                    {
                        "authorId": "34862665",
                        "name": "P. Peer"
                    },
                    {
                        "authorId": "51301592",
                        "name": "Vitomir \u0160truc"
                    },
                    {
                        "authorId": "1704880",
                        "name": "S. Dobri\u0161ek"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a2445408f08a3796786e0c95871fdc8af7a360c4",
                "externalIds": {
                    "CorpusId": 259108665
                },
                "corpusId": 259108665,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a2445408f08a3796786e0c95871fdc8af7a360c4",
                "title": "Improving Negative-Prompt Inversion via Proximal Guidance",
                "abstract": "DDIM inversion has revealed the remarkable potential of real image editing within diffusion-based methods. However, the accuracy of DDIM reconstruction degrades as larger classifier-free guidance (CFG) scales being used for enhanced editing. Null-text inversion (NTI) optimizes null embeddings to align the reconstruction and inversion trajectories with larger CFG scales, enabling real image editing with cross-attention control. Negative-prompt inversion (NPI) further offers a training-free closed-form solution of NTI. However, it may introduce artifacts and is still constrained by DDIM reconstruction quality. To overcome these limitations, we propose Proximal Negative-Prompt Inversion (ProxNPI), extending the concepts of NTI and NPI. We enhance NPI with a regularization term and reconstruction guidance, which reduces artifacts while capitalizing on its training-free nature. Our method provides an efficient and straightforward approach, effectively addressing real image editing tasks with minimal computational overhead.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3471102",
                        "name": "Ligong Han"
                    },
                    {
                        "authorId": "1902406591",
                        "name": "Song Wen"
                    },
                    {
                        "authorId": "2157956022",
                        "name": "Qi Chen"
                    },
                    {
                        "authorId": "2128662401",
                        "name": "Zhixing Zhang"
                    },
                    {
                        "authorId": "8799238",
                        "name": "Kunpeng Song"
                    },
                    {
                        "authorId": "30106897",
                        "name": "Mengwei Ren"
                    },
                    {
                        "authorId": "151184349",
                        "name": "Ruijiang Gao"
                    },
                    {
                        "authorId": "2109168002",
                        "name": "Yuxiao Chen"
                    },
                    {
                        "authorId": "1771885",
                        "name": "Ding Liu"
                    },
                    {
                        "authorId": "2004341222",
                        "name": "Qilong Zhangli"
                    },
                    {
                        "authorId": "1749519378",
                        "name": "Anastasis Stathopoulos"
                    },
                    {
                        "authorId": "3807016",
                        "name": "Jindong Jiang"
                    },
                    {
                        "authorId": "2151464031",
                        "name": "Zhaoyang Xia"
                    },
                    {
                        "authorId": "2018087",
                        "name": "Akash Srivastava"
                    },
                    {
                        "authorId": "1711560",
                        "name": "Dimitris N. Metaxas"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Instead, we parameterize c as a linear combination of the top-N principal directions of W space [189,190]:"
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a8d6467e4126d0ada4e824b30a6098056b843242",
                "externalIds": {
                    "CorpusId": 259114449
                },
                "corpusId": 259114449,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a8d6467e4126d0ada4e824b30a6098056b843242",
                "title": "Generative Models of Images and Neural Networks",
                "abstract": "Generative Models of Images and Neural Networks",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2219729481",
                        "name": "Bill Peebles"
                    },
                    {
                        "authorId": "35235273",
                        "name": "William S. Peebles"
                    },
                    {
                        "authorId": "1763086",
                        "name": "Alexei A. Efros"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Notably, there have been previous studies in understanding latent semantic transitions for natural images (Shen & Zhou, 2021; H\u00e4rk\u00f6nen et al., 2020; Patashnik et al., 2021; Wu et al., 2021).",
                "For example, Ha\u0308rko\u0308nen et al. (2020) proposed to edit fake images by adding weighted eigenvectors to its latent representations.",
                "Notably, there have been previous studies in understanding latent semantic transitions for natural images (Shen & Zhou, 2021; Ha\u0308rko\u0308nen et al., 2020; Patashnik et al., 2021; Wu et al., 2021)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "101d5c28f4b6daa7cb08ee0ce646f9ce9af3cae4",
                "externalIds": {
                    "CorpusId": 259282814
                },
                "corpusId": 259282814,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/101d5c28f4b6daa7cb08ee0ce646f9ce9af3cae4",
                "title": "LEA: Latent Eigenvalue Analysis in application to high- throughput phenotypic drug screening",
                "abstract": "Understanding the phenotypic characteristics of cells in culture and detecting perturbations introduced by drug stimulation is of great importance for biomedical research. However, a thorough and comprehensive analysis of phenotypic heterogeneity is challenged by the complex nature of cell-level data. Here, we propose a novel Latent Eigenvalue Analysis (LEA) framework and apply it to high-throughput phenotypic profiling with single-cell and single-organelle granularity. Using the publicly available SARS-CoV-2 datasets stained with the multiplexed fluorescent cell-painting protocol, we demonstrate the power of the LEA approach in the investigation of phenotypic changes induced by more than 1800 drug compounds. As a result, LEA achieves a robust quantification of phenotypic changes introduced by drug treatment. Moreover, this quantification can be biologically supported by simulating clearly observable phenotypic transitions in a broad spectrum of use cases. In conclusion, LEA represents a new and broadly applicable approach for quantitative and interpretable analysis in routine drug screening practice.",
                "year": 2023,
                "authors": []
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c1553e864ec182d96c3d1c2fe7a99f4a7c589039",
                "externalIds": {
                    "CorpusId": 259318955
                },
                "corpusId": 259318955,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c1553e864ec182d96c3d1c2fe7a99f4a7c589039",
                "title": "Modular Control and Services to Operate Lineless Mobile Assembly Systems",
                "abstract": "The increasing product variability and lack of skilled workers demand for autonomous, flexible production. Since assembly is considered a main cost driver and accounts for a major part of production time, research focuses on new technologies in assembly. The paradigm of Line-less Mobile Assembly Systems (LMAS) provides a solution for the future of assembly by mobilizing all resources. Thus, dynamic product routes through spatiotemporally configured assembly stations on a shop floor free of fixed obstacles are enabled. In this chapter, we present research focal points on different levels of LMAS, starting with the macroscopic level of formation planning, followed by the mesoscopic level of mobile robot control and multipurpose input devices and the microscopic level of services, such as interpreting autonomous decisions and in-network computing. We provide cross-level data and knowledge transfer through a novel ontology-based knowledge management. Overall, our work contributes to future safe and predictable human-robot collaboration in dynamic LMAS stations based on accurate online formation and motion planning of mobile robots, novel human-machine interfaces and networking technologies, as well as trustworthy",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1405049044",
                        "name": "A. Kluge-Wilkes"
                    },
                    {
                        "authorId": "48861700",
                        "name": "Ralph Baier"
                    },
                    {
                        "authorId": "20543442",
                        "name": "Ike Kunze"
                    },
                    {
                        "authorId": "2120175832",
                        "name": "Aleksandra M\u00fcller"
                    },
                    {
                        "authorId": "2000456366",
                        "name": "Amir Shahidi"
                    },
                    {
                        "authorId": "103763981",
                        "name": "Dominik Wolfschl\u00e4ger"
                    },
                    {
                        "authorId": "1735841",
                        "name": "C. Brecher"
                    },
                    {
                        "authorId": "2754908",
                        "name": "B. Corves"
                    },
                    {
                        "authorId": "2586116",
                        "name": "M. H\u00fcsing"
                    },
                    {
                        "authorId": "3070800",
                        "name": "V. Nitsch"
                    },
                    {
                        "authorId": "46253301",
                        "name": "R. Schmitt"
                    },
                    {
                        "authorId": "1719689",
                        "name": "Klaus Wehrle"
                    }
                ]
            }
        },
        {
            "contexts": [
                "related to the latent space unsupervisedly [30], [31]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "8d758c4ff17355a3763be205c8ac7c1fc91e0dc7",
                "externalIds": {
                    "DBLP": "journals/tgrs/LiuWWZ23",
                    "DOI": "10.1109/TGRS.2023.3293478",
                    "CorpusId": 259683042
                },
                "corpusId": 259683042,
                "publicationVenue": {
                    "id": "70628d6a-97aa-4571-9701-bc0eb3989c32",
                    "name": "IEEE Transactions on Geoscience and Remote Sensing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Geosci Remote Sens"
                    ],
                    "issn": "0196-2892",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=36",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8d758c4ff17355a3763be205c8ac7c1fc91e0dc7",
                "title": "View-Semantic Transformer With Enhancing Diversity for Sparse-View SAR Target Recognition",
                "abstract": "With the rapid development of supervised learning-based synthetic aperture radar (SAR) target recognition technology, it is easy to find that the recognition performance is proportional to the number of training samples. However, the biased data distribution and under-representation of the model caused by incomplete data within categories exacerbate the challenge of SAR interpretation. In this article, we propose a new view-semantic transformer network (VSTNet) that generates synthesized samples to complete the statistical distribution of training data and improve the discriminative representation of the model. First, SAR images from different views are encoded into a disentangled latent space, which allows us to synthesize data with more diverse views by manipulating view-semantic features. Second, the synthesized data as a complement effectively expands the training set and alleviates the overfitting problem of limited data in sparse views. Third, the proposed method unifies SAR image synthesis and SAR target recognition into an end-to-end framework to boost their performance against each other. Experiments conducted on moving and stationary target acquisition and recognition (MSTAR) data demonstrate the robustness and effectiveness of the proposed method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144264532",
                        "name": "Zhunga Liu"
                    },
                    {
                        "authorId": "2211984306",
                        "name": "Feiyan Wu"
                    },
                    {
                        "authorId": "2833510",
                        "name": "Zaidao Wen"
                    },
                    {
                        "authorId": "2109373208",
                        "name": "Zuo-wei Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We speculate from an empirical perspective that the success of editing an opened mouth and the failure of editing eyebrows/nose shape may both ascribe to the entangled nature of the StyleGAN latent space, as prior arts [1, 10, 9, 26] have already managed to change the mouth openness via StyleGAN latent manipulation but none (to our knowledge) have succeeded in editing eyebrows/nose in the same way."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f750fc16f3a99aa5c697d2f153773e2bd1c8c415",
                "externalIds": {
                    "CorpusId": 260140769
                },
                "corpusId": 260140769,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f750fc16f3a99aa5c697d2f153773e2bd1c8c415",
                "title": "Text2Face: Text-based Face Generation with Geometry and Appearance Control",
                "abstract": "Recent years have witnessed the emergence of various techniques proposed for text-based human face generation and manipulation. Such methods, targeting at bridging the semantic gap between text and visual contents, provide users with a deft hand to turn ideas into visuals via the interface of text and enable more di-versified multimedia applications. However, due to the flexibility of linguistic expressiveness, the mapping from sentences to desired facial images is clearly many-to-many, causing ambiguities during text-to-face generation. To alleviate these ambiguities, we introduce a local-to-global framework with two graph neural networks (one for geometry and the other for appearance) embedded in to model the inter-dependency among facial parts. This is based upon our key observation that the geometry and appearance attributes among different facial components are not mutually independent, i.e ., the combinations of part-level facial features are not arbitrary and thus do not conform to a uniform distribution. By learning from the dataset distribution and enabling recommendations given partial descriptions of human faces, these networks are extremely suitable for our text-to-face task. Our method is capable of generating high-quality attribute-conditioned facial images from text. Extensive experiments have confirmed the superiority and usability of our method over the prior art.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2225377916",
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "authorId": "2129482293",
                        "name": "Junliang Chen"
                    },
                    {
                        "authorId": "3169698",
                        "name": "Hongbo Fu"
                    },
                    {
                        "authorId": "2215800053",
                        "name": "Jianjun Zhao"
                    },
                    {
                        "authorId": "2999411",
                        "name": "Shu-Yu Chen"
                    },
                    {
                        "authorId": "144614914",
                        "name": "Lin Gao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "GANSpace [29] and SeFa [53] identified important directions in the GAN latent space in an unsupervised manner; the discovered directions often correspond to meaningful semantic edits."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "656a492b9aead315671a8318b191d520dd88113d",
                "externalIds": {
                    "DBLP": "journals/tmm/HouZLS23",
                    "DOI": "10.1109/TMM.2022.3160360",
                    "CorpusId": 247536162
                },
                "corpusId": 247536162,
                "publicationVenue": {
                    "id": "10e76a35-58d6-443c-9683-fc16f2dd0a92",
                    "name": "IEEE transactions on multimedia",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Transactions on Multimedia",
                        "IEEE Trans Multimedia",
                        "IEEE trans multimedia"
                    ],
                    "issn": "1520-9210",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=6046"
                },
                "url": "https://www.semanticscholar.org/paper/656a492b9aead315671a8318b191d520dd88113d",
                "title": "TextFace: Text-to-Style Mapping Based Face Generation and Manipulation",
                "abstract": "As a subtopic of text-to-image synthesis, text-to-face generation has great potential in face-related applications. In this paper, we propose a generic text-to-face framework, namely, TextFace, to achieve diverse and high-quality face image generation from text descriptions. We introduce text-to-style mapping, a novel method where the text description can be directly encoded into the latent space of a pretrained StyleGAN. Guided by our text-image similarity matching and face captioning-based text alignment, the textual latent code can be fed into the generator of a well-trained StyleGAN to produce diverse face images with high resolution (1024\u00d71024). Furthermore, our model inherently supports semantic face editing using text descriptions. Finally, experimental results quantitatively and qualitatively demonstrate the superior performance of our model.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3468964",
                        "name": "Xianxu Hou"
                    },
                    {
                        "authorId": "2119704887",
                        "name": "Xiaokang Zhang"
                    },
                    {
                        "authorId": "2111162634",
                        "name": "Yudong Li"
                    },
                    {
                        "authorId": "121640365",
                        "name": "Linlin Shen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These methods seek to find the semantically meaningful path in the latent space in either a supervised [11], [12], [14], [60], [61] or unsupervised manner [62], [63], then mov-"
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "ea91ab2b5f85c82d58566c4f877f0e500f887482",
                "externalIds": {
                    "DBLP": "journals/tmm/HouZLSM23",
                    "DOI": "10.1109/TMM.2022.3155903",
                    "CorpusId": 247281573
                },
                "corpusId": 247281573,
                "publicationVenue": {
                    "id": "10e76a35-58d6-443c-9683-fc16f2dd0a92",
                    "name": "IEEE transactions on multimedia",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Transactions on Multimedia",
                        "IEEE Trans Multimedia",
                        "IEEE trans multimedia"
                    ],
                    "issn": "1520-9210",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=6046"
                },
                "url": "https://www.semanticscholar.org/paper/ea91ab2b5f85c82d58566c4f877f0e500f887482",
                "title": "Lifelong Age Transformation With a Deep Generative Prior",
                "abstract": "In this paper, we consider the lifelong age progression and regression task, which requires to synthesize a person's appearance across a wide range of ages. We propose a simple yet effective learning framework to achieve this by exploiting the prior knowledge of faces captured by well-trained generative adversarial networks (GANs). Specifically, we first utilize a pretrained GAN to synthesize face images with different ages, with which we then learn to model the conditional aging process in the GAN latent space. Moreover, we also introduce a cycle consistency loss in the GAN latent space to preserve a person's identity. As a result, our model can reliably predict a person's appearance for different ages by modifying both shape and texture of the head. Both qualitative and quantitative experimental results demonstrate the superiority of our method over concurrent works. Furthermore, we demonstrate that our approach can also achieve high-quality age transformation for painting portraits and cartoon characters without additional age annotations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3468964",
                        "name": "Xianxu Hou"
                    },
                    {
                        "authorId": "2119704887",
                        "name": "Xiaokang Zhang"
                    },
                    {
                        "authorId": "2111208513",
                        "name": "Hanbang Liang"
                    },
                    {
                        "authorId": "121640365",
                        "name": "Linlin Shen"
                    },
                    {
                        "authorId": "2055357243",
                        "name": "Zhong Ming"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "37e16c4f1db63f1fa41413d90977ca9f383f628b",
                "externalIds": {
                    "CorpusId": 260814890
                },
                "corpusId": 260814890,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/37e16c4f1db63f1fa41413d90977ca9f383f628b",
                "title": "Co-optimized Region and Layer Selection",
                "abstract": "X \u223c N (\u03bc,\u03a3) denotes a Gaussian random vector X with mean \u03bc and covariance \u03a3, and I denotes the identity matrix. Out of the four well known latent spaces of the StyleGAN2 [20], denoted by Z,W,W and S or the StyleSpace, CORAL extensively utilizes the W space. MLP abbreviates multi-layer perceptron in this paper. For simplicity, f (l) is used to denote original features while f\u2217(l) denotes edited features at each convolutional layer of StyleGAN2. Correspondingly, I denotes the original image and I\u2217 denotes the edited image. \u27e8x,y\u27e9 represents the dot product between vectors x and y. \u2225x\u22252 is the Euclidean norm of vector x. For any score, \u2191 is used to denote that a higher value is more desirable. The definition for \u2193 follows along similar lines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "12640493",
                        "name": "A. Notation"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c8ec2107239aa10fa7f4f722b230fd90e3b12440",
                "externalIds": {
                    "CorpusId": 260881526
                },
                "corpusId": 260881526,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c8ec2107239aa10fa7f4f722b230fd90e3b12440",
                "title": "Domain Expansion of Image Generators - Supplementary Materials",
                "abstract": "Method. We start with an unconditional pretrained generator, specifically StyleGAN [13]. We then make the generator condition on a one-hot vector, using the architecture proposed by Karras et al. [11]. This change involves adding a single MLP layer, whose input is the one-hot vector. Its output is concatenated to the random latent code and then fed to the generator. The class-conditioned generator is trained in a similar protocol to our method. The source domain uses class c = 0, which is analogous to the base subspace. Whenever the 0th class is sampled, we apply the original loss Lsrc and the memory replay regularization (See Sec. 3.3). Formally, the loss describing this training is",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1702992975",
                        "name": "Yotam Nitzan"
                    },
                    {
                        "authorId": "3282136",
                        "name": "Micha\u00ebl Gharbi"
                    },
                    {
                        "authorId": "2109976035",
                        "name": "Richard Zhang"
                    },
                    {
                        "authorId": "2071929129",
                        "name": "Taesung Park"
                    },
                    {
                        "authorId": "1922024303",
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    },
                    {
                        "authorId": "2177801",
                        "name": "Eli Shechtman"
                    },
                    {
                        "authorId": "2115489684",
                        "name": "Adobe Research"
                    },
                    {
                        "authorId": "52272816",
                        "name": "A. Overview"
                    }
                ]
            }
        },
        {
            "contexts": [
                "On input, GANSliders [22] and GANSpace [40] support goal-driven input customization for GANs through visual feedforward sliders and semantic controls, respectively."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "26207b32143a13f5b1b3f6d6db297f44efc6a291",
                "externalIds": {
                    "CorpusId": 262093574
                },
                "corpusId": 262093574,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/26207b32143a13f5b1b3f6d6db297f44efc6a291",
                "title": "Cells, Generators, and Lenses: Design Framework for Object-Oriented Interaction with Large Language Models",
                "abstract": "By integrating these objects in their designs, designers can create interfaces that support users to flexibly create, modify, and link these objects to iterate and experiment with diverse configurations for the generative process of LLMs. ABSTRACT Large Language Models (LLMs) have become the backbone of numerous writing interfaces with the goal of supporting end-users across diverse writing tasks. While LLMs reduce the effort of manual writing, end-users may need to experiment and iterate with various generation configurations (e.g., inputs and model parameters) until results meet their goals. However, these interfaces are not \u2217 Minsuk",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1410132100",
                        "name": "Tae Soo Kim"
                    },
                    {
                        "authorId": "10741225",
                        "name": "Minsuk Chang"
                    },
                    {
                        "authorId": "49380061",
                        "name": "Yoonjoo Lee"
                    },
                    {
                        "authorId": "1800981",
                        "name": "Juho Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We derive disentanglement of latent space in Style-GAN2 [6] from GANSpace [5].",
                "We derive disentanglement of latent space in StyleGAN2 [6] from GANSpace [5]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "e56841e661a5efa896b72b25a90f0d3b89adc639",
                "externalIds": {
                    "CorpusId": 262534377
                },
                "corpusId": 262534377,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e56841e661a5efa896b72b25a90f0d3b89adc639",
                "title": "Supplementary Materials for \u201cLift3D: Synthesize 3D Training Data by Lifting 2D GAN to 3D Generative Radiance Field\u201d",
                "abstract": "We derive disentanglement of latent space in StyleGAN2 [6] from GANSpace [5]. We disentangle StyleGAN2 and identify the first eight layers latents as the latents that control the object pose and the other eight layers latents as the latents that control the attributes except object shape. In Lift3D, our goal is to annotate these latents with pose labels for lifting process. We first use Blender EEVEE engine [3] to render a ShapeNet [2] model under 200 different views P, ranging from 0\u2212 360\u25e6 in azimuth, and 0\u2212 20\u25e6 in elevation. The rendered images thus naturally contain accurate ground truth pose labels. With a fixed pretrained StyleGAN2 [6], we initialize 200 different latents z \u2208 R from Gaussian distribution Z \u2208 N (0, I). The latents z are mapped to w \u2208 R16\u00d7512 by the mapping network in StyleGAN2. We optimize the latents w using Adam [7] optimizer with learning rate of 1e-3 for 5000 iterations. The loss function is a simple L1 loss. After optimization, the first eight layers of latents w are annotated with \u201cpseudo\u201d pose labels, as the disentanglement and interpretation process is non-perfect.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2041715720",
                        "name": "Leheng Li"
                    },
                    {
                        "authorId": "2144951084",
                        "name": "Qing Lian"
                    },
                    {
                        "authorId": "2213771562",
                        "name": "Luozhou Wang"
                    },
                    {
                        "authorId": "49093932",
                        "name": "Ningning Ma"
                    },
                    {
                        "authorId": "104375063",
                        "name": "Yingke Chen"
                    },
                    {
                        "authorId": "102598073",
                        "name": "Hkust"
                    },
                    {
                        "authorId": "30124089",
                        "name": "Nio"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[29]), and it might be useful to develop similar methods for this approach."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3fd407101f881444a0098a291217e7a234bebab8",
                "externalIds": {
                    "CorpusId": 263272776
                },
                "corpusId": 263272776,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3fd407101f881444a0098a291217e7a234bebab8",
                "title": "Generative A.I. helps extract ecological meaning from the complex three dimensional shapes of bird bills",
                "abstract": "Data on the three dimensional shape of organismal morphology is becoming increasingly availability, and forms part of a new revolution in high-throughput phenomics that promises to help understand ecological and evolutionary processes that in\ufb02uence phenotypes at unprecedented scales. However, in order to meet the potential of this revolution we need new data analysis tools to deal with the complexity and heterogeneity of large-scale phenotypic data such as 3D shapes. In this study we explore the potential of generative Arti\ufb01cial Intelligence to help organise and extract meaning from complex 3D data. Speci\ufb01cally, we train a deep representational learning method known as DeepSDF on a dataset of 3D scans of the bills of 2,020 bird species. The model is designed to learn a continuous vector representation of 3D shapes, along with a \u2018decoder\u2019 function, that allows the transformation from this vector space to the original 3d morphological space. We \ufb01nd that approach successfully learns coherent representations: particular directions in latent space are associated with discernible morphological meaning (such as elongation, \ufb02attening, etc.). More importantly, learned latent vectors have ecological meaning as shown by their ability to predict the trophic niche of the bird each bill belongs to with a high degree of accuracy. Unlike existing 3D morphometric techniques, this method has very little requirements for human supervised tasks such as landmark placement, increasing it accessibility to labs with fewer labour resources. It has fewer strong assumptions than alternative dimension reduction techniques such as PCA. The computational requirements for training the model, while substantial, is still within the reasonable reach of most researchers, with a \u02dc2000 shape model taking just over 2 days to train on only a single current generation consumer-level GPU. Once trained, 3D morphology predictions can be made from latent vectors very computationally cheaply.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "50268705",
                        "name": "Russell Dinnage"
                    },
                    {
                        "authorId": "1491322462",
                        "name": "Marian Kleineberg"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our current work seeks to design GANspire using a specific method, called diffractive [10], which enables to include joint art and health perspectives in deep learning and interaction prototyping.",
                "We proposed to call GANspire this deep learning tool, which enables to generate expressive breathing waveforms for art and health applications (see workflow in Figure 1).",
                "Ethical Implications\nThe breathing waveform dataset scraped for training GANspire was fully anonymised.",
                "We used TouchDesigner to map pressure waveform values generated by GANspire to that of the mechanical ventilator.",
                "We implemented GANspace, a technique for analysing and defining interpretable controls for image GANs [12], within our trained GAN.",
                "We eventually applied GANspire to create a soft inflatable object, whose inflatings and deflatings would be controlled by our generative model.",
                "Current work explores other creative applications of GANspire to control temporal evolution of other media, such as sound or image, i.e., making them \u201cbreathe\u201d, as well as other shapes, materials, and actuators to fabricate a more advanced version of our soft breathing object.",
                "One of the final aims of our work would be to use GANspire to control our soft breathing object in clinical settings, typically, observing how expressive breathings could produce empathic interactions between humans and the behavioural object, potentially appeasing the minds of patients suffering from chronic respiratory diseases [15].",
                "We hope to attend this year\u2019s workshop to discuss such socio-cultural issues with practitioners from the field of machine learning for creativity and design, along with artistic, design, and technical aspects of our ongoing work with GANspire."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "be5e00b27052002cf8c484c6e84e412c1a3cefa6",
                "externalIds": {
                    "CorpusId": 246900653
                },
                "corpusId": 246900653,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/be5e00b27052002cf8c484c6e84e412c1a3cefa6",
                "title": "GANspire: Generating Breathing Waveforms for Art-Health Applications",
                "abstract": "This paper introduces GANspire, a deep learning tool that generates expressive breathing waveforms for art and health applications. We describe our ongoing work and contributions, which include the development of a generative model of breathing pressure waveforms, the participatory design of an interface for creative exploration of breathing generation, and the artistic application of our tool to the creation of a soft breathing object. Breathing is an automatic vital function allowing gas exchanges between organisms and their environment [1]. In humans, breathing also has emotional and behavioral dimensions. Indeed, it can reveal inner states to others through visible and audible inhale-exhale movements, it serves as a conduit for verbal and non-verbal communication, and it can convey connectedness between individuals, with their environment and with themselves, e.g., through breathing synchronisation or mindfulness. Likewise, breathing can both generate and express empathic resonance, leading to empathic concern or empathic distress. While several works attempted to endow machines with empathic breathing attributes, either within art installations [2,3] or as means for health interventions [4,5], almost all of them relied on elementary models of breathing waveforms, e.g., a sine wave with varying amplitude and rate, which means overlooking the role of breathing variability and of fine-grained individual differences in breathing (personnalit\u00e9 ventilatoire [6,7]) in conveying expressive and empathic cues in human-human interaction, and by extension, in further human-machine interaction. As an interdisciplinary group of artists, designers, researchers, and clinicians, we were interested in exploring three entangled issues related to such breathing expressiveness. First, we wanted to leverage the representational power of deep learning to create a generative model of breathing pressure waveforms that accounts for fine-grained, individual variations of breathing among humans. Second, we needed to design a tool that facilitates exploration of the generative model for pulmonologists interested in better understanding breathing expressiveness. Third, we wished to apply our tool to the creation of a soft inflatable object by subtly controlling its inflatings and deflatings with generative deep learning, thus exploring breathing as a new form of creative and behavioural output [8,9], possibly vector of empathy in human-machine interaction. We proposed to call GANspire this deep learning tool, which enables to generate expressive breathing waveforms for art and health applications (see workflow in Figure 1). Our current work seeks to design GANspire using a specific method, called diffractive [10], which enables to include joint art and health perspectives in deep learning and interaction prototyping. The present paper details our ongoing contributions, which include the development of a generative model of breathing waveforms based on a generative adversarial network (GAN) algorithm, the design of an interface for creative exploration of breathing generation based on participatory design with pulmonologists, and the artistic application of our tool to the creation of a soft breathing object based on a mechanical ventilator and an artificial lung. \u2217hugo.scurto@ensad.fr 5th Workshop on Machine Learning for Creativity and Design at NeurIPS 2021, Sydney, Australia. Figure 1: GANspire is a deep learning tool that generates expressive breathing waveforms to control temporal evolution of media, such as sound, image, or soft physical objects. We started by crafting a generative model of breathing, approaching machine learning as a creative material, i.e., privileging qualitative aspects of signal generation over quantitative aspects of signal modelling. We scraped a dataset of breathing pressure waveforms, harvested in previous physiology studies from twenty healthy individuals at rest, during exercise, or during sleep, amounting to six hours of breathing data sampled at 1 kHz. We then trained a WaveGAN, an audio GAN developed for raw waveform generation [11], over this dataset. We found that a DCGAN loss and a 10-dimensional latent space produced the best results in reproducing fine-grained variations of breathing, based on pulmonologists\u2019 qualitative observations of randomly-sampled, 16-second slices of breathing waveform (see Appendix A). Current work seeks to build a dataset of expressive waveforms from actors, musicians, and performers who have expertise in breathing, thus improving diversity in the generative abilities of the model. We then sought to design an interface to facilitate exploration of the generative model, exploring whether control parameters related to breathing expressiveness could be discovered. We implemented GANspace, a technique for analysing and defining interpretable controls for image GANs [12], within our trained GAN. We found that performing principal component analysis at the first intermediate network layer enabled to capture most of the variability in breathing generation, based on quantitative analysis of variance of components. We used Marcelle, a web-based toolkit for designing interactive machine learning workflows [13], to code a parametric interface allowing exploration of the resulting generative abilities of our trained GAN (see Appendix B). Current work focuses on evaluating these control parameters in hands-on workshops with respiratory physicians, intensivists and physiologists to jointly refine interface and algorithmic design. We eventually applied GANspire to create a soft inflatable object, whose inflatings and deflatings would be controlled by our generative model. Specifically, we created a prototype behavioural object, by using a mechanical ventilator and an artificial lung (see video in Supplemental Material). We used TouchDesigner to map pressure waveform values generated by GANspire to that of the mechanical ventilator. We found that fine-grained variations encapsulated by our trained model somewhat expressively materialised within the inflatings and deflatings of our prototype object. Different control parameter values yielded a diversity of breathing movements, whose variations provided the object with rich behavioural aesthetics, sometimes appeasing, sometimes uncanny, but in arguably all cases, life-like [14]. On the other hand, the elementary model of breathing implemented as standard in the mechanical ventilator produced a much narrower range of movements, with arguably less life-like qualities. Current work explores other creative applications of GANspire to control temporal evolution of other media, such as sound or image, i.e., making them \u201cbreathe\u201d, as well as other shapes, materials, and actuators to fabricate a more advanced version of our soft breathing object. One of the final aims of our work would be to use GANspire to control our soft breathing object in clinical settings, typically, observing how expressive breathings could produce empathic interactions between humans and the behavioural object, potentially appeasing the minds of patients suffering from chronic respiratory diseases [15]. We believe that such a use case would raise new issues related to the application of creative machine learning to the domain of health. For example, what would be the social impact of creating artificially-empathic machines for both patient and carer communities? Alternatively, are there any advantages in using breathing, as a form of biosignal, to train a generative model for health or well-being, instead of scraping culturally-richer materials, such as music? We hope to attend this year\u2019s workshop to discuss such socio-cultural issues with practitioners from the field of machine learning for creativity and design, along with artistic, design, and technical aspects of our ongoing work with GANspire.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "21196663",
                        "name": "Hugo Scurto"
                    },
                    {
                        "authorId": "2046048",
                        "name": "Baptiste Caramiaux"
                    },
                    {
                        "authorId": "2967634",
                        "name": "T. Similowski"
                    },
                    {
                        "authorId": "2879845",
                        "name": "Samuel Bianchini"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Additional quantitative results In addition to the experimental evaluation in the main paper, we provide accuracy for the synthesis of specific hair tone using PGAN 1: [2]: 91%, [5]: 97%, Ours: 99%.",
                "We set \u03b1 := 1.0 for GANSpace to generate the edited latent codes in the target attribute.",
                "GANSpace We train GANSpace [2] on the pre-trained ProgressiveGAN that is used for the other methods."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "c5835ce1521a08ab2d0c5543b2070ecfc2e6cc0c",
                "externalIds": {
                    "CorpusId": 250602119
                },
                "corpusId": 250602119,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c5835ce1521a08ab2d0c5543b2070ecfc2e6cc0c",
                "title": "Cluster-guided Image Synthesis with Unconditional Models - Supplementary material",
                "abstract": "As shown in [4], maximizing the likelihood of the data (equivalent to minimizing DKL(pdata|pmodel )) penalizes assigning low probability to data samples, while the alternative DKL(pmodel|pdata ) penalizes implausible samples, which can lead to modedropping in GANs [1]. This is detrimental when modeling the conditional distribution of latent codes (Fig.1b). On the other hand, [4] show that under mild assumptions IMLE maximizes the likelihood of the data. To test our model choice, we replace IMLE with GAN and get a reduced facial pose accuracy of 87%. We also test VAE as an alternative likelihood-based model, which also drops accuracy to 90%. Contrary to IMLE, VAE and GAN focus on a subset of codes z and fail to maintain attribute consistency.",
                "year": 2022,
                "authors": []
            }
        },
        {
            "contexts": [
                "Recent advances in GAN inversion techniques [1,5,6,30] enable manipulation of real-world images [2,3,4,20,31,41].",
                "[20] suggest a PCA-based approach applied onto the latent space of StyleGAN [22] to identify key control directions for semantic edits such as aging, lighting, etc."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "7acd250ac31fadaaa3d01d42c0e1c9e60c07028a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-14512",
                    "DOI": "10.48550/arXiv.2203.14512",
                    "CorpusId": 247761905
                },
                "corpusId": 247761905,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7acd250ac31fadaaa3d01d42c0e1c9e60c07028a",
                "title": "Encode-in-Style: Latent-based Video Encoding using StyleGAN2",
                "abstract": ". We propose an end-to-end facial video encoding approach that facilitates data-efficient high-quality video re-synthesis by optimizing low-dimensional edits of a single Identity-latent. The approach builds on StyleGAN2 image inversion and multi-stage non-linear latent-space editing to generate videos that are nearly comparable to input videos. It economically captures face identity, head-pose, and complex facial motions at fine levels, and thereby bypasses training and person modeling which tend to hamper many re-synthesis approaches. The approach is designed with maximum data efficiency, where a single W + latent and 35 parameters per frame enable high-fidelity video rendering. This pipeline can also be used for puppeteering (i.e., motion transfer)",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "31668191",
                        "name": "T. Oorloff"
                    },
                    {
                        "authorId": "1964574",
                        "name": "Y. Yacoob"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We use GANSpace [21] and StyleCLIP [40] for finding an editing direction \u03b4w in the W latent space.",
                "After inversion, we can edit the inverted code by traversing semantically meaningful directions computed using supervised [9, 25, 47] or unsupervised approaches [17,21,41,48,52].",
                "We use GANSpace [21] and StyleCLIP [40] for finding an editing direction \u03b4w+ in the W+ latent space."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "0b4e6c7584a85f082ad70fe0ca778ab4467b273b",
                "externalIds": {
                    "CorpusId": 248961082
                },
                "corpusId": 248961082,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0b4e6c7584a85f082ad70fe0ca778ab4467b273b",
                "title": "Spatially-Adaptive Multilayer GAN Inversion and Editing",
                "abstract": "Existing GAN inversion and editing methods are well suited for only a target images that contain aligned objects with a clean background, such as portraits and animal faces, but often struggle for more difficult categories with complex scene layouts and object occlusions, such as cars, animals, and out-door images. We propose a new method to invert and edit such complex images in the latent space of GANs, such as StyleGAN2. Our key idea is to explore inversion with a collection of layers, spatially adapting the inversion process to the difficulty of the image. We learn to predict the \u201cinvertibility\u201d of different image segments and project each segment into a latent layer. Easier regions can be inverted into an earlier layer in the generator\u2019s latent space, while more challenging regions can be inverted into a later feature space. Experiments show that our method obtains better inversion results compared to the recent approaches on complex categories, while maintaining downstream editability.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2065083806",
                        "name": "Gaurav Parmar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The rotation and smile directions are obtained from InterFaceGAN [9], whereas other directions are borrowed from GANSpace [3].",
                "The smile direction is obtained from InterFaceGAN [9], whereas other directions are borrowed from GANSpace [3].",
                "The editing directions are obtained from GANSpace [3]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "563186d97fb0a41ab17ca31dbf257694223b8972",
                "externalIds": {
                    "CorpusId": 249878129
                },
                "corpusId": 249878129,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/563186d97fb0a41ab17ca31dbf257694223b8972",
                "title": "HyperInverter: Improving StyleGAN Inversion via Hypernetwork \u2014 Supplementary Material \u2014",
                "abstract": "In this supplementary material, we first discuss the potential negative social impacts of our research. Then, we present another user study for the churches domain. This survey and the previous one presented in the main paper for human faces show that our method works well for both domains under human assessment. Moreover, we present the difference map visualizations to analyze the image residuals from our Phase II update. We also include additional dataset and implementation details for reproducibility. Finally, we provide extensive visual examples for further qualitative evaluation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2142664005",
                        "name": "Tan M. Dinh"
                    },
                    {
                        "authorId": "2171416418",
                        "name": "Anh Tuan"
                    },
                    {
                        "authorId": "2171464324",
                        "name": "Tran Rang Nguyen"
                    },
                    {
                        "authorId": "143807806",
                        "name": "Binh-Son Hua"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020], discovering interpretability [H\u00e4rk\u00f6nen et al., 2020], and fine-tuning [Mo et al.",
                "Examples include improving fairness [Tan et al., 2020, Karakas et al., 2022], rule rewriting [Bau et al., 2020], discovering interpretability [H\u00e4rk\u00f6nen et al., 2020], and fine-tuning [Mo et al., 2020, Li et al., 2020, Zhao et al., 2020]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "afe54827b48eaeaebd378693f2c8a07cbc9f3d60",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-14389",
                    "DOI": "10.48550/arXiv.2206.14389",
                    "CorpusId": 250113382
                },
                "corpusId": 250113382,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/afe54827b48eaeaebd378693f2c8a07cbc9f3d60",
                "title": "Forgetting Data from Pre-trained GANs",
                "abstract": "Large pre-trained generative models are known to occasionally provide samples that may be undesirable for various reasons. The standard way to mitigate this is to re-train the models differently. In this work, we take a different, more compute-friendly approach and investigate how to post-edit a model after training so that it forgets certain kinds of samples. We provide three different algorithms for GANs that differ on how the samples to be forgotten are described. Extensive evaluations on real-world image datasets show that our algorithms are capable of forgetting data while retaining high generation quality at a fraction of the cost of full re-training",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "33470361",
                        "name": "Zhifeng Kong"
                    },
                    {
                        "authorId": "38120884",
                        "name": "Kamalika Chaudhuri"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[11] performed unsupervised learning, mainly PCA, on the latent space in StyleGAN as well as feature layers in BigGAN to find the directions of some editable features."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "9c23adb444f33c8cfd73f4f63431ca4cfb698945",
                "externalIds": {
                    "CorpusId": 250262671
                },
                "corpusId": 250262671,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9c23adb444f33c8cfd73f4f63431ca4cfb698945",
                "title": "Towards Social Video Verification to Combat Deepfakes via Deep Learning",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118831303",
                        "name": "Chen Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Finally, H\u00e4rk\u00f6nen et al. (2020) shows that keeping principal components from a PCA in the intermediate latent space of StyleGAN captures most variance of the model, which implies that GANs compress the high-dimensional latent space."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "200581160e1a14d5b8e44f4e52c76fb40ca5b7e4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-10541",
                    "DOI": "10.48550/arXiv.2207.10541",
                    "CorpusId": 250920518
                },
                "corpusId": 250920518,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/200581160e1a14d5b8e44f4e52c76fb40ca5b7e4",
                "title": "Optimal precision for GANs",
                "abstract": "When learning disconnected distributions, Generative adversarial networks (GANs) are known to face model misspeci\ufb01cation. Indeed, a continuous mapping from a unimodal latent distribution to a disconnected one is impossible, so GANs necessarily generate samples outside of the support of the target distribution. This raises a fundamental question: what is the latent space partition that minimizes the measure of these areas? Building on a recent result of geometric measure theory, we prove that an optimal GANs must structure its latent space as a \u2019simplicial cluster\u2019 - a Voronoi partition where cells are convex cones - when the dimension of the latent space is larger than the number of modes. In this con\ufb01guration, each Voronoi cell maps to a distinct mode of the data. We derive both an upper and a lower bound on the optimal precision of GANs learning disconnected manifolds. Interestingly, these two bounds have the same order of decrease: \u221a log m , m being the number of modes. Finally, we perform several experiments to exhibit the geometry of the latent space and experimentally show that GANs have a geometry with similar properties to the theoretical one.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51229923",
                        "name": "Thibaut Issenhuth"
                    },
                    {
                        "authorId": "40901191",
                        "name": "Ugo Tanielian"
                    },
                    {
                        "authorId": "143716734",
                        "name": "J\u00e9r\u00e9mie Mary"
                    },
                    {
                        "authorId": "145897899",
                        "name": "David Picard"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While aforementioned 2D GANs [15], [17], [22], [23] allow explicit head pose control to some extent, they fail to guarantee appearance consistency, leading to inconsistent identity or facial attributes when viewed from vastly different angles.",
                "correlates with the manipulated attribute [12], [13], [14], [15], [16], [16], [17], [18]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "25c6c02f2db2e90e5660a92fcd134cd33addc8ce",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-05434",
                    "DOI": "10.48550/arXiv.2209.05434",
                    "CorpusId": 252199614
                },
                "corpusId": 252199614,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/25c6c02f2db2e90e5660a92fcd134cd33addc8ce",
                "title": "Explicitly Controllable 3D-Aware Portrait Generation",
                "abstract": "\u2014In contrast to the traditional avatar creation pipeline which is a costly process, contemporary generative approaches directly learn the data distribution from photographs. While plenty of works extend unconditional generative models and achieve some levels of controllability, it is still challenging to ensure multi-view consistency, especially in large poses. In this work, we propose a network that generates 3D-aware portraits while being controllable according to semantic parameters regarding pose, identity, expression and illumination. Our network uses neural scene representation to model 3D-aware portraits, whose generation is guided by a parametric face model that supports explicit control. While the latent disentanglement can be further enhanced by contrasting images with partially different attributes, there still exists noticeable inconsistency in non-face areas, e.g. , hair and background, when animating expressions. We solve this by proposing a volume blending strategy in which we form a composite output by blending dynamic and static areas, with two parts segmented from the jointly learned semantic \ufb01eld. Our method outperforms prior arts in extensive experiments, producing realistic portraits with vivid expression in natural lighting when viewed from free viewpoints. It also demonstrates generalization ability to real images as well as out-of-domain data, showing great promise in real applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152949334",
                        "name": "Junshu Tang"
                    },
                    {
                        "authorId": "145803569",
                        "name": "Bo Zhang"
                    },
                    {
                        "authorId": "2157857267",
                        "name": "Binxin Yang"
                    },
                    {
                        "authorId": "2146320438",
                        "name": "Ting Zhang"
                    },
                    {
                        "authorId": "47514557",
                        "name": "Dong Chen"
                    },
                    {
                        "authorId": "2149343311",
                        "name": "Lizhuang Ma"
                    },
                    {
                        "authorId": "1716835",
                        "name": "Fang Wen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent works on image manipulation show that visual attributes can be controlled for more complex images [16, 18, 15, 17] and that generative models can be applied to larger datasets such as ImageNet [27].",
                "The latent space can also be used as a way to control the generation process, for instance to edit images [15, 16, 17]."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "be6516dda2fd5ca59c0a182e49dc95303a8dd2e0",
                "externalIds": {
                    "DBLP": "conf/ijcai/Le-CozHA22",
                    "CorpusId": 252760530
                },
                "corpusId": 252760530,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/be6516dda2fd5ca59c0a182e49dc95303a8dd2e0",
                "title": "Leveraging generative models to characterize the failure conditions of image classifiers",
                "abstract": "We address in this work the question of identifying the failure conditions of a given image classifier. To do so, we exploit the capacity of producing controllable distributions of high quality image data made available by recent Generative Adversarial Networks (StyleGAN2): the failure conditions are expressed as directions of strong performance degradation in the generative model latent space. This strategy of analysis is used to discover corner cases that combine multiple sources of corruption, and to compare in more details the behavior of different classifiers. The directions of degradation can also be rendered visually by generating data for better interpretability. Some degradations such as image quality can affect all classes, whereas other ones such as shape are more class-specific. The approach is demonstrated on the MNIST dataset that has been completed by two sources of corruption: noise and blur, and shows a promising way to better understand and control the risks of exploiting Artificial Intelligence components for safety-critical applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2039553828",
                        "name": "Adrien Le Coz"
                    },
                    {
                        "authorId": "1924996",
                        "name": "S. Herbin"
                    },
                    {
                        "authorId": "7167973",
                        "name": "F. Adjed"
                    }
                ]
            }
        },
        {
            "contexts": [
                "DE-AC52-07NA27344, Lawrence Livermore National Security, LLC.and was supported by the LLNLLDRD Program under Project No. 21-ERD-012.\nof image synthesis and manipulation tasks (Karras et al., 2019; Ha\u0308rko\u0308nen et al., 2020; Brock et al., 2019; Song et al., 2021).",
                "A desired property of any GAN inversion algorithm is that the latent codes can be semantically manipulated for downstream applications such as style transfer and attribute discovery (Voynov & Babenko, 2020; Ha\u0308rko\u0308nen et al., 2020; Plumerault et al., 2020; Jahanian et al., 2019; Wang et al., 2021).",
                "of image synthesis and manipulation tasks (Karras et al., 2019; H\u00e4rk\u00f6nen et al., 2020; Brock et al., 2019; Song et al., 2021)."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "580032e97d0c8840bc67e7cba6d040cf6508231d",
                "externalIds": {
                    "DBLP": "conf/icml/SubramanyamNNST22",
                    "CorpusId": 250340713
                },
                "corpusId": 250340713,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/580032e97d0c8840bc67e7cba6d040cf6508231d",
                "title": "Improved StyleGAN-v2 based Inversion for Out-of-Distribution Images",
                "abstract": "Inverting an image onto the latent space of pre-trained generators, e.g., StyleGAN-v2, has emerged as a popular strategy to leverage strong image priors for ill-posed restoration. Several studies have showed that this approach is effective at inverting images similar to the data used for training. However, with out-of-distribution (OOD) data that the generator has not been ex-posed to, existing inversion techniques produce sub-optimal results. In this paper, we propose SPHInX (StyleGAN with Projection Heads for Inverting X), an approach for accurately embedding OOD images onto the StyleGAN latent space. SPHInX optimizes a style projection head using a novel training strategy that imposes a vicinal regularization in the StyleGAN latent space. To further enhance OOD inversion, SPHInX can ad-ditionally optimize a content projection head and noise variables in every layer. Our empirical studies on a suite of OOD data show that, in addition to producing higher quality reconstructions over the state-of-the-art inversion techniques, SPHInX is effective for ill-posed restoration tasks while offering semantic editing capabilities.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2175280217",
                        "name": "Rakshith Subramanyam"
                    },
                    {
                        "authorId": "51881215",
                        "name": "V. Narayanaswamy"
                    },
                    {
                        "authorId": "1658781889",
                        "name": "M. Naufel"
                    },
                    {
                        "authorId": "49413461",
                        "name": "A. Spanias"
                    },
                    {
                        "authorId": "1744175",
                        "name": "Jayaraman J. Thiagarajan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For manipulating facial semantics, most works focus on changing the attributes [36, 32, 14]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "956c82851915f1b949f9336d56cc15f85321cc68",
                "externalIds": {
                    "DBLP": "conf/eccv/LuoZHCTWY22",
                    "DOI": "10.1007/978-3-031-19787-1_17",
                    "CorpusId": 253120732
                },
                "corpusId": 253120732,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/956c82851915f1b949f9336d56cc15f85321cc68",
                "title": "StyleFace: Towards Identity-Disentangled Face Generation on Megapixels",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108803212",
                        "name": "Yucheng Luo"
                    },
                    {
                        "authorId": "2146280986",
                        "name": "Junwei Zhu"
                    },
                    {
                        "authorId": "37391748",
                        "name": "Keke He"
                    },
                    {
                        "authorId": "2061528",
                        "name": "Wenqing Chu"
                    },
                    {
                        "authorId": "144970872",
                        "name": "Ying Tai"
                    },
                    {
                        "authorId": "1978245",
                        "name": "Chengjie Wang"
                    },
                    {
                        "authorId": "3063894",
                        "name": "Junchi Yan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Despite there are many methods aiming to find out these relations [8,15], most of them are too costly to be a practical option."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "948a5e0e549b50ea6313591b6b45792f57fc4589",
                "externalIds": {
                    "DBLP": "conf/eccv/YanGGWYL22",
                    "DOI": "10.1007/978-3-031-19806-9_28",
                    "CorpusId": 253099798
                },
                "corpusId": 253099798,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/948a5e0e549b50ea6313591b6b45792f57fc4589",
                "title": "Unbiased Manifold Augmentation for Coarse Class Subdivision",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50736086",
                        "name": "Baoming Yan"
                    },
                    {
                        "authorId": "2054043222",
                        "name": "Ke Gao"
                    },
                    {
                        "authorId": "2113838080",
                        "name": "Bo Gao"
                    },
                    {
                        "authorId": "2144736891",
                        "name": "Lin Wang"
                    },
                    {
                        "authorId": "2109761014",
                        "name": "Jiang Yang"
                    },
                    {
                        "authorId": "2109349122",
                        "name": "Xiaobo Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These techniques include unsupervised exploration [39], learning linear SVM models [34], principle component analysis on the latent codes [17], and k-means clustering of the activation features [10].",
                "Recent studies [34,17,44,35] have shown that it is possible to control semantic attributes of synthetic images by manipulating the latent space of a pre-trained GAN.",
                "Given a latent code w, let us consider that we have a modified latent code w\u0303 = w+\u2206w corresponding to a desired editing, obtained from a latent space editing method [34,17,35]."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "fee95516d60e08dd5e9374888b3e8c98f77164bc",
                "externalIds": {
                    "DBLP": "conf/eccv/YaoNGH22",
                    "DOI": "10.1007/978-3-031-19784-0_34",
                    "CorpusId": 253270057
                },
                "corpusId": 253270057,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/fee95516d60e08dd5e9374888b3e8c98f77164bc",
                "title": "A Style-Based GAN Encoder for High Fidelity Reconstruction of Images and Videos",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115586564",
                        "name": "Xu Yao"
                    },
                    {
                        "authorId": "1902919",
                        "name": "A. Newson"
                    },
                    {
                        "authorId": "1796594",
                        "name": "Y. Gousseau"
                    },
                    {
                        "authorId": "1806880",
                        "name": "P. Hellier"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "229bcba7473a82a23744884dc3c5136b873e7b39",
                "externalIds": {
                    "CorpusId": 253525601
                },
                "corpusId": 253525601,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/229bcba7473a82a23744884dc3c5136b873e7b39",
                "title": "A Binary Classifier Architecture",
                "abstract": "For all experiments, we train and use binary classifiers with the following architecture: We implement feedforward neural networks with 10 main layers, where each main layer consists of a convolution layer followed by batch normalization and ReLU activation. Additionally, we add pooling and dropout layers between main layers in an alternating fashion such that there are a total of 5 pooling layers and 3 dropout layers, where the dropout percentage is set to 0.5.",
                "year": 2022,
                "authors": []
            }
        },
        {
            "contexts": [
                "This is in contrast to past traversal studies [4, 14, 16, 28, 31] that found global linear directions to suffice for simple scalar attributes.",
                "Latent spaces of deep generative networks like generative adversarial networks (GANs) [13, 17, 18, 29] and variational autoencoders (VAEs) [19] are known to organize semantic attributes into disentangled subspaces without supervision [14, 16, 29, 37, 39].",
                "Most propose finding global linear directions correlated with scalar attributes of interest [4, 14, 12, 28, 31, 38, 43]."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "result"
            ],
            "citingPaper": {
                "paperId": "702d94eca868377006a3c7de941442229ddd0c9a",
                "externalIds": {
                    "DBLP": "conf/eccv/BalakrishnanGMP22",
                    "DOI": "10.1007/978-3-031-19790-1_31",
                    "CorpusId": 253121306
                },
                "corpusId": 253121306,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/702d94eca868377006a3c7de941442229ddd0c9a",
                "title": "Rayleigh EigenDirections (REDs): Nonlinear GAN Latent Space Traversals for Multidimensional Features",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47231927",
                        "name": "Guha Balakrishnan"
                    },
                    {
                        "authorId": "2176869",
                        "name": "Raghudeep Gadde"
                    },
                    {
                        "authorId": "1384255355",
                        "name": "Aleix M. Martinez"
                    },
                    {
                        "authorId": "1690922",
                        "name": "P. Perona"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Related to StyleGAN, the editing frameworks [16,33,35,4] analyze the linear and non-linear nature of the underlying W and W+ spaces."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "0e9997a18e97f9654331cd5a7c11ba574a48d881",
                "externalIds": {
                    "DBLP": "conf/eccv/ZhuAFW22",
                    "DOI": "10.1007/978-3-031-19787-1_37",
                    "CorpusId": 253120808
                },
                "corpusId": 253120808,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/0e9997a18e97f9654331cd5a7c11ba574a48d881",
                "title": "HairNet: Hairstyle Transfer with Pose Changes",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "100516590",
                        "name": "Peihao Zhu"
                    },
                    {
                        "authorId": "94395014",
                        "name": "Rameen Abdal"
                    },
                    {
                        "authorId": "145685385",
                        "name": "John C. Femiani"
                    },
                    {
                        "authorId": "1798011",
                        "name": "Peter Wonka"
                    }
                ]
            }
        },
        {
            "contexts": [
                "By identifying the principal components of the latent space, GANSpace [8] identifies important factors of variation."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "eabaf0003948824e3817b076bb75a3cc5eb0efa5",
                "externalIds": {
                    "DBLP": "conf/bmvc/ChungZSWC22",
                    "CorpusId": 253736279
                },
                "corpusId": 253736279,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/eabaf0003948824e3817b076bb75a3cc5eb0efa5",
                "title": "StyleFaceUV: a 3D Face UV Map Generator for View-Consistent Face Image Synthesis",
                "abstract": "Recent deep image generation models, such as StyleGAN2, face challenges to produce high-quality 2D face images with multi-view consistency. We address this issue by proposing an approach for generating detailed 3D faces using a pre-trained StyleGAN2 model. Our method estimates the 3D Morphable Model (3DMM) coefficients directly from the StyleGAN2\u2019s stylecode. To add more details to the produced 3D face models, we train a generator to produce two UV maps: a diffuse map to give the model a more faithful appearance and a generalized displacement map to add geometric details to the model. To achieve multi-view consistency, we also add a symmetric view image to recover information regarding the invisible side of a single image. The generated detailed 3D face models allow for consistent changes in viewing angles, expressions, and lighting conditions. Experimental results indicate that our method outperforms previous approaches both qualitatively and quantitatively.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2191617799",
                        "name": "Wei-Chieh Chung"
                    },
                    {
                        "authorId": "5415498",
                        "name": "Yu-Ting Wu"
                    },
                    {
                        "authorId": "143708263",
                        "name": "Yung-Yu Chuang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "29247d8e9cd69a3400c6b95ad14037e566fa29d5",
                "externalIds": {
                    "DOI": "10.22215/etd/2022-14942",
                    "CorpusId": 247992350
                },
                "corpusId": 247992350,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/29247d8e9cd69a3400c6b95ad14037e566fa29d5",
                "title": "Exploration of Latent Spaces of 3D Shapes via Isomap and Inverse Mapping",
                "abstract": "In this thesis, we introduce a method for exploring a latent space of 3D shapes learned by a deep neural network. The main idea of our method is to enable the exploration of the latent space through the navigation of a 2D embedding. The method is based on a combination of Isomap dimensionality reduction and an inverse mapping function. Specifically, given a dataset of 3D shapes, we first train an autoencoder neural network to learn a latent representation for the dataset. Then, we reduce the dimensionality of the latent space to two dimensions with the Isomap method. The 2D embedding of the latent space allows a user to easily navigate through the embedding and sample new points. Our method then translates the sampled points back into latent vectors with an inverse mapping function, while the latent vectors are decoded by the neural network into 3D shapes that can be inspected by the user. The inverse mapping is made possible by posing it as a radial basis function (RBF) scattered interpolation problem. We demonstrate with qualitative experiments that our exploration method has advantages compared to alternative approaches such as interpolation and principal component analysis. We also show with quantitative experiments that our method enables a meaningful exploration of the latent space.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "107662583",
                        "name": "Raghad Rowaida"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Existing methods for semantic control discovery include large amounts of annotated data, manual examination[10, 30, 36], or pre-trained classifiers[30].",
                "The superior properties of W space have attracted a host of researchers to develop advanced GAN inversion techniques[1, 2, 10, 29, 33, 34, 39]"
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "8a16585b9778cba9d10ff89b3e62783de6a6fff7",
                "externalIds": {
                    "DBLP": "conf/accv/LouLL22",
                    "DOI": "10.1007/978-3-031-26293-7_5",
                    "CorpusId": 254146156
                },
                "corpusId": 254146156,
                "publicationVenue": {
                    "id": "a8f26d13-e373-4e48-b57b-ef89bf48f4db",
                    "name": "Asian Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Asian Conf Comput Vis",
                        "ACCV"
                    ],
                    "url": "http://www.cvl.iis.u-tokyo.ac.jp/afcv/"
                },
                "url": "https://www.semanticscholar.org/paper/8a16585b9778cba9d10ff89b3e62783de6a6fff7",
                "title": "TeCM-CLIP: Text-Based Controllable Multi-attribute Face Image Manipulation",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2193054388",
                        "name": "Xudong Lou"
                    },
                    {
                        "authorId": "7894567",
                        "name": "Yiguang Liu"
                    },
                    {
                        "authorId": "1502892851",
                        "name": "Xuwei Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Also, we have compared our method with state-of-the-art methods [2, 3, 21, 33] on face attribute manipulation [15, 22].",
                "9) with off-the-shelf GAN manipulation approaches [15, 22, 27]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "e3b65861940d8142b5a1b57fc3bf6dde61a6979a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-09262",
                    "DOI": "10.48550/arXiv.2212.09262",
                    "CorpusId": 254854589
                },
                "corpusId": 254854589,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e3b65861940d8142b5a1b57fc3bf6dde61a6979a",
                "title": "Photo-Realistic Out-of-domain GAN inversion via Invertibility Decomposition",
                "abstract": "The \ufb01delity of Generative Adversarial Networks (GAN) inversion is impeded by Out-Of-Domain (OOD) areas (e.g., background, accessories) in the image. Detecting the OOD areas beyond the generation ability of the pretrained model and blending these regions with the input image can enhance \ufb01delity. The \u201cinvertibility mask\u201d \ufb01gures out these OOD areas, and existing methods predict the mask with the reconstruction error. However, the estimated mask is usually inaccurate due to the in\ufb02uence of the reconstruction error in the In-Domain (ID) area. In this paper, we propose a novel framework that enhances the \ufb01delity of human face inversion by designing a new module to decompose the input images to ID and OOD partitions with invertibility masks. Unlike previous works, our invertibility detector is simul-taneously learned with a spatial alignment module. We it-eratively align the generated features to the input geome-try and reduce the reconstruction error in the ID regions. Thus, the OOD areas are more distinguishable and can be precisely predicted. Then, we improve the \ufb01delity of our results by blending the OOD areas from the input image with the ID GAN inversion results. Our method produces photo-realistic results for real-world human face image inversion and manipulation. Extensive experiments demonstrate our method\u2019s superiority over existing methods in the quality of GAN inversion and attribute manipulation. Our code is available at: https://github.com/AbnerVictor/OOD-GAN-inversion",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2150440804",
                        "name": "Xin Yang"
                    },
                    {
                        "authorId": "2027354611",
                        "name": "Xiaogang Xu"
                    },
                    {
                        "authorId": "2109289860",
                        "name": "Ying-Cong Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In recent years many works have proposed to utilize such appealing properties of the StyleGAN latent spaces for image editing tasks [1, 12, 15, 27, 33, 35, 41].",
                "In particular, the state-of-theart StyleGAN models [17\u201319] have many practical applications such as image enhancement [6, 22, 39, 44], image editing [1, 12, 15, 27, 33, 35, 41], image-to-image translation [9, 14, 29, 34] thanks to their high-quality image generation and their latent representation that has rich semantics and disentangled controls for localized meaningful image manipulations.",
                "2 [12] Erik H\u00e4rk\u00f6nen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6f38b25f4b4d34599f9a634063534b03ef485bad",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-10229",
                    "DOI": "10.48550/arXiv.2212.10229",
                    "CorpusId": 254877631
                },
                "corpusId": 254877631,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6f38b25f4b4d34599f9a634063534b03ef485bad",
                "title": "StyleDomain: Analysis of StyleSpace for Domain Adaptation of StyleGAN",
                "abstract": "Domain adaptation of GANs is a problem of fine-tuning the state-of-the-art GAN models (e.g. StyleGAN) pretrained on a large dataset to a specific domain with few samples (e.g. painting faces, sketches, etc.). While there are a great number of methods that tackle this problem in different ways there are still many important questions that remain unan-swered. In this paper, we provide a systematic and in-depth analysis of the domain adaptation problem of GANs, focus-ing on the StyleGAN model. First, we perform a detailed exploration of the most important parts of StyleGAN that are responsible for adapting the generator to a new domain depending on the similarity between the source and target domains. In particular, we show that affine layers of StyleGAN can be sufficient for fine-tuning to similar domains. Second, inspired by these findings, we investigate StyleSpace to utilize it for domain adaptation. We show that there exist directions in the StyleSpace that can adapt StyleGAN to new domains. Further, we examine these directions and discover their many surprising properties. Finally, we leverage our analysis and findings to deliver practical improvements and applications in such standard tasks as image-to-image translation and cross-domain morphing.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "82901572",
                        "name": "Aibek Alanov"
                    },
                    {
                        "authorId": "2165156333",
                        "name": "Vadim Titov"
                    },
                    {
                        "authorId": "2184296381",
                        "name": "M. Nakhodnov"
                    },
                    {
                        "authorId": "2492721",
                        "name": "D. Vetrov"
                    }
                ]
            }
        },
        {
            "contexts": [
                "A key technique for using pretrained generative models to produce interested images is latent space navigation [32, 33, 80, 81, 46, 82, 83], which manipulates images by discovering interpretable directions in the latent space."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b22bc897b9f18b22fdd39a1973c31bf4f65f1a7d",
                "externalIds": {
                    "DBLP": "conf/nips/LiCMS022",
                    "CorpusId": 258509181
                },
                "corpusId": 258509181,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b22bc897b9f18b22fdd39a1973c31bf4f65f1a7d",
                "title": "Optimal Positive Generation via Latent Transformation for Contrastive Learning",
                "abstract": "Contrastive learning, which learns to contrast positive with negative pairs of samples, has been popular for self-supervised visual representation learning. Although great effort has been made to design proper positive pairs through data augmentation, few works attempt to generate optimal positives for each instance. Inspired by semantic consistency and computational advantage in latent space of pretrained generative models, this paper proposes to learn instance-specific latent transformations to generate Contrastive Optimal Positives (COP-Gen) for self-supervised contrastive learning. Specifically, we formulate COP-Gen as an instance-specific latent space navigator which minimizes the mutual information between the generated positive pair subject to the semantic consistency constraint. Theoretically, the learned latent transformation creates optimal positives for contrastive learning, which removes as much nuisance information as possible while preserving the semantics. Empirically, using generated positives by COP-Gen consistently outperforms other latent transformation methods and even real-image-based methods in self-supervised contrastive learning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1527098277",
                        "name": "Yinqi Li"
                    },
                    {
                        "authorId": "2116284897",
                        "name": "Hong Chang"
                    },
                    {
                        "authorId": "1798982",
                        "name": "Bingpeng Ma"
                    },
                    {
                        "authorId": "145455919",
                        "name": "S. Shan"
                    },
                    {
                        "authorId": "46772547",
                        "name": "Xilin Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "71a0a52803d4b25cc0f422996a178615893d58f7",
                "externalIds": {
                    "DOI": "10.33612/diss.507581028",
                    "CorpusId": 255633392
                },
                "corpusId": 255633392,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/71a0a52803d4b25cc0f422996a178615893d58f7",
                "title": "Generative Adversarial Networks for Diverse and Explainable Text-to-Image Generation",
                "abstract": ",",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48806403",
                        "name": "Zhenxing Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026generation is to identify directions of variation in the latent space for each meta-attribute and\nmanipulate the latent code for an input image along these directions to achieve the desired control, e.g., Shen et al. (2020); H\u00e4rk\u00f6nen et al. (2020); Voynov & Babenko (2020); Khrulkov et al. (2021)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e2bc83185fbac80d61d2ad7c242ebe7425a40d23",
                "externalIds": {
                    "DBLP": "journals/tmlr/BoseMG22",
                    "CorpusId": 258787951
                },
                "corpusId": 258787951,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e2bc83185fbac80d61d2ad7c242ebe7425a40d23",
                "title": "Controllable Generative Modeling via Causal Reasoning",
                "abstract": "Deep latent variable generative models excel at generating complex, high-dimensional data, often exhibiting impressive generalization beyond the training distribution. However, many such models in use today are black-boxes trained on large unlabelled datasets with statistical objectives and lack an interpretable understanding of the latent space required for controlling the generative process. We propose CAGE, a framework for controllable generation in latent variable models based on causal reasoning. Given a pair of attributes, CAGE infers the implicit cause-effect relationships between these attributes as induced by a deep generative model. This is achieved by defining and estimating a novel notion of unit-level causal effects in the latent space of the generative model. Thereafter, we use the inferred cause-effect relationships to design a novel strategy for controllable generation based on counterfactual sampling. Through a series of large-scale synthetic and human evaluations, we demonstrate that generating counterfactual samples which respect the underlying causal relationships inferred via CAGE leads to subjectively more realistic images.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2175276350",
                        "name": "Joey Bose"
                    },
                    {
                        "authorId": "2738769",
                        "name": "R. Monti"
                    },
                    {
                        "authorId": "1954250",
                        "name": "Aditya Grover"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Finally, additional GANspace [4] editing results can be found in Fig."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "bd8044c4abe229e50778b7c4fb5aac4a2d2faeee",
                "externalIds": {
                    "CorpusId": 260090630
                },
                "corpusId": 260090630,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bd8044c4abe229e50778b7c4fb5aac4a2d2faeee",
                "title": "3 D GAN Inversion with Pose Optimization-Supplementary Material",
                "abstract": "To implement the latent code encoder E , we follow both the implementation and training strategy of e4e [15], which is a well-proven encoder architecture to map the input image into the distribution ofW+. We manipulate the output dimension of the encoder into R to fit in our method. For the camera pose estimator P , we manipulate the simple resnet34 [5] encoder to find theta and phi angles which are further calculated into the extrinsic matrix. In the case of dataset which has an additional roll angle component in the rotation such as cat faces, we choose the 6D rotation representation proposed by [17]. Although the target images are cropped and refined by [2], there exists an additional camera translation that euler angles cannot thoroughly define. Thus, we set an additional coordinate variance on the camera position as a learnable parameter. See supplementary for details and qualitative evaluation of additional translation.",
                "year": 2022,
                "authors": []
            }
        },
        {
            "contexts": [
                "In the future, it would be interesting to similarly expose a subset of the parameters, along with relevant value ranges, ideally with semantic attributes similar to latent spaces for portrait editing with StyleGAN [Abdal et al. 2021; H\u00e4rk\u00f6nen et al. 2020]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "9a1bdf31bfecac774e19c104515486b7eb2f4f73",
                "externalIds": {
                    "CorpusId": 260484422
                },
                "corpusId": 260484422,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9a1bdf31bfecac774e19c104515486b7eb2f4f73",
                "title": "MatFormer: A Generative Model for Procedural Materials",
                "abstract": "where children of a node are visited in the order of the node\u2019s output slots. We randomize the order of children that are connected to the same output slot. t A but topological node ordering.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145630914",
                        "name": "Paul Guerrero"
                    },
                    {
                        "authorId": "2227756291",
                        "name": "Milo\u0161 Ha\u0161an"
                    },
                    {
                        "authorId": "41193203",
                        "name": "R. Mech"
                    },
                    {
                        "authorId": "1747280",
                        "name": "T. Boubekeur"
                    }
                ]
            }
        },
        {
            "contexts": [
                "11 contains additional HyperStyle editing results on the cars domain obtained with GANSpace [6].",
                "10 provides additional editing comparisons on the cars domain obtained with GANSpace [6].",
                "0 Model Source License StyleGAN2 [11] Nvidia Source Code License-NC pSp [20] MIT License e4e [26] MIT License ReStyle [1] MIT License PTI [21] MIT License IDInvert [30] MIT License InterFaceGAN [24] MIT License StyleCLIP [18] MIT License GANSpace [6] Apache 2."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "3980300bee7de0fea55e2b5ff9e9c6b8c516631b",
                "externalIds": {
                    "CorpusId": 260494217
                },
                "corpusId": 260494217,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3980300bee7de0fea55e2b5ff9e9c6b8c516631b",
                "title": "HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing",
                "abstract": "HyperStyle enables accurate and highly editable inversions of real images. While our tool aims to empower content creators, it can also be used to generate more convincing deep-fakes [25] and aid in the spread of disinformation [27]. However, powerful tools already exist for the detection of GAN-synthesized imagery [15, 28]. These tools continually evolve, which gives us hope that any potential misuse of our method can be mitigated. Another cause for concern is the bias that generative networks inherit from their training data [17]. Our model was similarly trained on such a biased set, and as a result, may display degraded performance when dealing with images from minority classes [16]. However, we have demonstrated that our model successfully generalizes beyond its training set, and allows us to similarly shift the GAN beyond its original domain. These properties allow us to better preserve minority traits when compared to prior works, and we hope that this benefit would similarly enable fairer treatment of minorities in downstream tasks.",
                "year": 2022,
                "authors": []
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "6a9cc06e691430f9fcbb97c24f96529b5ff23eeb",
                "externalIds": {
                    "CorpusId": 260506843
                },
                "corpusId": 260506843,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6a9cc06e691430f9fcbb97c24f96529b5ff23eeb",
                "title": "DeepFaceEditing: Deep Face Generation and Editing with Disentangled Geometry and Appearance Control",
                "abstract": "achieve fine control of facial details such as wrinkles. To address this issue, we propose DeepFaceEditing, a structured disentanglement framework specifically designed for face images to support face generation and editing with disentangled control of geometry and appearance. We adopt a local-to-global approach to incorporate the face domain knowledge: local component images are decomposed into geometry and appearance representations",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2999411",
                        "name": "Shu-Yu Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent studies have demonstrated that the features of StyleGAN2 have abundant information on facial attributes such as postures and expressions [13, 36, 37].",
                "There have been abundant works to exploit the pre-trained StyleGAN for various tasks, including image manipulation [41, 13, 37, 35], 3D reconstruction [30], image segmentation [22, 2], and semantic matching [31]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "2188f4c932a9a7ddfab8a819f0257ce3cb4fc2eb",
                "externalIds": {
                    "DBLP": "conf/nips/YangJNK22",
                    "CorpusId": 258509658
                },
                "corpusId": 258509658,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2188f4c932a9a7ddfab8a819f0257ce3cb4fc2eb",
                "title": "Dense Interspecies Face Embedding",
                "abstract": "Dense Interspecies Face Embedding (DIFE) is a new direction for understanding faces of various animals by extracting common features among animal faces including human face. There are three main obstacles for interspecies face understanding: (1) lack of animal data compared to human, (2) ambiguous connection between faces of various animals, and (3) extreme shape and style variance. To cope with the lack of data, we utilize multi-teacher knowledge distillation of CSE and StyleGAN2 requiring no additional data or label. Then we synthesize pseudo pair images through the latent space exploration of StyleGAN2 to \ufb01nd implicit associations between different animal faces. Finally, we introduce the semantic matching loss to overcome the problem of extreme shape differences between species. To quantitatively evaluate our method over possible previous methodologies like unsupervised keypoint detection, we perform interspecies facial keypoint transfer on MAFL and AP-10K. Furthermore, the results of other applications like interspecies face image manipulation and dense keypoint transfer are provided. The code is available at https://github.com/kingsj0405/dife.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1579938286",
                        "name": "Sejong Yang"
                    },
                    {
                        "authorId": "79372499",
                        "name": "Subin Jeon"
                    },
                    {
                        "authorId": "7532506",
                        "name": "Seonghyeon Nam"
                    },
                    {
                        "authorId": "2248551752",
                        "name": "Seon Joo Kim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The synthetic thermal database was built using GANSpace to manipulate the intermediate latent space w of StyleGAN2 and obtain images with different characteristics.",
                "Figure 10 shows some examples of directions in the latent space obtained with GANSpace, representing images generated with different characteristics.",
                "In this work, we used GANSpace [17], which allows us to discover",
                "GANSpace uses the StyleGAN w space (intermediate latent space) to identify directions in the latent space using PCA.",
                "The database must have variability in the images; therefore, it is proposed to explore the intermediate latent space w of StyleGAN2 using the GANSpace algorithm [17], which allows manipulation of the semantics of the thermal image generated in a way that is not supervised by using principal component analysis (PCA).",
                "rithm [17], which allows manipulation of the semantics of the thermal image generated in a way that is not supervised by using principal component analysis (PCA).",
                "Available: http://arxiv.org/abs/1912.04958 [17] E. H\u00e4rk\u00f6nen, A. Hertzmann, J. Lehtinen, and S. Paris, \u2018\u2018GANSpace: Discovering interpretable GAN controls,\u2019\u2019 2020, arXiv:2004.02546.",
                "In this work, we used GANSpace [17], which allows us to discover\n80518 VOLUME 9, 2021\ninterpretable editions in an unsupervised way via the use of PCA."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "ddbe0c858aa3051916b642b8eca3845b10f2c2fa",
                "externalIds": {
                    "DBLP": "journals/access/HermosillaTACV21",
                    "DOI": "10.1109/ACCESS.2021.3085423",
                    "CorpusId": 235384115
                },
                "corpusId": 235384115,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ddbe0c858aa3051916b642b8eca3845b10f2c2fa",
                "title": "Thermal Face Generation Using StyleGAN",
                "abstract": "This article proposes the use of generative adversarial networks (GANs) via StyleGAN2 to create high-quality synthetic thermal images and obtain training data to build thermal face recognition models using deep learning. We employed different variants of StyleGAN2, incorporating the new improved version of StyleGAN that uses adaptive discriminator augmentation (ADA). In addition, three different thermal databases from the literature were employed to train a thermal face detector based on YOLOv3 and to train StyleGAN2 and its variants, evaluating different metrics. The synthetic thermal database was built using GANSpace to manipulate the intermediate latent space w of StyleGAN2 and obtain images with different characteristics, such as eyeglasses, rotation, beards, etc. We carried out the training of 6 pretrained deep learning models for face recognition to validate the use of our synthetic thermal database, obtaining 99.98% accuracy for classifying synthetic thermal face images.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145147107",
                        "name": "Gabriel Hermosilla"
                    },
                    {
                        "authorId": "2110291990",
                        "name": "Diego-Ignacio Henr\u00edquez Tapia"
                    },
                    {
                        "authorId": "1403377645",
                        "name": "H. Allende-Cid"
                    },
                    {
                        "authorId": "2057449084",
                        "name": "Gonzalo Far\u00edas Castro"
                    },
                    {
                        "authorId": "2055385546",
                        "name": "Esteban Vera"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Inspired by work on latent-space manipulation [4, 16, 30, 31, 41], we also link the latent space with the image space, but here with the explicit goal to supervise our loss.",
                "Edits are made using GANSpace.",
                "Here, the editing method is to male with GANSpace.",
                "We evaluate our projection on four well-known editing methods: InterfaceGAN [30], GANSpace [16], StyleFlow [4] and random interpolation between latent vectors [19].",
                "InterfaceGAN StyleFlow GANSpace Interpolations realism t realism t realism realism\nIm2StyleGAN++ 0.973 0.096 0.929 0.211 0.960 1.00 w/o MAGEC loss 0.994 0.097 0.976 0.148 0.985 1.03 Full Method 0.998 0.122 0.982 0.202 0.984 1.04\nTable 2: Realism scores and \u201cimproved target\u201d scores of random image edits.",
                "Figure 1: From left to right: original image, projection in latent space using MAGEC loss, various edits (GANSpace, InterfaceGAN and StyleFlow respectively).",
                "Importantly, notice how our method gives better scores for an editing method not utilized to supervise the loss (GANSpace), suggesting that the latent vector doesn\u2019t overfit to one editing method, but is encouraged to become \u201cin-domain\u201d.",
                "Each edit operation consisted in changing one of 10 possible facial attributes to a random new value, using either [16] or [4].",
                "GANSpace [16] doesn\u2019t use an auxiliary classifier, but instead performs PCA analysis on the latent space."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "71033455503f1b00499635d28e190ccd134596a2",
                "externalIds": {
                    "DBLP": "conf/bmvc/GrechkaGC21",
                    "CorpusId": 246535194
                },
                "corpusId": 246535194,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/71033455503f1b00499635d28e190ccd134596a2",
                "title": "MAGECally invert images for realistic editing",
                "abstract": "Generative Adversarial Networks (GANs) are now able to generate astonishingly realistic high-resolution images. Recent work has shown the emergence of semantically-meaningful manipulations simply by editing the corresponding latent vector. However, a real image must \ufb01rst be inverted into its GAN latent code before editing. Previous work usually achieves accurate reconstruction, but poor-quality latent vectors: applying known editing methods onto these latent codes results in artifacts and erroneous edits. We aim to bridge the gap between reconstruction and editability. We propose a novel instance-optimization based inversion method, which speci\ufb01cally aims to maximize the semantic information of the latent vector, all while producing an accurate reconstruction. We introduce the i MAG e-lat E nt C onsistency loss (\u201cMAGEC\u201d), which allows supervision in the latent space, encouraging editability of the resulting latent vector. We provide extensive qualitative and quantitative evaluation to validate our method, using the recent state-of-the-art StyleGAN and show that our method outperforms baseline inversion methods, opening the door to new realms of real-image editing.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2152663254",
                        "name": "Asya Grechka"
                    },
                    {
                        "authorId": "3411801",
                        "name": "Jean-Fran\u00e7ois Goudou"
                    },
                    {
                        "authorId": "51021910",
                        "name": "M. Cord"
                    }
                ]
            }
        },
        {
            "contexts": [
                "From the 3rd column in each subfigure, from left to right are the manipulation result of GANSpace [18], that of InterFaceGAN [35] and ours.",
                "We project the real images of FFHQ to the latent spaceW+ of StyleGAN using the pretrained encoder [34], and manipulate the latent codes using each method with the suggested magnitude of edits (3 for InterFaceGAN, specified range based on attributes for GANSpace and 1 for our method).",
                "We compare our results with two state-of-the-art methods: InterFaceGAN [35] and GANSpace [18].",
                "We compare our method quantitatively with GANSpace and InterFaceGAN using three metrics: target attribute change rate, attribute preservation rate and identity preservation score.",
                "GANSpace [18] performed PCA in the latent space of generative networks, explored the principal directions and discovered interpretable controls.",
                "For example, when changing \u2018gender\u2019, both GANSpace and InterFaceGAN modify the hairstyle, and when changing \u2018age\u2019, GANSpace adds eyeglasses and InterFaceGAN affects smile.",
                "The official implementation of GANSpace on StyleGAN2 is available.",
                "The directions of GANSpace are discovered from PCA so that they may control several attributes simultaneously."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "9e113513ba2e80f392eabd6ab5d31ec688754af7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-11895",
                    "CorpusId": 235592793
                },
                "corpusId": 235592793,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9e113513ba2e80f392eabd6ab5d31ec688754af7",
                "title": "A Latent Transformer for Disentangled and Identity-Preserving Face Editing",
                "abstract": "High quality facial image editing is a challenging problem in the movie post-production industry, requiring a high degree of control and identity preservation. Previous works that attempt to tackle this problem may suffer from the entanglement of facial attributes and the loss of the person\u2019s identity. Furthermore, many algorithms are limited to a certain task. To tackle these limitations, we propose to edit facial attributes via the latent space of a StyleGAN generator, by training a dedicated latent transformation network and incorporating explicit disentanglement and identity preservation terms in the loss function. We further introduce a pipeline to generalize our face editing to videos. Our model achieves a disentangled, controllable, and identitypreserving facial attribute editing, even in the challenging case of real (i.e., non-synthetic) images and videos. We conduct extensive experiments on image and video datasets and show that our model outperforms other state-of-theart methods in visual quality and quantitative evaluation. Source codes will be available at https://github. com/InterDigitalInc/Latent-Transformer.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2115586564",
                        "name": "Xu Yao"
                    },
                    {
                        "authorId": "1902919",
                        "name": "A. Newson"
                    },
                    {
                        "authorId": "1796594",
                        "name": "Y. Gousseau"
                    },
                    {
                        "authorId": "1806880",
                        "name": "P. Hellier"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026either by directly learning a disentangled mapping of an attribute of interest [Nitzan et al. 2020] or by a latent space traversal [Abdal et al. 2020b; Denton et al. 2019; Goetschalckx et al. 2019; H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020; Shen and Zhou 2020; Voynov and Babenko 2020; Wu et al. 2020].",
                "Recently, many works have explored performing semantically meaningful manipulations in the latent space of a well-trained GAN generator, either by directly learning a disentangled mapping of an attribute of interest [53] or by a latent space traversal [13, 23, 28, 29, 60, 61, 66, 68].",
                "Other works [13, 29, 48, 60, 68] have approached the age transformation task by exploring the semantics of the latent space of a well-trained GAN, such as StyleGAN [38, 39], and perform a latent space traversal to obtain the desired transformed image.",
                "Other works [Abdal et al. 2020b; H\u00e4rk\u00f6nen et al. 2020; Liu et al. 2020; Shen et al. 2020; Wu et al. 2020] have approached the age transformation task by exploring the semantics of the latent space of a well-trained GAN, such as StyleGAN [Karras et al. 2019, 2020], and perform a latent space\u2026"
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "72c4851af6f65f70f3d349e1ef3e97fa6ea7fdbe",
                "externalIds": {
                    "ArXiv": "2102.02754",
                    "DBLP": "journals/corr/abs-2102-02754",
                    "DOI": "10.1145/3450626.3459805",
                    "CorpusId": 231802359
                },
                "corpusId": 231802359,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/72c4851af6f65f70f3d349e1ef3e97fa6ea7fdbe",
                "title": "Only a matter of style: age transformation using a style-based regression model",
                "abstract": "The task of age transformation illustrates the change of an individual's appearance over time. Accurately modeling this complex transformation over an input facial image is extremely challenging as it requires making convincing, possibly large changes to facial features and head shape, while still preserving the input identity. In this work, we present an image-to-image translation method that learns to directly encode real facial images into the latent space of a pre-trained unconditional GAN (e.g., StyleGAN) subject to a given aging shift. We employ a pre-trained age regression network to explicitly guide the encoder in generating the latent codes corresponding to the desired age. In this formulation, our method approaches the continuous aging process as a regression task between the input age and desired target age, providing fine-grained control over the generated image. Moreover, unlike approaches that operate solely in the latent space using a prior on the path controlling age, our method learns a more disentangled, non-linear path. Finally, we demonstrate that the end-to-end nature of our approach, coupled with the rich semantic latent space of StyleGAN, allows for further editing of the generated images. Qualitative and quantitative evaluations show the advantages of our method compared to state-of-the-art approaches. Code is available at our project page: https://yuval-alaluf.github.io/SAM.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1850630812",
                        "name": "Yuval Alaluf"
                    },
                    {
                        "authorId": "2819477",
                        "name": "Or Patashnik"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Ha\u0308rko\u0308nen et al. (2020) searches for important and meaningful directions by performing PCA in the style space of StyleGAN (Karras et al., 2019; 2020).",
                "For GANbased methods that extract disentangled representations from pretrained GANs, we consider serveral recent methods: GANspace (GS) (Ha\u0308rko\u0308nen et al., 2020), LatentDiscovery (LD) (Voynov & Babenko, 2020), ClosedForm (CF) (Shen & Zhou, 2020) and DeepSpectral (DS) (Khrulkov et al., 2021).",
                "For GANbased methods that extract disentangled representations from pretrained GANs, we consider serveral recent methods: GANspace (GS) (H\u00e4rk\u00f6nen et al., 2020), LatentDiscovery (LD) (Voynov & Babenko, 2020), ClosedForm (CF) (Shen & Zhou, 2020) and DeepSpectral (DS) (Khrulkov et al.",
                "These non-disentangled GAN-based methods discover semantically meaningful directions in the style space of StyleGAN (Karras et al., 2019; 2020) by analysing the distribution of the first-layer outputs (Ha\u0308rko\u0308nen et al., 2020) or layer weights (Shen & Zhou, 2020; Khrulkov et al., 2021).",
                ", 2019; 2020) by analysing the distribution of the first-layer outputs (H\u00e4rk\u00f6nen et al., 2020) or layer weights (Shen & Zhou, 2020; Khrulkov et al."
            ],
            "isInfluential": true,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4177364690a19fd9d64f25c5152ec57b94b5ccc5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-10543",
                    "CorpusId": 231986411
                },
                "corpusId": 231986411,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4177364690a19fd9d64f25c5152ec57b94b5ccc5",
                "title": "Do Generative Models Know Disentanglement? Contrastive Learning is All You Need",
                "abstract": "Disentangled generative models are typically trained with an extra regularization term, which encourages the traversal of each latent factor to make a distinct and independent change at the cost of generation quality. When traversing the latent space of generative models trained without the disentanglement term, the generated samples show semantically meaningful change, rais-ing the question: do generative models know disentanglement? We propose an unsupervised and model-agnostic method: Dis entanglement via Co ntrast ( DisCo ) in the Variation Space . DisCo consists of: ( i ) a Navigator providing traversal directions in the latent space, and ( ii ) a \u2206 - Contrastor composed of two shared-weight Encoders, which encode image pairs along these directions to disentangled representations respectively, and a difference operator to map the encoded representations to the Variation Space . We propose two more key techniques for DisCo : entropy-based domination loss to make the encoded representations more disentangled and the strategy of \ufb02ipping hard negatives to address directions with the same semantic meaning. By optimizing the Navigator to discover disentangled directions in the latent space and Encoders to extract disentangled representations from images with Contrastive Learning, DisCo achieves the state-of-the-art disentanglement given pretrained non-disentangled generative models, including GAN, VAE, and Flow. Project page at https://github.com/xrenaa/DisCo .",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1466503743",
                        "name": "Xuanchi Ren"
                    },
                    {
                        "authorId": "1958895984",
                        "name": "Tao Yang"
                    },
                    {
                        "authorId": "46393469",
                        "name": "Yuwang Wang"
                    },
                    {
                        "authorId": "1634494276",
                        "name": "Wenjun Zeng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Manipulation by perturbing the stRGB channels in early resolution layers (0,1,2), middle layers (3,4,5) and late layers (6,7,8).",
                "receding hairline (5,414,1) (6,322,2) (6,497,3) (6,504,8) hair style mouth smiling (6,501,1) size of face or eye lipstick (15,45,1) gender, face expression beard sideburns (12,237,2) other type of beard, gender goatee (9,421,1) other type of beard, gender chin double chin (9,132,1) size of neck, wrinkle ear earrings (entangled with gender) (8,81,1) gender, face shape eye glasses (3,288,1) (2,175,3) (3,120,4) (2,97,6) gender, wrinkle and beard",
                "hair black hair (12,479,1) different hair color, lighting blond hair (12,479,1) (12,266,3) gender, other hair color and style gray hair (11,286,1) glasses, gender, wrinkle and beard wavy hair (6,500,1) (8,128,2) (5,92,3) (6,394,7) (6,323,28) hair style, gender",
                "bangs (3,259,1) (6,285,2) (5,414,3) (6,128,4) (9,295,8) (6,322,9) (6,487,11) (6,504,14) hair style",
                "For 40 CelebA attributes, we first remove inactivated (9), ambiguous (2) or neutral (3) attributes.",
                "As shown in Figure 10, manipulating the early (coarse) resolutions (0,1,2) mainly affects the center of the target object (better visible in faces than in cars), manipulating the middle resolutions (3,4,5) typically affects the entire target object, and manipulating the late (fine) resolution layers (6,7,8) affects the entire image."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "8a19d4c348bbb15cba6b3a3688d6b3dd55f79b22",
                "externalIds": {
                    "CorpusId": 260516061
                },
                "corpusId": 260516061,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8a19d4c348bbb15cba6b3a3688d6b3dd55f79b22",
                "title": "StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation (supplementary material)",
                "abstract": "9. Structure of StyleGAN2 StyleSpace To supplement the description of the different StyleGAN2 latent spaces in Section 3, here we describe the structure of the StyleSpace S in more detail. Every major layer (every resolution) of the StyleGAN2 generator (synthesis network) consists of two convolution layers for feature map synthesis and a single convolution layer that converts the second feature map into an RGB image (referred to as tRGB), as shown in Figure 9. Each of these three convolution layers is modulated by a vector of style parameters. We denote the three different vectors of style parameters as s1, s2, and stRGB . These are obtained from the intermediate latent vectors w \u2208 W via three affine transformations, w1 \u2192 s1, w2 \u2192 s2, w2 \u2192 stRGB . InW space, w1 and w2 are the same vector, and it is the same vector for all layers. InW+ space, w1 and w2 are two different vectors, and every major layer has its own pair (w1, w2). The length of all the w vectors is 512. The numbers of style parameters used by the different layers are listed in Table 2. Note that in 4x4 resolution, there is only s1 and stRGB . The length of s is 512 from the early layers until layer 14. After that layer, the length decreases from 256 to 32. In total, for a 1024x1024 generator, there are 6048 style channels that control feature maps, and 3040 additional channels that control the tRGB blocks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "34815981",
                        "name": "Zongze Wu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In the main text, we define three methods for perturbations in the latent code of a GAN: 1) adding isotropic Gaussian noise, 2) moving along principle component axes [2], and 3) style-mixing the optimized latent code with a random latent code."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "bfb6986064f39ffa040634dc06db85188a948ed9",
                "externalIds": {
                    "CorpusId": 235702892
                },
                "corpusId": 235702892,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bfb6986064f39ffa040634dc06db85188a948ed9",
                "title": "Ensembling with Deep Generative Views Supplementary Material",
                "abstract": "In supplementary material, we provide additional details on dataset preparation and classifier training methods for each classification task. We show additional qualitative examples of the GAN reconstructions and the perturbation methods investigated in the main text, at both fine and coarse layers of the latent code. Finally, we provide additional results investigating different experiment settings and classifier training distributions under each type of latent perturbation method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51322829",
                        "name": "Lucy Chai"
                    },
                    {
                        "authorId": "1922024303",
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "authorId": "2177801",
                        "name": "Eli Shechtman"
                    },
                    {
                        "authorId": "2094770",
                        "name": "Phillip Isola"
                    },
                    {
                        "authorId": "2109976035",
                        "name": "Richard Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b585d7a8fed75ea9ed1fe4562edd025a00e466c9",
                "externalIds": {
                    "DBLP": "conf/iui/FriedmanP21",
                    "CorpusId": 235789486
                },
                "corpusId": 235789486,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b585d7a8fed75ea9ed1fe4562edd025a00e466c9",
                "title": "Image Co-Creation by Non-Programmers and Generative Adversarial Networks",
                "abstract": "Generative models such as generative adversarial networks are now being studied extensively. Eventually, however, many of them are intended for non-programmers to work with, e.g. designers, artists, or other content creators. What happens when such individuals are confronted with using GANs? We present a case study \u2013 a new course intended for non-programmer MA students in human-computer interaction, aimed at training them in authoring content using generative models. As their final assignment, the students were asked to train generative adversarial networks in order to generate images from a predefined category of their choice. The students either used a graphical user interface (GUI)-based software or modified preexisting python code using simplified Google Colab notebooks. We present several lessons learned from this course. First, we analyze the joint human-AI creation process and recognize points where students could intervene, with anecdotal examples of how they creatively explored these opportunities. Interestingly, while the majority of algorithmic research is focused on how to make models more controllable (e.g., via conditioning or latent space disentanglement), the students found ways to obtain their creative needs by mostly exploring the dataset level (as opposed to the model architecture). Additionally, we present the results of a short survey, comparing the two modes of work (GUI vs code).",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144220013",
                        "name": "D. Friedman"
                    },
                    {
                        "authorId": "143694589",
                        "name": "D. Pollak"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several methods have been proposed in this direction, including fine-tuning network weights for each image [7, 38, 46], choosing better or multiple layers to project and edit [1, 2, 19, 60], designing better encoders [45, 57], modeling image corruption and transformations [5, 24], and discovering meaningful latent directions [49, 16, 27, 21]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "0af6e4b23091b863258176d5765759926a4c85de",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-01073",
                    "CorpusId": 236772794
                },
                "corpusId": 236772794,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0af6e4b23091b863258176d5765759926a4c85de",
                "title": "SDEdit: Image Synthesis and Editing with Stochastic Differential Equations",
                "abstract": "We introduce a new image editing and synthesis framework, Stochastic Differential Editing (SDEdit), based on a recent generative model using stochastic differential equations (SDEs). Given an input image with user edits ( e.g ., hand-drawn color strokes), we \ufb01rst add noise to the input according to an SDE, and subsequently denoise it by simulating the reverse SDE to gradually increase its likelihood under the prior. Our method does not require task-speci\ufb01c loss function designs, which are critical components for recent image editing methods based on GAN inversion. Compared to conditional GANs, we do not need to collect new datasets of original and edited images for new applications. Therefore, our method can quickly adapt to various editing tasks at test time without re-training models. Our approach achieves strong performance on a wide range of applications, including image synthesis and editing guided by stroke paintings and image compositing.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "83262128",
                        "name": "Chenlin Meng"
                    },
                    {
                        "authorId": "115504645",
                        "name": "Yang Song"
                    },
                    {
                        "authorId": "2112626470",
                        "name": "Jiaming Song"
                    },
                    {
                        "authorId": "2110435874",
                        "name": "Jiajun Wu"
                    },
                    {
                        "authorId": "1922024303",
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "authorId": "2490652",
                        "name": "Stefano Ermon"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Several methods have been proposed in this direction, including fine-tuning network weights for each image [7, 38, 46], choosing better or multiple layers to project and edit [1, 2, 19, 60], designing better encoders [45, 57], modeling image corruption and transformations [5, 24], and discovering meaningful latent directions [49, 16, 27, 21]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "8b5d052ab3c6e22ab0dd61f23db89df398f3f456",
                "externalIds": {
                    "CorpusId": 236908363
                },
                "corpusId": 236908363,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8b5d052ab3c6e22ab0dd61f23db89df398f3f456",
                "title": "Image Synthesis and Editing with Stochastic Differential Equations",
                "abstract": "We introduce a new image editing and synthesis framework, Stochastic Differential Editing (SDEdit), based on a recent generative model using stochastic differential equations (SDEs). Given an input image with user edits (e.g., hand-drawn color strokes), we first add noise to the input according to an SDE, and subsequently denoise it by simulating the reverse SDE to gradually increase its likelihood under the prior. Our method does not require task-specific loss function designs, which are critical components for recent image editing methods based on GAN inversion. Compared to conditional GANs, we do not need to collect new datasets of original and edited images for new applications. Therefore, our method can quickly adapt to various editing tasks at test time without re-training models. Our approach achieves strong performance on a wide range of applications, including image synthesis and editing guided by stroke paintings and image compositing.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "83262128",
                        "name": "Chenlin Meng"
                    },
                    {
                        "authorId": "2157995251",
                        "name": "Yang Song"
                    },
                    {
                        "authorId": "2112626470",
                        "name": "Jiaming Song"
                    },
                    {
                        "authorId": "2110435874",
                        "name": "Jiajun Wu"
                    },
                    {
                        "authorId": "1922024303",
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "authorId": "2490652",
                        "name": "Stefano Ermon"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "ae22e3a167df01dfad3b3407ce567cc2fc13e011",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-10201",
                    "CorpusId": 237266527
                },
                "corpusId": 237266527,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ae22e3a167df01dfad3b3407ce567cc2fc13e011",
                "title": "Adaptable GAN Encoders for Image Reconstruction via Multi-type Latent Vectors with Two-scale Attentions",
                "abstract": "Although current deep generative adversarial networks (GANs) could synthesize high-quality (HQ) images, discovering novel GAN encoders for image reconstruction is still favorable. When embedding images to latent space, existing GAN encoders work well for aligned images (such as the human face), but they do not adapt to more generalized GANs. To our knowledge, current state-of-the-art GAN encoders do not have a proper encoder to reconstruct high-fidelity images from most misaligned HQ synthesized images on different GANs. Their performances are limited, especially on non-aligned and real images. We propose a novel method (named MTV-TSA) to handle such problems. Creating multi-type latent vectors (MTV) from latent space and two-scale attentions (TSA) from images allows designing a set of encoders that can be adaptable to a variety of pre-trained GANs. We generalize two sets of loss functions to optimize the encoders. The designed encoders could make GANs reconstruct higher fidelity images from most synthesized HQ images. In addition, the proposed method can reconstruct real images well and process them based on learned attribute directions. The designed encoders have unified convolutional blocks and could match well in current GAN architectures (such as PGGAN, StyleGANs, and BigGAN) by fine-tuning the corresponding normalization layers and the last block. Such welldesigned encoders can also be trained to converge more quickly.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2116578093",
                        "name": "Cheng Yu"
                    },
                    {
                        "authorId": "46315174",
                        "name": "Wenmin Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We will observe result quality and then explore semantic editing capabilities using [7], which should be exactly what is shown in GANSpace [14].",
                "A novel camera manifold formulation together with a multi-view embedding strategy [5, 6] for the first time allows GAN-generated images to be used in virtual environments and enables new applications like generative stereoscopic image synthesis, while preserving semantic editing capabilities, such as changes of lighting or facial expression [7, 8, 9]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "5730fadae797678927d74c1d03fe56939f3f06b8",
                "externalIds": {
                    "CorpusId": 240286167
                },
                "corpusId": 240286167,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5730fadae797678927d74c1d03fe56939f3f06b8",
                "title": "GAN-based 3D Manipulation of Car Models",
                "abstract": "Figure 1: Top row: We will build on the results obtained in FreeStyleGAN [1] where multi-view images a) are used to provide a coarse geometric proxy b). We can render the closest view with StyleGAN to generate a very realistic image c). This allows insertion into 3D virtual scenes d) and semantic manipulation e) or stereo viewing f). In this internship we will extend these ideas to models of cars captured with multiple images (left), allowing free-viewpoint rendering (center and right)",
                "year": 2021,
                "authors": []
            }
        },
        {
            "contexts": [
                ", [Abdal et al. 2020; H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020; Tewari et al. 2020]) have tried to analyze and disentangle the latent code of some pretrained GAN space [Karras et al.",
                "Many other works (e.g., [Abdal et al. 2020; H\u00e4rk\u00f6nen et al. 2020; Shen et al. 2020; Tewari et al. 2020]) have tried to analyze and disentangle the latent code of some pretrained GAN space [Karras et al. 2019] also with labeled data of specific attributes."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "9dec89ef724c7ab86890f2ce4f6cffbda880a103",
                "externalIds": {
                    "CorpusId": 236004236
                },
                "corpusId": 236004236,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9dec89ef724c7ab86890f2ce4f6cffbda880a103",
                "title": "DeepFaceEditing: Deep Face Generation and Editing with Disentangled Geometry and Appearance Control",
                "abstract": "achieve fine control of facial details such as wrinkles. To address this is- sue, we propose DeepFaceEditing, a structured disentanglement framework specifically designed for face images to support face generation and edit- ing with disentangled control of geometry and appearance. We adopt a local-to-global approach to incorporate the face domain knowledge: local component images are decomposed into geometry and appearance representations, which are fused consistently using a global fusion module to improve generation quality. We exploit sketches to assist in extracting a better geometry representation, which also supports intuitive geometry editing via sketching. The resulting method can either extract the geometry and appearance representations from face images, or directly extract the ge- ometry representation from face sketches. Such representations allow users to easily edit and synthesize face images, with decoupled control of their geometry and appearance. Both qualitative and quantitative evaluations show the superior detail and appearance control abilities of our method compared to state-of-the-art methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2999411",
                        "name": "Shu-Yu Chen"
                    },
                    {
                        "authorId": "2152943384",
                        "name": "Feng-Lin Liu"
                    },
                    {
                        "authorId": "7827503",
                        "name": "Yu-Kun Lai"
                    },
                    {
                        "authorId": "1734823",
                        "name": "Paul L. Rosin"
                    },
                    {
                        "authorId": "2525637",
                        "name": "Chunpeng Li"
                    },
                    {
                        "authorId": "3169698",
                        "name": "Hongbo Fu"
                    },
                    {
                        "authorId": "51190170",
                        "name": "Lin Gao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "736d4c1fd88d042a8a14d1bf816db73d3c604626",
                "externalIds": {
                    "CorpusId": 244129260
                },
                "corpusId": 244129260,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/736d4c1fd88d042a8a14d1bf816db73d3c604626",
                "title": "2021-04216-GAN-based 3D Manipulation of Car Models",
                "abstract": "The Inria Sophia Antipolis M\u00e9diterran\u00e9e center counts 34 research teams as well as 7 support departments. The center's staff (about 500 people including 320 Inria employees) is made up of scientists of different nationalities (250 foreigners of 50 nationalities), engineers, technicians and administrative staff. 1/3 of the staff are civil servants, the others are contractual agents. The majority of the center\u2019s research teams are located in Sophia Antipolis and Nice in the Alpes-Maritimes. Four teams are based in Montpellier and two teams are hosted in Bologna in Italy and Athens. The Center is a founding member of Universit\u00e9 C\u00f4te d'Azur and partner of the I-site MUSE supported by the University of Montpellier.",
                "year": 2021,
                "authors": []
            }
        },
        {
            "contexts": [
                "Similar to Figure 9 in the main text, we show additional results of applying GANSpace [3] edits to our customized models, horse rider (top) and gabled church (bottom)."
            ],
            "isInfluential": false,
            "intents": [
                "result"
            ],
            "citingPaper": {
                "paperId": "cb3c5754eff6356c623626c29c635600325abd3a",
                "externalIds": {
                    "CorpusId": 244407929
                },
                "corpusId": 244407929,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cb3c5754eff6356c623626c29c635600325abd3a",
                "title": "Sketch Your Own GAN Supplemental Material",
                "abstract": "Training details We use the same training hyperparameters as [5]. In particular, we are using softplus for GAN loss, and R1 regularization [7] on both the sketch and image discriminator, DY and DX . We do not use path length regularization, as it has no effect on the latent mapping network. Also, we set the batch size to 4 for all of our experiments, except when the sketch inputs are less than four, where we set the batch size to 1. Hyperparameters. We use the same hyperparameters for our full method in all of our experiments. In particular, we use \u03bbimage = 0.7 In the main text (Sec 4.1), we compared several variants of our method in our ablation studies. To make the comparison fair, for each variant, we tuned the loss weights for optimal performance. In Table 2, we list the hyperparameters used for each variant. The only exception is that we use \u03bbweight = 50 for the Lsketch + Lweight and Lsketch + Lweight + aug. variant model trained on the standing cat task. Also, if the variants are not listed in the table, the same loss weights as the full method are used. The search space of the \u03bbimage is [0.3, 0.5, 0.7, 1.0], and the search space of \u03bbweight is [0.1, 1, 10, 50, 100, 1000]. Data collection. In the main text (Sec. 4.1), we selected sets of 30 sketches with similar shapes and poses to designate as the user input: examples of sketches from these sets are shown in Figure 1. To evaluate generation quality, we collected images that match the input sketches from LSUN [10]. To retrieve matching images, we experimented with two sketch-image cross-domain matching methods. We applied both the SBIR method of Bui et al. [2] and chamfer distance [1]. Both of these retrieval results are shown in Figure 2. We observe that with chamfer distance, the retrieved images match poses of the sketches more faithfully. As a result, we adopt this method to generate our evaluation sets. However, we notice that there still exists outliers after the retrieval; hence, we hand-selected 2,500 images out of top 10,000 matches to curate the evaluation sets. A comparison between curated dataset and top chamfer matches are shown in Figure 3. Evaluation procedure. To evaluate each model, we sample 2,500 images without truncation and save them into png files. Likewise, the evaluation set described in the main text (Sec. 4.1) consists of 2,500 256\u00d7256 images stored in png. We evaluate the Fr\u00e9chet Inception Distance values using the CleanFID code [8].",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "12782331",
                        "name": "Sheng-Yu Wang"
                    },
                    {
                        "authorId": "144159726",
                        "name": "David Bau"
                    },
                    {
                        "authorId": "1922024303",
                        "name": "Jun-Yan Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The geometric head motion was generated as a random latent space walk along hand-picked directions from GANSpace [10] and SeFa [22]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1d35310fc59b28ececd07b3cfbaf4f376e7c5019",
                "externalIds": {
                    "CorpusId": 245474687
                },
                "corpusId": 245474687,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1d35310fc59b28ececd07b3cfbaf4f376e7c5019",
                "title": "Supplemental Material: Alias-Free Generative Adversarial Networks",
                "abstract": "StyleGAN2 and our generators yield comparable FIDs in all of these datasets. Visual inspection did not reveal anything surprising in the first three datasets, but in BEACHES our new generators seem to generate a somewhat reduced set of possible scene layouts properly. We suspect that this is related to the lack of noise inputs, which forces the generators to waste capacity for what is essentially random number generation [16]. Finding a way to reintroduce noise inputs without breaking equivariances is therefore an important avenue of future work.",
                "year": 2021,
                "authors": []
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "4fad202ba015e71a83528a72a8e08c8d33f5f8a8",
                "externalIds": {
                    "CorpusId": 245960874
                },
                "corpusId": 245960874,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4fad202ba015e71a83528a72a8e08c8d33f5f8a8",
                "title": "A StyleGAN-2 inspired Generative Adversarial Network for the PCA-controllable generation of drums samples for content-based retrieval",
                "abstract": "Generative Adversarial Networks (GAN) have proven incredibly effective at the task of generating highly realistic natural images. On top of this, approaches for the conditioning of the generation process by controlling specific attributes in the latent space (e.g. hair color, gender, age, beard, etc when trained on human faces) have been gaining more attention in recent years. In this work, we validate a StyleGAN-2 inspired architecture for the unlimited generation of high-quality magnitude spectrogram images, for the purpose of content-based retrieval. In addition, in the same way that it is possible to discover and control specific attributes relevant to the distribution of natural images, we demonstrate that the same is applicable to the domain of audio, showing that when trained on drum loops, some of these controllable latent dimensions directly relate to highly semantic factors such as BPM, rhythmic pattern, low pass and high pass filtering, etc. Even though these generated high-resolution spectrograms can be inverted back into the time-domain and made available for use (we demonstrate this using the Griffin-Lim algorithm), the purpose of this project was to validate the approach with the goal of content-based retrieval. Particularly, developing better search and discovery tools for querying a large collection of human-made audio samples.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2150006969",
                        "name": "Alejandro Koretzky"
                    },
                    {
                        "authorId": "2129472039",
                        "name": "Naveen Rajashekharappa"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following [11], principal axes of p(w) are identified with PCA."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "686bc724130fe50988188a839a97c0405f131834",
                "externalIds": {
                    "CorpusId": 245962337
                },
                "corpusId": 245962337,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/686bc724130fe50988188a839a97c0405f131834",
                "title": "STYLE-BASED DRUM SYNTHESIS WITH GAN INVERSION",
                "abstract": "Neural audio synthesizers exploit deep learning as an alternative to traditional synthesizers that generate audio from hand-designed components, such as oscillators and wavetables. For a neural audio synthesizer to be applicable to music creation, meaningful control over the output is essential. This paper provides an overview of an unsupervised approach to deriving useful feature controls learned by a generative model. A system for generation and transformation of drum samples using a style-based generative adversarial network (GAN) is proposed. The system provides functional control of audio style features, based on principal component analysis (PCA) applied to the intermediate latent space. Additionally, we propose the use of an encoder trained to invert input drums back to the latent space of the pre-trained GAN. We experiment with three modes of control and provide audio results on a supporting website.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2069949238",
                        "name": "Jake Drysdale"
                    },
                    {
                        "authorId": "103929969",
                        "name": "Maciej Tomczak"
                    },
                    {
                        "authorId": "2505173",
                        "name": "Jason Hockman"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Next, we describe the unsupervised attribute discovery problem for a single generative model formulated in [7], which we extend to multiple models subsequently.",
                "Many existing unsupervised GAN attribute discovery works [7, 21] rely on closed-form optimization to extract attribute directions, thereby requiring a post-hoc attribute alignment step.",
                "With the development of high quality generative models [6, 13, 11], this goal can be achieved using unsupervised attribute discovery methods [7, 21, 23], which show latent spaces learn the underlying attribute structure of the data."
            ],
            "isInfluential": false,
            "intents": [
                "background",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "9fa3231988c4c6b2c4da593317f3f4d9b605ec3d",
                "externalIds": {
                    "CorpusId": 247408564
                },
                "corpusId": 247408564,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9fa3231988c4c6b2c4da593317f3f4d9b605ec3d",
                "title": "Unsupervised Attribute Alignment for Characterizing Distribution Shift",
                "abstract": "Detecting and addressing distribution shift is an important task in machine learning. However, most of the machine learning solutions to deal with distribution shift lack the capability to identify the key characteristics of such a shift and present it to humans in an interpretable way. In this work, we propose a novel framework to compare two datasets and identify distribution shifts between the datasets. The key challenge is to identify generative factors of variation, which we refer to as attributes, that characterize the similarities and differences between the datasets. Producing this characterization requires finding a set of attributes that can be aligned between the two datasets and sets that are unique. We address this challenge through a novel approach that performs both attribute discovery and attribute alignment across the two distributions. We evaluate our algorithm\u2019s effectiveness at accurately identifying these attributes in two separate experiments, one involving two variants of MNIST and a second experiment involving two versions of dSprites.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2058026244",
                        "name": "Matthew Lyle Olson"
                    },
                    {
                        "authorId": "47130096",
                        "name": "Shusen Liu"
                    },
                    {
                        "authorId": "2860488",
                        "name": "Rushil Anirudh"
                    },
                    {
                        "authorId": "2064767378",
                        "name": "J. Thiagarajan"
                    },
                    {
                        "authorId": "37535697",
                        "name": "Weng-Keen Wong"
                    },
                    {
                        "authorId": "145466013",
                        "name": "P. Bremer"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For example, for the ffhq manifold, we use the first two PCA components identified by GANSpace, which correspond to head pose and gender, as visible in figure 4.",
                "Specifically we use GANSpace [10] to find interpretable semantic directions in the W space of StyleGAN2 [16], using PCA."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "366620cf47705c2e9bc5c1dcde870196a45ac684",
                "externalIds": {
                    "DBLP": "conf/nips/HoranRW21",
                    "CorpusId": 248497880
                },
                "corpusId": 248497880,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/366620cf47705c2e9bc5c1dcde870196a45ac684",
                "title": "When Is Unsupervised Disentanglement Possible?",
                "abstract": "A common assumption in many domains is that high dimensional data are a smooth nonlinear function of a small number of independent factors. When is it possible to recover the factors from unlabeled data? In the context of deep models this problem is called \u201cdisentanglement\u201d and was recently shown to be impossible without additional strong assumptions [17, 19]. In this paper, we show that the assumption of local isometry together with non-Gaussianity of the factors, is suf\ufb01cient to provably recover disentangled representations from data. We leverage recent advances in deep generative models to construct manifolds of highly realistic images for which the ground truth latent representation is known, and test whether modern and classical methods succeed in recovering the latent factors. For many different manifolds, we \ufb01nd that a spectral method that explicitly optimizes local isometry and non-Gaussianity consistently \ufb01nds the correct latent factors, while baseline deep autoencoders do not. We propose how to encourage deep autoencoders to \ufb01nd encodings that satisfy local isometry and show that this helps them discover disentangled representations. Overall, our results suggest that in some realistic settings, unsupervised disentanglement is provably possible, without any domain-speci\ufb01c assumptions. representation found by HLLE+ICA approximately satis\ufb01es local isometry while the encoding of the baseline autoencoder does not (bottom). In this paper we show that the assumptions of local isometry and non-Gaussianity are suf\ufb01cient to provably recover disentangled representations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "117968082",
                        "name": "Daniel P. Horan"
                    },
                    {
                        "authorId": "31523479",
                        "name": "Eitan Richardson"
                    },
                    {
                        "authorId": "30400079",
                        "name": "Yair Weiss"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This can be done by operating directly on the latent codes [15, 16] or by analysing the activation space of latent codes to discover interpretable directions of manipulation in latent space [17]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "726209f4845697ba66a627ed679300ac36ff0569",
                "externalIds": {
                    "DBLP": "conf/evoW/BroadLG21",
                    "DOI": "10.1007/978-3-030-72914-1_2",
                    "CorpusId": 232227185
                },
                "corpusId": 232227185,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/726209f4845697ba66a627ed679300ac36ff0569",
                "title": "Network Bending: Expressive Manipulation of Deep Generative Models",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47214633",
                        "name": "Terence Broad"
                    },
                    {
                        "authorId": "1745250",
                        "name": "F. Leymarie"
                    },
                    {
                        "authorId": "1691146",
                        "name": "M. Grierson"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This phenomenon has been observed in the previous work, where principal components from StyleGAN have resulted in the entanglement of facial attributes like gender and head rotation [13]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "880b0e12e562ae83f474760d4bd68dc1db929595",
                "externalIds": {
                    "DBLP": "conf/bmvc/SabooGSSJW21",
                    "CorpusId": 249892347
                },
                "corpusId": 249892347,
                "publicationVenue": {
                    "id": "78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                    "name": "British Machine Vision Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Br Mach Vis Conf",
                        "BMVC"
                    ],
                    "url": "http://www.bmva.org/bmvc/"
                },
                "url": "https://www.semanticscholar.org/paper/880b0e12e562ae83f474760d4bd68dc1db929595",
                "title": "Latent-optimization based Disease-aware Image Editing for Medical Image Augmentation",
                "abstract": "Data augmentation addresses the critical challenge of limited data in medical imaging. While generative adversarial networks (GANs) have been a popular choice in synthesizing medical images, controlled generation targeting disease-speci\ufb01c semantic has been dif\ufb01cult, partly due to the dif\ufb01culty to disentangle local disease-speci\ufb01c semantic factors from global disease-irrelevant factors. In this work, we present a semantic image editing framework for medical image augmentation that is able to generate smooth variations along the desired direction of disease attributes in user-de\ufb01ned regions of interest. This is achieved by discovering the optimal trajectory on the latent manifold of a pre-trained StyleGAN, guided by a mask of the region of interest and explicitly constrained by desired directions of semantic changes. We test the presented method on the public Chest X-ray dataset. To evaluate the quality of the generated medical images, we leverage both domain experts (pulmonologists) for qualitative assessments and present a novel metric to quantify the ability of the presented method to generate progression of disease severity in the synthesized images. We also show that data augmentation using the presented method improves downstream classi\ufb01cation tasks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2125377291",
                        "name": "Aakash Saboo"
                    },
                    {
                        "authorId": "50340328",
                        "name": "P. Gyawali"
                    },
                    {
                        "authorId": "2055375089",
                        "name": "Ankit Shukla"
                    },
                    {
                        "authorId": "2110200511",
                        "name": "Manoj Sharma"
                    },
                    {
                        "authorId": "2171968441",
                        "name": "Neeraj Jain"
                    },
                    {
                        "authorId": "48170028",
                        "name": "Linwei Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "221d679ef62b8c7d54d59316cbbec7403b324f8c",
                "externalIds": {
                    "CorpusId": 260509826
                },
                "corpusId": 260509826,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/221d679ef62b8c7d54d59316cbbec7403b324f8c",
                "title": "ReStyle: A Residual-Based StyleGAN Encoder via Iterative Re\ufb01nement Supplementary Materials",
                "abstract": "In this supplemental document we provide additional details and analysis to complement those provided in the main manuscript. Along with the additional details, we also perform an ablation study to validate our design choices and provide a large gallery of comparisons and results at full resolution using the proposed ReStyle scheme. Finally, we invite the readers to view the accompanying full-resolution animations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1850630812",
                        "name": "Yuval Alaluf"
                    },
                    {
                        "authorId": "2227680946",
                        "name": "Patashnik"
                    },
                    {
                        "authorId": "1388323541",
                        "name": "D. Cohen-Or"
                    },
                    {
                        "authorId": "2227696159",
                        "name": "Blavatnik"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also plot the FID values for one of the directions discovered with the GANSpace [4] approach in the latent space."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "07a652f4a44b3f66c36d0f9d6f54cae92ff752d9",
                "externalIds": {
                    "CorpusId": 260516225
                },
                "corpusId": 260516225,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/07a652f4a44b3f66c36d0f9d6f54cae92ff752d9",
                "title": "Navigating the GAN Parameter Space for Semantic Image Editing \u201d 1",
                "abstract": "Different generator layers were shown to capture different image properties [1]. Accordingly, navigating the parameter space of layers from different depths also discovers the effects of different types. For the LSUN-Horse dataset, Figure 7 visualizes the interpretable manipulations discovered at different depths, one manipulation per each StyleGAN2 layer. Notably, the earlier layers are generally responsible for global geometric transformations (size, leg length). Then, the intermediate layers typically result in more localized geometric manipulations (head size, thickness). They are followed by localized color manipulations (greens, white legs, background removal, shadows). The last layers correspond to global lighting effects (global lighting, horse reddening). Here we do not consider several final layers since they capture only trivial color-editing transformations. On other datasets, the distribution of typical effects over different layers is mostly the same.",
                "year": 2021,
                "authors": []
            }
        },
        {
            "contexts": [
                "In order to define consistent and semantically meaningful latent factors, we follow GANSpace [4] and perform PCA analysis on the W space of StyleGAN2."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4aba290e2157cf3f1649acfbed86004d7f0f0b6c",
                "externalIds": {
                    "CorpusId": 261032476
                },
                "corpusId": 261032476,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4aba290e2157cf3f1649acfbed86004d7f0f0b6c",
                "title": "When is Unsupervised Disentanglement Possible?",
                "abstract": "Proof: Donoho and Grimes [3] showed that as the number of examples goes to infinity, the H matrix converges to the (continuous) Hessian operator on the manifold whose d+1 eigenfunctions with zero eigenvalue are spanned by the latent factors. They also showed that if the support of p(z) is connected, then HLLE will recover the isometric embedding of the manifold up to a linear transformation. The assumption of non-Gaussianity means that running fastICA on the output of HLLE will provably recover the original independent factors. Similarly, Belkin and Niyogi [1] have shown that when the number of examples goes to infinity, the L matrix converges to the (continuous) manifold Laplacian, and Singer and Coifman [7] have shown that under local isometry, the d + 1 eigenfunctions with smallest eigenvalues are monotonic functions of the true latent functions. Since the eigenvalues are equal, LEM may recover a linear combination of the true factors and again the assumption of non-Gaussianity means that running fastICA on the output of LEM will provably recover the original independent factors.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "117968082",
                        "name": "Daniel P. Horan"
                    },
                    {
                        "authorId": "31523479",
                        "name": "Eitan Richardson"
                    },
                    {
                        "authorId": "30400079",
                        "name": "Yair Weiss"
                    }
                ]
            }
        },
        {
            "contexts": [
                "More recently, approaches have been found that allow the unsupervised identification of meaningful hyper-directions in GAN latent spaces, with an approach called GANspace [15] currently providing the most efficient method4.",
                "In a second step, we then analyze the most salient hyper-directions in the learned latent space with the help of the GANspace method.",
                "Importantly, GANspace is\n4In July 2020, another, conceptually different, approach was published [28] that shows even more promising results.",
                "We have also suggested to explore generative adversarial networks as a potential generative approach in digital art history and have documented a proof-of-concept approach utilizing StyleGAN and the GANspace algorithm to identify meaningful directions in the latent spaces of two GANs trained on art historical corpora."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e5aadc6bd00c6518cf97c22be259c4c621370c7f",
                "externalIds": {
                    "DBLP": "conf/chr/Offert020",
                    "CorpusId": 227924727
                },
                "corpusId": 227924727,
                "publicationVenue": {
                    "id": "d33c4b1c-10e4-476d-80a5-4be37c5f59b9",
                    "name": "Workshop on Computational Humanities Research",
                    "type": "conference",
                    "alternate_names": [
                        "CHR",
                        "Workshop Comput Humanit Res"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e5aadc6bd00c6518cf97c22be259c4c621370c7f",
                "title": "Generative Digital Humanities",
                "abstract": "While generative machine learning has recently attracted a significant amount of attention in the computer science community, its potential for the digital humanities has so far not been fully evaluated. In this paper, we examine generative adversarial networks, a state-of-the art generative machine learning technique. We argue that GANs can be particularly useful in digital art history, where they can be employed to facilitate the exploration of the semantic structure of large image corpora. Moreover, we posit that the foundational statistical distinction between discriminative and generative approaches offers an alternative critical perspective on machine learning in the digital humanities context. If \u201call models are wrong, some are useful\u201d, as the often-cited passage reads, we argue that, in case of the digital humanities, the most useful-wrong models are generative.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3405588",
                        "name": "Fabian Offert"
                    },
                    {
                        "authorId": "2679693",
                        "name": "Peter Bell"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Active research on disentangled representation learning has recently proposed interpretable controls for global image manipulation (H\u00e4rk\u00f6nen et al. 2020).",
                "Active research on disentangled representation learning has recently proposed interpretable controls for global image manipulation (Ha\u0308rko\u0308nen et al. 2020)."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "7df5a8b51caf00c828f0044ca8b45c2bc63df7f8",
                "externalIds": {
                    "DBLP": "conf/icccrea/BernsC20",
                    "CorpusId": 228097446
                },
                "corpusId": 228097446,
                "publicationVenue": {
                    "id": "5758d639-a450-4152-901d-7a78c8715aa7",
                    "name": "International Conference on Innovative Computing and Cloud Computing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Control Commun  Comput India",
                        "IEEE Int Conf Cogn Comput",
                        "IEEE International Conference Computer and Communications",
                        "Int Carpathian Control Conf",
                        "Int Conf Cogn Comput [services Soc",
                        "Int Conf Comput Cybern",
                        "IEEE International Conference on Cognitive Computing",
                        "IEEE Int Conf Comput Commun",
                        "International Conference on Computer Communication",
                        "Int Conf Innov Comput Cloud Comput",
                        "International Carpathian Control Conference",
                        "International Conference on Computational Creativity",
                        "Int Conf Comput Commun",
                        "International Conference on Control Communication & Computing India",
                        "ICCC",
                        "International Conference on Computational Cybernetics",
                        "Int Conf Comput Creativity",
                        "International Conference on Cognitive Computing [Services Society]",
                        "IEEE Int Conf Commun China",
                        "IEEE International Conference on Communications in China"
                    ],
                    "url": "http://computationalcreativity.net/",
                    "alternate_urls": [
                        "http://www.icccgovernors.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7df5a8b51caf00c828f0044ca8b45c2bc63df7f8",
                "title": "Bridging Generative Deep Learning and Computational Creativity",
                "abstract": "We aim to help bridge the research fields of generative deep learning and computational creativity by way of the creative AI community, and to advocate the common objective of more creatively autonomous generative learning systems. We argue here that generative deep learning models are inherently limited in their creative abilities because of a focus on learning for perfection. To highlight this, we present a series of techniques which actively diverge from standard usage of deep learning, with the specific intention of producing novel and interesting artefacts. We sketch out some avenues for improvement of the training and application of generative models and discuss how previous work on the evaluation of novelty in a computational creativity setting could be harnessed for such improvements. We end by describing how a two-way bridge between the research fields could be built.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1637443810",
                        "name": "Sebastian Berns"
                    },
                    {
                        "authorId": "1687610",
                        "name": "S. Colton"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020), and discovering meaningful latent directions (Shen et al., 2020; Goetschalckx et al., 2019; Jahanian et al., 2020; H\u00e4rk\u00f6nen et al., 2020).",
                "\u2026designing better encoders (Richardson et al., 2021; Tov et al., 2021), modeling image corruption and transformations (Anirudh et al., 2020; Huh et al., 2020), and discovering meaningful latent directions (Shen et al., 2020; Goetschalckx et al., 2019; Jahanian et al., 2020; Ha\u0308rko\u0308nen et al., 2020)."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": null,
                "externalIds": null,
                "corpusId": "245704806",
                "publicationVenue": null,
                "url": null,
                "title": "Stroke Image Perturb with SDE Reverse SDE Stroke Image Input Output",
                "abstract": null,
                "year": 2022,
                "authors": []
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "9b826004210837a2686d06c8ab29aeb966759ae5",
                "externalIds": {
                    "CorpusId": 248780864
                },
                "corpusId": 248780864,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9b826004210837a2686d06c8ab29aeb966759ae5",
                "title": "D EEP L EARNING AND S YNTHETIC M EDIA Forthcoming in Synthese",
                "abstract": "Deep learning algorithms are rapidly changing the way in which audiovisual media can be produced. Synthetic audiovisual media generated with deep learning \u2013 often subsumed colloquially under the label \u201cdeepfakes\u201d \u2013 have a number of impressive characteristics; they are increasingly trivial to produce, and can be indistinguishable from real sounds and images recorded with a sensor. Much attention has been dedicated to ethical concerns raised by this technological development. Here, I focus instead on a set of issues related to the notion of synthetic audiovisual media, its place within a broader taxonomy of audiovisual media, and how deep learning techniques differ from more traditional approaches to media synthesis. After reviewing important etiological features of deep learning pipelines for media manipulation and generation, I argue that \u201cdeepfakes\u201d and related synthetic media produced with such pipelines do not merely offer incremental improvements over previous methods, but challenge traditional taxonomical distinctions, and pave the way for genuinely novel kinds of audiovisual media.",
                "year": null,
                "authors": [
                    {
                        "authorId": "16273823",
                        "name": "Rapha\u00ebl Milli\u00e8re"
                    }
                ]
            }
        }
    ]
}