{
    "offset": 0,
    "data": [
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "c182bcd5f37f37fea9f3dad856dc381e0f19578a",
                "externalIds": {
                    "ArXiv": "2309.16459",
                    "CorpusId": 263134374
                },
                "corpusId": 263134374,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c182bcd5f37f37fea9f3dad856dc381e0f19578a",
                "title": "Augmenting LLMs with Knowledge: A survey on hallucination prevention",
                "abstract": "Large pre-trained language models have demonstrated their proficiency in storing factual knowledge within their parameters and achieving remarkable results when fine-tuned for downstream natural language processing tasks. Nonetheless, their capacity to access and manipulate knowledge with precision remains constrained, resulting in performance disparities on knowledge-intensive tasks when compared to task-specific architectures. Additionally, the challenges of providing provenance for model decisions and maintaining up-to-date world knowledge persist as open research frontiers. To address these limitations, the integration of pre-trained models with differentiable access mechanisms to explicit non-parametric memory emerges as a promising solution. This survey delves into the realm of language models (LMs) augmented with the ability to tap into external knowledge sources, including external knowledge bases and search engines. While adhering to the standard objective of predicting missing tokens, these augmented LMs leverage diverse, possibly non-parametric external modules to augment their contextual processing capabilities, departing from the conventional language modeling paradigm. Through an exploration of current advancements in augmenting large language models with knowledge, this work concludes that this emerging research direction holds the potential to address prevalent issues in traditional LMs, such as hallucinations, un-grounded responses, and scalability challenges.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2248161377",
                        "name": "Konstantinos Andriopoulos"
                    },
                    {
                        "authorId": "1800677",
                        "name": "J. Pouwelse"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[48] introduce Target-data Collection, which adds more favorable examples (e.",
                "[48] append bias control tokens to the input so the generative dialogue model learns properties of gender bias."
            ],
            "citingPaper": {
                "paperId": "76f676cfdb9129e881f1e15f78ae373dee4d6d9d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-14381",
                    "ArXiv": "2309.14381",
                    "DOI": "10.48550/arXiv.2309.14381",
                    "CorpusId": 262828449
                },
                "corpusId": 262828449,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/76f676cfdb9129e881f1e15f78ae373dee4d6d9d",
                "title": "Survey of Social Bias in Vision-Language Models",
                "abstract": "In recent years, the rapid advancement of machine learning (ML) models, particularly transformer-based pre-trained models, has revolutionized Natural Language Processing (NLP) and Computer Vision (CV) fields. However, researchers have discovered that these models can inadvertently capture and reinforce social biases present in their training datasets, leading to potential social harms, such as uneven resource allocation and unfair representation of specific social groups. Addressing these biases and ensuring fairness in artificial intelligence (AI) systems has become a critical concern in the ML community. The recent introduction of pre-trained vision-and-language (VL) models in the emerging multimodal field demands attention to the potential social biases present in these models as well. Although VL models are susceptible to social bias, there is a limited understanding compared to the extensive discussions on bias in NLP and CV. This survey aims to provide researchers with a high-level insight into the similarities and differences of social bias studies in pre-trained models across NLP, CV, and VL. By examining these perspectives, the survey aims to offer valuable guidelines on how to approach and mitigate social bias in both unimodal and multimodal settings. The findings and recommendations presented here can benefit the ML community, fostering the development of fairer and non-biased AI models in various applications and research endeavors.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "40221187",
                        "name": "Nayeon Lee"
                    },
                    {
                        "authorId": "23672613",
                        "name": "Yejin Bang"
                    },
                    {
                        "authorId": "116344405",
                        "name": "Holy Lovenia"
                    },
                    {
                        "authorId": "66986482",
                        "name": "Samuel Cahyawijaya"
                    },
                    {
                        "authorId": "47653392",
                        "name": "Wenliang Dai"
                    },
                    {
                        "authorId": "1683412",
                        "name": "Pascale Fung"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Dinan et al. (2020), for instance, mitigate gender bias in dialogue generation by binning each training example by the presence or absence of masculine or feminine gendered words, and appending a control token corresponding to the bin to each prompt. Xu et al. (2020) adapt Dinan et al. (2020)\u2019s approach to provide safety guarantees for chatbot applications. The authors use safety control tokens, generated by a safety classifier that measures offensiveness, bias, and other potential harms in text. Similarly, Lu et al. (2022) score training examples with a reward function that quantifies some unwanted property, such as toxicity or bias, which is used to quantize the examples into bins.",
                "Also incorporating human writers, Dinan et al. (2020) investigate targeted data collection to reduce gender bias in chat dialogue models by curating human-written diversified examples, priming crowd workers with examples and standards for the desired data. Sun et al. (2023) construct example discussions that demonstrate and explain facets of morality, including fairness, using rules-of-thumb that encode moral principles and judgments. To train models that can appropriately respond and recover to biased input or outputs, Ung et al. (2022) generate a set of dialogues with example recovery statements, such as apologies, after unsafe, offensive, or inappropriate utterances. Similarly, Kim et al. (2022) generate a dataset of responses to biased or otherwise problematic statements based on rules-of-thumb that follow social norms.",
                "Also incorporating human writers, Dinan et al. (2020) investigate targeted data collection to reduce gender bias in chat dialogue models by curating human-written diversified examples, priming crowd workers with examples and standards for the desired data. Sun et al. (2023) construct example discussions that demonstrate and explain facets of morality, including fairness, using rules-of-thumb that encode moral principles and judgments.",
                "Also incorporating human writers, Dinan et al. (2020) investigate targeted data collection to reduce gender bias in chat dialogue models by curating human-written diversified examples, priming crowd workers with examples and standards for the desired data. Sun et al. (2023) construct example discussions that demonstrate and explain facets of morality, including fairness, using rules-of-thumb that encode moral principles and judgments. To train models that can appropriately respond and recover to biased input or outputs, Ung et al. (2022) generate a set of dialogues with example recovery statements, such as apologies, after unsafe, offensive, or inappropriate utterances.",
                "Dinan et al. (2020), for instance, mitigate gender bias in dialogue generation by binning each training example by the presence or absence of masculine or feminine gendered words, and appending a control token corresponding to the bin to each prompt.",
                "Also incorporating human writers, Dinan et al. (2020) investigate targeted data collection to reduce gender bias in chat dialogue models by curating human-written diversified examples, priming crowd workers with examples and standards for the desired data.",
                "Dinan et al. (2020), for instance, mitigate gender bias in dialogue generation by binning each training example by the presence or absence of masculine or feminine gendered words, and appending a control token corresponding to the bin to each prompt. Xu et al. (2020) adapt Dinan et al. (2020)\u2019s approach to provide safety guarantees for chatbot applications."
            ],
            "citingPaper": {
                "paperId": "bcfa73aedf1b2d1ee4f168e21298a37ac55a37f7",
                "externalIds": {
                    "ArXiv": "2309.00770",
                    "DBLP": "journals/corr/abs-2309-00770",
                    "DOI": "10.48550/arXiv.2309.00770",
                    "CorpusId": 261530629
                },
                "corpusId": 261530629,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bcfa73aedf1b2d1ee4f168e21298a37ac55a37f7",
                "title": "Bias and Fairness in Large Language Models: A Survey",
                "abstract": "Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly-available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2237806749",
                        "name": "Isabel O. Gallegos"
                    },
                    {
                        "authorId": "2066337266",
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "authorId": "40080808",
                        "name": "Joe Barrow"
                    },
                    {
                        "authorId": "35631602",
                        "name": "Md. Mehrab Tanjim"
                    },
                    {
                        "authorId": "2109571021",
                        "name": "Sungchul Kim"
                    },
                    {
                        "authorId": "2462276",
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "authorId": "2117903209",
                        "name": "Tong Yu"
                    },
                    {
                        "authorId": "1940556",
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "authorId": "47699955",
                        "name": "Nesreen Ahmed"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "In this way, the training corpus is directly re-balanced by swapping or removing bias-related words and counterfactual data augmentation (CDA) (Zmigrod et al., 2019; Dinan et al., 2020; Webster et al., 2020; Dev et al., 2020; Barikeri et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "6f49541509d64cffe4b7b586c3a6c5386a06e442",
                "externalIds": {
                    "ArXiv": "2307.10522",
                    "DBLP": "journals/corr/abs-2307-10522",
                    "DOI": "10.48550/arXiv.2307.10522",
                    "CorpusId": 259859044
                },
                "corpusId": 259859044,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/6f49541509d64cffe4b7b586c3a6c5386a06e442",
                "title": "Gender-tuning: Empowering Fine-tuning for Debiasing Pre-trained Language Models",
                "abstract": "Recent studies have revealed that the widely-used Pre-trained Language Models (PLMs) propagate societal biases from the large unmoderated pre-training corpora. Existing solutions require debiasing training processes and datasets for debiasing, which are resource-intensive and costly. Furthermore, these methods hurt the PLMs' performance on downstream tasks. In this study, we propose Gender-tuning, which debiases the PLMs through fine-tuning on downstream tasks' datasets. For this aim, Gender-tuning integrates Masked Language Modeling (MLM) training objectives into fine-tuning's training process. Comprehensive experiments show that Gender-tuning outperforms the state-of-the-art baselines in terms of average gender bias scores in PLMs while improving PLMs' performance on downstream tasks solely using the downstream tasks' dataset. Also, Gender-tuning is a deployable debiasing tool for any PLM that works with original fine-tuning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "34941968",
                        "name": "Somayeh Ghanbarzadeh"
                    },
                    {
                        "authorId": "2151496366",
                        "name": "Yan Huang"
                    },
                    {
                        "authorId": "2542427",
                        "name": "H. Palangi"
                    },
                    {
                        "authorId": "2140569992",
                        "name": "R. C. Moreno"
                    },
                    {
                        "authorId": "7156764",
                        "name": "Hamed Khanpour"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "result"
            ],
            "isInfluential": false,
            "contexts": [
                "Downstream task analyses mostly consider shortcomings in dialogue-systems (Staliu\u0304naite\u0307 and Iacobacci, 2020; Dinan et al., 2020a).",
                "The analysed bias dimension in this work is the person being spoken about (Dinan et al., 2020b), in contrast to, e.g., Excell and Al Moubayed (2021) where the bias concerns the author of a comment.",
                "Downstream task analyses mostly consider shortcomings in dialogue-systems (Stali\u016bnait\u0117 and Iacobacci, 2020; Dinan et al., 2020a)."
            ],
            "citingPaper": {
                "paperId": "704dafb9ae3f9b452c96a86d30207f1193459c31",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-15298",
                    "ArXiv": "2306.15298",
                    "ACL": "2022.gebnlp-1.20",
                    "DOI": "10.18653/v1/2022.gebnlp-1.20",
                    "CorpusId": 250391069
                },
                "corpusId": 250391069,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/704dafb9ae3f9b452c96a86d30207f1193459c31",
                "title": "Gender Bias in BERT - Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task",
                "abstract": "Pretrained language models are publicly available and constantly finetuned for various real-life applications. As they become capable of grasping complex contextual information, harmful biases are likely increasingly intertwined with those models. This paper analyses gender bias in BERT models with two main contributions: First, a novel bias measure is introduced, defining biases as the difference in sentiment valuation of female and male sample versions. Second, we comprehensively analyse BERT?s biases on the example of a realistic IMDB movie classifier. By systematically varying elements of the training pipeline, we can conclude regarding their impact on the final model bias. Seven different public BERT models in nine training conditions, i.e. 63 models in total, are compared. Almost all conditions yield significant gender biases. Results indicate that reflected biases stem from public BERT models rather than task-specific data, emphasising the weight of responsible usage.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "151209594",
                        "name": "Sophie F. Jentzsch"
                    },
                    {
                        "authorId": "13671251",
                        "name": "Cigdem Turan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "availability of high-quality data, there are also technical [3] and ethical challenges [20], [21]."
            ],
            "citingPaper": {
                "paperId": "dc9f9419bef4aa2c00e3db1315c6b4dd18ed8c47",
                "externalIds": {
                    "DBLP": "conf/ijcnn/AdewumiAAPBSRAGTAMBOONANLL23",
                    "DOI": "10.1109/IJCNN54540.2023.10191208",
                    "CorpusId": 260387895
                },
                "corpusId": 260387895,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/dc9f9419bef4aa2c00e3db1315c6b4dd18ed8c47",
                "title": "AfriWOZ: Corpus for Exploiting Cross-Lingual Transfer for Dialogue Generation in Low-Resource, African Languages",
                "abstract": "Dialogue generation is an important NLP task fraught with many challenges. The challenges become more daunting for low-resource African languages. To enable the creation of dialogue agents for African languages, we contribute the first high-quality dialogue datasets for 6 African languages: Swahili, Wolof, Hausa, Nigerian Pidgin English, Kinyarwanda & Yor\u00f9b\u00e1. There are a total of 9,000 turns, each language having 1,500 turns, which we translate from a portion of the English multi-domain MultiWOZ dataset. Subsequently, we benchmark by investigating & analyzing the effectiveness of modelling through transfer learning by utilziing state-of-the-art (SoTA) deep monolingual models: DialoGPT and BlenderBot. We compare the models with a simple seq2seq baseline using perplexity. Besides this, we conduct human evaluation of single-turn conversations by using majority votes and measure inter-annotator agreement (IAA). We find that the hypothesis that deep monolingual models learn some abstractions that generalize across languages holds. We observe human-like conversations, to different degrees, in 5 out of the 6 languages. The language with the most transferable properties is the Nigerian Pidgin English, with a human-likeness score of 78.1%, of which 34.4% are unanimous. We freely provide the datasets and host the model checkpoints/demos on the HuggingFace hub for public access.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51221489",
                        "name": "Tosin P. Adewumi"
                    },
                    {
                        "authorId": "2056770646",
                        "name": "Mofetoluwa Adeyemi"
                    },
                    {
                        "authorId": "2047583795",
                        "name": "Aremu Anuoluwapo"
                    },
                    {
                        "authorId": "2162783577",
                        "name": "Bukola Peters"
                    },
                    {
                        "authorId": "1395556657",
                        "name": "Happy Buzaaba"
                    },
                    {
                        "authorId": "2135913982",
                        "name": "Oyerinde Samuel"
                    },
                    {
                        "authorId": "2162781574",
                        "name": "Amina Mardiyyah Rufai"
                    },
                    {
                        "authorId": "83263885",
                        "name": "Benjamin Ayoade Ajibade"
                    },
                    {
                        "authorId": "2162782825",
                        "name": "Tajudeen Gwadabe"
                    },
                    {
                        "authorId": "2226522729",
                        "name": "Mory Moussou Koulibaly Traore"
                    },
                    {
                        "authorId": "98725872",
                        "name": "T. Ajayi"
                    },
                    {
                        "authorId": "7744881",
                        "name": "Shamsuddeen Hassan Muhammad"
                    },
                    {
                        "authorId": "114850513",
                        "name": "Ahmed Baruwa"
                    },
                    {
                        "authorId": "2105439683",
                        "name": "Paul Owoicho"
                    },
                    {
                        "authorId": "2145191211",
                        "name": "Tol\u00falop\u00e9 \u00d2g\u00fanr\u00e8m\u00ed"
                    },
                    {
                        "authorId": "2162782962",
                        "name": "Phylis Ngigi"
                    },
                    {
                        "authorId": "1452686038",
                        "name": "Orevaoghene Ahia"
                    },
                    {
                        "authorId": "2162783296",
                        "name": "Ruqayya Nasir"
                    },
                    {
                        "authorId": "80342407",
                        "name": "F. Liwicki"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "For example, the ConvAI2 competition at NeurIPS 2018 featured large (at the time) pre-trained Transformers being used by the top two winning teams (Wolf et al., 2019; Golovanov et al., 2020; Dinan et al., 2020b).",
                "\u2026generate harmful or inappropriate content (Bender et al., 2021; Bommasani et al., 2021; Hendrycks et al., 2021; Weidinger et al., 2021; Bai et al., 2022b), including work from our group (Xu et al., 2020; Dinan et al., 2022, 2021; Smith et al., 2022a; Dinan et al., 2020a; Smith and Williams, 2021).",
                ", 2022b), including work from our group (Xu et al., 2020; Dinan et al., 2022, 2021; Smith et al., 2022a; Dinan et al., 2020a; Smith and Williams, 2021)."
            ],
            "citingPaper": {
                "paperId": "0b6edce3dde7e502c6b7c6d83bac0230ec912482",
                "externalIds": {
                    "ArXiv": "2306.04707",
                    "DBLP": "journals/corr/abs-2306-04707",
                    "DOI": "10.48550/arXiv.2306.04707",
                    "CorpusId": 259108498
                },
                "corpusId": 259108498,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0b6edce3dde7e502c6b7c6d83bac0230ec912482",
                "title": "Improving Open Language Models by Learning from Organic Interactions",
                "abstract": "We present BlenderBot 3x, an update on the conversational model BlenderBot 3, which is now trained using organic conversation and feedback data from participating users of the system in order to improve both its skills and safety. We are publicly releasing the participating de-identified interaction data for use by the research community, in order to spur further progress. Training models with organic data is challenging because interactions with people\"in the wild\"include both high quality conversations and feedback, as well as adversarial and toxic behavior. We study techniques that enable learning from helpful teachers while avoiding learning from people who are trying to trick the model into unhelpful or toxic responses. BlenderBot 3x is both preferred in conversation to BlenderBot 3, and is shown to produce safer responses in challenging situations. While our current models are still far from perfect, we believe further improvement can be achieved by continued use of the techniques explored in this work.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2155954521",
                        "name": "Jing Xu"
                    },
                    {
                        "authorId": "3092435",
                        "name": "Da Ju"
                    },
                    {
                        "authorId": "2180661097",
                        "name": "Joshua Lane"
                    },
                    {
                        "authorId": "100653935",
                        "name": "M. Komeili"
                    },
                    {
                        "authorId": "51324296",
                        "name": "Eric Michael Smith"
                    },
                    {
                        "authorId": "1666356365",
                        "name": "Megan Ung"
                    },
                    {
                        "authorId": "2286668",
                        "name": "Morteza Behrooz"
                    },
                    {
                        "authorId": "35114899",
                        "name": "W.K.F. Ngan"
                    },
                    {
                        "authorId": "2219692579",
                        "name": "Rashel Moritz"
                    },
                    {
                        "authorId": "2265067",
                        "name": "Sainbayar Sukhbaatar"
                    },
                    {
                        "authorId": "90841478",
                        "name": "Y-Lan Boureau"
                    },
                    {
                        "authorId": "145183709",
                        "name": "J. Weston"
                    },
                    {
                        "authorId": "35752280",
                        "name": "Kurt Shuster"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "propose a di\u21b5erent method for evaluating gender bias in dialogue datasets[25]."
            ],
            "citingPaper": {
                "paperId": "dab05c972fe4537e362b262b33dffc00de5f5311",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-02294",
                    "ArXiv": "2306.02294",
                    "DOI": "10.48550/arXiv.2306.02294",
                    "CorpusId": 259075978
                },
                "corpusId": 259075978,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/dab05c972fe4537e362b262b33dffc00de5f5311",
                "title": "Exposing Bias in Online Communities through Large-Scale Language Models",
                "abstract": "Progress in natural language generation research has been shaped by the ever-growing size of language models. While large language models pre-trained on web data can generate human-sounding text, they also reproduce social biases and contribute to the propagation of harmful stereotypes. This work utilises the flaw of bias in language models to explore the biases of six different online communities. In order to get an insight into the communities' viewpoints, we fine-tune GPT-Neo 1.3B with six social media datasets. The bias of the resulting models is evaluated by prompting the models with different demographics and comparing the sentiment and toxicity values of these generations. Together, these methods reveal that bias differs in type and intensity for the various models. This work not only affirms how easily bias is absorbed from training data but also presents a scalable method to identify and compare the bias of different datasets or communities. Additionally, the examples generated for this work demonstrate the limitations of using automated sentiment and toxicity classifiers in bias research.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2219550842",
                        "name": "Celine Wald"
                    },
                    {
                        "authorId": "32421394",
                        "name": "Lukas Pfahler"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", 2021), gender bias (Liu et al., 2020; Dinan et al., 2020) and other discriminated behavior (Sheng et al.",
                "Harmful differences in responses caused by different demographic personas are observed in well-known dialogue systems (Sheng et al., 2021; Dinan et al., 2020), including offensiveness, gender bias, race discrimination, etc."
            ],
            "citingPaper": {
                "paperId": "969559ec5fcdb98dc5690640b2560d8df6fa9591",
                "externalIds": {
                    "DBLP": "conf/acl/0003TZ23",
                    "ArXiv": "2305.13833",
                    "DOI": "10.48550/arXiv.2305.13833",
                    "CorpusId": 258840898
                },
                "corpusId": 258840898,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/969559ec5fcdb98dc5690640b2560d8df6fa9591",
                "title": "Reducing Sensitivity on Speaker Names for Text Generation from Dialogues",
                "abstract": "Changing speaker names consistently throughout a dialogue should not affect its meaning and corresponding outputs for text generation from dialogues. However, pre-trained language models, serving as the backbone for dialogue-processing tasks, have shown to be sensitive to nuances. This may result in unfairness in real-world applications. No comprehensive analysis of this problem has been done in the past. In this work, we propose to quantitatively measure a model's sensitivity on speaker names, and comprehensively evaluate a number of known methods for reducing speaker name sensitivity, including a novel approach of our own. Extensive experiments on multiple datasets provide a benchmark for this problem and show the favorable performance of our approach in sensitivity reduction and quality of generation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2056108122",
                        "name": "Qi Jia"
                    },
                    {
                        "authorId": "2112389755",
                        "name": "Haifeng Tang"
                    },
                    {
                        "authorId": "1796651",
                        "name": "Kenny Q. Zhu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[6] Emily Dinan, Angela Fan, Adina Williams, Jack Urbanek, Douwe Kiela, and Jason Weston.",
                "A line of work focuses on automatically obtaining cleaner data [1, 46, 6]."
            ],
            "citingPaper": {
                "paperId": "b7e207b4f8e7285d60558ce0eeeb072c3fed2034",
                "externalIds": {
                    "ArXiv": "2305.12798",
                    "DBLP": "journals/corr/abs-2305-12798",
                    "DOI": "10.48550/arXiv.2305.12798",
                    "CorpusId": 258832702
                },
                "corpusId": 258832702,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b7e207b4f8e7285d60558ce0eeeb072c3fed2034",
                "title": "LM-Switch: Lightweight Language Model Conditioning in Word Embedding Space",
                "abstract": "In recent years, large language models (LMs) have achieved remarkable progress across various natural language processing tasks. As pre-training and fine-tuning are costly and might negatively impact model performance, it is desired to efficiently adapt an existing model to different conditions such as styles, sentiments or narratives, when facing different audiences or scenarios. However, efficient adaptation of a language model to diverse conditions remains an open challenge. This work is inspired by the observation that text conditions are often associated with selection of certain words in a context. Therefore we introduce LM-Switch, a theoretically grounded, lightweight and simple method for generative language model conditioning. We begin by investigating the effect of conditions in Hidden Markov Models (HMMs), and establish a theoretical connection with language model. Our finding suggests that condition shifts in HMMs are associated with linear transformations in word embeddings. LM-Switch is then designed to deploy a learnable linear factor in the word embedding space for language model conditioning. We show that LM-Switch can model diverse tasks, and achieves comparable or better performance compared with state-of-the-art baselines in LM detoxification and generation control, despite requiring no more than 1% of parameters compared with baselines and little extra time overhead compared with base LMs. It is also able to learn from as few as a few sentences or one document. Moreover, a learned LM-Switch can be transferred to other LMs of different sizes, achieving a detoxification performance similar to the best baseline. We will make our code available to the research community following publication.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2118642562",
                        "name": "Chi Han"
                    },
                    {
                        "authorId": "9097644",
                        "name": "Jialiang Xu"
                    },
                    {
                        "authorId": "2118482058",
                        "name": "Manling Li"
                    },
                    {
                        "authorId": "51135899",
                        "name": "Y. Fung"
                    },
                    {
                        "authorId": "1726046634",
                        "name": "Chenkai Sun"
                    },
                    {
                        "authorId": "2057958465",
                        "name": "Nan Jiang"
                    },
                    {
                        "authorId": "1730531",
                        "name": "T. Abdelzaher"
                    },
                    {
                        "authorId": "2072975661",
                        "name": "Heng Ji"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Previous work typically focused on some specific biases in dialogue systems, such as gender [15, 29, 30, 46], race [14, 46], social class [46] and profession [14]."
            ],
            "citingPaper": {
                "paperId": "744a98cc2736fa71d3984602e10b68319a47c65e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-12434",
                    "ArXiv": "2305.12434",
                    "DOI": "10.48550/arXiv.2305.12434",
                    "CorpusId": 258833296
                },
                "corpusId": 258833296,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/744a98cc2736fa71d3984602e10b68319a47c65e",
                "title": "BiasAsker: Measuring the Bias in Conversational AI System",
                "abstract": "Powered by advanced Artificial Intelligence (AI) techniques, conversational AI systems, such as ChatGPT and digital assistants like Siri, have been widely deployed in daily life. However, such systems may still produce content containing biases and stereotypes, causing potential social problems. Due to the data-driven, black-box nature of modern AI techniques, comprehensively identifying and measuring biases in conversational systems remains a challenging task. Particularly, it is hard to generate inputs that can comprehensively trigger potential bias due to the lack of data containing both social groups as well as biased properties. In addition, modern conversational systems can produce diverse responses (e.g., chatting and explanation), which makes existing bias detection methods simply based on the sentiment and the toxicity hardly being adopted. In this paper, we propose BiasAsker, an automated framework to identify and measure social bias in conversational AI systems. To obtain social groups and biased properties, we construct a comprehensive social bias dataset, containing a total of 841 groups and 8,110 biased properties. Given the dataset, BiasAsker automatically generates questions and adopts a novel method based on existence measurement to identify two types of biases (i.e., absolute bias and related bias) in conversational systems. Extensive experiments on 8 commercial systems and 2 famous research models, such as ChatGPT and GPT-3, show that 32.83% of the questions generated by BiasAsker can trigger biased behaviors in these widely deployed conversational systems. All the code, data, and experimental results have been released to facilitate future research.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2167583580",
                        "name": "Yuxuan Wan"
                    },
                    {
                        "authorId": "2144328160",
                        "name": "Wenxuan Wang"
                    },
                    {
                        "authorId": "40532404",
                        "name": "Pinjia He"
                    },
                    {
                        "authorId": null,
                        "name": "Jiazhen Gu"
                    },
                    {
                        "authorId": "47468054",
                        "name": "Haonan Bai"
                    },
                    {
                        "authorId": "2146840128",
                        "name": "Michael R. Lyu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Moreover, existing works (Barikeri et al., 2021; Dinan et al., 2020; Liu et al., 2020b) only focus on English dialogue models.",
                "Dinan et al. (2020) propose new techniques to mitigate gender bias by balancing the genderedness of generated dialogue utterances."
            ],
            "citingPaper": {
                "paperId": "839cc546b58968e2a8cb968337fb2e3a279e2b00",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-11262",
                    "ACL": "2023.acl-long.757",
                    "ArXiv": "2305.11262",
                    "DOI": "10.48550/arXiv.2305.11262",
                    "CorpusId": 258823380
                },
                "corpusId": 258823380,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/839cc546b58968e2a8cb968337fb2e3a279e2b00",
                "title": "CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models",
                "abstract": "redWarning: This paper contains content that may be offensive or upsetting.Pretrained conversational agents have been exposed to safety issues, exhibiting a range of stereotypical human biases such as gender bias. However, there are still limited bias categories in current research, and most of them only focus on English. In this paper, we introduce a new Chinese dataset, CHBias, for bias evaluation and mitigation of Chinese conversational language models.Apart from those previous well-explored bias categories, CHBias includes under-explored bias categories, such as ageism and appearance biases, which received less attention. We evaluate two popular pretrained Chinese conversational models, CDial-GPT and EVA2.0, using CHBias. Furthermore, to mitigate different biases, we apply several debiasing methods to the Chinese pretrained models. Experimental results show that these Chinese pretrained models are potentially risky for generating texts that contain social biases, and debiasing methods using the proposed dataset can make response generation less biased while preserving the models\u2019 conversational capabilities.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1481819499",
                        "name": "Jiaxu Zhao"
                    },
                    {
                        "authorId": "2111764425",
                        "name": "Meng Fang"
                    },
                    {
                        "authorId": "2110009929",
                        "name": "Zijing Shi"
                    },
                    {
                        "authorId": "50024168",
                        "name": "Yitong Li"
                    },
                    {
                        "authorId": "2145142074",
                        "name": "Ling Chen"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "TGNB Harm Evaluations in LLMs Gender bias evaluation methods include toxicity measurements and word co-occurrence in OLG [23, 25, 37, 40, 59, 61].",
                "Results reflect more robust pronoun consistency for binary pronouns (\u00a74.2), the usage of generic masculine language during OLG (\u00a74.3), less toxic language when disclosing binary gender (\u00a75.2, \u00a75.3), and examples of invasive TGNB commentary (\u00a75.2).",
                "In NLG, [23] create a dataset of prompts to assess for harms in OLG across various domains (e.g., politics, occupation) using Wikipedia.",
                "Motivation To assess LLMs for misgendering behavior in OLG, we create an automatic misgendering evaluation tool.",
                "6https://github.com/anaeliaovalle/TANGO-Centering-Transgender-NonbinaryVoices-for-OLG-BiasEval 7Addressing someone using a pronoun that does match their gender identity.",
                "2 RELATEDWORK TGNB Harm Evaluations in LLMs Gender bias evaluation methods include toxicity measurements and word co-occurrence in OLG [23, 25, 37, 40, 59, 61].",
                "While the tool cannot be used with multiple referents, it is a good starting point for OLG misgendering assessments.",
                "We leverage these findings to drive our OLG harm assessment framework by asking two questions: (1) To what extent is gender non-affirmation in the form of misgendering present in models used for OLG? and (2) To what extent is gender non-affirmation in the form of negative responses to gender identity disclosure present in models used for OLG?",
                "C L\n] 1\nJ un\nwe illuminate ways in which harms may manifest in OLG for members of the queer2 community, specifically those who identify as transgender and nonbinary (TGNB).",
                "[25] Emily Dinan, Angela Fan, Adina Williams, Jack Urbanek, Douwe Kiela, and Jason Weston.",
                "This work centers the TGNB community by focusing on experienced and documented gender minoritization and marginalization to carefully guide the design of TGNB harm evaluations in OLG.",
                "(3) With these findings, we provide constructive suggestions for creating more gender-inclusive LLMs in each OLG experiment.",
                "In this section, we conduct OLG experiments that explore if and howmodels misgender individuals in text.",
                "Large language models (LLM) are being increasingly utilized for open language generation (OLG) in spaces such as content creation (e.g., story creation) and conversational AI (e.g., voice assistants, voice user interfaces).",
                "To address this gap, we center the experiences of the TGNB community to help inform the design of new harm evaluation techniques in OLG.",
                "We translate this to our work, asking if this behavior is reflected in OLG.",
                "MotivationWe draw from linguistics literature to further investigate misgendering behavior in OLG.",
                "Moreover, there is a dearth of knowledge on how the social reality surrounding experienced marginalization by TGNB persons contributes to and persists within OLG systems.",
                "To determine if this behavior persists in LLMs, we create a dataset to evaluate misgendering in OLG.",
                "This section measures possible harmful language in OLG across several forms of disclosing TGNB genders."
            ],
            "citingPaper": {
                "paperId": "233b18336720a311fd7c82b70481584c8c7edb87",
                "externalIds": {
                    "DBLP": "conf/fat/OvalleGDJCGZ023",
                    "ArXiv": "2305.09941",
                    "DOI": "10.1145/3593013.3594078",
                    "CorpusId": 258741207
                },
                "corpusId": 258741207,
                "publicationVenue": {
                    "id": "cdc83875-a82d-445c-b097-cbe91afe99a8",
                    "name": "Conference on Fairness, Accountability and Transparency",
                    "type": "conference",
                    "alternate_names": [
                        "FAccT",
                        "Conf Fairness Account Transpar"
                    ],
                    "url": "https://facctconference.org/"
                },
                "url": "https://www.semanticscholar.org/paper/233b18336720a311fd7c82b70481584c8c7edb87",
                "title": "\u201cI\u2019m fully who I am\u201d: Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation",
                "abstract": "Warning: This paper contains examples of gender non-affirmative language which could be offensive, upsetting, and/or triggering. Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization of TGNB persons contributes to and persists within Open Language Generation (OLG). This social knowledge serves as a guide for evaluating popular large language models (LLMs) on two key aspects: (1) misgendering and (2) harmful responses to gender disclosure. To do this, we introduce TANGO, a dataset of template-based real-world text curated from a TGNB-oriented community. We discover a dominance of binary gender norms reflected by the models; LLMs least misgendered subjects in generated text when triggered by prompts whose subjects used binary pronouns. Meanwhile, misgendering was most prevalent when triggering generation with singular they and neopronouns. When prompted with gender disclosures, TGNB disclosure generated the most stigmatizing language and scored most toxic, on average. Our findings warrant further research on how TGNB harms manifest in LLMs and serve as a broader case study toward concretely grounding the design of gender-inclusive AI in community voices and interdisciplinary literature.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51494507",
                        "name": "Anaelia Ovalle"
                    },
                    {
                        "authorId": "3436466",
                        "name": "Palash Goyal"
                    },
                    {
                        "authorId": "3475586",
                        "name": "J. Dhamala"
                    },
                    {
                        "authorId": "20742874",
                        "name": "Zachary Jaggers"
                    },
                    {
                        "authorId": "2110821190",
                        "name": "Kai Wei Chang"
                    },
                    {
                        "authorId": "143728483",
                        "name": "A. Galstyan"
                    },
                    {
                        "authorId": "1804104",
                        "name": "R. Zemel"
                    },
                    {
                        "authorId": "2139538015",
                        "name": "Rahul Gupta"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "ea5506172739270ff59384b759ba2a2689566ed6",
                "externalIds": {
                    "DBLP": "journals/ipm/SongGLSX23",
                    "DOI": "10.1016/j.ipm.2023.103277",
                    "CorpusId": 256671124
                },
                "corpusId": 256671124,
                "publicationVenue": {
                    "id": "37f5b9b7-f828-4ae1-a174-45b538cbd4e4",
                    "name": "Information Processing & Management",
                    "type": "journal",
                    "alternate_names": [
                        "Inf Process Manag",
                        "Inf Process  Manag",
                        "Information Processing and Management"
                    ],
                    "issn": "0306-4573",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/244/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/information-processing-and-management/",
                        "http://www.sciencedirect.com/science/journal/03064573",
                        "http://www.journals.elsevier.com/information-processing-and-management/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ea5506172739270ff59384b759ba2a2689566ed6",
                "title": "Measuring and mitigating language model biases in abusive language detection",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2067622524",
                        "name": "Rui Song"
                    },
                    {
                        "authorId": "1720285",
                        "name": "Fausto Giunchiglia"
                    },
                    {
                        "authorId": "2116202874",
                        "name": "Yingji Li"
                    },
                    {
                        "authorId": "1633040166",
                        "name": "Lida Shi"
                    },
                    {
                        "authorId": "2118498995",
                        "name": "Hao Xu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "While some of these have been studied in previous works and mitigations are included (Dinan et al., 2019), we",
                "While some of these have been studied in previous works and mitigations are included (Dinan et al., 2019), we acknowledge that these issues are not entirely resolved."
            ],
            "citingPaper": {
                "paperId": "82beb8a86d438e85a134182128d47607b1b04004",
                "externalIds": {
                    "ArXiv": "2304.13835",
                    "DBLP": "journals/corr/abs-2304-13835",
                    "DOI": "10.48550/arXiv.2304.13835",
                    "CorpusId": 258352487
                },
                "corpusId": 258352487,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/82beb8a86d438e85a134182128d47607b1b04004",
                "title": "Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models",
                "abstract": "Current dialogue research primarily studies pairwise (two-party) conversations, and does not address the everyday setting where more than two speakers converse together. In this work, we both collect and evaluate multi-party conversations to study this more general case. We use the LIGHT environment to construct grounded conversations, where each participant has an assigned character to role-play. We thus evaluate the ability of language models to act as one or more characters in such conversations. Models require two skills that pairwise-trained models appear to lack: (1) being able to decide when to talk; (2) producing coherent utterances grounded on multiple characters. We compare models trained on our new dataset to existing pairwise-trained dialogue models, as well as large language models with few-shot prompting. We find that our new dataset, MultiLIGHT, which we will publicly release, can help bring significant improvements in the group setting.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2215503937",
                        "name": "Jimmy Wei"
                    },
                    {
                        "authorId": "35752280",
                        "name": "Kurt Shuster"
                    },
                    {
                        "authorId": "3149531",
                        "name": "Arthur Szlam"
                    },
                    {
                        "authorId": "145183709",
                        "name": "J. Weston"
                    },
                    {
                        "authorId": "39219656",
                        "name": "Jack Urbanek"
                    },
                    {
                        "authorId": "100653935",
                        "name": "M. Komeili"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Dinan et al. (2020); Zhao et al. (2017, 2018) propose approaches to mitigate gender bias in LLMs with regularization objectives and counterfactual data augmentation."
            ],
            "citingPaper": {
                "paperId": "281a7a99c16ce8f53bfbfb7aeb460dbd28648d28",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-05335",
                    "ArXiv": "2304.05335",
                    "DOI": "10.48550/arXiv.2304.05335",
                    "CorpusId": 258060002
                },
                "corpusId": 258060002,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/281a7a99c16ce8f53bfbfb7aeb460dbd28648d28",
                "title": "Toxicity in ChatGPT: Analyzing Persona-assigned Language Models",
                "abstract": "Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Therefore, a clear understanding of the capabilities and limitations of LLMs is necessary. To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. This may be potentially defamatory to the persona and harmful to an unsuspecting user. Furthermore, we find concerning patterns where specific entities (e.g., certain races) are targeted more than others (3x more) irrespective of the assigned persona, that reflect inherent discriminatory biases in the model. We hope that our findings inspire the broader AI community to rethink the efficacy of current safety guardrails and develop better techniques that lead to robust, safe, and trustworthy AI systems.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "33341943",
                        "name": "A. Deshpande"
                    },
                    {
                        "authorId": "46258988",
                        "name": "Vishvak Murahari"
                    },
                    {
                        "authorId": "2590556",
                        "name": "Tanmay Rajpurohit"
                    },
                    {
                        "authorId": "51043791",
                        "name": "A. Kalyan"
                    },
                    {
                        "authorId": "2135381714",
                        "name": "Karthik Narasimhan"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Typical debiasing methods include counterfactual data augmentation (Zmigrod et al., 2019; Dinan et al., 2019; Webster et al., 2020; Barikeri et al., 2021), dropout regularization (Webster et al.",
                "Typical debiasing methods include counterfactual data augmentation (Zmigrod et al., 2019; Dinan et al., 2019; Webster et al., 2020; Barikeri et al., 2021), dropout regularization (Webster et al., 2020), self-debias (Schick et al., 2021), sentence embedding debias (Liang et al., 2020), and iterative\u2026"
            ],
            "citingPaper": {
                "paperId": "b3dcd48b68bdbb304fa53299496539c054638e0c",
                "externalIds": {
                    "ACL": "2023.eacl-main.89",
                    "DBLP": "conf/eacl/LuoG23",
                    "ArXiv": "2303.05670",
                    "DOI": "10.48550/arXiv.2303.05670",
                    "CorpusId": 257482682
                },
                "corpusId": 257482682,
                "publicationVenue": {
                    "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
                    "name": "Conference of the European Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Eur Chapter Assoc Comput Linguistics",
                        "EACL"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/eacl/"
                },
                "url": "https://www.semanticscholar.org/paper/b3dcd48b68bdbb304fa53299496539c054638e0c",
                "title": "Logic Against Bias: Textual Entailment Mitigates Stereotypical Sentence Reasoning",
                "abstract": "Due to their similarity-based learning objectives, pretrained sentence encoders often internalize stereotypical assumptions that reflect the social biases that exist within their training corpora. In this paper, we describe several kinds of stereotypes concerning different communities that are present in popular sentence representation models, including pretrained next sentence prediction and contrastive sentence representation models. We compare such models to textual entailment models that learn language logic for a variety of downstream language understanding tasks. By comparing strong pretrained models based on text similarity with textual entailment learning, we conclude that the explicit logic learning with textual entailment can significantly reduce bias and improve the recognition of social communities, without an explicit de-biasing process.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1944274",
                        "name": "Hongyin Luo"
                    },
                    {
                        "authorId": "144536129",
                        "name": "James Glass"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "812867453701f0929579e481bd15dfb0ebee1d7e",
                "externalIds": {
                    "ArXiv": "2302.13136",
                    "DBLP": "conf/aistats/WangCH23",
                    "DOI": "10.48550/arXiv.2302.13136",
                    "CorpusId": 257220117
                },
                "corpusId": 257220117,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/812867453701f0929579e481bd15dfb0ebee1d7e",
                "title": "Toward Fairness in Text Generation via Mutual Information Minimization based on Importance Sampling",
                "abstract": "Pretrained language models (PLMs), such as GPT2, have achieved remarkable empirical performance in text generation tasks. However, pretrained on large-scale natural language corpora, the generated text from PLMs may exhibit social bias against disadvantaged demographic groups. To improve the fairness of PLMs in text generation, we propose to minimize the mutual information between the semantics in the generated text sentences and their demographic polarity, i.e., the demographic group to which the sentence is referring. In this way, the mentioning of a demographic group (e.g., male or female) is encouraged to be independent from how it is described in the generated text, thus effectively alleviating the social bias. Moreover, we propose to efficiently estimate the upper bound of the above mutual information via importance sampling, leveraging a natural language corpus. We also propose a distillation mechanism that preserves the language modeling ability of the PLMs after debiasing. Empirical results on real-world benchmarks demonstrate that the proposed method yields superior performance in term of both fairness and language modeling ability.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "39077217",
                        "name": "Rui Wang"
                    },
                    {
                        "authorId": "144533942",
                        "name": "Pengyu Cheng"
                    },
                    {
                        "authorId": "145153424",
                        "name": "Ricardo Henao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Wolf et al. (2019) prepend persona sentences to personalize the history; while Su et al. (2022); Dinan et al. (2020); Keskar et al. (2019); Xu et al. (2020a) prepending task-specific signals to prompt and control the model."
            ],
            "citingPaper": {
                "paperId": "5c982f5dbb653d748e1ada858004e0847e09edff",
                "externalIds": {
                    "ACL": "2023.dialdoc-1.4",
                    "ArXiv": "2302.05888",
                    "DBLP": "journals/corr/abs-2302-05888",
                    "DOI": "10.48550/arXiv.2302.05888",
                    "CorpusId": 256827670
                },
                "corpusId": 256827670,
                "publicationVenue": {
                    "id": "fc51fcd0-8420-4934-b76d-e46d8a084906",
                    "name": "Workshop on Document-grounded Dialogue and Conversational Question Answering",
                    "type": "conference",
                    "alternate_names": [
                        "DialDoc",
                        "Workshop Doc Dialogue Conversational Quest Answering"
                    ],
                    "url": "https://aclanthology.org/venues/dialdoc/"
                },
                "url": "https://www.semanticscholar.org/paper/5c982f5dbb653d748e1ada858004e0847e09edff",
                "title": "Position Matters! Empirical Study of Order Effect in Knowledge-grounded Dialogue",
                "abstract": "With the power of large pretrained language models, various research works have integrated knowledge into dialogue systems. The traditional techniques treat knowledge as part of the input sequence for the dialogue system, prepending a set of knowledge statements in front of dialogue history.However, such a mechanism forces knowledge sets to be concatenated in an ordered manner, making models implicitly pay imbalanced attention to the sets during training.In this paper, we first investigate how the order of the knowledge set can influence autoregressive dialogue systems\u2019 responses. We conduct experiments on two commonly used dialogue datasets with two types of transformer-based models and find that models view the input knowledge unequally. To this end, we propose a simple and novel technique to alleviate the order effect by modifying the position embeddings of knowledge input in these models. With the proposed position embedding method, the experimental results show that each knowledge statement is uniformly considered to generate responses.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2087042748",
                        "name": "Hsuan Su"
                    },
                    {
                        "authorId": "2109680564",
                        "name": "Shachi H. Kumar"
                    },
                    {
                        "authorId": "2066417452",
                        "name": "Sahisnu Mazumder"
                    },
                    {
                        "authorId": "73227155",
                        "name": "Wenda Chen"
                    },
                    {
                        "authorId": "2175808",
                        "name": "R. Manuvinakurike"
                    },
                    {
                        "authorId": "3442103",
                        "name": "Eda Okur"
                    },
                    {
                        "authorId": "38531701",
                        "name": "Saurav Sahay"
                    },
                    {
                        "authorId": "1896095",
                        "name": "L. Nachman"
                    },
                    {
                        "authorId": "2187684073",
                        "name": "Shang-Tse Chen"
                    },
                    {
                        "authorId": "1706104",
                        "name": "Hung-yi Lee"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "For example, these models often exhibit social biases (Barikeri et al., 2021; Dinan et al., 2020) and inappropriately align them-",
                "For example, these models often exhibit social biases (Barikeri et al., 2021; Dinan et al., 2020) and inappropriately align them-\n\u2217Work done during an internship at Amazon Alexa AI.",
                "Keskar et al. (2019) and Dinan et al. (2020) investigated using control signals to condition generation from language models."
            ],
            "citingPaper": {
                "paperId": "3c6d41ee2c3c78509b6a47e0d097087b99006ca6",
                "externalIds": {
                    "ArXiv": "2302.00871",
                    "DBLP": "journals/corr/abs-2302-00871",
                    "DOI": "10.48550/arXiv.2302.00871",
                    "CorpusId": 256503647
                },
                "corpusId": 256503647,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3c6d41ee2c3c78509b6a47e0d097087b99006ca6",
                "title": "Using In-Context Learning to Improve Dialogue Safety",
                "abstract": "While large neural-based conversational models have become increasingly proficient dialogue agents, recent work has highlighted safety issues with these systems. For example, these systems can be goaded into generating toxic content, which often perpetuates social biases or stereotypes. We investigate a retrieval-based method for reducing bias and toxicity in responses from chatbots. It uses in-context learning to steer a model towards safer generations. Concretely, to generate a response to an unsafe dialogue context, we retrieve demonstrations of safe responses to similar dialogue contexts. We find our method performs competitively with strong baselines without requiring training. For instance, using automatic evaluation, we find our best fine-tuned baseline only generates safe responses to unsafe dialogue contexts from DiaSafety 4.04% more than our approach. Finally, we also propose a re-ranking procedure which can further improve response safeness.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "150247363",
                        "name": "Nicholas Meade"
                    },
                    {
                        "authorId": "2921001",
                        "name": "Spandana Gella"
                    },
                    {
                        "authorId": "8223433",
                        "name": "Devamanyu Hazarika"
                    },
                    {
                        "authorId": "1491232062",
                        "name": "Prakhar Gupta"
                    },
                    {
                        "authorId": "2152284471",
                        "name": "Di Jin"
                    },
                    {
                        "authorId": "145732771",
                        "name": "Siva Reddy"
                    },
                    {
                        "authorId": "2152797401",
                        "name": "Yang Liu"
                    },
                    {
                        "authorId": "1395813836",
                        "name": "Dilek Z. Hakkani-T\u00fcr"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Work has already been done to address some of these issues particularly when it comes to character representation and dialogue (Dinan et al., 2019), however no analysis has been done on the underlying objects or learned relationships."
            ],
            "citingPaper": {
                "paperId": "1a5a30c055bb6f76abbd559e9bdb836740f2d64c",
                "externalIds": {
                    "ArXiv": "2301.05746",
                    "DBLP": "journals/corr/abs-2301-05746",
                    "DOI": "10.48550/arXiv.2301.05746",
                    "CorpusId": 255942060
                },
                "corpusId": 255942060,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1a5a30c055bb6f76abbd559e9bdb836740f2d64c",
                "title": "Infusing Commonsense World Models with Graph Knowledge",
                "abstract": "While language models have become more capable of producing compelling language, we find there are still gaps in maintaining consistency, especially when describing events in a dynamically changing world. We study the setting of generating narratives in an open world text adventure game, where a graph representation of the underlying game state can be used to train models that consume and output both grounded graph representations and natural language descriptions and actions. We build a large set of tasks by combining crowdsourced and simulated gameplays with a novel dataset of complex actions in order to to construct such models. We find it is possible to improve the consistency of action narration models by training on graph contexts and targets, even if graphs are not present at test time. This is shown both in automatic metrics and human evaluations. We plan to release our code, the new set of tasks, and best performing models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2201328230",
                        "name": "Alexander Gurung"
                    },
                    {
                        "authorId": "100653935",
                        "name": "M. Komeili"
                    },
                    {
                        "authorId": "3149531",
                        "name": "Arthur Szlam"
                    },
                    {
                        "authorId": "145183709",
                        "name": "J. Weston"
                    },
                    {
                        "authorId": "39219656",
                        "name": "Jack Urbanek"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Both language models and curated datasets often exhibit demographic imbalances (Dinan et al., 2020; Weidinger et al., 2021; Sheng et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "f8c041f0bd857c148788c284ae48117b70985558",
                "externalIds": {
                    "ArXiv": "2212.10465",
                    "DBLP": "journals/corr/abs-2212-10465",
                    "DOI": "10.48550/arXiv.2212.10465",
                    "CorpusId": 254877312
                },
                "corpusId": 254877312,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f8c041f0bd857c148788c284ae48117b70985558",
                "title": "SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization",
                "abstract": "We present SODA: the first publicly available, million-scale high-quality social dialogue dataset. In contrast to most existing crowdsourced, small-scale dialogue corpora, we distill 1.5M socially-grounded dialogues from a large language model (InstructGPT; Ouyang et al., 2022). Dialogues are distilled by contextualizing social commonsense knowledge from a knowledge graph (Atomic10x; West et al., 2022). Human evaluation shows that dialogues in SODA are more consistent, specific, and (surprisingly) natural than those in prior human-authored datasets. Using SODA, we train COSMO: a generalizable conversation model that is significantly more natural and consistent on unseen datasets than best-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna). Experiments reveal COSMO is sometimes even preferred to the original human-written gold responses. Additionally, our results shed light on the distinction between knowledge-enriched conversations and natural social chitchats. We make our data, models, and code public.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "32609381",
                        "name": "Hyunwoo Kim"
                    },
                    {
                        "authorId": "2689239",
                        "name": "Jack Hessel"
                    },
                    {
                        "authorId": "2112504145",
                        "name": "Liwei Jiang"
                    },
                    {
                        "authorId": "50085131",
                        "name": "Ximing Lu"
                    },
                    {
                        "authorId": "7877122",
                        "name": "Youngjae Yu"
                    },
                    {
                        "authorId": "1557324013",
                        "name": "Pei Zhou"
                    },
                    {
                        "authorId": "39227408",
                        "name": "Ronan Le Bras"
                    },
                    {
                        "authorId": "2715920",
                        "name": "Malihe Alikhani"
                    },
                    {
                        "authorId": "70308241",
                        "name": "Gunhee Kim"
                    },
                    {
                        "authorId": "2729164",
                        "name": "Maarten Sap"
                    },
                    {
                        "authorId": "1699545",
                        "name": "Yejin Choi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Controlling Dialogue Systems has been a focus of research to generate engaging responses (Ghazarian et al., 2021), prevent toxic content and biases (Dinan et al., 2020; Xu et al., 2021a), steer the conversation towards specific keywords or topics (Tang et al., 2019; Gupta et al., 2022a), and ground\u2026",
                ", 2021), prevent toxic content and biases (Dinan et al., 2020; Xu et al., 2021a), steer the conversation towards specific keywords or topics (Tang et al."
            ],
            "citingPaper": {
                "paperId": "8616da9215843353f2169916766054dfbd50a671",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-10557",
                    "ArXiv": "2212.10557",
                    "DOI": "10.48550/arXiv.2212.10557",
                    "CorpusId": 254877561
                },
                "corpusId": 254877561,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8616da9215843353f2169916766054dfbd50a671",
                "title": "DialGuide: Aligning Dialogue Model Behavior with Developer Guidelines",
                "abstract": "Dialogue models are able to generate coherent and fluent responses, but they can still be challenging to control and may produce non-engaging, unsafe results. This unpredictability diminishes user trust and can hinder the use of the models in the real world. To address this, we introduce DialGuide, a novel framework for controlling dialogue model behavior using natural language rules, or guidelines. These guidelines provide information about the context they are applicable to and what should be included in the response, allowing the models to generate responses that are more closely aligned with the developer's expectations and intent. We evaluate DialGuide on three tasks in open-domain dialogue response generation: guideline selection, response generation, and response entailment verification. Our dataset contains 10,737 positive and 15,467 negative dialogue context-response-guideline triplets across two domains - chit-chat and safety. We provide baseline models for the tasks and benchmark their performance. We also demonstrate that DialGuide is effective in the dialogue safety domain, producing safe and engaging responses that follow developer guidelines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1491232062",
                        "name": "Prakhar Gupta"
                    },
                    {
                        "authorId": "2152797401",
                        "name": "Yang Liu"
                    },
                    {
                        "authorId": "2152284471",
                        "name": "Di Jin"
                    },
                    {
                        "authorId": "2127328167",
                        "name": "Behnam Hedayatnia"
                    },
                    {
                        "authorId": "2921001",
                        "name": "Spandana Gella"
                    },
                    {
                        "authorId": "30986714",
                        "name": "Sijia Liu"
                    },
                    {
                        "authorId": "26882347",
                        "name": "P. Lange"
                    },
                    {
                        "authorId": "32057282",
                        "name": "J. Hirschberg"
                    },
                    {
                        "authorId": "1395813836",
                        "name": "Dilek Z. Hakkani-T\u00fcr"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "df89991976988fbade2927021b11a3ff9167ed9b",
                "externalIds": {
                    "DBLP": "journals/nle/ChurchSOCK23",
                    "DOI": "10.1017/s1351324922000481",
                    "CorpusId": 254903643
                },
                "corpusId": 254903643,
                "publicationVenue": {
                    "id": "b0dc264e-1ef6-4c58-be54-d2e6137ac35f",
                    "name": "Natural Language Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "Nat Lang Eng"
                    ],
                    "issn": "1351-3249",
                    "url": "https://www.cambridge.org/core/journals/natural-language-engineering",
                    "alternate_urls": [
                        "http://journals.cambridge.org/action/displayJournal?jid=NLE"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/df89991976988fbade2927021b11a3ff9167ed9b",
                "title": "Emerging trends: Unfair, biased, addictive, dangerous, deadly, and insanely profitable",
                "abstract": "\n There has been considerable work recently in the natural language community and elsewhere on Responsible AI. Much of this work focuses on fairness and biases (henceforth Risks 1.0), following the 2016 best seller: Weapons of Math Destruction. Two books published in 2022,\u00a0The Chaos Machine and Like, Comment, Subscribe, raise additional risks to public health/safety/security such as genocide, insurrection, polarized politics, vaccinations (henceforth, Risks 2.0). These books suggest that the use of machine learning to maximize engagement in social media has created a Frankenstein Monster that is exploiting human weaknesses with persuasive technology, the illusory truth effect, Pavlovian conditioning, and Skinner\u2019s intermittent variable reinforcement. Just as we cannot expect tobacco companies to sell fewer cigarettes and prioritize public health ahead of profits, so too, it may be asking too much of companies (and countries) to stop trafficking in misinformation given that it is so effective and so insanely profitable (at least in the short term). Eventually, we believe the current chaos will end, like the lawlessness in Wild West, because chaos is bad for business. As computer scientists, this paper will summarize criticisms from other fields and focus on implications for computer science; we will not attempt to contribute to those other fields. There is quite a bit of work in computer science on these risks, especially on Risks 1.0 (bias and fairness), but more work is needed, especially on Risks 2.0 (addictive, dangerous, and deadly).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1737832816",
                        "name": "Kenneth Ward Church"
                    },
                    {
                        "authorId": "33858862",
                        "name": "Annika Marie Schoene"
                    },
                    {
                        "authorId": "2159737809",
                        "name": "John E. Ortega"
                    },
                    {
                        "authorId": "35893789",
                        "name": "Raman Chandrasekar"
                    },
                    {
                        "authorId": "2121386495",
                        "name": "Valia Kordoni"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "fecbce223a6da402b6df875dfa5ff0fd3fc457c2",
                "externalIds": {
                    "ACL": "2023.sigdial-1.1",
                    "DBLP": "journals/corr/abs-2212-02745",
                    "ArXiv": "2212.02745",
                    "DOI": "10.48550/arXiv.2212.02745",
                    "CorpusId": 254275175
                },
                "corpusId": 254275175,
                "publicationVenue": {
                    "id": "6a470734-72c6-4809-a07d-d34dee0df4a1",
                    "name": "SIGDIAL Conferences",
                    "type": "conference",
                    "alternate_names": [
                        "SIGDIAL",
                        "SIGDIAL Conf",
                        "Annu Meet Sp\u00e9c Interest Group Discourse Dialogue",
                        "Annual Meeting of the Special Interest Group on Discourse and Dialogue"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fecbce223a6da402b6df875dfa5ff0fd3fc457c2",
                "title": "Sources of Noise in Dialogue and How to Deal with Them",
                "abstract": "Training dialogue systems often entails dealing with noisy training examples and unexpected user inputs. Despite their prevalence, there currently lacks an accurate survey of dialogue noise, nor is there a clear sense of the impact of each noise type on task performance. This paper addresses this gap by first constructing a taxonomy of noise encountered by dialogue systems. In addition, we run a series of experiments to show how different models behave when subjected to varying levels of noise and types of noise. Our results reveal that models are quite robust to label errors commonly tackled by existing denoising algorithms, but that performance suffers from dialogue-specific noise. Driven by these observations, we design a data cleaning algorithm specialized for conversational settings and apply it as a proof-of-concept for targeted dialogue denoising.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51250248",
                        "name": "Derek Chen"
                    },
                    {
                        "authorId": "2167255986",
                        "name": "Zhou Yu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "\u2026have explored multilingual scenarios (e.g., Lauscher and Glava\u0161, 2019; Lauscher et al., 2020c; Ahn and Oh, 2021), more fine-grained biases (Dinan et al., 2020b), and more biases, beyond the prominent sexism and racism dimensions (e.g., Zhao et al., 2018; Rudinger et al., 2018), like\u2026",
                "Here, Webster et al. (2020) and Lauscher et al. (2021) rely on CDA (Zhao et al., 2018) and Dinan et al. (2020a) use control codes to guide the biases.",
                ", for dialog (e.g., Sheng et al., 2019; Dinan et al., 2020a; Barikeri et al., 2021), co-reference resolution (Zhao et al.",
                "\u20262019; Qian et al., 2019; Webster et al., 2020; Nangia et al., 2020; Sap et al., 2020) and in downstream scenarios, e.g., for dialog (e.g., Sheng et al., 2019; Dinan et al., 2020a; Barikeri et al., 2021), co-reference resolution (Zhao et al., 2018), and NLI (Rudinger et al., 2017; Dev et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "a8c09c41f39d798dc4201eeec1452fe617e428df",
                "externalIds": {
                    "ArXiv": "2211.04256",
                    "DBLP": "journals/corr/abs-2211-04256",
                    "ACL": "2022.emnlp-main.533",
                    "DOI": "10.48550/arXiv.2211.04256",
                    "CorpusId": 253397743
                },
                "corpusId": 253397743,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/a8c09c41f39d798dc4201eeec1452fe617e428df",
                "title": "Bridging Fairness and Environmental Sustainability in Natural Language Processing",
                "abstract": "Fairness and environmental impact are important research directions for the sustainable development of artificial intelligence. However, while each topic is an active research area in natural language processing (NLP), there is a surprising lack of research on the interplay between the two fields. This lacuna is highly problematic, since there is increasing evidence that an exclusive focus on fairness can actually hinder environmental sustainability, and vice versa. In this work, we shed light on this crucial intersection in NLP by (1) investigating the efficiency of current fairness approaches through surveying example methods for reducing unfair stereotypical bias from the literature, and (2) evaluating a common technique to reduce energy consumption (and thus environmental impact) of English NLP models, knowledge distillation (KD), for its impact on fairness. In this case study, we evaluate the effect of important KD factors, including layer and dimensionality reduction, with respect to: (a) performance on the distillation task (natural language inference and semantic similarity prediction), and (b) multiple measures and dimensions of stereotypical bias (e.g., gender bias measured via the Word Embedding Association Test). Our results lead us to clarify current assumptions regarding the effect of KD on unfair bias: contrary to other findings, we show that KD can actually decrease model fairness.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2190173964",
                        "name": "Marius Hessenthaler"
                    },
                    {
                        "authorId": "2268272",
                        "name": "Emma Strubell"
                    },
                    {
                        "authorId": "2022288",
                        "name": "Dirk Hovy"
                    },
                    {
                        "authorId": "29891652",
                        "name": "Anne Lauscher"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Native speakers may find grammatical errors produced by such systems silly, jarring, or even offensive, for example if the utterance refers to an entity with the wrong tense, formality, animacy, or gender (Dinan et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "632d5072fd8f5a2ee190678ce4b5324460765b41",
                "externalIds": {
                    "ACL": "2022.gem-1.6",
                    "ArXiv": "2211.02423",
                    "DBLP": "journals/corr/abs-2211-02423",
                    "DOI": "10.48550/arXiv.2211.02423",
                    "CorpusId": 253370738
                },
                "corpusId": 253370738,
                "publicationVenue": {
                    "id": "b1dc244e-c5b3-447d-b200-a46d55e8d8ee",
                    "name": "IEEE Games Entertainment Media Conference",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Game Entertain Media Conf",
                        "GEM",
                        "Int Conf Genet Evol Method",
                        "International Conference on Genetic and Evolutionary Methods"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/632d5072fd8f5a2ee190678ce4b5324460765b41",
                "title": "CLSE: Corpus of Linguistically Significant Entities",
                "abstract": "One of the biggest challenges of natural language generation (NLG) is the proper handling of named entities. Named entities are a common source of grammar mistakes such as wrong prepositions, wrong article handling, or incorrect entity inflection. Without factoring linguistic representation, such errors are often underrepresented when evaluating on a small set of arbitrarily picked argument values, or when translating a dataset from a linguistically simpler language, like English, to a linguistically complex language, like Russian. However, for some applications, broadly precise grammatical correctness is critical \u2013 native speakers may find entity-related grammar errors silly, jarring, or even offensive. To enable the creation of more linguistically diverse NLG datasets, we release a Corpus of Linguistically Significant Entities (CLSE) annotated by linguist experts. The corpus includes 34 languages and covers 74 different semantic types to support various applications from airline ticketing to video games. To demonstrate one possible use of CLSE, we produce an augmented version of the Schema-Guided Dialog Dataset, SGD-CLSE. Using the CLSE\u2019s entities and a small number of human translations, we create a linguistically representative NLG evaluation benchmark in three languages: French (high-resource), Marathi (low-resource), and Russian (highly inflected language). We establish quality baselines for neural, template-based, and hybrid NLG systems and discuss the strengths and weaknesses of each approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2405897",
                        "name": "A. Chuklin"
                    },
                    {
                        "authorId": "2145805787",
                        "name": "Justin Zhao"
                    },
                    {
                        "authorId": "26688118",
                        "name": "Mihir Kale"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "dba29a38290ff3dfbb8646ac976e48506a5f5101",
                "externalIds": {
                    "ACL": "2022.emnlp-main.344",
                    "DBLP": "conf/emnlp/KimKSHY22",
                    "ArXiv": "2210.12687",
                    "DOI": "10.48550/arXiv.2210.12687",
                    "CorpusId": 249364288
                },
                "corpusId": 249364288,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/dba29a38290ff3dfbb8646ac976e48506a5f5101",
                "title": "BotsTalk: Machine-sourced Framework for Automatic Curation of Large-scale Multi-skill Dialogue Datasets",
                "abstract": "To build open-domain chatbots that are able to use diverse communicative skills, we propose a novel framework BotsTalk, where multiple agents grounded to the specific target skills participate in a conversation to automatically annotate multi-skill dialogues. We further present Blended Skill BotsTalk (BSBT), a large-scale multi-skill dialogue dataset comprising 300K conversations. Through extensive experiments, we demonstrate that our dataset can be effective for multi-skill dialogue systems which require an understanding of skill blending as well as skill grounding. Our code and data are available at https://github.com/convei-lab/BotsTalk.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2110104000",
                        "name": "Minju Kim"
                    },
                    {
                        "authorId": "2184057435",
                        "name": "Chaehyeong Kim"
                    },
                    {
                        "authorId": "2188768271",
                        "name": "Yongho Song"
                    },
                    {
                        "authorId": "2153642272",
                        "name": "Seung-won Hwang"
                    },
                    {
                        "authorId": "1898428",
                        "name": "Jinyoung Yeo"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                ", 2021), debiasing (Dinan et al., 2020), to improving generalization, robustness (Dhole et al.",
                "For instance, Dinan et al. (2020) changes gendered words in a sentence to instill gender invariance for bias mitigation.",
                "In comparison, rule-based models can precisely control for such behaviors, mitigate bias (Dinan et al., 2020), or introduce invariance in embedding space specific to the needs of the downstream tasks.",
                "If used as data augmentation for training, the transformation might mitigate gender bias, as shown in (Dinan et al., 2020).",
                "Data augmentation in NLP can be useful in many situations, from low resource data setting, domain adaptation (Wei et al., 2021), debiasing (Dinan et al., 2020), to improving generalization, robustness (Dhole et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "5fd93eda50213f52d7f19f14efeb3ca5c58dc7bb",
                "externalIds": {
                    "ArXiv": "2210.13749",
                    "DBLP": "journals/corr/abs-2210-13749",
                    "ACL": "2022.aacl-main.30",
                    "DOI": "10.48550/arXiv.2210.13749",
                    "CorpusId": 253107551
                },
                "corpusId": 253107551,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5fd93eda50213f52d7f19f14efeb3ca5c58dc7bb",
                "title": "AugCSE: Contrastive Sentence Embedding with Diverse Augmentations",
                "abstract": "Data augmentation techniques have been proven useful in many applications in NLP fields. Most augmentations are task-specific, and cannot be used as a general-purpose tool. In our work, we present AugCSE, a unified framework to utilize diverse sets of data augmentations to achieve a better, general-purpose, sentence embedding model. Building upon the latest sentence embedding models, our approach uses a simple antagonistic discriminator that differentiates the augmentation types. With the finetuning objective borrowed from domain adaptation, we show that diverse augmentations, which often lead to conflicting contrastive signals, can be tamed to produce a better and more robust sentence representation. Our methods achieve state-of-the-art results on downstream transfer tasks and perform competitively on semantic textual similarity tasks, using only unsupervised data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9352923",
                        "name": "Zilu Tang"
                    },
                    {
                        "authorId": "1665851460",
                        "name": "Muhammed Yusuf Kocyigit"
                    },
                    {
                        "authorId": "2129412",
                        "name": "D. Wijaya"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "\u2026Kumar et al. (2020); Bartl et al. (2020) Sen et al. (2021)\n1\nProst et al. (2019); Qian et al. (2019) Emami et al. (2019); Habash et al. (2019) Dinan et al. (2020); Costa-juss\u00e0 and de Jorge (2020) Basta et al. (2020)\n2\nPark et al. (2018); Stafanovic\u030cs et al. (2020) Saunders and Byrne (2020);\u2026"
            ],
            "citingPaper": {
                "paperId": "b0096a2431773e34e5c72f559b87e01f5c15d5e0",
                "externalIds": {
                    "ArXiv": "2210.11471",
                    "ACL": "2022.gebnlp-1.17",
                    "DBLP": "journals/corr/abs-2210-11471",
                    "DOI": "10.18653/v1/2022.gebnlp-1.17",
                    "CorpusId": 250390436
                },
                "corpusId": 250390436,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b0096a2431773e34e5c72f559b87e01f5c15d5e0",
                "title": "Choose Your Lenses: Flaws in Gender Bias Evaluation",
                "abstract": "Considerable efforts to measure and mitigate gender bias in recent years have led to the introduction of an abundance of tasks, datasets, and metrics used in this vein. In this position paper, we assess the current paradigm of gender bias evaluation and identify several flaws in it. First, we highlight the importance of extrinsic bias metrics that measure how a model\u2019s performance on some task is affected by gender, as opposed to intrinsic evaluations of model representations, which are less strongly connected to specific harms to people interacting with systems. We find that only a few extrinsic metrics are measured in most studies, although more can be measured. Second, we find that datasets and metrics are often coupled, and discuss how their coupling hinders the ability to obtain reliable conclusions, and how one may decouple them. We then investigate how the choice of the dataset and its composition, as well as the choice of the metric, affect bias measurement, finding significant variations across each of them. Finally, we propose several guidelines for more reliable gender bias evaluation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1398583303",
                        "name": "Hadas Orgad"
                    },
                    {
                        "authorId": "2083259",
                        "name": "Yonatan Belinkov"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026al., 2019a), language models (Nadeem et al., 2021), and models for specific downstream tasks in NLP (Rudinger et al., 2018; Stanovsky et al., 2019; Dinan et al., 2020) are prone to social biases, which may have a negative impact on their performance and the social effect when applied in reality\u2026"
            ],
            "citingPaper": {
                "paperId": "a457ea100a8f4e3dd685d172eba525271af78e29",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-08859",
                    "ArXiv": "2210.08859",
                    "DOI": "10.48550/arXiv.2210.08859",
                    "CorpusId": 252918150
                },
                "corpusId": 252918150,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a457ea100a8f4e3dd685d172eba525271af78e29",
                "title": "Social Biases in Automatic Evaluation Metrics for NLG",
                "abstract": "Many studies have revealed that word embeddings, language models, and models for specific downstream tasks in NLP are prone to social biases, especially gender bias. Recently these techniques have been gradually applied to automatic evaluation metrics for text generation. In the paper, we propose an evaluation method based on Word Embeddings Association Test (WEAT) and Sentence Embeddings Association Test (SEAT) to quantify social biases in evaluation metrics and discover that social biases are also widely present in some model-based automatic evaluation metrics. Moreover, we construct gender-swapped meta-evaluation datasets to explore the potential impact of gender bias in image caption and text summarization tasks. Results show that given gender-neutral references in the evaluation, model-based evaluation metrics may show a preference for the male hypothesis, and the performance of them, i.e. the correlation between evaluation metrics and human judgments, usually has more significant variation after gender swapping.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "82340188",
                        "name": "Mingqi Gao"
                    },
                    {
                        "authorId": "117908148",
                        "name": "Xiaojun Wan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "c5be8b50d8167eee8193287f55efa16423034c88",
                "externalIds": {
                    "ACL": "2023.gitt-1.1",
                    "DBLP": "journals/corr/abs-2210-07538",
                    "ArXiv": "2210.07538",
                    "DOI": "10.48550/arXiv.2210.07538",
                    "CorpusId": 252907446
                },
                "corpusId": 252907446,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c5be8b50d8167eee8193287f55efa16423034c88",
                "title": "The User-Aware Arabic Gender Rewriter",
                "abstract": "We introduce the User-Aware Arabic Gender Rewriter, a user-centric web-based system for Arabic gender rewriting in contexts involving two users. The system takes either Arabic or English sentences as input, and provides users with the ability to specify their desired first and/or second person target genders. The system outputs gender rewritten alternatives of the Arabic sentences (provided directly or as translation outputs) to match the target users\u2019 gender preferences.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "66589548",
                        "name": "Bashar Alhafni"
                    },
                    {
                        "authorId": "40010893",
                        "name": "Ossama Obeid"
                    },
                    {
                        "authorId": "1696645",
                        "name": "Nizar Habash"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Augmentation with synthetically generated data has also been explored for gender bias mitigation in dialogue (Dinan et al., 2020; Liu et al., 2020) and translation",
                "Augmentation Discrimination Adding synthetically generated data (Dinan et al., 2020; Liu et al., 2020; Stafanovi\u010ds et al., 2020) Toxicity Adding safer example data (Mathew et al.",
                "Augmentation with synthetically generated data has also been explored for gender bias mitigation in dialogue (Dinan et al., 2020; Liu et al., 2020) and translation models (Stafanovic\u030cs et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "44e02c4735e2e6cce3214e30bba1e30a92804bdd",
                "externalIds": {
                    "ArXiv": "2210.07700",
                    "ACL": "2023.eacl-main.241",
                    "DBLP": "conf/eacl/KumarBNAT23",
                    "DOI": "10.48550/arXiv.2210.07700",
                    "CorpusId": 252907607
                },
                "corpusId": 252907607,
                "publicationVenue": {
                    "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
                    "name": "Conference of the European Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Eur Chapter Assoc Comput Linguistics",
                        "EACL"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/eacl/"
                },
                "url": "https://www.semanticscholar.org/paper/44e02c4735e2e6cce3214e30bba1e30a92804bdd",
                "title": "Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey",
                "abstract": "Recent advances in the capacity of large language models to generate human-like text have resulted in their increased adoption in user-facing settings. In parallel, these improvements have prompted a heated discourse around the risks of societal harms they introduce, whether inadvertent or malicious. Several studies have explored these harms and called for their mitigation via development of safer, fairer models. Going beyond enumerating the risks of harms, this work provides a survey of practical methods for addressing potential threats and societal harms from language generation models. We draw on several prior works\u2019 taxonomies of language model risks to present a structured overview of strategies for detecting and ameliorating different kinds of risks/harms of language generators. Bridging diverse strands of research, this survey aims to serve as a practical guide for both LM researchers and practitioners, with explanations of different strategies\u2019 motivations, their limitations, and open problems for future research.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51467955",
                        "name": "Sachin Kumar"
                    },
                    {
                        "authorId": "143820870",
                        "name": "Vidhisha Balachandran"
                    },
                    {
                        "authorId": "79336318",
                        "name": "Lucille Njoo"
                    },
                    {
                        "authorId": "49513989",
                        "name": "Antonios Anastasopoulos"
                    },
                    {
                        "authorId": "2073587169",
                        "name": "Yulia Tsvetkov"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "For example, gender and racial biases exist in dialogue generation systems [7] and PLMs [17]."
            ],
            "citingPaper": {
                "paperId": "37e8151365c93578e6d645b27377ebb0414b22ed",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-05211",
                    "ArXiv": "2210.05211",
                    "DOI": "10.48550/arXiv.2210.05211",
                    "CorpusId": 252815768
                },
                "corpusId": 252815768,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/37e8151365c93578e6d645b27377ebb0414b22ed",
                "title": "A Win-win Deal: Towards Sparse and Robust Pre-trained Language Models",
                "abstract": "Despite the remarkable success of pre-trained language models (PLMs), they still face two challenges: First, large-scale PLMs are inefficient in terms of memory footprint and computation. Second, on the downstream tasks, PLMs tend to rely on the dataset bias and struggle to generalize to out-of-distribution (OOD) data. In response to the efficiency problem, recent studies show that dense PLMs can be replaced with sparse subnetworks without hurting the performance. Such subnetworks can be found in three scenarios: 1) the fine-tuned PLMs, 2) the raw PLMs and then fine-tuned in isolation, and even inside 3) PLMs without any parameter fine-tuning. However, these results are only obtained in the in-distribution (ID) setting. In this paper, we extend the study on PLMs subnetworks to the OOD setting, investigating whether sparsity and robustness to dataset bias can be achieved simultaneously. To this end, we conduct extensive experiments with the pre-trained BERT model on three natural language understanding (NLU) tasks. Our results demonstrate that \\textbf{sparse and robust subnetworks (SRNets) can consistently be found in BERT}, across the aforementioned three scenarios, using different training and compression methods. Furthermore, we explore the upper bound of SRNets using the OOD information and show that \\textbf{there exist sparse and almost unbiased BERT subnetworks}. Finally, we present 1) an analytical study that provides insights on how to promote the efficiency of SRNets searching process and 2) a solution to improve subnetworks' performance at high sparsity. The code is available at https://github.com/llyx97/sparse-and-robust-PLM.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "7896029",
                        "name": "Yuanxin Liu"
                    },
                    {
                        "authorId": "33427918",
                        "name": "Fandong Meng"
                    },
                    {
                        "authorId": "1390641501",
                        "name": "Zheng Lin"
                    },
                    {
                        "authorId": "153154545",
                        "name": "JiangNan Li"
                    },
                    {
                        "authorId": "143655088",
                        "name": "Peng Fu"
                    },
                    {
                        "authorId": "47184362",
                        "name": "Yanan Cao"
                    },
                    {
                        "authorId": "2154491752",
                        "name": "Weiping Wang"
                    },
                    {
                        "authorId": "48128428",
                        "name": "Jie Zhou"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "NLG models are known to suffer from biases learnable from training or finetuning on data, such as gender bias (Dinan et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "afd4a900b8f2d4beef34dcfa3ffef60b95b95de4",
                "externalIds": {
                    "DBLP": "conf/coling/KehLGFJAH22",
                    "ACL": "2022.coling-1.547",
                    "ArXiv": "2209.07752",
                    "DOI": "10.48550/arXiv.2209.07752",
                    "CorpusId": 252355378
                },
                "corpusId": 252355378,
                "publicationVenue": {
                    "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
                    "name": "International Conference on Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Linguistics",
                        "COLING"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/coling/"
                },
                "url": "https://www.semanticscholar.org/paper/afd4a900b8f2d4beef34dcfa3ffef60b95b95de4",
                "title": "PINEAPPLE: Personifying INanimate Entities by Acquiring Parallel Personification Data for Learning Enhanced Generation",
                "abstract": "A personification is a figure of speech that endows inanimate entities with properties and actions typically seen as requiring animacy. In this paper, we explore the task of personification generation. To this end, we propose PINEAPPLE: Personifying INanimate Entities by Acquiring Parallel Personification data for Learning Enhanced generation. We curate a corpus of personifications called PersonifCorp, together with automatically generated de-personified literalizations of these personifications. We demonstrate the usefulness of this parallel corpus by training a seq2seq model to personify a given literal input. Both automatic and human evaluations show that fine-tuning with PersonifCorp leads to significant gains in personification-related qualities such as animacy and interestingness. A detailed qualitative analysis also highlights key strengths and imperfections of PINEAPPLE over baselines, demonstrating a strong ability to generate diverse and creative personifications that enhance the overall appeal of a sentence.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "150299584",
                        "name": "Sedrick Scott Keh"
                    },
                    {
                        "authorId": null,
                        "name": "Kevin Lu"
                    },
                    {
                        "authorId": "2126048085",
                        "name": "Varun Gangal"
                    },
                    {
                        "authorId": "152913678",
                        "name": "Steven Y. Feng"
                    },
                    {
                        "authorId": "2130911561",
                        "name": "Harsh Jhamtani"
                    },
                    {
                        "authorId": "2715920",
                        "name": "Malihe Alikhani"
                    },
                    {
                        "authorId": "144547315",
                        "name": "E. Hovy"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "NLG models are known to suffer from biases learnable from training or finetuning on data, such as gender bias (Dinan et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "f2f83a1e928c5542f5ad13109dfbaebfab29ab5b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-06275",
                    "ACL": "2023.eacl-main.36",
                    "ArXiv": "2209.06275",
                    "DOI": "10.48550/arXiv.2209.06275",
                    "CorpusId": 252220861
                },
                "corpusId": 252220861,
                "publicationVenue": {
                    "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
                    "name": "Conference of the European Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Eur Chapter Assoc Comput Linguistics",
                        "EACL"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/eacl/"
                },
                "url": "https://www.semanticscholar.org/paper/f2f83a1e928c5542f5ad13109dfbaebfab29ab5b",
                "title": "PANCETTA: Phoneme Aware Neural Completion to Elicit Tongue Twisters Automatically",
                "abstract": "Tongue twisters are meaningful sentences that are difficult to pronounce. The process of automatically generating tongue twisters is challenging since the generated utterance must satisfy two conditions at once: phonetic difficulty and semantic meaning. Furthermore, phonetic difficulty is itself hard to characterize and is expressed in natural tongue twisters through a heterogeneous mix of phenomena such as alliteration and homophony. In this paper, we propose PANCETTA: Phoneme Aware Neural Completion to Elicit Tongue Twisters Automatically. We leverage phoneme representations to capture the notion of phonetic difficulty, and we train language models to generate original tongue twisters on two proposed task settings. To do this, we curate a dataset called TT-Corp, consisting of existing English tongue twisters. Through automatic and human evaluation, as well as qualitative analysis, we show that PANCETTA generates novel, phonetically difficult, fluent, and semantically meaningful tongue twisters.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "150299584",
                        "name": "Sedrick Scott Keh"
                    },
                    {
                        "authorId": "152913678",
                        "name": "Steven Y. Feng"
                    },
                    {
                        "authorId": "2126048085",
                        "name": "Varun Gangal"
                    },
                    {
                        "authorId": "2715920",
                        "name": "Malihe Alikhani"
                    },
                    {
                        "authorId": "144547315",
                        "name": "E. Hovy"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Machine learning models have a tendency to amplify at test-time biases that exist in their training data, a problem known as bias amplification [12, 38, 22].",
                "Machine learning models have a tendency to amplify at test-time biases that exist in their training data, a problem known as bias amplification (Dinan et al., 2019; Leino et al., 2019; Hall et al., 2022)."
            ],
            "citingPaper": {
                "paperId": "40b672e817624a2357f279cd3d92aa0ae8ddd330",
                "externalIds": {
                    "DBLP": "conf/icml/TaoriH23",
                    "ArXiv": "2209.03942",
                    "DOI": "10.48550/arXiv.2209.03942",
                    "CorpusId": 252118610
                },
                "corpusId": 252118610,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/40b672e817624a2357f279cd3d92aa0ae8ddd330",
                "title": "Data Feedback Loops: Model-driven Amplification of Dataset Biases",
                "abstract": "Datasets scraped from the internet have been critical to the successes of large-scale machine learning. Yet, this very success puts the utility of future internet-derived datasets at potential risk, as model outputs begin to replace human annotations as a source of supervision. In this work, we first formalize a system where interactions with one model are recorded as history and scraped as training data in the future. We then analyze its stability over time by tracking changes to a test-time bias statistic (e.g. gender bias of model predictions). We find that the degree of bias amplification is closely linked to whether the model's outputs behave like samples from the training distribution, a behavior which we characterize and define as consistent calibration. Experiments in three conditional prediction scenarios - image classification, visual role-labeling, and language generation - demonstrate that models that exhibit a sampling-like behavior are more calibrated and thus more stable. Based on this insight, we propose an intervention to help calibrate and stabilize unstable feedback systems. Code is available at https://github.com/rtaori/data_feedback.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46199305",
                        "name": "Rohan Taori"
                    },
                    {
                        "authorId": "2117567142",
                        "name": "Tatsunori Hashimoto"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "[16] use bias-controlled training to alleviate the problem of gender bias, while [17] develops a new training procedure to enhance chatbot models with crowd-workers iteratively."
            ],
            "citingPaper": {
                "paperId": "089c3a879253d730e98eaed7b945fa6c4113ca1f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-03463",
                    "ArXiv": "2209.03463",
                    "DOI": "10.1145/3548606.3560599",
                    "CorpusId": 252118408
                },
                "corpusId": 252118408,
                "publicationVenue": {
                    "id": "73f7fe95-b68b-468f-b7ba-3013ca879e50",
                    "name": "Conference on Computer and Communications Security",
                    "type": "conference",
                    "alternate_names": [
                        "Int Workshop Cogn Cell Syst",
                        "CCS",
                        "Comput Commun Secur",
                        "CcS",
                        "International Symposium on Community-centric Systems",
                        "International Workshop on Cognitive Cellular Systems",
                        "Conf Comput Commun Secur",
                        "Comb Comput Sci",
                        "Int Symp Community-centric Syst",
                        "Combinatorics and Computer Science",
                        "Circuits, Signals, and Systems",
                        "Computer and Communications Security",
                        "Circuit Signal Syst"
                    ],
                    "url": "https://dl.acm.org/conference/ccs"
                },
                "url": "https://www.semanticscholar.org/paper/089c3a879253d730e98eaed7b945fa6c4113ca1f",
                "title": "Why So Toxic?: Measuring and Triggering Toxic Behavior in Open-Domain Chatbots",
                "abstract": "Chatbots are used in many applications, e.g., automated agents, smart home assistants, interactive characters in online games, etc. Therefore, it is crucial to ensure they do not behave in undesired manners, providing offensive or toxic responses to users. This is not a trivial task as state-of-the-art chatbot models are trained on large, public datasets openly collected from the Internet. This paper presents a first-of-its-kind, large-scale measurement of toxicity in chatbots. We show that publicly available chatbots are prone to providing toxic responses when fed toxic queries. Even more worryingly, some non-toxic queries can trigger toxic responses too. We then set out to design and experiment with an attack, ToxicBuddy, which relies on fine-tuning GPT-2 to generate non-toxic queries that make chatbots respond in a toxic manner. Our extensive experimental evaluation demonstrates that our attack is effective against public chatbot models and outperforms manually-crafted malicious queries proposed by previous work. We also evaluate three defense mechanisms against ToxicBuddy, showing that they either reduce the attack performance at the cost of affecting the chatbot's utility or are only effective at mitigating a portion of the attack. This highlights the need for more research from the computer security and online safety communities to ensure that chatbot models do not hurt their users. Overall, we are confident that ToxicBuddy can be used as an auditing tool and that our work will pave the way toward designing more effective defenses for chatbot safety.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152258340",
                        "name": "Waiman Si"
                    },
                    {
                        "authorId": "144588806",
                        "name": "M. Backes"
                    },
                    {
                        "authorId": "144728530",
                        "name": "Jeremy Blackburn"
                    },
                    {
                        "authorId": "2064581974",
                        "name": "Emiliano De Cristofaro"
                    },
                    {
                        "authorId": "2350947",
                        "name": "G. Stringhini"
                    },
                    {
                        "authorId": "3447293",
                        "name": "Savvas Zannettou"
                    },
                    {
                        "authorId": "2145956085",
                        "name": "Yang Zhang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "17bcb1edbe068e8fe6a97da552c70a77a15bbce7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-07858",
                    "ArXiv": "2209.07858",
                    "DOI": "10.48550/arXiv.2209.07858",
                    "CorpusId": 252355458
                },
                "corpusId": 252355458,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/17bcb1edbe068e8fe6a97da552c70a77a15bbce7",
                "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned",
                "abstract": "We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2081806483",
                        "name": "Deep Ganguli"
                    },
                    {
                        "authorId": "2154608229",
                        "name": "Liane Lovitt"
                    },
                    {
                        "authorId": "1583434563",
                        "name": "John Kernion"
                    },
                    {
                        "authorId": "119609682",
                        "name": "Amanda Askell"
                    },
                    {
                        "authorId": "1486307451",
                        "name": "Yuntao Bai"
                    },
                    {
                        "authorId": "148070327",
                        "name": "Saurav Kadavath"
                    },
                    {
                        "authorId": "2056658938",
                        "name": "Benjamin Mann"
                    },
                    {
                        "authorId": "3439053",
                        "name": "Ethan Perez"
                    },
                    {
                        "authorId": "2833768",
                        "name": "Nicholas Schiefer"
                    },
                    {
                        "authorId": "1978097132",
                        "name": "Kamal Ndousse"
                    },
                    {
                        "authorId": "2149890773",
                        "name": "Andy Jones"
                    },
                    {
                        "authorId": "1799822",
                        "name": "Sam Bowman"
                    },
                    {
                        "authorId": "2111073313",
                        "name": "Anna Chen"
                    },
                    {
                        "authorId": "2154608209",
                        "name": "Tom Conerly"
                    },
                    {
                        "authorId": "2142833890",
                        "name": "Nova DasSarma"
                    },
                    {
                        "authorId": "1943097969",
                        "name": "Dawn Drain"
                    },
                    {
                        "authorId": "2866708",
                        "name": "Nelson Elhage"
                    },
                    {
                        "authorId": "1403602266",
                        "name": "S. El-Showk"
                    },
                    {
                        "authorId": "30176974",
                        "name": "Stanislav Fort"
                    },
                    {
                        "authorId": "2068552",
                        "name": "Z. Dodds"
                    },
                    {
                        "authorId": "103143311",
                        "name": "T. Henighan"
                    },
                    {
                        "authorId": "39182747",
                        "name": "Danny Hernandez"
                    },
                    {
                        "authorId": "2162194147",
                        "name": "Tristan Hume"
                    },
                    {
                        "authorId": "1666368339",
                        "name": "Josh Jacobson"
                    },
                    {
                        "authorId": "2154610174",
                        "name": "Scott Johnston"
                    },
                    {
                        "authorId": "49604482",
                        "name": "S. Kravec"
                    },
                    {
                        "authorId": "2061321863",
                        "name": "Catherine Olsson"
                    },
                    {
                        "authorId": "1380664820",
                        "name": "Sam Ringer"
                    },
                    {
                        "authorId": "2175781319",
                        "name": "Eli Tran-Johnson"
                    },
                    {
                        "authorId": "2698777",
                        "name": "Dario Amodei"
                    },
                    {
                        "authorId": "31035595",
                        "name": "Tom B. Brown"
                    },
                    {
                        "authorId": "2117706920",
                        "name": "Nicholas Joseph"
                    },
                    {
                        "authorId": "52238703",
                        "name": "Sam McCandlish"
                    },
                    {
                        "authorId": "37232298",
                        "name": "C. Olah"
                    },
                    {
                        "authorId": "2053807409",
                        "name": "Jared Kaplan"
                    },
                    {
                        "authorId": "2115193883",
                        "name": "Jack Clark"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Mihaylov and Nakov, 2019) and bias (Dinan et al., 2019a), less work has studied robust learning from organic conversations with potentially adversarial feedback.",
                "The latter are intended to be difficult for a model to spot and understand, e.g. unsafe text that does not contain any profanity words but can be understood to be unsafe only through its deeper semantic meaning, see Dinan et al. (2019b).",
                "\u2026on detecting undesirable behavior such as toxic language (Xu et al., 2020; Dinan et al., 2021), trolling (Tomaiuolo et al., 2020; Mihaylov and Nakov, 2019) and bias (Dinan et al., 2019a), less work has studied robust learning from organic conversations with potentially adversarial feedback.",
                "In our baseline approach, and all other subsequent approaches, we employ a 128M parameter transformer model as a classifier, using the pre-trained model from Dinan et al. (2019b).",
                "We perform these experiments \u201czero-shot\u201d by taking an off-the-shelf safety classifier of dialogue utterances from the existing work of Dinan et al. (2019b)4.",
                "We use the conversational safety data collected in Dinan et al. (2019b), which is a pool of 30,000 utterances, half of which is collected as standard inputs, and half where crowdworkers were asked to give difficult adversarial inputs."
            ],
            "citingPaper": {
                "paperId": "1bff5af76552f6d3c881ab051275d037f26f66be",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-03295",
                    "ArXiv": "2208.03295",
                    "DOI": "10.48550/arXiv.2208.03295",
                    "CorpusId": 251371667
                },
                "corpusId": 251371667,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1bff5af76552f6d3c881ab051275d037f26f66be",
                "title": "Learning from data in the mixed adversarial non-adversarial case: Finding the helpers and ignoring the trolls",
                "abstract": "The promise of interaction between intelligent conversational agents and humans is that models can learn from such feedback in order to improve. Unfortunately, such exchanges in the wild will not always involve human utterances that are benign or of high quality, and will include a mixture of engaged (helpers) and unengaged or even malicious users (trolls). In this work we study how to perform robust learning in such an environment. We introduce a benchmark evaluation, SafetyMix, which can evaluate methods that learn safe vs. toxic language in a variety of adversarial settings to test their robustness. We propose and analyze several mitigating learning algorithms that identify trolls either at the example or at the user level. Our main finding is that user-based methods, that take into account that troll users will exhibit adversarial behavior across multiple examples, work best in a variety of settings on our benchmark. We then test these methods in a further real-life setting of conversations collected during deployment, with similar results.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3092435",
                        "name": "Da Ju"
                    },
                    {
                        "authorId": "2155954521",
                        "name": "Jing Xu"
                    },
                    {
                        "authorId": "90841478",
                        "name": "Y-Lan Boureau"
                    },
                    {
                        "authorId": "145183709",
                        "name": "J. Weston"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Finally, we add the Funpedia task (Dinan et al., 2020b) \u2013 which involves learning to produce an engaging dialogue utterance given a wikipedia sentence \u2013\nand the LIGHT (Urbanek et al., 2019) and LIGHT WILD (Shuster et al., 2021b) tasks \u2013 which are open-domain dialogue tasks grounded in a medieval\u2026",
                "\u2026in particular, to generate harmful or inappropriate content (Bender et al., 2021; Bommasani et al., 2021; Hendrycks et al., 2021; Weidinger et al., 2021), including work from our group (Xu et al., 2020; Dinan et al., 2022, 2021; Smith et al., 2022; Dinan et al., 2020a; Smith and Williams, 2021).",
                "For example, the ConvAI2 competition at NeurIPS 2018 featured large (at the time) pre-trained transformers being used by the top two winning teams (Wolf et al., 2019; Golovanov et al., 2020; Dinan et al., 2020c).",
                "Many recent works have also focused on the potential of conversational models for bias, either based on gender and its intersections (Dinan et al., 2020a,b; Xu et al., 2020; Smith and Williams, 2021) or several axes of demographic axis more broadly (Barikeri et al., 2021; Perez et al., 2022; Smith\u2026",
                ", 2021), including work from our group (Xu et al., 2020; Dinan et al., 2022, 2021; Smith et al., 2022; Dinan et al., 2020a; Smith and Williams, 2021)."
            ],
            "citingPaper": {
                "paperId": "a3076ecfed0571fbbb5217a5cc6b4b6f24f6f7dd",
                "externalIds": {
                    "ArXiv": "2208.03188",
                    "DBLP": "journals/corr/abs-2208-03188",
                    "DOI": "10.48550/arXiv.2208.03188",
                    "CorpusId": 251371589
                },
                "corpusId": 251371589,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a3076ecfed0571fbbb5217a5cc6b4b6f24f6f7dd",
                "title": "BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage",
                "abstract": "We present BlenderBot 3, a 175B parameter dialogue model capable of open-domain conversation with access to the internet and a long-term memory, and having been trained on a large number of user defined tasks. We release both the model weights and code, and have also deployed the model on a public web page to interact with organic users. This technical report describes how the model was built (architecture, model and training scheme), and details of its deployment, including safety mechanisms. Human evaluations show its superiority to existing open-domain dialogue agents, including its predecessors (Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for continual learning using the data collected from deployment, which will also be publicly released. The goal of this research program is thus to enable the community to study ever-improving responsible agents that learn through interaction.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "35752280",
                        "name": "Kurt Shuster"
                    },
                    {
                        "authorId": "2155954521",
                        "name": "Jing Xu"
                    },
                    {
                        "authorId": "100653935",
                        "name": "M. Komeili"
                    },
                    {
                        "authorId": "3092435",
                        "name": "Da Ju"
                    },
                    {
                        "authorId": "51324296",
                        "name": "Eric Michael Smith"
                    },
                    {
                        "authorId": "3849208",
                        "name": "Stephen Roller"
                    },
                    {
                        "authorId": "1666356365",
                        "name": "Megan Ung"
                    },
                    {
                        "authorId": "2108267192",
                        "name": "Moya Chen"
                    },
                    {
                        "authorId": "153695382",
                        "name": "Kushal Arora"
                    },
                    {
                        "authorId": "2180661097",
                        "name": "Joshua Lane"
                    },
                    {
                        "authorId": "2286668",
                        "name": "Morteza Behrooz"
                    },
                    {
                        "authorId": "35114899",
                        "name": "W.K.F. Ngan"
                    },
                    {
                        "authorId": "1753626755",
                        "name": "Spencer Poff"
                    },
                    {
                        "authorId": "39589154",
                        "name": "Naman Goyal"
                    },
                    {
                        "authorId": "3149531",
                        "name": "Arthur Szlam"
                    },
                    {
                        "authorId": "90841478",
                        "name": "Y-Lan Boureau"
                    },
                    {
                        "authorId": "2272979",
                        "name": "Melanie Kambadur"
                    },
                    {
                        "authorId": "145183709",
                        "name": "J. Weston"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Depending on the downstream application that AlexaTM 20B is being applied to, one or several of the prior techniques from literature (Gupta et al., 2022; Dathathri et al., 2019; Dinan et al., 2019; Sheng et al., 2021; Dinan et al., 2020; Liu et al., 2020a; Sheng et al., 2019; Roller et al., 2021; Liang et al., 2021; Dinan et al., 2021; Dhamala et al., 2021; Schick et al., 2021; Ouyang et al., 2022) might be utilizable for detoxifying and debiasing the model.",
                "\u2026to, one or several of the prior techniques from literature (Gupta et al., 2022; Dathathri et al., 2019; Dinan et al., 2019; Sheng et al., 2021; Dinan et al., 2020; Liu et al., 2020a; Sheng et al., 2019; Roller et al., 2021; Liang et al., 2021; Dinan et al., 2021; Dhamala et al., 2021; Schick\u2026"
            ],
            "citingPaper": {
                "paperId": "914254fac74a2da051cccf6ca16afcaad416a079",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-01448",
                    "ArXiv": "2208.01448",
                    "DOI": "10.48550/arXiv.2208.01448",
                    "CorpusId": 251253416
                },
                "corpusId": 251253416,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/914254fac74a2da051cccf6ca16afcaad416a079",
                "title": "AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model",
                "abstract": "In this work, we demonstrate that multilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks, are more efficient few-shot learners than decoder-only models on various tasks. In particular, we train a 20 billion parameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B) and show that it achieves state-of-the-art (SOTA) performance on 1-shot summarization tasks, outperforming a much larger 540B PaLM decoder model. AlexaTM 20B also achieves SOTA in 1-shot machine translation, especially for low-resource languages, across almost all language pairs supported by the model (Arabic, English, French, German, Hindi, Italian, Japanese, Marathi, Portuguese, Spanish, Tamil, and Telugu) on Flores-101 dataset. We also show in zero-shot setting, AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2 datasets and provides SOTA performance on multilingual tasks such as XNLI, XCOPA, Paws-X, and XWinograd. Overall, our results present a compelling case for seq2seq models as a powerful alternative to decoder-only models for Large-scale Language Model (LLM) training.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2805456",
                        "name": "Saleh Soltan"
                    },
                    {
                        "authorId": "2773408",
                        "name": "Shankar Ananthakrishnan"
                    },
                    {
                        "authorId": "120590817",
                        "name": "Jack G. M. FitzGerald"
                    },
                    {
                        "authorId": "145542597",
                        "name": "Rahul Gupta"
                    },
                    {
                        "authorId": "1836135",
                        "name": "Wael Hamza"
                    },
                    {
                        "authorId": "144165565",
                        "name": "Haidar Khan"
                    },
                    {
                        "authorId": "102648923",
                        "name": "Charith S. Peris"
                    },
                    {
                        "authorId": "38696444",
                        "name": "Stephen Rawls"
                    },
                    {
                        "authorId": "146177177",
                        "name": "Andrew Rosenbaum"
                    },
                    {
                        "authorId": "1681193",
                        "name": "Anna Rumshisky"
                    },
                    {
                        "authorId": "1588348842",
                        "name": "Chandan Prakash"
                    },
                    {
                        "authorId": "1734869335",
                        "name": "Mukund Sridhar"
                    },
                    {
                        "authorId": "1761263",
                        "name": "Fabian Triefenbach"
                    },
                    {
                        "authorId": "3363380",
                        "name": "Apurv Verma"
                    },
                    {
                        "authorId": "5108268",
                        "name": "G. Tur"
                    },
                    {
                        "authorId": "2104644641",
                        "name": "Premkumar Natarajan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "3ee1128a48dbb57ecaf3ebab0708989cffd71322",
                "externalIds": {
                    "DBLP": "journals/iswa/FazzingaGT22",
                    "DOI": "10.1016/j.iswa.2022.200113",
                    "CorpusId": 251688741
                },
                "corpusId": 251688741,
                "publicationVenue": {
                    "id": "e902b33a-fbe2-45e9-adbe-320952691b1e",
                    "name": "Intelligent Systems with Applications",
                    "type": "journal",
                    "alternate_names": [
                        "Intelligent Systems and Applications",
                        "SAI Intell Syst Conf",
                        "IntelliSys",
                        "SAI Intelligent Systems Conference",
                        "Intell Syst Appl"
                    ],
                    "issn": "2667-3053",
                    "url": "https://www.journals.elsevier.com/intelligent-systems-with-applications"
                },
                "url": "https://www.semanticscholar.org/paper/3ee1128a48dbb57ecaf3ebab0708989cffd71322",
                "title": "A privacy-preserving dialogue system based on argumentation",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1726971",
                        "name": "Bettina Fazzinga"
                    },
                    {
                        "authorId": "143978279",
                        "name": "Andrea Galassi"
                    },
                    {
                        "authorId": "2896208",
                        "name": "Paolo Torroni"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "a82a08b5e6a11f4d6fdff95dd30177957ed7855e",
                "externalIds": {
                    "ArXiv": "2207.06591",
                    "CorpusId": 257804597
                },
                "corpusId": 257804597,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a82a08b5e6a11f4d6fdff95dd30177957ed7855e",
                "title": "A methodology to characterize bias and harmful stereotypes in natural language processing in Latin America",
                "abstract": "Automated decision-making systems, especially those based on natural language processing, are pervasive in our lives. They are not only behind the internet search engines we use daily, but also take more critical roles: selecting candidates for a job, determining suspects of a crime, diagnosing autism and more. Such automated systems make errors, which may be harmful in many ways, be it because of the severity of the consequences (as in health issues) or because of the sheer number of people they affect. When errors made by an automated system affect a population more than others, we call the system \\textit{biased}. Most modern natural language technologies are based on artifacts obtained from enormous volumes of text using machine learning, namely language models and word embeddings. Since they are created by applying subsymbolic machine learning, mostly artificial neural networks, they are opaque and practically uninterpretable by direct inspection, thus making it very difficult to audit them. In this paper, we present a methodology that spells out how social scientists, domain experts, and machine learning experts can collaboratively explore biases and harmful stereotypes in word embeddings and large language models. Our methodology is based on the following principles: * focus on the linguistic manifestations of discrimination on word embeddings and language models, not on the mathematical properties of the models * reduce the technical barrier for discrimination experts%, be it social scientists, domain experts or other * characterize through a qualitative exploratory process in addition to a metric-based approach * address mitigation as part of the training process, not as an afterthought",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2276687",
                        "name": "L. A. Alemany"
                    },
                    {
                        "authorId": "2066254822",
                        "name": "Luciana Benotti"
                    },
                    {
                        "authorId": "2139773809",
                        "name": "Hern\u00e1n Maina"
                    },
                    {
                        "authorId": "143956405",
                        "name": "Luc'ia Gonz'alez"
                    },
                    {
                        "authorId": "73773689",
                        "name": "M. Rajngewerc"
                    },
                    {
                        "authorId": "2213485642",
                        "name": "Lautaro Mart'inez"
                    },
                    {
                        "authorId": "2216725309",
                        "name": "Jos'e L. S'anchez"
                    },
                    {
                        "authorId": "120494256",
                        "name": "M. Schilman"
                    },
                    {
                        "authorId": "2213060824",
                        "name": "Guido Ivetta"
                    },
                    {
                        "authorId": "2176182678",
                        "name": "Alexia Halvorsen"
                    },
                    {
                        "authorId": "2213329754",
                        "name": "Amanda Rojo"
                    },
                    {
                        "authorId": "2091620256",
                        "name": "M. Bordone"
                    },
                    {
                        "authorId": "2079934550",
                        "name": "Beatriz Busaniche"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Numerous paradigms for language model debiasing were proposed, including feature extraction-based (Pryzant et al., 2020), data augmentations (Zhao et al., 2019; Lu et al., 2020; Dinan et al., 2020), or paraphrasing (Ma et al., 2020).",
                ", 2020), data augmentations (Zhao et al., 2019; Lu et al., 2020; Dinan et al., 2020), or paraphrasing (Ma et al."
            ],
            "citingPaper": {
                "paperId": "853a43c4fc36038d70763b592704cdb813169a00",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-02463",
                    "ACL": "2022.gebnlp-1.6",
                    "ArXiv": "2207.02463",
                    "DOI": "10.48550/arXiv.2207.02463",
                    "CorpusId": 250311509
                },
                "corpusId": 250311509,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/853a43c4fc36038d70763b592704cdb813169a00",
                "title": "Gender Biases and Where to Find Them: Exploring Gender Bias in Pre-Trained Transformer-based Language Models Using Movement Pruning",
                "abstract": "Language model debiasing has emerged as an important field of study in the NLP community. Numerous debiasing techniques were proposed, but bias ablation remains an unaddressed issue. We demonstrate a novel framework for inspecting bias in pre-trained transformer-based language models via movement pruning. Given a model and a debiasing objective, our framework finds a subset of the model containing less bias than the original model. We implement our framework by pruning the model while fine-tuning it on the debasing objective. Optimized are only the pruning scores \u2013 parameters coupled with the model\u2019s weights that act as gates. We experiment with pruning attention heads, an important building block of transformers: we prune square blocks, as well as establish a new way of pruning the entire heads. Lastly, we demonstrate the usage of our framework using gender bias, and based on our findings, we propose an improvement to an existing debiasing method. Additionally, we re-discover a bias-performance trade-off: the better the model performs, the more bias it contains.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2143200341",
                        "name": "Przemyslaw K. Joniak"
                    },
                    {
                        "authorId": "1705519",
                        "name": "Akiko Aizawa"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026generated via our LLM-D-based approaches could used both to test for undesired behaviour in classifiers and potentially to mitigate that behaviour via methods such as dataset augmentation, as has been found useful in various settings, e.g. Dinan et al. (2020), Hall Maudslay et al. (2019)."
            ],
            "citingPaper": {
                "paperId": "7e7cc29b042793b27688beb765dc604dee65d536",
                "externalIds": {
                    "ACL": "2022.woah-1.20",
                    "ArXiv": "2206.13757",
                    "DBLP": "journals/corr/abs-2206-13757",
                    "DOI": "10.48550/arXiv.2206.13757",
                    "CorpusId": 250089342
                },
                "corpusId": 250089342,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7e7cc29b042793b27688beb765dc604dee65d536",
                "title": "Flexible text generation for counterfactual fairness probing",
                "abstract": "A common approach for testing fairness issues in text-based classifiers is through the use of counterfactuals: does the classifier output change if a sensitive attribute in the input is changed? Existing counterfactual generation methods typically rely on wordlists or templates, producing simple counterfactuals that fail to take into account grammar, context, or subtle sensitive attribute references, and could miss issues that the wordlist creators had not considered. In this paper, we introduce a task for generating counterfactuals that overcomes these shortcomings, and demonstrate how large language models (LLMs) can be leveraged to accomplish this task. We show that this LLM-based method can produce complex counterfactuals that existing methods cannot, comparing the performance of various counterfactual generation methods on the Civil Comments dataset and showing their value in evaluating a toxicity classifier.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2540014",
                        "name": "Zee Fryer"
                    },
                    {
                        "authorId": "82840075",
                        "name": "Vera Axelrod"
                    },
                    {
                        "authorId": "1409971380",
                        "name": "Ben Packer"
                    },
                    {
                        "authorId": "2638246",
                        "name": "Alex Beutel"
                    },
                    {
                        "authorId": "2144168512",
                        "name": "Jilin Chen"
                    },
                    {
                        "authorId": "20825661",
                        "name": "Kellie Webster"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "030b4834bb553e229fb627285fc5407f4434498b",
                "externalIds": {
                    "DBLP": "conf/cvpr/PapakiposB22",
                    "DOI": "10.1109/CVPRW56347.2022.00027",
                    "CorpusId": 251019796
                },
                "corpusId": 251019796,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/030b4834bb553e229fb627285fc5407f4434498b",
                "title": "AugLy: Data Augmentations for Adversarial Robustness",
                "abstract": "We introduce AugLy, a data augmentation library with a focus on adversarial robustness. AugLy provides a wide array of augmentations for multiple modalities (audio, image, text, & video). These augmentations were inspired by those that real users perform on social media platforms, some of which were not already supported by existing data augmentation libraries. AugLy can be used for any purpose where data augmentations are useful, but it is particularly well-suited for evaluating robustness and systematically generating adversarial attacks. In this paper we present how AugLy works, benchmark it against existing libraries, and use it to evaluate the robustness of various state-of-the-art models to showcase AugLy\u2019s utility. We found that models trained using a wider variety of augmentations were indeed more robust to AugLy augmentations, which validates the hypothesis that training on augmented data improves robustness against adversarial attacks. The AugLy repository can be found at https://github.com/facebookresearch/AugLy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51149919",
                        "name": "Zoe Papakipos"
                    },
                    {
                        "authorId": "1749686057",
                        "name": "Joanna Bitton"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026used to measure and often lessen the severity of social bias in text data (Hall Maudslay et al., 2019; Prabhakaran et al., 2019; Zmigrod\net al., 2019; Dinan et al., 2020a,b; Webster et al., 2020; Ma et al., 2021; Smith and Williams, 2021; Renduchintala and Williams, 2022; Emmery et al., 2022).",
                "However, word list approaches are necessarily limited (Dinan et al., 2020a) and which words are included can really matter (Sedoc and Ungar, 2019)."
            ],
            "citingPaper": {
                "paperId": "011095a0082e5e301f9bf30267b193c1c9e7e370",
                "externalIds": {
                    "ACL": "2022.emnlp-main.646",
                    "DBLP": "journals/corr/abs-2205-12586",
                    "ArXiv": "2205.12586",
                    "DOI": "10.48550/arXiv.2205.12586",
                    "CorpusId": 249062690
                },
                "corpusId": 249062690,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/011095a0082e5e301f9bf30267b193c1c9e7e370",
                "title": "Perturbation Augmentation for Fairer NLP",
                "abstract": "Unwanted and often harmful social biases are becoming ever more salient in NLP research, affecting both models and datasets. In this work, we ask whether training on demographically perturbed data leads to fairer language models. We collect a large dataset of human annotated text perturbations and train a neural perturbation model, which we show outperforms heuristic alternatives. We find that (i) language models (LMs) pre-trained on demographically perturbed corpora are typically more fair, and (ii) LMs finetuned on perturbed GLUE datasets exhibit less demographic bias on downstream tasks, and (iii) fairness improvements do not come at the expense of performance on downstream tasks. Lastly, we discuss outstanding questions about how best to evaluate the (un)fairness of large language models. We hope that this exploration of neural demographic perturbation will help drive more improvement towards fairer NLP.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2149798086",
                        "name": "Rebecca Qian"
                    },
                    {
                        "authorId": "51519704",
                        "name": "Candace Ross"
                    },
                    {
                        "authorId": "2166312768",
                        "name": "Jude Fernandes"
                    },
                    {
                        "authorId": "51324296",
                        "name": "Eric Michael Smith"
                    },
                    {
                        "authorId": "2111313627",
                        "name": "Douwe Kiela"
                    },
                    {
                        "authorId": "2110032535",
                        "name": "Adina Williams"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "Dinan et al. (2020); Liu et al. (2020a,b) discuss gender bias in dialogue generation and Sheng et al. (2021b) investigates the ad hominems in dialogue responses regarding the race perspective."
            ],
            "citingPaper": {
                "paperId": "9def79d90fc13e8bd06d3ae8c6b4c754ceffbe64",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-12554",
                    "ArXiv": "2205.12554",
                    "DOI": "10.48550/arXiv.2205.12554",
                    "CorpusId": 249062853
                },
                "corpusId": 249062853,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9def79d90fc13e8bd06d3ae8c6b4c754ceffbe64",
                "title": "Helpfulness and Fairness of Task-Oriented Dialogue Systems",
                "abstract": "Goal-oriented dialogue systems aim to help users achieve certain goals. Therefore, how humans perceive their helpfulness is important. However, neither the human-perceived helpfulness of goal-oriented dialogue systems nor its fairness implication has been well studied. In this paper, we study computational measurements of helpfulness. We first formally define a dialogue response as helpful if it is relevant&coherent, useful, and informative to a query. Then, we collect human annotations for the helpfulness of dialogue responses based on our definition and build a classifier to automatically determine the helpfulness of a response. We further propose to use the helpfulness level of a dialogue system towards different user queries to measure the fairness of a dialogue system. Experiments with state-of-the-art dialogue systems under three information-seeking scenarios reveal that existing systems tend to be more helpful for questions regarding concepts from highly-developed countries than less-developed countries, uncovering potential fairness concerns underlying the current goal-oriented dialogue systems.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2128089588",
                        "name": "Jiao Sun"
                    },
                    {
                        "authorId": "2118739951",
                        "name": "Yu Hou"
                    },
                    {
                        "authorId": "2125034176",
                        "name": "Jiin Kim"
                    },
                    {
                        "authorId": "3157053",
                        "name": "Nanyun Peng"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "In particular, many works focus on generative models (Dinan et al., 2020a,b; Xu et al., 2021b; Kirk et al., 2021; Sheng et al., 2021b; Nozza et al., 2021; Renduchintala et al., 2021; Baheti et al., 2021; Perez et al., 2022), which are well known to pose unique challenges for automatic evaluation\u2026",
                "ing the frequency of different demographic terms using a word list, for example, those signifying gender (Dinan et al., 2020a); religion, race, gender, and orientation (Barikeri et al.",
                "Finally, the process of assembling word lists itself can be tricky, as seed lexica often have several practical (Antoniak and Mimno, 2021) and conceptual (Dinan et al., 2020b) disadvantages, especially when they consist of paired gendered words.",
                "DialoGPT We specifically use a DialoGPT model tuned further on the ConvAI2 dataset (Dinan et al. 2020c, model from Smith and Williams 2021) to acclimate the model to BlenderBot-style prompts containing two sentences of persona information (Roller et al., 2021).",
                "\u2026set of techniques for measuring bias in generated text involves computing the frequency of different demographic terms using a word list, for example, those signifying gender (Dinan et al., 2020a); religion, race, gender, and orientation (Barikeri et al., 2021); or occupations (Kirk et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "7ef43bacd43393ff116e6fcda6a52a6902e016d7",
                "externalIds": {
                    "DBLP": "conf/emnlp/SmithHKPW22",
                    "ACL": "2022.emnlp-main.625",
                    "ArXiv": "2205.09209",
                    "DOI": "10.18653/v1/2022.emnlp-main.625",
                    "CorpusId": 253224433
                },
                "corpusId": 253224433,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/7ef43bacd43393ff116e6fcda6a52a6902e016d7",
                "title": "\u201cI\u2019m sorry to hear that\u201d: Finding New Biases in Language Models with a Holistic Descriptor Dataset",
                "abstract": "As language models grow in popularity, it becomes increasingly important to clearly measure all possible markers of demographic identity in order to avoid perpetuating existing societal harms. Many datasets for measuring bias currently exist, but they are restricted in their coverage of demographic axes and are commonly used with preset bias tests that presuppose which types of biases models can exhibit. In this work, we present a new, more inclusive bias measurement dataset, HolisticBias, which includes nearly 600 descriptor terms across 13 different demographic axes. HolisticBias was assembled in a participatory process including experts and community members with lived experience of these terms. These descriptors combine with a set of bias measurement templates to produce over 450,000 unique sentence prompts, which we use to explore, identify, and reduce novel forms of bias in several generative models. We demonstrate that HolisticBias is effective at measuring previously undetectable biases in token likelihoods from language models, as well as in an offensiveness classifier. We will invite additions and amendments to the dataset, which we hope will serve as a basis for more easy-to-use and standardized methods for evaluating bias in NLP models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51324296",
                        "name": "Eric Michael Smith"
                    },
                    {
                        "authorId": "120861776",
                        "name": "Melissa Hall"
                    },
                    {
                        "authorId": "2272979",
                        "name": "Melanie Kambadur"
                    },
                    {
                        "authorId": "6072807",
                        "name": "Eleonora Presani"
                    },
                    {
                        "authorId": "2110032535",
                        "name": "Adina Williams"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "We collected these from existing bibliographies of surveyed papers provided in [2, 18].",
                "[18] Emily Dinan, Angela Fan, Adina Williams, Jack Urbanek, Douwe Kiela, and Jason Weston."
            ],
            "citingPaper": {
                "paperId": "b2563d102456d5140ecb4111e7f08481f720d9a4",
                "externalIds": {
                    "ArXiv": "2205.02526",
                    "DBLP": "conf/fat/DevinneyBB22",
                    "DOI": "10.1145/3531146.3534627",
                    "CorpusId": 248524791
                },
                "corpusId": 248524791,
                "publicationVenue": {
                    "id": "cdc83875-a82d-445c-b097-cbe91afe99a8",
                    "name": "Conference on Fairness, Accountability and Transparency",
                    "type": "conference",
                    "alternate_names": [
                        "FAccT",
                        "Conf Fairness Account Transpar"
                    ],
                    "url": "https://facctconference.org/"
                },
                "url": "https://www.semanticscholar.org/paper/b2563d102456d5140ecb4111e7f08481f720d9a4",
                "title": "Theories of \u201cGender\u201d in NLP Bias Research",
                "abstract": "The rise of concern around Natural Language Processing (NLP) technologies containing and perpetuating social biases has led to a rich and rapidly growing area of research. Gender bias is one of the central biases being analyzed, but to date there is no comprehensive analysis of how \u201cgender\u201d is theorized in the field. We survey nearly 200 articles concerning gender bias in NLP to discover how the field conceptualizes gender both explicitly (e.g. through definitions of terms) and implicitly (e.g. through how gender is operationalized in practice). In order to get a better idea of emerging trajectories of thought, we split these articles into two sections by time. We find that the majority of the articles do not make their theorization of gender explicit, even if they clearly define \u201cbias.\u201d Almost none use a model of gender that is intersectional or inclusive of nonbinary genders; and many conflate sex characteristics, social gender, and linguistic gender in ways that disregard the existence and experience of trans, nonbinary, and intersex people. There is an increase between the two time-sections in statements acknowledging that gender is a complicated reality, however, very few articles manage to put this acknowledgment into practice. In addition to analyzing these findings, we provide specific recommendations to facilitate interdisciplinary work, and to incorporate theory and methodology from Gender Studies. Our hope is that this will produce more inclusive gender bias research in NLP.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51236664",
                        "name": "Hannah Devinney"
                    },
                    {
                        "authorId": "52604880",
                        "name": "Jenny Bj\u00f6rklund"
                    },
                    {
                        "authorId": "2077434671",
                        "name": "H. Bj\u00f6rklund"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "As noted earlier, any system capable of generating natural language, even within the limits of fantasy domains as seen in certain games, is capable of accidental or intentional harmful and biased language use\u2014a property which we mitigate but do not entirely eliminate through our value prior (Sheng et al., 2019; Dinan et al., 2020).",
                "\u2026of generating natural language, even within the limits of fantasy domains as seen in certain games, is capable of accidental or intentional harmful and biased language use\u2014a property which we mitigate but do not entirely eliminate through our value prior (Sheng et al., 2019; Dinan et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "871037ca207f36ffc5322d7815a2dd58951a6227",
                "externalIds": {
                    "ArXiv": "2205.01975",
                    "ACL": "2022.naacl-main.439",
                    "DBLP": "journals/corr/abs-2205-01975",
                    "DOI": "10.48550/arXiv.2205.01975",
                    "CorpusId": 248512563
                },
                "corpusId": 248512563,
                "publicationVenue": {
                    "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
                    "name": "North American Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "North Am Chapter Assoc Comput Linguistics",
                        "NAACL"
                    ],
                    "url": "https://www.aclweb.org/portal/naacl"
                },
                "url": "https://www.semanticscholar.org/paper/871037ca207f36ffc5322d7815a2dd58951a6227",
                "title": "Aligning to Social Norms and Values in Interactive Narratives",
                "abstract": "We focus on creating agents that act in alignment with socially beneficial norms and values in interactive narratives or text-based games\u2014environments wherein an agent perceives and interacts with a world through natural language. Such interactive agents are often trained via reinforcement learning to optimize task performance, even when such rewards may lead to agent behaviors that violate societal norms\u2014causing harm either to the agent itself or other entities in the environment. Social value alignment refers to creating agents whose behaviors conform to expected moral and social norms for a given context and group of people\u2014in our case, it means agents that behave in a manner that is less harmful and more beneficial for themselves and others.We build on the Jiminy Cricket benchmark (Hendrycks et al. 2021), a set of 25 annotated interactive narratives containing thousands of morally salient scenarios covering everything from theft and bodily harm to altruism. We introduce the GALAD (Game-value ALignment through Action Distillation) agent that uses the social commonsense knowledge present in specially trained language models to contextually restrict its action space to only those actions that are aligned with socially beneficial values. An experimental study shows that the GALAD agent makes decisions efficiently enough to improve state-of-the-art task performance by 4% while reducing the frequency of socially harmful behaviors by 25% compared to strong contemporary value alignment approaches.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "19179135",
                        "name": "Prithviraj Ammanabrolu"
                    },
                    {
                        "authorId": "2112504145",
                        "name": "Liwei Jiang"
                    },
                    {
                        "authorId": "2729164",
                        "name": "Maarten Sap"
                    },
                    {
                        "authorId": "2548384",
                        "name": "Hannaneh Hajishirzi"
                    },
                    {
                        "authorId": "1699545",
                        "name": "Yejin Choi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026(Rabinovich et al., 2017; Vanmassenhove et al., 2018; Stafanovic\u030cs et al., 2020; Savoldi et al., 2021), dialogue systems (Cercas Curry et al., 2020; Dinan et al., 2020; Liu et al., 2020a,b; Sheng et al., 2021), language modeling (Lu et al., 2018; Bordia and Bowman, 2019; Sheng et al., 2019; Vig\u2026",
                ", 2021), dialogue systems (Cercas Curry et al., 2020; Dinan et al., 2020; Liu et al., 2020a,b; Sheng et al., 2021), language modeling (Lu et al."
            ],
            "citingPaper": {
                "paperId": "058d75d34ee82147699d080986a353a276bffe48",
                "externalIds": {
                    "DBLP": "conf/naacl/AlhafniHB22",
                    "ACL": "2022.naacl-main.46",
                    "ArXiv": "2205.02211",
                    "DOI": "10.48550/arXiv.2205.02211",
                    "CorpusId": 248512863
                },
                "corpusId": 248512863,
                "publicationVenue": {
                    "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
                    "name": "North American Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "North Am Chapter Assoc Comput Linguistics",
                        "NAACL"
                    ],
                    "url": "https://www.aclweb.org/portal/naacl"
                },
                "url": "https://www.semanticscholar.org/paper/058d75d34ee82147699d080986a353a276bffe48",
                "title": "User-Centric Gender Rewriting",
                "abstract": "In this paper, we define the task of gender rewriting in contexts involving two users (I and/or You) \u2013 first and second grammatical persons with independent grammatical gender preferences. We focus on Arabic, a gender-marking morphologically rich language. We develop a multi-step system that combines the positive aspects of both rule-based and neural rewriting models. Our results successfully demonstrate the viability of this approach on a recently created corpus for Arabic gender rewriting, achieving 88.42 M2 F0.5 on a blind test set. Our proposed system improves over previous work on the first-person-only version of this task, by 3.05 absolute increase in M2 F0.5. We demonstrate a use case of our gender rewriting system by using it to post-edit the output of a commercial MT system to provide personalized outputs based on the users\u2019 grammatical gender preferences. We make our code, data, and pretrained models publicly available.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "66589548",
                        "name": "Bashar Alhafni"
                    },
                    {
                        "authorId": "1696645",
                        "name": "Nizar Habash"
                    },
                    {
                        "authorId": "2063374",
                        "name": "Houda Bouamor"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "We additionally evaluated OPT-175B on the ConvAI2 hidden test set, which has never been publicly released, and achieved 10.7 ppl and .185 UF1, matching the performance of the validation set.",
                "In particular, we follow Roller et al. (2021), and evaluate on ConvAI2 (Dinan et al., 2020b), Wizard of Wikipedia (Dinan et al., 2019b), Empathetic Dialogues (Rashkin et al., 2019), and Blended Skill Talk (Smith et al., 2020).",
                "To address concerns of leakage, we searched our pre-training corpus for the first conversation in the ConvAI2 dataset, but we did not find any overlap.",
                "We see that OPT-175B significantly outperforms the alsounsupervised Reddit 2.7B model on all tasks, and\nperforms competitively with the fully supervised BlenderBot 1 model, especially in the ConvAI2 dataset.",
                "We were somewhat surprised that the evaluations of the unsupervised OPT-175B model were as competitive as BlenderBot 1 on the ConvAI2 dataset.",
                "The second conversational intelligence challenge (ConvAI2).",
                "We report Perplexity and Unigram F1 (UF1) overlap, following the metrics of the ConvAI2 competition (Dinan et al., 2020b).",
                "Furthermore, we evaluated OPT-175B on a subset of the ConvAI2like MultiSessionChat (MSC) dataset (Xu et al., 2021b) and obtained a perplexity of 9.7 and UF1 of .177, indicating the model is generalizing well across multiple PersonaChat-like datasets.",
                "There has been a great deal of work on mitigations for toxicity and biases (Dathathri et al., 2019; Dinan et al., 2019a; Sheng et al., 2019; Dinan et al., 2020a; Liu et al., 2019a; Krause et al., 2020; Xu et al., 2020; Liang et al., 2021; Dinan et al., 2021; Xu et al., 2021a; Dhamala et al., 2021;\u2026",
                "There has been a great deal of work on mitigations for toxicity and biases (Dathathri et al., 2019; Dinan et al., 2019a; Sheng et al., 2019; Dinan et al., 2020a; Liu et al., 2019a; Krause et al., 2020; Xu et al., 2020; Liang et al., 2021; Dinan et al., 2021; Xu et al., 2021a; Dhamala et al., 2021; Schick et al., 2021; Ouyang et al., 2022).",
                "This may indicate leakage of the ConvAI2 dataset into the general pre-training corpus or even into the validation data as evaluated in Table 2."
            ],
            "citingPaper": {
                "paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-01068",
                    "ArXiv": "2205.01068",
                    "CorpusId": 248496292
                },
                "corpusId": 248496292,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/13a0d8bb38f739990c8cd65a44061c6534f17221",
                "title": "OPT: Open Pre-trained Transformer Language Models",
                "abstract": "Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108244542",
                        "name": "Susan Zhang"
                    },
                    {
                        "authorId": "3849208",
                        "name": "Stephen Roller"
                    },
                    {
                        "authorId": "39589154",
                        "name": "Naman Goyal"
                    },
                    {
                        "authorId": "2347956",
                        "name": "Mikel Artetxe"
                    },
                    {
                        "authorId": "2108267192",
                        "name": "Moya Chen"
                    },
                    {
                        "authorId": "1782969",
                        "name": "Shuohui Chen"
                    },
                    {
                        "authorId": "2065332326",
                        "name": "Christopher Dewan"
                    },
                    {
                        "authorId": "2138579860",
                        "name": "Mona T. Diab"
                    },
                    {
                        "authorId": "2116235416",
                        "name": "Xian Li"
                    },
                    {
                        "authorId": "143724481",
                        "name": "Xi Victoria Lin"
                    },
                    {
                        "authorId": "39980906",
                        "name": "Todor Mihaylov"
                    },
                    {
                        "authorId": "40511414",
                        "name": "Myle Ott"
                    },
                    {
                        "authorId": "88728159",
                        "name": "Sam Shleifer"
                    },
                    {
                        "authorId": "35752280",
                        "name": "Kurt Shuster"
                    },
                    {
                        "authorId": "2082239112",
                        "name": "Daniel Simig"
                    },
                    {
                        "authorId": "2146367061",
                        "name": "Punit Singh Koura"
                    },
                    {
                        "authorId": "5382923",
                        "name": "Anjali Sridhar"
                    },
                    {
                        "authorId": "1785372925",
                        "name": "Tianlu Wang"
                    },
                    {
                        "authorId": "1982950",
                        "name": "Luke Zettlemoyer"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Attempts are being made with debiasing techniques to address some of these challenges (Dinan et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "67d2d0c0c4a711004cd48a03c61c30e0dba1bd47",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-00965",
                    "ArXiv": "2205.00965",
                    "DOI": "10.48550/arXiv.2205.00965",
                    "CorpusId": 248496872
                },
                "corpusId": 248496872,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/67d2d0c0c4a711004cd48a03c61c30e0dba1bd47",
                "title": "State-of-the-art in Open-domain Conversational AI: A Survey",
                "abstract": "We survey SoTA open-domain conversational AI models with the objective of presenting the prevailing challenges that still exist to spur future research. In addition, we provide statistics on the gender of conversational AI in order to guide the ethics discussion surrounding the issue. Open-domain conversational AI models are known to have several challenges, including bland, repetitive responses and performance degradation when prompted with figurative language, among others. First, we provide some background by discussing some topics of interest in conversational AI. We then discuss the method applied to the two investigations carried out that make up this study. The first investigation involves a search for recent SoTA open-domain conversational AI models, while the second involves the search for 100 conversational AI to assess their gender. Results of the survey show that progress has been made with recent SoTA conversational AI, but there are still persistent challenges that need to be solved, and the female gender is more common than the male for conversational AI. One main takeaway is that hybrid models of conversational AI offer more advantages than any single architecture. The key contributions of this survey are (1) the identification of prevailing challenges in SoTA open-domain conversational AI, (2) the rarely held discussion on open-domain conversational AI for low-resource languages, and (3) the discussion about the ethics surrounding the gender of conversational AI.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51221489",
                        "name": "Tosin P. Adewumi"
                    },
                    {
                        "authorId": "80342407",
                        "name": "F. Liwicki"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "As noted in Dinan et al. (2020), toxicity 149 in generated dialogue may begin with biases and 150\n3www.hatebase.org\noffensive content in the training data, and debias-151 ing techniques focused on gender can reduce the152 amount of sexist comments generated by the re-153 sulting system."
            ],
            "citingPaper": {
                "paperId": "c8419e68f370c9293c185f542779992b41079566",
                "externalIds": {
                    "ArXiv": "2204.10521",
                    "DBLP": "journals/corr/abs-2204-10521",
                    "ACL": "2022.findings-acl.307",
                    "DOI": "10.48550/arXiv.2204.10521",
                    "CorpusId": 248366513
                },
                "corpusId": 248366513,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/c8419e68f370c9293c185f542779992b41079566",
                "title": "Rethinking Offensive Text Detection as a Multi-Hop Reasoning Problem",
                "abstract": "We introduce the task of implicit offensive text detection in dialogues, where a statement may have either an offensive or non-offensive interpretation, depending on the listener and context. We argue that reasoning is crucial for understanding this broader class of offensive utterances, and release SLIGHT, a dataset to support research on this task. Experiments using the data show that state-of-the-art methods of offense detection perform poorly when asked to detect implicitly offensive statements, achieving only {\\sim} 11\\% accuracy. In contrast to existing offensive text detection datasets, SLIGHT features human-annotated chains of reasoning which describe the mental process by which an offensive interpretation can be reached from each ambiguous statement. We explore the potential for a multi-hop reasoning approach by utilizing existing entailment models to score the probability of these chains, and show that even naive reasoning models can yield improved performance in most situations. Analysis of the chains provides insight into the human interpretation process and emphasizes the importance of incorporating additional commonsense knowledge.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2145894968",
                        "name": "Qiang Zhang"
                    },
                    {
                        "authorId": "2300343",
                        "name": "Jason Naradowsky"
                    },
                    {
                        "authorId": "1768065",
                        "name": "Yusuke Miyao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Besides the challenge of availability of data or high-quality data, there are also technical (Roller et al., 2021) and ethical challenges (Dinan et al., 2020; Javed et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "8abbfb4bd1321eb35c85d0fb48b98aa2612617ec",
                "externalIds": {
                    "ArXiv": "2204.08083",
                    "CorpusId": 260595032
                },
                "corpusId": 260595032,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8abbfb4bd1321eb35c85d0fb48b98aa2612617ec",
                "title": "AfriWOZ: Corpus for Exploiting Cross-Lingual Transferability for Generation of Dialogues in Low-Resource, African Languages",
                "abstract": "Dialogue generation is an important NLP task fraught with many challenges. The challenges become more daunting for low-resource African languages. To enable the creation of dialogue agents for African languages, we contribute the first high-quality dialogue datasets for 6 African languages: Swahili, Wolof, Hausa, Nigerian Pidgin English, Kinyarwanda&Yor\\`ub\\'a. These datasets consist of 1,500 turns each, which we translate from a portion of the English multi-domain MultiWOZ dataset. Subsequently, we investigate&analyze the effectiveness of modelling through transfer learning by utilziing state-of-the-art (SoTA) deep monolingual models: DialoGPT and BlenderBot. We compare the models with a simple seq2seq baseline using perplexity. Besides this, we conduct human evaluation of single-turn conversations by using majority votes and measure inter-annotator agreement (IAA). We find that the hypothesis that deep monolingual models learn some abstractions that generalize across languages holds. We observe human-like conversations, to different degrees, in 5 out of the 6 languages. The language with the most transferable properties is the Nigerian Pidgin English, with a human-likeness score of 78.1%, of which 34.4% are unanimous. We freely provide the datasets and host the model checkpoints/demos on the HuggingFace hub for public access.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51221489",
                        "name": "Tosin P. Adewumi"
                    },
                    {
                        "authorId": "2056770646",
                        "name": "Mofetoluwa Adeyemi"
                    },
                    {
                        "authorId": "2047583795",
                        "name": "Aremu Anuoluwapo"
                    },
                    {
                        "authorId": "2162783577",
                        "name": "Bukola Peters"
                    },
                    {
                        "authorId": "1395556657",
                        "name": "Happy Buzaaba"
                    },
                    {
                        "authorId": "2135913982",
                        "name": "Oyerinde Samuel"
                    },
                    {
                        "authorId": "2162781574",
                        "name": "Amina Mardiyyah Rufai"
                    },
                    {
                        "authorId": "83263885",
                        "name": "Benjamin Ayoade Ajibade"
                    },
                    {
                        "authorId": "2162782825",
                        "name": "Tajudeen Gwadabe"
                    },
                    {
                        "authorId": "2162713888",
                        "name": "M. Traore"
                    },
                    {
                        "authorId": "98725872",
                        "name": "T. Ajayi"
                    },
                    {
                        "authorId": "7744881",
                        "name": "Shamsuddeen Hassan Muhammad"
                    },
                    {
                        "authorId": "114850513",
                        "name": "Ahmed Baruwa"
                    },
                    {
                        "authorId": "2105439683",
                        "name": "Paul Owoicho"
                    },
                    {
                        "authorId": "2145191211",
                        "name": "Tol\u00falop\u00e9 \u00d2g\u00fanr\u00e8m\u00ed"
                    },
                    {
                        "authorId": "2162782962",
                        "name": "Phylis Ngigi"
                    },
                    {
                        "authorId": "2229432740",
                        "name": "Orevaoghene Ahia"
                    },
                    {
                        "authorId": "2162783296",
                        "name": "Ruqayya Nasir"
                    },
                    {
                        "authorId": "80342407",
                        "name": "F. Liwicki"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026the natural language processing community, work has focused on combating gender bias in co-reference resolution (Zhao et al., 2018), dialogue (Dinan et al., 2019; Lee et al., 2019; Liu et al., 2020), detection of abusive language (Park et al., 2018), machine translation (Stanovsky et al.,\u2026",
                ", 2018), dialogue (Dinan et al., 2019; Lee et al., 2019; Liu et al., 2020), detection of abusive language (Park et al.",
                "Studies have indicated that the words used in biographies about women compared to biographies about men (Dinan et al., 2019) also differs, and is reflective of gendered terminology."
            ],
            "citingPaper": {
                "paperId": "4b256efcc9627d5f83a91de577623ee5479bc8f9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-05879",
                    "ArXiv": "2204.05879",
                    "DOI": "10.48550/arXiv.2204.05879",
                    "CorpusId": 248047193
                },
                "corpusId": 248047193,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4b256efcc9627d5f83a91de577623ee5479bc8f9",
                "title": "Generating Full Length Wikipedia Biographies: The Impact of Gender Bias on the Retrieval-Based Generation of Women Biographies",
                "abstract": "Generating factual, long-form text such as Wikipedia articles raises three key challenges: how to gather relevant evidence, how to structure information into well-formed text, and how to ensure that the generated text is factually correct. We address these by developing a model for English text that uses a retrieval mechanism to identify relevant supporting information on the web and a cache-based pre-trained encoder-decoder to generate long-form biographies section by section, including citation information. To assess the impact of available web evidence on the output text, we compare the performance of our approach when generating biographies about women (for which less information is available on the web) vs. biographies generally. To this end, we curate a dataset of 1,500 biographies about women. We analyze our generated text to understand how differences in available web evidence data affect generation. We evaluate the factuality, fluency, and quality of the generated texts using automatic metrics and human evaluation. We hope that these techniques can be used as a starting point for human writers, to aid in reducing the complexity inherent in the creation of long-form, factual text.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144270981",
                        "name": "Angela Fan"
                    },
                    {
                        "authorId": "2065132524",
                        "name": "Claire Gardent"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "dc1e128a927335feb6074984ee6f68624acec06e",
                "externalIds": {
                    "DOI": "10.1126/sciadv.abm2463",
                    "CorpusId": 247865256,
                    "PubMed": "35363515"
                },
                "corpusId": 247865256,
                "publicationVenue": {
                    "id": "cb30f0c9-2980-4b7d-bbcb-68fc5472b97c",
                    "name": "Science Advances",
                    "type": "journal",
                    "alternate_names": [
                        "Sci Adv"
                    ],
                    "issn": "2375-2548",
                    "url": "http://www.scienceadvances.org/",
                    "alternate_urls": [
                        "https://advances.sciencemag.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/dc1e128a927335feb6074984ee6f68624acec06e",
                "title": "Based on billions of words on the internet, people = men.",
                "abstract": "Recent advances have made it possible to precisely measure the extent to which any two words are used in similar contexts. In turn, this measure of similarity in linguistic context also captures the extent to which the concepts being denoted are similar. When extracted from massive corpora of text written by millions of individuals, this measure of linguistic similarity can provide insight into the collective concepts of a linguistic community, concepts that both reflect and reinforce widespread ways of thinking. Using this approach, we investigated the collective concept person/people, which forms the basis for nearly all societal decision- and policy-making. In three studies and three preregistered replications with similarity metrics extracted from a corpus of over 630 billion English words, we found that the collective concept person/people is not gender-neutral but rather prioritizes men over women-a fundamental bias in our species' collective view of itself.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "38936000",
                        "name": "April H. Bailey"
                    },
                    {
                        "authorId": "81840293",
                        "name": "Adina Williams"
                    },
                    {
                        "authorId": "2142562401",
                        "name": "Andrei Cimpian"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Further, generations can include toxic language and bias, especially with certain contexts and topics (Xu et al., 2020; Dinan et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "f8292d4ddf7a6dfe240eeaa9685f5d18eed9a3f6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-13224",
                    "ArXiv": "2203.13224",
                    "DOI": "10.48550/arXiv.2203.13224",
                    "CorpusId": 247627671
                },
                "corpusId": 247627671,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/f8292d4ddf7a6dfe240eeaa9685f5d18eed9a3f6",
                "title": "Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion",
                "abstract": "Language models (LMs) have recently been shown to generate more factual responses by employing modularity (Zhou et al., 2021) in combination with retrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et al. (2021) to include internet search as a module. Our SeeKeR (Search engine->Knowledge->Response) method thus applies a single LM to three modular tasks in succession: search, generating knowledge, and generating a final response. We show that, when using SeeKeR as a dialogue model, it outperforms the state-of-the-art model BlenderBot 2 (Chen et al., 2021) on open-domain knowledge-grounded conversations for the same number of parameters, in terms of consistency, knowledge and per-turn engagingness. SeeKeR applied to topical prompt completions as a standard language model outperforms GPT2 (Radford et al., 2019) and GPT3 (Brown et al., 2020) in terms of factuality and topicality, despite GPT3 being a vastly larger model. Our code and models are made publicly available.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "35752280",
                        "name": "Kurt Shuster"
                    },
                    {
                        "authorId": "100653935",
                        "name": "M. Komeili"
                    },
                    {
                        "authorId": "46196592",
                        "name": "Leonard Adolphs"
                    },
                    {
                        "authorId": "3849208",
                        "name": "Stephen Roller"
                    },
                    {
                        "authorId": "3149531",
                        "name": "Arthur Szlam"
                    },
                    {
                        "authorId": "145183709",
                        "name": "J. Weston"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Approaches to mitigate bias in LMs can be broadly summarized as: (a) training or finetuning on a balanced dataset (Solaiman and Dennison, 2021; Dinan et al., 2020)), (b) attaching prefix at inference or training time (Sheng et al., 2020), and (c) using a bias or attribute classifier (e.g., toxicity\u2026"
            ],
            "citingPaper": {
                "paperId": "3c759e2f16bfde8d31189631e4893d3ac8ff05f2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-12574",
                    "ArXiv": "2203.12574",
                    "ACL": "2022.findings-acl.55",
                    "DOI": "10.48550/arXiv.2203.12574",
                    "CorpusId": 247619104
                },
                "corpusId": 247619104,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/3c759e2f16bfde8d31189631e4893d3ac8ff05f2",
                "title": "Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal",
                "abstract": "Language models excel at generating coherent text, and model compression techniques such as knowledge distillation have enabled their use in resource-constrained settings. However, these models can be biased in multiple ways, including the unfounded association of male and female genders with gender-neutral professions. Therefore, knowledge distillation without any fairness constraints may preserve or exaggerate the teacher model\u2019s biases onto the distilled model. To this end, we present a novel approach to mitigate gender disparity in text generation by learning a fair model during knowledge distillation. We propose two modifications to the base knowledge distillation based on counterfactual role reversal\u2014modifying teacher probabilities and augmenting the training set. We evaluate gender polarity across professions in open-ended text generated from the resulting distilled and finetuned GPT\u20132 models and demonstrate a substantial reduction in gender disparity with only a minor compromise in utility. Finally, we observe that language models that reduce gender polarity in language generation do not improve embedding fairness or downstream classification fairness.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2058888124",
                        "name": "Umang Gupta"
                    },
                    {
                        "authorId": "3475586",
                        "name": "J. Dhamala"
                    },
                    {
                        "authorId": "40574366",
                        "name": "Varun Kumar"
                    },
                    {
                        "authorId": "3363380",
                        "name": "Apurv Verma"
                    },
                    {
                        "authorId": "100984698",
                        "name": "Yada Pruksachatkun"
                    },
                    {
                        "authorId": "2143841730",
                        "name": "Satyapriya Krishna"
                    },
                    {
                        "authorId": "2139538015",
                        "name": "Rahul Gupta"
                    },
                    {
                        "authorId": "2782886",
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "authorId": "1719898",
                        "name": "G. V. Steeg"
                    },
                    {
                        "authorId": "143728483",
                        "name": "A. Galstyan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "We believe both these behaviors could be dramatically reduced with adversarial data collection (Dinan et al., 2019).",
                "There are many ways to mitigate these harms, including by fine-tuning on a small, valuetargeted dataset (Solaiman and Dennison, 2021), filtering the pretraining dataset (Ngo et al., 2021), or human-in-the-loop data collection (Dinan et al., 2019; Xu et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-02155",
                    "ArXiv": "2203.02155",
                    "DOI": "10.48550/arXiv.2203.02155",
                    "CorpusId": 246426909
                },
                "corpusId": 246426909,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d766bffc357127e0dc86dd69561d5aeb520d6f4c",
                "title": "Training language models to follow instructions with human feedback",
                "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "31793034",
                        "name": "Long Ouyang"
                    },
                    {
                        "authorId": "49387725",
                        "name": "Jeff Wu"
                    },
                    {
                        "authorId": "2115903168",
                        "name": "Xu Jiang"
                    },
                    {
                        "authorId": "2061137049",
                        "name": "Diogo Almeida"
                    },
                    {
                        "authorId": "2064084601",
                        "name": "Carroll L. Wainwright"
                    },
                    {
                        "authorId": "2051714782",
                        "name": "Pamela Mishkin"
                    },
                    {
                        "authorId": "2111387504",
                        "name": "Chong Zhang"
                    },
                    {
                        "authorId": "144517868",
                        "name": "Sandhini Agarwal"
                    },
                    {
                        "authorId": "2117680841",
                        "name": "Katarina Slama"
                    },
                    {
                        "authorId": "2064770039",
                        "name": "Alex Ray"
                    },
                    {
                        "authorId": "47971768",
                        "name": "J. Schulman"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "2151735262",
                        "name": "Fraser Kelton"
                    },
                    {
                        "authorId": "2142365973",
                        "name": "Luke E. Miller"
                    },
                    {
                        "authorId": "2151735251",
                        "name": "Maddie Simens"
                    },
                    {
                        "authorId": "119609682",
                        "name": "Amanda Askell"
                    },
                    {
                        "authorId": "2930640",
                        "name": "P. Welinder"
                    },
                    {
                        "authorId": "145791315",
                        "name": "P. Christiano"
                    },
                    {
                        "authorId": "2990741",
                        "name": "J. Leike"
                    },
                    {
                        "authorId": "49407415",
                        "name": "Ryan J. Lowe"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Bias amplification is concerning as it can foster the proliferation of undesired stereotypes [12, 41, 46, 47] or lead to unjustifiable differences in model accuracy between subgroups of users [7, 11].",
                "When the group is, for example, a gender group, an age group, or an ethnic group, such a bias can be harmful [12, 20, 38, 41, 46, 47]."
            ],
            "citingPaper": {
                "paperId": "b562ce4b85a97de5a9e909c7dcd090d0fa5f19ec",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-11706",
                    "ArXiv": "2201.11706",
                    "CorpusId": 246294816
                },
                "corpusId": 246294816,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b562ce4b85a97de5a9e909c7dcd090d0fa5f19ec",
                "title": "A Systematic Study of Bias Amplification",
                "abstract": "Recent research suggests that predictions made by machine-learning models can amplify biases present in the training data. When a model amplifies bias, it makes certain predictions at a higher rate for some groups than expected based on training-data statistics. Mitigating such bias amplification requires a deep understanding of the mechanics in modern machine learning that give rise to that amplification. We perform the first systematic, controlled study into when and how bias amplification occurs. To enable this study, we design a simple image-classification problem in which we can tightly control (synthetic) biases. Our study of this problem reveals that the strength of bias amplification is correlated to measures such as model accuracy, model capacity, model overconfidence, and amount of training data. We also find that bias amplification can vary greatly during training. Finally, we find that bias amplification may depend on the difficulty of the classification task relative to the difficulty of recognizing group membership: bias amplification appears to occur primarily when it is easier to recognize group membership than class membership. Our results suggest best practices for training machine-learning models that we hope will help pave the way for the development of better mitigation strategies. Code can be found at https://github.com/facebookresearch/cv_bias_amplification.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "120861776",
                        "name": "Melissa Hall"
                    },
                    {
                        "authorId": "1803520",
                        "name": "L. Maaten"
                    },
                    {
                        "authorId": "47029037",
                        "name": "Laura Gustafson"
                    },
                    {
                        "authorId": "3289587",
                        "name": "Aaron B. Adcock"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recent work has focused on reducing gender bias in machine translation and generation [68, 66, 69, 11]."
            ],
            "citingPaper": {
                "paperId": "21adf4285eb85f4a50106b84906f41c2bd68d510",
                "externalIds": {
                    "ArXiv": "2201.09119",
                    "DBLP": "conf/nips/HuL21",
                    "CorpusId": 243843906
                },
                "corpusId": 243843906,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/21adf4285eb85f4a50106b84906f41c2bd68d510",
                "title": "A Causal Lens for Controllable Text Generation",
                "abstract": "Controllable text generation concerns two fundamental tasks of wide applications, namely generating text of given attributes (i.e., attribute-conditional generation), and minimally editing existing text to possess desired attributes (i.e., text attribute transfer). Extensive prior work has largely studied the two problems separately, and developed different conditional models which, however, are prone to producing biased text (e.g., various gender stereotypes). This paper proposes to formulate controllable text generation from a principled causal perspective which models the two tasks with a unified framework. A direct advantage of the causal formulation is the use of rich causality tools to mitigate generation biases and improve control. We treat the two tasks as interventional and counterfactual causal inference based on a structural causal model, respectively. We then apply the framework to the challenging practical setting where confounding factors (that induce spurious correlations) are observable only on a small fraction of data. Experiments show significant superiority of the causal approach over previous conditional models for improved control accuracy and reduced bias.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2749311",
                        "name": "Zhiting Hu"
                    },
                    {
                        "authorId": "144180616",
                        "name": "Erran L. Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Dialog models [84] can learn, and even amplify, biases in the training data."
            ],
            "citingPaper": {
                "paperId": "b3848d32f7294ec708627897833c4097eb4d8778",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-08239",
                    "ArXiv": "2201.08239",
                    "CorpusId": 246063428
                },
                "corpusId": 246063428,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b3848d32f7294ec708627897833c4097eb4d8778",
                "title": "LaMDA: Language Models for Dialog Applications",
                "abstract": "We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9501591",
                        "name": "Romal Thoppilan"
                    },
                    {
                        "authorId": "1490889580",
                        "name": "Daniel De Freitas"
                    },
                    {
                        "authorId": "2115876918",
                        "name": "Jamie Hall"
                    },
                    {
                        "authorId": "1846258",
                        "name": "Noam M. Shazeer"
                    },
                    {
                        "authorId": "1490888815",
                        "name": "Apoorv Kulshreshtha"
                    },
                    {
                        "authorId": "2061550",
                        "name": "Heng-Tze Cheng"
                    },
                    {
                        "authorId": "2150572756",
                        "name": "Alicia Jin"
                    },
                    {
                        "authorId": "2150572221",
                        "name": "Taylor Bos"
                    },
                    {
                        "authorId": "2150777298",
                        "name": "Leslie Baker"
                    },
                    {
                        "authorId": "144708948",
                        "name": "Yu Du"
                    },
                    {
                        "authorId": "1720837956",
                        "name": "Yaguang Li"
                    },
                    {
                        "authorId": "8386466",
                        "name": "Hongrae Lee"
                    },
                    {
                        "authorId": "2115689465",
                        "name": "H. Zheng"
                    },
                    {
                        "authorId": "3010652",
                        "name": "Amin Ghafouri"
                    },
                    {
                        "authorId": "2150572520",
                        "name": "Marcelo Menegali"
                    },
                    {
                        "authorId": "2145438541",
                        "name": "Yanping Huang"
                    },
                    {
                        "authorId": "2048712",
                        "name": "M. Krikun"
                    },
                    {
                        "authorId": "150077954",
                        "name": "Dmitry Lepikhin"
                    },
                    {
                        "authorId": "47901308",
                        "name": "James Qin"
                    },
                    {
                        "authorId": "7167328",
                        "name": "Dehao Chen"
                    },
                    {
                        "authorId": "2145139570",
                        "name": "Yuanzhong Xu"
                    },
                    {
                        "authorId": "2111317372",
                        "name": "Zhifeng Chen"
                    },
                    {
                        "authorId": "145625142",
                        "name": "Adam Roberts"
                    },
                    {
                        "authorId": "40377863",
                        "name": "Maarten Bosma"
                    },
                    {
                        "authorId": "2389316",
                        "name": "Yanqi Zhou"
                    },
                    {
                        "authorId": "2152948655",
                        "name": "Chung-Ching Chang"
                    },
                    {
                        "authorId": "115361655",
                        "name": "I. Krivokon"
                    },
                    {
                        "authorId": "69540629",
                        "name": "W. Rusch"
                    },
                    {
                        "authorId": "144851733",
                        "name": "Marc Pickett"
                    },
                    {
                        "authorId": "1398655031",
                        "name": "K. Meier-Hellstern"
                    },
                    {
                        "authorId": "144844426",
                        "name": "M. Morris"
                    },
                    {
                        "authorId": "2155007",
                        "name": "Tulsee Doshi"
                    },
                    {
                        "authorId": "2150575477",
                        "name": "Renelito Delos Santos"
                    },
                    {
                        "authorId": "2145151992",
                        "name": "Toju Duke"
                    },
                    {
                        "authorId": "152181934",
                        "name": "J. S\u00f8raker"
                    },
                    {
                        "authorId": "70326942",
                        "name": "Ben Zevenbergen"
                    },
                    {
                        "authorId": "3331141",
                        "name": "Vinodkumar Prabhakaran"
                    },
                    {
                        "authorId": "2152965375",
                        "name": "Mark D\u00edaz"
                    },
                    {
                        "authorId": "2083807",
                        "name": "B. Hutchinson"
                    },
                    {
                        "authorId": "2053872069",
                        "name": "Kristen Olson"
                    },
                    {
                        "authorId": "145142660",
                        "name": "Alejandra Molina"
                    },
                    {
                        "authorId": "1413971055",
                        "name": "Erin Hoffman-John"
                    },
                    {
                        "authorId": "2108300711",
                        "name": "Josh Lee"
                    },
                    {
                        "authorId": "1745337",
                        "name": "Lora Aroyo"
                    },
                    {
                        "authorId": "22167999",
                        "name": "Ravi Rajakumar"
                    },
                    {
                        "authorId": "1724714282",
                        "name": "Alena Butryna"
                    },
                    {
                        "authorId": "48024953",
                        "name": "Matthew Lamm"
                    },
                    {
                        "authorId": "104146486",
                        "name": "V. Kuzmina"
                    },
                    {
                        "authorId": "2067536215",
                        "name": "Joseph Fenton"
                    },
                    {
                        "authorId": "2112929869",
                        "name": "Aaron Cohen"
                    },
                    {
                        "authorId": "144864136",
                        "name": "R. Bernstein"
                    },
                    {
                        "authorId": "2186634",
                        "name": "R. Kurzweil"
                    },
                    {
                        "authorId": "1453482151",
                        "name": "Blaise Aguera-Arcas"
                    },
                    {
                        "authorId": "40498222",
                        "name": "Claire Cui"
                    },
                    {
                        "authorId": "2098130534",
                        "name": "M. Croak"
                    },
                    {
                        "authorId": "143829044",
                        "name": "E. Chi"
                    },
                    {
                        "authorId": "1998340269",
                        "name": "Quoc Le"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In NLP, there are studies that augment text to assess a model\u2019s biases towards gender [31, 3] and ethnicity[26]."
            ],
            "citingPaper": {
                "paperId": "cfe166f0916d2fd251d1f735fc85f6ab77aedca8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-06494",
                    "ArXiv": "2201.06494",
                    "CorpusId": 246016299
                },
                "corpusId": 246016299,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cfe166f0916d2fd251d1f735fc85f6ab77aedca8",
                "title": "AugLy: Data Augmentations for Robustness",
                "abstract": "We introduce AugLy, a data augmentation library with a focus on adversarial robustness. AugLy provides a wide array of augmentations for multiple modalities (audio, image, text,&video). These augmentations were inspired by those that real users perform on social media platforms, some of which were not already supported by existing data augmentation libraries. AugLy can be used for any purpose where data augmentations are useful, but it is particularly well-suited for evaluating robustness and systematically generating adversarial attacks. In this paper we present how AugLy works, benchmark it compared against existing libraries, and use it to evaluate the robustness of various state-of-the-art models to showcase AugLy's utility. The AugLy repository can be found at https://github.com/facebookresearch/AugLy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51149919",
                        "name": "Zoe Papakipos"
                    },
                    {
                        "authorId": "1749686057",
                        "name": "Joanna Bitton"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Output: Unbiased text/unbiased model political bias [78], gender bias [28, 104], subjective bias [101],"
            ],
            "citingPaper": {
                "paperId": "be8e58320203a92bfacc1a1f95f6e65f3ee4fa5c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-05337",
                    "ArXiv": "2201.05337",
                    "DOI": "10.1145/3617680",
                    "CorpusId": 245986550
                },
                "corpusId": 245986550,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/be8e58320203a92bfacc1a1f95f6e65f3ee4fa5c",
                "title": "A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models",
                "abstract": "Controllable Text Generation (CTG) is emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the limited level of interpretability of deep neural networks, the controllability of these methods need to be guaranteed. To this end, controllable text generation using transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the recent 3-4 years, targeting different CTG tasks that require different types of controlled constraints. In this paper, we present a systematic critical review on the common tasks, main approaches, and evaluation methods in this area. Finally, we discuss the challenges that the field is facing, and put forward various promising future directions. To the best of our knowledge, this is the first survey paper to summarize the state-of-the-art CTG techniques from the perspective of Transformer-based PLMs. We hope it can help researchers and practitioners in the related fields to quickly track the academic and technological frontier, providing them with a landscape of the area and a roadmap for future research.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2119078848",
                        "name": "Hanqing Zhang"
                    },
                    {
                        "authorId": "2112980597",
                        "name": "Haolin Song"
                    },
                    {
                        "authorId": "2155604469",
                        "name": "Shaoyu Li"
                    },
                    {
                        "authorId": "92660691",
                        "name": "Ming Zhou"
                    },
                    {
                        "authorId": "2151679983",
                        "name": "Dawei Song"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "For example, in dialogue systems, researchers found the gender bias depends on utterance [49] and persona [85]."
            ],
            "citingPaper": {
                "paperId": "c3d208f80d9a6c8793ee3efd3ced8285d92f41d7",
                "externalIds": {
                    "DBLP": "journals/ai/LiuJWXV22",
                    "DOI": "10.1016/j.artint.2021.103654",
                    "CorpusId": 245692342
                },
                "corpusId": 245692342,
                "publicationVenue": {
                    "id": "96018464-22dc-4b5c-a172-c2f4a30ce131",
                    "name": "Artificial Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "Artif Intell"
                    ],
                    "issn": "0004-3702",
                    "alternate_issns": [
                        "2633-1403",
                        "2710-1673",
                        "2710-1681"
                    ],
                    "url": "http://www.elsevier.com/locate/artint",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00043702",
                        "https://www.journals.elsevier.com/artificial-intelligence"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c3d208f80d9a6c8793ee3efd3ced8285d92f41d7",
                "title": "Quantifying and alleviating political bias in language models",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "7247867",
                        "name": "Ruibo Liu"
                    },
                    {
                        "authorId": "1727055797",
                        "name": "Chenyan Jia"
                    },
                    {
                        "authorId": "119640649",
                        "name": "Jason Wei"
                    },
                    {
                        "authorId": "2007669250",
                        "name": "Guangxuan Xu"
                    },
                    {
                        "authorId": "1918441",
                        "name": "Soroush Vosoughi"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "8a6bda5739c9c975b49327b9fe891d908fdfa951",
                "externalIds": {
                    "ArXiv": "2111.04158",
                    "DBLP": "journals/corr/abs-2111-04158",
                    "CorpusId": 243847937
                },
                "corpusId": 243847937,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8a6bda5739c9c975b49327b9fe891d908fdfa951",
                "title": "A Word on Machine Ethics: A Response to Jiang et al. (2021)",
                "abstract": "Ethics is one of the longest standing intellectual endeavors of humanity. In recent years, the fields of AI and NLP have attempted to wrangle with how learning systems that interact with humans should be constrained to behave ethically. One proposal in this vein is the construction of morality models that can take in arbitrary text and output a moral judgment about the situation described. In this work, we focus on a single case study of the recently proposed Delphi model and offer a critique of the project's proposed method of automating morality judgments. Through an audit of Delphi, we examine broader issues that would be applicable to any similar attempt. We conclude with a discussion of how machine ethics could usefully proceed, by focusing on current and near-future uses of technology, in a way that centers around transparency, democratic values, and allows for straightforward accountability.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2138053020",
                        "name": "Zeerak Talat"
                    },
                    {
                        "authorId": "122317863",
                        "name": "Hagen Blix"
                    },
                    {
                        "authorId": "51130686",
                        "name": "Josef Valvoda"
                    },
                    {
                        "authorId": "123036976",
                        "name": "M. I. Ganesh"
                    },
                    {
                        "authorId": "2070989574",
                        "name": "Ryan Cotterell"
                    },
                    {
                        "authorId": "2110032535",
                        "name": "Adina Williams"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026al., 2020; Stafanovic\u030cs et al., 2020; Saunders et al., 2021; Savoldi et al., 2021; Ciora et al., 2021), dialogue systems (Cercas Curry et al., 2020; Dinan et al., 2020; Liu et al., 2020a,b; Sheng et al., 2021b,a), language modeling (Lu et al., 2018;\nBordia and Bowman, 2019; Sheng et al., 2019;\u2026",
                ", 2021), dialogue systems (Cercas Curry et al., 2020; Dinan et al., 2020a; Liu et al., 2020a; Liu et al., 2020b; Sheng et al., 2021b; Sheng et al., 2021a), language modeling (Lu et al."
            ],
            "citingPaper": {
                "paperId": "300e65aec7bda43e0849be3dd67acfde018228b5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-09216",
                    "ACL": "2022.lrec-1.199",
                    "ArXiv": "2110.09216",
                    "CorpusId": 239016475
                },
                "corpusId": 239016475,
                "publicationVenue": {
                    "id": "7474c4a0-75d9-4105-9809-8e7d5201c5e1",
                    "name": "International Conference on Language Resources and Evaluation",
                    "type": "conference",
                    "alternate_names": [
                        "LREC",
                        "Int Conf Lang Resour Evaluation"
                    ],
                    "url": "http://www.lrec-conf.org/"
                },
                "url": "https://www.semanticscholar.org/paper/300e65aec7bda43e0849be3dd67acfde018228b5",
                "title": "The Arabic Parallel Gender Corpus 2.0: Extensions and Analyses",
                "abstract": "Gender bias in natural language processing (NLP) applications, particularly machine translation, has been receiving increasing attention. Much of the research on this issue has focused on mitigating gender bias in English NLP models and systems. Addressing the problem in poorly resourced, and/or morphologically rich languages has lagged behind, largely due to the lack of datasets and resources. In this paper, we introduce a new corpus for gender identification and rewriting in contexts involving one or two target users (I and/or You) \u2013 first and second grammatical persons with independent grammatical gender preferences. We focus on Arabic, a gender-marking morphologically rich language. The corpus has multiple parallel components: four combinations of 1st and 2nd person in feminine and masculine grammatical genders, as well as English, and English to Arabic machine translation output. This corpus expands on Habash et al. (2019)\u2019s Arabic Parallel Gender Corpus (APGC v1.0) by adding second person targets as well as increasing the total number of sentences over 6.5 times, reaching over 590K words. Our new dataset will aid the research and development of gender identification, controlled text generation, and post-editing rewrite systems that could be used to personalize NLP applications and provide users with the correct outputs based on their grammatical gender preferences. We make the Arabic Parallel Gender Corpus (APGC v2.0) publicly available",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "66589548",
                        "name": "Bashar Alhafni"
                    },
                    {
                        "authorId": "1696645",
                        "name": "Nizar Habash"
                    },
                    {
                        "authorId": "2063374",
                        "name": "Houda Bouamor"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "CDA is a data-based debiasing strategy that is often used to mitigate gender bias (Zmigrod et al., 2019; Dinan et al., 2019; Webster et al., 2020; Barikeri et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "de6807676d8171472ed6cf421c4e4ed3cbb47699",
                "externalIds": {
                    "ArXiv": "2110.08527",
                    "ACL": "2022.acl-long.132",
                    "DBLP": "journals/corr/abs-2110-08527",
                    "DOI": "10.18653/v1/2022.acl-long.132",
                    "CorpusId": 239015827
                },
                "corpusId": 239015827,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/de6807676d8171472ed6cf421c4e4ed3cbb47699",
                "title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
                "abstract": "Recent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on. This has attracted attention to developing techniques that mitigate such biases. In this work, we perform an empirical survey of five recently proposed bias mitigation techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of each technique using three intrinsic bias benchmarks while also measuring the impact of these techniques on a model\u2019s language modeling ability, as well as its performance on downstream NLU tasks. We experimentally find that: (1) Self-Debias is the strongest debiasing technique, obtaining improved scores on all bias benchmarks; (2) Current debiasing techniques perform less consistently when mitigating non-gender biases; And (3) improvements on bias benchmarks such as StereoSet and CrowS-Pairs by using debiasing strategies are often accompanied by a decrease in language modeling ability, making it difficult to determine whether the bias mitigation was effective.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "150247363",
                        "name": "Nicholas Meade"
                    },
                    {
                        "authorId": "2133330526",
                        "name": "Elinor Poole-Dayan"
                    },
                    {
                        "authorId": "145732771",
                        "name": "Siva Reddy"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "der biases (Dinan et al., 2020), and are capable of generating toxic or otherwise unsafe content (Weidinger et al."
            ],
            "citingPaper": {
                "paperId": "e968a3c9590481cd13f2f86a7ac8839e3cf3455f",
                "externalIds": {
                    "ACL": "2022.acl-long.289",
                    "DBLP": "journals/corr/abs-2110-08467",
                    "ArXiv": "2110.08467",
                    "DOI": "10.18653/v1/2022.acl-long.289",
                    "CorpusId": 239015959
                },
                "corpusId": 239015959,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/e968a3c9590481cd13f2f86a7ac8839e3cf3455f",
                "title": "Improving Compositional Generalization with Self-Training for Data-to-Text Generation",
                "abstract": "Data-to-text generation focuses on generating fluent natural language responses from structured meaning representations (MRs). Such representations are compositional and it is costly to collect responses for all possible combinations of atomic meaning schemata, thereby necessitating few-shot generalization to novel MRs. In this work, we systematically study the compositional generalization of the state-of-the-art T5 models in few-shot data-to-text tasks. We show that T5 models fail to generalize to unseen MRs, and we propose a template-based input representation that considerably improves the model\u2019s generalization capability. To further improve the model\u2019s performance, we propose an approach based on self-training using fine-tuned BLEURT for pseudo-response selection. On the commonly-used SGD and Weather benchmarks, the proposed self-training approach improves tree accuracy by 46\\%+ and reduces the slot error rates by 73\\%+ over the strong T5 baselines in few-shot settings.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47613860",
                        "name": "Sanket Vaibhav Mehta"
                    },
                    {
                        "authorId": "30586030",
                        "name": "J. Rao"
                    },
                    {
                        "authorId": "144447820",
                        "name": "Yi Tay"
                    },
                    {
                        "authorId": "26688118",
                        "name": "Mihir Kale"
                    },
                    {
                        "authorId": "144729897",
                        "name": "Ankur P. Parikh"
                    },
                    {
                        "authorId": "2064913758",
                        "name": "Hongtao Zhong"
                    },
                    {
                        "authorId": "2268272",
                        "name": "Emma Strubell"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Different methods for avoiding inappropriate responses have been proposed (Dinan et al., 2020), especially by generating adversarial examples (Dinan et al.",
                "Different methods for avoiding inappropriate responses have been proposed (Dinan et al., 2020), especially by generating adversarial examples (Dinan et al., 2019; Xu et al., 2021a)."
            ],
            "citingPaper": {
                "paperId": "30873c32db5a219a58be928d5692cce48be1d3a0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-08118",
                    "ArXiv": "2110.08118",
                    "CorpusId": 239009514
                },
                "corpusId": 239009514,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/30873c32db5a219a58be928d5692cce48be1d3a0",
                "title": "Few-Shot Bot: Prompt-Based Learning for Dialogue Systems",
                "abstract": "Learning to converse using only a few examples is a great challenge in conversational AI. The current best conversational models, which are either good chit-chatters (e.g., BlenderBot) or goal-oriented systems (e.g., MinTL), are language models (LMs) fine-tuned on large conversational datasets. Training these models is expensive, both in terms of computational resources and time, and it is hard to keep them up to date with new conversational skills. A simple yet unexplored solution is prompt-based few-shot learning (Brown et al., 2020) which does not require gradient-based fine-tuning but instead uses a few examples in the LM context as the only source of learning. In this paper, we explore prompt-based few-shot learning in dialogue tasks. We benchmark LMs of different sizes in nine response generation tasks, which include four knowledge-grounded tasks, a task-oriented generations task, three open-chat tasks, and controlled stylistic generation, and five conversational parsing tasks, which include dialogue state tracking, graph path generation, persona information extraction, document retrieval, and internet query generation. The current largest released LM (GPT-J-6B) using prompt-based few-shot learning, and thus requiring no training, achieves competitive performance to fully trained state-of-the-art models. Moreover, we propose a novel prompt-based few-shot classifier, that also does not require any fine-tuning, to select the most appropriate prompt given a dialogue history. Finally, by combining the power of prompt-based few-shot learning and a Skill Selector, we create an end-to-end chatbot named the Few-Shot Bot (FSB), which automatically selects the most appropriate conversational skill, queries different knowledge bases or the internet, and uses the retrieved knowledge to generate a human-like response, all using only few dialogue examples per skill.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2111680936",
                        "name": "Andrea Madotto"
                    },
                    {
                        "authorId": "2146396528",
                        "name": "Zhaojiang Lin"
                    },
                    {
                        "authorId": "9162688",
                        "name": "Genta Indra Winata"
                    },
                    {
                        "authorId": "40539650",
                        "name": "Pascale Fung"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Counterfactual data has been augmented to alter the training distribution to balance gender-based statistics [35,48,17]."
            ],
            "citingPaper": {
                "paperId": "dc1f5140a89e721b86b26aff2e4c281491a88054",
                "externalIds": {
                    "DBLP": "conf/ausdm/BasharNKSK21",
                    "ArXiv": "2110.15728",
                    "DOI": "10.1007/978-981-16-8531-6_7",
                    "CorpusId": 240288913
                },
                "corpusId": 240288913,
                "publicationVenue": {
                    "id": "611b66d6-dd44-464a-a08a-96ee7fbf1d1d",
                    "name": "Australasian Data Mining Conference",
                    "type": "conference",
                    "alternate_names": [
                        "AusDM",
                        "Australas Data Min Conf"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=258"
                },
                "url": "https://www.semanticscholar.org/paper/dc1f5140a89e721b86b26aff2e4c281491a88054",
                "title": "Deep Learning for Bias Detection: From Inception to Deployment",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "36231591",
                        "name": "M. A. Bashar"
                    },
                    {
                        "authorId": "143658054",
                        "name": "R. Nayak"
                    },
                    {
                        "authorId": "2135908417",
                        "name": "Anjor Kothare"
                    },
                    {
                        "authorId": "2112608424",
                        "name": "Vishal Sharma"
                    },
                    {
                        "authorId": "2135932638",
                        "name": "Kesavan Kandadai"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "We use Reddit data through Pushshift (Baumgartner et al., 2020), an archive that has been widely used in NLP and related fields since its first release in 2015 (Hessel and Lee, 2019; Kennedy et al., 2020; Sap et al., 2020; Dinan et al., 2020, among many others)."
            ],
            "citingPaper": {
                "paperId": "d553a4d7cf26e8971ba0ac63d40e3cbefdfb642b",
                "externalIds": {
                    "DBLP": "conf/emnlp/ParkMRJKJT21",
                    "ArXiv": "2110.04419",
                    "DOI": "10.18653/v1/2021.findings-emnlp.288",
                    "CorpusId": 238583283
                },
                "corpusId": 238583283,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/d553a4d7cf26e8971ba0ac63d40e3cbefdfb642b",
                "title": "Detecting Community Sensitive Norm Violations in Online Conversations",
                "abstract": "Online platforms and communities establish their own norms that govern what behavior is acceptable within the community. Substantial effort in NLP has focused on identifying unacceptable behaviors and, recently, on forecasting them before they occur. However, these efforts have largely focused on toxicity as the sole form of community norm violation. Such focus has overlooked the much larger set of rules that moderators enforce. Here, we introduce a new dataset focusing on a more complete spectrum of community norms and their violations in the local conversational and global community contexts. We introduce a series of models that use this data to develop context- and community-sensitive norm violation detection, showing that these changes give high performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2115237283",
                        "name": "Chan Young Park"
                    },
                    {
                        "authorId": "32163938",
                        "name": "Julia Mendelsohn"
                    },
                    {
                        "authorId": "144994969",
                        "name": "Karthik Radhakrishnan"
                    },
                    {
                        "authorId": "2055619433",
                        "name": "Kinjal Jain"
                    },
                    {
                        "authorId": "51243238",
                        "name": "Tushar Kanakagiri"
                    },
                    {
                        "authorId": "3046220",
                        "name": "David Jurgens"
                    },
                    {
                        "authorId": "145317727",
                        "name": "Yulia Tsvetkov"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "043af12b66c2d598739921ac541d6d49c744aa07",
                "externalIds": {
                    "ArXiv": "2110.03262",
                    "DBLP": "conf/acl/AmmanabroluJR22",
                    "ACL": "2022.acl-long.557",
                    "DOI": "10.18653/v1/2022.acl-long.557",
                    "CorpusId": 238418980
                },
                "corpusId": 238418980,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/043af12b66c2d598739921ac541d6d49c744aa07",
                "title": "Situated Dialogue Learning through Procedural Environment Generation",
                "abstract": "We teach goal-driven agents to interactively act and speak in situated environments by training on generated curriculums. Our agents operate in LIGHT (Urbanek et al. 2019)\u2014a large-scale crowd-sourced fantasy text adventure game wherein an agent perceives and interacts with the world through textual natural language. Goals in this environment take the form of character-based quests, consisting of personas and motivations. We augment LIGHT by learning to procedurally generate additional novel textual worlds and quests to create a curriculum of steadily increasing difficulty for training agents to achieve such goals. In particular, we measure curriculum difficulty in terms of the rarity of the quest in the original training distribution\u2014an easier environment is one that is more likely to have been found in the unaugmented dataset. An ablation study shows that this method of learning from the tail of a distribution results in significantly higher generalization abilities as measured by zero-shot performance on never-before-seen quests.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "19179135",
                        "name": "Prithviraj Ammanabrolu"
                    },
                    {
                        "authorId": "2135057669",
                        "name": "Renee Jia"
                    },
                    {
                        "authorId": "2065904932",
                        "name": "Mark O. Riedl"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Dialog models are known to suffer from biases learnable from dialog training data, such as gender bias (Dinan et al., 2020).",
                "Past work has analyzed dialog models from the point of view of safety from toxic language (Xu et al., 2020; Dinan et al., 2019),\nand gender biases (Dinan et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "dc59b104f41d41d555c5b1a1fda7d69a5e080c53",
                "externalIds": {
                    "ACL": "2021.emnlp-main.592",
                    "DBLP": "journals/corr/abs-2110-00687",
                    "ArXiv": "2110.00687",
                    "DOI": "10.18653/v1/2021.emnlp-main.592",
                    "CorpusId": 238259663
                },
                "corpusId": 238259663,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/dc59b104f41d41d555c5b1a1fda7d69a5e080c53",
                "title": "Investigating Robustness of Dialog Models to Popular Figurative Language Constructs",
                "abstract": "Humans often employ figurative language use in communication, including during interactions with dialog systems. Thus, it is important for real-world dialog systems to be able to handle popular figurative language constructs like metaphor and simile. In this work, we analyze the performance of existing dialog models in situations where the input dialog context exhibits use of figurative language. We observe large gaps in handling of figurative language when evaluating the models on two open domain dialog datasets. When faced with dialog contexts consisting of figurative language, some models show very large drops in performance compared to contexts without figurative language. We encourage future research in dialog modeling to separately analyze and report results on figurative language in order to better test model capabilities relevant to real-world use. Finally, we propose lightweight solutions to help existing models become more robust to figurative language by simply using an external resource to translate figurative language to literal (non-figurative) forms while preserving the meaning to the best extent possible.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2006291",
                        "name": "Harsh Jhamtani"
                    },
                    {
                        "authorId": "2126048085",
                        "name": "Varun Gangal"
                    },
                    {
                        "authorId": "144547315",
                        "name": "E. Hovy"
                    },
                    {
                        "authorId": "1400419309",
                        "name": "Taylor Berg-Kirkpatrick"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "15124bd493aaa91ec1557e31486b4a0dab212707",
                "externalIds": {
                    "ArXiv": "2109.06437",
                    "DBLP": "conf/emnlp/HuangBSC21",
                    "DOI": "10.18653/v1/2021.findings-emnlp.326",
                    "CorpusId": 237502736
                },
                "corpusId": 237502736,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/15124bd493aaa91ec1557e31486b4a0dab212707",
                "title": "Uncovering Implicit Gender Bias in Narratives through Commonsense Inference",
                "abstract": "Pre-trained language models learn socially harmful biases from their training corpora, and may repeat these biases when used for generation. We study gender biases associated with the protagonist in model-generated stories. Such biases may be expressed either explicitly (\"women can't park\") or implicitly (e.g. an unsolicited male character guides her into a parking space). We focus on implicit biases, and use a commonsense reasoning engine to uncover them. Specifically, we infer and analyze the protagonist's motivations, attributes, mental states, and implications on others. Our findings regarding implicit biases are in line with prior work that studied explicit biases, for example showing that female characters' portrayal is centered around appearance, while male figures' focus on intellect.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2110511014",
                        "name": "Tenghao Huang"
                    },
                    {
                        "authorId": "9252833",
                        "name": "Faeze Brahman"
                    },
                    {
                        "authorId": "3103343",
                        "name": "Vered Shwartz"
                    },
                    {
                        "authorId": "37202877",
                        "name": "Snigdha Chaturvedi"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Another way of mitigation is data augmentation (Zhao et al., 2018; Park et al., 2018; Dinan et al., 2020), for example by using gender swapping on the coreference resolution task Zhao et al. (2018).",
                "Another way of mitigation is data augmentation (Zhao et al., 2018; Park et al., 2018; Dinan et al., 2020), for example by using gender swapping on the coreference resolution task Zhao et al."
            ],
            "citingPaper": {
                "paperId": "1aa1d6b29ad6fcef78d1eefacb2a7fd75e68c2c0",
                "externalIds": {
                    "DBLP": "conf/emnlp/AhnO21",
                    "ArXiv": "2109.05704",
                    "ACL": "2021.emnlp-main.42",
                    "DOI": "10.18653/v1/2021.emnlp-main.42",
                    "CorpusId": 237491723
                },
                "corpusId": 237491723,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/1aa1d6b29ad6fcef78d1eefacb2a7fd75e68c2c0",
                "title": "Mitigating Language-Dependent Ethnic Bias in BERT",
                "abstract": "In this paper, we study ethnic bias and how it varies across languages by analyzing and mitigating ethnic bias in monolingual BERT for English, German, Spanish, Korean, Turkish, and Chinese. To observe and quantify ethnic bias, we develop a novel metric called Categorical Bias score. Then we propose two methods for mitigation; first using a multilingual model, and second using contextual word alignment of two monolingual models. We compare our proposed methods with monolingual BERT and show that these methods effectively alleviate the ethnic bias. Which of the two methods works better depends on the amount of NLP resources available for that language. We additionally experiment with Arabic and Greek to verify that our proposed methods work for a wider variety of languages.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2007650741",
                        "name": "Jaimeen Ahn"
                    },
                    {
                        "authorId": "2463290",
                        "name": "Alice H. Oh"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Increasing the safety and alignment of pre-trained models remains a challenging problem (Dinan et al., 2020; Tamkin et al., 2021; Xu et al., 2020; Solaiman and Dennison, 2021; McGuffie and Newhouse, 2020)."
            ],
            "citingPaper": {
                "paperId": "77d956cdab4508d569ae5741549b78e715fd0749",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-07958",
                    "ACL": "2022.acl-long.229",
                    "ArXiv": "2109.07958",
                    "DOI": "10.18653/v1/2022.acl-long.229",
                    "CorpusId": 237532606
                },
                "corpusId": 237532606,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/77d956cdab4508d569ae5741549b78e715fd0749",
                "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
                "abstract": "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48639938",
                        "name": "Stephanie C. Lin"
                    },
                    {
                        "authorId": "2052366271",
                        "name": "Jacob Hilton"
                    },
                    {
                        "authorId": "47107786",
                        "name": "Owain Evans"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "As in \u00a74, we use the multi-dimensional gender bias classifier from Dinan et al. (2020b) to measure the amount of gender bias in conversation turns from Speaker A and B for 920,000 self-chats generated by each of our de-biased models.",
                "This is similar to the Counterfactual Data Augmentation techniques used by Maudslay et al. (2019); Dinan et al. (2020a); Liu et al. (2020a); Barikeri et al. (2021), and aims to ablate any association that the model may have between a certain name (or the gender associated with it) and the subject\u2026",
                "For dialogue, injecting demographic information into personas (i.e., text character descriptions provided as context to the conversational agent) has proven useful in measuring the amount of gender bias agents express (Dinan et al., 2020a; Sheng et al., 2021).",
                ", text character descriptions provided as context to the conversational agent) has proven useful in measuring the amount of gender bias agents express (Dinan et al., 2020a; Sheng et al., 2021).",
                "\u2026standard training datasets for dialogue models in the AI literature contain social biases or demographic imbalances, and it has been established that models learn them (Dixon et al., 2018; Bordia and\nBowman, 2019; Lee et al., 2019; Dinan et al., 2020a,b; Liu et al., 2020a,b; Sheng et al., 2021).",
                "Second, we measure the amount of gender bias in BlenderBot3B self-chats using the multidimensional gender bias classifier from Dinan et al. (2020b), which predicts the genderedness of an utterance based on its context (SPEAKING-AS dimension for Speaker A lines and SPEAKING-TO dimension for Speaker\u2026"
            ],
            "citingPaper": {
                "paperId": "063183d95a249d94c95d12e7e9462e0aa84b6d85",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-03300",
                    "ArXiv": "2109.03300",
                    "CorpusId": 237442178
                },
                "corpusId": 237442178,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/063183d95a249d94c95d12e7e9462e0aa84b6d85",
                "title": "Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models",
                "abstract": "All AI models are susceptible to learning biases in data that they are trained on. For generative dialogue models, being trained on real human conversations containing unbalanced gender and race/ethnicity references can lead to models that display learned biases, which we define here broadly as any measurable differences in the distributions of words or semantic content of conversations based on demographic groups. We measure the strength of such biases by producing artificial conversations between two copies of a dialogue model, conditioning one conversational partner to state a name commonly associated with a certain gender and/or race/ethnicity. We find that larger capacity models tend to exhibit more gender bias and greater stereotyping of occupations by gender. We show that several methods of tuning these dialogue models, specifically name scrambling, controlled generation, and unlikelihood training, are effective in reducing bias in conversation, including on a downstream conversational task. Name scrambling is also effective in lowering differences in token usage across conversations where partners have names associated with different genders or races/ethnicities.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51324296",
                        "name": "Eric Michael Smith"
                    },
                    {
                        "authorId": "81840293",
                        "name": "Adina Williams"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                ", 2019) or methods for mitigating the bias of the dialogue model (Liu et al., 2020; Dinan et al., 2020) are recommended to be jointly used with our method when deploying our model in production.",
                "\u2026methods for reducing the toxicity of the open-domain dialogue system (Xu et al., 2020; Dinan et al., 2019) or methods for mitigating the bias of the dialogue model (Liu et al., 2020; Dinan et al., 2020) are recommended to be jointly used with our method when deploying our model in production."
            ],
            "citingPaper": {
                "paperId": "718339cbafd0b4613d3389cc2c22592764b92c62",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-12582",
                    "ArXiv": "2108.12582",
                    "DOI": "10.18653/v1/2021.findings-emnlp.286",
                    "CorpusId": 237353086
                },
                "corpusId": 237353086,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/718339cbafd0b4613d3389cc2c22592764b92c62",
                "title": "Distilling the Knowledge of Large-scale Generative Models into Retrieval Models for Efficient Open-domain Conversation",
                "abstract": "Despite the remarkable performance of large-scale generative models in open-domain conversation, they are known to be less practical for building real-time conversation systems due to high latency. On the other hand, retrieval models could return responses with much lower latency but show inferior performance to the large-scale generative models since the conversation quality is bounded by the pre-defined response set. To take advantage of both approaches, we propose a new training method called G2R (Generative-to-Retrieval distillation) that preserves the efficiency of a retrieval model while leveraging the conversational ability of a large-scale generative model by infusing the knowledge of the generative model into the retrieval model. G2R consists of two distinct techniques of distillation: the data-level G2R augments the dialogue dataset with additional responses generated by the large-scale generative model, and the model-level G2R transfers the response quality score assessed by the generative model to the score of the retrieval model by the knowledge distillation loss. Through extensive experiments including human evaluation, we demonstrate that our retrieval-based conversation system trained with G2R shows a substantially improved performance compared to the baseline retrieval model while showing significantly lower inference latency than the large-scale generative models.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2141188926",
                        "name": "Beomsu Kim"
                    },
                    {
                        "authorId": "7274568",
                        "name": "Seokjun Seo"
                    },
                    {
                        "authorId": "2423429",
                        "name": "Seungju Han"
                    },
                    {
                        "authorId": "3451729",
                        "name": "Enkhbayar Erdenee"
                    },
                    {
                        "authorId": "50814662",
                        "name": "Buru Chang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Dinan et al. (2020a) trains dialogue models with attribute conditioning to mitigate bias by producing genderneutral responses.",
                "The BST dataset contains 5K polite conversations between crowdworkers which aims to blend 3 conversational skills into one dataset 1) engaging personality (Zhang et al., 2018b; Dinan et al., 2020b), 2) empathetic dialogue (Rashkin et al., 2019) and 3) knowledge incorporation (Dinan et al., 2019b).",
                "Previous\n1Our code and corpus are available at https:// github.com/abaheti95/ToxiChat\nresearch has shown that dialogue models can produce utterances that are gender and racially biased (Wolf et al., 2017; Sheng et al., 2020; Dinan et al., 2020a)."
            ],
            "citingPaper": {
                "paperId": "f56cda7ee6b3cfa427d045b6cc754ec68349c511",
                "externalIds": {
                    "ArXiv": "2108.11830",
                    "DBLP": "conf/emnlp/BahetiSRR21",
                    "ACL": "2021.emnlp-main.397",
                    "DOI": "10.18653/v1/2021.emnlp-main.397",
                    "CorpusId": 237303836
                },
                "corpusId": 237303836,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/f56cda7ee6b3cfa427d045b6cc754ec68349c511",
                "title": "Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts",
                "abstract": "Dialogue models trained on human conversations inadvertently learn to generate toxic responses. In addition to producing explicitly offensive utterances, these models can also implicitly insult a group or individual by aligning themselves with an offensive statement. To better understand the dynamics of contextually offensive language, we investigate the stance of dialogue model responses in offensive Reddit conversations. Specifically, we create ToxiChat, a crowd-annotated dataset of 2,000 Reddit threads and model responses labeled with offensive language and stance. Our analysis reveals that 42% of human responses agree with toxic comments, whereas only 13% agree with safe comments. This undesirable behavior is learned by neural dialogue models, such as DialoGPT, which we show are two times more likely to agree with offensive comments. To enable automatic detection of offensive language, we fine-tuned transformer-based classifiers on ToxiChat that achieve 0.71 F1 for offensive labels and 0.53 Macro-F1 for stance labels. Finally, we quantify the effectiveness of controllable text generation (CTG) methods to mitigate the tendency of neural dialogue models to agree with offensive comments. Compared to the baseline, our best CTG model achieves a 19% reduction in agreement with offensive comments and produces 29% fewer offensive replies. Our work highlights the need for further efforts to characterize and analyze inappropriate behavior in dialogue models, in order to help make them safer.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3458166",
                        "name": "Ashutosh Baheti"
                    },
                    {
                        "authorId": "2729164",
                        "name": "Maarten Sap"
                    },
                    {
                        "authorId": "1863425",
                        "name": "Alan Ritter"
                    },
                    {
                        "authorId": "2065904932",
                        "name": "Mark O. Riedl"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "In the future, we head to apply and develop corresponding mitigation techniques (following works such as Dinan et al. (2020) and Liu et al. (2020))."
            ],
            "citingPaper": {
                "paperId": "450adc746ec2f2df5201c96c74861b4a44bf650a",
                "externalIds": {
                    "ArXiv": "2108.07154",
                    "DBLP": "journals/corr/abs-2108-07154",
                    "ACL": "2022.lrec-1.621",
                    "CorpusId": 237091474
                },
                "corpusId": 237091474,
                "publicationVenue": {
                    "id": "7474c4a0-75d9-4105-9809-8e7d5201c5e1",
                    "name": "International Conference on Language Resources and Evaluation",
                    "type": "conference",
                    "alternate_names": [
                        "LREC",
                        "Int Conf Lang Resour Evaluation"
                    ],
                    "url": "http://www.lrec-conf.org/"
                },
                "url": "https://www.semanticscholar.org/paper/450adc746ec2f2df5201c96c74861b4a44bf650a",
                "title": "MMChat: Multi-Modal Chat Dataset on Social Media",
                "abstract": "Incorporating multi-modal contexts in conversation is an important step for developing more engaging dialogue systems. In this work, we explore this direction by introducing MMChat: a large scale Chinese multi-modal dialogue corpus (32.4M raw dialogues and 120.84K filtered dialogues). Unlike previous corpora that are crowd-sourced or collected from fictitious movies, MMChat contains image-grounded dialogues collected from real conversations on social media, in which the sparsity issue is observed. Specifically, image-initiated dialogues in common communications may deviate to some non-image-grounded topics as the conversation proceeds. To better investigate this issue, we manually annotate 100K dialogues from MMChat and further filter the corpus accordingly, which yields MMChat-hf. We develop a benchmark model to address the sparsity issue in dialogue generation tasks by adapting the attention routing mechanism on image features. Experiments demonstrate the usefulness of incorporating image features and the effectiveness in handling the sparsity of image features.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51456789",
                        "name": "Yinhe Zheng"
                    },
                    {
                        "authorId": "48390820",
                        "name": "Guanyi Chen"
                    },
                    {
                        "authorId": "2146075269",
                        "name": "Xin Liu"
                    },
                    {
                        "authorId": "2148874976",
                        "name": "K. Lin"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "beb4f0ef465212c5eae59e85dc838d3ba47dbacc",
                "externalIds": {
                    "ArXiv": "2108.03362",
                    "DBLP": "conf/ijcnlp/DevSZASHSKNPC22",
                    "CorpusId": 252907216
                },
                "corpusId": 252907216,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/beb4f0ef465212c5eae59e85dc838d3ba47dbacc",
                "title": "On Measures of Biases and Harms in NLP",
                "abstract": "Recent studies show that Natural Language Processing (NLP) technologies propagate societal biases about demographic groups associated with attributes such as gender, race, and nationality. To create interventions and mitigate these biases and associated harms, it is vital to be able to detect and measure such biases. While existing works propose bias evaluation and mitigation methods for various tasks, there remains a need to cohesively understand the biases and the specific harms they measure, and how different measures compare with each other. To address this gap, this work presents a practical framework of harms and a series of questions that practitioners can answer to guide the development of bias measures. As a validation of our framework and documentation questions, we also present several case studies of how existing bias measures in NLP -- both intrinsic measures of bias in representations and extrinsic measures of bias of downstream applications -- can be aligned with different harms and how our proposed documentation questions facilitates more holistic understanding of what bias measures are measuring.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50991767",
                        "name": "Sunipa Dev"
                    },
                    {
                        "authorId": "23923796",
                        "name": "Emily Sheng"
                    },
                    {
                        "authorId": "33524946",
                        "name": "Jieyu Zhao"
                    },
                    {
                        "authorId": "2187873102",
                        "name": "Aubrie Amstutz"
                    },
                    {
                        "authorId": "145478138",
                        "name": "Jiao Sun"
                    },
                    {
                        "authorId": "2118739951",
                        "name": "Yu Hou"
                    },
                    {
                        "authorId": "1997944417",
                        "name": "M. Sanseverino"
                    },
                    {
                        "authorId": "2125034176",
                        "name": "Jiin Kim"
                    },
                    {
                        "authorId": "50571574",
                        "name": "Akihiro Nishi"
                    },
                    {
                        "authorId": "3157053",
                        "name": "Nanyun Peng"
                    },
                    {
                        "authorId": "2782886",
                        "name": "Kai-Wei Chang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", 2019), dialogue generation (Dinan et al., 2020; Liu et al., 2020), and machine translation (Font & Costa-juss\u00e0, 2019).",
                "Data augmentation by controlling the gender attribute is an effective technique in mitigating gender bias in NLP processes (Dinan et al., 2020; Sun et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "8b430ae5af9d7991cb3e698b2b30296fdf43dd15",
                "externalIds": {
                    "DBLP": "journals/llc/HovyP21",
                    "PubMedCentral": "9285808",
                    "DOI": "10.1111/lnc3.12432",
                    "CorpusId": 237298625,
                    "PubMed": "35864931"
                },
                "corpusId": 237298625,
                "publicationVenue": {
                    "id": "984325f8-27b7-4088-9a6d-c3de9e23beca",
                    "name": "Language and Linguistics Compass",
                    "type": "journal",
                    "alternate_names": [
                        "Lang Linguistics Compass"
                    ],
                    "issn": "1749-818X",
                    "url": "http://www.blackwell-compass.com/subject/linguistics/",
                    "alternate_urls": [
                        "http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)1749-818X",
                        "http://www.blackwell-synergy.com/loi/lnco"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8b430ae5af9d7991cb3e698b2b30296fdf43dd15",
                "title": "Five sources of bias in natural language processing",
                "abstract": "Abstract Recently, there has been an increased interest in demographically grounded bias in natural language processing (NLP) applications. Much of the recent work has focused on describing bias and providing an overview of bias in a larger context. Here, we provide a simple, actionable summary of this recent work. We outline five sources where bias can occur in NLP systems: (1) the data, (2) the annotation process, (3) the input representations, (4) the models, and finally (5) the research design (or how we conceptualize our research). We explore each of the bias sources in detail in this article, including examples and links to related work, as well as potential counter\u2010measures.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144547315",
                        "name": "E. Hovy"
                    },
                    {
                        "authorId": "9358910",
                        "name": "Shrimai Prabhumoye"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Moreover, they are often vulnerable to biases and adversarial attacks [23,12,18]."
            ],
            "citingPaper": {
                "paperId": "d305db79d2addd09996cc7a891ef4b423e611cff",
                "externalIds": {
                    "ArXiv": "2107.12079",
                    "DBLP": "journals/corr/abs-2107-12079",
                    "DOI": "10.1007/978-3-030-89391-0_27",
                    "CorpusId": 236428689
                },
                "corpusId": 236428689,
                "publicationVenue": {
                    "id": "1bfb27d0-b811-4b54-b8c8-bb8373541f2c",
                    "name": "Chinese Conference on Logic and Argumentation",
                    "type": "conference",
                    "alternate_names": [
                        "CLAR",
                        "Chin Conf Log Argum"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d305db79d2addd09996cc7a891ef4b423e611cff",
                "title": "An Argumentative Dialogue System for COVID-19 Vaccine Information",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1726971",
                        "name": "Bettina Fazzinga"
                    },
                    {
                        "authorId": "143978279",
                        "name": "Andrea Galassi"
                    },
                    {
                        "authorId": "2896208",
                        "name": "Paolo Torroni"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "7574c1a2878ece33845028adaa67f84b77b0175e",
                "externalIds": {
                    "ArXiv": "2107.13076",
                    "DBLP": "journals/ijcci/ChubbMCMW22",
                    "DOI": "10.1016/j.ijcci.2021.100403",
                    "CorpusId": 236469244
                },
                "corpusId": 236469244,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7574c1a2878ece33845028adaa67f84b77b0175e",
                "title": "Interactive Storytelling for Children: A Case-study of Design and Development Considerations for Ethical Conversational AI",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "38137487",
                        "name": "J. Chubb"
                    },
                    {
                        "authorId": "2018752",
                        "name": "S. Missaoui"
                    },
                    {
                        "authorId": "3603257",
                        "name": "S. Concannon"
                    },
                    {
                        "authorId": "114060225",
                        "name": "Liam Maloney"
                    },
                    {
                        "authorId": "2110485355",
                        "name": "James Alfred Walker"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "Mitigating Gender Bias for Neural Dialogue Generation with Adversarial Learning.",
                "Text Data Text Classiication [50, 119, 182, 209, 277, 386] Embedding [46, 56, 156, 246, 274, 397] Language Modeling [48, 148, 238, 320, 371] Machine Translation [34, 91, 157, 329, 345] Dialogue Generation [106, 117, 229] Audio Data Speech Recognition [72, 178, 294, 336]",
                "Queens Are Powerful Too: Mitigating Gender Bias in Dialogue Generation."
            ],
            "citingPaper": {
                "paperId": "8b365890c0224f17fffb90bf33da46fccacd9331",
                "externalIds": {
                    "DBLP": "journals/tist/LiuWFLLJLJT23",
                    "ArXiv": "2107.06641",
                    "DOI": "10.1145/3546872",
                    "CorpusId": 235829506
                },
                "corpusId": 235829506,
                "publicationVenue": {
                    "id": "0d993d4a-09ba-4df8-90a4-7dfe25f0cb9e",
                    "name": "ACM Transactions on Intelligent Systems and Technology",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Intell Syst Technol"
                    ],
                    "issn": "2157-6904",
                    "url": "http://portal.acm.org/tist",
                    "alternate_urls": [
                        "https://tist.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8b365890c0224f17fffb90bf33da46fccacd9331",
                "title": "Trustworthy AI: A Computational Perspective",
                "abstract": "In the past few decades, artificial intelligence (AI) technology has experienced swift developments, changing everyone\u2019s daily life and profoundly altering the course of human society. The intention behind developing AI was and is to benefit humans by reducing labor, increasing everyday conveniences, and promoting social good. However, recent research and AI applications indicate that AI can cause unintentional harm to humans by, for example, making unreliable decisions in safety-critical scenarios or undermining fairness by inadvertently discriminating against a group or groups. Consequently, trustworthy AI has recently garnered increased attention regarding the need to avoid the adverse effects that AI could bring to people, so people can fully trust and live in harmony with AI technologies. A tremendous amount of research on trustworthy AI has been conducted and witnessed in recent years. In this survey, we present a comprehensive appraisal of trustworthy AI from a computational perspective to help readers understand the latest technologies for achieving trustworthy AI. Trustworthy AI is a large and complex subject, involving various dimensions. In this work, we focus on six of the most crucial dimensions in achieving trustworthy AI: (i) Safety & Robustness, (ii) Nondiscrimination & Fairness, (iii) Explainability, (iv) Privacy, (v) Accountability & Auditability, and (vi) Environmental Well-being. For each dimension, we review the recent related technologies according to a taxonomy and summarize their applications in real-world systems. We also discuss the accordant and conflicting interactions among different dimensions and discuss potential aspects for trustworthy AI to investigate in the future.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2143856455",
                        "name": "Haochen Liu"
                    },
                    {
                        "authorId": "2108941389",
                        "name": "Yiqi Wang"
                    },
                    {
                        "authorId": "41031455",
                        "name": "Wenqi Fan"
                    },
                    {
                        "authorId": "1390612725",
                        "name": "Xiaorui Liu"
                    },
                    {
                        "authorId": "1527096073",
                        "name": "Yaxin Li"
                    },
                    {
                        "authorId": "39720946",
                        "name": "Shaili Jain"
                    },
                    {
                        "authorId": "1739705",
                        "name": "Anil K. Jain"
                    },
                    {
                        "authorId": "1736632",
                        "name": "Jiliang Tang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "\u2026language model objective on pushshift.io Reddit data (Baumgartner et al., 2020) and fine-tuned on several dialog safety classification tasks, including Wikipedia Toxic Comments (Wulczyn et al., 2017) as well as the standard and adversarial Build-it Break-it Fix-it tasks from Dinan et al. (2019b).",
                "These biases can result in toxicity being associated with certain words, such as profanities or identity terms (Dinan et al., 2019b; Dixon et al., 2018), or language varieties, such as African American English (AAE) (Liu et al., 2019; Sap et al., 2019).",
                "There exist a number of other issues related to the problem of safety for conversational AI, which we consider outside the scope of this work.",
                "In particular for conversation, Dinan et al. (2019c) apply retrieval over Wikipedia to aid in open-domain dialogs.",
                "The topic of when and how to release LLMs trained by research groups has been of increasing interest to the community (Solaiman et al., 2019; Crootof, 2019; Ovadya & Whittlestone, 2019; Partnership on AI, 2020; Partnership on AI , 2021).",
                "This might be because the safety classifier was trained to identify dialog utterances that are \u201cnot OK to send in a friendly conversation with someone you just met online\u201d, which may encapsulate more than just toxic responses (Dinan et al., 2019b).",
                "Gathered from the literature on responsible AI, the topics of the framework are split out by concept for clarity and to allow for targeted mitigation measures, however the topics naturally support each other and are often not as clearly delineated for all applications.",
                "Dinan et al. (2019b) make a first attempt at this by building a dataset for offensive utterance detection within a multi-turn dialog context, but limited to human-human dialogs.",
                "Dinan et al. (2019a) find gender biases present in several conversational datasets, and evaluate three debiasing techniques: counterfactual data augmentation, targeted data collection, and bias controlled training.",
                "For the dialog domain, Xu et al. (2020) extend the strategy of Dinan et al. (2019b) for collecting and training on adversarial examples to the human-bot conversational setting, with crowdworkers attempting to elicit unsafe outputs from the system.",
                "While (to our knowledge) little work exists on this problem for conversational AI, Dinan et al. (2019b) highlight the risks of systems exhibiting the YEA-SAYER (ELIZA) EFFECT in such situations by potentially agreeing with user statements suggesting self-harm.",
                "To test how the model responds to toxic input, we select 180 examples from the Build-it Break-it Fix-it \u201cStandard\u201d dataset (Dinan et al., 2019b) which are labeled as unsafe.",
                "These two facts taken together can result in situations where the system generates inappropriate content (Dinan et al., 2019b), or responds inappropriately to offensive content (Cercas Curry & Rieser, 2018; Lee et al., 2019).",
                "For conversational AI, the language(s) the model was trained on, the demographic composition and size of the intended audience, and the intended audience\u2019s familiarity with concepts and limitations of machine learning and NLP are all important considerations.",
                "\u2022 Dialog safety classifier: We use a dialog safety classifier from Dinan et al. (2019b), and report the percentage of model responses that are flagged as unsafe by this classifier.",
                "We call conversational models trained in this paradigm end-to-end\n1We follow European Commission (2021)\u2019s definition of AI, which includes Machine Learning, statistical, as well as logic- and knowledge-based approaches.\nar X\niv :2\n10 7.",
                "Dinan et al. (2019b); Xu et al. (2020) augment training data for the task with adversarial examples elicited from crowd workers, and train Transformer-based models for these tasks.",
                "Evolving benchmarks, such as Dynabench (Kiela et al., 2021), or other adversarial iterative procedures (Dinan et al., 2019b; Nie et al., 2019; Xu et al., 2020) can provide the required adaptability: our societal standards and expectations change, and we would not tolerate models that do not reflect\u2026"
            ],
            "citingPaper": {
                "paperId": "2ef4ab54d00203f9ac610213ac3abc8e1fe541b4",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-03451",
                    "ArXiv": "2107.03451",
                    "CorpusId": 235765704
                },
                "corpusId": 235765704,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2ef4ab54d00203f9ac610213ac3abc8e1fe541b4",
                "title": "Anticipating Safety Issues in E2E Conversational AI: Framework and Tooling",
                "abstract": "Over the last several years, end-to-end neural conversational agents have vastly improved in their ability to carry a chit-chat conversation with humans. However, these models are often trained on large datasets from the internet, and as a result, may learn undesirable behaviors from this data, such as toxic or otherwise harmful language. Researchers must thus wrestle with the issue of how and when to release these models. In this paper, we survey the problem landscape for safety for end-to-end conversational AI and discuss recent and related work. We highlight tensions between values, potential positive impact and potential harms, and provide a framework for making decisions about whether and how to release these models, following the tenets of value-sensitive design. We additionally provide a suite of tools to enable researchers to make better-informed decisions about training and releasing end-to-end conversational AI models.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "31461304",
                        "name": "Emily Dinan"
                    },
                    {
                        "authorId": "17038002",
                        "name": "Gavin Abercrombie"
                    },
                    {
                        "authorId": "2055868822",
                        "name": "A. S. Bergman"
                    },
                    {
                        "authorId": "3416737",
                        "name": "S. Spruit"
                    },
                    {
                        "authorId": "2022288",
                        "name": "Dirk Hovy"
                    },
                    {
                        "authorId": "90841478",
                        "name": "Y-Lan Boureau"
                    },
                    {
                        "authorId": "1681799",
                        "name": "Verena Rieser"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", 2020), data augmentation or collection (Dinan et al., 2020), and different objective functions (Qian et al.",
                "Existing approaches towards mitigating biases in generation currently require retraining the models through adversarial trigger prompts (Sheng et al., 2020), data augmentation or collection (Dinan et al., 2020), and different objective functions (Qian et al., 2019; Huang et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "114aa720872462b0ca1b97bfdec0ebd56c36fd0a",
                "externalIds": {
                    "DBLP": "conf/icml/LiangWMS21",
                    "ArXiv": "2106.13219",
                    "CorpusId": 235623756
                },
                "corpusId": 235623756,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/114aa720872462b0ca1b97bfdec0ebd56c36fd0a",
                "title": "Towards Understanding and Mitigating Social Biases in Language Models",
                "abstract": "As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "28130078",
                        "name": "P. Liang"
                    },
                    {
                        "authorId": "2115397918",
                        "name": "Chiyu Wu"
                    },
                    {
                        "authorId": "49933077",
                        "name": "Louis-Philippe Morency"
                    },
                    {
                        "authorId": "145124475",
                        "name": "R. Salakhutdinov"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "One\nline of work addresses safety based on the fairness of chatbots regarding gender and race (Liu et al., 2020; Dinan et al., 2020; Lee et al., 2019b).",
                "of chatbots regarding gender and race (Liu et al., 2020; Dinan et al., 2020; Lee et al., 2019b)."
            ],
            "citingPaper": {
                "paperId": "d1290807d6089713a6710285ac115904c39c311d",
                "externalIds": {
                    "DBLP": "conf/sigdial/BangLIMF21",
                    "ACL": "2021.sigdial-1.57",
                    "ArXiv": "2106.06157",
                    "CorpusId": 235417195
                },
                "corpusId": 235417195,
                "publicationVenue": {
                    "id": "6a470734-72c6-4809-a07d-d34dee0df4a1",
                    "name": "SIGDIAL Conferences",
                    "type": "conference",
                    "alternate_names": [
                        "SIGDIAL",
                        "SIGDIAL Conf",
                        "Annu Meet Sp\u00e9c Interest Group Discourse Dialogue",
                        "Annual Meeting of the Special Interest Group on Discourse and Dialogue"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d1290807d6089713a6710285ac115904c39c311d",
                "title": "Assessing Political Prudence of Open-domain Chatbots",
                "abstract": "Politically sensitive topics are still a challenge for open-domain chatbots. However, dealing with politically sensitive content in a responsible, non-partisan, and safe behavior way is integral for these chatbots. Currently, the main approach to handling political sensitivity is by simply changing such a topic when it is detected. This is safe but evasive and results in a chatbot that is less engaging. In this work, as a first step towards a politically safe chatbot, we propose a group of metrics for assessing their political prudence. We then conduct political prudence analysis of various chatbots and discuss their behavior from multiple angles through our automatic metric and human evaluation metrics. The testsets and codebase are released to promote research in this area.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "23672613",
                        "name": "Yejin Bang"
                    },
                    {
                        "authorId": "40221187",
                        "name": "Nayeon Lee"
                    },
                    {
                        "authorId": "38524906",
                        "name": "Etsuko Ishii"
                    },
                    {
                        "authorId": "2111680936",
                        "name": "Andrea Madotto"
                    },
                    {
                        "authorId": "40539650",
                        "name": "Pascale Fung"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Dinan et al. (2020b) focus on multi-dimensional gender bias classification and controlled mitigation.",
                "Dinan et al. (2020a) analyze existing dialog data sets for gender bias and extend LIGHT (Urbanek et al., 2019), a resource for grounded dialog, with crowdsourced gender-balanced utterances.",
                "\u2026of work that focuses on detecting and mitigating biases in conversational systems is surprisingly limited (Lee et al., 2019; Liu et al., 2020a,b; Dinan et al., 2020a,b), albeit some more research has recently emerged in the wider context of biases in generalpurpose language generation models\u2026"
            ],
            "citingPaper": {
                "paperId": "2add974973ab45e46f1f8d3b932d24ba88cbb0b4",
                "externalIds": {
                    "DBLP": "conf/acl/BarikeriLVG20",
                    "ArXiv": "2106.03521",
                    "ACL": "2021.acl-long.151",
                    "DOI": "10.18653/v1/2021.acl-long.151",
                    "CorpusId": 235358955
                },
                "corpusId": 235358955,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/2add974973ab45e46f1f8d3b932d24ba88cbb0b4",
                "title": "RedditBias: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models",
                "abstract": "Text representation models are prone to exhibit a range of societal biases, reflecting the non-controlled and biased nature of the underlying pretraining data, which consequently leads to severe ethical issues and even bias amplification. Recent work has predominantly focused on measuring and mitigating bias in pretrained language models. Surprisingly, the landscape of bias measurements and mitigation resources and methods for conversational language models is still very scarce: it is limited to only a few types of bias, artificially constructed resources, and completely ignores the impact that debiasing methods may have on the final perfor mance in dialog tasks, e.g., conversational response generation. In this work, we present REDDITBIAS, the first conversational data set grounded in the actual human conversations from Reddit, allowing for bias measurement and mitigation across four important bias dimensions: gender,race,religion, and queerness. Further, we develop an evaluation framework which simultaneously 1)measures bias on the developed REDDITBIAS resource, and 2)evaluates model capability in dialog tasks after model debiasing. We use the evaluation framework to benchmark the widely used conversational DialoGPT model along with the adaptations of four debiasing methods. Our results indicate that DialoGPT is biased with respect to religious groups and that some debiasing techniques can remove this bias while preserving downstream task performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2107062845",
                        "name": "Soumya Barikeri"
                    },
                    {
                        "authorId": "29891652",
                        "name": "Anne Lauscher"
                    },
                    {
                        "authorId": "1747849",
                        "name": "Ivan Vulic"
                    },
                    {
                        "authorId": "2472657",
                        "name": "Goran Glavas"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Dialog models are known to suffer from biases learnable from dialog training data, such as gender bias (Dinan et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "43946aa78c80b6d7e6ffff837bdf4cff85f6a935",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-02833",
                    "ACL": "2021.findings-acl.357",
                    "ArXiv": "2106.02833",
                    "DOI": "10.18653/v1/2021.findings-acl.357",
                    "CorpusId": 235358430
                },
                "corpusId": 235358430,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/43946aa78c80b6d7e6ffff837bdf4cff85f6a935",
                "title": "Improving Automated Evaluation of Open Domain Dialog via Diverse Reference Augmentation",
                "abstract": "Multiple different responses are often plausible for a given open domain dialog context. Prior work has shown the importance of having multiple valid reference responses for meaningful and robust automated evaluations. In such cases, common practice has been to collect more human written references. However, such collection can be expensive, time consuming, and not easily scalable. Instead, we propose a novel technique for automatically expanding a human generated reference to a set of candidate references. We fetch plausible references from knowledge sources, and adapt them so that they are more fluent in context of the dialog instance in question. More specifically, we use (1) a commonsense knowledge base to elicit a large number of plausible reactions given the dialog history (2) relevant instances retrieved from dialog corpus, using similar past as well as future contexts. We demonstrate that our automatically expanded reference sets lead to large improvements in correlations of automated metrics with human ratings of system outputs for DailyDialog dataset.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3375999",
                        "name": "Varun Gangal"
                    },
                    {
                        "authorId": "2006291",
                        "name": "Harsh Jhamtani"
                    },
                    {
                        "authorId": "144547315",
                        "name": "E. Hovy"
                    },
                    {
                        "authorId": "1400419309",
                        "name": "Taylor Berg-Kirkpatrick"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Other works have attempted to mitigate biases, such as gender bias (Dinan et al., 2020a) and racial bias (Sap et al.",
                "Other works have attempted to mitigate biases, such as gender bias (Dinan et al., 2020a) and racial bias (Sap et al., 2019).",
                "For example, recent workshop on safety for conversational AI (Dinan et al., 2020b) introduced\n\u2217Equal contribution\nan example of such risk: Bickmore et al. (2018) asked participants to query conversational agents for advice in situations where medical information is needed."
            ],
            "citingPaper": {
                "paperId": "828ace8bc78f6ab0636d5c7183e773a0e49d367c",
                "externalIds": {
                    "DBLP": "conf/naacl/KimKHK21",
                    "ACL": "2021.naacl-main.121",
                    "MAG": "3167986947",
                    "DOI": "10.18653/V1/2021.NAACL-MAIN.121",
                    "CorpusId": 235097501
                },
                "corpusId": 235097501,
                "publicationVenue": {
                    "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
                    "name": "North American Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "North Am Chapter Assoc Comput Linguistics",
                        "NAACL"
                    ],
                    "url": "https://www.aclweb.org/portal/naacl"
                },
                "url": "https://www.semanticscholar.org/paper/828ace8bc78f6ab0636d5c7183e773a0e49d367c",
                "title": "How Robust are Fact Checking Systems on Colloquial Claims?",
                "abstract": "Knowledge is now starting to power neural dialogue agents. At the same time, the risk of misinformation and disinformation from dialogue agents also rises. Verifying the veracity of information from formal sources are widely studied in computational fact checking. In this work, we ask: How robust are fact checking systems on claims in colloquial style? We aim to open up new discussions in the intersection of fact verification and dialogue safety. In order to investigate how fact checking systems behave on colloquial claims, we transfer the styles of claims from FEVER (Thorne et al., 2018) into colloquialism. We find that existing fact checking systems that perform well on claims in formal style significantly degenerate on colloquial claims with the same semantics. Especially, we show that document retrieval is the weakest spot in the system even vulnerable to filler words, such as \u201cyeah\u201d and \u201cyou know\u201d. The document recall of WikiAPI retriever (Hanselowski et al., 2018) which is 90.0% on FEVER, drops to 72.2% on the colloquial claims. We compare the characteristics of colloquial claims to those of claims in formal style, and demonstrate the challenging issues in them.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3231991",
                        "name": "Byeongchang Kim"
                    },
                    {
                        "authorId": "32609381",
                        "name": "Hyunwoo Kim"
                    },
                    {
                        "authorId": "98486910",
                        "name": "Seokhee Hong"
                    },
                    {
                        "authorId": "70308241",
                        "name": "Gunhee Kim"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", replacing \u201che\u201d with \u201cthey\u201d following [14, 13]) and (ii) by substituting names with others that are statistically predictive [70] of another race or ethnicity (e."
            ],
            "citingPaper": {
                "paperId": "d25bb256e5b69f769a429750217b0d9ec1cf4d86",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-06052",
                    "ArXiv": "2106.06052",
                    "CorpusId": 235399978
                },
                "corpusId": 235399978,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d25bb256e5b69f769a429750217b0d9ec1cf4d86",
                "title": "Dynaboard: An Evaluation-As-A-Service Platform for Holistic Next-Generation Benchmarking",
                "abstract": "We introduce Dynaboard, an evaluation-as-a-service framework for hosting benchmarks and conducting holistic model comparison, integrated with the Dynabench platform. Our platform evaluates NLP models directly instead of relying on self-reported metrics or predictions on a single dataset. Under this paradigm, models are submitted to be evaluated in the cloud, circumventing the issues of reproducibility, accessibility, and backwards compatibility that often hinder benchmarking in NLP. This allows users to interact with uploaded models in real time to assess their quality, and permits the collection of additional metrics such as memory use, throughput, and robustness, which -- despite their importance to practitioners -- have traditionally been absent from leaderboards. On each task, models are ranked according to the Dynascore, a novel utility-based aggregation of these statistics, which users can customize to better reflect their preferences, placing more/less weight on a particular axis of evaluation or dataset. As state-of-the-art NLP models push the limits of traditional benchmarks, Dynaboard offers a standardized solution for a more diverse and comprehensive evaluation of model quality.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1736107",
                        "name": "Zhiyi Ma"
                    },
                    {
                        "authorId": "10324691",
                        "name": "Kawin Ethayarajh"
                    },
                    {
                        "authorId": "1500242049",
                        "name": "Tristan Thrush"
                    },
                    {
                        "authorId": "2116457668",
                        "name": "Somya Jain"
                    },
                    {
                        "authorId": "51183248",
                        "name": "Ledell Yu Wu"
                    },
                    {
                        "authorId": "3422908",
                        "name": "Robin Jia"
                    },
                    {
                        "authorId": "144922861",
                        "name": "Christopher Potts"
                    },
                    {
                        "authorId": "2110032535",
                        "name": "Adina Williams"
                    },
                    {
                        "authorId": "2111313627",
                        "name": "Douwe Kiela"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "73f949b35d0d6a234565ba219ad0f865c2db5657",
                "externalIds": {
                    "ArXiv": "2105.05541",
                    "DBLP": "journals/corr/abs-2105-05541",
                    "CorpusId": 234469667
                },
                "corpusId": 234469667,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/73f949b35d0d6a234565ba219ad0f865c2db5657",
                "title": "Evaluating Gender Bias in Natural Language Inference",
                "abstract": "Gender-bias stereotypes have recently raised significant ethical concerns in natural language processing. However, progress in detection and evaluation of gender bias in natural language understanding through inference is limited and requires further investigation. In this work, we propose an evaluation methodology to measure these biases by constructing a challenge task that involves pairing gender-neutral premises against a gender-specific hypothesis. We use our challenge task to investigate state-of-the-art NLI models on the presence of gender stereotypes using occupations. Our findings suggest that three models (BERT, RoBERTa, BART) trained on MNLI and SNLI datasets are significantly prone to gender-induced prediction errors. We also find that debiasing techniques such as augmenting the training dataset to ensure a gender-balanced dataset can help reduce such bias in certain cases.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1409842673",
                        "name": "Shanya Sharma"
                    },
                    {
                        "authorId": "1879591269",
                        "name": "Manan Dey"
                    },
                    {
                        "authorId": "40910779",
                        "name": "Koustuv Sinha"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "\u20262019)\nThere are also metrics for other bias evaluation setups in continuation generation tasks involving sentiment (Shwartz et al., 2020), the ratio of gendered words (Solaiman et al., 2019; Vig et al., 2020; Dinan et al., 2020a), and other novel metrics (Peng et al., 2020; Yeo and Chen, 2020).",
                "For autocomplete generation, Vig et al. (2020) analyze GPT-2 variants through a causal mediation analysis, finding that larger models contain more gender bias, and bias tends to be concentrated in a small number of neurons and attention heads.",
                ", 2020a,b) and the percentage of gendered words (Dinan et al., 2020a).",
                "Other works on dialogue biases (Dinan et al., 2020a; Sheng et al., 2020, 2021b) focus",
                "There are also works that induce control by incorporating a bias control code through conditional training (Dinan et al., 2020a), by appending a target value to inputs during training (Ma et al., 2020), by using a normative classifier to produce reward values for backpropagation (Peng et al.,\u2026",
                "For autocomplete generation, Sheng et al. (2019, 2020) and Groenwold et al. (2020) compare regard or sentiment scores across demographics, Shwartz et al. (2020) compare names across various intermediate metrics, Vig et al. (2020) measure proportional differences between the amount of bias under a gendered versus ambiguous reading, and Yeo and Chen (2020) compare occupations generated for different genders.",
                "Examples include: \u2022 Regard Ratio: negative-neutral-positive regard\nscore ratios of text generated from bias-inducing prompts (Sheng et al., 2019) \u2022 Sentiment Ratio: negative-neutral-positive sentiment score ratios of text generated from African American English (AAE) versus White-Aligned English (WAE) prompts (Groenwold et al., 2020) \u2022 Individual and Group Fairness through Sentiment: comparisons of the sentiment distributions of generated text across demographics and prompts (Huang et al., 2020) \u2022 Gendered Word Co-occurrence Score: mean and standard deviations of the absolute log ratio of probabilities: P(word|female terms) to P(word|male terms) across all words in generated text (Bordia and Bowman, 2019)\nThere are also metrics for other bias evaluation setups in continuation generation tasks involving sentiment (Shwartz et al., 2020), the ratio of gendered words (Solaiman et al., 2019; Vig et al., 2020; Dinan et al., 2020a), and other novel metrics (Peng et al., 2020; Yeo and Chen, 2020).",
                "CDA has been applied to datasets used for continued or fresh training in dialogue generation (Dinan et al., 2020a; Liu et al., 2020a) as well as machine translation (Saunders and Byrne, 2020; Costa-jussa\u0300 and de Jorge, 2020; Stafanovic\u030cs et al., 2020).",
                "Other works on dialogue biases (Dinan et al., 2020a; Sheng et al., 2020, 2021b) focus on Transformer-based models such as DialoGPT (Zhang et al., 2020) and other custom architectures.",
                "CDA has been applied to datasets used for continued or fresh training in dialogue generation (Dinan et al., 2020a; Liu et al., 2020a) as well as machine translation (Saunders and Byrne, 2020; Costa-juss\u00e0 and de Jorge, 2020; Stafanovi\u010ds et al.",
                "Bias studies in dialogue generation use relative scores by comparing sentiment and offensive language discrepancies (Henderson et al., 2018; Liu et al., 2020a,b) and the percentage of gendered words (Dinan et al., 2020a).",
                "Existing works analyzing biases in autocomplete generation have mostly examined Transformer-based models, including GPT (Shwartz et al., 2020), GPT2 (Solaiman et al., 2019; Sheng et al., 2019, 2020; Shwartz et al., 2020; Vig et al., 2020; Yeo and Chen, 2020; Huang et al., 2020; Dhamala et al., 2021; Schick et al., 2021), GPT-3 (Brown et al., 2020), CTRL (Dhamala et al., 2021), TransformerXL (Shwartz et al., 2020; Vig et al., 2020; Huang et al., 2020), and XLNet (Shwartz et al., 2020; Vig et al.,\n2020; Yeo and Chen, 2020), though Bordia and Bowman (2019); Qian et al. (2019) also look at LSTM-based models.",
                "There are also works that induce control by incorporating a bias control code through conditional training (Dinan et al., 2020a), by ap-",
                "Biases can be towards people described in text, people who produce the text, or people to whom the text is addressed (Dinan et al., 2020b).",
                "Vig et al. (2020) also use prompts to investigate gender biases, though they do so in the context of a causal mediation analysis."
            ],
            "citingPaper": {
                "paperId": "76a786b1acd6d1aca56e12a8a1db34569fdf9f3a",
                "externalIds": {
                    "DBLP": "conf/acl/ShengCNP20",
                    "ArXiv": "2105.04054",
                    "ACL": "2021.acl-long.330",
                    "DOI": "10.18653/v1/2021.acl-long.330",
                    "CorpusId": 234337004
                },
                "corpusId": 234337004,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/76a786b1acd6d1aca56e12a8a1db34569fdf9f3a",
                "title": "Societal Biases in Language Generation: Progress and Challenges",
                "abstract": "Technology for language generation has advanced rapidly, spurred by advancements in pre-training large models on massive amounts of data and the need for intelligent agents to communicate in a natural manner. While techniques can effectively generate fluent text, they can also produce undesirable societal biases that can have a disproportionately negative impact on marginalized populations. Language generation presents unique challenges for biases in terms of direct user interaction and the structure of decoding techniques. To better understand these challenges, we present a survey on societal biases in language generation, focusing on how data and techniques contribute to biases and progress towards reducing biases. Motivated by a lack of studies on biases from decoding techniques, we also conduct experiments to quantify the effects of these techniques. By further discussing general trends and open challenges, we call to attention promising directions for research and the importance of fairness and inclusivity considerations for language generation applications.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "23923796",
                        "name": "Emily Sheng"
                    },
                    {
                        "authorId": "2782886",
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "authorId": "145603129",
                        "name": "P. Natarajan"
                    },
                    {
                        "authorId": "3157053",
                        "name": "Nanyun Peng"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Moreover, open dialogue chatbot models have been found to amplify gender bias that exist in training dialogues [7, 16]."
            ],
            "citingPaper": {
                "paperId": "dda28c0ac5c2699236e58739202b25ae65f65532",
                "externalIds": {
                    "DBLP": "conf/chi/MaurielloLHSJP21",
                    "DOI": "10.1145/3411763.3451799",
                    "CorpusId": 233361747
                },
                "corpusId": 233361747,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/dda28c0ac5c2699236e58739202b25ae65f65532",
                "title": "SAD: A Stress Annotated Dataset for Recognizing Everyday Stressors in SMS-like Conversational Systems",
                "abstract": "There is limited infrastructure for providing stress management services to those in need. To address this problem, chatbots are viewed as a scalable solution. However, one limiting factor is having clear definitions and examples of daily stress on which to build models and methods for routing appropriate advice during conversations. We developed a dataset of 6850 SMS-like sentences that can be used to classify input using a scheme of 9 stressor categories derived from: stress management literature, live conversations from a prototype chatbot system, crowdsourcing, and targeted web scraping from an online repository. In addition to releasing this dataset, we show results that are promising for classification purposes. Our contributions include: (i) a categorization of daily stressors, (ii) a dataset of SMS-like sentences, (iii) an analysis of this dataset that demonstrates its potential efficacy, and (iv) a demonstration of its utility for implementation via a simulation of model response times.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2934690",
                        "name": "M. Mauriello"
                    },
                    {
                        "authorId": "2003550684",
                        "name": "Emmanuel Thierry Lincoln"
                    },
                    {
                        "authorId": "2047508490",
                        "name": "Grace Hon"
                    },
                    {
                        "authorId": "2080572237",
                        "name": "Dorien Simon"
                    },
                    {
                        "authorId": "1746807",
                        "name": "Dan Jurafsky"
                    },
                    {
                        "authorId": "145656681",
                        "name": "P. Paredes"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Previous existing works have taken different approaches to address the issue of gender bias by detecting the male/female ratio of images [19, 20], measuring fairness in dialogues systems [10, 23], language modeling [4], machine translation [7], and coreference resolution [42]."
            ],
            "citingPaper": {
                "paperId": "d6246cc6ace146bc8c1f4b004370386878b634fe",
                "externalIds": {
                    "DBLP": "conf/www/DaconL21",
                    "DOI": "10.1145/3442442.3452325",
                    "CorpusId": 235324884
                },
                "corpusId": 235324884,
                "publicationVenue": {
                    "id": "e07422f9-c065-40c3-a37b-75e98dce79fe",
                    "name": "The Web Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Web Conf",
                        "WWW"
                    ],
                    "url": "http://www.iw3c2.org/"
                },
                "url": "https://www.semanticscholar.org/paper/d6246cc6ace146bc8c1f4b004370386878b634fe",
                "title": "Does Gender Matter in the News? Detecting and Examining Gender Bias in News Articles",
                "abstract": "To attract unsuspecting readers, news article headlines and abstracts are often written with speculative sentences or clauses. Male dominance in the news is very evident, whereas females are seen as \u201ceye candy\u201d or \u201cinferior\u201d, and are underrepresented and under-examined within the same news categories as their male counterparts. In this paper, we present an initial study on gender bias in news abstracts in two large English news datasets used for news recommendation and news classification. We perform three large-scale, yet effective text-analysis fairness measurements on 296,965 news abstracts. In particular, to our knowledge we construct two of the largest benchmark datasets of possessive (gender-specific and gender-neutral) nouns and attribute (career-related and family-related) words datasets1 which we will release to foster both bias and fairness research aid in developing fair NLP models to eliminate the paradox of gender bias. Our studies demonstrate that females are immensely marginalized and suffer from socially-constructed biases in the news. This paper individually devises a methodology whereby news content can be analyzed on a large scale utilizing natural language processing (NLP) techniques from machine learning (ML) to discover both implicit and explicit gender biases.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1380259269",
                        "name": "Jamell Dacon"
                    },
                    {
                        "authorId": "2143856455",
                        "name": "Haochen Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Dialogue Personas Most similar to our work, Dinan et al. (2020) explore how different personas lead to different amounts of generated gendered words and pursue strategies for mitigation."
            ],
            "citingPaper": {
                "paperId": "34be38f7f18f3fb4a58256ddf96365f2934551dd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-08728",
                    "ArXiv": "2104.08728",
                    "CorpusId": 233296615
                },
                "corpusId": 233296615,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/34be38f7f18f3fb4a58256ddf96365f2934551dd",
                "title": "Revealing Persona Biases in Dialogue Systems",
                "abstract": "Dialogue systems in the form of chatbots and personal assistants are being increasingly integrated into people's lives. Modern dialogue systems may consider adopting anthropomorphic personas, mimicking societal demographic groups to appear more approachable and trustworthy to users. However, the adoption of a persona can result in the adoption of biases. In this paper, we present the first large-scale study on persona biases in dialogue systems and conduct analyses on personas of different social classes, sexual orientations, races, and genders. We define persona biases as harmful differences in responses (e.g., varying levels of offensiveness, agreement with harmful statements) generated from adopting different demographic personas. Furthermore, we introduce an open-source framework, UnitPersonaBias, to explore and aggregate persona biases in dialogue systems. By analyzing the Blender and DialoGPT dialogue systems, we observe that adopting personas can actually decrease harmful responses, compared to not using any personas. Additionally, we find that persona choices can affect the degree of harms in generated responses and thus should be systematically evaluated before deployment. We also analyze how personas can result in different amounts of harm towards specific demographics.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "23923796",
                        "name": "Emily Sheng"
                    },
                    {
                        "authorId": "2054031596",
                        "name": "Josh Arnold"
                    },
                    {
                        "authorId": "1564034697",
                        "name": "Zhou Yu"
                    },
                    {
                        "authorId": "2782886",
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "authorId": "3157053",
                        "name": "Nanyun Peng"
                    }
                ]
            }
        },
        {
            "intents": [
                "result"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026al., 2016; Hovy and Spruit, 2016; Caliskan et al., 2017; Rudinger et al., 2017; Garg et al., 2018; Garimella\net al., 2019; Gonen and Goldberg, 2019; Dinan et al., 2020a,b), we further observe in our results yet a higher order type of stereotyping that negatively affects women, namely androcentrism\u2026"
            ],
            "citingPaper": {
                "paperId": "180182c45291f814e06f44c47f285798cae6098f",
                "externalIds": {
                    "ArXiv": "2104.07838",
                    "DBLP": "conf/acl/RenduchintalaW22",
                    "ACL": "2022.acl-long.243",
                    "DOI": "10.18653/v1/2022.acl-long.243",
                    "CorpusId": 233289702
                },
                "corpusId": 233289702,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/180182c45291f814e06f44c47f285798cae6098f",
                "title": "Investigating Failures of Automatic Translation\n\nin the Case of Unambiguous Gender",
                "abstract": "Transformer-based models are the modern work horses for neural machine translation (NMT), reaching state of the art across several benchmarks. Despite their impressive accuracy, we observe a systemic and rudimentary class of errors made by current state-of-the-art NMT models with regards to translating from a language that doesn\u2019t mark gender on nouns into others that do. We find that even when the surrounding context provides unambiguous evidence of the appropriate grammatical gender marking, no tested model was able to accurately gender occupation nouns systematically. We release an evaluation scheme and dataset for measuring the ability of NMT models to translate gender morphology correctly in unambiguous contexts across syntactically diverse sentences. Our dataset translates from an English source into 20 languages from several different language families. With the availability of this dataset, our hope is that the NMT community can iterate on solutions for this class of especially egregious errors.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3286437",
                        "name": "Adithya Renduchintala"
                    },
                    {
                        "authorId": "2110032535",
                        "name": "Adina Williams"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u20262017; Gonen and Goldberg, 2019; Dev and Phillips, 2019; Garg et al., 2018; Lauscher et al., 2020a) and even more models for removing or attenuating such biases have been developed (Zhao et al., 2019; Bordia and Bowman, 2019; Dinan et al., 2020; Webster et al., 2020; Qian et al., 2019, inter alia)."
            ],
            "citingPaper": {
                "paperId": "713ac8f8adbc5a049de2f996ca03b149faec0abd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-06598",
                    "ArXiv": "2103.06598",
                    "ACL": "2021.eacl-demos.11",
                    "DOI": "10.18653/v1/2021.eacl-demos.11",
                    "CorpusId": 232185097
                },
                "corpusId": 232185097,
                "publicationVenue": {
                    "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
                    "name": "Conference of the European Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Eur Chapter Assoc Comput Linguistics",
                        "EACL"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/eacl/"
                },
                "url": "https://www.semanticscholar.org/paper/713ac8f8adbc5a049de2f996ca03b149faec0abd",
                "title": "DebIE: A Platform for Implicit and Explicit Debiasing of Word Embedding Spaces",
                "abstract": "Recent research efforts in NLP have demonstrated that distributional word vector spaces often encode stereotypical human biases, such as racism and sexism. With word representations ubiquitously used in NLP models and pipelines, this raises ethical issues and jeopardizes the fairness of language technologies. While there exists a large body of work on bias measures and debiasing methods, to date, there is no platform that would unify these research efforts and make bias measuring and debiasing of representation spaces widely accessible. In this work, we present DebIE, the first integrated platform for (1) measuring and (2) mitigating bias in word embeddings. Given an (i) embedding space (users can choose between the predefined spaces or upload their own) and (ii) a bias specification (users can choose between existing bias specifications or create their own), DebIE can (1) compute several measures of implicit and explicit bias and modify the embedding space by executing two (mutually composable) debiasing models. DebIE\u2019s functionality can be accessed through four different interfaces: (a) a web application, (b) a desktop application, (c) a REST-ful API, and (d) as a command-line application. DebIE is available at: debie.informatik.uni-mannheim.de.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2062924459",
                        "name": "Niklas Friedrich"
                    },
                    {
                        "authorId": "29891652",
                        "name": "Anne Lauscher"
                    },
                    {
                        "authorId": "1801255",
                        "name": "Simone Paolo Ponzetto"
                    },
                    {
                        "authorId": "1666177566",
                        "name": "Goran Glavavs"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "8a7df164c4687e3c402b1cbf0ab404de49cfb75e",
                "externalIds": {
                    "DBLP": "conf/icml/PerezKC21",
                    "ArXiv": "2103.03872",
                    "CorpusId": 232135266
                },
                "corpusId": 232135266,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8a7df164c4687e3c402b1cbf0ab404de49cfb75e",
                "title": "Rissanen Data Analysis: Examining Dataset Characteristics via Description Length",
                "abstract": "We introduce a method to determine if a certain capability helps to achieve an accurate model of given data. We view labels as being generated from the inputs by a program composed of subroutines with different capabilities, and we posit that a subroutine is useful if and only if the minimal program that invokes it is shorter than the one that does not. Since minimum program length is uncomputable, we instead estimate the labels' minimum description length (MDL) as a proxy, giving us a theoretically-grounded method for analyzing dataset characteristics. We call the method Rissanen Data Analysis (RDA) after the father of MDL, and we showcase its applicability on a wide variety of settings in NLP, ranging from evaluating the utility of generating subquestions before answering a question, to analyzing the value of rationales and explanations, to investigating the importance of different parts of speech, and uncovering dataset gender bias.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3439053",
                        "name": "Ethan Perez"
                    },
                    {
                        "authorId": "1743722",
                        "name": "Douwe Kiela"
                    },
                    {
                        "authorId": "1979489",
                        "name": "Kyunghyun Cho"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Standard Data We consider the Wikipedia Toxic Comments dataset (WTC) (Wulczyn et al., 2017) designed to identify personal attacks online, consisting of \u223c150k examples; we use the version that treats the data as a two-class problem (Khatri et al., 2018a; Dinan et al., 2019c).",
                "\u2026dataset (Zhang et al., 2018) focuses on personality and engaging the other speaker, Empathetic Dialogues (Rashkin et al., 2019) focuses on empathy, and Wizard of Wikipedia (Dinan et al., 2019c) focuses\n1https://github.com/huggingface/ tokenizers\n2https://files.pushshift.io/reddit/\non knowledge.",
                "Firstly, we find our newly trained models superior to existing models from Dinan et al. (2019b) when using the same training sets, likely due to improved pushshift.io Reddit pre-training of our transformers compared to their BERT models.",
                "We consider Transformer-based classifiers, following the same structure as in Dinan et al. (2019b), with two sizes: 256M and 622M parameter models.",
                "Dinan et al. (2019a) measured gender bias in several conversational datasets and proposed three techniques to address it: counterfactual data augmentation, targeted data collection, and bias controlled training.",
                "In addition, we consider a dataset more specifically collected for safety in open-domain dialogue of (Dinan et al., 2019b), which consists of a further 8,000 offensive examples.",
                "The resulting conversational models were shown to use less gendered words, be less offensive, while being as engaging (Dinan et al., 2019a).",
                "The classifiers we propose in this work can be seen as improvements over the variants introduced in Dinan et al. (2019b).",
                "We detailed previously how safety classifiers can be trained to be adversarially robust to human utterances, see Section 3.1.1 or Dinan et al. (2019b).",
                "\u2022 Gender Bias Mitigation (\u00a73.4): Using strategies from Dinan et al. (2019a) to force the model to respond with gender neutral language.",
                "We select a topic at random from 1087 topics judged as safe from the Wizard of Wikipedia conversational topic list (Dinan et al., 2019c).",
                "The work of\nDinan et al. (2019b) thus also explored an adversarial collection scheme to make classifiers more robust.",
                "Gender bias is exhibited across a wide range of conversational datasets, including Reddit (Dinan et al., 2019a).",
                "Dinan et al. (2019b); Nie et al. (2019) are examples of such evolving benchmarks4.",
                "For example, the baseline BST 2.7B only provides OK responses 55% of the time on the adversarial test set, whereas our Safety classifier improves that to 87.5%, superior to the existing work of Dinan et al. (2019b) which yields 77.7%.",
                "\u2026collection setup are given in Appendix A.\nFigure 1 demonstrates how this adversarial setup differs from the \u201cBuild-it, Break-it, Fix-it\u201d setup from Dinan et al. (2019b): namely, in the former, the \u201cbreaker\u201d (or adversarial user) tries to break a classifier by submitting human-authored\u2026",
                "(Some) (Most)\nTwo-stage models with classifiers\nBST 2.7B + Multi-Turn Safety Classifier (Dinan et al., 2019b) 78.2 6.7 6.7 8.4 BST 2.7B + Safety Classifier 87.2 5.6 3.9 3.3 BST 2.7B + Safety Classifier (Semi-Sup."
            ],
            "citingPaper": {
                "paperId": "4fa24cc5b17e8ff1eb5a01fd37a9d267a57ac563",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-07079",
                    "ArXiv": "2010.07079",
                    "MAG": "3093233911",
                    "CorpusId": 222341902
                },
                "corpusId": 222341902,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4fa24cc5b17e8ff1eb5a01fd37a9d267a57ac563",
                "title": "Recipes for Safety in Open-domain Chatbots",
                "abstract": "Models trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which include offensive or otherwise toxic behavior and unwanted biases. We investigate a variety of methods to mitigate these issues in the context of open-domain generative dialogue models. We introduce a new human-and-model-in-the-loop framework for both training safer models and for evaluating them, as well as a novel method to distill safety considerations inside generative models without the use of an external classifier at deployment time. We conduct experiments comparing these methods and find our new techniques are (i) safer than existing models as measured by automatic and human evaluations while (ii) maintaining usability metrics such as engagingness relative to the state of the art. We then discuss the limitations of this work by analyzing failure cases of our models.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2155954521",
                        "name": "Jing Xu"
                    },
                    {
                        "authorId": "3092435",
                        "name": "Da Ju"
                    },
                    {
                        "authorId": "6649233",
                        "name": "Margaret Li"
                    },
                    {
                        "authorId": "90841478",
                        "name": "Y-Lan Boureau"
                    },
                    {
                        "authorId": "145183709",
                        "name": "J. Weston"
                    },
                    {
                        "authorId": "31461304",
                        "name": "Emily Dinan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "NLP papers on spurious associations have addressed social biases (Dixon et al., 2018; Zhao et al., 2018; Kiritchenko & Mohammad, 2018; Dinan et al., 2019; May et al., 2019), spurious signals learned due to annotation heuristics adopted by crowd workers (Gururangan et al., 2018; Poliak et al.,\u2026",
                "NLP papers on spurious associations have addressed social biases (Dixon et al., 2018; Zhao et al., 2018; Kiritchenko & Mohammad, 2018; Dinan et al., 2019; May et al., 2019), spurious signals learned due to annotation heuristics adopted by crowd workers (Gururangan et al."
            ],
            "citingPaper": {
                "paperId": "24fcdaf969089e6a411f7cebc9274bbc53c25e42",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-02114",
                    "ArXiv": "2010.02114",
                    "MAG": "3089770252",
                    "CorpusId": 222133021
                },
                "corpusId": 222133021,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/24fcdaf969089e6a411f7cebc9274bbc53c25e42",
                "title": "Explaining The Efficacy of Counterfactually-Augmented Data",
                "abstract": "In attempts to produce machine learning models less reliant on spurious patterns in training data, researchers have recently proposed a human-in-the-loop process for generating counterfactually augmented datasets. As applied in NLP, given some documents and their (initial) labels, humans are tasked with revising the text to make a (given) counterfactual label applicable. Importantly, the instructions prohibit edits that are not necessary to flip the applicable label. Models trained on the augmented (original and revised) data have been shown to rely less on semantically irrelevant words and to generalize better out-of-domain. While this work draws on causal thinking, casting edits as interventions and relying on human understanding to assess outcomes, the underlying causal model is not clear nor are the principles underlying the observed improvements in out-of-domain evaluation. In this paper, we explore a toy analog, using linear Gaussian models. Our analysis reveals interesting relationships between causal models, measurement noise, out-of-domain generalization, and reliance on spurious signals. Interestingly our analysis suggests that data corrupted by adding noise to causal features will degrade out-of-domain performance, while noise added to non-causal features may make models more robust out-of-domain. This analysis yields interesting insights that help to explain the efficacy of counterfactually augmented data. Finally, we present a large-scale empirical study that supports this hypothesis.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "9264826",
                        "name": "Divyansh Kaushik"
                    },
                    {
                        "authorId": "80366270",
                        "name": "Amrith Rajagopal Setlur"
                    },
                    {
                        "authorId": "144547315",
                        "name": "E. Hovy"
                    },
                    {
                        "authorId": "32219137",
                        "name": "Zachary Chase Lipton"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "\u2026et al. (2020): ConvAI2 (Dinan et al., 2020b), EmpatheticDialogues (ED) (Rashkin et al., 2019), Wiz-\n1Unlike in those works, the output of the encoder is then passed to a decoder, as in the late fusion case.\nard of Wikipedia (WoW) (Dinan et al., 2019c), and BlendedSkillTalk (Smith et al., 2020).",
                "Wizard of Wikipedia (WoW) The Wizard of Wikipedia dataset (Dinan et al., 2019c) involves\n4https://pytorch.org/hub/ facebookresearch_WSL-Images_resnext/\n5https://github.com/facebookresearch/ vilbert-multi-task\ntwo speakers discussing a given topic in depth, comprising 194k utterances.",
                "\u2026ways in which the MMB Style model could potentially display gen-\nder bias: for instance, there is no safeguard against it misgendering a person in an image, and many common text datasets are known to contain gender bias (Dinan et al., 2019a, 2020a), which may lead to bias in models trained on them.",
                "ore with our best models is safety - that is, ensuring that our models are not offensive to their conversational partners. Dialogue safety is indeed a well-studied, but still unsolved, research area (Dinan et al., 2019b;Liu et al.,2019;Dinan et al.,2019a;Blodgett et al.,2020;Khatri et al.,2018;Schafer and\u00a8 Burtenshaw,2019;Zhang et al.,2018a), yet we note that safety in the context of image-dialogue is relatively le",
                "To mitigate this problem, we first measure our models\u2019 toxicity using an openly available blocklist3 and an offensive language classifier presented in Dinan et al. (2019b).",
                "Dialogue safety is indeed a well-studied, but still unsolved, research area (Dinan et al., 2019b; Liu et al., 2019; Dinan et al., 2019a; Blodgett et al., 2020; Khatri et al., 2018; Sch\u00e4fer and Burtenshaw, 2019; Zhang et al., 2018a), yet we note that safety in the context of image-dialogue is\u2026",
                "\u2026this, we train a version of the MMB Style model in which we examine the label of each training example to determine whether it contains female or male words, and then a string representing that classification is appended to the example\u2019s context string (Dinan et al., 2019a), for input to the model.",
                " to an unsafe response given a multi-modal context. To mitigate this problem, we \ufb01rst measure our models\u2019 toxicity using an openly available blocklist7 and an offensive language classi\ufb01er presented inDinan et al. (2019b). We de\ufb01ne the term \u201ctoxicity\u201d to mean the ratio between the number of offensive utterances and the total number of utterances generated by the model. We evaluate our model on the Image-Chat validat"
            ],
            "citingPaper": {
                "paperId": "cf58cbdaf475109da7c528e6d5d390ed97fba6b2",
                "externalIds": {
                    "ACL": "2021.emnlp-main.398",
                    "DBLP": "journals/corr/abs-2010-01082",
                    "ArXiv": "2010.01082",
                    "MAG": "3090998540",
                    "DOI": "10.18653/v1/2021.emnlp-main.398",
                    "CorpusId": 222125009
                },
                "corpusId": 222125009,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/cf58cbdaf475109da7c528e6d5d390ed97fba6b2",
                "title": "Multi-Modal Open-Domain Dialogue",
                "abstract": "Recent work in open-domain conversational agents has demonstrated that significant improvements in humanness and user preference can be achieved via massive scaling in both pre-training data and model size (Adiwardana et al., 2020; Roller et al., 2020). However, if we want to build agents with human-like abilities, we must expand beyond handling just text. A particularly important topic is the ability to see images and communicate about what is perceived. With the goal of getting humans to engage in multi-modal dialogue, we investigate combining components from state-of-the-art open-domain dialogue agents with those from state-of-the-art vision models. We study incorporating different image fusion schemes and domain-adaptive pre-training and fine-tuning strategies, and show that our best resulting model outperforms strong existing models in multi-modal dialogue while simultaneously performing as well as its predecessor (text-only) BlenderBot (Roller et al., 2020) in text-based conversation. We additionally investigate and incorporate safety components in our final model, and show that such efforts do not diminish model performance with respect to human preference.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "35752280",
                        "name": "Kurt Shuster"
                    },
                    {
                        "authorId": "51324296",
                        "name": "Eric Michael Smith"
                    },
                    {
                        "authorId": "3092435",
                        "name": "Da Ju"
                    },
                    {
                        "authorId": "145183709",
                        "name": "J. Weston"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "815 tem, enabling us to filter the possible outputs of the agent; and (2) dataset bias via curation through controlled crowdsourcing in the case of LIGHTQuests\u2014the methods to debias the original LIGHT dataset can be found in Dinan et al. (2020) and crowdsourcing methods for the original ATOMIC work can be found in Sap et al.",
                "\u2026possible outputs of the agent; and (2) dataset bias via curation through controlled crowdsourcing in the case of LIGHTQuests\u2014the methods to debias the original LIGHT dataset can be found in Dinan et al. (2020) and crowdsourcing methods for the original ATOMIC work can be found in Sap et al. (2019)."
            ],
            "citingPaper": {
                "paperId": "34b274329317b8e7aa8f8af60f3aa22ed7d87cbb",
                "externalIds": {
                    "MAG": "3090518591",
                    "ACL": "2021.naacl-main.64",
                    "DBLP": "journals/corr/abs-2010-00685",
                    "ArXiv": "2010.00685",
                    "DOI": "10.18653/V1/2021.NAACL-MAIN.64",
                    "CorpusId": 222125301
                },
                "corpusId": 222125301,
                "publicationVenue": {
                    "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
                    "name": "North American Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "North Am Chapter Assoc Comput Linguistics",
                        "NAACL"
                    ],
                    "url": "https://www.aclweb.org/portal/naacl"
                },
                "url": "https://www.semanticscholar.org/paper/34b274329317b8e7aa8f8af60f3aa22ed7d87cbb",
                "title": "How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and Act in Fantasy Worlds",
                "abstract": "We seek to create agents that both act and communicate with other agents in pursuit of a goal. Towards this end, we extend LIGHT (Urbanek et al. 2019)\u2014a large-scale crowd-sourced fantasy text-game\u2014with a dataset of quests. These contain natural language motivations paired with in-game goals and human demonstrations; completing a quest might require dialogue or actions (or both). We introduce a reinforcement learning system that (1) incorporates large-scale language modeling-based and commonsense reasoning-based pre-training to imbue the agent with relevant priors; and (2) leverages a factorized action space of action commands and dialogue, balancing between the two. We conduct zero-shot evaluations using held-out human expert demonstrations, showing that our agents are able to act consistently and talk naturally with respect to their motivations.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "19179135",
                        "name": "Prithviraj Ammanabrolu"
                    },
                    {
                        "authorId": "39219656",
                        "name": "Jack Urbanek"
                    },
                    {
                        "authorId": "6649233",
                        "name": "Margaret Li"
                    },
                    {
                        "authorId": "3149531",
                        "name": "Arthur Szlam"
                    },
                    {
                        "authorId": "1389854357",
                        "name": "Tim Rocktaschel"
                    },
                    {
                        "authorId": "145183709",
                        "name": "J. Weston"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "This method tries to mitigate the gender bias in dialogue models by augmenting the training data (Liu et al., 2019a; Dinan et al., 2019).",
                "Dinan et al. (2019) analyze gender bias in persona-based dialogue models and proposes a combination debiasing method.",
                "There are debiasing methods in NLP such as data augmentation (Dinan et al., 2019) and word embeddings regularization (Liu et al., 2019a).",
                "There are debiasing methods in NLP such as data augmentation (Dinan et al., 2019) and word embeddings regularization (Liu et al."
            ],
            "citingPaper": {
                "paperId": "0ec122ced09eda481239db7c6db6bb66ff635229",
                "externalIds": {
                    "MAG": "3087790324",
                    "DBLP": "conf/emnlp/LiuWWLLT20",
                    "ACL": "2020.emnlp-main.64",
                    "ArXiv": "2009.13028",
                    "DOI": "10.18653/v1/2020.emnlp-main.64",
                    "CorpusId": 221970809
                },
                "corpusId": 221970809,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/0ec122ced09eda481239db7c6db6bb66ff635229",
                "title": "Mitigating Gender Bias for Neural Dialogue Generation with Adversarial Learning",
                "abstract": "Dialogue systems play an increasingly important role in various aspects of our daily life. It is evident from recent research that dialogue systems trained on human conversation data are biased. In particular, they can produce responses that reflect people's gender prejudice. Many debiasing methods have been developed for various NLP tasks, such as word embedding. However, they are not directly applicable to dialogue systems because they are likely to force dialogue models to generate similar responses for different genders. This greatly degrades the diversity of the generated responses and immensely hurts the performance of the dialogue models. In this paper, we propose a novel adversarial learning framework Debiased-Chat to train dialogue models free from gender bias while keeping their performance. Extensive experiments on two real-world conversation datasets show that our framework significantly reduces gender bias in dialogue models while maintaining the response quality. The implementation of the proposed framework is released.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "66442354",
                        "name": "Haochen Liu"
                    },
                    {
                        "authorId": "2108329255",
                        "name": "Wentao Wang"
                    },
                    {
                        "authorId": "2108941389",
                        "name": "Yiqi Wang"
                    },
                    {
                        "authorId": "2146672392",
                        "name": "Hui Liu"
                    },
                    {
                        "authorId": "2117940912",
                        "name": "Zitao Liu"
                    },
                    {
                        "authorId": "1736632",
                        "name": "Jiliang Tang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Other work has focused on removing gender bias from language models (Bordia and Bowman, 2019; Dinan et al., 2020; Bolukbasi et al., 2016)."
            ],
            "citingPaper": {
                "paperId": "07bcda1dff9bb696ea9cbc69303eee8bd3d85bd6",
                "externalIds": {
                    "DBLP": "conf/emnlp/KrauseGMKJSR21",
                    "ArXiv": "2009.06367",
                    "MAG": "3085190015",
                    "DOI": "10.18653/v1/2021.findings-emnlp.424",
                    "CorpusId": 221655075
                },
                "corpusId": 221655075,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/07bcda1dff9bb696ea9cbc69303eee8bd3d85bd6",
                "title": "GeDi: Generative Discriminator Guided Sequence Generation",
                "abstract": "While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives stronger controllability than the state of the art method while also achieving generation speeds more than 30 times faster. Additionally, training GeDi on only four topics allows us to controllably generate new topics zero-shot from just a keyword, unlocking a new capability that previous controllable generation methods do not have. Lastly, we show that GeDi can make GPT-2 (1.5B parameters) significantly less toxic without sacrificing linguistic quality, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "9340968",
                        "name": "Ben Krause"
                    },
                    {
                        "authorId": "144049726",
                        "name": "Akhilesh Deepak Gotmare"
                    },
                    {
                        "authorId": "143775536",
                        "name": "Bryan McCann"
                    },
                    {
                        "authorId": "2844898",
                        "name": "N. Keskar"
                    },
                    {
                        "authorId": "2708940",
                        "name": "Shafiq R. Joty"
                    },
                    {
                        "authorId": "2166511",
                        "name": "R. Socher"
                    },
                    {
                        "authorId": "8937909",
                        "name": "Nazneen Rajani"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "ing set (Dinan et al., 2019a), and we make use of that publicly available data here as well.",
                "Additionally, gender bias concerns have been previously studied within the available LIGHT MTurk training set (Dinan et al., 2019a), and we make use of that publicly available data here as well.",
                "Game Safety We employ a safety classifier (Dinan et al., 2019b) on both human and model turns.",
                "A number of crowdsourced or scraped datasets have been developed to that end, including Daily Dialogue (Li et al., 2017), PersonaChat (Li et al., 2016a), Empathetic Dialogues (Rashkin et al., 2019) and Wizard of Wikipedia (Dinan et al., 2019c)."
            ],
            "citingPaper": {
                "paperId": "63913530782522e0d7ca5deceb40c08d606cafab",
                "externalIds": {
                    "ArXiv": "2008.08076",
                    "DBLP": "journals/corr/abs-2008-08076",
                    "MAG": "3073992377",
                    "CorpusId": 221150578
                },
                "corpusId": 221150578,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/63913530782522e0d7ca5deceb40c08d606cafab",
                "title": "Deploying Lifelong Open-Domain Dialogue Learning",
                "abstract": "Much of NLP research has focused on crowdsourced static datasets and the supervised learning paradigm of training once and then evaluating test performance. As argued in de Vries et al. (2020), crowdsourced data has the issues of lack of naturalness and relevance to real-world use cases, while the static dataset paradigm does not allow for a model to learn from its experiences of using language (Silver et al., 2013). In contrast, one might hope for machine learning systems that become more useful as they interact with people. In this work, we build and deploy a role-playing game, whereby human players converse with learning agents situated in an open-domain fantasy world. We show that by training models on the conversations they have with humans in the game the models progressively improve, as measured by automatic metrics and online engagement scores. This learning is shown to be more efficient than crowdsourced data when applied to conversations with real users, as well as being far cheaper to collect.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "35752280",
                        "name": "Kurt Shuster"
                    },
                    {
                        "authorId": "39219656",
                        "name": "Jack Urbanek"
                    },
                    {
                        "authorId": "31461304",
                        "name": "Emily Dinan"
                    },
                    {
                        "authorId": "3149531",
                        "name": "Arthur Szlam"
                    },
                    {
                        "authorId": "145183709",
                        "name": "J. Weston"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "Vague/unstated Rudinger et al. (2018); Webster et al. (2018); Dinan et al. (2019); Florez (2019); Jumelet et al. (2019); Lauscher et al. (2019); Liang et al. (2019); Maudslay et al. (2019); May et al. (2019); Prates et al. (2019); Prost et al. (2019); Qian et al. (2019); Swinger et al. (2019); Zhao\u2026",
                "\u2026et al. (2018); Shen et al. (2018); Bordia and Bowman (2019); Cao and Daum\u00e9 (2019); Cho et al. (2019); Davidson et al. (2019); Dev et al. (2019); Dinan et al. (2019); Fisher (2019); Florez (2019); Font and Costa-juss\u00e0 (2019); Garg et al. (2019); Huang et al. (2019); Liu et al. (2019); Nozza et\u2026"
            ],
            "citingPaper": {
                "paperId": "d47a682723f710395454687319bb55635e653105",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2005-14050",
                    "MAG": "3032388710",
                    "ArXiv": "2005.14050",
                    "ACL": "2020.acl-main.485",
                    "DOI": "10.18653/v1/2020.acl-main.485",
                    "CorpusId": 218971825
                },
                "corpusId": 218971825,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/d47a682723f710395454687319bb55635e653105",
                "title": "Language (Technology) is Power: A Critical Survey of \u201cBias\u201d in NLP",
                "abstract": "We survey 146 papers analyzing \u201cbias\u201d in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing \u201cbias\u201d is an inherently normative process. We further find that these papers\u2019 proposed quantitative techniques for measuring or mitigating \u201cbias\u201d are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing \u201cbias\u201d in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of \u201cbias\u201d---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements\u2014and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3422038",
                        "name": "Su Lin Blodgett"
                    },
                    {
                        "authorId": "2881033",
                        "name": "Solon Barocas"
                    },
                    {
                        "authorId": "2065041692",
                        "name": "Hal Daum'e"
                    },
                    {
                        "authorId": "1831395",
                        "name": "Hanna M. Wallach"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "However, with the wide application of neural dialogue models, the ethical challenges they bring are attracting more and more attention (Henderson et al., 2017; Liu et al., 2019a; Dinan et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "51a007a8914e419489e41a7e41464edcdbf22007",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2005-13170",
                    "ArXiv": "2005.13170",
                    "MAG": "3031668985",
                    "CorpusId": 218900654
                },
                "corpusId": 218900654,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/51a007a8914e419489e41a7e41464edcdbf22007",
                "title": "Chat as Expected: Learning to Manipulate Black-box Neural Dialogue Models",
                "abstract": "Recently, neural network based dialogue systems have become ubiquitous in our increasingly digitalized society. However, due to their inherent opaqueness, some recently raised concerns about using neural models are starting to be taken seriously. In fact, intentional or unintentional behaviors could lead to a dialogue system to generate inappropriate responses. Thus, in this paper, we investigate whether we can learn to craft input sentences that result in a black-box neural dialogue model being manipulated into having its outputs contain target words or match target sentences. We propose a reinforcement learning based model that can generate such desired inputs automatically. Extensive experiments on a popular well-trained state-of-the-art neural dialogue model show that our method can successfully seek out desired inputs that lead to the target outputs in a considerable portion of cases. Consequently, our work reveals the potential of neural dialogue models to be manipulated, which inspires and opens the door towards developing strategies to defend them.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "66442354",
                        "name": "Haochen Liu"
                    },
                    {
                        "authorId": null,
                        "name": "Zhiwei Wang"
                    },
                    {
                        "authorId": "12524628",
                        "name": "Tyler Derr"
                    },
                    {
                        "authorId": "1736632",
                        "name": "Jiliang Tang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Finally, many scholars have proposed a variety of computational techniques for mitigating gender norms and stereotypes in a wide range of languagebased applications (Dev and Phillips, 2019; Dinan et al., 2019; Ethayarajh et al., 2019; Hall Maudslay et al., 2019; Stanovsky et al., 2019; Tan and Celis, 2019; Zhou et al., 2019; Zmigrod et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "a2741413ababcf422216f9628887685e7d427ddc",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2005-01204",
                    "ArXiv": "2005.01204",
                    "MAG": "3020923013",
                    "DOI": "10.1162/tacl_a_00355",
                    "CorpusId": 218486825
                },
                "corpusId": 218486825,
                "publicationVenue": {
                    "id": "e0dbf116-86aa-418d-859f-a49952d7e44a",
                    "name": "Transactions of the Association for Computational Linguistics",
                    "type": "journal",
                    "alternate_names": [
                        "Trans Assoc Comput Linguistics",
                        "TACL"
                    ],
                    "issn": "2307-387X",
                    "url": "https://www.mitpressjournals.org/loi/tacl",
                    "alternate_urls": [
                        "http://www.transacl.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a2741413ababcf422216f9628887685e7d427ddc",
                "title": "On the Relationships Between the Grammatical Genders of Inanimate Nouns and Their Co-Occurring Adjectives and Verbs",
                "abstract": "Abstract We use large-scale corpora in six different gendered languages, along with tools from NLP and information theory, to test whether there is a relationship between the grammatical genders of inanimate nouns and the adjectives used to describe those nouns. For all six languages, we find that there is a statistically significant relationship. We also find that there are statistically significant relationships between the grammatical genders of inanimate nouns and the verbs that take those nouns as direct objects, as indirect objects, and as subjects. We defer deeper investigation of these relationships for future work.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "81840293",
                        "name": "Adina Williams"
                    },
                    {
                        "authorId": "1750769",
                        "name": "Ryan Cotterell"
                    },
                    {
                        "authorId": "1390095176",
                        "name": "Lawrence Wolf-Sonkin"
                    },
                    {
                        "authorId": "6894443",
                        "name": "Dami\u00e1n E. Blasi"
                    },
                    {
                        "authorId": "1831395",
                        "name": "Hanna M. Wallach"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Control models have been used in a variety of settings to adjust length (Fan et al., 2018), bias (Dinan et al., 2019), and style (See et al., 2019).",
                ", 2018), bias (Dinan et al., 2019), and style (See et al."
            ],
            "citingPaper": {
                "paperId": "452bb9d66c4a15e34dce28a0b9c8e56ab99b5fc4",
                "externalIds": {
                    "ArXiv": "2005.00352",
                    "DBLP": "journals/corr/abs-2005-00352",
                    "MAG": "3023237241",
                    "CorpusId": 218470582
                },
                "corpusId": 218470582,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/452bb9d66c4a15e34dce28a0b9c8e56ab99b5fc4",
                "title": "Multilingual Unsupervised Sentence Simplification",
                "abstract": "Progress in Sentence Simplification has been hindered by the lack of supervised data, particularly in languages other than English. Previous work has aligned sentences from original and simplified corpora such as English Wikipedia and Simple English Wikipedia, but this limits corpus size, domain, and language. In this work, we propose using unsupervised mining techniques to automatically create training corpora for simplification in multiple languages from raw Common Crawl web data. When coupled with a controllable generation mechanism that can flexibly adjust attributes such as length and lexical complexity, these mined paraphrase corpora can be used to train simplification systems in any language. We further incorporate multilingual unsupervised pretraining methods to create even stronger models and show that by training on mined data rather than supervised corpora, we outperform the previous best results. We evaluate our approach on English, French, and Spanish simplification benchmarks and reach state-of-the-art performance with a totally unsupervised approach. We will release our models and code to mine the data in any language included in Common Crawl.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "143792623",
                        "name": "Louis Martin"
                    },
                    {
                        "authorId": "144270981",
                        "name": "Angela Fan"
                    },
                    {
                        "authorId": "1400417301",
                        "name": "Eric Villemonte de la Clergerie"
                    },
                    {
                        "authorId": "1713934",
                        "name": "Antoine Bordes"
                    },
                    {
                        "authorId": "68990982",
                        "name": "Beno\u00eet Sagot"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "In particular, NLP models often learn to replicate unwanted gender biases present in society (Bolukbasi et al., 2016; Hovy and Spruit, 2016; Caliskan et al., 2017; Rudinger et al., 2017; Garg et al., 2018; Gonen and Goldberg, 2019; Dinan et al., 2020).",
                "ited in coverage and applicability to a variety of domains (Dinan et al., 2020).",
                "have been found to be amplified in machine learning models (Lee et al., 2019; Dinan et al., 2020; Liu et al., 2019)."
            ],
            "citingPaper": {
                "paperId": "ad9d93406f3cf3ffe5a640cb4d742f202339a511",
                "externalIds": {
                    "MAG": "3104617516",
                    "ArXiv": "2005.00614",
                    "DBLP": "conf/emnlp/DinanFWWKW20",
                    "ACL": "2020.emnlp-main.23",
                    "DOI": "10.18653/v1/2020.emnlp-main.23",
                    "CorpusId": 218487627
                },
                "corpusId": 218487627,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/ad9d93406f3cf3ffe5a640cb4d742f202339a511",
                "title": "Multi-Dimensional Gender Bias Classification",
                "abstract": "Machine learning models are trained to find patterns in data. NLP models can inadvertently learn socially undesirable patterns when training on gender biased text. In this work, we propose a general framework that decomposes gender bias in text along several pragmatic and semantic dimensions: bias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker. Using this fine-grained framework, we automatically annotate eight large scale datasets with gender information. In addition, we collect a novel, crowdsourced evaluation benchmark of utterance-level gender rewrites. Distinguishing between gender bias along multiple dimensions is important, as it enables us to train finer-grained gender bias classifiers. We show our classifiers prove valuable for a variety of important applications, such as controlling for gender bias in generative models, detecting gender bias in arbitrary text, and shed light on offensive language in terms of genderedness.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "31461304",
                        "name": "Emily Dinan"
                    },
                    {
                        "authorId": "144270981",
                        "name": "Angela Fan"
                    },
                    {
                        "authorId": "51183248",
                        "name": "Ledell Yu Wu"
                    },
                    {
                        "authorId": "145183709",
                        "name": "J. Weston"
                    },
                    {
                        "authorId": "1743722",
                        "name": "Douwe Kiela"
                    },
                    {
                        "authorId": "81840293",
                        "name": "Adina Williams"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background",
                "result"
            ],
            "isInfluential": true,
            "contexts": [
                "Retriever We fine-tune the retrieval models on ConvAI2, Wizard of Wikipedia, Empathetic Dialogues, and Blended Skill Talk datasets (BST variants of each7) and automatically evaluate them by measuring hits@1/K on the validation sets of each of these datasets.",
                ", 2019b) and mitigating gender bias in dialogue generation (Dinan et al., 2019a) but much work remains to be done.",
                "8We also compared adding a Wizard of Wikipedia-based topic vs. not to the context, and in that case saw no discernible difference in evaluation scores.",
                "We then assessed whether those generations were safe or not using two different methods: using an unsafe word list, or the safety classifier of Dinan et al. (2019b), both methods being available in ParlAI (Miller et al., 2017).",
                "We have also previously conducted studies into mitigating gender bias in dialogue through the use of conditional generation, controlling the amount of gendered words to be more neutral, with preliminary success (Dinan et al., 2019a).",
                "For example, the ConvAI2 dataset (Zhang et al., 2018) focuses on personality and engaging the other speaker, Empathetic Dialogues (Rashkin et al., 2019) focuses on empathy, and Wizard of Wikipedia (Dinan et al., 2019c) focuses on knowledge.",
                "A retrieval system over Wikipedia was used from which the dialogues were grounded during the human-human crowdsourced conversations.",
                "This mirrors results found in some recent papers comparing generation and retrieval (Li et al., 2016; Dinan et al., 2019c).",
                "We use the same retrieval system as in that cited work, which uses a TF-IDF-based inverted index lookup over a Wikipedia dump2 to produce an initial set of knowledge candidates.",
                "We have previously investigated building better classifiers of toxic language by collecting adversarial toxic data that fools existing classifiers and is then used as additional data to make them more robust, in a series of rounds (Dinan et al., 2019b).",
                "We hence refer to this as a Wizard Generative model, as the supervised training signal of how to use knowledge in dialogue comes from the Wizard of Wikipedia task, even though we multi-task on other tasks as well.",
                "Wizard of Wikipedia (WoW): The Wizard of Wikipedia task involves discussing a given topic in depth, where the goal is to both engage the partner as well as display expert knowledge (Dinan et al., 2019c).",
                "7https://parl.ai/projects/bst\nWe also report perplexity both before and after fine-tuning each of these models on the ConvAI2, Wizard of Wikipedia, Empathetic Dialogues, and Blended Skill Talk datasets.",
                "We can then condition the generation on the retrieved knowledge, as done in models proposed for the Wizard of Wikipedia task (Dinan et al., 2019c)."
            ],
            "citingPaper": {
                "paperId": "9b539d413393047b28bb7be9b195f142aaf7a80e",
                "externalIds": {
                    "ACL": "2021.eacl-main.24",
                    "MAG": "3023786569",
                    "DBLP": "journals/corr/abs-2004-13637",
                    "ArXiv": "2004.13637",
                    "DOI": "10.18653/v1/2021.eacl-main.24",
                    "CorpusId": 216562425
                },
                "corpusId": 216562425,
                "publicationVenue": {
                    "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
                    "name": "Conference of the European Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Eur Chapter Assoc Comput Linguistics",
                        "EACL"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/eacl/"
                },
                "url": "https://www.semanticscholar.org/paper/9b539d413393047b28bb7be9b195f142aaf7a80e",
                "title": "Recipes for Building an Open-Domain Chatbot",
                "abstract": "Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we highlight other ingredients. Good conversation requires blended skills: providing engaging talking points, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available. Human evaluations show our best models outperform existing approaches in multi-turn dialogue on engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "144745718",
                        "name": "Stephen Roller"
                    },
                    {
                        "authorId": "31461304",
                        "name": "Emily Dinan"
                    },
                    {
                        "authorId": "39589154",
                        "name": "Naman Goyal"
                    },
                    {
                        "authorId": "3092435",
                        "name": "Da Ju"
                    },
                    {
                        "authorId": "2066769956",
                        "name": "Mary Williamson"
                    },
                    {
                        "authorId": "11323179",
                        "name": "Yinhan Liu"
                    },
                    {
                        "authorId": "2155954521",
                        "name": "Jing Xu"
                    },
                    {
                        "authorId": "40511414",
                        "name": "Myle Ott"
                    },
                    {
                        "authorId": "35752280",
                        "name": "Kurt Shuster"
                    },
                    {
                        "authorId": "51324296",
                        "name": "Eric Michael Smith"
                    },
                    {
                        "authorId": "2656573",
                        "name": "Y.-Lan Boureau"
                    },
                    {
                        "authorId": "145183709",
                        "name": "J. Weston"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Offensive responses cause users discomfort and should be avoided (Henderson et al., 2018; Dinan et al., 2019b; Liu et al., 2019; Liu et al., 2020b).",
                "In the work (Dinan et al., 2019a), the authors examine gender bias in both dialogue datasets and generative dialogue models.",
                "In this measurement, we apply an offensive language detection model (Dinan et al., 2019b) to predict whether a response is offensive or not."
            ],
            "citingPaper": {
                "paperId": "5334e1857e910e2c7855c909c9495fb0ea28efbb",
                "externalIds": {
                    "MAG": "2982277189",
                    "DBLP": "journals/corr/abs-1910-10486",
                    "ACL": "2020.coling-main.390",
                    "ArXiv": "1910.10486",
                    "DOI": "10.18653/V1/2020.COLING-MAIN.390",
                    "CorpusId": 204838020
                },
                "corpusId": 204838020,
                "publicationVenue": {
                    "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
                    "name": "International Conference on Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Linguistics",
                        "COLING"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/coling/"
                },
                "url": "https://www.semanticscholar.org/paper/5334e1857e910e2c7855c909c9495fb0ea28efbb",
                "title": "Does Gender Matter? Towards Fairness in Dialogue Systems",
                "abstract": "Recently there are increasing concerns about the fairness of Artificial Intelligence (AI) in real-world applications such as computer vision and recommendations. For example, recognition algorithms in computer vision are unfair to black people such as poorly detecting their faces and inappropriately identifying them as \u201cgorillas\u201d. As one crucial application of AI, dialogue systems have been extensively applied in our society. They are usually built with real human conversational data; thus they could inherit some fairness issues which are held in the real world. However, the fairness of dialogue systems has not been well investigated. In this paper, we perform a pioneering study about the fairness issues in dialogue systems. In particular, we construct a benchmark dataset and propose quantitative measures to understand fairness in dialogue models. Our studies demonstrate that popular dialogue models show significant prejudice towards different genders and races. Besides, to mitigate the bias in dialogue systems, we propose two simple but effective debiasing methods. Experiments show that our methods can reduce the bias in dialogue systems significantly. The dataset and the implementation are released to foster fairness research in dialogue systems.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "66442354",
                        "name": "Haochen Liu"
                    },
                    {
                        "authorId": "1380259269",
                        "name": "Jamell Dacon"
                    },
                    {
                        "authorId": "41031455",
                        "name": "Wenqi Fan"
                    },
                    {
                        "authorId": "2146672392",
                        "name": "Hui Liu"
                    },
                    {
                        "authorId": "2117940912",
                        "name": "Zitao Liu"
                    },
                    {
                        "authorId": "1736632",
                        "name": "Jiliang Tang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "c3aba0f3185989768b22a29f8f431d80a5710171",
                "externalIds": {
                    "CorpusId": 255771485
                },
                "corpusId": 255771485,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c3aba0f3185989768b22a29f8f431d80a5710171",
                "title": "Chatbots & Dialogue Systems",
                "abstract": null,
                "year": 2023,
                "authors": []
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026as related documents (Zhou et al., 2018; Dinan et al., 2019) and user-based features such as persona (Zhang et al., 2018; Majumder et al., 2020; Dinan et al., 2020b), emotion (Rashkin et al., 2019), social norms (Kim\n4For a more comprehensive literature review, refer to survey papers on\u2026",
                "ConvAI2 (Zhang et al., 2018; Dinan et al., 2020b): This dataset is designed for persona\n5The purpose of this analysis is to find out if there are any notable patterns associated with the inclusion of situational statements rather than benchmarking response generation systems.",
                "Therefore, the data and system output should be closely monitored, either manually or through automatic methods such as debiasing techniques (Liu et al., 2020; Dinan et al., 2020a)."
            ],
            "citingPaper": {
                "paperId": "5252ed78cc59700c905a13dafafeaf713d2e5af8",
                "externalIds": {
                    "ACL": "2023.nlp4convai-1.2",
                    "DOI": "10.18653/v1/2023.nlp4convai-1.2",
                    "CorpusId": 260063121
                },
                "corpusId": 260063121,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5252ed78cc59700c905a13dafafeaf713d2e5af8",
                "title": "On the Underspecification of Situations in Open-domain Conversational Datasets",
                "abstract": "Advances of open-domain conversational systems have been achieved through the creation of numerous conversation datasets. However, many of the commonly used datasets contain little or no information about the conversational situation, such as relevant objects/people, their properties, and relationships. This absence leads to underspecification of the problem space and typically results in undesired dialogue system behavior. This position paper discusses the current state of the field associated with processing situational information. An analysis of response generation using three datasets shows that explicitly provided situational information can improve the coherence and specificity of generated responses, but further experiments reveal that generation systems can be misled by irrelevant information. Our conclusions from this evaluation provide insights into the problem and directions for future research.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145671279",
                        "name": "Naoki Otani"
                    },
                    {
                        "authorId": "50007145",
                        "name": "J. Araki"
                    },
                    {
                        "authorId": "2109893608",
                        "name": "Hyeongsik Kim"
                    },
                    {
                        "authorId": "144547315",
                        "name": "E. Hovy"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Despite numerous bias mitigation approaches put forth (Cao and Daum\u00e9 III, 2020; Dinan et al., 2020a; Hube and Fetahu, 2019; Webster et al., 2018; Zhao et al., 2018), many have limited efficacy, failing to address the complexity of biased language (Stan\u0301czak and Augenstein, 2021; Blodgett et al.,\u2026",
                "Additionally, despite writing of four gender values (unknown, neutral, feminine, and masculine), the dataset and classifiers of Dinan et al. (2020b) are limited to \u201cmasculine and feminine classes\u201d (317).",
                "Despite numerous bias mitigation approaches put forth (Cao and Daum\u00e9 III, 2020; Dinan et al., 2020a; Hube and Fetahu, 2019; Webster et al., 2018; Zhao et al., 2018), many have limited efficacy, failing to address the complexity of biased language (Sta\u0144czak and Augenstein, 2021; Blodgett et al.",
                "\u2026bias work, for example, often uses a binary gender framework either in its conceptualization (such as Webster et al. (2018)) or application (such as Dinan et al. (2020b)), and tends to focus on one variety of gender bias, stereotypes (Stan\u0301czak and Augenstein, 2021; Doughman et al., 2021;\u2026",
                "Though Dinan et al. (2020b) also provide a framework for defining types of gender bias, their framework focuses on relationships between people in a conversation, identifying \u201cbias when speaking ABOUT someone, bias when speaking TO someone, and bias from speaking AS someone\u201d (316)."
            ],
            "citingPaper": {
                "paperId": "b4cd18f01b37c98e5e7652c23dece7e977215ef8",
                "externalIds": {
                    "ACL": "2022.gebnlp-1.4",
                    "DOI": "10.18653/v1/2022.gebnlp-1.4",
                    "CorpusId": 250390754
                },
                "corpusId": 250390754,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b4cd18f01b37c98e5e7652c23dece7e977215ef8",
                "title": "Uncertainty and Inclusivity in Gender Bias Annotation: An Annotation Taxonomy and Annotated Datasets of British English Text",
                "abstract": "Mitigating harms from gender biased language in Natural Language Processing (NLP) systems remains a challenge, and the situated nature of language means bias is inescapable in NLP data. Though efforts to mitigate gender bias in NLP are numerous, they often vaguely define gender and bias, only consider two genders, and do not incorporate uncertainty into models. To address these limitations, in this paper we present a taxonomy of gender biased language and apply it to create annotated datasets. We created the taxonomy and annotated data with the aim of making gender bias in language transparent. If biases are communicated clearly, varieties of biased language can be better identified and measured. Our taxonomy contains eleven types of gender biases inclusive of people whose gender expressions do not fit into the binary conceptions of woman and man, and whose gender differs from that they were assigned at birth, while also allowing annotators to document unknown gender information. The taxonomy and annotated data will, in future work, underpin analysis and more equitable language model development.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "115741036",
                        "name": "Lucy Havens"
                    },
                    {
                        "authorId": "2089542577",
                        "name": "B. Alex"
                    },
                    {
                        "authorId": "2090872775",
                        "name": "Benjamin Bach"
                    },
                    {
                        "authorId": "1749920",
                        "name": "Melissa Mhairi Terras"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026the natural language processing community, work has focused on combating gender bias in co-reference resolution (Zhao et al., 2018), dialogue (Dinan et al., 2019; Lee et al., 2019; Liu et al., 2020), detection of abusive language (Park et al., 2018), machine translation (Stanovsky et al.,\u2026",
                ", 2018), dialogue (Dinan et al., 2019; Lee et al., 2019; Liu et al., 2020), detection of abusive language (Park et al.",
                "Studies have indicated that the words used in biographies about women compared to biographies about men (Dinan et al., 2019) also differs, and is reflective of gendered terminology."
            ],
            "citingPaper": {
                "paperId": "4919cd4ad287a3f0679846bd95c6805cb8dda4bd",
                "externalIds": {
                    "ACL": "2022.acl-long.586",
                    "DBLP": "conf/acl/FanG22",
                    "DOI": "10.18653/v1/2022.acl-long.586",
                    "CorpusId": 248779872
                },
                "corpusId": 248779872,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/4919cd4ad287a3f0679846bd95c6805cb8dda4bd",
                "title": "Generating Biographies on Wikipedia: The Impact of Gender Bias on the Retrieval-Based Generation of Women Biographies",
                "abstract": "Generating factual, long-form text such as Wikipedia articles raises three key challenges: how to gather relevant evidence, how to structure information into well-formed text, and how to ensure that the generated text is factually correct. We address these by developing a model for English text that uses a retrieval mechanism to identify relevant supporting information on the web and a cache-based pre-trained encoder-decoder to generate long-form biographies section by section, including citation information. To assess the impact of available web evidence on the output text, we compare the performance of our approach when generating biographies about women (for which less information is available on the web) vs. biographies generally. To this end, we curate a dataset of 1,500 biographies about women. We analyze our generated text to understand how differences in available web evidence data affect generation. We evaluate the factuality, fluency, and quality of the generated texts using automatic metrics and human evaluation. We hope that these techniques can be used as a starting point for human writers, to aid in reducing the complexity inherent in the creation of long-form, factual text.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144270981",
                        "name": "Angela Fan"
                    },
                    {
                        "authorId": "2065132524",
                        "name": "Claire Gardent"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Dinan et al. (2020b) point out that there are three types of gender bias in chat bots: the first one being due to the gender of the person that speakers are talking about, the\nsecond being due to the gender of the speaker, and the last being due to the gender of the addressee.",
                "Similarly, Dinan et al. (2020a) propose to reduce gender bias via data augmentation, targeted data collection, and biascontrolled training."
            ],
            "citingPaper": {
                "paperId": "eb1ac44bbc0fe07c5f31f459c7199211239e90b8",
                "externalIds": {
                    "DBLP": "conf/acl-convai/KannEKDR22",
                    "ACL": "2022.nlp4convai-1.13",
                    "DOI": "10.18653/v1/2022.nlp4convai-1.13",
                    "CorpusId": 248780022
                },
                "corpusId": 248780022,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/eb1ac44bbc0fe07c5f31f459c7199211239e90b8",
                "title": "Open-domain Dialogue Generation: What We Can Do, Cannot Do, And Should Do Next",
                "abstract": "Human\u2013computer conversation has long been an interest of artificial intelligence and natural language processing research. Recent years have seen a dramatic improvement in quality for both task-oriented and open-domain dialogue systems, and an increasing amount of research in the area. The goal of this work is threefold: (1) to provide an overview of recent advances in the field of open-domain dialogue, (2) to summarize issues related to ethics, bias, and fairness that the field has identified as well as typical errors of dialogue systems, and (3) to outline important future challenges. We hope that this work will be of interest to both new and experienced researchers in the area.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3422953",
                        "name": "Katharina Kann"
                    },
                    {
                        "authorId": "146057134",
                        "name": "Abteen Ebrahimi"
                    },
                    {
                        "authorId": "137873505",
                        "name": "Joewie J. Koh"
                    },
                    {
                        "authorId": "2732135",
                        "name": "Shiran Dudy"
                    },
                    {
                        "authorId": "3077150",
                        "name": "A. Roncone"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026on generative models (Perez et al., 2022; Xu et al., 2021b; Kirk et al., 2021a; Sheng et al., 2021b; Nozza et al., 2021; Renduchintala et al., 2021; Dinan et al., 2020a,b), which are well known to pose unique challenges for automatic evaluation (Lowe et al., 2017; Howcroft et al., 2020;\u2026",
                "A popular set of techniques for measuring bias in generated text involves computing the frequency of different words on a word list, for example, those signifying gender (Dinan et al., 2020a); religion, race, gender, and orientation (Barikeri et al.",
                "A popular set of techniques for measuring bias in generated text involves computing the frequency of different words on a word list, for example, those signifying gender (Dinan et al., 2020a); religion, race, gender, and orientation (Barikeri et al., 2021); or occupations (Kirk et al., 2021b)."
            ],
            "citingPaper": {
                "paperId": "ab06a5c808bfd10680057b8b9899e669bcbc3e51",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-09209",
                    "DOI": "10.48550/arXiv.2205.09209",
                    "CorpusId": 248887683
                },
                "corpusId": 248887683,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ab06a5c808bfd10680057b8b9899e669bcbc3e51",
                "title": "\"I'm sorry to hear that\": finding bias in language models with a holistic descriptor dataset",
                "abstract": "As language models grow in popularity, their biases across all possible markers of demographic identity should be measured and ad-dressed in order to avoid perpetuating existing societal harms. Many datasets for measuring bias currently exist, but they are restricted in their coverage of demographic axes, and are commonly used with preset bias tests that pre-suppose which types of biases the models exhibit. In this work, we present a new, more inclusive dataset, H OLISTIC B IAS , which consists of nearly 600 descriptor terms across 13 different demographic axes. H OLISTIC B IAS was assembled in conversation with experts and community members with lived experience through a participatory process. We use these descriptors combinatorially in a set of bias measurement templates to produce over 450,000 unique sentence prompts, and we use these prompts to explore, identify, and reduce novel forms of bias in several generative models. We demonstrate that our dataset is highly ef\ufb01cacious for measuring previously unmea-surable biases in token likelihoods and generations from language models, as well as in an offensiveness classi\ufb01er. We will invite additions and amendments to the dataset, and we hope it will help serve as a basis for easy-to-use and more standardized methods for evaluating bias in NLP models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "51324296",
                        "name": "Eric Michael Smith"
                    },
                    {
                        "authorId": "2165660870",
                        "name": "Melissa Hall Melanie Kambadur"
                    },
                    {
                        "authorId": "6072807",
                        "name": "Eleonora Presani"
                    },
                    {
                        "authorId": "2110032535",
                        "name": "Adina Williams"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "There is existing and ongoing research on ways to mitigate these outputs (e.g. Xu et al., 2020; Dinan et al., 2019; Faal et al., 2022; Dinan et al., 2020), though Gonen and Goldberg (2019) argue that debiasing methods are insufficient and do not remove bias entirely.",
                "There is existing and ongoing research on ways to mitigate these outputs (e.g. Xu et al., 2020; Dinan et al., 2019; Faal et al., 2022; Dinan et al., 2020), though"
            ],
            "citingPaper": {
                "paperId": "31c7bfb0daf0c958d28ff1cd142ea00b26f1037a",
                "externalIds": {
                    "ACL": "2022.bea-1.28",
                    "DOI": "10.18653/v1/2022.bea-1.28",
                    "CorpusId": 250390612
                },
                "corpusId": 250390612,
                "publicationVenue": {
                    "id": "70cb7170-a1b1-440a-8aed-0d0140a013c6",
                    "name": "Workshop on Innovative Use of NLP for Building Educational Applications",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop Innov Use NLP Build Educ Appl",
                        "UNLPBEA",
                        "BEA"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/31c7bfb0daf0c958d28ff1cd142ea00b26f1037a",
                "title": "Towards an open-domain chatbot for language practice",
                "abstract": "State-of-the-art chatbots for English are now able to hold conversations on virtually any topic (e.g. Adiwardana et al., 2020; Roller et al., 2021). However, existing dialogue systems in the language learning domain still use hand-crafted rules and pattern matching, and are much more limited in scope. In this paper, we make an initial foray into adapting open-domain dialogue generation for second language learning. We propose and implement decoding strategies that can adjust the difficulty level of the chatbot according to the learner\u2019s needs, without requiring further training of the chatbot. These strategies are then evaluated using judgements from human examiners trained in language education. Our results show that re-ranking candidate outputs is a particularly effective strategy, and performance can be further improved by adding sub-token penalties and filtering.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2121302328",
                        "name": "Gladys Tyen"
                    },
                    {
                        "authorId": "40424713",
                        "name": "Mark Brenchley"
                    },
                    {
                        "authorId": "143726824",
                        "name": "Andrew Caines"
                    },
                    {
                        "authorId": "33490976",
                        "name": "P. Buttery"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026investigate through analysis of the models, their training regimes, and the data that they rely on (Hall Maudslay et al., 2019; Zhao et al., 2019; Dinan et al., 2020a,b; Vargas and Cotterell, 2020; Smith and Williams, 2021; Talat et al., 2021), rather than seeking to imbue models with a sense of\u2026"
            ],
            "citingPaper": {
                "paperId": "f19298ea19dcd7bd69ab76cf6a18f801052e26e4",
                "externalIds": {
                    "ACL": "2022.naacl-main.56",
                    "DBLP": "conf/naacl/TalatBVGCW22",
                    "DOI": "10.18653/v1/2022.naacl-main.56",
                    "CorpusId": 250390668
                },
                "corpusId": 250390668,
                "publicationVenue": {
                    "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
                    "name": "North American Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "North Am Chapter Assoc Comput Linguistics",
                        "NAACL"
                    ],
                    "url": "https://www.aclweb.org/portal/naacl"
                },
                "url": "https://www.semanticscholar.org/paper/f19298ea19dcd7bd69ab76cf6a18f801052e26e4",
                "title": "On the Machine Learning of Ethical Judgments from Natural Language",
                "abstract": "Ethics is one of the longest standing intellectual endeavors of humanity. In recent years, the fields of AI and NLP have attempted to address issues of harmful outcomes in machine learning systems that are made to interface with humans. One recent approach in this vein is the construction of NLP morality models that can take in arbitrary text and output a moral judgment about the situation described. In this work, we offer a critique of such NLP methods for automating ethical decision-making. Through an audit of recent work on computational approaches for predicting morality, we examine the broader issues that arise from such efforts. We conclude with a discussion of how machine ethics could usefully proceed in NLP, by focusing on current and near-future uses of technology, in a way that centers around transparency, democratic values, and allows for straightforward accountability.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2138053020",
                        "name": "Zeerak Talat"
                    },
                    {
                        "authorId": "122317863",
                        "name": "Hagen Blix"
                    },
                    {
                        "authorId": "51130686",
                        "name": "Josef Valvoda"
                    },
                    {
                        "authorId": "123036976",
                        "name": "M. I. Ganesh"
                    },
                    {
                        "authorId": "2070989574",
                        "name": "Ryan Cotterell"
                    },
                    {
                        "authorId": "81840293",
                        "name": "Adina Williams"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Consequently, these biased word embeddings have effects on downstream applications (Dinan et al., 2020; Blodgett et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "ecee71b8599818dedb46ce3e1245f1c94b6cbb82",
                "externalIds": {
                    "ACL": "2022.gebnlp-1.14",
                    "DOI": "10.18653/v1/2022.gebnlp-1.14",
                    "CorpusId": 250391071
                },
                "corpusId": 250391071,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ecee71b8599818dedb46ce3e1245f1c94b6cbb82",
                "title": "Unsupervised Mitigating Gender Bias by Character Components: A Case Study of Chinese Word Embedding",
                "abstract": "Word embeddings learned from massive text collections have demonstrated significant levels of discriminative biases.However, debias on the Chinese language, one of the most spoken languages, has been less explored.Meanwhile, existing literature relies on manually created supplementary data, which is time- and energy-consuming.In this work, we propose the first Chinese Gender-neutral word Embedding model (CGE) based on Word2vec, which learns gender-neutral word embeddings without any labeled data.Concretely, CGE utilizes and emphasizes the rich feminine and masculine information contained in radicals, i.e., a kind of component in Chinese characters, during the training procedure.This consequently alleviates discriminative gender biases.Experimental results on public benchmark datasets show that our unsupervised method outperforms the state-of-the-art supervised debiased word embedding models without sacrificing the functionality of the embedding model.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46772896",
                        "name": "Xiuying Chen"
                    },
                    {
                        "authorId": "2135609879",
                        "name": "Mingzhe Li"
                    },
                    {
                        "authorId": "2055863987",
                        "name": "Rui Yan"
                    },
                    {
                        "authorId": "2118502950",
                        "name": "Xin Gao"
                    },
                    {
                        "authorId": "2928371",
                        "name": "Xiangliang Zhang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "94428842fb8f9edf70b7700bd63cab80f69a6eb9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-06591",
                    "DOI": "10.48550/arXiv.2207.06591",
                    "CorpusId": 250526323
                },
                "corpusId": 250526323,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/94428842fb8f9edf70b7700bd63cab80f69a6eb9",
                "title": "A tool to overcome technical barriers for bias assessment in human language technologies",
                "abstract": "Automatic processing of language is be-coming pervasive in our lives, often taking central roles in our decision making, like choosing the wording for our messages and mails, translating our readings, or even having full conversations with us. Word embeddings are a key component of modern natural language processing systems. They provide a representation of words that has boosted the performance of many applications, working as a semblance of meaning. Word embeddings seem to capture a semblance of the meaning of words from raw text, but, at the same time, they also dis-till stereotypes and societal biases which are subsequently relayed to the \ufb01nal applications. Such biases can be discriminatory. audit these technologies.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2276687",
                        "name": "L. A. Alemany"
                    },
                    {
                        "authorId": "2066254822",
                        "name": "Luciana Benotti"
                    },
                    {
                        "authorId": "143956405",
                        "name": "Luc'ia Gonz'alez"
                    },
                    {
                        "authorId": "2150296733",
                        "name": "Jorge S'anchez"
                    },
                    {
                        "authorId": "2079934550",
                        "name": "Beatriz Busaniche"
                    },
                    {
                        "authorId": "2176182678",
                        "name": "Alexia Halvorsen"
                    },
                    {
                        "authorId": "2091620256",
                        "name": "M. Bordone"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "(Dinan et al., 2019) present an example of a bias measure that uses a crowdsourced dataset (LIGHT from (Urbanek et al.",
                "(Dinan et al., 2019) present an example of a bias measure that uses a crowdsourced dataset (LIGHT from (Urbanek et al., 2019)) to evaluate gender biases\u2014in this case, through the percentage of gendered words."
            ],
            "citingPaper": {
                "paperId": "7899377f3a23c9e073573efb6af368973c387844",
                "externalIds": {
                    "CorpusId": 251342439
                },
                "corpusId": 251342439,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7899377f3a23c9e073573efb6af368973c387844",
                "title": "Survey: Bias in NLP",
                "abstract": "In this paper, we present a comprehensive study of societal biases that result from the application of standard NLP tasks, focus-ing on how data and techniques contribute to biases and the progress towards detecting them. We look at different metrics for measurement of bias. We analyze how social biases with respect to different demographics get reflected on corpora of major entertainment source like movies and multilingual social media posts.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2166204873",
                        "name": "Niteesh Mallela"
                    },
                    {
                        "authorId": "145532184",
                        "name": "P. Bhattacharyya"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[151] Emily Dinan, Angela Fan, Adina Williams, Jack Urbanek, Douwe Kiela, and Jason Weston.",
                "There has been extensive work studying gender and racial biases in word embeddings [70, 90, 329, 397, 489, 706], contextual embeddings such as ELMo [467], BERT [149, 324, 354, 407], and GPT [209, 358, 481, 571], and generative models [151, 252, 358, 479, 521]."
            ],
            "citingPaper": {
                "paperId": "63f93a6d9c38d656933706acfc720684470bc108",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-03430",
                    "DOI": "10.48550/arXiv.2209.03430",
                    "CorpusId": 252118396
                },
                "corpusId": 252118396,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/63f93a6d9c38d656933706acfc720684470bc108",
                "title": "Foundations and Recent Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions",
                "abstract": "Multimodal machine learning is a vibrant multi-disciplinary research field that aims to design computer agents with intelligent capabilities such as understanding, reasoning, and learning through integrating multiple communicative modalities, including linguistic, acoustic, visual, tactile, and physiological messages. With the recent interest in video understanding, embodied autonomous agents, text-to-image generation, and multisensor fusion in application domains such as healthcare and robotics, multimodal machine learning has brought unique computational and theoretical challenges to the machine learning community given the heterogeneity of data sources and the interconnections often found between modalities. However, the breadth of progress in multimodal research has made it difficult to identify the common themes and open questions in the field. By synthesizing a broad range of application domains and theoretical frameworks from both historical and recent perspectives, this paper is designed to provide an overview of the computational and theoretical foundations of multimodal machine learning. We start by defining two key principles of modality heterogeneity and interconnections that have driven subsequent innovations, and propose a taxonomy of 6 core technical challenges: representation, alignment, reasoning, generation, transference, and quantification covering historical and recent trends. Recent technical achievements will be presented through the lens of this taxonomy, allowing researchers to understand the similarities and differences across new approaches. We end by motivating several open problems for future research as identified by our taxonomy.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "28130078",
                        "name": "P. Liang"
                    },
                    {
                        "authorId": "144802290",
                        "name": "Amir Zadeh"
                    },
                    {
                        "authorId": "49933077",
                        "name": "Louis-Philippe Morency"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Dinan et al. (2020); Liu et al. (2020a,b) discuss gender bias in dialogue generation and Sheng et al. (2021b) investigates the ad hominems in dia-\nlogue responses regarding the race perspective."
            ],
            "citingPaper": {
                "paperId": "035043d03a7867e4b1791e99ad535dea07733335",
                "externalIds": null,
                "corpusId": "252876625",
                "publicationVenue": null,
                "url": "http://www.semanticscholar.org/paper/035043d03a7867e4b1791e99ad535dea07733335",
                "title": "On Helpfulness Of Task-Oriented Dialogue Systems And Its Fairness Implication",
                "abstract": null,
                "year": 2022,
                "authors": []
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Its use case includes hate speech detection [11, 12], machine translation [35, 48] and dialogue generation [14].",
                "This method has proven to be successful within the field of natural language processing [11,12,14,35,36].",
                "Such an approach has proved promising for several use cases within the field of natural language processing [11, 12,14,35,36], recommendation systems [47] as well as visual question answering systems [41]."
            ],
            "citingPaper": {
                "paperId": "3040f6116564a7d33dc718a5a119690ed5d64ad8",
                "externalIds": {
                    "DBLP": "conf/eccv/CheongKG22",
                    "DOI": "10.1007/978-3-031-25072-9_16",
                    "CorpusId": 253528179
                },
                "corpusId": 253528179,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3040f6116564a7d33dc718a5a119690ed5d64ad8",
                "title": "Counterfactual Fairness for Facial Expression Recognition",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2006271645",
                        "name": "J. Cheong"
                    },
                    {
                        "authorId": "48382704",
                        "name": "S. Kalkan"
                    },
                    {
                        "authorId": "1781916",
                        "name": "H. Gunes"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Most prior work in bias mitigation has largely taken the \u201cone-size-fits-all\u201d approach, with most models being agnostic to the language of the speakers behind the language (Sun et al., 2019; Liang et al., 2020; Dinan et al., 2020; Garimella et al., 2021)."
            ],
            "citingPaper": {
                "paperId": "6c4c6f2e16a8b884ad1050d3a383a2b3eb83904d",
                "externalIds": {
                    "DBLP": "conf/ijcnlp/GarimellaMA22",
                    "ACL": "2022.aacl-short.38",
                    "CorpusId": 253762006
                },
                "corpusId": 253762006,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6c4c6f2e16a8b884ad1050d3a383a2b3eb83904d",
                "title": "Demographic-Aware Language Model Fine-tuning as a Bias Mitigation Technique",
                "abstract": "BERT-like language models (LMs), when exposed to large unstructured datasets, are known to learn and sometimes even amplify the biases present in such data. These biases generally reflect social stereotypes with respect to gender, race, age, and others. In this paper, we analyze the variations in gender and racial biases in BERT, a large pre-trained LM, when exposed to different demographic groups. Specifically, we investigate the effect of fine-tuning BERT on text authored by historically disadvantaged demographic groups in comparison to that by advantaged groups. We show that simply by fine-tuning BERT-like LMs on text authored by certain demographic groups can result in the mitigation of social biases in these LMs against various target groups.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "31099365",
                        "name": "Aparna Garimella"
                    },
                    {
                        "authorId": "2105984203",
                        "name": "Rada Mihalcea"
                    },
                    {
                        "authorId": "2121347719",
                        "name": "Akhash Amarnath"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "At the training level, Dinan et al. (2020) adapted the training process and applied bias controlled training to generative dialogue models to make them generate an equal number of gendered words for both genders considered."
            ],
            "citingPaper": {
                "paperId": "7f1cce61d8099be424a6f3b8036180562e5cef14",
                "externalIds": {
                    "ACL": "2022.nlpcss-1.6",
                    "DOI": "10.18653/v1/2022.nlpcss-1.6",
                    "CorpusId": 256461226
                },
                "corpusId": 256461226,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7f1cce61d8099be424a6f3b8036180562e5cef14",
                "title": "To Prefer or to Choose? Generating Agency and Power Counterfactuals Jointly for Gender Bias Mitigation",
                "abstract": "Gender bias may emerge from an unequal representation of agency and power, for example, by portraying women frequently as passive and powerless (\u201cShe accepted her future\u201d) and men as proactive and powerful (\u201cHe chose his future\u201d). When language models learn from respective texts, they may reproduce or even amplify the bias. An effective way to mitigate bias is to generate counterfactual sentences with opposite agency and power to the training. Recent work targeted agency-specific verbs from a lexicon to this end. We argue that this is insufficient, due to the interaction of agency and power and their dependence on context. In this paper, we thus develop a new rewriting model that identifies verbs with the desired agency and power in the context of the given sentence. The verbs\u2019 probability is then boosted to encourage the model to rewrite both connotations jointly. According to automatic metrics, our model effectively controls for power while being competitive in agency to the state of the art. In our main evaluation, human annotators favored its counterfactuals in terms of both connotations, also deeming its meaning preservation better.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2187454927",
                        "name": "Maja Stahl"
                    },
                    {
                        "authorId": "83854974",
                        "name": "Maximilian Splieth\u00f6ver"
                    },
                    {
                        "authorId": "2626599",
                        "name": "Henning Wachsmuth"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "\u2026text systems, we propose re-purposing methods for con-\ntrolling text generation (Ghazvininejad et al., 2017; Holtzman et al., 2018; Tambwekar et al., 2018; Keskar et al., 2019; Sahar et al., 2020) which are being used to control \u2018bias\u2019 in text generation (Sheng et al., 2020; Dinan et al., 2020).",
                "We highlight: input processing methods such as \u2018smart prompts\u2019 (Sheng et al., 2020), model conditioning methods such as in Tambwekar et al. (2018); Keskar et al. (2019); Dinan et al. (2020), and output processing methods such as \u2018guided decoding\u2019 (Ghazvininejad et al., 2017; Holtzman et al., 2018).",
                ", 2020) which are being used to control \u2018bias\u2019 in text generation (Sheng et al., 2020; Dinan et al., 2020).",
                "\u2026generation (Ghazvininejad et al., 2017; Holtzman et al., 2018; Tambwekar et al., 2018; Keskar et al., 2019; Sahar et al., 2020; Sheng et al., 2020; Dinan et al., 2020) and the intersection of causal inference and language (Tan et al., 2014; Gligoric\u0301 et al., 2019; Sridhar and Getoor, 2019;\u2026"
            ],
            "citingPaper": {
                "paperId": "bb6fc10dac56acae9ddeaac84f5e2194d46bf1d0",
                "externalIds": {
                    "ACL": "2021.hcinlp-1.17",
                    "CorpusId": 233364935
                },
                "corpusId": 233364935,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bb6fc10dac56acae9ddeaac84f5e2194d46bf1d0",
                "title": "RE-AIMing Predictive Text",
                "abstract": "Our increasing reliance on mobile applications means much of our communication is mediated with the support of predictive text systems. How do these systems impact interpersonal communication and broader society? In what ways are predictive text systems harmful, to whom, and why? In this paper, we focus on predictive text systems on mobile devices and attempt to answer these questions. We introduce the concept of a \u2018text entry intervention\u2019 as a way to evaluate predictive text systems through an interventional lens, and consider the Reach, Effectiveness, Adoption, Implementation, and Maintenance (RE-AIM) of predictive text systems. We finish with a discussion of opportunities for NLP.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2052503107",
                        "name": "Matthew Higgs"
                    },
                    {
                        "authorId": "152641011",
                        "name": "Claire H Mccallum"
                    },
                    {
                        "authorId": "144565644",
                        "name": "S. Sutton"
                    },
                    {
                        "authorId": "2056928747",
                        "name": "Mark Warner"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "Dixon et al. (2018) analyze biases in a toxicity classification model through the Wikipedia Talk Pages dataset as well as through a templated test set.",
                "Dixon et al. (2018) analyze biases in a toxicity classification model through the Wikipedia Talk Pages dataset as well as through a templated test set. Jigsaw (Jigsaw, 2019) contains comments from the Civil Comments platform labeled with six types of toxicity (e.g., toxic, obscene, etc) and identity attributes (e.g., white, woman, etc). Along with this dataset, Jigsaw (2019) present a bias evaluation following that of Borkan et al.",
                "Dinan et al. (2020) present an example of a bias measure that uses a crowdsourced",
                "Dinan et al. (2020) present an example of a bias measure that uses a crowdsourced\n4Sheng et al. (2021) has a more comprehensive survey that goes beyond bias measures.\ndataset (LIGHT from Urbanek et al. (2019)) to evaluate gender biases\u2014in this case, through the percentage of gendered words."
            ],
            "citingPaper": {
                "paperId": "bdd8530d72f51661f24247c9b5cd936ddb8e5c59",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-03362",
                    "CorpusId": 236956541
                },
                "corpusId": 236956541,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bdd8530d72f51661f24247c9b5cd936ddb8e5c59",
                "title": "What do Bias Measures Measure?",
                "abstract": "Natural Language Processing (NLP) models propagate social biases about protected attributes such as gender, race, and nationality. To create interventions and mitigate these biases and associated harms, it is vital to be able to detect and measure such biases. While many existing works propose bias evaluation methodologies for different tasks, there remains a need to cohesively understand what biases and normative harms each of these measures captures and how different measures compare. To address this gap, this work presents a comprehensive survey of existing bias measures in NLP as a function of the associated NLP tasks, metrics, datasets, and social biases and corresponding harms. This survey also organizes metrics into different categories to present advantages and disadvantages. Finally, we propose a documentation standard for bias measures to aid their development, categorization, and appropriate usage.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50991767",
                        "name": "Sunipa Dev"
                    },
                    {
                        "authorId": "23923796",
                        "name": "Emily Sheng"
                    },
                    {
                        "authorId": "2110117732",
                        "name": "Jieyu Zhao"
                    },
                    {
                        "authorId": "145478138",
                        "name": "Jiao Sun"
                    },
                    {
                        "authorId": "2118739951",
                        "name": "Yu Hou"
                    },
                    {
                        "authorId": "1997944417",
                        "name": "M. Sanseverino"
                    },
                    {
                        "authorId": "2125034176",
                        "name": "Jiin Kim"
                    },
                    {
                        "authorId": "3157053",
                        "name": "Nanyun Peng"
                    },
                    {
                        "authorId": "2782886",
                        "name": "Kai-Wei Chang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Additionally, gender bias concerns have been previously studied within the available LIGHT MTurk training set (Dinan et al., 2020), and we make use of that publicly available data here as well."
            ],
            "citingPaper": {
                "paperId": "b7a64a22f69a17ea0b007f7748580f7641b55cb6",
                "externalIds": {
                    "DBLP": "conf/acl/ShusterUDSW21",
                    "ACL": "2021.findings-acl.54",
                    "DOI": "10.18653/v1/2021.findings-acl.54",
                    "CorpusId": 236478290
                },
                "corpusId": 236478290,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/b7a64a22f69a17ea0b007f7748580f7641b55cb6",
                "title": "Dialogue in the Wild: Learning from a Deployed Role-Playing Game with Humans and Bots",
                "abstract": "Much of NLP research has focused on crowdsourced static datasets and the supervised learning paradigm of training once and then evaluating test performance. As argued in de Vries et al. (2020), crowdsourced data has the issues of lack of naturalness and relevance to real-world use cases, while the static dataset paradigm does not allow for a model to learn from its experiences of using language (Silver et al., 2013). In contrast, one might hope for machine learning systems that become more useful as they interact with people. In this work, we build and deploy a role-playing game, whereby human players converse with learning agents situated in an open-domain fantasy world. We show that by training models on the conversations they have with humans in the game the models progressively improve, as measured by automatic metrics and online engagement scores. This learning is shown to be more ef\ufb01cient than crowdsourced data when applied to conversations with real users, as well as being far cheaper to collect.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "35752280",
                        "name": "Kurt Shuster"
                    },
                    {
                        "authorId": "39219656",
                        "name": "Jack Urbanek"
                    },
                    {
                        "authorId": "31461304",
                        "name": "Emily Dinan"
                    },
                    {
                        "authorId": "3149531",
                        "name": "Arthur Szlam"
                    },
                    {
                        "authorId": "145183709",
                        "name": "J. Weston"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "\u2026language modeling, coreference resolution, natural language inference, machine translation, and sentiment analysis (Sheng et al., 2019; Rudinger et al., 2018; Lu et al., 2018; Dinan et al., 2019; Rudinger et al., 2017; Kiritchenko and Mohammad, 2018); Blodgett et al. (2020) provide a review."
            ],
            "citingPaper": {
                "paperId": "54db327cd53fe043449c9f242d3fc34c593a70ef",
                "externalIds": {
                    "ACL": "2021.findings-acl.355",
                    "DBLP": "conf/acl/SotnikovaCDR21",
                    "DOI": "10.18653/v1/2021.findings-acl.355",
                    "CorpusId": 236477764
                },
                "corpusId": 236477764,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/54db327cd53fe043449c9f242d3fc34c593a70ef",
                "title": "Analyzing Stereotypes in Generative Text Inference Tasks",
                "abstract": "Stereotypes are inferences drawn about people based on their demographic attributes, which may result in harms to users when a system is deployed. In generative language-inference tasks, given a premise, a model produces plausible hypotheses that follow either logically (natural language inference) or common-sensically (commonsense inference). Such tasks are therefore a fruitful setting in which to explore the degree to which NLP systems encode stereotypes. In our work, we study how stereotypes manifest when the potential targets of stereotypes are situated in real-life, neutral contexts. We collect human judgments on the presence of stereotypes in generated inferences, and compare how perceptions of stereotypes vary due to annotator positionality",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2172271065",
                        "name": "Anna Sotnikova"
                    },
                    {
                        "authorId": "48696491",
                        "name": "Yang Trista Cao"
                    },
                    {
                        "authorId": "1722360",
                        "name": "Hal Daum\u00e9"
                    },
                    {
                        "authorId": "2034613",
                        "name": "Rachel Rudinger"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "34350431963ca25f215ec5bf5d624a10f74678e9",
                "externalIds": {
                    "DBLP": "conf/sigdial/SeeM21",
                    "ACL": "2021.sigdial-1.1",
                    "CorpusId": 236777459
                },
                "corpusId": 236777459,
                "publicationVenue": {
                    "id": "6a470734-72c6-4809-a07d-d34dee0df4a1",
                    "name": "SIGDIAL Conferences",
                    "type": "conference",
                    "alternate_names": [
                        "SIGDIAL",
                        "SIGDIAL Conf",
                        "Annu Meet Sp\u00e9c Interest Group Discourse Dialogue",
                        "Annual Meeting of the Special Interest Group on Discourse and Dialogue"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/34350431963ca25f215ec5bf5d624a10f74678e9",
                "title": "Understanding and predicting user dissatisfaction in a neural generative chatbot",
                "abstract": "Neural generative dialogue agents have shown an increasing ability to hold short chitchat conversations, when evaluated by crowdworkers in controlled settings. However, their performance in real-life deployment \u2013 talking to intrinsically-motivated users in noisy environments \u2013 is less well-explored. In this paper, we perform a detailed case study of a neural generative model deployed as part of Chirpy Cardinal, an Alexa Prize socialbot. We find that unclear user utterances are a major source of generative errors such as ignoring, hallucination, unclearness and repetition. However, even in unambiguous contexts the model frequently makes reasoning errors. Though users express dissatisfaction in correlation with these errors, certain dissatisfaction types (such as offensiveness and privacy objections) depend on additional factors \u2013 such as the user\u2019s personal attitudes, and prior unaddressed dissatisfaction in the conversation. Finally, we show that dissatisfied user utterances can be used as a semi-supervised learning signal to improve the dialogue system. We train a model to predict next-turn dissatisfaction, and show through human evaluation that as a ranking function, it selects higher-quality neural-generated utterances.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "13070498",
                        "name": "A. See"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Moreover, they are often vulnerable to biases, privacy violations, adversarial attacks, and safety concerns [1,10,14,21]."
            ],
            "citingPaper": {
                "paperId": "3e13b33b04bb33cb330a6ba73674bf4c562d4fd5",
                "externalIds": {
                    "DBLP": "conf/aiia/FazzingaGT21",
                    "CorpusId": 244531972
                },
                "corpusId": 244531972,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3e13b33b04bb33cb330a6ba73674bf4c562d4fd5",
                "title": "A Preliminary Evaluation of a Privacy-Preserving Dialogue System",
                "abstract": "Dialogue systems are AI applications widely used in many contexts requiring user interaction. However, unconstrained interaction may lead to users communicating sensitive data. This raises concerns about how these systems handle personal data, and about their compliance with relevant laws, regulations, and ethical principles. We propose to integrate advanced natural language processing techniques in a dialogue system architecture based on computational argumentation, ensuring that user data are ethically managed and regulations are respected. A preliminary experimental evaluation of our proposal over a COVID-19 vaccine information case study shows promising results. Copyright \u00a9 2021 Copyright for this paper by its authors.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1726971",
                        "name": "Bettina Fazzinga"
                    },
                    {
                        "authorId": "143978279",
                        "name": "Andrea Galassi"
                    },
                    {
                        "authorId": "2896208",
                        "name": "Paolo Torroni"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Serious problems of representational bias, often related directly to undersampling of distinct geographic regions and/or demographic groups, have been uncovered in gold-standard genomic databases used throughout the biomedical sciences [40]\u2013[42], in speech and facial/gesture recognition services [43], [44], in pop-"
            ],
            "citingPaper": {
                "paperId": "528e08db5e05523ce50b861c00bb494524197a27",
                "externalIds": {
                    "DBLP": "journals/access/WesleyM21",
                    "DOI": "10.1109/ACCESS.2021.3096034",
                    "CorpusId": 236185341
                },
                "corpusId": 236185341,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/528e08db5e05523ce50b861c00bb494524197a27",
                "title": "Methods for Measuring Geodiversity in Large Overhead Imagery Datasets",
                "abstract": "Geographic variation in the appearance of objects on Earth is readily observable in remotely sensed imagery (RSI) and somewhat intuitive to understand for most people \u2013 many classes of objects (houses, vehicles, crop fields etc.) simply look different depending on their location. This variation has recently been shown to have important implications when training machine learning models on geotagged image datasets for specific object detection and classification tasks. For example, models trained on datasets with ethnocentric biases in image content have been shown to misclassify objects in under-sampled regions, particularly in least-developed countries. The need to evaluate the growing corpus of RSI datasets for representativeness, heterogeneity and geodiversity is therefore high; yet scalable methods for measuring these concepts are absent in the remote sensing domain. This paper introduces the first dataset analysis methods for detecting and assessing geodiversity problems in large RSI datasets, based on geospatial adaptations of the Fr\u00e9chet Inception Distance and Inception Score in the deep learning framework. Geospatial Fr\u00e9chet Distance is proposed as a dissimilarity measure for image features of an object class across geographic regions \u2013 useful for comparing differences in object class appearance in different locations and/or spatial scales. A complementary Geospatial Inception Score is proposed to quantify heterogeneity of geographic context present in dataset labels within particular regions/locations, taking into account the labels themselves as well as their immediate surroundings. Rigorous tests of these methods on simulated RSI datasets demonstrate their stability, sensitivity, and the broad range of dataset analyses to which they can be applied.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "121741064",
                        "name": "A. Wesley"
                    },
                    {
                        "authorId": "2000107",
                        "name": "T. Matisziw"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                ", 2019), but also gender identities (Font and Costa-juss\u00e0, 2019; Dinan et al., 2019; Dinan et al., 2020).",
                ", 2019), dialogue systems (Dinan et al., 2019), and language modeling (Lu et al.",
                "\u2026et al., 2018; Font and Costa-juss\u00e0, 2019; Moryossef et al., 2019; Stanovsky et al., 2019; Stafanovic\u030cs et al., 2020; Gonen and Webster, 2020), named entity recognition (Mehrabi et al., 2019), dialogue systems (Dinan et al., 2019), and language modeling (Lu et al., 2018; Bordia and Bowman, 2019).",
                "\u2026at a great pace and raised expectation about the quality of results and especially their impact in a social context, including not only race (Merullo et al., 2019) and politics (Fan et al., 2019), but also gender identities (Font and Costa-juss\u00e0, 2019; Dinan et al., 2019; Dinan et al., 2020)."
            ],
            "citingPaper": {
                "paperId": "59aad68ed92cab0bd1c7282b4f3ff6adc12c48e5",
                "externalIds": {
                    "ACL": "2020.gebnlp-1.12",
                    "CorpusId": 228076553
                },
                "corpusId": 228076553,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/59aad68ed92cab0bd1c7282b4f3ff6adc12c48e5",
                "title": "Gender-Aware Reinflection using Linguistically Enhanced Neural Models",
                "abstract": "In this paper, we present an approach for sentence-level gender reinflection using linguistically enhanced sequence-to-sequence models. Our system takes an Arabic sentence and a given target gender as input and generates a gender-reinflected sentence based on the target gender. We formulate the problem as a user-aware grammatical error correction task and build an encoderdecoder architecture to jointly model reinflection for both masculine and feminine grammatical genders. We also show that adding linguistic features to our model leads to better reinflection results. The results on a blind test set using our best system show improvements over previous work, with a 3.6% absolute increase in M2 F0.5. Bias Statement Most NLP systems are unaware of their users\u2019 preferred grammatical gender. Such systems typically generate a single output for a specific input without considering any user information. Beyond being simply incorrect in many cases, such output patterns create representational harm by propagating social biases and inequalities of the world we live in. While such biases can be traced back to the NLP systems\u2019 training data, balancing and cleaning the training data will not guarantee the correctness of a single output that is arrived at without accounting for user preferences. Our view is that NLP systems should utilize grammatical gender preference information to provide the correct user-aware output, particularly for gender-marking morphologically rich languages. When the grammatical gender preference information is unavailable to the systems, all gender-specific outputs should be generated and properly marked. We acknowledge that by limiting the choice of gender expression to the grammatical gender choices in Arabic, we exclude other alternatives such as non-binary gender or no-gender expressions. We are not aware of any sociolinguistics published research that discusses such alternatives for Arabic, although there are growing grassroots efforts, e.g., the Ebdal Project.1",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "66589548",
                        "name": "Bashar Alhafni"
                    },
                    {
                        "authorId": "1696645",
                        "name": "Nizar Habash"
                    },
                    {
                        "authorId": "2063374",
                        "name": "Houda Bouamor"
                    }
                ]
            }
        }
    ]
}