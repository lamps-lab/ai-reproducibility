{
    "offset": 0,
    "data": [
        {
            "isInfluential": false,
            "contexts": [
                "More broadly, testing recourse infeasibility is valuable for eliciting individual preferences over recourse actions [80, 85, 84], measuring the effort required to obtain a target prediction [32, 27], building classifiers that incentivize improvement [70, 46, 47, 2, 31], or preventing strategic manipulation [28, 17, 51, 57, 56, 22, 10, 30]."
            ],
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "18a65d3136df8429b63e9a6352151d4d7374d8eb",
                "externalIds": {
                    "ArXiv": "2308.12820",
                    "DBLP": "journals/corr/abs-2308-12820",
                    "DOI": "10.48550/arXiv.2308.12820",
                    "CorpusId": 261100891
                },
                "corpusId": 261100891,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/18a65d3136df8429b63e9a6352151d4d7374d8eb",
                "title": "Prediction without Preclusion: Recourse Verification with Reachable Sets",
                "abstract": "Machine learning models are often used to decide who will receive a loan, a job interview, or a public benefit. Standard techniques to build these models use features about people but overlook their actionability. In turn, models can assign predictions that are fixed, meaning that consumers who are denied loans, interviews, or benefits may be permanently locked out from access to credit, employment, or assistance. In this work, we introduce a formal testing procedure to flag models that assign fixed predictions that we call recourse verification. We develop machinery to reliably determine if a given model can provide recourse to its decision subjects from a set of user-specified actionability constraints. We demonstrate how our tools can ensure recourse and adversarial robustness in real-world datasets and use them to study the infeasibility of recourse in real-world lending datasets. Our results highlight how models can inadvertently assign fixed predictions that permanently bar access, and we provide tools to design algorithms that account for actionability when developing models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2233292052",
                        "name": "Avni Kothari"
                    },
                    {
                        "authorId": "21228890",
                        "name": "B. Kulynych"
                    },
                    {
                        "authorId": "27836724",
                        "name": "Tsui-Wei Weng"
                    },
                    {
                        "authorId": "3072590",
                        "name": "Berk Ustun"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [],
            "intents": [],
            "citingPaper": {
                "paperId": "db73bb8100d5fda9a7cebd3adc64f3b0e3d2802a",
                "externalIds": {
                    "DBLP": "conf/ijcai/EstornellCD0V23",
                    "DOI": "10.24963/ijcai.2023/45",
                    "CorpusId": 259923069
                },
                "corpusId": 259923069,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/db73bb8100d5fda9a7cebd3adc64f3b0e3d2802a",
                "title": "Incentivizing Recourse through Auditing in Strategic Classification",
                "abstract": "The increasing automation of high-stakes decisions with direct impact on the lives and well-being of individuals raises a number of important considerations. Prominent among these is strategic behavior by individuals hoping to achieve a more desirable outcome. Two forms of such behavior are commonly studied: 1) misreporting of individual attributes, and 2) recourse, or actions that truly change such attributes. The former involves deception, and is inherently undesirable, whereas the latter may well be a desirable goal insofar as it changes true individual qualification. We study misreporting and recourse as strategic choices by individuals within a unified framework. In particular, we propose auditing as a means to incentivize recourse actions over attribute manipulation, and characterize optimal audit policies for two types of principals, utility-maximizing and recourse-maximizing. Additionally, we consider subsidies as an incentive for recourse over manipulation, and show that even a utility-maximizing principal would be willing to devote a considerable amount of audit budget to providing such subsidies. Finally, we consider the problem of optimizing fines for failed audits, and bound the total cost incurred by the population as a result of audits.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "103056375",
                        "name": "Andrew Estornell"
                    },
                    {
                        "authorId": "49070286",
                        "name": "Yatong Chen"
                    },
                    {
                        "authorId": "40583483",
                        "name": "Sanmay Das"
                    },
                    {
                        "authorId": "40013516",
                        "name": "Yang Liu"
                    },
                    {
                        "authorId": "1699600",
                        "name": "Yevgeniy Vorobeychik"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [
                "However, recent work considers the issue of training classifiers which are robust to strategic manipulation for more general cost functions (Levanon and Rosenfeld 2021)."
            ],
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "57a5c41ac894db338aa1a8fe9c6476fac9b5bbce",
                "externalIds": {
                    "DBLP": "conf/aaai/Vorobeychik23",
                    "DOI": "10.1609/aaai.v37i13.26796",
                    "CorpusId": 259748557
                },
                "corpusId": 259748557,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/57a5c41ac894db338aa1a8fe9c6476fac9b5bbce",
                "title": "The Many Faces of Adversarial Machine Learning",
                "abstract": "Adversarial machine learning (AML) research is concerned with robustness of machine learning models and algorithms to malicious tampering. Originating at the intersection between machine learning and cybersecurity, AML has come to have broader research appeal, stretching traditional notions of security to include applications of computer vision, natural language processing, and network science. In addition, the problems of strategic classification, algorithmic recourse, and counterfactual explanations have essentially the same core mathematical structure as AML, despite distinct motivations. I give a simplified overview of the central problems in AML, and then discuss both the security-motivated AML domains, and the problems above unrelated to security. These together span a number of important AI subdisciplines, but can all broadly be viewed as concerned with trustworthy AI. My goal is to clarify both the technical connections among these, as well as the substantive differences, suggesting directions for future research.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1699600",
                        "name": "Yevgeniy Vorobeychik"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [
                "2 Related work Strategic responses to algorithmic decision-making There is a growing line of work at the intersection of economics and computation on algorithmic decision-making with incentives, under the umbrella of strategic classification or strategic learning [21, 16, 15, 32, 44, 3, 8, 10, 22, 25, 24, 20, 30, 34, 35, 23, 17, 27]."
            ],
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ff9505c277b25dde10f0fae73449d1ebb9265cba",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-06250",
                    "ArXiv": "2306.06250",
                    "DOI": "10.48550/arXiv.2306.06250",
                    "CorpusId": 259137459
                },
                "corpusId": 259137459,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ff9505c277b25dde10f0fae73449d1ebb9265cba",
                "title": "Strategic Apple Tasting",
                "abstract": "Algorithmic decision-making in high-stakes domains often involves assigning decisions to agents with incentives to strategically modify their input to the algorithm. In addition to dealing with incentives, in many domains of interest (e.g. lending and hiring) the decision-maker only observes feedback regarding their policy for rounds in which they assign a positive decision to the agent; this type of feedback is often referred to as apple tasting (or one-sided) feedback. We formalize this setting as an online learning problem with apple-tasting feedback where a principal makes decisions about a sequence of $T$ agents, each of which is represented by a context that may be strategically modified. Our goal is to achieve sublinear strategic regret, which compares the performance of the principal to that of the best fixed policy in hindsight, if the agents were truthful when revealing their contexts. Our main result is a learning algorithm which incurs $\\tilde{\\mathcal{O}}(\\sqrt{T})$ strategic regret when the sequence of agents is chosen stochastically. We also give an algorithm capable of handling adversarially-chosen agents, albeit at the cost of $\\tilde{\\mathcal{O}}(T^{(d+1)/(d+2)})$ strategic regret (where $d$ is the dimension of the context). Our algorithms can be easily adapted to the setting where the principal receives bandit feedback -- this setting generalizes both the linear contextual bandit problem (by considering agents with incentives) and the strategic classification problem (by allowing for partial feedback).",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1570730776",
                        "name": "Keegan Harris"
                    },
                    {
                        "authorId": "27983734",
                        "name": "Chara Podimata"
                    },
                    {
                        "authorId": "1768074",
                        "name": "Zhiwei Steven Wu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [],
            "intents": [],
            "citingPaper": {
                "paperId": "4fd10eb6f8effb73eda1ead2168d4f1cdc470a60",
                "externalIds": {
                    "ArXiv": "2306.06305",
                    "CorpusId": 259138532
                },
                "corpusId": 259138532,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4fd10eb6f8effb73eda1ead2168d4f1cdc470a60",
                "title": "A Central Limit Theorem for Stochastic Saddle Point Optimization",
                "abstract": "In this work, we study the Uncertainty Quantification (UQ) of an algorithmic estimator of the saddle point of a strongly-convex strongly-concave objective. Specifically, we show that the averaged iterates of a Stochastic Extra-Gradient (SEG) method for a Saddle Point Problem (SPP) converges almost surely to the saddle point and follows a Central Limit Theorem (CLT) with optimal covariance under two different noise settings, namely the martingale-difference noise and the state-dependent Markov noise. To ensure the stability of the algorithm dynamics under the state-dependent Markov noise, we propose a variant of SEG with truncated varying sets. Our work opens the door for online inference of SPP with numerous potential applications in GAN, robust optimization, and reinforcement learning to name a few. We illustrate our results through numerical experiments.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2154707859",
                        "name": "Abhishek Roy"
                    },
                    {
                        "authorId": "102791014",
                        "name": "Yian Ma"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [
                "Strategic Classification Strategic classification considers the effect of deploying a classifier in an environment with strategic players, who want to change their features in order to influence how they are classified (Hardt et al., 2016; Levanon and Rosenfeld, 2021; Miller et al., 2020; Tsirtsis et al., 2019; Yatong Chen, 2021).",
                "\u2026classification considers the effect of deploying a classifier in an environment with strategic players, who want to change their features in order to influence how they are classified (Hardt et al., 2016; Levanon and Rosenfeld, 2021; Miller et al., 2020; Tsirtsis et al., 2019; Yatong Chen, 2021)."
            ],
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "a69f15c4659b8c69b9b3446c3cc45e27d3e10251",
                "externalIds": {
                    "ArXiv": "2306.00497",
                    "DBLP": "journals/corr/abs-2306-00497",
                    "DOI": "10.48550/arXiv.2306.00497",
                    "CorpusId": 258999503
                },
                "corpusId": 258999503,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a69f15c4659b8c69b9b3446c3cc45e27d3e10251",
                "title": "The Risks of Recourse in Binary Classification",
                "abstract": "Algorithmic recourse provides explanations that help users overturn an unfavorable decision by a machine learning system. But so far very little attention has been paid to whether providing recourse is beneficial or not. We introduce an abstract learning-theoretic framework that compares the risks (i.e. expected losses) for classification with and without algorithmic recourse. This allows us to answer the question of when providing recourse is beneficial or harmful at the population level. Surprisingly, we find that there are many plausible scenarios in which providing recourse turns out to be harmful, because it pushes users to regions of higher class uncertainty and therefore leads to more mistakes. We further study whether the party deploying the classifier has an incentive to strategize in anticipation of having to provide recourse, and we find that sometimes they do, to the detriment of their users. Providing algorithmic recourse may therefore also be harmful at the systemic level. We confirm our theoretical findings in experiments on simulated and real-world data. All in all, we conclude that the current concept of algorithmic recourse is not reliably beneficial, and therefore requires rethinking.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "11280476",
                        "name": "H. Fokkema"
                    },
                    {
                        "authorId": "29817320",
                        "name": "D. Garreau"
                    },
                    {
                        "authorId": "1775951",
                        "name": "T. Erven"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [
                "Strategic classification [11, 6, 20, 42], as well as other problems studying strategic agent behavior, frequently use models of agent behavior in order to compute Stackelberg equilibria, which are direct analogues of performative optima.",
                "Such models include best-response models for strategic classification [11, 14, 20, 10], rational-agent models in economics [32, 40], and parametric distribution shifts [12, 25, 13], among others."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "d5b03f29bd8b65b2ccbe3949c78e26b12f6b6af8",
                "externalIds": {
                    "ArXiv": "2305.18728",
                    "DBLP": "journals/corr/abs-2305-18728",
                    "DOI": "10.48550/arXiv.2305.18728",
                    "CorpusId": 258967936
                },
                "corpusId": 258967936,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d5b03f29bd8b65b2ccbe3949c78e26b12f6b6af8",
                "title": "Plug-in Performative Optimization",
                "abstract": "When predictions are performative, the choice of which predictor to deploy influences the distribution of future observations. The overarching goal in learning under performativity is to find a predictor that has low \\emph{performative risk}, that is, good performance on its induced distribution. One family of solutions for optimizing the performative risk, including bandits and other derivative-free methods, is agnostic to any structure in the performative feedback, leading to exceedingly slow convergence rates. A complementary family of solutions makes use of explicit \\emph{models} for the feedback, such as best-response models in strategic classification, enabling significantly faster rates. However, these rates critically rely on the feedback model being well-specified. In this work we initiate a study of the use of possibly \\emph{misspecified} models in performative prediction. We study a general protocol for making use of models, called \\emph{plug-in performative optimization}, and prove bounds on its excess risk. We show that plug-in performative optimization can be far more efficient than model-agnostic strategies, as long as the misspecification is not too extreme. Altogether, our results support the hypothesis that models--even if misspecified--can indeed help with learning in performative settings.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1469395222",
                        "name": "Licong Lin"
                    },
                    {
                        "authorId": "7830023",
                        "name": "Tijana Zrnic"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [
                "outcome and true outcome \u2013 however, may lead to improvement [11, 12, 21, 60, 66, 78]."
            ],
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "1f903d37b186c9d6b9ffd5d75a9524f9a85375ec",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-09035",
                    "ArXiv": "2305.09035",
                    "DOI": "10.48550/arXiv.2305.09035",
                    "CorpusId": 258714783
                },
                "corpusId": 258714783,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1f903d37b186c9d6b9ffd5d75a9524f9a85375ec",
                "title": "Algorithmic Censoring in Dynamic Learning Systems",
                "abstract": "Dynamic learning systems subject to selective labeling exhibit censoring, i.e. persistent negative predictions assigned to one or more subgroups of points. In applications like consumer finance, this results in groups of applicants that are persistently denied and thus never enter into the training data. In this work, we formalize censoring, demonstrate how it can arise, and highlight difficulties in detection. We consider safeguards against censoring - recourse and randomized-exploration - both of which ensure we collect labels for points that would otherwise go unobserved. The resulting techniques allow examples from censored groups to enter into the training data and correct the model. Our results highlight the otherwise unmeasured harms of censoring and demonstrate the effectiveness of mitigation strategies across a range of data generating processes.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "118088079",
                        "name": "Jennifer Chien"
                    },
                    {
                        "authorId": "2464550",
                        "name": "Margaret E. Roberts"
                    },
                    {
                        "authorId": "3072590",
                        "name": "Berk Ustun"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [],
            "intents": [],
            "citingPaper": {
                "paperId": "e868235f56c540358cd0d26cda84fbf7ca486bc5",
                "externalIds": {
                    "DBLP": "conf/sigecom/AhmadiBY23",
                    "ArXiv": "2302.12355",
                    "DOI": "10.1145/3580507.3597818",
                    "CorpusId": 257206161
                },
                "corpusId": 257206161,
                "publicationVenue": {
                    "id": "3c2dd17d-6c11-4bb6-9d01-1fc2064cb8df",
                    "name": "ACM Conference on Economics and Computation",
                    "type": "conference",
                    "alternate_names": [
                        "Economics and Computation",
                        "Electron Commer",
                        "EC",
                        "ACM Conf Econ Comput",
                        "Econ Comput",
                        "Electronic Commerce"
                    ],
                    "url": "http://www.acm.org/sigecom/"
                },
                "url": "https://www.semanticscholar.org/paper/e868235f56c540358cd0d26cda84fbf7ca486bc5",
                "title": "Fundamental Bounds on Online Strategic Classification",
                "abstract": "We study the problem of online binary classification where strategic agents can manipulate their observable features in predefined ways, modeled by a manipulation graph, in order to receive a positive classification. We show this setting differs in fundamental ways from classic (non-strategic) online classification. For instance, whereas in the non-strategic case, a mistake bound of ln |H| is achievable via the halving algorithm when the target function belongs to a known class H, we show that no deterministic algorithm can achieve a mistake bound o(\u0394) in the strategic setting, where \u0394 is the maximum degree of the manipulation graph (even when |H| = O(\u0394)). We complement this with a general algorithm achieving mistake bound O(\u0394 ln |H|). We also extend this to the agnostic setting, and show that this algorithm achieves a \u0394 multiplicative regret (mistake bound of O(\u0394 \u00b7 OPT + \u0394 \u00b7 ln |H|)), and that no deterministic algorithm can achieve o(\u0394) multiplicative regret. Next, we study two randomized models based on whether the random choices are made before or after agents respond, and show they exhibit fundamental differences. In the first, fractional model, at each round the learner deterministically chooses a probability distribution over classifiers inducing expected values on each vertex (probabilities of being classified as positive), which the strategic agents respond to. We show that any learner in this model has to suffer linear regret. On the other hand, in the second randomized algorithms model, while the adversary who selects the next agent must respond to the learner's probability distribution over classifiers, the agent then responds to the actual hypothesis classifier drawn from this distribution. Surprisingly, we show this model is more advantageous to the learner, and we design randomized algorithms that achieve sublinear regret bounds against both oblivious and adaptive adversaries.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2120590362",
                        "name": "Saba Ahmadi"
                    },
                    {
                        "authorId": "1690967",
                        "name": "Avrim Blum"
                    },
                    {
                        "authorId": "150180166",
                        "name": "Kunhe Yang"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [],
            "intents": [],
            "citingPaper": {
                "paperId": "a3f6787cf472be34f71908dd5ab69d75a52babe3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-06559",
                    "ArXiv": "2302.06559",
                    "DOI": "10.48550/arXiv.2302.06559",
                    "CorpusId": 256827002
                },
                "corpusId": 256827002,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a3f6787cf472be34f71908dd5ab69d75a52babe3",
                "title": "Recommending to Strategic Users",
                "abstract": "Recommendation systems are pervasive in the digital economy. An important assumption in many deployed systems is that user consumption reflects user preferences in a static sense: users consume the content they like with no other considerations in mind. However, as we document in a large-scale online survey, users do choose content strategically to influence the types of content they get recommended in the future. We model this user behavior as a two-stage noisy signalling game between the recommendation system and users: the recommendation system initially commits to a recommendation policy, presents content to the users during a cold start phase which the users choose to strategically consume in order to affect the types of content they will be recommended in a recommendation phase. We show that in equilibrium, users engage in behaviors that accentuate their differences to users of different preference profiles. In addition, (statistical) minorities out of fear of losing their minority content exposition may not consume content that is liked by mainstream users. We next propose three interventions that may improve recommendation quality (both on average and for minorities) when taking into account strategic consumption: (1) Adopting a recommendation system policy that uses preferences from a prior, (2) Communicating to users that universally liked (\"mainstream\") content will not be used as basis of recommendation, and (3) Serving content that is personalized-enough yet expected to be liked in the beginning. Finally, we describe a methodology to inform applied theory modeling with survey results.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2059769656",
                        "name": "A. Haupt"
                    },
                    {
                        "authorId": "1397904824",
                        "name": "Dylan Hadfield-Menell"
                    },
                    {
                        "authorId": "27983734",
                        "name": "Chara Podimata"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [
                "Efficient learning algorithms have been proposed for the original batch setting (Levanon & Rosenfeld, 2021; 2022), as well as online formulations (Chen et al., 2020; Ahmadi et al., 2021).",
                "We now turn to experiments based on real data using two public datasets: (i) spam, used originally in Hardt et al. (2016), and (ii) card fraud, used in Levanon & Rosenfeld (2021).",
                "This has made it the target of much recent interest (Dong et al., 2018; Miller et al., 2020; Tsirtsis & Gomez Rodriguez, 2020; Jagadeesan et al., 2021; Ghalme et al., 2021; Zrnic et al., 2021; Levanon & Rosenfeld, 2021; 2022; Estornell\n1Technion \u2013 Israel Institute of Technology, Haifa, Israel."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "3a2daffdb7ccaf8cfe7b6c292b20a9756c0c2b1e",
                "externalIds": {
                    "DBLP": "conf/icml/HorowitzR23",
                    "ArXiv": "2302.06280",
                    "DOI": "10.48550/arXiv.2302.06280",
                    "CorpusId": 256826873
                },
                "corpusId": 256826873,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/3a2daffdb7ccaf8cfe7b6c292b20a9756c0c2b1e",
                "title": "Causal Strategic Classification: A Tale of Two Shifts",
                "abstract": "When users can benefit from certain predictive outcomes, they may be prone to act to achieve those outcome, e.g., by strategically modifying their features. The goal in strategic classification is therefore to train predictive models that are robust to such behavior. However, the conventional framework assumes that changing features does not change actual outcomes, which depicts users as\"gaming\"the system. Here we remove this assumption, and study learning in a causal strategic setting where true outcomes do change. Focusing on accuracy as our primary objective, we show how strategic behavior and causal effects underlie two complementing forms of distribution shift. We characterize these shifts, and propose a learning algorithm that balances between these two forces and over time, and permits end-to-end training. Experiments on synthetic and semi-synthetic data demonstrate the utility of our approach.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2106233279",
                        "name": "G. Horowitz"
                    },
                    {
                        "authorId": "3138200",
                        "name": "Nir Rosenfeld"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [
                "Regularization has been used to control incentives in Rosenfeld et al. (2020); Levanon & Rosenfeld (2021), but in distinct settings and towards different goals (i.e., unrelated to recommendation or diversity), and for user responses that fully decompose."
            ],
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4130c9480e80481403d536d88489850bb2c7dec3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-04336",
                    "ArXiv": "2302.04336",
                    "DOI": "10.48550/arXiv.2302.04336",
                    "CorpusId": 256697126
                },
                "corpusId": 256697126,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/4130c9480e80481403d536d88489850bb2c7dec3",
                "title": "Performative Recommendation: Diversifying Content via Strategic Incentives",
                "abstract": "The primary goal in recommendation is to suggest relevant content to users, but optimizing for accuracy often results in recommendations that lack diversity. To remedy this, conventional approaches such as re-ranking improve diversity by presenting more diverse items. Here we argue that to promote inherent and prolonged diversity, the system must encourage its creation. Towards this, we harness the performative nature of recommendation, and show how learning can incentivize strategic content creators to create diverse content. Our approach relies on a novel form of regularization that anticipates strategic changes to content, and penalizes for content homogeneity. We provide analytic and empirical results that demonstrate when and how diversity can be incentivized, and experimentally demonstrate the utility of our approach on synthetic and semi-synthetic data.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2051415542",
                        "name": "Itay Eilat"
                    },
                    {
                        "authorId": "3138200",
                        "name": "Nir Rosenfeld"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [],
            "intents": [],
            "citingPaper": {
                "paperId": "64db95a59e9e915f15c27f7724839dadd21f93e7",
                "externalIds": {
                    "ArXiv": "2211.14236",
                    "DBLP": "journals/corr/abs-2211-14236",
                    "DOI": "10.48550/arXiv.2211.14236",
                    "CorpusId": 254017711
                },
                "corpusId": 254017711,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/64db95a59e9e915f15c27f7724839dadd21f93e7",
                "title": "Strategyproof Decision-Making in Panel Data Settings and Beyond",
                "abstract": "We consider the classical problem of decision-making using panel data, in which a decision-maker gets noisy, repeated measurements of multiple units (or agents). We consider a setup where there is a pre-intervention period, when the principal observes the outcomes of each unit, after which the principal uses these observations to assign a treatment to each unit. Unlike this classical setting, we permit the units generating the panel data to be strategic, i.e. units may modify their pre-intervention outcomes in order to receive a more desirable intervention. The principal's goal is to design a strategyproof intervention policy, i.e. a policy that assigns units to their correct interventions despite their potential strategizing. We first identify a necessary and sufficient condition under which a strategyproof intervention policy exists, and provide a strategyproof mechanism with a simple closed form when one does exist. Along the way, we prove impossibility results for strategic multiclass classification, which may be of independent interest. When there are two interventions, we establish that there always exists a strategyproof mechanism, and provide an algorithm for learning such a mechanism. For three or more interventions, we provide an algorithm for learning a strategyproof mechanism if there exists a sufficiently large gap in the principal's rewards between different interventions. Finally, we empirically evaluate our model using real-world panel data collected from product sales over 18 months. We find that our methods compare favorably to baselines which do not take strategic interactions into consideration, even in the presence of model misspecification.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1570730776",
                        "name": "Keegan Harris"
                    },
                    {
                        "authorId": "49203555",
                        "name": "A. Agarwal"
                    },
                    {
                        "authorId": "27983734",
                        "name": "Chara Podimata"
                    },
                    {
                        "authorId": "1768074",
                        "name": "Zhiwei Steven Wu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [
                "\u2026Alacaoglu et al. (2022); Bru\u0308ckner et al. (2012); Hardt et al. (2016); Dong et al. (2018); Hu et al. (2019); Milli et al. (2019); Miller et al. (2020); Ghalme et al. (2021); Ahmadi et al. (2021); Levanon and Rosenfeld (2021); Zrnic et al. (2021); Nair et al. (2022) and the references therein."
            ],
            "intents": [],
            "citingPaper": {
                "paperId": "556e465c0c4b5eaf25b87b60d96322416d36ff9b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-11040",
                    "ArXiv": "2208.11040",
                    "DOI": "10.48550/arXiv.2208.11040",
                    "CorpusId": 251741105
                },
                "corpusId": 251741105,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/556e465c0c4b5eaf25b87b60d96322416d36ff9b",
                "title": "Strategic Decision-Making in the Presence of Information Asymmetry: Provably Efficient RL with Algorithmic Instruments",
                "abstract": "We study offline reinforcement learning under a novel model called strategic MDP, which characterizes the strategic interactions between a principal and a sequence of myopic agents with private types. Due to the bilevel structure and private types, strategic MDP involves information asymmetry between the principal and the agents. We focus on the offline RL problem where the goal is to learn the optimal policy of the principal concerning a target population of agents, based on a pre-collected dataset that consists of historical interactions. The unobserved private types confound such a dataset as they affect both the rewards and observations received by the principal. We propose a novel algorithm, pessimistic policy learning with algorithmic instruments (PLAN), which leverages the ideas of instrumental variable regression and the pessimism principle to learn a near-optimal principal\u2019s policy in the context of general function approximation. Our algorithm is based on the critical observation that the principal\u2019s actions serve as valid instrumental variables. In particular, under a partial coverage assumption on the offline dataset, we prove that PLAN outputs a 1/ \u221a K-optimal policy with K being the number of collected trajectories. We further apply our framework to some special cases of strategic MDP, including strategic regression (Harris et al., 2021b), strategic bandit, and noncompliance in recommendation systems (Robins, 1998).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1605759982",
                        "name": "Mengxin Yu"
                    },
                    {
                        "authorId": "150358650",
                        "name": "Zhuoran Yang"
                    },
                    {
                        "authorId": "144958605",
                        "name": "Jianqing Fan"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [
                "\u2026include statistical learning characterizations (Zhang & Conitzer, 2021; Sundaram et al., 2021; Ghalme et al., 2021), practical optimization methods (Levanon & Rosenfeld, 2021; 2022), relaxation of key assumptions (Ghalme et al., 2021; Bechavod et al., 2022; Jagadeesan et al., 2021; Levanon &\u2026",
                "\u2026Bechavod et al., 2022; Jagadeesan et al., 2021; Levanon & Rosenfeld, 2022; Eilat et al., 2022), relations to causal aspects (Miller et al., 2020; Chen et al., 2020a), and consideration of societal implications (Milli et al., 2019; Hu et al., 2019; Chen et al., 2020b; Levanon & Rosenfeld, 2021)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "c0d997676429e61cd370dd136a9541d23fa7619c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2206-08542",
                    "ArXiv": "2206.08542",
                    "DOI": "10.48550/arXiv.2206.08542",
                    "CorpusId": 249847919
                },
                "corpusId": 249847919,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/c0d997676429e61cd370dd136a9541d23fa7619c",
                "title": "Strategic Representation",
                "abstract": "Humans have come to rely on machines for reducing excessive information to manageable representations. But this reliance can be abused -- strategic machines might craft representations that manipulate their users. How can a user make good choices based on strategic representations? We formalize this as a learning problem, and pursue algorithms for decision-making that are robust to manipulation. In our main setting of interest, the system represents attributes of an item to the user, who then decides whether or not to consume. We model this interaction through the lens of strategic classification (Hardt et al. 2016), reversed: the user, who learns, plays first; and the system, which responds, plays second. The system must respond with representations that reveal `nothing but the truth' but need not reveal the entire truth. Thus, the user faces the problem of learning set functions under strategic subset selection, which presents distinct algorithmic and statistical challenges. Our main result is a learning algorithm that minimizes error despite strategic representations, and our theoretical analysis sheds light on the trade-off between learning effort and susceptibility to manipulation.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50490499",
                        "name": "V. Nair"
                    },
                    {
                        "authorId": "3402720",
                        "name": "Ganesh Ghalme"
                    },
                    {
                        "authorId": "1404997778",
                        "name": "Inbal Talgam-Cohen"
                    },
                    {
                        "authorId": "3138200",
                        "name": "Nir Rosenfeld"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "contexts": [
                "\u2026behavior (Zhang & Conitzer, 2021; Sundaram et al., 2021; Ghalme et al., 2021), algorithmic hardness (Hardt et al., 2016), practical optimization methods (Levanon & Rosenfeld, 2021; 2022), and societal implications (Milli et al., 2019; Hu et al., 2019; Chen et al., 2020; Levanon & Rosenfeld, 2021).",
                "Modification costs are defined by a cost function c(x, x\u2032) (known to all); here we focus mainly on 2-norm costs c(x, x\u2032) = \u2016x\u2212x\u20162 as used in [30,13], but also discuss other costs from [11,29,5].",
                "L G\n] 4\nFor a wide range of settings, learning under independent user responses has been shown to be theoretically possible (Hardt et al., 2016; Zhang & Conitzer, 2021; Sundaram et al., 2021) and practically feasible (Levanon & Rosenfeld, 2021; 2022).",
                "This, along with its clean formulation as a learning problem, have made strategic classification the target of much recent interest [40,48,29,16,24,51,14,27,30,34,1,4].",
                "For a wide range of settings, learning under independent user responses has been shown to be theoretically possible [22,48,40] and practically feasible [29,30].",
                "\u2026response to h. Modification costs are defined by a cost function c(x, x\u2032) (known to all); here we focus mainly on 2-norm costs c(x, x\u2032) = \u2016x\u2212 x\u2032\u20162 (Levanon & Rosenfeld, 2022; Chen et al., 2020), but also discuss other costs (Br\u00fcckner et al., 2012; Levanon & Rosenfeld, 2021; Bechavod et al., 2022).",
                "\u2026as a learning problem, have made strategic classification the target of much recent interest (Sundaram et al., 2021; Zhang & Conitzer, 2021; Levanon & Rosenfeld, 2021; Ghalme et al., 2021; Jagadeesan et al., 2021; Zrnic et al., 2021; Estornell et al., 2021; Lechner & Urner, 2021; Harris et\u2026",
                "Various aspects of learning have been studied, including: generalization behavior [48,40,16], algorithmic hardness [22], practical optimization methods [29,30], and societal implications [36,23,13,29]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "cc435dddfb8042b518444502507bbc1e62c1a3ff",
                "externalIds": {
                    "DBLP": "conf/iclr/EilatFBR23",
                    "ArXiv": "2205.15765",
                    "DOI": "10.48550/arXiv.2205.15765",
                    "CorpusId": 249209577
                },
                "corpusId": 249209577,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/cc435dddfb8042b518444502507bbc1e62c1a3ff",
                "title": "Strategic Classification with Graph Neural Networks",
                "abstract": "Strategic classification studies learning in settings where users can modify their features to obtain favorable predictions. Most current works focus on simple classifiers that trigger independent user responses. Here we examine the implications of learning with more elaborate models that break the independence assumption. Motivated by the idea that applications of strategic classification are often social in nature, we focus on \\emph{graph neural networks}, which make use of social relations between users to improve predictions. Using a graph for learning introduces inter-user dependencies in prediction; our key point is that strategic users can exploit these to promote their goals. As we show through analysis and simulation, this can work either against the system -- or for it. Based on this, we propose a differentiable framework for strategically-robust learning of graph-based classifiers. Experiments on several real networked datasets demonstrate the utility of our approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2051415542",
                        "name": "Itay Eilat"
                    },
                    {
                        "authorId": "2007782343",
                        "name": "Ben Finkelshtein"
                    },
                    {
                        "authorId": "46906102",
                        "name": "Chaim Baskin"
                    },
                    {
                        "authorId": "3138200",
                        "name": "Nir Rosenfeld"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [
                ", the standard accuracy measure in strategic classification [22, 34, 33])."
            ],
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "0bb976bce77bea2a0fa75488af4ea12e01117a6b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-10842",
                    "ArXiv": "2205.10842",
                    "DOI": "10.48550/arXiv.2205.10842",
                    "CorpusId": 248986948
                },
                "corpusId": 248986948,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0bb976bce77bea2a0fa75488af4ea12e01117a6b",
                "title": "Addressing Strategic Manipulation Disparities in Fair Classification",
                "abstract": "In real-world classification settings, such as loan application evaluation or content moderation on online platforms, individuals respond to classifier predictions by strategically updating their features to increase their likelihood of receiving a particular (positive) decision (at a certain cost). Yet, when different demographic groups have different feature distributions or pay different update costs, prior work has shown that individuals from minority groups often pay a higher cost to update their features. Fair classification aims to address such classifier performance disparities by constraining the classifiers to satisfy statistical fairness properties. However, we show that standard fairness constraints do not guarantee that the constrained classifier reduces the disparity in strategic manipulation cost. To address such biases in strategic settings and provide equal opportunities for strategic manipulation, we propose a constrained optimization framework that constructs classifiers that lower the strategic manipulation cost for minority groups. We develop our framework by studying theoretical connections between group-specific strategic cost disparity and standard selection rate fairness metrics (e.g., statistical rate and true positive rate). Empirically, we show the efficacy of this approach over multiple real-world datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "35604162",
                        "name": "Vijay Keswani"
                    },
                    {
                        "authorId": "144776615",
                        "name": "L. E. Celis"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [
                "Extensions of the preceding, in which model designers anticipate and aim to prevent malicious behavior, exist in the strategic classification [67, 99, 104, 120, 135, 140, 155, 158] and adversarial robustness [41, 49, 76, 246] literature."
            ],
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "56f531eb3a8eaa03a6dbcf74a6c12c80b2e90f45",
                "externalIds": {
                    "DBLP": "journals/csur/KarimiBSV23",
                    "DOI": "10.1145/3527848",
                    "CorpusId": 232118646
                },
                "corpusId": 232118646,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/56f531eb3a8eaa03a6dbcf74a6c12c80b2e90f45",
                "title": "A Survey of Algorithmic Recourse: Contrastive Explanations and Consequential Recommendations",
                "abstract": "Machine learning is increasingly used to inform decision making in sensitive situations where decisions have consequential effects on individuals\u2019 lives. In these settings, in addition to requiring models to be accurate and robust, socially relevant values such as fairness, privacy, accountability, and explainability play an important role in the adoption and impact of said technologies. In this work, we focus on algorithmic recourse, which is concerned with providing explanations and recommendations to individuals who are unfavorably treated by automated decision-making systems. We first perform an extensive literature review, and align the efforts of many authors by presenting unified definitions, formulations, and solutions to recourse. Then, we provide an overview of the prospective research directions toward which the community may engage, challenging existing assumptions and making explicit connections to other ethical challenges such as security, privacy, and fairness.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47937964",
                        "name": "A. Karimi"
                    },
                    {
                        "authorId": "2161028504",
                        "name": "Gilles Barthe"
                    },
                    {
                        "authorId": "1707625",
                        "name": "B. Sch\u00f6lkopf"
                    },
                    {
                        "authorId": "144991545",
                        "name": "I. Valera"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [
                "\u2026community [Milli et al., 2019; Hu et al., 2019; Braverman and Garg, 2020; Kleinberg and Raghavan, 2019; Dong et al., 2018; Zrnic et al., 2021; Levanon and Rosenfeld, 2021] and the economics community [Frankel and Kartik, 2020; Ball, 2020; Hennessy and Goodhart, 2020; Frankel and Kartik,\u2026",
                "The standard model has received significant attention from both the machine learning community [Milli et al., 2019; Hu et al., 2019; Braverman and Garg, 2020; Kleinberg and Raghavan, 2019; Dong et al., 2018; Zrnic et al., 2021; Levanon and Rosenfeld, 2021] and the economics community [Frankel and Kartik, 2020; Ball, 2020; Hennessy and Goodhart, 2020; Frankel and Kartik, 2019]."
            ],
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "494d73f65951e8e8c2cb60aab1d21dd522818c15",
                "externalIds": {
                    "DBLP": "conf/nips/HardtJM22",
                    "ArXiv": "2203.17232",
                    "DOI": "10.48550/arXiv.2203.17232",
                    "CorpusId": 247839528
                },
                "corpusId": 247839528,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/494d73f65951e8e8c2cb60aab1d21dd522818c15",
                "title": "Performative Power",
                "abstract": "We introduce the notion of performative power, which measures the ability of a firm operating an algorithmic system, such as a digital content recommendation platform, to cause change in a population of participants. We relate performative power to the economic study of competition in digital economies. Traditional economic concepts struggle with identifying anti-competitive patterns in digital platforms not least due to the complexity of market definition. In contrast, performative power is a causal notion that is identifiable with minimal knowledge of the market, its internals, participants, products, or prices. Low performative power implies that a firm can do no better than to optimize their objective on current data. In contrast, firms of high performative power stand to benefit from steering the population towards more profitable behavior. We confirm in a simple theoretical model that monopolies maximize performative power. A firm's ability to personalize increases performative power, while competition and outside options decrease performative power. On the empirical side, we propose an observational causal design to identify performative power from discontinuities in how digital platforms display content. This allows to repurpose causal effects from various studies about digital platforms as lower bounds on performative power. Finally, we speculate about the role that performative power might play in competition policy and antitrust enforcement in digital marketplaces.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1775622",
                        "name": "Moritz Hardt"
                    },
                    {
                        "authorId": "145096211",
                        "name": "Meena Jagadeesan"
                    },
                    {
                        "authorId": "1403842911",
                        "name": "Celestine Mendler-D\u00fcnner"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "contexts": [
                "This has made it the focus of many recent works (Sundaram et al., 2021; Zhang & Conitzer, 2021; Levanon & Rosenfeld, 2021; Ghalme et al., 2021; Jagadeesan et al., 2021; Zrnic et al., 2021; Estornell et al., 2021; Lechner & Urner, 2021)\nBut despite the elegant way in which it extends standard binary\u2026",
                "As a preliminary project, we addressed the optimization part of SC in our work: \u201dStrategic Classification Made Practical\u201d (SCMP) (Levanon and Rosenfeld, 2021), in which we present a learning framework for strategic classification that is practical and flexible.",
                "This has made it the focus of many recent works (Sundaram et al., 2021; Zhang and Conitzer, 2021; Levanon and Rosenfeld, 2021; Ghalme et al., 2021; Jagadeesan et al., 2021; Zrnic et al., 2021; Estornell et al., 2021; Lechner and Urner, 2021) But despite the elegant way in which it extends standard binary classification, strategic classification remains narrow in the scope of strategic behavior it permits.",
                "Other works focus on practical aspects, such as Levanon & Rosenfeld (2021) who propose a differentiable learning framework for strategic classification."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "f4186a28f6b33057cdb1a45eb7cb4a4eb3e21d73",
                "externalIds": {
                    "DBLP": "conf/icml/LevanonR22",
                    "ArXiv": "2202.04357",
                    "CorpusId": 246680340
                },
                "corpusId": 246680340,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f4186a28f6b33057cdb1a45eb7cb4a4eb3e21d73",
                "title": "Generalized Strategic Classification and the Case of Aligned Incentives",
                "abstract": "Strategic classification studies learning in settings where self-interested users can strategically modify their features to obtain favorable predictive outcomes. A key working assumption, however, is that\"favorable\"always means\"positive\"; this may be appropriate in some applications (e.g., loan approval), but reduces to a fairly narrow view of what user interests can be. In this work we argue for a broader perspective on what accounts for strategic user behavior, and propose and study a flexible model of generalized strategic classification. Our generalized model subsumes most current models but includes other novel settings; among these, we identify and target one intriguing sub-class of problems in which the interests of users and the system are aligned. This setting reveals a surprising fact: that standard max-margin losses are ill-suited for strategic inputs. Returning to our fully generalized model, we propose a novel max-margin framework for strategic learning that is practical and effective, and which we analyze theoretically. We conclude with a set of experiments that empirically demonstrate the utility of our approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2028200676",
                        "name": "Sagi Levanon"
                    },
                    {
                        "authorId": "3138200",
                        "name": "Nir Rosenfeld"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [
                "In the broader context, our work falls under the umbrella of what can be called \u201cstrategic considerations in machine learning systems,\u201d with a growing recent body of work at the intersection of machine learning, algorithmic game theory and artificial intelligence that addresses this topic from different angles, including learning from strategic data [15, 27, 18, 22, 30, 25, 36, 23], security games [41, 40, 11], and recommendation systems [52, 9]."
            ],
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "80494f609d34141d33d412f143289a6b0ef55892",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-07640",
                    "ArXiv": "2112.07640",
                    "CorpusId": 245130955
                },
                "corpusId": 245130955,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/80494f609d34141d33d412f143289a6b0ef55892",
                "title": "How and Why to Manipulate Your Own Agent",
                "abstract": "The usage of automated learning agents is becoming increasingly prevalent in many online economic applications such as online auctions and automated trading. Motivated by such applications, this paper is dedicated to fundamental modeling and analysis of the strategic situations that the users of automated learning agents are facing. We consider strategic settings where several users engage in a repeated online interaction, assisted by regret-minimizing learning agents that repeatedly play a\"game\"on their behalf. We propose to view the outcomes of the agents' dynamics as inducing a\"meta-game\"between the users. Our main focus is on whether users can benefit in this meta-game from\"manipulating\"their own agents by misreporting their parameters to them. We define a general framework to model and analyze these strategic interactions between users of learning agents for general games and analyze the equilibria induced between the users in three classes of games. We show that, generally, users have incentives to misreport their parameters to their own agents, and that such strategic user behavior can lead to very different outcomes than those anticipated by standard analysis.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "7239133",
                        "name": "Y. Kolumbus"
                    },
                    {
                        "authorId": "1689609",
                        "name": "N. Nisan"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [
                "Other recent work in this area includes developing practically useful tools for modeling strategic behavior, such as differentiable surrogates for strategic responses and regularizers for inducing socially advantageous strategic responses (Levanon and Rosenfeld, 2021); incorporating more realistic limitations on the best-response behavior of the agents (Ghalme et al.",
                "\u2026behavior, such as differentiable surrogates for strategic responses and regularizers for inducing socially advantageous strategic responses (Levanon and Rosenfeld, 2021); incorporating more realistic limitations on the best-response behavior of the agents (Ghalme et al., 2021; Jagadeesan\u2026"
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "8c62607bf5b9f032e40f830c780769ecbde11b89",
                "externalIds": {
                    "ArXiv": "2112.07042",
                    "DBLP": "journals/corr/abs-2112-07042",
                    "CorpusId": 245131621
                },
                "corpusId": 245131621,
                "publicationVenue": {
                    "id": "2d136b11-c2b5-484b-b008-7f4a852fd61e",
                    "name": "International Conference on Artificial Intelligence and Statistics",
                    "type": "conference",
                    "alternate_names": [
                        "AISTATS",
                        "Int Conf Artif Intell Stat"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8c62607bf5b9f032e40f830c780769ecbde11b89",
                "title": "How to Learn when Data Gradually Reacts to Your Model",
                "abstract": "A recent line of work has focused on training machine learning (ML) models in the performative setting, i.e. when the data distribution reacts to the deployed model. The goal in this setting is to learn a model which both induces a favorable data distribution and performs well on the induced distribution, thereby minimizing the test loss. Previous work on finding an optimal model assumes that the data distribution immediately adapts to the deployed model. In practice, however, this may not be the case, as the population may take time to adapt to the model. In many applications, the data distribution depends on both the currently deployed ML model and on the\"state\"that the population was in before the model was deployed. In this work, we propose a new algorithm, Stateful Performative Gradient Descent (Stateful PerfGD), for minimizing the performative loss even in the presence of these effects. We provide theoretical guarantees for the convergence of Stateful PerfGD. Our experiments confirm that Stateful PerfGD substantially outperforms previous state-of-the-art methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1502231522",
                        "name": "Zachary Izzo"
                    },
                    {
                        "authorId": "145085305",
                        "name": "James Y. Zou"
                    },
                    {
                        "authorId": "143954840",
                        "name": "Lexing Ying"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [
                "The strategic learning literature [11, 10, 5, 25, 16, 1, 12, 13, 22] broadly studies machine learning questions in the presence of strategic decision-subjects."
            ],
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "266dafcac568c0c370c47064f512cdcd13efd7dc",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-06283",
                    "ArXiv": "2112.06283",
                    "CorpusId": 245124333
                },
                "corpusId": 245124333,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/266dafcac568c0c370c47064f512cdcd13efd7dc",
                "title": "Bayesian Persuasion for Algorithmic Recourse",
                "abstract": "When subjected to automated decision-making, decision subjects may strategically modify their observable features in ways they believe will maximize their chances of receiving a favorable decision. In many practical situations, the underlying assessment rule is deliberately kept secret to avoid gaming and maintain competitive advantage. The resulting opacity forces the decision subjects to rely on incomplete information when making strategic feature modifications. We capture such settings as a game of Bayesian persuasion, in which the decision maker offers a form of recourse to the decision subject by providing them with an action recommendation (or signal) to incentivize them to modify their features in desirable ways. We show that when using persuasion, the decision maker and decision subject are never worse off in expectation, while the decision maker can be significantly better off. While the decision maker's problem of finding the optimal Bayesian incentive-compatible (BIC) signaling policy takes the form of optimization over infinitely-many variables, we show that this optimization can be cast as a linear program over finitely-many regions of the space of possible assessment rules. While this reformulation simplifies the problem dramatically, solving the linear program requires reasoning about exponentially-many variables, even in relatively simple cases. Motivated by this observation, we provide a polynomial-time approximation scheme that recovers a near-optimal signaling policy. Finally, our numerical simulations on semi-synthetic data empirically demonstrate the benefits of using persuasion in the algorithmic recourse setting.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1570730776",
                        "name": "Keegan Harris"
                    },
                    {
                        "authorId": "15110752",
                        "name": "Valerie Chen"
                    },
                    {
                        "authorId": "2117060486",
                        "name": "Joon Sik Kim"
                    },
                    {
                        "authorId": "145532827",
                        "name": "Ameet Talwalkar"
                    },
                    {
                        "authorId": "2253600",
                        "name": "Hoda Heidari"
                    },
                    {
                        "authorId": "1768074",
                        "name": "Zhiwei Steven Wu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [
                "Finally, there is a growing recent body of work studying various aspects of interactions between strategic players and learning algorithms, including learning from strategic data [13, 32, 17, 21, 38, 30, 43, 28], security games [48, 47, 7], and optimization against regret minimizing opponents [20, 11]."
            ],
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "5fc9ecbcfcb7f971b9425af2570fdfb7b0ca0baa",
                "externalIds": {
                    "DBLP": "conf/www/KolumbusN22",
                    "ArXiv": "2110.11855",
                    "DOI": "10.1145/3485447.3512055",
                    "CorpusId": 239616381
                },
                "corpusId": 239616381,
                "publicationVenue": {
                    "id": "e07422f9-c065-40c3-a37b-75e98dce79fe",
                    "name": "The Web Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Web Conf",
                        "WWW"
                    ],
                    "url": "http://www.iw3c2.org/"
                },
                "url": "https://www.semanticscholar.org/paper/5fc9ecbcfcb7f971b9425af2570fdfb7b0ca0baa",
                "title": "Auctions between Regret-Minimizing Agents",
                "abstract": "We analyze a scenario in which software agents implemented as regret-minimizing algorithms engage in a repeated auction on behalf of their users. We study first-price and second-price auctions, as well as their generalized versions (e.g., as those used for ad auctions). Using both theoretical analysis and simulations, we show that, surprisingly, in second-price auctions the players have incentives to misreport their true valuations to their own learning agents, while in first-price auctions it is a dominant strategy for all players to truthfully report their valuations to their agents.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "7239133",
                        "name": "Y. Kolumbus"
                    },
                    {
                        "authorId": "1689609",
                        "name": "N. Nisan"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [
                ", 2020), and practical considerations for optimization (Levanon and Rosenfeld, 2021) have also mostly built on standard microfoundations.",
                "\u20262019) whether classifiers incentivize improvement as opposed to gaming (Kleinberg and Raghavan, 2019; Miller et al., 2020; Shavit et al., 2020; Haghtalab et al., 2020), and practical considerations for optimization (Levanon and Rosenfeld, 2021) have also mostly built on standard microfoundations."
            ],
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "a762bbd712d91458fb300b9bb8744e25b7532f10",
                "externalIds": {
                    "DBLP": "conf/icml/JagadeesanMH21",
                    "ArXiv": "2106.12705",
                    "CorpusId": 235129599
                },
                "corpusId": 235129599,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/a762bbd712d91458fb300b9bb8744e25b7532f10",
                "title": "Alternative Microfoundations for Strategic Classification",
                "abstract": "When reasoning about strategic behavior in a machine learning context it is tempting to combine standard microfoundations of rational agents with the statistical decision theory underlying classification. In this work, we argue that a direct combination of these standard ingredients leads to brittle solution concepts of limited descriptive and prescriptive value. First, we show that rational agents with perfect information produce discontinuities in the aggregate response to a decision rule that we often do not observe empirically. Second, when any positive fraction of agents is not perfectly strategic, desirable stable points -- where the classifier is optimal for the data it entails -- cease to exist. Third, optimal decision rules under standard microfoundations maximize a measure of negative externality known as social burden within a broad class of possible assumptions about agent behavior. Recognizing these limitations we explore alternatives to standard microfoundations for binary classification. We start by describing a set of desiderata that help navigate the space of possible assumptions about how agents respond to a decision rule. In particular, we analyze a natural constraint on feature manipulations, and discuss properties that are sufficient to guarantee the robust existence of stable points. Building on these insights, we then propose the noisy response model. Inspired by smoothed analysis and empirical observations, noisy response incorporates imperfection in the agent responses, which we show mitigates the limitations of standard microfoundations. Our model retains analytical tractability, leads to more robust insights about stable points, and imposes a lower social burden at optimality.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145096211",
                        "name": "Meena Jagadeesan"
                    },
                    {
                        "authorId": "1403842911",
                        "name": "Celestine Mendler-D\u00fcnner"
                    },
                    {
                        "authorId": "1775622",
                        "name": "Moritz Hardt"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [
                "Given this framework, a number of recent works have studied natural learning dynamics for learning models that are robust to strategic manipulation of the data [6, 17, 20, 30, 38, 48]."
            ],
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "52b9897815f6426929bdedf32509f79c8dff4613",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-12529",
                    "ArXiv": "2106.12529",
                    "CorpusId": 235606412
                },
                "corpusId": 235606412,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/52b9897815f6426929bdedf32509f79c8dff4613",
                "title": "Who Leads and Who Follows in Strategic Classification?",
                "abstract": "As predictive models are deployed into the real world, they must increasingly contend with strategic behavior. A growing body of work on strategic classification treats this problem as a Stackelberg game: the decision-maker\"leads\"in the game by deploying a model, and the strategic agents\"follow\"by playing their best response to the deployed model. Importantly, in this framing, the burden of learning is placed solely on the decision-maker, while the agents' best responses are implicitly treated as instantaneous. In this work, we argue that the order of play in strategic classification is fundamentally determined by the relative frequencies at which the decision-maker and the agents adapt to each other's actions. In particular, by generalizing the standard model to allow both players to learn over time, we show that a decision-maker that makes updates faster than the agents can reverse the order of play, meaning that the agents lead and the decision-maker follows. We observe in standard learning settings that such a role reversal can be desirable for both the decision-maker and the strategic agents. Finally, we show that a decision-maker with the freedom to choose their update frequency can induce learning dynamics that converge to Stackelberg equilibria with either order of play.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "7830023",
                        "name": "Tijana Zrnic"
                    },
                    {
                        "authorId": "3371636",
                        "name": "Eric V. Mazumdar"
                    },
                    {
                        "authorId": "2062932485",
                        "name": "S. Sastry"
                    },
                    {
                        "authorId": "1694621",
                        "name": "Michael I. Jordan"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [
                "Levanon & Rosenfeld (2021) provide a practical, differentiable learning framework for learning in diverse strategic settings."
            ],
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "297285244e9eedca25f86327de11fa6c5ac84e52",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-11592",
                    "ArXiv": "2102.11592",
                    "CorpusId": 232013502
                },
                "corpusId": 232013502,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/297285244e9eedca25f86327de11fa6c5ac84e52",
                "title": "Strategic Classification in the Dark",
                "abstract": "Strategic classification studies the interaction between a classification rule and the strategic agents it governs. Under the assumption that the classifier is known, rational agents respond to it by manipulating their features. However, in many real-life scenarios of high-stake classification (e.g., credit scoring), the classifier is not revealed to the agents, which leads agents to attempt to learn the classifier and game it too. In this paper we generalize the strategic classification model to such scenarios. We define the price of opacity as the difference in prediction error between opaque and transparent strategy-robust classifiers, characterize it, and give a sufficient condition for this price to be strictly positive, in which case transparency is the recommended policy. Our experiments show how Hardt et al.'s robust classifier is affected by keeping agents in the dark.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3402720",
                        "name": "Ganesh Ghalme"
                    },
                    {
                        "authorId": "50490499",
                        "name": "V. Nair"
                    },
                    {
                        "authorId": "2051415542",
                        "name": "Itay Eilat"
                    },
                    {
                        "authorId": "1404997778",
                        "name": "Inbal Talgam-Cohen"
                    },
                    {
                        "authorId": "3138200",
                        "name": "Nir Rosenfeld"
                    }
                ]
            }
        },
        {
            "isInfluential": true,
            "contexts": [
                "Strategic classification made practical: reproduction\nAnonymous Author(s) Affiliation Address email\nReproducibility Summary\nScope of Reproducibility\nIn this work, the paper Strategic Classification Made Practical[3] is evaluated through a reproduction study.",
                "This problem of classification while users strategically modify their features is referred to as strategic classification, and it is the main subject of the paper Strategic Classification Made Practical[3].",
                "This paper describes our efforts to reproduce the work from the paper Strategic Classification Made Practical[3], which addresses the problem of strategic classification in a manner that is more practical than previous approaches, more flexible than previous approaches and takes social good into account.",
                "Reproducibility Summary Scope of Reproducibility In this work, the paper Strategic Classification Made Practical[3] is evaluated through a reproduction study."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "21c0ac1a729404a8f633aac869f1d6b1c1ef1971",
                "externalIds": {
                    "CorpusId": 249120374
                },
                "corpusId": 249120374,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/21c0ac1a729404a8f633aac869f1d6b1c1ef1971",
                "title": "Strategic classification made practical: reproduction",
                "abstract": "The reproduction of the original paper as well as the extended implementation were successful. We were able to reproduce the original results and examine the performance of the proposed model in an environment where strategic and non-strategic users both present. Linear models seem to struggle with different proportions of strategic users, while the non-linear model (RNN) achieves good performance regardless of the proportion of strategic users.",
                "year": 2022,
                "authors": []
            }
        },
        {
            "isInfluential": false,
            "contexts": [],
            "intents": [],
            "citingPaper": {
                "paperId": "57e4688de4adfdd066a0dac31d618ef3e00f6f57",
                "externalIds": {
                    "CorpusId": 253968060
                },
                "corpusId": 253968060,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/57e4688de4adfdd066a0dac31d618ef3e00f6f57",
                "title": "Strategy-Aware Contextual Bandits",
                "abstract": "Algorithmic tools are often used to make decisions about people in high-stakes domains. In the presence of such automated decision making, there is incentive for strategic agents to modify their input to the algorithm in order to receive a more desirable outcome. While previous work on strategic classification attempts to capture this phenomenon, these models fail to take into account the multiple actions a decision maker usually has at their disposal, and the fact that they often have access only to bandit feedback . In contrast, we capture this setting as a contextual bandit problem, in which a decision maker must take actions based on a sequence of strategically modified contexts. We provide a low-strategic-regret algorithm for the two action setting, and prove that sublinear strategic regret is generally not possible for settings in which the number of actions is greater than two. Along the way, we obtain impossibility results for multi-class strategic classification which may be of independent interest.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1570730776",
                        "name": "Keegan Harris"
                    },
                    {
                        "authorId": "27983734",
                        "name": "Chara Podimata"
                    },
                    {
                        "authorId": "1768074",
                        "name": "Zhiwei Steven Wu"
                    }
                ]
            }
        },
        {
            "isInfluential": false,
            "contexts": [
                "Another line of work called strategic classification [7, 15, 10] deals with applying causal interventions to instances.",
                "For example [30, 29] aim to improve fairness; [15, 7, 10] aim to train the models so that the predicted output is preserved under strategic perturbation of the instance space."
            ],
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "6c4cb1e11f80ff3057e72e76540bdf819544ba53",
                "externalIds": {
                    "DBLP": "conf/nips/NKDS22",
                    "CorpusId": 254293451
                },
                "corpusId": 254293451,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/6c4cb1e11f80ff3057e72e76540bdf819544ba53",
                "title": "Learning Recourse on Instance Environment to Enhance Prediction Accuracy",
                "abstract": "Machine Learning models are often susceptible to poor performance on instances sampled from bad environments. For example, an image classifier could provide low accuracy on images captured under low lighting conditions. In high stake ML applications, such as AI-driven medical diagnostics, a better option could be to provide recourse in the form of alternative environment settings in which to recapture the instance for more reliable diagnostics. In this paper, we propose a model called R ECOURSE N ET that learns to apply recourse on the space of environments so that the recoursed instances are amenable to better predictions by the classifier. Learning to output optimal recourse is challenging because we do not assume access to the underlying physical process that generates the recoursed instances. Also, the optimal setting could be instance-dependent \u2014 for example the best camera angle for object recognition could be a function of the object\u2019s shape. We propose a novel three-level training method that (a) Learns a classifier that is optimized for high performance under recourse, (b) Learns a recourse predictor when the training data may contain only limited instances under good environment settings, and (c) Triggers recourse selectively only when recourse is likely to improve classifier confidence. We experiment with synthetic and real world datasets to show the efficacy of our proposed approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1505464856",
                        "name": "Lokesh Nagalapatti"
                    },
                    {
                        "authorId": "2193790205",
                        "name": "Guntakanti Sai Koushik"
                    },
                    {
                        "authorId": "152356803",
                        "name": "A. De"
                    },
                    {
                        "authorId": "1770124",
                        "name": "Sunita Sarawagi"
                    }
                ]
            }
        }
    ]
}