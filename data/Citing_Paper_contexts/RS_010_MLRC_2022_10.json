{
    "offset": 0,
    "data": [
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "e558af81309b5a0e10738411e7035e98009a4e7e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-09962",
                    "ArXiv": "2303.09962",
                    "DOI": "10.1109/CVPR52729.2023.01576",
                    "CorpusId": 257622853
                },
                "corpusId": 257622853,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e558af81309b5a0e10738411e7035e98009a4e7e",
                "title": "Adversarial Counterfactual Visual Explanations",
                "abstract": "Counterfactual explanations and adversarial attacks have a related goal: flipping output labels with minimal perturbations regardless of their characteristics. Yet, adversarial attacks cannot be used directly in a counterfactual explanation perspective, as such perturbations are perceived as noise and not as actionable and understandable image modifications. Building on the robust learning literature, this paper proposes an elegant method to turn adversarial attacks into semantically meaningful perturbations, without modifying the classifiers to explain. The proposed approach hypothesizes that Denoising Diffusion Probabilistic Models are excellent regularizers for avoiding high-frequency and out-of-distribution perturbations when generating adversarial attacks. The paper's key idea is to build attacks through a diffusion model to polish them. This allows studying the target model regardless of its robustification level. Extensive experimentation shows the advantages of our counterfactual explanation approach over current State-of-the-Art in multiple testbeds.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "94637863",
                        "name": "Guillaume Jeanneret"
                    },
                    {
                        "authorId": "145304110",
                        "name": "Lo\u00efc Simon"
                    },
                    {
                        "authorId": "82117876",
                        "name": "F. Jurie"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "140e19a5aca947c0102a341bbf7eee55b7523140",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-12857",
                    "ArXiv": "2211.12857",
                    "DOI": "10.1109/CVPR52729.2023.01784",
                    "CorpusId": 253801921
                },
                "corpusId": 253801921,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/140e19a5aca947c0102a341bbf7eee55b7523140",
                "title": "Explaining Image Classifiers with Multiscale Directional Image Representation",
                "abstract": "Image classifiers are known to be difficult to interpret and therefore require explanation methods to understand their decisions. We present ShearletX, a novel mask explanation method for image classifiers based on the shearlet transform - a multiscale directional image representation. Current mask explanation methods are regularized by smoothness constraints that protect against undesirable fine-grained explanation artifacts. However, the smoothness of a mask limits its ability to separate fine-detail patterns, that are relevant for the classifier, from nearby nuisance patterns, that do not affect the classifier. ShearletX solves this problem by avoiding smoothness regularization all together, replacing it by shearlet sparsity constraints. The resulting explanations consist of a few edges, textures, and smooth parts of the original image, that are the most relevant for the decision of the classifier. To support our method, we propose a mathematical definition for explanation artifacts and an information theoretic score to evaluate the quality of mask explanations. We demonstrate the superiority of ShearletX over previous mask based explanation methods using these new metrics, and present exemplary situations where separating fine-detail patterns allows explaining phenomena that were not explainable before.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48189362",
                        "name": "S. Kolek"
                    },
                    {
                        "authorId": "2191896874",
                        "name": "Robert Windesheim"
                    },
                    {
                        "authorId": "1422639064",
                        "name": "H\u00e9ctor Andrade-Loarca"
                    },
                    {
                        "authorId": "3125779",
                        "name": "Gitta Kutyniok"
                    },
                    {
                        "authorId": "2265243",
                        "name": "R. Levie"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Since the introduction of CAM [83], DeepLift [68], and LIME [58], the explainability literature has witnessed remarkable growth, and various approaches have been proposed to identify important features [61, 47, 10, 13, 38, 35, 39, 71, 51] with varying degrees of success [1]."
            ],
            "citingPaper": {
                "paperId": "5e116b6fc68336b09c6d8474b3c529d7ac50ae86",
                "externalIds": {
                    "ArXiv": "2207.07038",
                    "CorpusId": 260164985
                },
                "corpusId": 260164985,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5e116b6fc68336b09c6d8474b3c529d7ac50ae86",
                "title": "SHAP-XRT: The Shapley Value Meets Conditional Independence Testing",
                "abstract": "The complex nature of artificial neural networks raises concerns on their reliability, trustworthiness, and fairness in real-world scenarios. The Shapley value -- a solution concept from game theory -- is one of the most popular explanation methods for machine learning models. More traditionally, from a statistical perspective, feature importance is defined in terms of conditional independence. So far, these two approaches to interpretability and feature importance have been considered separate and distinct. In this work, we show that Shapley-based explanation methods and conditional independence testing are closely related. We introduce the $\\textbf{SHAP}$ley-E$\\textbf{X}$planation $\\textbf{R}$andomization $\\textbf{T}$est (SHAP-XRT), a testing procedure inspired by the Conditional Randomization Test (CRT) for a specific notion of local (i.e., on a sample) conditional independence. With it, we prove that for binary classification problems, the marginal contributions in the Shapley value provide lower and upper bounds to the $p$-values of their respective tests. Furthermore, we show that the Shapley value itself provides an upper bound to the $p$-value of a global (i.e., overall) null hypothesis. As a result, we further our understanding of Shapley-based explanation methods from a novel perspective and characterize under which conditions one can make statistically valid claims about feature importance via the Shapley value.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2064606668",
                        "name": "Jacopo Teneggi"
                    },
                    {
                        "authorId": "2176187091",
                        "name": "Beepul Bharti"
                    },
                    {
                        "authorId": "3295351",
                        "name": "Yaniv Romano"
                    },
                    {
                        "authorId": "2714145",
                        "name": "Jeremias Sulam"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "27ba6aa0ebe1e8c699763e7ce9a09f4f8ddc2ab1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-08252",
                    "ArXiv": "2110.08252",
                    "DOI": "10.1007/978-3-031-04083-2_6",
                    "CorpusId": 239017037
                },
                "corpusId": 239017037,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/27ba6aa0ebe1e8c699763e7ce9a09f4f8ddc2ab1",
                "title": "A Rate-Distortion Framework for Explaining Black-box Model Decisions",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48189362",
                        "name": "S. Kolek"
                    },
                    {
                        "authorId": "2153960546",
                        "name": "Duc Anh Nguyen"
                    },
                    {
                        "authorId": "2265243",
                        "name": "R. Levie"
                    },
                    {
                        "authorId": "143627859",
                        "name": "Joan Bruna"
                    },
                    {
                        "authorId": "3125779",
                        "name": "Gitta Kutyniok"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "0e19fab5eefea304690cfdd58299c9d4fb29edef",
                "externalIds": {
                    "CorpusId": 263769426
                },
                "corpusId": 263769426,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0e19fab5eefea304690cfdd58299c9d4fb29edef",
                "title": "[Re] Reproducibility Study of \u2019CartoonX: Cartoon Explanations of Image Classifiers\u2019",
                "abstract": "Scope of Reproducibility \u2014 In this reproducibility study, we verify the claims and contribu\u2010 tions in Cartoon Explanations of Image Classifiers by Kolek et al. [1]. These include (i) A proposed technique named CartoonX used to extract visual explanations for predictions via image classification networks, (ii) CartoonX being able to reveal piece\u2010wise smooth regions of the image, unlike previous methods, which extract relevant pixel\u2010sparse re\u2010 gions, and (iii) CartoonX achieving lower distortion values than these methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2169565270",
                        "name": "Sina Taslimi"
                    },
                    {
                        "authorId": "2256429438",
                        "name": "Luke Chin A Foeng"
                    },
                    {
                        "authorId": "2256426453",
                        "name": "Pratik Kayal"
                    },
                    {
                        "authorId": "2256419492",
                        "name": "Aditya Prakash Patra"
                    },
                    {
                        "authorId": "2256275271",
                        "name": "Koustuv Sinha"
                    },
                    {
                        "authorId": "1452678770",
                        "name": "Maurits J. R. Bleeker"
                    },
                    {
                        "authorId": "81679142",
                        "name": "Samarth Bhargav"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Since the introduction of CAM [54], DeepLift [47], and LIME [40], the explainability literature has witnessed remarkable growth, and various approaches have been proposed to identify important features [42, 29, 28, 25, 34, 49, 9, 7] with varying degrees of success [1].",
                "[28] Stefan Kolek, Duc Anh Nguyen, Ron Levie, Joan Bruna, and Gitta Kutyniok."
            ],
            "citingPaper": {
                "paperId": "223ecca4bf55e09a6ac90cb8e099e4782795882e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-07038",
                    "DOI": "10.48550/arXiv.2207.07038",
                    "CorpusId": 250526367
                },
                "corpusId": 250526367,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/223ecca4bf55e09a6ac90cb8e099e4782795882e",
                "title": "From Shapley back to Pearson: Hypothesis Testing via the Shapley Value",
                "abstract": "Machine learning models, in particular arti\ufb01cial neural networks, are increasingly used to inform decision making in high-stakes scenarios across a variety of \ufb01elds\u2014from \ufb01nancial services, to public safety, and healthcare. While neural networks have achieved remarkable performance in many settings, their complex nature raises concerns on their reliability, trustworthiness, and fairness in real-world scenarios. As a result, several a-posteriori explanation methods have been proposed to highlight the features that in\ufb02uence a model\u2019s prediction. Notably, the Shapley value\u2014a game theoretic quantity that satis\ufb01es several desirable properties\u2014has gained popularity in the machine learning explainability literature. More traditionally, however, feature importance in statistical learning has been formalized by conditional independence, and a standard way to test for it is via Conditional Randomization Tests (CRTs). So far, these two perspectives on interpretability and feature importance have been considered distinct and separate. In this work, we show that Shapley-based explanation methods and conditional independence testing for feature importance are closely related. More precisely, we prove that for binary classi\ufb01cation problems, evaluating a Shapley coe\ufb03cient amounts to performing a speci\ufb01c set of conditional independence tests, as implemented by a procedure similar to the CRT but for a di\ufb00erent null hypothesis. Furthermore, the obtained game-theoretic values upper bound the p -values of such tests. As a result, we grant large Shapley coe\ufb03cients with a precise statistical sense of importance with controlled type I error.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2064606668",
                        "name": "Jacopo Teneggi"
                    },
                    {
                        "authorId": "2176187091",
                        "name": "Beepul Bharti"
                    },
                    {
                        "authorId": "3295351",
                        "name": "Yaniv Romano"
                    },
                    {
                        "authorId": "2714145",
                        "name": "Jeremias Sulam"
                    }
                ]
            }
        }
    ]
}