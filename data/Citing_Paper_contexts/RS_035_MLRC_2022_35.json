{
    "offset": 0,
    "data": [
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Many prior studies have found hate-speech classification to be biased against Black people [16, 27, 28, 46] and other marginalized groups [54].",
                "Anti-Black biases in hate speech/offensive speech classification [27, 28, 46] and sexist biases in nudity classification [20] have both been identified in prior literature on content moderation.",
                "Prior research Black social media users are more likely to have their speech be classified as hate speech or otherwise offensive speech by content moderation systems [27, 28, 46], more likely to be classified as negative sentiment in sentiment analysis systems [31], have higher word error rates in speech-to-text captioning [33, 52], and broadly to have poor performance on natural language processing systems in general [6, 60].",
                "These issues are part of a larger phenomenon of unequal treatment of minoritized groups by technological systems; from algorithm biases making unfair and inaccurate predictions and classification for marginalized groups [5, 27, 28], to racism and bigotry on social media causing these groups to experience heightened harassment, abuse, and mistreatment [40, 47], Black content creators must work around these challenges to succeed in the creator economy.",
                "1, these \u2019shadowbanning\u2019 experiences imply the existence of biases against Black folks and other marginalized groups, consistent with content moderation biases identified on other platforms [6, 27, 28, 46]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "e4d7041d752f7934d606f06c1726d8dc698250ab",
                "externalIds": {
                    "DOI": "10.1145/3610169",
                    "CorpusId": 263623055
                },
                "corpusId": 263623055,
                "publicationVenue": {
                    "id": "425553d6-e479-478a-8dd7-ae59d3f32b72",
                    "name": "Proceedings of the ACM on Human-Computer Interaction",
                    "alternate_names": [
                        "Proc ACM Human-computer Interact"
                    ],
                    "issn": "2573-0142",
                    "url": "https://dl.acm.org/citation.cfm?id=3120954",
                    "alternate_urls": [
                        "https://dl.acm.org/citation.cfm?id=J1598&picked=prox"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e4d7041d752f7934d606f06c1726d8dc698250ab",
                "title": "\"Honestly, I Think TikTok has a Vendetta Against Black Creators\": Understanding Black Content Creator Experiences on TikTok",
                "abstract": "As video-sharing social-media platforms have increased in popularity, a 'creator economy' has emerged in which platform users make online content to share with wide audiences, often for profit. As the creator economy has risen in popularity, so have concerns of racism and discrimination on social media. Black content creators across multiple platforms have identified challenges with racism and discrimination, perpetuated by platform users, companies that collaborate with creators for sponsored content, and the algorithms governing these platforms. In this work, we provide a qualitative study of the experiences of Black content creators on one video-sharing platform, TikTok. We conduct 12 semi-structured interviews with Black TikTok content creators to understand their experiences, identify the challenges they face, and understand their perceptions of the platform. We find that some common challenges include: content moderation, monetization, harassment and bullying from viewers, lack of transparency of recommendation and filtering algorithms, and the perception that content from Black creators is treated unfairly by those algorithms. We then suggest design interventions to mitigate the challenges, bolster positive aspects, and overall cultivate an inclusive algorithmic experience for Black creators on TikTok",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2146887091",
                        "name": "Camille Harris"
                    },
                    {
                        "authorId": "2253789619",
                        "name": "Amber Gayle Johnson"
                    },
                    {
                        "authorId": "2253583646",
                        "name": "Sadie Palmer"
                    },
                    {
                        "authorId": "2254124348",
                        "name": "Diyi Yang"
                    },
                    {
                        "authorId": "143709703",
                        "name": "A. Bruckman"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The black-box nature of these classifiers and datasets contain the risk of predicting text features of some identities as more offensive than others without sufficient understanding of contexts surrounding the identity, such as African-American English (Sap et al., 2019; Harris et al., 2022)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e0a36a0e9e8d0cbc3305df79b25f6cee6ef3f6d0",
                "externalIds": {
                    "ArXiv": "2308.09270",
                    "DBLP": "journals/corr/abs-2308-09270",
                    "DOI": "10.48550/arXiv.2308.09270",
                    "CorpusId": 261031701
                },
                "corpusId": 261031701,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e0a36a0e9e8d0cbc3305df79b25f6cee6ef3f6d0",
                "title": "Profile Update: The Effects of Identity Disclosure on Network Connections and Language",
                "abstract": "Our social identities determine how we interact and engage with the world surrounding us. In online settings, individuals can make these identities explicit by including them in their public biography, possibly signaling a change to what is important to them and how they should be viewed. Here, we perform the first large-scale study on Twitter that examines behavioral changes following identity signal addition on Twitter profiles. Combining social networks with NLP and quasi-experimental analyses, we discover that after disclosing an identity on their profiles, users (1) generate more tweets containing language that aligns with their identity and (2) connect more to same-identity users. We also examine whether adding an identity signal increases the number of offensive replies and find that (3) the combined effect of disclosing identity via both tweets and profiles is associated with a reduced number of offensive replies from others.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2111881583",
                        "name": "Minje Choi"
                    },
                    {
                        "authorId": "144463004",
                        "name": "Daniel M. Romero"
                    },
                    {
                        "authorId": "3046220",
                        "name": "David Jurgens"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "This phenomenon of perpetuating biases from annotated data has been discovered and investigated in tasks such as hate speech detection (Sap et al., 2021; Xia et al., 2020; Harris et al., 2022; Davidson et al., 2019) and sentiment analysis (Kir-",
                "\u2026biases from annotated data has been discovered and investigated in tasks such as hate speech detection (Sap et al., 2021; Xia et al., 2020; Harris et al., 2022; Davidson et al., 2019) and sentiment analysis (Kiritchenko and Mohammad, 2018; Garg et al., 2022) where individual perspectives\u2026"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3de99f885cfc0c2145cd584df7df4230cccaea04",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-14291",
                    "ArXiv": "2305.14291",
                    "DOI": "10.48550/arXiv.2305.14291",
                    "CorpusId": 258841278
                },
                "corpusId": 258841278,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/3de99f885cfc0c2145cd584df7df4230cccaea04",
                "title": "Evaluation of African American Language Bias in Natural Language Generation",
                "abstract": "We evaluate how well LLMs understand African American Language (AAL) in comparison to their performance on White Mainstream English (WME), the encouraged\"standard\"form of English taught in American classrooms. We measure LLM performance using automatic metrics and human judgments for two tasks: a counterpart generation task, where a model generates AAL (or WME) given WME (or AAL), and a masked span prediction (MSP) task, where models predict a phrase that was removed from their input. Our contributions include: (1) evaluation of six pre-trained, large language models on the two language generation tasks; (2) a novel dataset of AAL text from multiple contexts (social media, hip-hop lyrics, focus groups, and linguistic interviews) with human-annotated counterparts in WME; and (3) documentation of model performance gaps that suggest bias and identification of trends in lack of understanding of AAL features.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "101192062",
                        "name": "N. Deas"
                    },
                    {
                        "authorId": "69026665",
                        "name": "Jessica A. Grieser"
                    },
                    {
                        "authorId": "1484441195",
                        "name": "Shana Kleiner"
                    },
                    {
                        "authorId": "2767140",
                        "name": "D. Patton"
                    },
                    {
                        "authorId": "1402934614",
                        "name": "Elsbeth Turcan"
                    },
                    {
                        "authorId": "145590324",
                        "name": "K. McKeown"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                ", 2022) and data bias (Park et al., 2018; Dixon et al., 2018; Dodge et al., 2021; Harris et al., 2022) are the cause of this impact, and some studies have investigated the connection between training data and downstream task model behavior (Gonen and Webster, 2020; Li et al.",
                "\u2026et al., 2019; Sap et al., 2019; Davani et al., 2022; Sap et al., 2022) and data bias (Park et al., 2018; Dixon et al., 2018; Dodge et al., 2021; Harris et al., 2022) are the cause of this impact, and some studies have investigated the connection between training data and downstream task model\u2026"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5471114e37448bea2457b74894b1ecb92bbcfdf6",
                "externalIds": {
                    "DBLP": "conf/acl/FengPLT23",
                    "ACL": "2023.acl-long.656",
                    "ArXiv": "2305.08283",
                    "DOI": "10.48550/arXiv.2305.08283",
                    "CorpusId": 258686693
                },
                "corpusId": 258686693,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/5471114e37448bea2457b74894b1ecb92bbcfdf6",
                "title": "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models",
                "abstract": "Language models (LMs) are pretrained on diverse data sources\u2014news, discussion forums, books, online encyclopedias. A significant portion of this data includes facts and opinions which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. Our work develops new methods to (1) measure media biases in LMs trained on such corpora, along social and economic axes, and (2) measure the fairness of downstream NLP models trained on top of politically biased LMs. We focus on hate speech and misinformation detection, aiming to empirically quantify the effects of political (social, economic) biases in pretraining data on the fairness of high-stakes social-oriented tasks. Our findings reveal that pretrained LMs do have political leanings which reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and media biases into misinformation detectors. We discuss the implications of our findings for NLP research and propose future directions to mitigate unfairness.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2114887261",
                        "name": "Shangbin Feng"
                    },
                    {
                        "authorId": "50487261",
                        "name": "Chan Young Park"
                    },
                    {
                        "authorId": "2169159066",
                        "name": "Yuhan Liu"
                    },
                    {
                        "authorId": "2073587169",
                        "name": "Yulia Tsvetkov"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6b2395ebcdc3f284d30fefe5d9877aa3caa7b53f",
                "externalIds": {
                    "ArXiv": "2303.09001",
                    "DBLP": "conf/aies/ChanBR23",
                    "DOI": "10.1145/3600211.3604658",
                    "CorpusId": 257557741
                },
                "corpusId": 257557741,
                "publicationVenue": {
                    "id": "ace94611-0469-4818-ae70-43bdb8082d73",
                    "name": "AAAI/ACM Conference on AI, Ethics, and Society",
                    "type": "conference",
                    "alternate_names": [
                        "AAAI/ACM conference Artificial Intelligence, Ethics, and Society",
                        "AIES",
                        "AAAI/ACM Conf AI Ethics Soc",
                        "AAAI/ACM conf Artif Intell Ethics Soc",
                        "AIES "
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6b2395ebcdc3f284d30fefe5d9877aa3caa7b53f",
                "title": "Reclaiming the Digital Commons: A Public Data Trust for Training Data",
                "abstract": "Democratization of AI means not only that people can freely use AI, but also that people can collectively decide how AI is to be used. In particular, collective decision-making power is required to redress the negative externalities from the development of increasingly advanced AI systems, including degradation of the digital commons and unemployment from automation. The rapid pace of AI development and deployment currently leaves little room for this power. Monopolized in the hands of private corporations, the development of the most capable foundation models has proceeded largely without public input. There is currently no implemented mechanism for ensuring that the economic value generated by such models is redistributed to account for their negative externalities. The citizens that have generated the data necessary to train models do not have input on how their data are to be used. In this work, we propose that a public data trust assert control over training data for foundation models. In particular, this trust should scrape the internet as a digital commons, to license to commercial model developers for a percentage cut of revenues from deployment. First, we argue in detail for the existence of such a trust. We also discuss feasibility and potential risks. Second, we detail a number of ways for a data trust to incentivize model developers to use training data only from the trust. We propose a mix of verification mechanisms, potential regulatory action, and positive incentives. We conclude by highlighting other potential benefits of our proposed data trust and connecting our work to ongoing efforts in data and compute governance.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2154992032",
                        "name": "Alan Chan"
                    },
                    {
                        "authorId": "2070768742",
                        "name": "Herbie Bradley"
                    },
                    {
                        "authorId": "1420542737",
                        "name": "Nitarshan Rajkumar"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Another major documented issue is that the Perspective model displays false positive bias for AAE (African American English) dialect examples [64] and for a host of demographic identity terms that were over-represented among the toxic comments in the training dataset [20].",
                "and that profanity in particular is a significant factor in driving misclassification of AAE [31].",
                "Highlighted examples of unique insights raised by study participants\nand that profanity in particular is a significant factor in driving misclassification of AAE [31]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5277f82b2b422e6fd79ebfa36df5bc3f8c889c2b",
                "externalIds": {
                    "DBLP": "journals/pacmhci/LamGMHLB22a",
                    "DOI": "10.1145/3555625",
                    "CorpusId": 252035713
                },
                "corpusId": 252035713,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5277f82b2b422e6fd79ebfa36df5bc3f8c889c2b",
                "title": "End-User Audits: A System Empowering Communities to Lead Large-Scale Investigations of Harmful Algorithmic Behavior",
                "abstract": "Because algorithm audits are conducted by technical experts, audits are necessarily limited to the hypotheses that experts think to test. End users hold the promise to expand this purview, as they inhabit spaces and witness algorithmic impacts that auditors do not. In pursuit of this goal, we propose end-user audits-system-scale audits led by non-technical users-and present an approach that scaffolds end users in hypothesis generation, evidence identification, and results communication. Today, performing a system-scale audit requires substantial user effort to label thousands of system outputs, so we introduce a collaborative filtering technique that leverages the algorithmic system's own disaggregated training data to project from a small number of end user labels onto the full test set. Our end-user auditing tool, IndieLabel, employs these predicted labels so that users can rapidly explore where their opinions diverge from the algorithmic system's outputs. By highlighting topic areas where the system is under-performing for the user and surfacing sets of likely error cases, the tool guides the user in authoring an audit report. In an evaluation of end-user audits on a popular comment toxicity model with 17 non-technical participants, participants both replicated issues that formal audits had previously identified and also raised previously underreported issues such as under-flagging on veiled forms of hate that perpetuate stigma and over-flagging of slurs that have been reclaimed by marginalized communities.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2003820466",
                        "name": "Michelle S. Lam"
                    },
                    {
                        "authorId": "39504881",
                        "name": "Mitchell L. Gordon"
                    },
                    {
                        "authorId": "1393229896",
                        "name": "D. Metaxa"
                    },
                    {
                        "authorId": "1697703",
                        "name": "Jeffrey T. Hancock"
                    },
                    {
                        "authorId": "9522307",
                        "name": "J. Landay"
                    },
                    {
                        "authorId": "145879842",
                        "name": "Michael S. Bernstein"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "afbb735c4e1e08acf8eab3c6fbb02204ff7697ee",
                "externalIds": {
                    "DOI": "10.1109/ICCCIS56430.2022.10037649",
                    "CorpusId": 256743647
                },
                "corpusId": 256743647,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/afbb735c4e1e08acf8eab3c6fbb02204ff7697ee",
                "title": "Hate Speech Detection in Hindi language using BERT and Convolution Neural Network",
                "abstract": "Social media has become crucial in our lives; it inculcates our opinions by providing untreated information. Whether we might be not participating actively but indirectly everyone became part of its coverage. Wide spread of information over the internet without any validation made it hard to analyze the impact of misleading information. Cyber hate, which is used as a tool to incite violence against a group of people based on ethnicity, nationality, language, sexual orientation, religious faiths, etc., poses a disgraceful utilization of social media. Previous apposite studies reported hate speech mainly in the English language. Less effort has been made for the resource-constraint language such as Hindi, Marathi, Kannada, etc. This work entitles hate speech detection in low-resource Hindi language using BERT and Deep Convolution Neural Network. The proposed Hindi Hate Speech BERT Convolution Neural Network model intends to detect hate speech in real-time so that any harmful incidence can be avoided as early as possible. This model presents a two-stage architecture: In the first stage, we have applied a pre-trained BERT encoder to generate encodings. In the second stage, a convolution neural network followed by a sigmoid layer is used to detect text as hatred or non-hatred. Our model achieved 0.84 & 0.77 f1-score for Hasoc 2020 and Hasoc 2021 dataset respectively.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "151460620",
                        "name": "Shubham Shukla"
                    },
                    {
                        "authorId": "1808508",
                        "name": "Sushama Nagpal"
                    },
                    {
                        "authorId": "49715318",
                        "name": "Sangeeta Sabharwal"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "\u2026et al., 2018; Zhao et al., 2018) and other categories of social biases (Nangia et al., 2020; Nadeem et al., 2021; Parrish et al., 2022), perform poorly against minority demographic groups (Koh et al., 2021; Harris et al., 2022) or dialectical variations (Ziems et al., 2022; Tan et al., 2020)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "711d5e8ddbb840ad31a9ffa3d38590603ba69a92",
                "externalIds": {
                    "ArXiv": "2210.09150",
                    "DBLP": "journals/corr/abs-2210-09150",
                    "DOI": "10.48550/arXiv.2210.09150",
                    "CorpusId": 252917981
                },
                "corpusId": 252917981,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/711d5e8ddbb840ad31a9ffa3d38590603ba69a92",
                "title": "Prompting GPT-3 To Be Reliable",
                "abstract": "Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3's reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM's factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions. Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152358188",
                        "name": "Chenglei Si"
                    },
                    {
                        "authorId": "144702900",
                        "name": "Zhe Gan"
                    },
                    {
                        "authorId": "2149231840",
                        "name": "Zhengyuan Yang"
                    },
                    {
                        "authorId": "2992833",
                        "name": "Shuohang Wang"
                    },
                    {
                        "authorId": "2124948371",
                        "name": "Jianfeng Wang"
                    },
                    {
                        "authorId": "1389036863",
                        "name": "Jordan L. Boyd-Graber"
                    },
                    {
                        "authorId": "29957038",
                        "name": "Lijuan Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ff63687562743aa8f98735302ee5db27c57e1ece",
                "externalIds": {
                    "ArXiv": "2210.06351",
                    "DBLP": "journals/corr/abs-2210-06351",
                    "ACL": "2023.trustnlp-1.10",
                    "DOI": "10.48550/arXiv.2210.06351",
                    "CorpusId": 252846537
                },
                "corpusId": 252846537,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ff63687562743aa8f98735302ee5db27c57e1ece",
                "title": "A Keyword Based Approach to Understanding the Overpenalization of Marginalized Groups by English Marginal Abuse Models on Twitter",
                "abstract": "Harmful content detection models tend to have higher false positive rates for content from marginalized groups. In the context of marginal abuse modeling on Twitter, such disproportionate penalization poses the risk of reduced visibility, where marginalized communities lose the opportunity to voice their opinion on the platform. Current approaches to algorithmic harm mitigation, and bias detection for NLP models are often very ad hoc and subject to human bias. We make two main contributions in this paper. First, we design a novel methodology, which provides a principled approach to detecting and measuring the severity of potential harms associated with a text-based model. Second, we apply our methodology to audit Twitter\u2019s English marginal abuse model, which is used for removing amplification eligibility of marginally abusive content. Without utilizing demographic labels or dialect classifiers, we are still able to detect and measure the severity of issues related to the over-penalization of the speech of marginalized communities, such as the use of reclaimed speech, counterspeech, and identity related terms. In order to mitigate the associated harms, we experiment with adding additional true negative examples and find that doing so provides improvements to our fairness metrics without large degradations in model performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "32254917",
                        "name": "Kyra Yee"
                    },
                    {
                        "authorId": "47540766",
                        "name": "Alice Schoenauer Sebag"
                    },
                    {
                        "authorId": "90784578",
                        "name": "Olivia Redfield"
                    },
                    {
                        "authorId": "23923796",
                        "name": "Emily Sheng"
                    },
                    {
                        "authorId": "47608784",
                        "name": "Matthias Eck"
                    },
                    {
                        "authorId": "49738225",
                        "name": "Luca Belli"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e4e9d556e9725a5fdb2e133b61243ff7c1ca8aeb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-06935",
                    "ArXiv": "2202.06935",
                    "DOI": "10.1613/jair.1.13715",
                    "CorpusId": 246822399
                },
                "corpusId": 246822399,
                "publicationVenue": {
                    "id": "aef12dca-60a0-4ca3-819b-cad26d309d4e",
                    "name": "Journal of Artificial Intelligence Research",
                    "type": "journal",
                    "alternate_names": [
                        "JAIR",
                        "J Artif Intell Res",
                        "The Journal of Artificial Intelligence Research"
                    ],
                    "issn": "1076-9757",
                    "url": "http://www.jair.org/"
                },
                "url": "https://www.semanticscholar.org/paper/e4e9d556e9725a5fdb2e133b61243ff7c1ca8aeb",
                "title": "Repairing the Cracked Foundation: A Survey of Obstacles in Evaluation Practices for Generated Text",
                "abstract": "Evaluation practices in natural language generation (NLG) have many known flaws, but improved evaluation approaches are rarely widely adopted. This issue has become more urgent, since neural generation models have improved to the point where their outputs can often no longer be distinguished based on the surface-level features that older metrics rely on. This paper surveys the issues with human and automatic model evaluations and with commonly used datasets in NLG that have been pointed out over the past 20 years. We summarize, categorize, and discuss how researchers have been addressing these issues and what their findings mean for the current state of model evaluations. Building on those insights, we lay out a long-term vision for evaluation research and propose concrete steps for researchers to improve their evaluation processes. Finally, we analyze 66 generation papers from recent NLP conferences in how well they already follow these suggestions and identify which areas require more drastic changes to the status quo.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3159346",
                        "name": "Sebastian Gehrmann"
                    },
                    {
                        "authorId": "40684993",
                        "name": "Elizabeth Clark"
                    },
                    {
                        "authorId": "145450400",
                        "name": "Thibault Sellam"
                    }
                ]
            }
        }
    ]
}