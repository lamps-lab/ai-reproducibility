{
    "offset": 0,
    "data": [
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a7c741d66be2dca8d62cc8a21a53d01137a2662f",
                "externalIds": {
                    "DOI": "10.1016/j.engappai.2023.106755",
                    "CorpusId": 259970706
                },
                "corpusId": 259970706,
                "publicationVenue": {
                    "id": "1a24ea21-4c37-41d8-9e76-ab802d4afb3e",
                    "name": "Engineering applications of artificial intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "Eng appl artif intell",
                        "Eng Appl Artif Intell",
                        "Engineering Applications of Artificial Intelligence"
                    ],
                    "issn": "0952-1976",
                    "url": "http://www.sciencedirect.com/science/journal/09521976"
                },
                "url": "https://www.semanticscholar.org/paper/a7c741d66be2dca8d62cc8a21a53d01137a2662f",
                "title": "Progressive convolutional transformer for image restoration",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2147200675",
                        "name": "Yecong Wan"
                    },
                    {
                        "authorId": "134473682",
                        "name": "Mingwen Shao"
                    },
                    {
                        "authorId": "2153514280",
                        "name": "Yuanshuo Cheng"
                    },
                    {
                        "authorId": "2113966790",
                        "name": "Deyu Meng"
                    },
                    {
                        "authorId": "1724520",
                        "name": "W. Zuo"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b0660627171fc37c51b1729de88a8cd42130a45d",
                "externalIds": {
                    "DBLP": "journals/tcsv/JiangLZL23",
                    "DOI": "10.1109/TCSVT.2023.3248585",
                    "CorpusId": 257198428
                },
                "corpusId": 257198428,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b0660627171fc37c51b1729de88a8cd42130a45d",
                "title": "Few-Shot Learning for Image Denoising",
                "abstract": "Deep Neural Networks (DNNs) have achieved impressive results on the task of image denoising, but there are two serious problems. First, the denoising ability of DNNs-based image denoising models using traditional training strategies heavily relies on extensive training on clean-noise image pairs. Second, image denoising models based on DNNs usually have large parameters and high computational complexity. To address these issues, this paper proposes a two-stage Few-Shot Learning for Image Denoising (FSLID). Our FSLID is a two-stage denoising strategy integrating Basic Feature Learner (BFL), Denoising Feature Inducer (DFI), and Shared Image Reconstructor (SIR). BFL and SIR are first jointly unsupervised to train on the base image dataset $\\mathcal {D}_{base}$ consisting of easily collected high-quality clean images. Following this, the trained BFL extracts the guided features and constraint features for the noisy and corresponding clean images in the novel image dataset $\\mathcal {D}_{novel}$ , respectively. Furthermore, DFI encodes the noisy features of the noisy images in $\\mathcal {D}_{novel}$ . Then, inducing both the guided features and noisy features, DFI can generate the denoising prior features for the SIR with frozen weights to adaptively denoise the noisy images. Furthermore, we propose refined, low-channel-count, recursive multi-branch Multi-Scale Feature Recursive (MSFR) to modularly formulate an efficient DFI to capture more diverse contextual features information under a limited number of feature channels. Thus, compared with the baseline models, the FSLID composed of the proposed MSFR can significantly reduce the number of model parameters and computational complexity. Extensive experimental results demonstrate our FSLID significantly outperforms well-established baselines on multiple datasets and settings. We hope that our work will encourage further research to explore the field of few-shot image denoising.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2153963636",
                        "name": "Bo Jiang"
                    },
                    {
                        "authorId": "2143370518",
                        "name": "Yao Lu"
                    },
                    {
                        "authorId": "37437068",
                        "name": "Bob Zhang"
                    },
                    {
                        "authorId": "2114182496",
                        "name": "Guangming Lu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "2ab0db35face47e1a5a03d2757162270bda89507",
                "externalIds": {
                    "DOI": "10.1016/j.cviu.2023.103816",
                    "CorpusId": 261665715
                },
                "corpusId": 261665715,
                "publicationVenue": {
                    "id": "5fbb417b-d7a5-44e6-856d-993f0624ed9c",
                    "name": "Computer Vision and Image Understanding",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Vis Image Underst"
                    ],
                    "issn": "1077-3142",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/622809/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/10773142",
                        "http://www.idealibrary.com/links/toc/cviu",
                        "https://www.journals.elsevier.com/computer-vision-and-image-understanding"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2ab0db35face47e1a5a03d2757162270bda89507",
                "title": "CPNet: Continuity Preservation Network for infrared video colorization",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2239002435",
                        "name": "Cheng Cheng"
                    },
                    {
                        "authorId": "2113289501",
                        "name": "Hang Wang"
                    },
                    {
                        "authorId": "2238655836",
                        "name": "Xiang Liao"
                    },
                    {
                        "authorId": "2238651425",
                        "name": "Gang Cheng"
                    },
                    {
                        "authorId": "2238890001",
                        "name": "Hongbin Sun"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "68ae5b63fccbb1c443e5015ac73a6d4702c1d102",
                "externalIds": {
                    "DOI": "10.1016/j.neunet.2023.08.056",
                    "CorpusId": 262029543,
                    "PubMed": "37776616"
                },
                "corpusId": 262029543,
                "publicationVenue": {
                    "id": "a13f3cb8-2492-4ccb-9329-73a5ddcaab9b",
                    "name": "Neural Networks",
                    "type": "journal",
                    "alternate_names": [
                        "Neural Netw"
                    ],
                    "issn": "0893-6080",
                    "url": "http://www.elsevier.com/locate/neunet",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/841/description",
                        "http://www.sciencedirect.com/science/journal/08936080"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/68ae5b63fccbb1c443e5015ac73a6d4702c1d102",
                "title": "Single image denoising with a feature-enhanced network.",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "52168055",
                        "name": "Ruibin Zhuge"
                    },
                    {
                        "authorId": "46585071",
                        "name": "Jinghua Wang"
                    },
                    {
                        "authorId": "2243380838",
                        "name": "Zenglin Xu"
                    },
                    {
                        "authorId": "2192587057",
                        "name": "Yong Xu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "9ceb9a02486d6cb41b5b49bd9745f899b95330bb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-06776",
                    "ArXiv": "2308.06776",
                    "DOI": "10.48550/arXiv.2308.06776",
                    "CorpusId": 260887508
                },
                "corpusId": 260887508,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9ceb9a02486d6cb41b5b49bd9745f899b95330bb",
                "title": "Unsupervised Image Denoising in Real-World Scenarios via Self-Collaboration Parallel Generative Adversarial Branches",
                "abstract": "Deep learning methods have shown remarkable performance in image denoising, particularly when trained on large-scale paired datasets. However, acquiring such paired datasets for real-world scenarios poses a significant challenge. Although unsupervised approaches based on generative adversarial networks offer a promising solution for denoising without paired datasets, they are difficult in surpassing the performance limitations of conventional GAN-based unsupervised frameworks without significantly modifying existing structures or increasing the computational complexity of denoisers. To address this problem, we propose a SC strategy for multiple denoisers. This strategy can achieve significant performance improvement without increasing the inference complexity of the GAN-based denoising framework. Its basic idea is to iteratively replace the previous less powerful denoiser in the filter-guided noise extraction module with the current powerful denoiser. This process generates better synthetic clean-noisy image pairs, leading to a more powerful denoiser for the next iteration. This baseline ensures the stability and effectiveness of the training network. The experimental results demonstrate the superiority of our method over state-of-the-art unsupervised methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2115252477",
                        "name": "X. Lin"
                    },
                    {
                        "authorId": "144532998",
                        "name": "Chao Ren"
                    },
                    {
                        "authorId": "2111310314",
                        "name": "Xiao Liu"
                    },
                    {
                        "authorId": "2110284557",
                        "name": "Jie Huang"
                    },
                    {
                        "authorId": "46359335",
                        "name": "Yinjie Lei"
                    }
                ]
            }
        },
        {
            "contexts": [
                "images, and let the model learn the mapping of a noisy image to a clean image [37], [75]\u2013[78], [81], [82], [86] or let the residual of a clean image and a noisy image be used as a target, and learn to separate the noise from the noisy image [47], [79], [80]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "e4561cf0cee2139b7c9085822a4899ce6a5aace3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-00247",
                    "ArXiv": "2308.00247",
                    "DOI": "10.48550/arXiv.2308.00247",
                    "CorpusId": 260351426
                },
                "corpusId": 260351426,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e4561cf0cee2139b7c9085822a4899ce6a5aace3",
                "title": "Unleashing the Power of Self-Supervised Image Denoising: A Comprehensive Review",
                "abstract": "The advent of deep learning has brought a revolutionary transformation to image denoising techniques. However, the persistent challenge of acquiring noise-clean pairs for supervised methods in real-world scenarios remains formidable, necessitating the exploration of more practical self-supervised image denoising. This paper focuses on self-supervised image denoising methods that offer effective solutions to address this challenge. Our comprehensive review thoroughly analyzes the latest advancements in self-supervised image denoising approaches, categorizing them into three distinct classes: General methods, Blind Spot Network (BSN)-based methods, and Transformer-based methods. For each class, we provide a concise theoretical analysis along with their practical applications. To assess the effectiveness of these methods, we present both quantitative and qualitative experimental results on various datasets, utilizing classical algorithms as benchmarks. Additionally, we critically discuss the current limitations of these methods and propose promising directions for future research. By offering a detailed overview of recent developments in self-supervised image denoising, this review serves as an invaluable resource for researchers and practitioners in the field, facilitating a deeper understanding of this emerging domain and inspiring further advancements.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2239889018",
                        "name": "Dan Zhang"
                    },
                    {
                        "authorId": "2205259878",
                        "name": "Fangfang Zhou"
                    },
                    {
                        "authorId": "2226282356",
                        "name": "Yuanzhou Wei"
                    },
                    {
                        "authorId": "48520620",
                        "name": "X. Yang"
                    },
                    {
                        "authorId": "2226925937",
                        "name": "Yuan Gu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "16749df89bb5d09c570fccad0fac3bbe7fc41b6c",
                "externalIds": {
                    "DOI": "10.1016/j.eswa.2023.121339",
                    "CorpusId": 261309589
                },
                "corpusId": 261309589,
                "publicationVenue": {
                    "id": "987139ae-a65d-49bb-aaf6-fb764dc40b19",
                    "name": "Expert systems with applications",
                    "type": "journal",
                    "alternate_names": [
                        "Expert syst appl",
                        "Expert Systems With Applications",
                        "Expert Syst Appl"
                    ],
                    "issn": "0957-4174",
                    "url": "https://www.journals.elsevier.com/expert-systems-with-applications/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/expert-systems-with-applications",
                        "http://www.sciencedirect.com/science/journal/09574174"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/16749df89bb5d09c570fccad0fac3bbe7fc41b6c",
                "title": "Single image deraining using scale constraint iterative update network",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1390863714",
                        "name": "Yitong Yang"
                    },
                    {
                        "authorId": "1591131546",
                        "name": "Yongjun Zhang"
                    },
                    {
                        "authorId": "150356192",
                        "name": "Zhongwei Cui"
                    },
                    {
                        "authorId": "2112674491",
                        "name": "Haoliang Zhao"
                    },
                    {
                        "authorId": "2210993430",
                        "name": "Ting Ouyang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Next, we improved the structure of the loss used in both the SADNet and Restormer structures.",
                "We selected SADNet [25], which represents a small network based on the UNet improvement, and Restormer, with a more complex network and better fitting abilities, as the night defogging network."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "096818d8e088d91ab028c1d5810aae32aa9015cb",
                "externalIds": {
                    "PubMedCentral": "10455821",
                    "DOI": "10.3390/jimaging9080153",
                    "CorpusId": 260300470,
                    "PubMed": "37623685"
                },
                "corpusId": 260300470,
                "publicationVenue": {
                    "id": "c0fc53c7-b0ed-487d-9191-1262c8322621",
                    "name": "Journal of Imaging",
                    "type": "journal",
                    "alternate_names": [
                        "J Imaging"
                    ],
                    "issn": "2313-433X",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-556372",
                    "alternate_urls": [
                        "https://www.mdpi.com/journal/jimaging",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-556372"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/096818d8e088d91ab028c1d5810aae32aa9015cb",
                "title": "Nighttime Image Dehazing by Render",
                "abstract": "Nighttime image dehazing presents unique challenges due to the unevenly distributed haze caused by the color change of artificial light sources. This results in multiple interferences, including atmospheric light, glow, and direct light, which make the complex scattering haze interference difficult to accurately distinguish and remove. Additionally, obtaining pairs of high-definition data for fog removal at night is a difficult task. These challenges make nighttime image dehazing a particularly challenging problem to solve. To address these challenges, we introduced the haze scattering formula to more accurately express the haze in three-dimensional space. We also proposed a novel data synthesis method using the latest CG textures and lumen lighting technology to build scenes where various hazes can be seen clearly through ray tracing. We converted the complex 3D scattering relationship transformation into a 2D image dataset to better learn the mapping from 3D haze to 2D haze. Additionally, we improved the existing neural network and established a night haze intensity evaluation label based on the idea of optical PSF. This allowed us to adjust the haze intensity of the rendered dataset according to the intensity of the real haze image and improve the accuracy of dehazing. Our experiments showed that our data construction and network improvement achieved better visual effects, objective indicators, and calculation speed.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2219446104",
                        "name": "Z. Jin"
                    },
                    {
                        "authorId": "46854729",
                        "name": "H. Feng"
                    },
                    {
                        "authorId": "48559708",
                        "name": "Zhi-hai Xu"
                    },
                    {
                        "authorId": "51511855",
                        "name": "Yue-ting Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "86d9a94141da293f1bb99979c12d30e523abca28",
                "externalIds": {
                    "DOI": "10.1117/1.JEI.32.4.043019",
                    "CorpusId": 260229637
                },
                "corpusId": 260229637,
                "publicationVenue": {
                    "id": "c677ab24-0c04-487d-83e2-c252af9479c8",
                    "name": "Journal of Electronic Imaging (JEI)",
                    "type": "journal",
                    "alternate_names": [
                        "J Electron Imaging (JEI",
                        "Journal of Electronic Imaging",
                        "J Electron Imaging"
                    ],
                    "issn": "1017-9909",
                    "url": "https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging",
                    "alternate_urls": [
                        "http://electronicimaging.spiedigitallibrary.org/journal.aspx"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/86d9a94141da293f1bb99979c12d30e523abca28",
                "title": "Multiple forward-backward strategies for two-stage video denoising",
                "abstract": "Abstract. Video denoising is a low-level task that uses the information of neighbor frames to denoise polluted frames. Using the information provided by neighboring frames, noise can be effectively removed and the denoising effect is improved. Through continuous development, single-stage and multi-stage denoising methods have been proposed. However, these existing denoising methods simply use the information provided by neighbor frames to denoise the polluted frames, paying insufficient attention to the polluted frames and making insufficient use of the information of the polluted frames. We propose a two-stage video denoising network called multiple forward-backward strategies for two-stage video denoising, which improves the denoising performance and retains more detailed information by enhancing the utilization of information of polluted frames. Extensive experiments have been conducted, and the experimental results show that the proposed method outperforms the state-of-the-art denoising methods in both qualitative and quantitative comparisons.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2211388184",
                        "name": "Xinxiu Qiu"
                    },
                    {
                        "authorId": "145183674",
                        "name": "Z. Miao"
                    },
                    {
                        "authorId": "1778986",
                        "name": "Wanru Xu"
                    },
                    {
                        "authorId": "12438606",
                        "name": "Shanshan Gong"
                    },
                    {
                        "authorId": "2211412661",
                        "name": "Lihuan Zheng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u20262021), MPRNet (Zamir et al. 2021), HE (Gonzalez and Woods 2008), LDR (Lee, Lee, and Kim 2013), WAHE (Arici, Dikbas, and Altunbasak 2009), SADNet (Chang et al. 2020), DnCNN (Zhang et al. 2017)), two domain adaptation methods (ENT (Rusak et al. 2021), BNA (Schneider et al. 2020)) and one feature\u2026",
                "We select eight image enhancement methods (SRN-Deblur (Tao et al. 2018), MIMOUNet (Cho et al. 2021), MPRNet (Zamir et al. 2021), HE (Gonzalez and Woods 2008), LDR (Lee, Lee, and Kim 2013), WAHE (Arici, Dikbas, and Altunbasak 2009), SADNet (Chang et al. 2020), DnCNN (Zhang et al. 2017)), two domain adaptation methods (ENT (Rusak et al. 2021), BNA (Schneider et al. 2020)) and one feature restoration method (FD-Module (Wang et al. 2020)) as comparison."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1d4ddca6c68ed963763711f545844557d6085b8a",
                "externalIds": {
                    "DBLP": "conf/aaai/ZhouMZXX23",
                    "DOI": "10.1609/aaai.v37i3.25492",
                    "CorpusId": 259725238
                },
                "corpusId": 259725238,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/1d4ddca6c68ed963763711f545844557d6085b8a",
                "title": "Robust Feature Rectification of Pretrained Vision Models for Object Recognition",
                "abstract": "Pretrained vision models for object recognition often suffer a dramatic performance drop with degradations unseen during training. In this work, we propose a RObust FEature Rectification module (ROFER) to improve the performance of pretrained models against degradations. Specifically, ROFER first estimates the type and intensity of the degradation that corrupts the image features. Then, it leverages a Fully Convolutional Network (FCN) to rectify the features from the degradation by pulling them back to clear features. ROFER is a general-purpose module that can address various degradations simultaneously, including blur, noise, and low contrast. Besides, it can be plugged into pretrained models seamlessly to rectify the degraded features without retraining the whole model. Furthermore, ROFER can be easily extended to address composite degradations by adopting a beam search algorithm to find the composition order. Evaluations on CIFAR-10 and Tiny-ImageNet demonstrate that the accuracy of ROFER is 5% higher than that of SOTA methods on different degradations. With respect to composite degradations, ROFER improves the accuracy of a pretrained CNN by 10% and 6% on CIFAR-10 and Tiny-ImageNet respectively.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "25457390",
                        "name": "Shengchao Zhou"
                    },
                    {
                        "authorId": "3182192",
                        "name": "Gaofeng Meng"
                    },
                    {
                        "authorId": "2156123014",
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "authorId": "2154865",
                        "name": "R. Xu"
                    },
                    {
                        "authorId": "1683738",
                        "name": "Shiming Xiang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "9f5408d8c08a2f7beb35cb16c6ccdcf67b3c7d3d",
                "externalIds": {
                    "DBLP": "conf/aaai/ShiNGZL0DY023",
                    "DOI": "10.1609/aaai.v37i2.25319",
                    "CorpusId": 259733979
                },
                "corpusId": 259733979,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/9f5408d8c08a2f7beb35cb16c6ccdcf67b3c7d3d",
                "title": "Memory-Oriented Structural Pruning for Efficient Image Restoration",
                "abstract": "Deep learning (DL) based methods have significantly pushed forward the state-of-the-art for image restoration (IR) task. Nevertheless, DL-based IR models are highly computation- and memory-intensive. The surging demands for processing higher-resolution images and multi-task paralleling in practical mobile usage further add to their computation and memory burdens. In this paper, we reveal the overlooked memory redundancy of the IR models and propose a Memory-Oriented Structural Pruning (MOSP) method. To properly compress the long-range skip connections (a major source of the memory burden), we introduce a compactor module onto each skip connection to decouple the pruning of the skip connections and the main branch. MOSP progressively prunes the original model layers and the compactors to cut down the peak memory while maintaining high IR quality. Experiments on real image denoising, image super-resolution and low-light image enhancement show that MOSP can yield models with higher memory efficiency while better preserving performance compared with baseline pruning methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "46692796",
                        "name": "Xiangsheng Shi"
                    },
                    {
                        "authorId": "6636914",
                        "name": "Xuefei Ning"
                    },
                    {
                        "authorId": "2223374285",
                        "name": "Lidong Guo"
                    },
                    {
                        "authorId": "153276638",
                        "name": "Tianchen Zhao"
                    },
                    {
                        "authorId": "2194341309",
                        "name": "En-hao Liu"
                    },
                    {
                        "authorId": "143997941",
                        "name": "Y. Cai"
                    },
                    {
                        "authorId": "1388030824",
                        "name": "Yuhan Dong"
                    },
                    {
                        "authorId": "2177314863",
                        "name": "Huazhong Yang"
                    },
                    {
                        "authorId": "40457242",
                        "name": "Yu Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "image denoising [32], dehazing [33], and SR [15]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "f106d33308794c67efb49f8fb3496a511332ec2a",
                "externalIds": {
                    "DBLP": "journals/tcasI/ZhangM023",
                    "DOI": "10.1109/TCSI.2023.3258446",
                    "CorpusId": 257724372
                },
                "corpusId": 257724372,
                "publicationVenue": {
                    "id": "65967e36-f7db-476f-9d00-fd080a5a8483",
                    "name": "IEEE Transactions on Circuits and Systems Part 1: Regular Papers",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Circuit Syst Part 1 Regul Pap",
                        "IEEE Trans Circuit Syst I-regular Pap",
                        "IEEE Transactions on Circuits and Systems I-regular Papers"
                    ],
                    "issn": "1549-8328",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=8919",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8919"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f106d33308794c67efb49f8fb3496a511332ec2a",
                "title": "An Efficient Accelerator Based on Lightweight Deformable 3D-CNN for Video Super-Resolution",
                "abstract": "Deformable convolutional networks (DCNs) have shown outstanding potential in video super-resolution with their powerful inter-frame feature alignment. However, deploying DCNs on resource-limited devices is challenging, due to their high computational complexity and irregular memory accesses. In this work, an algorithm-hardware co-optimization framework is proposed to accelerate the DCNs on field-programmable gate array (FPGA). Firstly, at the algorithm level, an anchor-based lightweight deformable network (ALDNet) is proposed to extract spatio-temporal information from the aligned features, boosting the visual effects with low model complexity. Secondly, to reduce intensive multiplications, an innovative shift-based deformable 3D convolution is developed using low-cost bit shifts and additions, maintaining comparable reconstruction quality. Thirdly, at the hardware level, a dedicated critical processing core, together with a block-level interleaving storage scheme, is presented to avoid dynamic and irregular memory accesses caused by the deformable convolutions. Finally, an overall architecture is designed to accelerate the ALDNet and implemented on an Intel Stratix 10GX platform. Experimental results demonstrate that the proposed design can provide significantly better visual perception than other FPGA-based super-resolution implementations. Meanwhile, compared with the prior hardware accelerators, our design can achieve $2.75\\times $ and $1.63\\times $ improvements in terms of throughput and energy efficiency, respectively.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "10763777",
                        "name": "Siyu Zhang"
                    },
                    {
                        "authorId": "66322339",
                        "name": "W. Mao"
                    },
                    {
                        "authorId": "2135360811",
                        "name": "Zhongfeng Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "f17a4da16259fe0cfd7939ea978d2f184390c93a",
                "externalIds": {
                    "DBLP": "conf/cvpr/SongB23",
                    "DOI": "10.1109/CVPRW59228.2023.00285",
                    "CorpusId": 260917158
                },
                "corpusId": 260917158,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f17a4da16259fe0cfd7939ea978d2f184390c93a",
                "title": "Hard-negative Sampling with Cascaded Fine-Tuning Network to Boost Flare Removal Performance in the Nighttime Images",
                "abstract": "When light passes through a camera lens, it creates a residue called \"flare\" due to the interaction between foreign substances on the lens surface and internal glasses. At night, images can be distorted by flare due to multiple light sources, and research has been conducted using neural networks to remove the flare and solve this problem. However, to our knowledge, research on this approach has only recently begun, and the results are still limited, with only a few models available for use. Further research is needed to determine if the existing models provide optimal results. As part of the mentioned research, we propose a cascaded neural network structure as a means of fine-tuning earlier models to improve their performance. We optimize the performance of the proposed model by constructing triplets using the outputs of two identical neural networks and applying contrastive learning. To demonstrate the superiority of the proposed method, we quantitatively evaluated it by measuring PSNR and SSIM. We also visually compared the differences in image details after removing the flare. Experimental results confirmed that the images reconstructed by the proposed model were superior in terms of PSNR and SSIM in streak regions, compared to the results generated by the reference model.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3010833",
                        "name": "Soonyong Song"
                    },
                    {
                        "authorId": "40996591",
                        "name": "Heechul Bae"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "fdfda333b337ceac53a2d47811d320083d23ab27",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-10146",
                    "ArXiv": "2305.10146",
                    "DOI": "10.1109/ICME55011.2023.00469",
                    "CorpusId": 258740769
                },
                "corpusId": 258740769,
                "publicationVenue": {
                    "id": "975e1529-a14b-436f-ad7e-a09d8eabc50d",
                    "name": "IEEE International Conference on Multimedia and Expo",
                    "type": "conference",
                    "alternate_names": [
                        "ICME",
                        "Int Conf Multimedia Expo",
                        "IEEE Int Conf Multimedia Expo",
                        "International Conference on Multimedia and Expo"
                    ],
                    "issn": "1945-7871",
                    "alternate_issns": [
                        "2330-7927"
                    ],
                    "url": "http://csdl2.computer.org/persagen/ProceedingByFilter.jsp?filter=I",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conferences.jsp",
                        "http://www.wikicfp.com/cfp/program?id=1418"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fdfda333b337ceac53a2d47811d320083d23ab27",
                "title": "CS-PCN: Context-Space Progressive Collaborative Network for Image Denoising",
                "abstract": "Currently, image-denoising methods based on deep learning cannot adequately reconcile contextual semantic information and spatial details. To take these information optimizations into consideration, in this paper, we propose a Context-Space Progressive Collaborative Network (CS-PCN) for image denoising. CS-PCN is a multi-stage hierarchical architecture composed of a context mining siamese sub-network (CM2S) and a space synthesis sub-network (3S). CM2S aims at extracting rich multi-scale contextual information by sequentially connecting multi-layer feature processors (MLFP) for semantic information pre-processing, attention encoder-decoders (AED) for multi-scale information, and multi-conv attention controllers (MCAC) for supervised feature fusion. 3S parallels MLFP and a single-scale cascading block to learn image details, which not only maintains the contextual information but also emphasizes the complementary spatial ones. Experimental results show that CS-PCN achieves significant performance improvement in synthetic and real-world noise removal.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2217530367",
                        "name": "Yuqi Jiang"
                    },
                    {
                        "authorId": "2124618379",
                        "name": "Chune Zhang"
                    },
                    {
                        "authorId": "2108412985",
                        "name": "Jiao Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The MCWNNM [3], TWSC [4], DnCNN-B [5], CBDNet [18], RIDNet [12], AINDNet [22], VDN [20], SADNet [28], DANet+ [31], DeamNet [50], VDIR [32], and CycleISP [51] were compared.",
                "The BM3D [1], TNRD [68], MCWNNM [3], TWSC [4], DnCNN-B [5], CBDNet [18], RIDNet [12], AINDNet [22], VDN [20], SADNet [28], DANet+ [31], DeamNet [50], VDIR [32], CycleISP [51], BUIFD [19], IRCNN [26], FFDNet [6], ADNet [11], BRDNet [9], DudeNet [10], ADNet [11], and AirNet [27] were utilized for comparison.",
                "Models Parameters Gray Color TNRD [68] 27 DnCNN-B [5] 668 673 BUIFD [19] 1075 1085 IRCNN [26] 186 188 FFDNet [6] 485 852 ADNet [11] 519 521 DudeNet [10] 1077 1079 BRDNet [9] 1113 1117 RIDNet [12] 1497 1499 CBDNet [18] - 4365 VDN [20] 7810 7817 VDIR [32] - 2227 DeamNet [50] 1873 1876 AINDNet [22] - 13764 AirNet [27] - 8930 SADNet [28] 3450 3451 DANet+ [31] - 9154 CycleISP [51] - 2837 DRANet 1612 1617",
                "A spatial-adaptive denoising network (SADNet) was designed in [28] for image blind denoising, where the deformable convolution [29, 30] was used to adjust the convolution kernel size to fit the spatial support area of the network."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "f2a686be79b1d194f76fc8d6bb424e4d079ca3cb",
                "externalIds": {
                    "ArXiv": "2305.04269",
                    "DBLP": "journals/corr/abs-2305-04269",
                    "DOI": "10.48550/arXiv.2305.04269",
                    "CorpusId": 258558182
                },
                "corpusId": 258558182,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f2a686be79b1d194f76fc8d6bb424e4d079ca3cb",
                "title": "Dual Residual Attention Network for Image Denoising",
                "abstract": "In image denoising, deep convolutional neural networks (CNNs) can obtain favorable performance on removing spatially invariant noise. However, many of these networks cannot perform well on removing the real noise (i.e. spatially variant noise) generated during image acquisition or transmission, which severely sets back their application in practical image denoising tasks. Instead of continuously increasing the network depth, many researchers have revealed that expanding the width of networks can also be a useful way to improve model performance. It also has been verified that feature filtering can promote the learning ability of the models. Therefore, in this paper, we propose a novel Dual-branch Residual Attention Network (DRANet) for image denoising, which has both the merits of a wide model architecture and attention-guided feature learning. The proposed DRANet includes two different parallel branches, which can capture complementary features to enhance the learning ability of the model. We designed a new residual attention block (RAB) and a novel hybrid dilated residual attention block (HDRAB) for the upper and the lower branches, respectively. The RAB and HDRAB can capture rich local features through multiple skip connections between different convolutional layers, and the unimportant features are dropped by the residual attention modules. Meanwhile, the long skip connections in each branch, and the global feature fusion between the two parallel branches can capture the global features as well. Moreover, the proposed DRANet uses downsampling operations and dilated convolutions to increase the size of the receptive field, which can enable DRANet to capture more image context information. Extensive experiments demonstrate that compared with other state-of-the-art denoising methods, our DRANet can produce competitive denoising performance both on synthetic and real-world noise removal.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2148650937",
                        "name": "Wencong Wu"
                    },
                    {
                        "authorId": "2131145260",
                        "name": "Shijie Liu"
                    },
                    {
                        "authorId": "2164757090",
                        "name": "Yi Zhou"
                    },
                    {
                        "authorId": "1969342",
                        "name": "Yungang Zhang"
                    },
                    {
                        "authorId": "2068337123",
                        "name": "Yu Xiang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "SADNet [137] introduces a residual spatial-adaptive block and context block to sample related features and obtain multi-scale information.",
                "DNN Methods Supervised MLP [115] 2012 AWGN A multilayer perceptron model TNRD [116] 2016 AWGN A trainable nonlinear diffusion model DnCNN [24] 2017 AWGN A CNN model with batch normalization and residual learning NLNet [30] 2017 AWGN A nonlocal CNN model for grayscale/color image denoising UDNet [117] 2018 AWGN A robust and flexible CNN model using UNet HSID-CNN [118] 2018 AWGN A 2D and 3D combined CNN model VNLNet [31] 2018 AWGN The first nonlocal CNN model for video denoising FFDNet [119] 2018 AWGN A flexible CNN model with tunable input noise levels HSI-DeNet [120] 2018 AWGN and Stripe A CNN model for mixed noise removal PRI-PB-CNN [121] 2018 AWGN an Rician A combination of sliding window scheme and 3D CNN GCBD [28] 2018 AWGN and Realistic A GAN-based blind denoiser DnGAN [122] 2018 AWGN and Blur A GAN model with maximum a posteriori (MAP) framework DBF [123] 2019 AWGN and Realistic Integration of CNN into a boosting algorithm FCCF [40] 2019 AWGN and Realistic A deep fusion scheme of collaborative filtering (CBM3D) and CNN DIDN [124] 2019 AWGN and Realistic A deep iterative down-up CNN model CBDNet [125] 2019 Realistic A convolutional blind denoising network ViDeNN [126] 2019 AWGN A combination of spatial and temporal filtering with CNN RIDNet [127] 2019 AWGN and Realistic A single stage model with feature attention MIFCN [128] 2019 Realistic A fully convolutional network model DRDN [129] 2019 Realistic A dynamic residual dense network model SGN [130] 2019 Realistic A self-guided network with top-down architecture ADGAN [131] 2019 Realistic An attentive GAN model with noise domain adaptation QRNN3D [132] 2020 AWGN and Stripe A deep recurrent neural network model with 3D convolution CycleISP [133] 2020 Realistic A GAN based framework modeling the camera pipeline GCDN [134] 2020 AWGN and Realistic A graph convolution network based denoising model FastDVDNet [43] 2020 AWGN A real-time video denoising network without flow estimation DANet [135] 2020 Realistic A Bayesian framework for noise removal and generation LIDIA [136] 2020 AWGN A lightweight model with instance adaptation SADNet [137] 2020 AWGN A CNN model with residual spatial-adaptive blocks AINDNet [138] 2020 AWGN and Realistic A CNN model based on a transfer learning scheme MIRNet [139] 2020 Realistic A multi-scale model with parallel convolution streams PD-denosing [140] 2020 AWGN and Realistic A CNN model with pixel-shuffle down-sampling adaptation ADRN [141] 2020 AWGN A deep residual network model with channel attention scheme MPRNet [142] 2021 Realistic A multi-stage network with cross-stage feature fusion and attention modules DRUNet [143] 2021 AWGN A plug-and-play method with deep denoiser prior DudeNet [144] 2021 AWGN and Realistic A dual network with four different blocks and sparse mechanism InvDN [145] 2021 Realistic An invertible denoising network with wavelet transformation Restormer [145] 2021 AWGN and Realistic An efficient Transformer model with multi-head attention networks DCDicL [146] 2021 AWGN A deep convolution dictionary learning denoising network NBNet [147] 2021 AWGN and Realistic A UNet based model that learns subspace basis and image projection DeamNet [148] 2021 AWGN and Realistic A CNN model with adaptive consistency prior and self-attention mechanism PNGAN [42] 2021 AWGN and Realistic A GAN based model for real image synthesis with domain alignment SwinIR [149] 2021 AWGN A shifted window (Swin) Transformer model with self-attention mechanism NAFNet [150] 2022 Realistic A UNet based model without nonlinear activation layers MalleNet [151] 2022 AWGN and Realistic A CNN with spatially-varying convolution kernels Masked [152] 2023 AWGN and Realistic A novel training approach that randomly masks pixels of the input image SERT [153] 2023 AWGN, Mixed and Realistic A Transformer framework with rectangle self-attention modeling",
                "Category Training srategy Methods Year Noise modeling Key words\nDNN Methods\nSupervised\nMLP [115] 2012 AWGN A multilayer perceptron model TNRD [116] 2016 AWGN A trainable nonlinear diffusion model DnCNN [24] 2017 AWGN A CNN model with batch normalization and residual learning NLNet [30] 2017 AWGN A nonlocal CNN model for grayscale/color image denoising\nUDNet [117] 2018 AWGN A robust and flexible CNN model using UNet HSID-CNN [118] 2018 AWGN A 2D and 3D combined CNN model\nVNLNet [31] 2018 AWGN The first nonlocal CNN model for video denoising FFDNet [119] 2018 AWGN A flexible CNN model with tunable input noise levels\nHSI-DeNet [120] 2018 AWGN and Stripe A CNN model for mixed noise removal PRI-PB-CNN [121] 2018 AWGN an Rician A combination of sliding window scheme and 3D CNN\nGCBD [28] 2018 AWGN and Realistic A GAN-based blind denoiser DnGAN [122] 2018 AWGN and Blur A GAN model with maximum a posteriori (MAP) framework\nDBF [123] 2019 AWGN and Realistic Integration of CNN into a boosting algorithm FCCF [40] 2019 AWGN and Realistic A deep fusion scheme of collaborative filtering (CBM3D) and CNN\nDIDN [124] 2019 AWGN and Realistic A deep iterative down-up CNN model CBDNet [125] 2019 Realistic A convolutional blind denoising network ViDeNN [126] 2019 AWGN A combination of spatial and temporal filtering with CNN RIDNet [127] 2019 AWGN and Realistic A single stage model with feature attention MIFCN [128] 2019 Realistic A fully convolutional network model DRDN [129] 2019 Realistic A dynamic residual dense network model SGN [130] 2019 Realistic A self-guided network with top-down architecture ADGAN [131] 2019 Realistic An attentive GAN model with noise domain adaptation QRNN3D [132] 2020 AWGN and Stripe A deep recurrent neural network model with 3D convolution CycleISP [133] 2020 Realistic A GAN based framework modeling the camera pipeline GCDN [134] 2020 AWGN and Realistic A graph convolution network based denoising model\nFastDVDNet [43] 2020 AWGN A real-time video denoising network without flow estimation DANet [135] 2020 Realistic A Bayesian framework for noise removal and generation LIDIA [136] 2020 AWGN A lightweight model with instance adaptation\nSADNet [137] 2020 AWGN A CNN model with residual spatial-adaptive blocks AINDNet [138] 2020 AWGN and Realistic A CNN model based on a transfer learning scheme MIRNet [139] 2020 Realistic A multi-scale model with parallel convolution streams\nPD-denosing [140] 2020 AWGN and Realistic A CNN model with pixel-shuffle down-sampling adaptation ADRN [141] 2020 AWGN A deep residual network model with channel attention scheme\nMPRNet [142] 2021 Realistic A multi-stage network with cross-stage feature fusion and attention modules DRUNet [143] 2021 AWGN A plug-and-play method with deep denoiser prior DudeNet [144] 2021 AWGN and Realistic A dual network with four different blocks and sparse mechanism\nInvDN [145] 2021 Realistic An invertible denoising network with wavelet transformation Restormer [145] 2021 AWGN and Realistic An efficient Transformer model with multi-head attention networks DCDicL [146] 2021 AWGN A deep convolution dictionary learning denoising network NBNet [147] 2021 AWGN and Realistic A UNet based model that learns subspace basis and image projection DeamNet [148] 2021 AWGN and Realistic A CNN model with adaptive consistency prior and self-attention mechanism PNGAN [42] 2021 AWGN and Realistic A GAN based model for real image synthesis with domain alignment SwinIR [149] 2021 AWGN A shifted window (Swin) Transformer model with self-attention mechanism NAFNet [150] 2022 Realistic A UNet based model without nonlinear activation layers MalleNet [151] 2022 AWGN and Realistic A CNN with spatially-varying convolution kernels Masked [152] 2023 AWGN and Realistic A novel training approach that randomly masks pixels of the input image\nSERT [153] 2023 AWGN, Mixed and Realistic A Transformer framework with rectangle self-attention modeling\nSelf-supervised/ Unsupervised\nNoise2Noise [154] 2018 AWGN and Poisson A model trained on noisy image pairs of the same scene SCGAN [155] 2019 AWGN Unsupervised modeling with self-consistent GAN Noise2Void [156] 2019 AWGN A blind-spot masking strategy that excludes central pixels of the input Self2Self [157] 2020 AWGN and Realistic A dropout based strategy that generates noisy pairs with the Bernoulli sampler\nC2N [158] 2021 Realistic A GAN based model that generates real noisy images from arbitrary clean ones UDVD [159] 2021 AWGN An unsupervised video denoising network based on blind-spot strategy\nR2R [160] 2021 AWGN and Realistic A model trained on recorrupted noisy pairs Neighbor2Neighbor [161] 2021 AWGN and Realistic A model trained on noisy pairs generated by the downsampling strategy\nIDR [162] 2022 AWGN and Realistic A model trained on noisy pairs with an iterative refinement strategy CVF-SID [163] 2022 Realistic A CNN model with a self-supervised cycle\nBlind2Unblind [164] 2022 AWGN and Realistic A blind-spot network with a global-aware mask mapper AP-BSN [165] 2022 Realistic A blind-spot network (BSN) with an asymmetric pixel-shuffle downsampling strategy DDS2M [166] 2023 AWGN A deep diffusion model with a variational spatio-spectral module C-BSN [167] 2023 Realistic A conditional BSN with downsampled invariance loss\nMM-BSN [168] 2023 Realistic A BSN method with multiple masks of different shapes LGBPN [169] 2023 Realistic A BSN with a patch-masked convolution module and a dilated Transformer block SASL [170] 2023 Realistic A BSN with spatially adaptive supervision\ntraditional denoisers at certain noise levels."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "8b8777a918b1eeff120058b32029b8e2344b289a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-08990",
                    "ArXiv": "2304.08990",
                    "DOI": "10.48550/arXiv.2304.08990",
                    "CorpusId": 258187270
                },
                "corpusId": 258187270,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/8b8777a918b1eeff120058b32029b8e2344b289a",
                "title": "A Comparison of Image Denoising Methods",
                "abstract": "The advancement of imaging devices and countless images generated everyday pose an increasingly high demand on image denoising, which still remains a challenging task in terms of both effectiveness and efficiency. To improve denoising quality, numerous denoising techniques and approaches have been proposed in the past decades, including different transforms, regularization terms, algebraic representations and especially advanced deep neural network (DNN) architectures. Despite their sophistication, many methods may fail to achieve desirable results for simultaneous noise removal and fine detail preservation. In this paper, to investigate the applicability of existing denoising techniques, we compare a variety of denoising methods on both synthetic and real-world datasets for different applications. We also introduce a new dataset for benchmarking, and the evaluations are performed from four different perspectives including quantitative metrics, visual effects, human ratings and computational cost. Our experiments demonstrate: (i) the effectiveness and efficiency of representative traditional denoisers for various denoising tasks, (ii) a simple matrix-based algorithm may be able to produce similar results compared with its tensor counterparts, and (iii) the notable achievements of DNN models, which exhibit impressive generalization ability and show state-of-the-art performance on various datasets. In spite of the progress in recent years, we discuss shortcomings and possible extensions of existing techniques. Datasets, code and results are made publicly available and will be continuously updated at https://github.com/ZhaomingKong/Denoising-Comparison.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "40843510",
                        "name": "Zhaoming Kong"
                    },
                    {
                        "authorId": "2059078700",
                        "name": "Fang Deng"
                    },
                    {
                        "authorId": "2212852359",
                        "name": "Haomin Zhuang"
                    },
                    {
                        "authorId": "2145263935",
                        "name": "Xiaowei Yang"
                    },
                    {
                        "authorId": "1491885364",
                        "name": "Jun Yu"
                    },
                    {
                        "authorId": "2148919788",
                        "name": "Lifang He"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Therefore, a context block [26] here is introduced into the minimum size network between the extraction path and expansion path to expand the receptive field while maintaining the resolution, as shown in Fig.",
                "conventional methods lack adaptability to image content and result in over-smoothing artifacts [26].",
                "2 illustrates the neural network structure of the SpatialAdaptive denoising network (SADNet) in SAID method [26]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "891a499c3fddafc608a1afca1b2d8af7261c235f",
                "externalIds": {
                    "DOI": "10.1109/JLT.2022.3231973",
                    "CorpusId": 255177913
                },
                "corpusId": 255177913,
                "publicationVenue": {
                    "id": "a1f606c0-a59f-4771-bdb9-aac3a2ca72eb",
                    "name": "Journal of Lightwave Technology",
                    "type": "journal",
                    "alternate_names": [
                        "J Light Technol"
                    ],
                    "issn": "0733-8724",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=50",
                    "alternate_urls": [
                        "http://jlt.osa.org/journal/JLT/about.cfm",
                        "http://www.opticsinfobase.org/jlt/journal/JLT/about.cfm",
                        "http://jlt.osa.org/issue.cfm"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/891a499c3fddafc608a1afca1b2d8af7261c235f",
                "title": "SNR Enhancement for BOTDR With Spatial-Adaptive Image Denoising Method",
                "abstract": "A spatial-adaptive image denoising (SAID) method is proposed to enhance the signal-to-noise ratio (SNR) of Brillouin optical time domain reflectometer (BOTDR) without deterioration of spatial resolution. Through analyzing the principle of denoising, the spatial-adaptive denoising network structure is constructed, which can effectively reduce noise and avoid over-smoothing. Moreover, a high-similarity dataset generation method is designed to further enhance the denoising effect. The SAID method is experimentally implemented on two-dimension Brillouin gain spectrum matrix over a 25.1 km-long optical fiber with the spatial resolution of 2 m. The experimental results show that the SNR enhancement of SAID method is 21.92 dB, and the data processing time is only 2.48 s. Whereas the SNR enhancement of conventional 10000 times trace averaging method is merely 9.98 dB, but the data processing time is increased to 174.52 s. Therefore, it is demonstrated that the SAID method has obvious advantages in SNR enhancement and data processing speed without deteriorating the spatial resolution of BOTDR.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2142976800",
                        "name": "Qinglin Wang"
                    },
                    {
                        "authorId": "144074487",
                        "name": "Qing Bai"
                    },
                    {
                        "authorId": "2144472700",
                        "name": "Yuting Liu"
                    },
                    {
                        "authorId": "2110161639",
                        "name": "Jingxian Wang"
                    },
                    {
                        "authorId": "47904472",
                        "name": "Yu Wang"
                    },
                    {
                        "authorId": "52481125",
                        "name": "Bao-quan Jin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "f453aab9d889abf8dfd6a38f50da83a8d204887f",
                "externalIds": {
                    "DBLP": "journals/ijitsa/XueW23",
                    "DOI": "10.4018/ijitsa.321456",
                    "CorpusId": 258168265
                },
                "corpusId": 258168265,
                "publicationVenue": {
                    "id": "4fed2ae2-bd9d-4eec-aa27-42b54fae81d4",
                    "name": "International Journal of Information Technologies and Systems Approach",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Inf Technol Syst Approach"
                    ],
                    "issn": "1935-570X",
                    "url": "http://www.idea-group.com/",
                    "alternate_urls": [
                        "https://www.igi-global.com/journal/international-journal-information-technologies-systems/1098"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f453aab9d889abf8dfd6a38f50da83a8d204887f",
                "title": "An Efficient Self-Refinement and Reconstruction Network for Image Denoising",
                "abstract": "Recent works tend to design effective but deep and complex denoising networks, which usually ignored the industrial requirement of efficiency. In this paper, an effective and efficient self-refinement and reconstruction network (SRRNet) is proposed for image denoising. It is based on the encoder-decoder architecture and three improvements are introduced to solve the problem. Specifically, four novel residual connections of different types are proposed as building blocks to maintain original contextual details. A high-resolution reconstruction module is introduced to connect cross-level encoders and corresponding decoders, so as to boost information flow and result in realistic clear image. And multi-scale dual attention is used for suppressing noise and enhancing beneficial dependency. SRRNet achieves PSNR of 39.83 dB and 39.96 dB on SIDD and DND respectively. Compared with other works, the accuracy is higher and the complexity is lower. Extensive experiments in real-world image denoising and Gaussian noise removal prove that SRRNet balances performance and temporal cost better.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2214550270",
                        "name": "Jinqiang Xue"
                    },
                    {
                        "authorId": "1715610",
                        "name": "Qi Wu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "TABLE IV REAL IMAGE DENOISING COMPARISON WITH DNCNN [12], BM3D [40], CBDNET [46], RIDNET [47], AINDNET [48], VDN [49], SADNET [50], DANET+ [51], CYCLEISR [52], DEAMNET [53] ON SIDD [38] (? DENOTE THE MODEL USING ADDITIONAL TRAINING SETS TO TRAIN THE MODEL)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ff5b6bd96615db278d62f5aa7ea298685f8d928e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-06274",
                    "ArXiv": "2304.06274",
                    "DOI": "10.48550/arXiv.2304.06274",
                    "CorpusId": 258107959
                },
                "corpusId": 258107959,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ff5b6bd96615db278d62f5aa7ea298685f8d928e",
                "title": "EWT: Efficient Wavelet-Transformer for Single Image Denoising",
                "abstract": "Transformer-based image denoising methods have achieved encouraging results in the past year. However, it must uses linear operations to model long-range dependencies, which greatly increases model inference time and consumes GPU storage space. Compared with convolutional neural network-based methods, current Transformer-based image denoising methods cannot achieve a balance between performance improvement and resource consumption. In this paper, we propose an Efficient Wavelet Transformer (EWT) for image denoising. Specifically, we use Discrete Wavelet Transform (DWT) and Inverse Wavelet Transform (IWT) for downsampling and upsampling, respectively. This method can fully preserve the image features while reducing the image resolution, thereby greatly reducing the device resource consumption of the Transformer model. Furthermore, we propose a novel Dual-stream Feature Extraction Block (DFEB) to extract image features at different levels, which can further reduce model inference time and GPU memory usage. Experiments show that our method speeds up the original Transformer by more than 80%, reduces GPU memory usage by more than 60%, and achieves excellent denoising results. All code will be public.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1492115432",
                        "name": "Juncheng Li"
                    },
                    {
                        "authorId": "2175654022",
                        "name": "Bodong Cheng"
                    },
                    {
                        "authorId": "47558464",
                        "name": "Ying Chen"
                    },
                    {
                        "authorId": "3306402",
                        "name": "Guangwei Gao"
                    },
                    {
                        "authorId": "40227204",
                        "name": "T. Zeng"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "3f50c684962a6dcf67a05a181df600e99d068df7",
                "externalIds": {
                    "DOI": "10.1117/12.2662187",
                    "CorpusId": 258128628
                },
                "corpusId": 258128628,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3f50c684962a6dcf67a05a181df600e99d068df7",
                "title": "DR-UNet: dynamic residual U-Net for blind correction of optical degradation",
                "abstract": "Due to the size limitation of mobile devices, their optical design is difficult to reach the level of professional equipment. Corresponding restoration methods are then needed to compensate for the shortage. However, most of the models are still static, which leads to their limited representation ability of images. To tackle this problem, we propose a plug-and-play deformable residual block for efficiently sampling the spatially related features at different scales. Moreover, considering that the optical degradation is closely correlated with the field-of-view (FOV), we introduce a FOV attention block based on omni-dimensional dynamic convolution to integrate spatial features. On this basis, we further propose a novel optical degradation correction model called DR-UNet. It is constructed on an encoder-decoder structure to capture multiscale information, along with several context blocks. By correcting the optical degradation in images from coarse to fine, we finally obtain high-quality and degradation-free images. Extensive results demonstrate that our method can compete favorably with some state-of-the-art methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2145789929",
                        "name": "Jingwen Zhou"
                    },
                    {
                        "authorId": "2141309413",
                        "name": "Shiqi Chen"
                    },
                    {
                        "authorId": "40422449",
                        "name": "Qi Li"
                    },
                    {
                        "authorId": "15762417",
                        "name": "Tong Li"
                    },
                    {
                        "authorId": "46854729",
                        "name": "H. Feng"
                    }
                ]
            }
        },
        {
            "contexts": [
                "image [5], [6], [8], [9], [10], [24], [25], [26], [27].",
                "[5], [6], [7] have relatively better denoising effects on public datasets."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "452a7db28880248d768163d1d4db19b1de85f08d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-01627",
                    "ArXiv": "2304.01627",
                    "DOI": "10.1109/ACCESS.2023.3243829",
                    "CorpusId": 256805237
                },
                "corpusId": 256805237,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/452a7db28880248d768163d1d4db19b1de85f08d",
                "title": "Self-Supervised Image Denoising for Real-World Images With Context-Aware Transformer",
                "abstract": "In recent years, the development of deep learning has been pushing image denoising to a new level. Among them, self-supervised denoising is increasingly popular because it does not require any prior knowledge. Most of the existing self-supervised methods are based on convolutional neural networks (CNN), which are restricted by the locality of the receptive field and would cause color shifts or textures loss. In this paper, we propose a novel Denoise Transformer for real-world image denoising, which is mainly constructed with Context-aware Denoise Transformer (CADT) units and Secondary Noise Extractor (SNE) block. CADT is designed as a dual-branch structure, where the global branch uses a window-based Transformer encoder to extract the global information, while the local branch focuses on the extraction of local features with small receptive field. By incorporating CADT as basic components, we build a hierarchical network to directly learn the noise distribution information through residual learning and obtain the first stage denoised output. Then, we design SNE in low computation for secondary global noise extraction. Finally the blind spots are collected from the Denoise Transformer output and reconstructed, forming the final denoised image. Extensive experiments on the real-world SIDD benchmark achieve 50.62/0.990 for PSNR/SSIM, which is competitive with the current state-of-the-art method and only 0.17/0.001 lower. Visual comparisons on public sRGB, Raw-RGB and greyscale datasets prove that our proposed Denoise Transformer has a competitive performance, especially on blurred textures and low-light images, without using additional knowledge, e.g., noise level or noise type, regarding the underlying unknown noise.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2239889018",
                        "name": "Dan Zhang"
                    },
                    {
                        "authorId": "2205259878",
                        "name": "Fangfang Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Subsequently, many researchers proposed other image denoising methods [2, 5, 9, 10, 17, 21, 25, 32] based on deep learning by adding AWGN to clean sRGB images.",
                "The supervised denoising methods [2,5,7,12,40,41,43] have relatively better performance than the self-supervised."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "5cd662006548f34a5e3427bdff83980e293a0233",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-01598",
                    "ArXiv": "2304.01598",
                    "DOI": "10.1109/CVPRW59228.2023.00441",
                    "CorpusId": 257921883
                },
                "corpusId": 257921883,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5cd662006548f34a5e3427bdff83980e293a0233",
                "title": "MM-BSN: Self-Supervised Image Denoising for Real-World with Multi-Mask based on Blind-Spot Network",
                "abstract": "Recent advances in deep learning have been pushing image denoising techniques to a new level. In self-supervised image denoising, blind-spot network (BSN) is one of the most common methods. However, most of the existing BSN algorithms use a dot-based central mask, which is recognized as inefficient for images with large-scale spatially correlated noise. In this paper, we give the definition of large-noise and propose a multi-mask strategy using multiple convolutional kernels masked in different shapes to further break the noise spatial correlation. Furthermore, we propose a novel self-supervised image denoising method that combines the multi-mask strategy with BSN (MM-BSN). We show that different masks can cause significant performance differences, and the proposed MM-BSN can efficiently fuse the features extracted by multi-masked layers, while recovering the texture structures destroyed by multi-masking and information transmission. Our MM-BSN can be used to address the problem of large-noise denoising, which cannot be efficiently handled by other BSN methods. Extensive experiments on public real-world datasets demonstrate that the proposed MM-BSN achieves state-of-the-art performance among self-supervised and even unpaired image denoising methods for sRGB images denoising, without any labelling effort or prior knowledge. Code can be found in https://github.com/dannie125/MM-BSN.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2239889018",
                        "name": "Dan Zhang"
                    },
                    {
                        "authorId": "2205259878",
                        "name": "Fangfang Zhou"
                    },
                    {
                        "authorId": "2214140548",
                        "name": "Yuwen Jiang"
                    },
                    {
                        "authorId": "1893387",
                        "name": "Zhengming Fu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "1ae4e75d4be3d78bc02714c25300670a95c9c8ef",
                "externalIds": {
                    "DBLP": "journals/dsp/LiaoLC23",
                    "DOI": "10.1016/j.dsp.2023.104052",
                    "CorpusId": 258158863
                },
                "corpusId": 258158863,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1ae4e75d4be3d78bc02714c25300670a95c9c8ef",
                "title": "Residual dense network with non-residual guidance for blind image denoising",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "40206046",
                        "name": "Jan-Ray Liao"
                    },
                    {
                        "authorId": "2527661",
                        "name": "Kun-Feng Lin"
                    },
                    {
                        "authorId": "2112351080",
                        "name": "Yen-Cheng Chang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Method RIDNet AINDNet VDN SADNet DANet+ CycleISP MIRNet DeamNet MPRNet DAGL Uformer MAXIM Restormer CSformer CSformer\u2217 Dataset [5] [39] [90] [11] [91] [93] [94] [64] [95] [55] [83] [77] [92] (Ours) (Ours)"
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "236b74fe07face5baa38ba8083fb5106ab7a7f43",
                "externalIds": {
                    "ArXiv": "2303.17316",
                    "DBLP": "journals/corr/abs-2303-17316",
                    "DOI": "10.48550/arXiv.2303.17316",
                    "CorpusId": 257833783
                },
                "corpusId": 257833783,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/236b74fe07face5baa38ba8083fb5106ab7a7f43",
                "title": "Masked Autoencoders as Image Processors",
                "abstract": "Transformers have shown significant effectiveness for various vision tasks including both high-level vision and low-level vision. Recently, masked autoencoders (MAE) for feature pre-training have further unleashed the potential of Transformers, leading to state-of-the-art performances on various high-level vision tasks. However, the significance of MAE pre-training on low-level vision tasks has not been sufficiently explored. In this paper, we show that masked autoencoders are also scalable self-supervised learners for image processing tasks. We first present an efficient Transformer model considering both channel attention and shifted-window-based self-attention termed CSformer. Then we develop an effective MAE architecture for image processing (MAEIP) tasks. Extensive experimental results show that with the help of MAEIP pre-training, our proposed CSformer achieves state-of-the-art performance on various image processing tasks, including Gaussian denoising, real image denoising, single-image motion deblurring, defocus deblurring, and image deraining.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "19269060",
                        "name": "Huiyu Duan"
                    },
                    {
                        "authorId": "2149662083",
                        "name": "Wei Shen"
                    },
                    {
                        "authorId": "2246414",
                        "name": "Xiongkuo Min"
                    },
                    {
                        "authorId": "146558480",
                        "name": "Danyang Tu"
                    },
                    {
                        "authorId": "2212995262",
                        "name": "Long Teng"
                    },
                    {
                        "authorId": "2117997534",
                        "name": "Jia Wang"
                    },
                    {
                        "authorId": "2087717114",
                        "name": "Guangtao Zhai"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Qualitative enhancement comparisons of our model on synthetic compression blur samples with SADNet (Chang et al., 2020) and MPRNet (Zamir et al.",
                "A B C D E\nFIGURE 7 Qualitative enhancement comparisons of our model on synthetic compression blur samples with SADNet (Chang et al., 2020) and MPRNet (Zamir et al., 2021).",
                "The baselines for compression deblur are (Dong et al., 2015; Chang et al., 2020; Chen et al., 2021; Jiang et al., 2021; Zamir et al., 2021)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "2c68bab6be3cf67cbebc2667ee1a00353eebdd30",
                "externalIds": {
                    "PubMedCentral": "10086424",
                    "DOI": "10.3389/fpls.2023.1154176",
                    "CorpusId": 257770333,
                    "PubMed": "37056495"
                },
                "corpusId": 257770333,
                "publicationVenue": {
                    "id": "e110cc75-cd00-4b7f-968c-fd70b464a553",
                    "name": "Frontiers in Plant Science",
                    "type": "journal",
                    "alternate_names": [
                        "Front Plant Sci"
                    ],
                    "issn": "1664-462X",
                    "url": "http://www.frontiersin.org/about/journalseries",
                    "alternate_urls": [
                        "http://www.frontiersin.org/plant_science",
                        "https://www.frontiersin.org/journals/plant-science",
                        "http://journal.frontiersin.org/journal/plant-science"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2c68bab6be3cf67cbebc2667ee1a00353eebdd30",
                "title": "All-in-one aerial image enhancement network for forest scenes",
                "abstract": "Drone monitoring plays an irreplaceable and significant role in forest firefighting due to its characteristics of wide-range observation and real-time messaging. However, aerial images are often susceptible to different degradation problems before performing high-level visual tasks including but not limited to smoke detection, fire classification, and regional localization. Recently, the majority of image enhancement methods are centered around particular types of degradation, necessitating the memory unit to accommodate different models for distinct scenarios in practical applications. Furthermore, such a paradigm requires wasted computational and storage resources to determine the type of degradation, making it difficult to meet the real-time and lightweight requirements of real-world scenarios. In this paper, we propose an All-in-one Image Enhancement Network (AIENet) that can restore various degraded images in one network. Specifically, we design a new multi-scale receptive field image enhancement block, which can better reconstruct high-resolution details of target regions of different sizes. In particular, this plug-and-play module enables it to be embedded in any learning-based model. And it has better flexibility and generalization in practical applications. This paper takes three challenging image enhancement tasks encountered in drone monitoring as examples, whereby we conduct task-specific and all-in-one image enhancement experiments on a synthetic forest dataset. The results show that the proposed AIENet outperforms the state-of-the-art image enhancement algorithms quantitatively and qualitatively. Furthermore, extra experiments on high-level vision detection also show the promising performance of our method compared with some recent baselines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2213713876",
                        "name": "Zhaoqi Chen"
                    },
                    {
                        "authorId": "2109153608",
                        "name": "Chuansheng Wang"
                    },
                    {
                        "authorId": "2191202443",
                        "name": "Fuquan Zhang"
                    },
                    {
                        "authorId": "1712174",
                        "name": "Ling Zhang"
                    },
                    {
                        "authorId": "144740656",
                        "name": "A. Grau"
                    },
                    {
                        "authorId": "143664896",
                        "name": "Edmundo Guerra"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "334d4538826682953fcef4ef07bb9dcf44aafac2",
                "externalIds": {
                    "DBLP": "conf/iccai/XiaWY23",
                    "DOI": "10.1145/3594315.3594332",
                    "CorpusId": 260380488
                },
                "corpusId": 260380488,
                "publicationVenue": {
                    "id": "7d3e02cf-5e53-49c4-be9c-73d29d1912ab",
                    "name": "International Conference on Computing and Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Artif Intell",
                        "International Conference in Central Asia on Internet",
                        "ICCAI",
                        "Int Conf Central Asia Internet"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/334d4538826682953fcef4ef07bb9dcf44aafac2",
                "title": "Pseudo 3D-Attention for Real Image Denoising",
                "abstract": "Attention mechanism has shown great potential in image noise reduction tasks. We can choose different attention mechanisms according to different noise reduction tasks to enhance the ability of network to extract corresponding information. The problem of blurring and smoothing of the denoised image in the real image denoising task is considered. This paper recommends an attention depth fusion mechanism to solve this problem. It is different from the previous use of serial or parallel channels and spatial attention mechanisms to output features directly. After parallel connection, we use the shuffle operation to achieve cross channel communication and accelerate feature fusion of the two attention mechanisms. Then use a simple MLP module to perform cross direction channel attention calculation on the acquired spatial and channel attention features. And completing the deep fusion and feature interaction of spatial attention mechanisms and channel attention mechanisms. Final we use the Cross-attention to further enhance ability of the model to extract global feature and forward long semantic information.We name this module Pseudo 3D Attention. Finally, we conduct evaluations on real image denoising benchmarks including SIDD, DND, CC15, PolyU. We proposed method achieve competitive results. In particular, PSNR of 39.71dB and SSIM of 0.961 in SIDD is achieved without extra an train set.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2077455354",
                        "name": "Xin Xia"
                    },
                    {
                        "authorId": "2119797116",
                        "name": "Hao Wu"
                    },
                    {
                        "authorId": "2057289880",
                        "name": "Guowu Yuan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In detail, we report the evaluation results from the classic denoising method BM3D [11], CNN-based methods DnCNN [59], CBDNet [15], RIDNet [3], AINDNet [19], VDN [52], SADNet [6], DANet [53], CycleISP [54], MIRNet [55], DeamNet [39], DAGL [32], MAXIM [45], and Transformerbased methods Uformer [49] and Restormer [57].",
                "Firstly, we set the layer numbers of both branches the same, which are [2, 4, 4, 6, 4, 4, 2].",
                "BM3D DnCNN CBDNet RIDNet AINDNet VDN SADNet DANet CycleISP MIRNet DeamNet DAGL MAXIM Uformer Restormer Xformer Dataset Method [11] [59] [15] [3] [19] [52] [6] [53] [54] [55] [39] [32] [45] [49] [57] (ours)"
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "edef246794a1eb2df69e9d6d1a3c72ca0516f124",
                "externalIds": {
                    "ArXiv": "2303.06440",
                    "DBLP": "journals/corr/abs-2303-06440",
                    "DOI": "10.48550/arXiv.2303.06440",
                    "CorpusId": 257496765
                },
                "corpusId": 257496765,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/edef246794a1eb2df69e9d6d1a3c72ca0516f124",
                "title": "Xformer: Hybrid X-Shaped Transformer for Image Denoising",
                "abstract": "In this paper, we present a hybrid X-shaped vision Transformer, named Xformer, which performs notably on image denoising tasks. We explore strengthening the global representation of tokens from different scopes. In detail, we adopt two types of Transformer blocks. The spatial-wise Transformer block performs fine-grained local patches interactions across tokens defined by spatial dimension. The channel-wise Transformer block performs direct global context interactions across tokens defined by channel dimension. Based on the concurrent network structure, we design two branches to conduct these two interaction fashions. Within each branch, we employ an encoder-decoder architecture to capture multi-scale features. Besides, we propose the Bidirectional Connection Unit (BCU) to couple the learned representations from these two branches while providing enhanced information fusion. The joint designs make our Xformer powerful to conduct global information modeling in both spatial and channel dimensions. Extensive experiments show that Xformer, under the comparable model complexity, achieves state-of-the-art performance on the synthetic and real-world image denoising tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2120357021",
                        "name": "Jiale Zhang"
                    },
                    {
                        "authorId": "2410227",
                        "name": "Yulun Zhang"
                    },
                    {
                        "authorId": "4398255",
                        "name": "Jinjin Gu"
                    },
                    {
                        "authorId": "46436215",
                        "name": "Jiahua Dong"
                    },
                    {
                        "authorId": "3254296",
                        "name": "L. Kong"
                    },
                    {
                        "authorId": "2159107948",
                        "name": "Xiaokang Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "They use globally-shared convolution kernels in the convolution operator for aggregating neighboring information, such as using dilated convolutions [6, 11] to increase the receptive fields and adopting multi-stage [71] or multi-scale features [23, 70] for better encoding spatial context."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "cf6d377daf58526f3fc575079289ae225dbd2bba",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-02881",
                    "ArXiv": "2303.02881",
                    "DOI": "10.48550/arXiv.2303.02881",
                    "CorpusId": 257365040
                },
                "corpusId": 257365040,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cf6d377daf58526f3fc575079289ae225dbd2bba",
                "title": "KBNet: Kernel Basis Network for Image Restoration",
                "abstract": "How to aggregate spatial information plays an essential role in learning-based image restoration. Most existing CNN-based networks adopt static convolutional kernels to encode spatial information, which cannot aggregate spatial information adaptively. Recent transformer-based architectures achieve adaptive spatial aggregation. But they lack desirable inductive biases of convolutions and require heavy computational costs. In this paper, we propose a kernel basis attention (KBA) module, which introduces learnable kernel bases to model representative image patterns for spatial information aggregation. Different kernel bases are trained to model different local structures. At each spatial location, they are linearly and adaptively fused by predicted pixel-wise coefficients to obtain aggregation weights. Based on the KBA module, we further design a multi-axis feature fusion (MFF) block to encode and fuse channel-wise, spatial-invariant, and pixel-adaptive features for image restoration. Our model, named kernel basis network (KBNet), achieves state-of-the-art performances on more than ten benchmarks over image denoising, deraining, and deblurring tasks while requiring less computational cost than previous SOTA methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "39184742",
                        "name": "Y. Zhang"
                    },
                    {
                        "authorId": "2108463482",
                        "name": "Dasong Li"
                    },
                    {
                        "authorId": "2119204728",
                        "name": "Xiaoyu Shi"
                    },
                    {
                        "authorId": "93174483",
                        "name": "Dailan He"
                    },
                    {
                        "authorId": "2210837359",
                        "name": "Kangning Song"
                    },
                    {
                        "authorId": "2144665580",
                        "name": "Xiaogang Wang"
                    },
                    {
                        "authorId": "46636770",
                        "name": "Hongwei Qin"
                    },
                    {
                        "authorId": "49404547",
                        "name": "Hongsheng Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "5f6b228ec6ad819f86db02bfd328d875d45820d2",
                "externalIds": {
                    "DBLP": "journals/cmpb/ChiSHYWW23",
                    "DOI": "10.1016/j.cmpb.2023.107449",
                    "CorpusId": 257244145,
                    "PubMed": "36871547"
                },
                "corpusId": 257244145,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5f6b228ec6ad819f86db02bfd328d875d45820d2",
                "title": "PILN: A posterior information learning network for blind reconstruction of lung CT images",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2686775",
                        "name": "Jianning Chi"
                    },
                    {
                        "authorId": "2185821087",
                        "name": "Zhiyi Sun"
                    },
                    {
                        "authorId": "2118234222",
                        "name": "Xiaoying Han"
                    },
                    {
                        "authorId": "9305845",
                        "name": "Xiaosheng Yu"
                    },
                    {
                        "authorId": "2113269531",
                        "name": "Huan Wang"
                    },
                    {
                        "authorId": "7513726",
                        "name": "Chengdong Wu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "ce4d2945688c717e723b295ede59a725c974c0e8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-13358",
                    "ArXiv": "2301.13358",
                    "DOI": "10.48550/arXiv.2301.13358",
                    "CorpusId": 256416394
                },
                "corpusId": 256416394,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ce4d2945688c717e723b295ede59a725c974c0e8",
                "title": "Hierarchical Disentangled Representation for Invertible Image Denoising and Beyond",
                "abstract": "Image denoising is a typical ill-posed problem due to complex degradation. Leading methods based on normalizing flows have tried to solve this problem with an invertible transformation instead of a deterministic mapping. However, the implicit bijective mapping is not explored well. Inspired by a latent observation that noise tends to appear in the high-frequency part of the image, we propose a fully invertible denoising method that injects the idea of disentangled learning into a general invertible neural network to split noise from the high-frequency part. More specifically, we decompose the noisy image into clean low-frequency and hybrid high-frequency parts with an invertible transformation and then disentangle case-specific noise and high-frequency components in the latent space. In this way, denoising is made tractable by inversely merging noiseless low and high-frequency parts. Furthermore, we construct a flexible hierarchical disentangling framework, which aims to decompose most of the low-frequency image information while disentangling noise from the high-frequency part in a coarse-to-fine manner. Extensive experiments on real image denoising, JPEG compressed artifact removal, and medical low-dose CT image restoration have demonstrated that the proposed method achieves competing performance on both quantitative metrics and visual quality, with significantly less computational cost.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2072591366",
                        "name": "Wenchao Du"
                    },
                    {
                        "authorId": "50688815",
                        "name": "Hu Chen"
                    },
                    {
                        "authorId": "39509574",
                        "name": "Yan Zhang"
                    },
                    {
                        "authorId": "2118587201",
                        "name": "H. Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "With the development of deep convolutional neural networks (CNNs), many learning-based methods [61, 62, 9, 63, 41] have been proposed for image denoising.",
                "Following RDN [63] and SADNet [9], we adopt 800 highresolution training images from the DIV2K dataset [3] to train our models for Gaussian denoising at four different noise levels (\u03c3 = 10, 30, 50, 70).",
                "To evaluate denoising performance on synthetic noisy images, we compared our proposed DUMRN with several state-of-the-art denoising methods including classical model-based methods (BM3D [13] and CBM3D [12]), deep unfolding methods (TNRD[11], CSCNet [50] and DeamNet [46]), and deep-learning based methods (DnCNN [61],\nFFDNet [62], NLRN[38], SADNet [9], DudeNet [53], COLANet [41], Neb2Neb [30], and RDN [63]).",
                "[9] incorporated multi-size dilated convolutions into a U-Net [47] structure to capture multi-scale contextual information, which helps to restore rich details in complex scenes.",
                "Benefiting from the incorporation of the physical model and deep CNNs, our model also outperforms state-of-the-art deep learning-based methods DnCNN, SADNet, COLANet, and RDN. Taking color image denoising with noise level \u03c3 = 50 as an example, our DUMRN obtains 0.65dB/0.0207, 0.45dB/0.0161,\nand 1.37dB/0.0320 improvements over DnCNN on Kodak24, CBSD68, and Urban100 respectively.",
                "FFDNet [62], NLRN[38], SADNet [9], DudeNet [53], COLANet [41], Neb2Neb [30], and RDN [63]).",
                "Recently, several methods [23, 9, 59, 58] have employed multi-scale strategies to enlarge the receptive field and improve the performance of deep networks.",
                "Like other denoising methods [61, 62, 63, 9, 41, 53], we performed data augmentation on the training images, using random flipping and rotation.",
                "To assess the effectiveness of the proposed network, we adopt the same L2 loss function as previous works [61, 62, 9, 41].",
                "Although DUMRN is a little slower than DnCNN [61], DudeNet [53], and SADNet [9], it achieves much better denoising performance."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "d114aa29f5afbd8d8ff039e54d6417c5fd795611",
                "externalIds": {
                    "DBLP": "journals/cvm/XuYYW23",
                    "DOI": "10.1007/s41095-022-0277-5",
                    "CorpusId": 249634953
                },
                "corpusId": 249634953,
                "publicationVenue": {
                    "id": "d2dfc02a-9028-4345-b6cf-556b76ac435b",
                    "name": "Computational Visual Media",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Vis Media"
                    ],
                    "issn": "2096-0433",
                    "url": "http://www.springer.com/41095",
                    "alternate_urls": [
                        "https://link.springer.com/journal/41095"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d114aa29f5afbd8d8ff039e54d6417c5fd795611",
                "title": "Deep unfolding multi-scale regularizer network for image denoising",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Jingzhao Xu"
                    },
                    {
                        "authorId": "1878816590",
                        "name": "Mengke Yuan"
                    },
                    {
                        "authorId": "2118185623",
                        "name": "Dong Yan"
                    },
                    {
                        "authorId": "2112665639",
                        "name": "Tieru Wu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "our method with CBM3D [53], DnCNN [2], FFDNet [54], IRCNN [10], DHDN [55], SADNet [56], RDN [57], and IPT [14].",
                "For the denoising, we compared our method with CBM3D [53], DnCNN [2], FFDNet [54], IRCNN [10], DHDN [55], SADNet [56], RDN [57], and IPT [14]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "1153c7c6997358aa8d393b6d3574264a8a5937aa",
                "externalIds": {
                    "DBLP": "journals/tip/KimKY23",
                    "DOI": "10.1109/TIP.2022.3226892",
                    "CorpusId": 254531409,
                    "PubMed": "37015481"
                },
                "corpusId": 254531409,
                "publicationVenue": {
                    "id": "e117fa7f-05b7-4dd6-b64b-0a0a7c0393d8",
                    "name": "IEEE Transactions on Image Processing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Image Process"
                    ],
                    "issn": "1057-7149",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=83",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1153c7c6997358aa8d393b6d3574264a8a5937aa",
                "title": "Task-Agnostic Vision Transformer for Distributed Learning of Image Processing",
                "abstract": "Recently, distributed learning approaches have been studied for using data from multiple sources without sharing them, but they are not usually suitable in applications where each client carries out different tasks. Meanwhile, Transformer has been widely explored in computer vision area due to its capability to learn the common representation through global attention. By leveraging the advantages of Transformer, here we present a new distributed learning framework for multiple image processing tasks, allowing clients to learn distinct tasks with their local data. This arises from a disentangled representation of local and non-local features using a task-specific head/tail and a task-agnostic Vision Transformer. Each client learns a translation from its own task to a common representation using the task-specific networks, while the Transformer body on the server learns global attention between the features embedded in the representation. To enable decomposition between the task-specific and common representations, we propose an alternating training strategy between clients and server. Experimental results on distributed learning for various tasks show that our method synergistically improves the performance of each client with its own data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "101482378",
                        "name": "Boah Kim"
                    },
                    {
                        "authorId": "2109216792",
                        "name": "Jeongsol Kim"
                    },
                    {
                        "authorId": "30547794",
                        "name": "Jong-Chul Ye"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following (Chang et al. 2020), our ADFNet adopts an encoder-decoder framework to pursue an effective and efficient target.",
                "Besides, multi-scale features have played an important role in the image denoising task, including two mainstream designs: global encoder-decoder architectures (Chang et al. 2020) and local multi-scale feature extraction module (Zamir et al.",
                "2019), dilated convolution (Chang et al. 2020), multi-scale design (Gou et al.",
                "To verify the effectiveness of the proposed ADFNet for Gaussian noisy images, we compare it with the existing methods include DnCNN (Zhang et al. 2017), FFDNet (Zhang, Zuo, and Zhang 2018), RNAN (Zhang et al. 2019), RIDNet (Tian, Xu, and Zuo 2020), RDN (Zhang et al. 2020), SADNet (Chang et al. 2020), DeamNet (Ren et al. 2021), P3AN (Hu et al. 2021), and MSANet (Gou et al. 2022).",
                "2020), SADNet (Chang et al. 2020), DeamNet (Ren et al."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "6f14a642033b72454afbecdb90c82872d8085dd2",
                "externalIds": {
                    "DBLP": "conf/aaai/ShenZZ23",
                    "ArXiv": "2211.12051",
                    "DOI": "10.48550/arXiv.2211.12051",
                    "CorpusId": 253761461
                },
                "corpusId": 253761461,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/6f14a642033b72454afbecdb90c82872d8085dd2",
                "title": "Adaptive Dynamic Filtering Network for Image Denoising",
                "abstract": "In image denoising networks, feature scaling is widely used to enlarge the receptive field size and reduce computational costs. This practice, however, also leads to the loss of high-frequency information and fails to consider within-scale characteristics. Recently, dynamic convolution has exhibited powerful capabilities in processing high-frequency information (e.g., edges, corners, textures), but previous works lack sufficient spatial contextual information in filter generation. To alleviate these issues, we propose to employ dynamic convolution to improve the learning of high-frequency and multi-scale features. Specifically, we design a spatially enhanced kernel generation (SEKG) module to improve dynamic convolution, enabling the learning of spatial context information with a very low computational complexity. Based on the SEKG module, we propose a dynamic convolution block (DCB) and a multi-scale dynamic convolution block (MDCB). The former enhances the high-frequency information via dynamic convolution and preserves low-frequency information via skip connections. The latter utilizes shared adaptive dynamic kernels and the idea of dilated convolution to achieve efficient multi-scale feature extraction. The proposed multi-dimension feature integration (MFI) mechanism further fuses the multi-scale features, providing precise and contextually enriched feature representations. Finally, we build an efficient denoising network with the proposed DCB and MDCB, named ADFNet. It achieves better performance with low computational complexity on real-world and synthetic Gaussian noisy datasets. The source code is available at https://github.com/it-hao/ADFNet.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2110770224",
                        "name": "Hao Shen"
                    },
                    {
                        "authorId": "2005663349",
                        "name": "Zhongliu Zhao"
                    },
                    {
                        "authorId": "2163662859",
                        "name": "Wandi Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Shao, X.; Chen, P.; Zhang, Y.; Chang, S.K. Domain Fusion CNN-LSTM for Short-Term Power Consumption Forecasting.",
                "Meng Chang [24] proposed a new adaptive denoising network (SADNet) that can effectively remove blind noise from single images.",
                "Chang, M.; Li, Q.; Feng, H.; Xu, Z. Spatial-Adaptive Network for Single Image Denoising."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "78d11482595ffca54d136ede8685882c51691b66",
                "externalIds": {
                    "PubMedCentral": "9738395",
                    "DBLP": "journals/sensors/DongLSLJC22",
                    "DOI": "10.3390/s22239043",
                    "CorpusId": 253823723,
                    "PubMed": "36501744"
                },
                "corpusId": 253823723,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/78d11482595ffca54d136ede8685882c51691b66",
                "title": "Balancing of Motor Armature Based on LSTM-ZPF Signal Processing",
                "abstract": "Signal processing is important in the balancing of the motor armature, where the balancing accuracy depends on the extraction of the signal amplitude and phase from the raw vibration signal. In this study, a motor armature dynamic balancing method based on the long short-term memory network (LSTM) and zero-phase filter (ZPF) is proposed. This method mainly focuses on the extraction accuracy of amplitude and phase from unbalanced signals of the motor armature. The ZPF is used to accurately extract the phase, while the LSTM network is trained to extract the amplitude. The proposed method combines the advantages of both methods, whereby the problems of phase shift and amplitude loss when used alone are solved, and the motor armature unbalance signal is accurately obtained. The unbalanced mass and phase are calculated using the influence coefficient method. The effectiveness of the proposed method is proven using the simulated motor armature vibration signal, and an experimental investigation is undertaken to verify the dynamic balancing method. Two amplitude evaluation metrics and three phase evaluation metrics are proposed to judge the extraction accuracy of the amplitude and phase, whereas amplitude and frequency spectrum analysis are used to judge the dynamic balancing results. The results illustrate that the proposed method has higher dynamic balancing accuracy. Moreover, it has better extraction accuracy for the amplitude and phase of unbalanced signals compared with other methods, and it has good anti-noise performance. The determination coefficient of the amplitude is 0.9999, and the average absolute error of the phase is 2.4\u00b0. The proposed method considers both fidelity and denoising, which ensuring the accuracy of armature dynamic balancing.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2176086463",
                        "name": "Ruiwen Dong"
                    },
                    {
                        "authorId": "14455712",
                        "name": "Mengxuan Li"
                    },
                    {
                        "authorId": "1401339655",
                        "name": "Ao Sun"
                    },
                    {
                        "authorId": "2191961733",
                        "name": "Zhenrong Lu"
                    },
                    {
                        "authorId": "2113405971",
                        "name": "Dong Jiang"
                    },
                    {
                        "authorId": "2157153362",
                        "name": "Weiyu Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For image denoising, the applicability of GANs has been explored in recent years (Chen et al. 2018; Kim et al. 2019; Yue et al. 2020; Chang et al. 2020; Lin et al. 2019; Marras et al. 2020).",
                "While the NSS has been broadly explored in classic denoisers, a few works have attempted to incorporate this internal image property into deep networks for image denoising  (Lefkimmiatis 2017; Xia and Chakrabarti 2020; Pl\u00f6tz and Roth 2018; Zhang et al. 2019; Liu et al. 2018; Guo et al. 2021; Xu et al. 2020; Lefkimmiatis 2018; Chang et al. 2020; Tachella et al. 2021)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "0472008f88d254595000f0f5b239c2e0aa0970d2",
                "externalIds": {
                    "DBLP": "journals/air/IzadiSH23",
                    "DOI": "10.1007/s10462-022-10305-2",
                    "CorpusId": 253559876
                },
                "corpusId": 253559876,
                "publicationVenue": {
                    "id": "ea8553fe-2467-4367-afee-c4deb3754820",
                    "name": "Artificial Intelligence Review",
                    "type": "journal",
                    "alternate_names": [
                        "Artif Intell Rev"
                    ],
                    "issn": "0269-2821",
                    "url": "https://link.springer.com/journal/10462"
                },
                "url": "https://www.semanticscholar.org/paper/0472008f88d254595000f0f5b239c2e0aa0970d2",
                "title": "Image denoising in the deep learning era",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2066074715",
                        "name": "S. Izadi"
                    },
                    {
                        "authorId": "2069910192",
                        "name": "D. Sutton"
                    },
                    {
                        "authorId": "3049056",
                        "name": "G. Hamarneh"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "eec23b4f00353dce070ff7c8993613ea001690fb",
                "externalIds": {
                    "DBLP": "journals/tcsv/GuanLLLLFYG22",
                    "DOI": "10.1109/TCSVT.2022.3182990",
                    "CorpusId": 249695681
                },
                "corpusId": 249695681,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/eec23b4f00353dce070ff7c8993613ea001690fb",
                "title": "Memory-Efficient Deformable Convolution Based Joint Denoising and Demosaicing for UHD Images",
                "abstract": "This paper introduces deformable convolution in deep learning based joint denoising and demosaicing (JDD), which yields more adaptable representation and larger receptive fields in features extraction for a superior restoration performance. However, the deformable convolution generally leads to considerable computational load and irregular memory access bottleneck, limiting its extensive deployment on edge devices. To address this issue, we develop grouping strategy and assign independent offsets to each kernel group to reduce the computation latency while keeping the accuracy. Motivated by the exploration for aggregate distribution characteristics of deformable offsets, we present the offset sharing methodology to simplify the memory access complexity of deformable convolution. As for hardware acceleration, we specially design a novel deformable matrix multiplication workflow incorporated with a deformable memory mapping unit to boost the computational throughput. The verification experiments on FPGA demonstrate that the proposed deformable convolution based JDD can restore 4K Ultra High Definition (UHD) images at 70FPS and yields significant promotion in visual effect and objective quality assessment.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2054060377",
                        "name": "Juntao Guan"
                    },
                    {
                        "authorId": "40071336",
                        "name": "R. Lai"
                    },
                    {
                        "authorId": "2170613210",
                        "name": "Yang Lu"
                    },
                    {
                        "authorId": "2110454889",
                        "name": "Yangang Li"
                    },
                    {
                        "authorId": "2155642952",
                        "name": "Huanan Li"
                    },
                    {
                        "authorId": "11068914",
                        "name": "Lichen Feng"
                    },
                    {
                        "authorId": "2118772647",
                        "name": "Yintang Yang"
                    },
                    {
                        "authorId": "2151996289",
                        "name": "Lin Gu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[6] utilized the deformable convolution to sample the spatially related features for weighting."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "569e8827cc1c07ecef0f86490f0f1535058bf92f",
                "externalIds": {
                    "DBLP": "journals/tomccap/Huang0G23",
                    "DOI": "10.1145/3569583",
                    "CorpusId": 253120380
                },
                "corpusId": 253120380,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/569e8827cc1c07ecef0f86490f0f1535058bf92f",
                "title": "FastCNN: Towards Fast and Accurate Spatiotemporal Network for HEVC Compressed Video Enhancement",
                "abstract": "Deep neural networks have achieved remarkable success in HEVC compressed video quality enhancement. However, most existing multiframe-based methods either deliver unsatisfactory results or consume a significant amount of resources to leverage temporal information of neighboring frames. For the sake of practicality, a thorough investigation of the architecture design of the video quality enhancement network regarding enhancement performance, model parameters, and running speed is essential. In this article, we first propose an efficient alignment module that can quickly and accurately aggregate the spatiotemporal information of neighboring frames. The proposed module estimates deformable offsets progressively in lower-resolution space motivated by the observation of offset correlations between adjacent pixels. Then, the quantization parameter (QP) that represents compression level prior knowledge is utilized to guide aligned feature enhancement. By combining alignment feature distillation with residual feature correction, we obtain an efficient QP attention block. To save the storage space of the network, we design a hash buffer to store QP embedding features. These efficient components allow our network to effectively exploit temporal redundancies and obtain favorable enhancement capability while maintaining a lightweight structure and fast running speed. Extensive experiments demonstrate that the proposed approach outperforms state-of-the-art methods over different QPs by up to 0.09 to 0.11 dB, whereas the inference time can be reduced by up to 69%.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48783468",
                        "name": "Zhijie Huang"
                    },
                    {
                        "authorId": "144291080",
                        "name": "Jun Sun"
                    },
                    {
                        "authorId": "3078325",
                        "name": "Xiaopeng Guo"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "c9af4b0eb79a8ae372d987ac277ae9d8292dc5af",
                "externalIds": {
                    "ArXiv": "2210.13552",
                    "DBLP": "conf/wacv/CondeVVT23",
                    "DOI": "10.1109/WACV56688.2023.00189",
                    "CorpusId": 253107864
                },
                "corpusId": 253107864,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/c9af4b0eb79a8ae372d987ac277ae9d8292dc5af",
                "title": "Perceptual Image Enhancement for Smartphone Real-Time Applications",
                "abstract": "Recent advances in camera designs and imaging pipelines allow us to capture high-quality images using smartphones. However, due to the small size and lens limitations of the smartphone cameras, we commonly find artifacts or degradation in the processed images. The most common unpleasant effects are noise artifacts, diffraction artifacts, blur, and HDR overexposure. Deep learning methods for image restoration can successfully remove these artifacts. However, most approaches are not suitable for real-time applications on mobile devices due to their heavy computation and memory requirements.In this paper, we propose LPIENet, a lightweight network for perceptual image enhancement, with the focus on deploying it on smartphones. Our experiments show that, with much fewer parameters and operations, our model can deal with the mentioned artifacts and achieve competitive performance compared with state-of-the-art methods on standard benchmarks. Moreover, to prove the efficiency and reliability of our approach, we deployed the model directly on commercial smartphones and evaluated its performance. Our model can process 2K resolution images under 1 second in mid-level commercial smartphones.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1411880229",
                        "name": "Marcos V. Conde"
                    },
                    {
                        "authorId": "1679977404",
                        "name": "Florin-Alexandru Vasluianu"
                    },
                    {
                        "authorId": "1399251138",
                        "name": "Javier Vazquez-Corral"
                    },
                    {
                        "authorId": "1732855",
                        "name": "R. Timofte"
                    }
                ]
            }
        },
        {
            "contexts": [
                "denoising [21, 56], gamma correction, tone mapping and auto-contrast) to the pipeline in the same order, except white-balancing strategies."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "04e228afd82755c1b150c656c410710bf0aff65e",
                "externalIds": {
                    "ArXiv": "2210.09090",
                    "DBLP": "conf/wacv/KinliYOK23",
                    "DOI": "10.1109/WACV56688.2023.00488",
                    "CorpusId": 252917913
                },
                "corpusId": 252917913,
                "publicationVenue": {
                    "id": "acd15a6d-3248-41fb-8439-9a40aabe5608",
                    "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "Workshop on Applications of Computer Vision",
                        "WACV",
                        "IEEE Work Conf Appl Comput Vis",
                        "Workshop Appl Comput Vis"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=2993"
                },
                "url": "https://www.semanticscholar.org/paper/04e228afd82755c1b150c656c410710bf0aff65e",
                "title": "Modeling the Lighting in Scenes as Style for Auto White-Balance Correction",
                "abstract": "Style may refer to different concepts (e.g. painting style, hairstyle, texture, color, filter, etc.) depending on how the feature space is formed. In this work, we propose a novel idea of interpreting the lighting in the single- and multi-illuminant scenes as the concept of style. To verify this idea, we introduce an enhanced auto white-balance (AWB) method that models the lighting in single- and mixed-illuminant scenes as the style factor. Our AWB method does not require any illumination estimation step, yet contains a network learning to generate the weighting maps of the images with different WB settings. Proposed network utilizes the style information, extracted from the scene by a multi-head style extraction module. AWB correction is completed after blending these weighting maps and the scene. Experiments on single- and mixed-illuminant datasets demonstrate that our proposed method achieves promising correction results when compared to the recent works. This shows that the lighting in the scenes with multiple illuminations can be modeled by the concept of style. Source code and trained models are available on https://github.com/birdortyedi/lighting-as-style-awb-correction.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1388060209",
                        "name": "Furkan Kinli"
                    },
                    {
                        "authorId": "2175250900",
                        "name": "Dogacan Yilmaz"
                    },
                    {
                        "authorId": "41079271",
                        "name": "B. \u00d6zcan"
                    },
                    {
                        "authorId": "2460659",
                        "name": "Mustafa Furkan K\u0131ra\u00e7"
                    }
                ]
            }
        },
        {
            "contexts": [
                "They generally contain several convolution layers with skip connections and non-linear activation functions, such as recursively branched deconvolutional network RBDN [21], multi-level wavelet based network MWCNN [22], feed-forward blind denoising network DnCNN [23], fast and flexible denoising network FFDNet [24] and residual spatial-adaptive denoising network SADNet [25]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "ff49d87310e164ee0caec93fbe50c707f8274682",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-09135",
                    "ArXiv": "2210.09135",
                    "DOI": "10.48550/arXiv.2210.09135",
                    "CorpusId": 252918297
                },
                "corpusId": 252918297,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ff49d87310e164ee0caec93fbe50c707f8274682",
                "title": "Gated Recurrent Unit for Video Denoising",
                "abstract": "Current video denoising methods perform temporal fusion by designing convolutional neural networks (CNN) or combine spatial denoising with temporal fusion into basic recurrent neural networks (RNNs). However, there have not yet been works which adapt gated recurrent unit (GRU) mechanisms for video denoising. In this letter, we propose a new video denoising model based on GRU, namely GRU-VD. First, the reset gate is employed to mark the content related to the current frame in the previous frame output. Then the hidden activation works as an initial spatial-temporal denoising with the help from the marked relevant content. Finally, the update gate recursively fuses the initial denoised result with previous frame output to further increase accuracy. To handle various light conditions adaptively, the noise standard deviation of the current frame is also fed to these three modules. A weighted loss is adopted to regulate initial denoising and final fusion at the same time. The experimental results show that the GRU-VD network not only can achieve better quality than state of the arts objectively and subjectively, but also can obtain satisfied subjective quality on real video.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2061501611",
                        "name": "Kai Guo"
                    },
                    {
                        "authorId": "2163110183",
                        "name": "Seungwon Choi"
                    },
                    {
                        "authorId": "3002000",
                        "name": "Jongseong Choi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also compare our method with several state-of-the-art CNN-based filters, including DnCNN [65], TNRD [8], RDN [68], SADNet [6], KPN [40], ADNet [46], and DeamNet [42].",
                "In addition, our method also achieves better accuracy than the CNN-based methods, i.e. TNRD, DnCNN, SADNet, RDN, in all the four tasks.",
                "(a) Input (b) Bilateral (c) Guided (d) NLM\n(e) BM3D (f) TNRD (g) DnCNN (h) SADNet\n(i) RDN (j) Swin (k) IPT (l) KPN\n(m) ADNet (n) DeamNet (o) RNAN (p) SwinIR\n(q) Resformer (r) Ours (s) GT\nFig.",
                "Base(\ufffd2=50) Large(\ufffd2=50)\nMethod CBSD68 Urban100 CBSD68 Urban100\nTNRD 26.64/0.762 25.51/0.804 27.25/0.765 26.30/0.814 DnCNN 26.45/0.729 25.22/0.768 27.46/0.776 26.63/0.826 SADNet 27.50/0.785 26.53/0.829 27.96/0.801 27.57/0.861 RDN 27.53/0.778 26.85/0.834 27.78/0.789 27.38/0.850 Swin 25.99/0.738 24.26/0.765 27.66/0.790 26.71/0.840 IPT 27.10/0.765 26.17/0.817 27.83/0.806 27.55/0.860 KPN 27.17/0.762 26.42/0.828 27.69/0.797 27.32/0.848 ADNet 26.33/0.756 25.69/0.808 27.32/0.762 26.53/0.828 DeamNet 27.59/0.758 27.38/0.853 27.79/0.798 27.68/0.863 RNAN 27.82/0.793 27.30/0.850 28.01/0.801 27.95/0.872 SwinIR 27.73/0.789 27.05/0.843 27.90/0.794 27.71/0.868 Resformer 27.65/0.792 27.24/0.853 27.94/0.801 27.89/0.871 Ours 27.94/0.799 27.64/0.864 28.15/0.806 28.29/0.876\nTable 9.",
                "Method Flops/G params/M memory/MB time/s/img\nTNRD 2.832 0.056 2961 0.1816 DnCNN 28.02 0.558 2175 0.0223 SADNet 14.52 3.451 1363 0.0082 RDN 278.6 5.552 13761 0.0651 Swin 77.72 1.557 9435 0.1131 IPT 573.2 176.7 13346 0.2565 KPN 40.61 27.65 2399 0.0217 ADNet 26.16 0.521 2501 0.0270 DeamNet 111.9 1.876 6077 0.0707 SwinIR 43.99 0.866 14058 0.0935 Resformer 107.9 26.12 20381 0.3003 Ours 121.7 2.425 4105 0.0898\nTable 7.",
                "Method Gaussian Bilateral Guided NLM BM3D TNRD DnCNN SADNet RDN Swin PSNR 21.98 22.39 22.23 22.14 22.41 22.85 22.49 22.97 23.03 22.31 SSIM 0.796 0.812 0.808 0.768 0.829 0.820 0.684 0.830 0.837 0.765\nMethod IPT KPN ADNet DeamNet RNAN SwinIR Resformer CRM LIME BIMEF PSNR 19.08 21.71 22.86 22.19 23.00 23.01 23.04 17.20 16.76 13.88 SSIM 0.719 0.727 0.750 0.818 0.831 0.832 0.829 0.622 0.444 0.595\nMethod SRIE MF RRM Dong JED DeepUPE RetinexNet KinD GLAD Ours PSNR 13.03 16.97 15.36 16.72 13.69 13.36 16.77 19.66 19.72 23.11 SSIM 0.607 0.505 0.654 0.479 0.658 0.465 0.429 0.821 0.685 0.849\nWe highlight the best-performing model in each metrics.",
                "We also compare our method with several state-of-the-art CNN-based ilters, including DnCNN [65], TNRD [8], RDN [68], SADNet [6], KPN [40], ADNet[46] and DeamNet[42].",
                "LIVE1 Classic5\nMethod CQL=10 CQL=20 CQL=30 CQL=40 CQL=10 CQL=20 CQL=30 CQL=40\nGaussian 25.24/0.735 26.44/0.785 26.84/0.803 27.05/0.812 27.69/0.757 28.63/0.796 28.95/0.811 29.09/0.818 Bilateral 26.07/0.766 28.52/0.842 29.83/0.872 30.73/0.889 28.48/0.785 30.60/0.843 31.75/0.868 32.45/0.881 Guided 25.96/0.742 27.67/0.790 28.45/0.809 28.95/0.820 28.08/0.762 29.41/0.801 30.06/0.818 30.42/0.827 NLM 25.69/0.749 28.06/0.830 29.37/0.865 30.29/0.886 27.82/0.767 30.14/0.841 31.51/0.872 32.46/0.889 BM3D 26.06/0.768 28.61/0.848 29.97/0.880 30.91/0.897 28.72/0.795 31.05/0.854 32.30/0.875 33.09/0.886 SA-DCT 26.63/0.778 28.83/0.848 30.03/0.879 30.88/0.897 28.88/0.795 30.91/0.853 32.13/0.879 32.99/0.894 ARCNN 26.50/0.774 28.75/0.846 30.01/0.877 30.83/0.894 28.59/0.784 30.75/0.848 32.00/0.877 32.87/0.894 TNRD 26.89/0.783 29.19/0.853 30.48/0.885 31.39/0.902 28.99/0.792 31.15/0.853 32.42/0.880 33.34/0.897 DnCNN 26.78/0.786 29.13/0.856 30.48/0.887 31.38/0.903 28.98/0.799 31.27/0.859 32.58/0.885 33.40/0.900 SADNet 27.12/0.792 29.29/0.859 30.67/0.890 31.44/0.905 29.12/0.802 31.32/0.861 32.62/0.885 33.37/0.898 RDN 27.13/0.793 29.48/0.861 30.80/0.891 31.70/0.908 29.28/0.804 31.42/0.861 32.71/0.887 33.60/0.901 Swin 26.37/0.768 28.58/0.840 29.79/0.873 30.64/0.891 28.39/0.777 30.52/0.842 31.82/0.873 32.71/0.891 IPT 22.04/0.721 19.64/0.751 17.18/0.735 16.69/0.738 20.87/0.675 19.07/0.691 16.76/0.663 15.13/0.669 KPN 27.14/0.794 29.55/0.871 30.82/0.895 31.73/0.915 29.26/0.806 31.47/0.865 32.73/0.882 33.62/0.899 ADNet 26.95/0.787 29.23/0.841 30.52/0.892 31.40/0.911 29.12/0.799 31.25/0.855 32.48/0.885 33.47/0.900 DeamNet 27.20/0.791 29.53/0.856 30.89/0.895 31.79/0.909 29.34/0.809 31.46/0.869 32.63/0.871 33.52/0.887 RNAN 27.23/0.796 29.64/0.864 30.88/0.894 31.78/0.910 29.27/0.807 31.50/0.864 32.78/0.889 33.66/0.903 SwinIR 27.22/0.794 29.60/0.862 30.90/0.892 31.81/0.908 29.26/0.803 31.48/0.861 32.79/0.887 33.68/0.902 Restormer 27.22/0.796 29.53/0.864 30.87/0.893 31.79/0.910 29.24/0.808 31.52/0.864 32.79/0.888 33.64/0.902 Ours 27.31/0.806 29.68/0.873 30.97/0.902 31.85/0.918 29.42/0.817 31.60/0.873 32.87/0.898 33.74/0.912\nWe highlight the best-performing model in each column."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a5bb41fbda148a2e70bc5d3c6c1edaeb6986fa7c",
                "externalIds": {
                    "DBLP": "journals/tomccap/XuLWHYWD23",
                    "DOI": "10.1145/3566125",
                    "CorpusId": 252782203
                },
                "corpusId": 252782203,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a5bb41fbda148a2e70bc5d3c6c1edaeb6986fa7c",
                "title": "CUR Transformer: A Convolutional Unbiased Regional Transformer for Image Denoising",
                "abstract": "Image denoising is a fundamental problem in computer vision and multimedia computation. Non-local filters are effective for image denoising. But existing deep learning methods that use non-local computation structures are mostly designed for high-level tasks, and global self-attention is usually adopted. For the task of image denoising, they have high computational complexity and have a lot of redundant computation of uncorrelated pixels. To solve this problem and combine the marvelous advantages of non-local filter and deep learning, we propose a Convolutional Unbiased Regional (CUR) transformer. Based on the prior that, for each pixel, its similar pixels are usually spatially close, our insights are that (1) we partition the image into non-overlapped windows and perform regional self-attention to reduce the search range of each pixel, and (2) we encourage pixels across different windows to communicate with each other. Based on our insights, the CUR transformer is cascaded by a series of convolutional regional self-attention (CRSA) blocks with U-style short connections. In each CRSA block, we use convolutional layers to extract the query, key, and value features, namely Q, K, and V, of the input feature. Then, we partition the Q, K, and V features into local non-overlapped windows and perform regional self-attention within each window to obtain the output feature of this CRSA block. Among different CRSA blocks, we perform the unbiased window partition by changing the partition positions of the windows. Experimental results show that the CUR transformer outperforms the state-of-the-art methods significantly on four low-level vision tasks, including real and synthetic image denoising, JPEG compression artifact reduction, and low-light image enhancement.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2153294049",
                        "name": "Kang Xu"
                    },
                    {
                        "authorId": "2109460351",
                        "name": "Weixin Li"
                    },
                    {
                        "authorId": "2187430678",
                        "name": "Xia Wang"
                    },
                    {
                        "authorId": "2115451819",
                        "name": "Xiaojie Wang"
                    },
                    {
                        "authorId": "145829312",
                        "name": "Ke Yan"
                    },
                    {
                        "authorId": "2109688007",
                        "name": "Xiaoyan Hu"
                    },
                    {
                        "authorId": "48477439",
                        "name": "Xuan Dong"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "6784a90e54ec1b37aa77f7ccd5add5cfb1f782e7",
                "externalIds": {
                    "DBLP": "journals/ijon/BaiLYLZ23",
                    "DOI": "10.1016/j.neucom.2022.09.098",
                    "CorpusId": 252475212
                },
                "corpusId": 252475212,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6784a90e54ec1b37aa77f7ccd5add5cfb1f782e7",
                "title": "MSPNet: Multi-stage progressive network for image denoising",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2141377310",
                        "name": "Yu Bai"
                    },
                    {
                        "authorId": "152524968",
                        "name": "Meiqin Liu"
                    },
                    {
                        "authorId": "145670268",
                        "name": "Chao Yao"
                    },
                    {
                        "authorId": "49043799",
                        "name": "Chunyu Lin"
                    },
                    {
                        "authorId": "2129511552",
                        "name": "Yao Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e05aca5f3d24f250596fb7a7d8cc008f0478dfaf",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-14704",
                    "ArXiv": "2208.14704",
                    "DOI": "10.1145/3503161.3547767",
                    "CorpusId": 251953567
                },
                "corpusId": 251953567,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e05aca5f3d24f250596fb7a7d8cc008f0478dfaf",
                "title": "ELMformer: Efficient Raw Image Restoration with a Locally Multiplicative Transformer",
                "abstract": "In order to get raw images of high quality for downstream Image Signal Process (ISP), in this paper we present an Efficient Locally Multiplicative Transformer called ELMformer for raw image restoration. ELMformer contains two core designs especially for raw images whose primitive attribute is single-channel. The first design is a Bi-directional Fusion Projection (BFP) module, where we consider both the color characteristics of raw images and spatial structure of single-channel. The second one is that we propose a Locally Multiplicative Self-Attention (L-MSA) scheme to effectively deliver information from the local space to relevant parts. ELMformer can efficiently reduce the computational consumption and perform well on raw image restoration tasks. Enhanced by these two core designs, ELMformer achieves the highest performance and keeps the lowest FLOPs on raw denoising and raw deblurring benchmarks compared with state-of-the-arts. Extensive experiments demonstrate the superiority and generalization ability of ELMformer. On SIDD benchmark, our method has even better denoising performance than ISP-based methods which need huge amount of additional sRGB training images.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1825715698",
                        "name": "Jiaqi Ma"
                    },
                    {
                        "authorId": "9311837",
                        "name": "Shengyuan Yan"
                    },
                    {
                        "authorId": "1720539",
                        "name": "L. Zhang"
                    },
                    {
                        "authorId": "2110529729",
                        "name": "Guoli Wang"
                    },
                    {
                        "authorId": "1737486",
                        "name": "Qian Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The background block [27] obtained good results in the image fragment [28] and the blur removal task [29]."
            ],
            "isInfluential": false,
            "intents": [
                "result"
            ],
            "citingPaper": {
                "paperId": "42d5326b08537ae327a40a1f344305bd147a58e7",
                "externalIds": {
                    "DBLP": "journals/sivp/LinLJWLCG23",
                    "DOI": "10.1007/s11760-022-02321-0",
                    "CorpusId": 251328893
                },
                "corpusId": 251328893,
                "publicationVenue": {
                    "id": "11437c2b-f0a0-4db5-ac17-56dd7a223080",
                    "name": "Signal, Image and Video Processing",
                    "type": "journal",
                    "alternate_names": [
                        "Signal Image Video Process"
                    ],
                    "issn": "1863-1703",
                    "url": "https://link.springer.com/journal/11760"
                },
                "url": "https://www.semanticscholar.org/paper/42d5326b08537ae327a40a1f344305bd147a58e7",
                "title": "Image defogging based on multi-input and multi-scale UNet",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2112752497",
                        "name": "Zhengchun Lin"
                    },
                    {
                        "authorId": "2167909183",
                        "name": "Qingxing Luo"
                    },
                    {
                        "authorId": "2143493912",
                        "name": "Yunzhi Jiang"
                    },
                    {
                        "authorId": "2031695",
                        "name": "Junchang Wang"
                    },
                    {
                        "authorId": "2118155959",
                        "name": "Siyuan Li"
                    },
                    {
                        "authorId": "2180433027",
                        "name": "Gongwen Cheng"
                    },
                    {
                        "authorId": "2097963482",
                        "name": "Zheng Genrang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "b8487058943eb5ae391590ef0325da0093767409",
                "externalIds": {
                    "DOI": "10.1016/j.gep.2022.119270",
                    "CorpusId": 251818491,
                    "PubMed": "36028213"
                },
                "corpusId": 251818491,
                "publicationVenue": {
                    "id": "dd30c070-8ca7-434d-8f3a-e47930840750",
                    "name": "Gene Expression Patterns",
                    "type": "journal",
                    "alternate_names": [
                        "Gene Expr Pattern"
                    ],
                    "issn": "1567-133X",
                    "url": "https://www.journals.elsevier.com/gene-expression-patterns",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/1567133X"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b8487058943eb5ae391590ef0325da0093767409",
                "title": "Double enhanced residual network for biological image denoising.",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2052628621",
                        "name": "Bo Fu"
                    },
                    {
                        "authorId": "2190289791",
                        "name": "Xiangyi Zhang"
                    },
                    {
                        "authorId": "2108948228",
                        "name": "Liyan Wang"
                    },
                    {
                        "authorId": "51110571",
                        "name": "Yonggong Ren"
                    },
                    {
                        "authorId": "2064354857",
                        "name": "D. Thanh"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "38572b37c6c30af11fbef57d04d0750bd2920d95",
                "externalIds": {
                    "DBLP": "journals/kbs/HuangZRTH22",
                    "DOI": "10.1016/j.knosys.2022.109776",
                    "CorpusId": 251917227
                },
                "corpusId": 251917227,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/38572b37c6c30af11fbef57d04d0750bd2920d95",
                "title": "A prior-guided deep network for real image denoising and its applications",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2110284557",
                        "name": "Jie Huang"
                    },
                    {
                        "authorId": "2146631661",
                        "name": "Zhibo Zhao"
                    },
                    {
                        "authorId": "144532998",
                        "name": "Chao Ren"
                    },
                    {
                        "authorId": "3002539",
                        "name": "Qizhi Teng"
                    },
                    {
                        "authorId": "2146255",
                        "name": "Xiaohai He"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Furthermore, several recent methods using deep CNNs[33,34,35,36,37,38] have also demonstrated promising denoising performance."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "d655c1e075f68110f526c558db8c191fa77cc7ee",
                "externalIds": {
                    "ArXiv": "2207.13879",
                    "DBLP": "journals/corr/abs-2207-13879",
                    "DOI": "10.48550/arXiv.2207.13879",
                    "CorpusId": 251135371
                },
                "corpusId": 251135371,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/d655c1e075f68110f526c558db8c191fa77cc7ee",
                "title": "Real Image Restoration via Structure-preserving Complementarity Attention",
                "abstract": "Since convolutional neural networks perform well in learning generalizable image priors from large-scale data, these models have been widely used in image denoising tasks. However, the computational complexity increases dramatically as well on complex model. In this paper, We propose a novel lightweight Complementary Attention Module, which includes a density module and a sparse module, which can cooperatively mine dense and sparse features for feature complementary learning to build an efficient lightweight architecture. Moreover, to reduce the loss of details caused by denoising, this paper constructs a gradient-based structure-preserving branch. We utilize gradient-based branches to obtain additional structural priors for denoising, and make the model pay more attention to image geometric details through gradient loss optimization.Based on the above, we propose an efficiently Unet structured network with dual branch, the visual results show that can effectively preserve the structural details of the original image, we evaluate benchmarks including SIDD and DND, where SCANet achieves state-of-the-art performance in PSNR and SSIM while significantly reducing computational cost.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2145783273",
                        "name": "Yuan-fang Zhang"
                    },
                    {
                        "authorId": "2108550321",
                        "name": "Gen Li"
                    },
                    {
                        "authorId": "2110833657",
                        "name": "Lei Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similar to many denoising methods [10,12], the plug-in denoisers d0 and d1 are designed in a multiscale manner."
            ],
            "isInfluential": false,
            "intents": [
                "result"
            ],
            "citingPaper": {
                "paperId": "93c28e42afb35b34468785d3998f02014cbe465e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-10869",
                    "ArXiv": "2207.10869",
                    "DOI": "10.48550/arXiv.2207.10869",
                    "CorpusId": 251018232
                },
                "corpusId": 251018232,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/93c28e42afb35b34468785d3998f02014cbe465e",
                "title": "Optimizing Image Compression via Joint Learning with Denoising",
                "abstract": "High levels of noise usually exist in today's captured images due to the relatively small sensors equipped in the smartphone cameras, where the noise brings extra challenges to lossy image compression algorithms. Without the capacity to tell the difference between image details and noise, general image compression methods allocate additional bits to explicitly store the undesired image noise during compression and restore the unpleasant noisy image during decompression. Based on the observations, we optimize the image compression algorithm to be noise-aware as joint denoising and compression to resolve the bits misallocation problem. The key is to transform the original noisy images to noise-free bits by eliminating the undesired noise during compression, where the bits are later decompressed as clean images. Specifically, we propose a novel two-branch, weight-sharing architecture with plug-in feature denoisers to allow a simple and effective realization of the goal with little computational cost. Experimental results show that our method gains a significant improvement over the existing baseline methods on both the synthetic and real-world datasets. Our source code is available at https://github.com/felixcheng97/DenoiseCompression.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1832280277",
                        "name": "Ka Leong Cheng"
                    },
                    {
                        "authorId": "2154871075",
                        "name": "Yueqi Xie"
                    },
                    {
                        "authorId": "2157737759",
                        "name": "Qifeng Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We then modify the original loss function of CBDNet and SADNet, i.e., L2 + LAsymmetric + LTV and L2, by attaching our proposed objective.",
                ", CBDNet [13] and SADNet [4] are availed as the generation network G(\u00b7).",
                "The most representative denoising network, i.e., FFDNet [62], as well as the state-of-theart denoising networks, i.e., CBDNet [13] and SADNet [4] are availed as the generation network G(\u00b7)."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "19f83c24c56904754be700247b416cee704d5738",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-09302",
                    "ArXiv": "2207.09302",
                    "DOI": "10.48550/arXiv.2207.09302",
                    "CorpusId": 250644264
                },
                "corpusId": 250644264,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/19f83c24c56904754be700247b416cee704d5738",
                "title": "Deep Semantic Statistics Matching (D2SM) Denoising Network",
                "abstract": "The ultimate aim of image restoration like denoising is to find an exact correlation between the noisy and clear image domains. But the optimization of end-to-end denoising learning like pixel-wise losses is performed in a sample-to-sample manner, which ignores the intrinsic correlation of images, especially semantics. In this paper, we introduce the Deep Semantic Statistics Matching (D2SM) Denoising Network. It exploits semantic features of pretrained classification networks, then it implicitly matches the probabilistic distribution of clear images at the semantic feature space. By learning to preserve the semantic distribution of denoised images, we empirically find our method significantly improves the denoising capabilities of networks, and the denoised results can be better understood by high-level vision tasks. Comprehensive experiments conducted on the noisy Cityscapes dataset demonstrate the superiority of our method on both the denoising performance and semantic segmentation accuracy. Moreover, the performance improvement observed on our extended tasks including super-resolution and dehazing experiments shows its potentiality as a new general plug-and-play component.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "77733241",
                        "name": "Kangfu Mei"
                    },
                    {
                        "authorId": "1741177",
                        "name": "Vishal M. Patel"
                    },
                    {
                        "authorId": "40372975",
                        "name": "Rui Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "On image denoising task, our IDLIR is compared with DnCNN, MLP, BM3D, CBDNet, RIDNet, AINDNet, VDN, SADNet, DANet+ and CycleISP.",
                ", RESCAN [3] , PReNet [2] , MSPFN [4] for deraining and SADNet [7] and DANet+ [8] for denoising.",
                ", MPRNet [1], PReNet [2] , RESCAN [3] and MSPFN [4] for image deraining, and HINet [5], DnCNN [6], SADNet [7] and DANet+ [8] for image denoising.",
                "Image denoising and image deraining are two representative tasks, and a variety of restoration methods have been developed, e.g., MPRNet [1], PReNet [2] , RESCAN [3] and MSPFN [4] for image deraining, and HINet [5], DnCNN [6], SADNet [7] and DANet+ [8] for image denoising.",
                "Thanks to the power of deep learning, state-of-the-art restoration networks have been developed by learning a mapping from degraded image to latent clean image, e.g., RESCAN [3] , PReNet [2] , MSPFN [4] for deraining and SADNet [7] and DANet+ [8] for denoising."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "13f3f698d5f624249077f3d9a5e636b7f216ce52",
                "externalIds": {
                    "DBLP": "conf/icmcs/MaRY22",
                    "DOI": "10.1109/ICME52920.2022.9859813",
                    "CorpusId": 251848132
                },
                "corpusId": 251848132,
                "publicationVenue": {
                    "id": "975e1529-a14b-436f-ad7e-a09d8eabc50d",
                    "name": "IEEE International Conference on Multimedia and Expo",
                    "type": "conference",
                    "alternate_names": [
                        "ICME",
                        "Int Conf Multimedia Expo",
                        "IEEE Int Conf Multimedia Expo",
                        "International Conference on Multimedia and Expo"
                    ],
                    "issn": "1945-7871",
                    "alternate_issns": [
                        "2330-7927"
                    ],
                    "url": "http://csdl2.computer.org/persagen/ProceedingByFilter.jsp?filter=I",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conferences.jsp",
                        "http://www.wikicfp.com/cfp/program?id=1418"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/13f3f698d5f624249077f3d9a5e636b7f216ce52",
                "title": "Integrating Degradation Learning into Image Restoration",
                "abstract": "Existing image restoration methods usually assume specific degradation model, e.g., linear combination of clean image and degradation map in image denoising and deraining. Benefiting from the power of deep learning, a restoration mapping can be learned from degraded image to latent clean image. In this paper, we propose to integrate degradation learning into image restoration (IDLIR), where degradation model can be learned from training samples. In particular, IDLIR is an iterative restoration framework, where latent clean image and degradation map can be extracted from current residual degraded image, and are then fused by a degradation network to reconstruct degraded image. Then the residual degradation image can be updated by computing the difference between input and reconstructed degraded images. By taking denoising and deraining as examples, IDLIR is compared with state-of-the-art methods on several benchmark datasets. IDLIR performs better than state-of-the-art methods quantitatively and qualitatively.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2183083299",
                        "name": "Mingyu Ma"
                    },
                    {
                        "authorId": "2068940407",
                        "name": "Dongwei Ren"
                    },
                    {
                        "authorId": "2182951042",
                        "name": "Yajun Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In addition, to maintain the global context information and enlarge the receptive filed, Context Block (CB) [64] is introduced which also constructs multi-scale information and makes network robust to noise.",
                "\u201cD2Conv\u201d (Dynamic Deformable Convolution, introduced in Section V-C) is utilized based on the encoded physical feature from Physics-informed Network fH\u2217 and the \u201cCB\u201d (Context Block [64]) helps keep the global context of features at the bottleneck of Restoration Network fRN .",
                "The encoder and decoder of Restoration Network is composed of HINBlocks and ResBlocks respectively adopted from HINet [65] which proposes half-instance normalization for image restoration."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "7e2c72528ca284d512eb3943316904d9a8dc578a",
                "externalIds": {
                    "ArXiv": "2206.06070",
                    "DBLP": "journals/corr/abs-2206-06070",
                    "DOI": "10.1109/TCI.2022.3233467",
                    "CorpusId": 249625965
                },
                "corpusId": 249625965,
                "publicationVenue": {
                    "id": "f2b47cba-3a35-4bd0-9e7e-fd2b23338309",
                    "name": "IEEE Transactions on Computational Imaging",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Comput Imaging"
                    ],
                    "issn": "2333-9403",
                    "url": "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6745852"
                },
                "url": "https://www.semanticscholar.org/paper/7e2c72528ca284d512eb3943316904d9a8dc578a",
                "title": "Annular Computational Imaging: Capture Clear Panoramic Images Through Simple Lens",
                "abstract": "Panoramic Annular Lens (PAL) composed of few lenses has great potential in panoramic surrounding sensing tasks for mobile and wearable devices because of its tiny size and large Field of View (FoV). However, the image quality of tiny-volume PAL confines to optical limit due to the lack of lenses for aberration correction. In this paper, we propose an Annular Computational Imaging (ACI) framework to break the optical limit of light-weight PAL design. To facilitate learning-based image restoration, we introduce a wave-based simulation pipeline for panoramic imaging and tackle the synthetic-to-real gap through multiple data distributions. The proposed pipeline can be easily adapted to any PAL with design parameters and is suitable for loose-tolerance designs. Furthermore, we design the Physics Informed Image Restoration Network (PI$^{2}$RNet) considering the physical priors of panoramic imaging and single-pass physics-informed engine. At the dataset level, we create the DIVPano dataset and the extensive experiments on it illustrate that our proposed network sets the new state of the art in the panoramic image restoration under spatially-variant degradation. In addition, the evaluation of the proposed ACI on a simple PAL with only 3 spherical lenses reveals the delicate balance between high-quality panoramic imaging and compact design. To the best of our knowledge, we are the first to explore Computational Imaging (CI) in PAL.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2113805888",
                        "name": "Qi Jiang"
                    },
                    {
                        "authorId": "2034192426",
                        "name": "Haowen Shi"
                    },
                    {
                        "authorId": "2110832828",
                        "name": "Lei Sun"
                    },
                    {
                        "authorId": "2112313423",
                        "name": "Shaohua Gao"
                    },
                    {
                        "authorId": "8689702",
                        "name": "Kailun Yang"
                    },
                    {
                        "authorId": "7200505",
                        "name": "Kaiwei Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "With the great learning capability of deep neural networks [12, 13], plenty of methods were proposed to solve the image restoration tasks [3, 25, 35, 46, 48, 49], as well as image dehazing [7, 27, 31, 36], in a supervised manner."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "9426a074b530522171354dbe3e92d12d05845adc",
                "externalIds": {
                    "DBLP": "conf/cvpr/YangWLZGT22",
                    "DOI": "10.1109/CVPR52688.2022.00208",
                    "CorpusId": 250551953
                },
                "corpusId": 250551953,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9426a074b530522171354dbe3e92d12d05845adc",
                "title": "Self-augmented Unpaired Image Dehazing via Density and Depth Decomposition",
                "abstract": "To overcome the overfitting issue of dehazing models trained on synthetic hazy-clean image pairs, many recent methods attempted to improve models' generalization ability by training on unpaired data. Most of them simply formulate dehazing and rehazing cycles, yet ignore the physical properties of the real-world hazy environment, i.e. the haze varies with density and depth. In this paper, we propose a self-augmented image dehazing framework, termed D4 (Dehazing via Decomposing transmission map into Density and Depth) for haze generation and removal. Instead of merely estimating transmission maps or clean content, the proposed framework focuses on exploring scattering coefficient and depth information contained in hazy and clean images. With estimated scene depth, our method is capable of re-rendering hazy images with different thick-nesses which further benefits the training of the dehazing network. It is worth noting that the whole training process needs only unpaired hazy and clean images, yet succeeded in recovering the scattering coefficient, depth map and clean content from a single hazy image. Comprehensive experiments demonstrate our method outperforms state-of-the-art unpaired dehazing methods with much fewer parameters and FLOPs. Our code is available at https://github.com/YaN9-Y/D4.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152919041",
                        "name": "Yang Yang"
                    },
                    {
                        "authorId": "1409848027",
                        "name": "Chaoyue Wang"
                    },
                    {
                        "authorId": "34469457",
                        "name": "Risheng Liu"
                    },
                    {
                        "authorId": "2143838857",
                        "name": "Lin Zhang"
                    },
                    {
                        "authorId": "33465926",
                        "name": "Xiaojie Guo"
                    },
                    {
                        "authorId": "2075330732",
                        "name": "Dacheng Tao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "de81a91bd1f44877637a4e27dce5fd914a4b24cf",
                "externalIds": {
                    "DBLP": "conf/cvpr/LeeCSM22",
                    "DOI": "10.1109/CVPR52688.2022.00218",
                    "CorpusId": 250961853
                },
                "corpusId": 250961853,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/de81a91bd1f44877637a4e27dce5fd914a4b24cf",
                "title": "KNN Local Attention for Image Restoration",
                "abstract": "Recent works attempt to integrate the non-local operation with CNNs or Transformer, achieving remarkable performance in image restoration tasks. The global similarity, however, has the problems of the lack of locality and the high computational complexity that is quadratic to an input resolution. The local attention mechanism alleviates these issues by introducing the inductive bias of the locality with convolution-like operators. However, by focusing only on adjacent positions, the local attention suffers from an insufficient receptive field for image restoration. In this paper, we propose a new attention mechanism for image restoration, called k-NN Image Transformer (KiT), that rectifies the above mentioned limitations. Specifically, the KiT groups k-nearest neighbor patches with locality sensitive hashing (LSH), and the grouped patches are aggregated into each query patch by performing a pair-wise local attention. In this way, the pair-wise operation establishes nonlocal connectivity while maintaining the desired properties of the local attention, i.e., inductive bias of locality and linear complexity to input resolution. The proposed method outperforms state-of-the-art restoration approaches on image denoising, deblurring and deraining benchmarks. The code will be available soon.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2110726838",
                        "name": "Hunsang Lee"
                    },
                    {
                        "authorId": "1971492765",
                        "name": "Hyesong Choi"
                    },
                    {
                        "authorId": "144442279",
                        "name": "K. Sohn"
                    },
                    {
                        "authorId": "2065130",
                        "name": "Dongbo Min"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We choose MLP [2], CBDNet [3], RIDNet [4], DANet [7], SADNet [8] and DeamNet [10] as the comparison methods and use the commonly used PSNR and SSIM as the quantitative metrics to measure the denoising performance."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "a278110808a66ba405931a3d827d89a270e87a49",
                "externalIds": {
                    "DOI": "10.1117/12.2638621",
                    "CorpusId": 249026628
                },
                "corpusId": 249026628,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a278110808a66ba405931a3d827d89a270e87a49",
                "title": "A LUNet based on large kernel attention mechanism for image denoising",
                "abstract": "U-shaped networks are widely used in the field of image denoising with their multiscale and jump connection structures in recent years. The feature extraction structures mainly used in previous works are convolutional neural networks (CNNs), but their ability to extract information at a distance is poor. In this paper, we propose a U-shaped network structure combined with large kernel attention structure for image denoising, in which the CNNs structure can effectively extract local information while the large kernel attention structure has better extraction of global information and lower computational cost compared with Transformer, through which local-global features are learned on the feature maps of the input noisy images at different scales, which can effectively enhance the performance of the network and improve the image denoising task. The performance of the network can be enhanced to improve the performance of the image denoising task.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46461525",
                        "name": "Yonggang Yao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We compare our DGUNet with several recent methods [10, 53, 76, 77, 79] and report the evaluation results (PSNR and SSIM) in Tab."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "22755044094d1eff9d2b61bf2861649446d31bf9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-13348",
                    "ArXiv": "2204.13348",
                    "DOI": "10.1109/CVPR52688.2022.01688",
                    "CorpusId": 248426764
                },
                "corpusId": 248426764,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/22755044094d1eff9d2b61bf2861649446d31bf9",
                "title": "Deep Generalized Unfolding Networks for Image Restoration",
                "abstract": "Deep neural networks (DNN) have achieved great suc-cess in image restoration. However, most DNN methods are designed as a black box, lacking transparency and inter-pretability. Although some methods are proposed to combine traditional optimization algorithms with DNN, they usually demand pre-defined degradation processes or hand-crafted assumptions, making it difficult to deal with complex and real-world applications. In this paper, we propose a Deep Generalized Unfolding Network (DGUNet) for image restoration. Concretely, without loss of interpretability, we integrate a gradient estimation strategy into the gradi-ent descent step of the Proximal Gradient Descent (PGD) algorithm, driving it to deal with complex and real-world image degradation. In addition, we design inter-stage in-formation pathways across proximal mapping in different PGD iterations to rectify the intrinsic information loss in most deep unfolding networks (DUN) through a multi-scale and spatial-adaptive way. By integrating the flexible gradi-ent descent and informative proximal mapping, we unfold the iterative PGD algorithm into a trainable DNN. Exten-sive experiments on various image restoration tasks demon-strate the superiority of our method in terms of state-of-the-art performance, interpretability, and generalizability. The source code is available at github.com/MC-E/DGUNet.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1993674386",
                        "name": "Chong Mou"
                    },
                    {
                        "authorId": "2183688138",
                        "name": "Qian Wang"
                    },
                    {
                        "authorId": "2155126959",
                        "name": "Jian Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, deep learning models [6], [9], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42] make significant advances in image denoising, yielding"
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "bb3c793a5a6d3be2454d22a76ada5b6a94ec42f5",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-01649",
                    "ArXiv": "2205.01649",
                    "DOI": "10.1109/TPAMI.2022.3167175",
                    "CorpusId": 212725053,
                    "PubMed": "35417348"
                },
                "corpusId": 212725053,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bb3c793a5a6d3be2454d22a76ada5b6a94ec42f5",
                "title": "Learning Enriched Features for Fast Image Restoration and Enhancement",
                "abstract": "Given a degraded input image, image restoration aims to recover the missing high-quality image content. Numerous applications demand effective image restoration, e.g., computational photography, surveillance, autonomous vehicles, and remote sensing. Significant advances in image restoration have been made in recent years, dominated by convolutional neural networks (CNNs). The widely-used CNN-based methods typically operate either on full-resolution or on progressively low-resolution representations. In the former case, spatial details are preserved but the contextual information cannot be precisely encoded. In the latter case, generated outputs are semantically reliable but spatially less accurate. This paper presents a new architecture with a holistic goal of maintaining spatially-precise high-resolution representations through the entire network, and receiving complementary contextual information from the low-resolution representations. The core of our approach is a multi-scale residual block containing the following key elements: (a) parallel multi-resolution convolution streams for extracting multi-scale features, (b) information exchange across the multi-resolution streams, (c) non-local attention mechanism for capturing contextual information, and (d) attention based multi-scale feature aggregation. Our approach learns an enriched set of features that combines contextual information from multiple scales, while simultaneously preserving the high-resolution spatial details. Extensive experiments on six real image benchmark datasets demonstrate that our method, named as MIRNet-v2, achieves state-of-the-art results for a variety of image processing tasks, including defocus deblurring, image denoising, super-resolution, and image enhancement. The source code and pre-trained models are available at https://github.com/swz30/MIRNetv2.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3323621",
                        "name": "Syed Waqas Zamir"
                    },
                    {
                        "authorId": "153150198",
                        "name": "Aditya Arora"
                    },
                    {
                        "authorId": "152973423",
                        "name": "Salman Hameed Khan"
                    },
                    {
                        "authorId": "145684318",
                        "name": "Munawar Hayat"
                    },
                    {
                        "authorId": "2358803",
                        "name": "F. Khan"
                    },
                    {
                        "authorId": "37144787",
                        "name": "Ming-Hsuan Yang"
                    },
                    {
                        "authorId": "144082425",
                        "name": "Ling Shao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Later the optimization strategies were substitute by deep learning methods, building on excellent results reported for in image processing tasks like denoising [40,30,39,3], deblurring [37,25,4], super-resolution [35,26,7,21], etc."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "31676d32fef7581024e5fe6b1a9b5ee044c56639",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-10562",
                    "ArXiv": "2203.10562",
                    "DOI": "10.48550/arXiv.2203.10562",
                    "CorpusId": 247594215
                },
                "corpusId": 247594215,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/31676d32fef7581024e5fe6b1a9b5ee044c56639",
                "title": "CRISPnet: Color Rendition ISP Net",
                "abstract": "Image signal processors (ISPs) are historically grown legacy software systems for reconstructing color images from noisy raw sensor measurements. They are usually composited of many heuristic blocks for denoising, demosaicking, and color restoration. Color reproduction in this context is of particular importance, since the raw colors are often severely distorted, and each smart phone manufacturer has developed their own characteristic heuristics for improving the color rendition, for example of skin tones and other visually important colors. In recent years there has been strong interest in replacing the historically grown ISP systems with deep learned pipelines. Much progress has been made in approximating legacy ISPs with such learned models. However, so far the focus of these efforts has been on reproducing the structural features of the images, with less attention paid to color rendition. Here we present CRISPnet, the first learned ISP model to specifically target color rendition accuracy relative to a complex, legacy smart phone ISP. We achieve this by utilizing both image metadata (like a legacy ISP would), as well as by learning simple global semantics based on image classification -- similar to what a legacy ISP does to determine the scene type. We also contribute a new ISP image dataset consisting of both high dynamic range monitor data, as well as real-world data, both captured with an actual cell phone ISP pipeline under a variety of lighting conditions, exposure times, and gain settings.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "102663715",
                        "name": "M. Souza"
                    },
                    {
                        "authorId": "1752192",
                        "name": "W. Heidrich"
                    }
                ]
            }
        },
        {
            "contexts": [
                "After that, more advanced denoisers [3, 6, 13, 35, 36, 39] are proposed to improve denoising performance under supervision.",
                ", U-Net [29], DnCNN [38], FFDNet [39], RIDNet [3], SANet [6], rely on numerous noisy-clean pairs, which are costly and hard to collect.",
                "However, supervised denoisers, e.g., U-Net [29], DnCNN [38], FFDNet [39], RIDNet [3], SANet [6], rely on numerous noisy-clean pairs, which are costly and hard to collect.",
                "\u2217Corresponding author With the development of neural networks, learningbased denoisers [3, 6, 13, 29, 35, 36, 38, 39] have recently shown superior performance than traditional methods [5, 8, 9, 12]."
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "3949f666dc13ee806a119e0e1e0ea7625e1b20c1",
                "externalIds": {
                    "DBLP": "conf/cvpr/WangLL022",
                    "ArXiv": "2203.06967",
                    "DOI": "10.1109/CVPR52688.2022.00207",
                    "CorpusId": 247447122
                },
                "corpusId": 247447122,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3949f666dc13ee806a119e0e1e0ea7625e1b20c1",
                "title": "Blind2Unblind: Self-Supervised Image Denoising with Visible Blind Spots",
                "abstract": "Real noisy-clean pairs on a large scale are costly and difficult to obtain. Meanwhile, supervised denoisers trained on synthetic data perform poorly in practice. Self-supervised denoisers, which learn only from single noisy images, solve the data collection problem. However, self-supervised denoising methods, especially blindspot-driven ones, suffer sizable information loss during input or network design. The absence of valuable information dramatically reduces the upper bound of denoising performance. In this paper, we propose a simple yet efficient approach called Blind2Unblind to overcome the information loss in blindspot-driven denoising methods. First, we introduce a global-aware mask mapper that enables global perception and accelerates training. The mask mapper samples all pixels at blind spots on denoised volumes and maps them to the same channel, allowing the loss function to optimize all blind spots at once. Second, we propose a revisible loss to train the denoising network and make blind spots visible. The denoiser can learn directly from raw noise images without losing information or being trapped in identity mapping. We also theoretically analyze the convergence of the revisible loss. Extensive experiments on synthetic and real-world datasets demonstrate the superior performance of our approach compared to previous work. Code is available at https://github.com/demonsjin/Blind2Unblind.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108726150",
                        "name": "Zejin Wang"
                    },
                    {
                        "authorId": "2108477514",
                        "name": "Jiazheng Liu"
                    },
                    {
                        "authorId": "2143596330",
                        "name": "Guoqing Li"
                    },
                    {
                        "authorId": "2109259932",
                        "name": "Hua Han"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Note that the FLOPs number of our method (14.1G) is much smaller than that of RDN (46.6G) or SADNet (45.8G), when the input is a 64 \u00d7 64 RGB image.",
                "state-of-the-art task-specific methods, including deraining methods (DDN [23], SPANet [67], RESCAN [43], PreNet [58], BRN [57],SPDNet [73] and PCNet [34]); demoireing methods ( DMCNN [63], MopNet [29], FHDe2Net [29], HRDN [71], WDNet [47] and MBCNN [84]); and denoising methods (DnCNN [78], FFDNet [80], RDN [81], and SADNet [9]) on PSNR.",
                "A.2 Comparison with task-specific methods\nAs shown in Table 7, we firstly compare our methods with state-of-the-art denoising methods (DnCNN [78], FFDNet [80], RDN [81], and SADNet [9]).",
                "In the supplementary material, we compare our TAPE-Net with 12\nstate-of-the-art task-specific methods, including deraining methods (DDN [23], SPANet [67], RESCAN [43], PreNet [58], BRN [57],SPDNet [73] and PCNet [34]); demoireing methods ( DMCNN [63], MopNet [29], FHDe2Net [29], HRDN [71], WDNet [47] and MBCNN [84]); and denoising methods (DnCNN [78], FFDNet [80], RDN [81], and SADNet [9]) on PSNR. Qualitative results on deraining are shown in Figs."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "03244185e343d6cc2397269c510f33433a7df502",
                "externalIds": {
                    "ArXiv": "2203.06074",
                    "DBLP": "journals/corr/abs-2203-06074",
                    "DOI": "10.48550/arXiv.2203.06074",
                    "CorpusId": 247411392
                },
                "corpusId": 247411392,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/03244185e343d6cc2397269c510f33433a7df502",
                "title": "TAPE: Task-Agnostic Prior Embedding for Image Restoration",
                "abstract": "Learning a generalized prior for natural image restoration is an important yet challenging task. Early methods mostly involved handcrafted priors including normalized sparsity, l_0 gradients, dark channel priors, etc. Recently, deep neural networks have been used to learn various image priors but do not guarantee to generalize. In this paper, we propose a novel approach that embeds a task-agnostic prior into a transformer. Our approach, named Task-Agnostic Prior Embedding (TAPE), consists of two stages, namely, task-agnostic pre-training and task-specific fine-tuning, where the first stage embeds prior knowledge about natural images into the transformer and the second stage extracts the knowledge to assist downstream image restoration. Experiments on various types of degradation validate the effectiveness of TAPE. The image restoration performance in terms of PSNR is improved by as much as 1.45dB and even outperforms task-specific algorithms. More importantly, TAPE shows the ability of disentangling generalized image priors from degraded images, which enjoys favorable transfer ability to unknown downstream tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146017850",
                        "name": "Lin Liu"
                    },
                    {
                        "authorId": "3041937",
                        "name": "Lingxi Xie"
                    },
                    {
                        "authorId": "2108250420",
                        "name": "Xiaopeng Zhang"
                    },
                    {
                        "authorId": "3325819",
                        "name": "Shanxin Yuan"
                    },
                    {
                        "authorId": "2143735670",
                        "name": "Xiangyu Chen"
                    },
                    {
                        "authorId": "38272296",
                        "name": "Wen-gang Zhou"
                    },
                    {
                        "authorId": "2108508109",
                        "name": "Houqiang Li"
                    },
                    {
                        "authorId": "1400120070",
                        "name": "Qi Tian"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Specifically, MSANet outperforms RIDNet with 2.48dB (0.0049), SADNet with 0.6dB (0.0024), DeamNet with 1.49dB (0.0073) in PSNR (SSIM) values.",
                "In brief, MSANet achieves the highest PSNR and SSIM values compared to other methods, e.g., 0.85dB, 0.1dB, 0.09dB gains in PSNR, and 0.0066, 0.0015, 0.0013 gains in SSIM over the RIDNet, SADNet, and DeamNet, respectively.",
                "In addition, [6, 11, 35] employed encoderdecoder architectures to combine the high-to-low with low-to-high resolution features through the skip-connections.",
                "To solve this problem, image denoising, as an essential step for image perception, has been extensively studied in the past decades [6, 10, 50, 51].",
                ", BM3D [10], DnCNN [50], FFDNet [51], CLEARER [12], RNAN [53], SADNet [6] and DeamNet [33].",
                "In recent, a large number of methods have been proposed and achieved state-of-the-art performance [6, 12, 33, 50, 51].",
                "3, CBDNet and PD result in residual noises and pseudo artifacts, RIDNet, SADNet and DeamNet severely destroy the textures and obtain over-smoothed results.",
                "Taking the noise level of 70 as an example, our method can achieve PSNR gains about 0.03 \u223c 0.27dB, and SSIM gains about 0.0010 \u223c 0.0111 over the stateof-the-art methods, i.e., SADNet, RNAN, and DeamNet.",
                "SADNet [6] proposed residual spatial-adaptive block and multi-scale context block to constitute a denoising network.",
                "For comparisons, we choose seven representative denoising methods, i.e., BM3D [10], DnCNN [50], FFDNet [51], CLEARER [12], RNAN [53], SADNet [6] and DeamNet [33].",
                "For comparisons, we compare MSANet with 10 denoising methods, i.e., CDnCNN-B, CBM3D [9], FFDNet+, CBDNet, N3Net [32], PD [54], PR [47], RIDNet [4], SADNet and DeamNet, and use the corresponding pretrained models provided by their authors and refer to their results reported in the online submission system and papers.",
                "With the popularity of deep neural networks, various network architectures have been designed and achieved state-of-the-art performance [6, 33, 50, 51]."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "d839dea604fa2c42dc0ec64b44446e8beca32b5e",
                "externalIds": {
                    "ArXiv": "2203.04313",
                    "DBLP": "conf/nips/GouHLZP22",
                    "DOI": "10.48550/arXiv.2203.04313",
                    "CorpusId": 247319040
                },
                "corpusId": 247319040,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d839dea604fa2c42dc0ec64b44446e8beca32b5e",
                "title": "Multi-Scale Adaptive Network for Single Image Denoising",
                "abstract": "Multi-scale architectures have shown effectiveness in a variety of tasks thanks to appealing cross-scale complementarity. However, existing architectures treat different scale features equally without considering the scale-specific characteristics, \\textit{i.e.}, the within-scale characteristics are ignored in the architecture design. In this paper, we reveal this missing piece for multi-scale architecture design and accordingly propose a novel Multi-Scale Adaptive Network (MSANet) for single image denoising. Specifically, MSANet simultaneously embraces the within-scale characteristics and the cross-scale complementarity thanks to three novel neural blocks, \\textit{i.e.}, adaptive feature block (AFeB), adaptive multi-scale block (AMB), and adaptive fusion block (AFuB). In brief, AFeB is designed to adaptively preserve image details and filter noises, which is highly expected for the features with mixed details and noises. AMB could enlarge the receptive field and aggregate the multi-scale information, which meets the need of contextually informative features. AFuB devotes to adaptively sampling and transferring the features from one scale to another scale, which fuses the multi-scale features with varying characteristics from coarse to fine. Extensive experiments on both three real and six synthetic noisy image datasets show the superiority of MSANet compared with 12 methods. The code could be accessed from https://github.com/XLearning-SCU/2022-NeurIPS-MSANet.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1781457837",
                        "name": "Yuanbiao Gou"
                    },
                    {
                        "authorId": "2111802431",
                        "name": "Peng Hu"
                    },
                    {
                        "authorId": "2075420316",
                        "name": "Jiancheng Lv"
                    },
                    {
                        "authorId": "9245535",
                        "name": "Xiaocui Peng"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", AWGN, read noise), while the model from [14, 26, 27, 28, 29] have the ability to process the real-world signal-dependent noise (e."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c3c90955b75e540e885f6517de8436ac95e02d98",
                "externalIds": {
                    "ArXiv": "2203.01645",
                    "DBLP": "journals/corr/abs-2203-01645",
                    "DOI": "10.23919/EUSIPCO55093.2022.9909521",
                    "CorpusId": 247223095
                },
                "corpusId": 247223095,
                "publicationVenue": {
                    "id": "ebcbaf26-62a6-4cb2-b637-b29091ca04d6",
                    "name": "European Signal Processing Conference",
                    "type": "conference",
                    "alternate_names": [
                        "EUSIPCO",
                        "Eur Signal Process Conf"
                    ],
                    "issn": "2076-1465",
                    "alternate_issns": [
                        "2219-5491"
                    ],
                    "url": "https://www.eurasip.org/index.php?Itemid=89&id=80&option=com_content&view=article",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conhome/1801907/all-proceedings"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c3c90955b75e540e885f6517de8436ac95e02d98",
                "title": "Selective Residual M-Net for Real Image Denoising",
                "abstract": "Image restoration is a low-level vision task which is to restore degraded images to noise-free images. With the success of deep neural networks, the convolutional neural networks surpass the traditional restoration methods and become the main-stream in the computer vision area. To advance the performance of denoising algorithms, we propose a blind real image denoising network (SRMNet) by employing a hierarchical architecture improved from U-Net. Specifically, we use a selective kernel with residual block on the hierarchical structure called M-Net to enrich the multi-scale semantic information. Furthermore, our SRMNet has competitive performance results on two synthetic and two real-world noisy datasets in terms of quantitative metrics and visual quality. The source code and pretrained model are available at https://github.com/FanChiMao/SRMNet.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2153500901",
                        "name": "Chi-Mao Fan"
                    },
                    {
                        "authorId": "2110028901",
                        "name": "Tsung-Jung Liu"
                    },
                    {
                        "authorId": "3006921",
                        "name": "Kuan-Hsien Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "4a7e0d46f3a2ef3d8093c80fda53cd35df3ee3d3",
                "externalIds": {
                    "DBLP": "journals/ijon/SuXY22",
                    "DOI": "10.1016/j.neucom.2022.02.046",
                    "CorpusId": 247134149
                },
                "corpusId": 247134149,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4a7e0d46f3a2ef3d8093c80fda53cd35df3ee3d3",
                "title": "A survey of deep learning approaches to image restoration",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2116965320",
                        "name": "Jingwen Su"
                    },
                    {
                        "authorId": "2112878392",
                        "name": "Boyan Xu"
                    },
                    {
                        "authorId": "1709042",
                        "name": "Hujun Yin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "1c57aeeacd1cd0f3d6b3954f0ead2ed447a86c15",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-06944",
                    "ArXiv": "2205.06944",
                    "DOI": "10.3390/electronics11030418",
                    "CorpusId": 246439101
                },
                "corpusId": 246439101,
                "publicationVenue": {
                    "id": "ccd8e532-73c6-414f-bc91-271bbb2933e2",
                    "name": "Electronics",
                    "type": "journal",
                    "issn": "1450-5843",
                    "alternate_issns": [
                        "2079-9292",
                        "0883-4989"
                    ],
                    "url": "http://www.electronics.etfbl.net/",
                    "alternate_urls": [
                        "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-247562",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-247562",
                        "https://www.mdpi.com/journal/electronics"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1c57aeeacd1cd0f3d6b3954f0ead2ed447a86c15",
                "title": "Dense residual Transformer for image denoising",
                "abstract": "Image denoising is an important low-level computer vision task, which aims to reconstruct a noise-free and high-quality image from a noisy image. With the development of deep learning, convolutional neural network (CNN) has been gradually applied and achieved great success in image denoising, image compression, image enhancement, etc. Recently, Transformer has been a hot technique, which is widely used to tackle computer vision tasks. However, few Transformer-based methods have been proposed for low-level vision tasks. In this paper, we proposed an image denoising network structure based on Transformer, which is named DenSformer. DenSformer consists of three modules, including a preprocessing module, a local-global feature extraction module, and a reconstruction module. Specifically, the local-global feature extraction module consists of several Sformer groups, each of which has several ETransformer layers and a convolution layer, together with a residual connection. These Sformer groups are densely skip-connected to fuse the feature of different layers, and they jointly capture the local and global information from the given noisy images. We conduct our model on comprehensive experiments. In synthetic noise removal, DenSformer outperforms other state-of-the-art methods by up to 0.06\u20130.28 dB in gray-scale images and 0.57\u20131.19 dB in color images. In real noise removal, DenSformer can achieve comparable performance, while the number of parameters can be reduced by up to 40%. Experimental results prove that our DenSformer achieves improvement compared to some state-of-the-art methods, both for the synthetic noise data and real noise data, in the objective and subjective evaluations.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145670268",
                        "name": "Chao Yao"
                    },
                    {
                        "authorId": "2108913671",
                        "name": "Shuo Jin"
                    },
                    {
                        "authorId": "152524968",
                        "name": "Meiqin Liu"
                    },
                    {
                        "authorId": "2151825065",
                        "name": "Xiaojuan Ban"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
                "externalIds": {
                    "DBLP": "conf/cvpr/TuTZYMBL22",
                    "ArXiv": "2201.02973",
                    "DOI": "10.1109/CVPR52688.2022.00568",
                    "CorpusId": 245837508
                },
                "corpusId": 245837508,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fbf68eb0cf8237cef59ebdb301569c79b9676ff9",
                "title": "MAXIM: Multi-Axis MLP for Image Processing",
                "abstract": "Recent progress on Transformers and multilayer perceptron (MLP) models provide new network architectural designs for computer vision tasks. Although these models proved to be effective in many vision tasks such as image recognition, there remain challenges in adapting them for lowlevel vision. The inflexibility to support high-resolution images and limitations of local attention are perhaps the main bottlenecks. In this work, we present a multi-axis MLP based architecture called MAXIM, that can serve as an efficient and flexible general-purpose vision backbone for image processing tasks. MAXIM uses a UNet-shaped hierarchical structure and supports long-range interactions enabled by spatially-gated MLPs. Specifically, MAXIM contains two MLP-based building blocks: a multi-axis gated MLP that allows for efficient and scalable spatial mixing of local and global visual cues, and a cross-gating block, an alternative to cross-attention, which accounts for cross-feature conditioning. Both these modules are exclusively based on MLPs, but also benefit from being both global and \u2018fully-convolutional\u2019, two properties that are desirable for image processing. Our extensive experimental results show that the proposed MAXIM model achieves state-of-the-art performance on more than ten benchmarks across a range of image processing tasks, including denoising, deblurring, de raining, dehazing, and enhancement while requiring fewer or comparable numbers of parameters and FLOPs than competitive models. The source code and trained models will be available at https://github.com/google-research/maxim.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40992714",
                        "name": "Zhengzhong Tu"
                    },
                    {
                        "authorId": "1980498754",
                        "name": "Hossein Talebi"
                    },
                    {
                        "authorId": "2119079641",
                        "name": "Han Zhang"
                    },
                    {
                        "authorId": "1454990616",
                        "name": "Feng Yang"
                    },
                    {
                        "authorId": "1718280",
                        "name": "P. Milanfar"
                    },
                    {
                        "authorId": "1747569",
                        "name": "A. Bovik"
                    },
                    {
                        "authorId": "2110412307",
                        "name": "Yinxiao Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "1f5048ba20afffacb1d32ab48834ffb8607e039e",
                "externalIds": {
                    "DOI": "10.1109/IUCC-CIT-DSCI-SmartCNS55181.2021.00079",
                    "CorpusId": 247230423
                },
                "corpusId": 247230423,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1f5048ba20afffacb1d32ab48834ffb8607e039e",
                "title": "MMPDNet: Multi-Stage & Multi-Attention Progressive Image Denoising",
                "abstract": "Deep convolutional neural networks (CNNs) have gained great success in many low-level tasks of computer vision, especially in the task of image denoising. However, there are two major drawbacks for denoising tasks, one is that it is very hard to choose a strategy to make deeper CNN convergence, other is that mainstream approaches do not do well a complex balance between spatial details and high-level features information while reconstructing clean images. In this paper, we proposed a novel multi-stage and multi-attention architecture of CNN for image denoising. In detail, our model is a multi-stage network, in each stage, we separate degraded inputs into different patches to learn the reconstruct mapping, thus, the overall denoising process is divided into several easily convergence steps. Meanwhile, a channel-spatial attention module is proposed to learn high-level features from different dimensions. Furthermore, an effectively supervised module is employed between every two-stage to refine the reconstructed results. The resulting multi-stage and multi-attention architecture, named MMPDNet, and its extensive experiment results deliver strong performance gains on some primary real-word denoising datasets, including SIDD and DND.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2157249395",
                        "name": "Jiangbo Xue"
                    },
                    {
                        "authorId": "2157312303",
                        "name": "Jiu Liang"
                    },
                    {
                        "authorId": "1491630395",
                        "name": "Jinhe He"
                    },
                    {
                        "authorId": "2153636379",
                        "name": "Yu Zhang"
                    },
                    {
                        "authorId": "2157272026",
                        "name": "Yanda Hu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Noises suffered from demosaicing procedure gradually deminish after iterately soving Equation (5).",
                "We iterately conduct the demosaic and denoising algorithm according to Equation (5)."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "023be485af5a37c2a337820af8a28005101f63cc",
                "externalIds": {
                    "DOI": "10.1117/12.2606869",
                    "CorpusId": 244677857
                },
                "corpusId": 244677857,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/023be485af5a37c2a337820af8a28005101f63cc",
                "title": "A plug-and-play demosaicing and denoising method on mobile cameras",
                "abstract": "Image demosaicing and denoising are two important processes in the ISP pipeline of mobile cameras, because almost all mobile cameras in use today require colorful images generated by demosaicing algorithm, and the small sensor area of mobile cameras triggers low signal-to-noise ratio. Over the years, a considerable number of sequential demosaicing and denoising methods have been proposed, while they su\ufb00er from estimating the noise distribution and adjusting the hyper-parameters in order to balance demosaicing and denoising. There exit simultaneous demosaicing and denoising methods solving these problems. But they lack guidelines designed for mobile cameras. We propose a Plug-and-Play (PnP) demosaicing and denoising method on mobile cameras. Our method is built on PnP demosaicing framework which is derived from variable splitting theory. Any color demosaicing algorithm (i.e., bilinear, Malvar) can be plugged into our framework. We novelly trained an ISO conditioned denoiser for the framework and iteratively apply the denoiser in it. The ISO conditioned denoiser not only removes noise from the demosaicing procedure itself but also noise from camera sensors. By introducing ISO settings to the denoiser, our method takes possession of the adaptability and robustness in various capturing environments under di\ufb00erent camera settings. Our method has only two hyperparameters to tune, which eases the hyper-parameter adjustment in sequential demosaicing and denoising methods. Extensive experiments on synthetic datasets show that our method performs better than sequential demosaicing and denoising methods and is practical for mobile cameras.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "13032398",
                        "name": "Haoying Li"
                    },
                    {
                        "authorId": "2142729891",
                        "name": "Honghao Chen"
                    },
                    {
                        "authorId": "144405454",
                        "name": "Meng Chang"
                    },
                    {
                        "authorId": "46854729",
                        "name": "H. Feng"
                    },
                    {
                        "authorId": "48559708",
                        "name": "Zhi-hai Xu"
                    },
                    {
                        "authorId": "40422449",
                        "name": "Qi Li"
                    },
                    {
                        "authorId": "51511855",
                        "name": "Yue-ting Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "1, 2 [6] Meng Chang, Qi Li, Huajun Feng, and Zhihai Xu."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "1e88d5afe19aea324d33541f60a90b7036894c32",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-09881",
                    "ArXiv": "2111.09881",
                    "DOI": "10.1109/CVPR52688.2022.00564",
                    "CorpusId": 244346144
                },
                "corpusId": 244346144,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1e88d5afe19aea324d33541f60a90b7036894c32",
                "title": "Restormer: Efficient Transformer for High-Resolution Image Restoration",
                "abstract": "Since convolutional neural networks (CNNs) perform well at learning generalizable image priors from large-scale data, these models have been extensively applied to image restoration and related tasks. Recently, another class of neural architectures, Transformers, have shown significant performance gains on natural language and high-level vision tasks. While the Transformer model mitigates the shortcomings of CNNs (i.e., limited receptive field and inadaptability to input content), its computational complexity grows quadratically with the spatial resolution, therefore making it infeasible to apply to most image restoration tasks involving high-resolution images. In this work, we propose an efficient Transformer model by making several key designs in the building blocks (multi-head attention and feed-forward network) such that it can capture long-range pixel interactions, while still remaining applicable to large images. Our model, named Restoration Transformer (Restormer), achieves state-of-the-art results on several image restoration tasks, including image deraining, single-image motion deblurring, defocus deblurring (single-image and dual-pixel data), and image denoising (Gaussian grayscale/color denoising, and real image denoising). The source code and pre-trained models are available at https://github.com/swz30/Restormer.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3323621",
                        "name": "Syed Waqas Zamir"
                    },
                    {
                        "authorId": "153150198",
                        "name": "Aditya Arora"
                    },
                    {
                        "authorId": "152973423",
                        "name": "Salman Hameed Khan"
                    },
                    {
                        "authorId": "145684318",
                        "name": "Munawar Hayat"
                    },
                    {
                        "authorId": "2358803",
                        "name": "F. Khan"
                    },
                    {
                        "authorId": "37144787",
                        "name": "Ming-Hsuan Yang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "4c089f9dfd003005a0c83a8c2e57fda75f6fb22b",
                "externalIds": {
                    "DOI": "10.1155/2021/4464985",
                    "CorpusId": 243475357
                },
                "corpusId": 243475357,
                "publicationVenue": {
                    "id": "28795e86-51fc-402b-ac17-8ab964fd1d7a",
                    "name": "Advances in Mathematical Physics",
                    "type": "journal",
                    "alternate_names": [
                        "Adv Math Phys"
                    ],
                    "issn": "1687-9120",
                    "url": "https://www.hindawi.com/journals/amp/"
                },
                "url": "https://www.semanticscholar.org/paper/4c089f9dfd003005a0c83a8c2e57fda75f6fb22b",
                "title": "Adaptive Extraction of Oil Painting Texture Features Based on Reaction Diffusion Equation",
                "abstract": "The oil painting retrieval technology based on the reaction diffusion equation has attracted widespread attention in the fields of oil painting processing and pattern recognition. The description and extraction of oil painting information and the classification method of oil paintings are two important processes in content-based oil painting retrieval. Inspired by the restoration and decomposition functional model of equal oil painting, we propose a reaction diffusion equation model. The new model contains two reaction diffusion equations with different principal parts. One principal part is total variation diffusion, which is used to remove noise. The other main part is thermal diffusion, which is used to modify the source term of the denoising reaction-diffusion equation to achieve the effect of protecting the texture of the oil painting. The interaction of the two reaction-diffusion equations finally achieves denoising while maintaining the boundaries and textures. Under the framework of the above reaction diffusion equation model, we introduce Laplace flow to replace the original total variation flow, so that the new denoising reaction diffusion equation combines the isotropic diffusion and total variation flow of the thermal reaction diffusion equation to achieve the effect of adaptive theoretical research. Using regularization methods and methods, we, respectively, get the well-posedness of the two model solutions, which provides the necessary preparation for numerical calculations. Based on the statistical theory and classification principles of support vector machines, combined with the characteristics of oil painting classification, the research and analysis are carried out from the three important aspects of kernel function, training algorithm, and multiclass classifier algorithm that affect the classification effect and speed. Numerical experiments show that the given filter model has a better processing effect on images with different types and different degrees of noise pollution. On this basis, an oil painting classification system based on texture features is designed, combined with an improved gray-level cooccurrence matrix algorithm and a multiclass support vector machine classification model, to extract, train, and classify oil paintings. Experiments with three types of oil paintings prove that the system can achieve a good oil painting classification effect. Different from the original model, the new model is based on the framework of reaction-diffusion equations. In addition, the new model has good effects in removing step effects, maintaining boundaries and denoising, especially in maintaining texture.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1505808003",
                        "name": "Qicai Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "RIDNet [29], SADNet [51], MIRNet-v2 [52].",
                "We also compare our DDS-Net with other state-of-theart static denoising methods: FFDNet [50], CBDNet [28], RIDNet [29], SADNet [51], MIRNet-v2 [52].",
                "Besides, the visual results of DDS-Nets are comparable to SADNet and MIRNet-v2."
            ],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "e4b8654e932c3950fe876340927195af67a98b45",
                "externalIds": {
                    "ArXiv": "2110.08940",
                    "DBLP": "journals/corr/abs-2110-08940",
                    "DOI": "10.1109/TIP.2023.3246792",
                    "CorpusId": 239016845,
                    "PubMed": "37027761"
                },
                "corpusId": 239016845,
                "publicationVenue": {
                    "id": "e117fa7f-05b7-4dd6-b64b-0a0a7c0393d8",
                    "name": "IEEE Transactions on Image Processing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Image Process"
                    ],
                    "issn": "1057-7149",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=83",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e4b8654e932c3950fe876340927195af67a98b45",
                "title": "Dynamic Slimmable Denoising Network",
                "abstract": "Recently, tremendous human-designed and automatically searched neural networks have been applied to image denoising. However, previous works intend to handle all noisy images in a pre-defined static network architecture, which inevitably leads to high computational complexity for good denoising quality. Here, we present a dynamic slimmable denoising network (DDS-Net), a general method to achieve good denoising quality with less computational complexity, via dynamically adjusting the channel configurations of networks at test time with respect to different noisy images. Our DDS-Net is empowered with the ability of dynamic inference by a dynamic gate, which can predictively adjust the channel configuration of networks with negligible extra computation cost. To ensure the performance of each candidate sub-network and the fairness of the dynamic gate, we propose a three-stage optimization scheme. In the first stage, we train a weight-shared slimmable super network. In the second stage, we evaluate the trained slimmable super network in an iterative way and progressively tailor the channel numbers of each layer with minimal denoising quality drop. By a single pass, we can obtain several sub-networks with good performance under different channel configurations. In the last stage, we identify easy and hard samples in an online way and train a dynamic gate to predictively select the corresponding sub-network with respect to different noisy images. Extensive experiments demonstrate our DDS-Net consistently outperforms the state-of-the-art individually trained static denoising networks.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "10705544",
                        "name": "Zutao Jiang"
                    },
                    {
                        "authorId": "46651877",
                        "name": "Changlin Li"
                    },
                    {
                        "authorId": "144950946",
                        "name": "Xiaojun Chang"
                    },
                    {
                        "authorId": "2237811",
                        "name": "Jihua Zhu"
                    },
                    {
                        "authorId": "2143684966",
                        "name": "Yi Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following the works proposed by [46, 8], Deformable ResBlocks are employed in each scale of the decoder (as shown in the bottom right of Fig."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "4971c15476999d9635602daad7d1bae9a3c0a70d",
                "externalIds": {
                    "DBLP": "conf/iccv/ChenFGXC21",
                    "DOI": "10.1109/ICCV48922.2021.00263",
                    "CorpusId": 244462373
                },
                "corpusId": 244462373,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/4971c15476999d9635602daad7d1bae9a3c0a70d",
                "title": "Extreme-Quality Computational Imaging via Degradation Framework",
                "abstract": "To meet the space limitation of optical elements, free-form surfaces or high-order aspherical lenses are adopted in mobile cameras to compress volume. However, the application of free-form surfaces also introduces the problem of image quality mutation. Existing model-based deconvolution methods are inefficient in dealing with the degradation that shows a wide range of spatial variants over regions. And the deep learning techniques in low-level and physics-based vision suffer from a lack of accurate data. To address this issue, we develop a degradation framework to estimate the spatially variant point spread functions (PSFs) of mobile cameras. When input extreme-quality digital images, the proposed framework generates degraded images sharing a common domain with real-world photographs. Supplied with the synthetic image pairs, we design a Field-Of-View shared kernel prediction network (FOV-KPN) to perform spatial-adaptive reconstruction on real degraded photos. Extensive experiments demonstrate that the proposed approach achieves extreme-quality computational imaging and outperforms the state-of-the-art methods. Furthermore, we illustrate that our technique can be integrated into existing postprocessing systems, resulting in significantly improved visual quality.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2141309413",
                        "name": "Shiqi Chen"
                    },
                    {
                        "authorId": "46854729",
                        "name": "H. Feng"
                    },
                    {
                        "authorId": "2135715431",
                        "name": "Keming Gao"
                    },
                    {
                        "authorId": "48559708",
                        "name": "Zhi-hai Xu"
                    },
                    {
                        "authorId": "51511855",
                        "name": "Yue-ting Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In terms of the hyperparameter of training, our settings are basically the same as [Chang et al. 2020].",
                "2008], denoising [Buades et al. 2005; Chang et al. 2020; Condat 2010; Foi et al. 2008], white balancing [Gijsenij et al.",
                "Therefore, we introduce a context block [Chang et al. 2020] into the minimum scale between encoder and decoder, which increases the receptive field and reconstructs multiscale information without further downsampling.",
                "\u2026image signal processing subtasks, such as image demosaicing [Dubois 2006; Hirakawa and Parks 2005; Li et al. 2008], denoising [Buades et al. 2005; Chang et al. 2020; Condat 2010; Foi et al. 2008], white balancing [Gijsenij et al. 2012; Schwartzburg et al. 2014; van de Weijer et al. 2007], and\u2026",
                "Therefore, we introduce a\ncontext block [Chang et al. 2020] into the minimum scale between encoder and decoder, which increases the receptive field and reconstructs multiscale information without further downsampling.",
                "While different from the method in [Chang et al. 2020], we directly concatenate the input and the upsampled offset."
            ],
            "isInfluential": true,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "45fc81aa26f99523ee8ba1f1a410c82e881dd703",
                "externalIds": {
                    "ArXiv": "2305.05867",
                    "DBLP": "journals/tog/ChenFPXLC21",
                    "MAG": "3204952166",
                    "DOI": "10.1145/3474088",
                    "CorpusId": 244220835
                },
                "corpusId": 244220835,
                "publicationVenue": {
                    "id": "aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                    "name": "ACM Transactions on Graphics",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Trans Graph"
                    ],
                    "issn": "0730-0301",
                    "url": "http://www.acm.org/tog/",
                    "alternate_urls": [
                        "http://portal.acm.org/tog",
                        "https://tog.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/45fc81aa26f99523ee8ba1f1a410c82e881dd703",
                "title": "Optical Aberrations Correction in Postprocessing Using Imaging Simulation",
                "abstract": "As the popularity of mobile photography continues to grow, considerable effort is being invested in the reconstruction of degraded images. Due to the spatial variation in optical aberrations, which cannot be avoided during the lens design process, recent commercial cameras have shifted some of these correction tasks from optical design to postprocessing systems. However, without engaging with the optical parameters, these systems only achieve limited correction for aberrations. In this work, we propose a practical method for recovering the degradation caused by optical aberrations. Specifically, we establish an imaging simulation system based on our proposed optical point spread function model. Given the optical parameters of the camera, it generates the imaging results of these specific devices. To perform the restoration, we design a spatial-adaptive network model on synthetic data pairs generated by the imaging simulation system, eliminating the overhead of capturing training data by a large amount of shooting and registration. Moreover, we comprehensively evaluate the proposed method in simulations and experimentally with a customized digital-single-lens-reflex camera lens and HUAWEI HONOR 20, respectively. The experiments demonstrate that our solution successfully removes spatially variant blur and color dispersion. When compared with the state-of-the-art deblur methods, the proposed approach achieves better results with a lower computational overhead. Moreover, the reconstruction technique does not introduce artificial texture and is convenient to transfer to current commercial cameras. Project Page: https://github.com/TanGeeGo/ImagingSimulation.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2141309413",
                        "name": "Shiqi Chen"
                    },
                    {
                        "authorId": "46854729",
                        "name": "H. Feng"
                    },
                    {
                        "authorId": "51478556",
                        "name": "Dexin Pan"
                    },
                    {
                        "authorId": "48559708",
                        "name": "Zhi-hai Xu"
                    },
                    {
                        "authorId": "40422449",
                        "name": "Qi Li"
                    },
                    {
                        "authorId": "51511855",
                        "name": "Yue-ting Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Deep Learning With the rapid development of computing power, deep learning technology has led to many breakthroughs in the field of vision, including low-level denoising [10,30,48], deblurring [15,31], super-resolution [32,52] and high-level recognition [53], segmentation [54].",
                "SADNet [10] introduced the deformable convolution to implement spatial",
                "Many works based on deep convolutional neural networks (DCNNs) have achieved excellent results [10,23,25]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "80014ff5474af950cdd259d97cfa3f64d4d39aa1",
                "externalIds": {
                    "MAG": "3197977908",
                    "DOI": "10.3390/photonics8090376",
                    "CorpusId": 239634650
                },
                "corpusId": 239634650,
                "publicationVenue": {
                    "id": "98ba351e-edaa-4625-97de-de4d134f4c3e",
                    "name": "Photonics",
                    "type": "journal",
                    "issn": "2304-6732",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-364027",
                    "alternate_urls": [
                        "https://www.mdpi.com/journal/photonics",
                        "https://www.mdpi.com/2304-6732/4/2/37",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-364027"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/80014ff5474af950cdd259d97cfa3f64d4d39aa1",
                "title": "Image Restoration Based on End-to-End Unrolled Network",
                "abstract": "Recent studies on image restoration (IR) methods under unrolled optimization frameworks have shown that deep convolutional neural networks (DCNNs) can be implicitly used as priors to solve inverse problems. Due to the ill-conditioned nature of the inverse problem, the selection of prior knowledge is crucial for the process of IR. However, the existing methods use a fixed DCNN in each iteration, and so they cannot fully adapt to the image characteristics at each iteration stage. In this paper, we combine deep learning with traditional optimization and propose an end-to-end unrolled network based on deep priors. The entire network contains several iterations, and each iteration is composed of analytic solution updates and a small multiscale deep denoiser network. In particular, we use different denoiser networks at different stages to improve adaptability. Compared with a fixed DCNN, it greatly reduces the number of computations when the total parameters are equal and the number of iterations is the same, but the gains from a practical runtime are not as significant as indicated in the FLOP count. The experimental results of our method of three IR tasks, including denoising, deblurring, and lensless imaging, demonstrate that our proposed method achieves state-of-the-art performances in terms of both visual effects and quantitative evaluations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2146848877",
                        "name": "Xiaoping Tao"
                    },
                    {
                        "authorId": "2111824809",
                        "name": "Hao Zhou"
                    },
                    {
                        "authorId": "51511855",
                        "name": "Yue-ting Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "6fbe765ae06b6ad726de7683577ffce34f39862a",
                "externalIds": {
                    "MAG": "3193629242",
                    "DBLP": "journals/kbs/HuangHDQLQS21",
                    "DOI": "10.1016/J.KNOSYS.2021.107384",
                    "CorpusId": 238689445
                },
                "corpusId": 238689445,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6fbe765ae06b6ad726de7683577ffce34f39862a",
                "title": "Learning Deformable and Attentive Network for image restoration",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144093630",
                        "name": "Yuan Huang"
                    },
                    {
                        "authorId": "1837363",
                        "name": "Xingsong Hou"
                    },
                    {
                        "authorId": "3126881",
                        "name": "Yujie Dun"
                    },
                    {
                        "authorId": "144411970",
                        "name": "Jie Qin"
                    },
                    {
                        "authorId": "40241836",
                        "name": "Li Liu"
                    },
                    {
                        "authorId": "6468417",
                        "name": "Xueming Qian"
                    },
                    {
                        "authorId": "144082425",
                        "name": "Ling Shao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The results of [62], [64], [65], [66] are obtained from their reported results and those of [52], [57], [60], [61], [67], [68] are obtained from [57].",
                "These approaches perform well on\nTABLE 4 Quantitative Evaluations for the Natural Image Denoising Problem on the SIDD Validation Dataset [63] in Terms of PSNR\nMethods BM3D [52] DnCNN [67] FFDNet [60] CBDNet [61] RIDNet [68] IERD [57] GradNet [64] AINDNet [62] DANet [65] SADNet [66] SVLRM\nAvg.",
                "Methods BM3D [52] DnCNN [67] FFDNet [60] CBDNet [61] RIDNet [68] IERD [57] GradNet [64] AINDNet [62] DANet [65] SADNet [66] SVLRM"
            ],
            "isInfluential": false,
            "intents": [
                "result",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "117ed042304665e6c89ac48aaa7f4e1720e2b032",
                "externalIds": {
                    "DBLP": "journals/pami/DongPRLTY22",
                    "DOI": "10.1109/TPAMI.2021.3102575",
                    "CorpusId": 236946135,
                    "PubMed": "34357863"
                },
                "corpusId": 236946135,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/117ed042304665e6c89ac48aaa7f4e1720e2b032",
                "title": "Learning Spatially Variant Linear Representation Models for Joint Filtering",
                "abstract": "Joint filtering mainly uses an additional guidance image as a prior and transfers its structures to the target image in the filtering process. Different from existing approaches that rely on local linear models or hand-designed objective functions to extract the structural information from the guidance image, we propose a new joint filtering method based on a spatially variant linear representation model (SVLRM), where the target image is linearly represented by the guidance image. However, learning SVLRMs for vision tasks is a highly ill-posed problem. To estimate the spatially variant linear representation coefficients, we develop an effective approach based on a deep convolutional neural network (CNN). As such, the proposed deep CNN (constrained by the SVLRM) is able to model the structural information of both the guidance and input images. We show that the proposed approach can be effectively applied to a variety of applications, including depth/RGB image upsampling and restoration, flash deblurring, natural image denoising, and scale-aware filtering. In addition, we show that the linear representation model can be extended to high-order representation models (e.g., quadratic and cubic polynomial representations). Extensive experimental results demonstrate that the proposed method performs favorably against the state-of-the-art methods that have been specifically designed for each task.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3367248",
                        "name": "Jiangxin Dong"
                    },
                    {
                        "authorId": "9416881",
                        "name": "Jin-shan Pan"
                    },
                    {
                        "authorId": "145335572",
                        "name": "Jimmy S. J. Ren"
                    },
                    {
                        "authorId": "1737218",
                        "name": "Liang Lin"
                    },
                    {
                        "authorId": "8053308",
                        "name": "Jinhui Tang"
                    },
                    {
                        "authorId": "37144787",
                        "name": "Ming-Hsuan Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For the evaluation of the cascaded designs, we tested seven handcrafted or learning-based denoisers, namely BM3D [89], DnCNN [90], FFDNet [91], CBDNet [92], GRDN [78], SADNet [93], and CycleISP [74]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "0e117fa041ce0803d4753c37d335861107b33d9c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-14844",
                    "ArXiv": "2106.14844",
                    "DOI": "10.1109/TIP.2022.3155948",
                    "CorpusId": 235658182,
                    "PubMed": "35259104"
                },
                "corpusId": 235658182,
                "publicationVenue": {
                    "id": "e117fa7f-05b7-4dd6-b64b-0a0a7c0393d8",
                    "name": "IEEE Transactions on Image Processing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Image Process"
                    ],
                    "issn": "1057-7149",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=83",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0e117fa041ce0803d4753c37d335861107b33d9c",
                "title": "Progressive Joint Low-Light Enhancement and Noise Removal for Raw Images",
                "abstract": "Low-light imaging on mobile devices is typically challenging due to insufficient incident light coming through the relatively small aperture, resulting in low image quality. Most of the previous works on low-light imaging focus either only on a single task such as illumination adjustment, color enhancement, or noise removal; or on a joint illumination adjustment and denoising task that heavily relies on short-long exposure image pairs from specific camera models. These approaches are less practical and generalizable in real-world settings where camera-specific joint enhancement and restoration is required. In this paper, we propose a low-light imaging framework that performs joint illumination adjustment, color enhancement, and denoising to tackle this problem. Considering the difficulty in model-specific data collection and the ultra-high definition of the captured images, we design two branches: a coefficient estimation branch and a joint operation branch. The coefficient estimation branch works in a low-resolution space and predicts the coefficients for enhancement via bilateral learning, whereas the joint operation branch works in a full-resolution space and progressively performs joint enhancement and denoising. In contrast to existing methods, our framework does not need to recollect massive data when adapted to another camera model, which significantly reduces the efforts required to fine-tune our approach for practical usage. Through extensive experiments, we demonstrate its great potential in real-world low-light imaging applications.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50028690",
                        "name": "Yucheng Lu"
                    },
                    {
                        "authorId": "30993544",
                        "name": "Seung\u2010Won Jung"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[6], in order to better estimate the current offset values, we transfer the offset values obtained in the last offset block {\u0394plast,\u0394mlast} to the current offset block (the purple line in Figs."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "b2afd1d819221bcf747806135139939e4c1fa54b",
                "externalIds": {
                    "DBLP": "journals/vc/LuFZ021",
                    "DOI": "10.1007/s00371-021-02204-4",
                    "CorpusId": 237435328
                },
                "corpusId": 237435328,
                "publicationVenue": {
                    "id": "9a037417-d032-481a-bb54-de987ec2138b",
                    "name": "The Visual Computer",
                    "type": "journal",
                    "alternate_names": [
                        "Vis Comput"
                    ],
                    "issn": "0178-2789",
                    "url": "https://link.springer.com/journal/371"
                },
                "url": "https://www.semanticscholar.org/paper/b2afd1d819221bcf747806135139939e4c1fa54b",
                "title": "Denoising Monte Carlo renderings via a multi-scale featured dual-residual GAN",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2141538586",
                        "name": "Yifan Lu"
                    },
                    {
                        "authorId": "10693159",
                        "name": "Siyuan Fu"
                    },
                    {
                        "authorId": "2128029743",
                        "name": "Xiao Hua Zhang"
                    },
                    {
                        "authorId": "145833207",
                        "name": "Ning Xie"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[5] proposed a novel spatial-adaptive denoising network for efficient single image noise removal.",
                "The learning-based methods focus on learning a latent mapping from the noisy image to the clean version, and can be divided into traditional learning-based [14, 9, 46, 33, 36] and deep network-based methods [31, 62, 41, 22, 18, 6, 17, 5, 13]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "7515d56bf86c279b394c27f68fd73346db54b572",
                "externalIds": {
                    "DBLP": "conf/cvpr/0002HWZ21",
                    "DOI": "10.1109/CVPR46437.2021.00849",
                    "CorpusId": 235702920
                },
                "corpusId": 235702920,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7515d56bf86c279b394c27f68fd73346db54b572",
                "title": "Adaptive Consistency Prior based Deep Network for Image Denoising",
                "abstract": "Recent studies have shown that deep networks can achieve promising results for image denoising. However, how to simultaneously incorporate the valuable achievements of traditional methods into the network design and improve network interpretability is still an open problem. To solve this problem, we propose a novel model-based denoising method to inform the design of our denoising network. First, by introducing a non-linear filtering operator, a reliability matrix, and a high-dimensional feature transformation function into the traditional consistency prior, we propose a novel adaptive consistency prior (ACP). Second, by incorporating the ACP term into the maximum a posteriori framework, a model-based denoising method is proposed. This method is further used to inform the network design, leading to a novel end-to-end trainable and interpretable deep denoising network, called DeamNet. Note that the unfolding process leads to a promising module called dual element-wise attention mechanism (DEAM) module. To the best of our knowledge, both our ACP constraint and DEAM module have not been reported in the previous literature. Extensive experiments verify the superiority of DeamNet on both synthetic and real noisy image datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144532998",
                        "name": "Chao Ren"
                    },
                    {
                        "authorId": "2146255",
                        "name": "Xiaohai He"
                    },
                    {
                        "authorId": "2108719586",
                        "name": "Chuncheng Wang"
                    },
                    {
                        "authorId": "2146631661",
                        "name": "Zhibo Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "4575ca5bcb688f3fd90f2464d082b9c22f965aba",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-10967",
                    "ArXiv": "2105.10967",
                    "DOI": "10.1109/CVPR46437.2021.00571",
                    "CorpusId": 235166372
                },
                "corpusId": 235166372,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4575ca5bcb688f3fd90f2464d082b9c22f965aba",
                "title": "FBI-Denoiser: Fast Blind Image Denoiser for Poisson-Gaussian Noise",
                "abstract": "We consider the challenging blind denoising problem for Poisson-Gaussian noise, in which no additional information about clean images or noise level parameters is available. Particularly, when only \"single\" noisy images are available for training a denoiser, the denoising performance of existing methods was not satisfactory. Recently, the blind pixelwise affine image denoiser (BP-AIDE) was proposed and significantly improved the performance in the above setting, to the extent that it is competitive with denoisers which utilized additional information. However, BP-AIDE seriously suffered from slow inference time due to the inefficiency of noise level estimation procedure and that of the blind-spot network (BSN) architecture it used. To that end, we propose Fast Blind Image Denoiser (FBI-Denoiser) for Poisson-Gaussian noise, which consists of two neural network models; 1) PGE-Net that estimates Poisson-Gaussian noise parameters 2000 times faster than the conventional methods and 2) FBI-Net that realizes a much more efficient BSN for pixelwise affine denoiser in terms of the number of parameters and inference speed. Consequently, we show that our FBI-Denoiser blindly trained solely based on single noisy images can achieve the state-of-the-art performance on several real-world noisy image benchmark datasets with much faster inference time (\u00d7 10), compared to BP-AIDE.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1820774436",
                        "name": "Jaeseok Byun"
                    },
                    {
                        "authorId": "34352481",
                        "name": "Sungmin Cha"
                    },
                    {
                        "authorId": "4842965",
                        "name": "Taesup Moon"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "9b8cbe9a636b00b3543d8e1f98c24de12d6c3d17",
                "externalIds": {
                    "DBLP": "conf/cvpr/0002LZCC21",
                    "ArXiv": "2105.06086",
                    "DOI": "10.1109/CVPRW53098.2021.00027",
                    "CorpusId": 234482841
                },
                "corpusId": 234482841,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/9b8cbe9a636b00b3543d8e1f98c24de12d6c3d17",
                "title": "HINet: Half Instance Normalization Network for Image Restoration",
                "abstract": "In this paper, we explore the role of Instance Normalization in low-level vision tasks. Specifically, we present a novel block: Half Instance Normalization Block (HIN Block), to boost the performance of image restoration networks. Based on HIN Block, we design a simple and powerful multi-stage network named HINet, which consists of two subnetworks. With the help of HIN Block, HINet surpasses the state-of-the-art (SOTA) on various image restoration tasks. For image denoising, we exceed it 0.11dB and 0.28 dB in PSNR on SIDD dataset, with only 7.5% and 30% of its multiplier-accumulator operations (MACs), 6.8\u00d7 and 2.9\u00d7 speedup respectively. For image deblurring, we get comparable performance with 22.5% of its MACs and 3.3\u00d7 speedup on REDS and GoPro datasets. For image deraining, we exceed it by 0.3 dB in PSNR on the average result of multiple datasets with 1.4\u00d7 speedup. With HINet, we won the 1st place on the NTIRE 2021 Image Deblurring Challenge - Track2. JPEG Artifacts, with a PSNR of 29.70.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "49330296",
                        "name": "Liangyu Chen"
                    },
                    {
                        "authorId": "2124828026",
                        "name": "Xin Lu"
                    },
                    {
                        "authorId": "40539618",
                        "name": "J. Zhang"
                    },
                    {
                        "authorId": "2091913855",
                        "name": "Xiaojie Chu"
                    },
                    {
                        "authorId": "2145775927",
                        "name": "Chengpeng Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Denoising is a widely studied research topic [6, 13], however the numerous works recently proposed [41, 25, 27, 9] suggest that the interest towards this problem is still very active, especially in the more challenging case of real raw data [4, 21, 23, 10].",
                "These methods most notably leverage residual learning [41], wavelet decomposition [27], attention mechanisms [25], and spatially adaptive processing [9]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "20119ff9829d400fa2d864e5b4434f7f7bea26bd",
                "externalIds": {
                    "DBLP": "conf/cvpr/MaggioniHLXFS21",
                    "ArXiv": "2103.05407",
                    "DOI": "10.1109/CVPR46437.2021.00347",
                    "CorpusId": 232168694
                },
                "corpusId": 232168694,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/20119ff9829d400fa2d864e5b4434f7f7bea26bd",
                "title": "Efficient Multi-Stage Video Denoising with Recurrent Spatio-Temporal Fusion",
                "abstract": "In recent years, denoising methods based on deep learning have achieved unparalleled performance at the cost of large computational complexity. In this work, we propose an Efficient Multi-stage Video Denoising algorithm, called EMVD, to drastically reduce the complexity while maintaining or even improving the performance. First, a fusion stage reduces the noise through a recursive combination of all past frames in the video. Then, a denoising stage removes the noise in the fused frame. Finally, a refinement stage restores the missing high frequency in the denoised frame. All stages operate on a transform-domain representation obtained by learnable and invertible linear operators which simultaneously increase accuracy and decrease complexity of the model. A single loss on the final output is sufficient for successful convergence, hence making EMVD easy to train. Experiments on real raw data demonstrate that EMVD outperforms the state of the art when complexity is constrained, and even remains competitive against methods whose complexities are several orders of magnitude higher. Further, the low complexity and memory requirements of EMVD enable real-time video denoising on commercial SoC in mobile devices.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48098273",
                        "name": "Matteo Maggioni"
                    },
                    {
                        "authorId": "9221057",
                        "name": "Yibin Huang"
                    },
                    {
                        "authorId": "2029513284",
                        "name": "Cheng Li"
                    },
                    {
                        "authorId": "2112869469",
                        "name": "Shuai Xiao"
                    },
                    {
                        "authorId": "2068056090",
                        "name": "Zhongqian Fu"
                    },
                    {
                        "authorId": "2059836292",
                        "name": "Fenglong Song"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "837ac4ed6825502f0460caec45e12e734c85b113",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2102-04906",
                    "ArXiv": "2102.04906",
                    "DOI": "10.1109/TPAMI.2021.3117837",
                    "CorpusId": 231855426,
                    "PubMed": "34613907"
                },
                "corpusId": 231855426,
                "publicationVenue": {
                    "id": "25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                    "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Pattern Anal Mach Intell"
                    ],
                    "issn": "0162-8828",
                    "url": "http://www.computer.org/tpami/",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tpami",
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=34"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/837ac4ed6825502f0460caec45e12e734c85b113",
                "title": "Dynamic Neural Networks: A Survey",
                "abstract": "Dynamic neural network is an emerging research topic in deep learning. Compared to static models which have fixed computational graphs and parameters at the inference stage, dynamic networks can adapt their structures or parameters to different inputs, leading to notable advantages in terms of accuracy, computational efficiency, adaptiveness, etc. In this survey, we comprehensively review this rapidly developing area by dividing dynamic networks into three main categories: 1) sample-wise dynamic models that process each sample with data-dependent architectures or parameters; 2) spatial-wise dynamic networks that conduct adaptive computation with respect to different spatial locations of image data; and 3) temporal-wise dynamic models that perform adaptive inference along the temporal dimension for sequential data such as videos and texts. The important research problems of dynamic networks, e.g., architecture design, decision making scheme, optimization technique and applications, are reviewed systematically. Finally, we discuss the open problems in this field together with interesting future research directions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "40961502",
                        "name": "Yizeng Han"
                    },
                    {
                        "authorId": "143983679",
                        "name": "Gao Huang"
                    },
                    {
                        "authorId": "1760750",
                        "name": "Shiji Song"
                    },
                    {
                        "authorId": "2118929751",
                        "name": "Le Yang"
                    },
                    {
                        "authorId": "2109566342",
                        "name": "Honghui Wang"
                    },
                    {
                        "authorId": "39041697",
                        "name": "Yulin Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our method obtains considerable gains over the state-of-the-art approaches, i.e., 0.19 dB over CycleISP [86] on SIDD and 0.21 dB over SADNet [11] on DND. Note that the DND dataset does not contain any training images, i.e., the complete publicly released dataset is just a test set.",
                "Noisy Image CycleISP [86] AINDNet [40] DANet [85] SADNet [11] MPRNet (Ours)",
                "Reference Noisy RIDNet [4] AINDNet [40] VDN [84] SADNet [11] CycleISP [86] DANet [85] MPRNet (Ours)"
            ],
            "isInfluential": true,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "92d50602db5746f03b91562e2cc8a98bec584e9b",
                "externalIds": {
                    "ArXiv": "2102.02808",
                    "DBLP": "journals/corr/abs-2102-02808",
                    "DOI": "10.1109/CVPR46437.2021.01458",
                    "CorpusId": 231802205
                },
                "corpusId": 231802205,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/92d50602db5746f03b91562e2cc8a98bec584e9b",
                "title": "Multi-Stage Progressive Image Restoration",
                "abstract": "Image restoration tasks demand a complex balance between spatial details and high-level contextualized information while recovering images. In this paper, we propose a novel synergistic design that can optimally balance these competing goals. Our main proposal is a multi-stage architecture, that progressively learns restoration functions for the degraded inputs, thereby breaking down the overall recovery process into more manageable steps. Specifically, our model first learns the contextualized features using encoder-decoder architectures and later combines them with a high-resolution branch that retains local information. At each stage, we introduce a novel per-pixel adaptive design that leverages in-situ supervised attention to reweight the local features. A key ingredient in such a multi-stage architecture is the information exchange between different stages. To this end, we propose a two-faceted approach where the information is not only exchanged sequentially from early to late stages, but lateral connections between feature processing blocks also exist to avoid any loss of information. The resulting tightly interlinked multi-stage architecture, named as MPRNet, delivers strong performance gains on ten datasets across a range of tasks including image deraining, deblurring, and denoising. The source code and pre-trained models are available at https://github.com/swz30/MPRNet.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3323621",
                        "name": "Syed Waqas Zamir"
                    },
                    {
                        "authorId": "153150198",
                        "name": "Aditya Arora"
                    },
                    {
                        "authorId": "152973423",
                        "name": "Salman Hameed Khan"
                    },
                    {
                        "authorId": "145684318",
                        "name": "Munawar Hayat"
                    },
                    {
                        "authorId": "2358803",
                        "name": "F. Khan"
                    },
                    {
                        "authorId": "1715634",
                        "name": "Ming-Hsuan Yang"
                    },
                    {
                        "authorId": "144082425",
                        "name": "Ling Shao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "95042006075799ce0f7734e80584f9974b621c57",
                "externalIds": {
                    "ArXiv": "2101.08525",
                    "DBLP": "journals/corr/abs-2101-08525",
                    "CorpusId": 231662136
                },
                "corpusId": 231662136,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/95042006075799ce0f7734e80584f9974b621c57",
                "title": "GhostSR: Learning Ghost Features for Efficient Image Super-Resolution",
                "abstract": "Modern single image super-resolution (SISR) system based on convolutional neural networks (CNNs) achieves fancy performance while requires huge computational costs. The problem on feature redundancy is well studied in visual recognition task, but rarely discussed in SISR. Based on the observation that many features in SISR models are also similar to each other, we propose to use shift operation to generate the redundant features (i.e., ghost features). Compared with depth-wise convolution which is time-consuming on GPU-like devices, shift operation can bring a practical inference acceleration for CNNs on common hardwares. We analyze the benefits of shift operation on SISR task and make the shift orientation learnable based on Gumbel-Softmax trick. Besides, a clustering procedure is explored based on pre-trained models to identify the intrinsic filters for generating intrinsic features. The ghost features will be derived by moving these intrinsic features along a specific orientation. Finally, the complete output features are constructed by concatenating the intrinsic and ghost features together. Extensive experiments on several benchmark models and datasets demonstrate that both the non-compact and lightweight SISR models embedded with the proposed method can achieve a comparable performance to that of their baselines with a large reduction of parameters, FLOPs and GPU inference latency. For instance, we reduce the parameters by 46%, FLOPs by 46% and GPU inference latency by 42% of $\\times2$ EDSR network with basically lossless performance.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2055490179",
                        "name": "Ying Nie"
                    },
                    {
                        "authorId": "3826388",
                        "name": "Kai Han"
                    },
                    {
                        "authorId": "2125024057",
                        "name": "Zhenhua Liu"
                    },
                    {
                        "authorId": "1569696821",
                        "name": "An Xiao"
                    },
                    {
                        "authorId": "2115656092",
                        "name": "Yiping Deng"
                    },
                    {
                        "authorId": "1691522",
                        "name": "Chunjing Xu"
                    },
                    {
                        "authorId": "2108702980",
                        "name": "Yunhe Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "opment of deep neural networks has a significant boost for image denoising algorithms [56], [57], [58], [59], [60], [61], [62]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "b1c3d93316901c4b60ce9c8657af0d9d1b4d4345",
                "externalIds": {
                    "MAG": "3116456582",
                    "DBLP": "journals/corr/abs-2012-09859",
                    "ArXiv": "2012.09859",
                    "CorpusId": 229331654
                },
                "corpusId": 229331654,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b1c3d93316901c4b60ce9c8657af0d9d1b4d4345",
                "title": "Object Detection based on OcSaFPN in Aerial Images with Noise",
                "abstract": "Taking the deep learning-based algorithms into account has become a crucial way to boost object detection performance in aerial images. While various neural network representations have been developed, previous works are still inefficient to investigate the noise-resilient performance, especially on aerial images with noise taken by the cameras with telephoto lenses, and most of the research is concentrated in the field of denoising. Of course, denoising usually requires an additional computational burden to obtain higher quality images, while noise-resilient is more of a description of the robustness of the network itself to different noises, which is an attribute of the algorithm itself. For this reason, the work will be started by analyzing the noise-resilient performance of the neural network, and then propose two hypotheses to build a noise-resilient structure. Based on these hypotheses, we compare the noise-resilient ability of the Oct-ResNet with frequency division processing and the commonly used ResNet. In addition, previous feature pyramid networks used for aerial object detection tasks are not specifically designed for the frequency division feature maps of the Oct-ResNet, and they usually lack attention to bridging the semantic gap between diverse feature maps from different depths. On the basis of this, a novel octave convolution-based semantic attention feature pyramid network (OcSaFPN) is proposed to get higher accuracy in object detection with noise. The proposed algorithm tested on three datasets demonstrates that the proposed OcSaFPN achieves a state-of-the-art detection performance with Gaussian noise or multiplicative noise. In addition, more experiments have proved that the OcSaFPN structure can be easily added to existing algorithms, and the noise-resilient ability can be effectively improved.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2136334270",
                        "name": "Chengyuan Li"
                    },
                    {
                        "authorId": "2157176234",
                        "name": "Jun Liu"
                    },
                    {
                        "authorId": "1752545991",
                        "name": "H. Hong"
                    },
                    {
                        "authorId": "2125042978",
                        "name": "Wenju Mao"
                    },
                    {
                        "authorId": "1390842107",
                        "name": "Chenjie Wang"
                    },
                    {
                        "authorId": "103654960",
                        "name": "Chudi Hu"
                    },
                    {
                        "authorId": "1474220168",
                        "name": "X. Su"
                    },
                    {
                        "authorId": "2151264226",
                        "name": "B. Luo"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "43cb4886a8056d5005702edbc51be327542b2124",
                "externalIds": {
                    "ArXiv": "2012.00364",
                    "DBLP": "conf/cvpr/Chen000DLMX0021",
                    "MAG": "3109319753",
                    "DOI": "10.1109/CVPR46437.2021.01212",
                    "CorpusId": 227239228
                },
                "corpusId": 227239228,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/43cb4886a8056d5005702edbc51be327542b2124",
                "title": "Pre-Trained Image Processing Transformer",
                "abstract": "As the computing power of modern hardware is increasing strongly, pre-trained deep learning models (e.g., BERT, GPT-3) learned on large-scale datasets have shown their effectiveness over conventional methods. The big progress is mainly contributed to the representation ability of transformer and its variant architectures. In this paper, we study the low-level computer vision task (e.g., denoising, super-resolution and deraining) and develop a new pre-trained model, namely, image processing transformer (IPT). To maximally excavate the capability of transformer, we present to utilize the well-known ImageNet benchmark for generating a large amount of corrupted image pairs. The IPT model is trained on these images with multi-heads and multi-tails. In addition, the contrastive learning is introduced for well adapting to different image processing tasks. The pre-trained model can therefore efficiently employed on desired task after fine-tuning. With only one pre-trained model, IPT outperforms the current state-of-the-art methods on various low-level benchmarks. Code is available at https://github.com/huawei-noah/Pretrained-IPT and https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/IPT",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2118023932",
                        "name": "Hanting Chen"
                    },
                    {
                        "authorId": "2108702980",
                        "name": "Yunhe Wang"
                    },
                    {
                        "authorId": "50412584",
                        "name": "Tianyu Guo"
                    },
                    {
                        "authorId": null,
                        "name": "Chang Xu"
                    },
                    {
                        "authorId": "2115656092",
                        "name": "Yiping Deng"
                    },
                    {
                        "authorId": "2125024057",
                        "name": "Zhenhua Liu"
                    },
                    {
                        "authorId": "2217676532",
                        "name": "Siwei Ma"
                    },
                    {
                        "authorId": "1691522",
                        "name": "Chunjing Xu"
                    },
                    {
                        "authorId": "2004428678",
                        "name": "Chao Xu"
                    },
                    {
                        "authorId": "2153706048",
                        "name": "Wen Gao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Due to powerful nonlinear modeling capabilities, deep learning has become the dominant method for image denoising [8, 23, 30, 46]."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "c95a824333d81d5e2c62c024019f10c8e3124801",
                "externalIds": {
                    "ArXiv": "2011.14512",
                    "MAG": "3107650596",
                    "DBLP": "journals/corr/abs-2011-14512",
                    "CorpusId": 227228233
                },
                "corpusId": 227228233,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c95a824333d81d5e2c62c024019f10c8e3124801",
                "title": "Adaptive noise imitation for image denoising",
                "abstract": "The effectiveness of existing denoising algorithms typically relies on accurate pre-defined noise statistics or plenty of paired data, which limits their practicality. In this work, we focus on denoising in the more common case where noise statistics and paired data are unavailable. Considering that denoising CNNs require supervision, we develop a new \\textbf{adaptive noise imitation (ADANI)} algorithm that can synthesize noisy data from naturally noisy images. To produce realistic noise, a noise generator takes unpaired noisy/clean images as input, where the noisy image is a guide for noise generation. By imposing explicit constraints on the type, level and gradient of noise, the output noise of ADANI will be similar to the guided noise, while keeping the original clean background of the image. Coupling the noisy data output from ADANI with the corresponding ground-truth, a denoising CNN is then trained in a fully-supervised manner. Experiments show that the noisy data produced by ADANI are visually and statistically similar to real ones so that the denoising CNN in our method is competitive to other networks trained with external paired data.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "30655873",
                        "name": "Huangxing Lin"
                    },
                    {
                        "authorId": "2169534059",
                        "name": "Yihong Zhuang"
                    },
                    {
                        "authorId": "1950637",
                        "name": "Yue Huang"
                    },
                    {
                        "authorId": "2713947",
                        "name": "Xinghao Ding"
                    },
                    {
                        "authorId": "1841911",
                        "name": "Yizhou Yu"
                    },
                    {
                        "authorId": "1753642167",
                        "name": "Xiaoqing Liu"
                    },
                    {
                        "authorId": "143855009",
                        "name": "J. Paisley"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "66eacb4b57f5bff7ef5d22501bb801912b3cdab0",
                "externalIds": {
                    "MAG": "3105237892",
                    "DBLP": "journals/corr/abs-2011-03462",
                    "ArXiv": "2011.03462",
                    "CorpusId": 226278148
                },
                "corpusId": 226278148,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/66eacb4b57f5bff7ef5d22501bb801912b3cdab0",
                "title": "A Comprehensive Comparison of Multi-Dimensional Image Denoising Methods",
                "abstract": "Filtering multi-dimensional images such as color images, color videos, multispectral images and magnetic resonance images is challenging in terms of both effectiveness and efficiency. Leveraging the nonlocal self-similarity (NLSS) characteristic of images and sparse representation in the transform domain, the block-matching and 3D filtering (BM3D) based methods show powerful denoising performance. Recently, numerous new approaches with different regularization terms, transforms and advanced deep neural network (DNN) architectures are proposed to improve denoising quality. In this paper, we extensively compare over 60 methods on both synthetic and real-world datasets. We also introduce a new color image and video dataset for benchmarking, and our evaluations are performed from four different perspectives including quantitative metrics, visual effects, human ratings and computational cost. Comprehensive experiments demonstrate: (i) the effectiveness and efficiency of the BM3D family for various denoising tasks, (ii) a simple matrix-based algorithm could produce similar results compared with its tensor counterparts, and (iii) several DNN models trained with synthetic Gaussian noise show state-of-the-art performance on real-world color image and video datasets. Despite the progress in recent years, we discuss shortcomings and possible extensions of existing techniques. Datasets and codes for evaluation are made publicly available at this https URL.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "40843510",
                        "name": "Zhaoming Kong"
                    },
                    {
                        "authorId": "50031194",
                        "name": "Xiaowei Yang"
                    },
                    {
                        "authorId": "40901818",
                        "name": "Lifang He"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Methods SIDD DND Model ProfilePSNR SSIM PSNR SSIM # Param FLOPs Time CBM3D [4] 25.65 0.685 34.51 0.851 - - 21.49\nDnCNN [17] 23.66 0.583 32.43 0.790 0.67 175 0.22 CBDNet [22] 30.78 0.801 38.06 0.942 4.37 161 0.19 RIDNet [23] 38.71 0.951 39.26 0.953 1.5 393 0.68\nAINDNet [86] 38.95 0.952 39.37 0.951 13.76 1284 0.49 VDN [27] 39.23 0.955 39.38 0.952 7.81 168 0.20\nSADNet [87] 39.46 0.957 39.59 0.952 4.23 76 0.22 DANet [24] 39.47 0.957 39.58 0.955 9.15 59 0.12 CycleISP [88] 39.52 0.957 39.56 0.956 2.83 739 1.36 MPRNet [25] 39.62 0.958 39.80 0.954 15.74 2296 2.75\nVIRNet (Ours) 39.64 0.958 39.83 0.954 15.40 658 0.88 PNGAN [89] 40.06 0.960 40.25 0.962 15.74 2296 2.75\nFig.",
                "We compared VIRNet with several typical real-world denoising methods, including MPRNet [25], CycleISP [88], DANet [24], SADNet [87], VDN [27] and so on (see Table 4).",
                "Combining the results in Table 1 and Table 2, it should be rational to say\n9 (a) Noisy (b) VDN (c) SADNet (d) DANet (e) CycleISP (g) VIRNet (f) MPRNet\nFig."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "8f097497148173dd986fa55f5a4186315cdf1766",
                "externalIds": {
                    "ArXiv": "2008.10796",
                    "CorpusId": 250088761
                },
                "corpusId": 250088761,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8f097497148173dd986fa55f5a4186315cdf1766",
                "title": "Deep Variational Network Toward Blind Image Restoration",
                "abstract": "\u2014Blind image restoration (IR) is a common yet challenging problem in computer vision. Classical model-based methods and recent deep learning (DL)-based methods represent two different methodologies for this problem, each with their own merits and drawbacks. In this paper, we propose a novel blind image restoration method, aiming to integrate both the advantages of them. Speci\ufb01cally, we construct a general Bayesian generative model for the blind IR, which explicitly depicts the degradation process. In this proposed model, a pixel-wise non-i.i.d. Gaussian distribution is employed to \ufb01t the image noise. It is with more \ufb02exibility than the simple i.i.d. Gaussian or Laplacian distributions as adopted in most of conventional methods, so as to handle more complicated noise types contained in the image degradation. To solve the model, we design a variational inference algorithm where all the expected posteriori distributions are parameterized as deep neural networks to increase their model capability. Notably, such an inference algorithm induces a uni\ufb01ed framework to jointly deal with the tasks of degradation estimation and image restoration. Further, the degradation information estimated in the former task is utilized to guide the latter IR process. Experiments on two typical blind IR tasks, namely image denoising and super-resolution, demonstrate that the proposed method achieves superior performance over current state-of-the-arts. The source code is available at https://github.com/zsyOAOA/VIRNet.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "10159337",
                        "name": "Zongsheng Yue"
                    },
                    {
                        "authorId": "7906116",
                        "name": "Hongwei Yong"
                    },
                    {
                        "authorId": "46317290",
                        "name": "Qian Zhao"
                    },
                    {
                        "authorId": "39089563",
                        "name": "Lei Zhang"
                    },
                    {
                        "authorId": "1803714",
                        "name": "Deyu Meng"
                    },
                    {
                        "authorId": "2173920185",
                        "name": "Kwan-Yen K. Wong"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "a5cd5210d306b957076a9f751f1e23b51b524784",
                "externalIds": {
                    "DBLP": "journals/access/LeeCSM23",
                    "DOI": "10.1109/ACCESS.2023.3242556",
                    "CorpusId": 256581907
                },
                "corpusId": 256581907,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a5cd5210d306b957076a9f751f1e23b51b524784",
                "title": "Cross-Scale KNN Image Transformer for Image Restoration",
                "abstract": "Numerous image restoration approaches have been proposed based on attention mechanism, achieving superior performance to convolutional neural networks (CNNs) based counterparts. However, they do not leverage the attention model in a form fully suited to the image restoration tasks. In this paper, we propose an image restoration network with a novel attention mechanism, called cross-scale <inline-formula> <tex-math notation=\"LaTeX\">$k$ </tex-math></inline-formula>-NN image Transformer (CS-KiT), that effectively considers several factors such as locality, non-locality, and cross-scale aggregation, which are essential to image restoration. To achieve locality and non-locality, the CS-KiT builds <inline-formula> <tex-math notation=\"LaTeX\">$k$ </tex-math></inline-formula>-nearest neighbor relation of local patches and aggregates similar patches through local attention. To induce cross-scale aggregation, we ensure that each local patch embraces different scale information with scale-aware patch embedding (SPE) which predicts an input patch scale through a combination of multi-scale convolution branches. We show the effectiveness of the CS-KiT with experimental results, outperforming state-of-the-art restoration approaches on image denoising, deblurring, and deraining benchmarks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2110726838",
                        "name": "Hunsang Lee"
                    },
                    {
                        "authorId": "1971492765",
                        "name": "Hyesong Choi"
                    },
                    {
                        "authorId": "144442279",
                        "name": "K. Sohn"
                    },
                    {
                        "authorId": "2065130",
                        "name": "Dongbo Min"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Specifically, we still use the VAE [23] for image quality estimation and develop a new restoration network based on deformable network architecture like [24]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "ced9203b1d947979ef3355529ae0ee6e672663e2",
                "externalIds": {
                    "DBLP": "journals/access/YoonC23",
                    "DOI": "10.1109/ACCESS.2023.3261268",
                    "CorpusId": 257754487
                },
                "corpusId": 257754487,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ced9203b1d947979ef3355529ae0ee6e672663e2",
                "title": "JPEG Artifact Reduction Based on Deformable Offset Gating Network Controlled by a Variational Autoencoder",
                "abstract": "For the reduction of JPEG compression artifacts, there have been many methods using deep neural networks. Most of them use the JPEG compression quality factor (QF) as prior knowledge in designing and training the networks. However, since the images we get from the Internet are often recompressed, the given QF is not so informative or misleading. Also, early works validated their methods on low QFs less than 50, while recent smartphones use high QFs larger than or equal to 90. In this paper, we propose a new JPEG artifacts reduction network considering the above-stated problems. Specifically, to extract quality information from the input image itself instead of the QF provided in the header of the JPEG file, we use a variational autoencoder (VAE) and regard its latent vector as quality information. In designing the artifact reduction network, we let the network change flexibly according to the input image quality by employing a deformable offset gating (DOG) network. The gating network and VAE are merged as our overall network, dubbed DOG-VAE, where the information from the VAE is used to adjust the DOG network according to the input quality. The DOG-VAE is trained end-to-end with the QFs in the range of [10], [90]. Extensive experiments validate that our method achieves comparable results to the state-of-the-art method for monochrome images and better results for color images. Our codes are available at https://github.com/yunjh410/DOGNet.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2155237068",
                        "name": "Jeonghwan Yoon"
                    },
                    {
                        "authorId": "1707645",
                        "name": "N. Cho"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The context block comprises several dilated convolution blocks and a final fusion block, and we put it into the minimum scale between the encoder and the decoder, similar to a previous network [58]."
            ],
            "isInfluential": false,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "bee28d86b78c1d57f4e1f0133c09de3c8b725274",
                "externalIds": {
                    "DBLP": "journals/tgrs/LiZCXLFC23",
                    "DOI": "10.1109/TGRS.2023.3300549",
                    "CorpusId": 260392469
                },
                "corpusId": 260392469,
                "publicationVenue": {
                    "id": "70628d6a-97aa-4571-9701-bc0eb3989c32",
                    "name": "IEEE Transactions on Geoscience and Remote Sensing",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Geosci Remote Sens"
                    ],
                    "issn": "0196-2892",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=36",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bee28d86b78c1d57f4e1f0133c09de3c8b725274",
                "title": "Imaging Simulation and Learning-Based Image Restoration for Remote Sensing Time Delay and Integration Cameras",
                "abstract": "Time delay and integration (TDI) cameras are widely used in remote sensing areas because they capture high-resolution and high signal-to-noise ratio (SNR) images and images in low-light environments. However, the image quality captured by TDI cameras may be affected by many degradation factors, including jitter, charge transfer time mismatch, and drift angle. Moreover, compared with the single-line push-broom cameras and area gaze cameras used in remote sensing, the degraded effect of the TDI camera may accumulate during the charge accumulation process. In this article, we present a fast imaging simulation method for remote sensing TDI cameras based on image resampling that can accurately simulate the degraded image quality affected by different degradation factors. The simulated image pairs can provide a sufficient dataset for modern supervised-learning image restoration methods. In addition, we present a novel network, containing a row-attention block and row-encoder block to help resolve the row-variant blur to resolve the degraded images. We test our image restoration method on the simulated degraded image datasets and real images; the results show that the proposed method can effectively restore degraded images. Our restoration method does not rely on auxiliary information detected by high-frequency sensors or multispectral bands, and it achieves better results than other blind restoration methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2168188966",
                        "name": "Menghao Li"
                    },
                    {
                        "authorId": "2163087261",
                        "name": "Ziran Zhang"
                    },
                    {
                        "authorId": "2141309413",
                        "name": "Shiqi Chen"
                    },
                    {
                        "authorId": "48559708",
                        "name": "Zhi-hai Xu"
                    },
                    {
                        "authorId": "40422449",
                        "name": "Qi Li"
                    },
                    {
                        "authorId": "46854729",
                        "name": "H. Feng"
                    },
                    {
                        "authorId": "51511855",
                        "name": "Yue-ting Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Note that the FLOPs number of our method (14.1G) is much smaller than that of RDN (46.6G) or SADNet (45.8G), when the input is a 64 \u00d7 64 RGB image.",
                "Method DnCNN [21] FFDNet [22] RDN [23] SADNet [1] TAPE-Net (Ours) TAPE-Net-swin-L (Ours)\nPSNR/SSIM 34.31/0.892 33.26/0.890 38.70/0.901 38.41/0.900 37.90/0.896 38.76/0.901 FLOPS (G) 14.4 0.87 46.6 45.8 14.1 5.3\n2 Table S3: Quantitative comparison with the state-of-the-art deraining methods on Rain200L and Rain200H.",
                "As shown in Table S2, we firstly compare our methods with state-of-the-art denoising methods (DnCNN [21], FFDNet [22], RDN [23], and SADNet [1]).",
                "Method DnCNN [21] FFDNet [22] RDN [23] SADNet [1] TAPE-Net (Ours) TAPE-Net-swin-L (Ours) PSNR/SSIM 34."
            ],
            "isInfluential": true,
            "intents": [
                "methodology"
            ],
            "citingPaper": {
                "paperId": "93f788524dfb45fc8e4fc3a21fa6d94a66788ec5",
                "externalIds": {
                    "CorpusId": 253546897
                },
                "corpusId": 253546897,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/93f788524dfb45fc8e4fc3a21fa6d94a66788ec5",
                "title": "A Comparison with general image restoration methods",
                "abstract": "As shown in Table S2, we firstly compare our methods with state-of-the-art denoising methods (DnCNN [21], FFDNet [22], RDN [23], and SADNet [1]). Note that the FLOPs number of our method (14.1G) is much smaller than that of RDN (46.6G) or SADNet (45.8G), when the input is a 64 \u00d7 64 RGB image. We replace our backbone model (original transformer) with a specially designed transformer, Swin transformer [14] (termed as \u2018TAPE-Net-swin-L\u2019 in Table S2), which outperforms all the SOTA methods with our pre-training strategy. And we also compare with SOTA deraining methods in Table S3 and compare with SOTA demoireing methods in Table S4. Our method outperforms all these task-specific SOTA methods.",
                "year": 2022,
                "authors": []
            }
        },
        {
            "contexts": [
                "Similarly, [25] introduces deformable convolution [26] to adapt to spatial textures and edges for image denoising task."
            ],
            "isInfluential": false,
            "intents": [
                "background"
            ],
            "citingPaper": {
                "paperId": "0505f883c76e6d03d030b665771865ce918c75f6",
                "externalIds": {
                    "DBLP": "journals/access/JinWZ22",
                    "DOI": "10.1109/ACCESS.2022.3160711",
                    "CorpusId": 248268108
                },
                "corpusId": 248268108,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0505f883c76e6d03d030b665771865ce918c75f6",
                "title": "Denoising of Maritime Unmanned Aerial Vehicles Photography Based on Guidance Correlation Pixel Sampling and Aggregation",
                "abstract": "In this paper, we proposed a guidance correlation pixel Sampling and aggregation image denoising for maritime Unmanned Aerial Vehicles(UAVs) photography, providing a reliable data base for maritime reconnaissance work. The overall step is mainly composed of two parts, pixel sampling and pixel aggregation. To improve the content correlation of sampled pixels, we propose a guided sampling scheme based on the basic estimated map and extend this algorithm to the restoration of maritime UAVs image. Finally, a UAVs image denoising system is shown. Our experimental results show that the proposed algorithm can effectively remove noise and achieves 32.98dB and 31.83dB in Set12 and BSD68 datasets with less and image distortion. In the actual scene, the PSNR of our denoising algorithm has reached 35.33dB, meets the basic needs of practical vision and follow-up research.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2162970482",
                        "name": "Gang Jin"
                    },
                    {
                        "authorId": "2141318485",
                        "name": "Jianguo Wei"
                    },
                    {
                        "authorId": "27041538",
                        "name": "Jingsheng Zhai"
                    }
                ]
            }
        },
        {
            "contexts": [
                "CBM3D TNRD MLP DnCNN CBDNet SADNet RIDNet VDN AINDNet GNSCNet COLA-Net Dataset [69] [35] [70] [16] [71] [60] [25] [72] [61] [62] [31] Proposed 25.",
                "BM3D DnCNN MWCNN NLRN SADNet RIDNet AINDNet GNSCNet DudeNet GCDN DAGL COLA-Net Dataset \u03c3 [12] [16] [22] [32] [60] [25] [61] [62] [63] [64] [65] [31] Proposed 32."
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "00e185cfd8ac513f6d9c6119517786069d6f2592",
                "externalIds": {
                    "DBLP": "journals/access/HuangHDCQ22",
                    "DOI": "10.1109/ACCESS.2022.3148201",
                    "CorpusId": 246450534
                },
                "corpusId": 246450534,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/00e185cfd8ac513f6d9c6119517786069d6f2592",
                "title": "A Non-local Enhanced Network for Image Restoration",
                "abstract": "Non-local modules have been widely studied in image restoration (IR) tasks since they can learn long-range dependencies to enhance local features. However, most existing non-local modules still focus on extracting long-range dependencies within a single image or feature map. On the other hand, most IR methods simply employ a single type of non-local module in the network. A combination of various types of non-local modules to enhance local features can be more effective. In this paper, we propose a batch-wise non-local module to explore richer non-local dependencies within images. Furthermore, we combine various non-local extractors (different attention modules) with the proposed batch-wise non-local module as the Enhanced Batch-wise Non-local Attentive module (EBNA). Besides exploring richer non-local information, we build the Non-local and Local Information extracting Block (NLIB), in which we combine the EBNA with DEformable-Convolution Block (DECB) to utilize richer non-local and adaptive local information. Finally, We embed the NLIB within a U-net-like structure and build the Non-local Enhanced Network (NLENet). Extensive experiments on synthetic image denoising, real image denoising, JPEG artifacts removal, and real image super resolution tasks demonstrate that our proposed network achieves state-of-the-art performance on several IR benchmark datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144093630",
                        "name": "Yuan Huang"
                    },
                    {
                        "authorId": "1837363",
                        "name": "Xingsong Hou"
                    },
                    {
                        "authorId": "3126881",
                        "name": "Yujie Dun"
                    },
                    {
                        "authorId": "121238673",
                        "name": "Zan Chen"
                    },
                    {
                        "authorId": "6468417",
                        "name": "Xueming Qian"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "0aaa0a28754a68df9444c0dba83b523b8568724c",
                "externalIds": {
                    "DBLP": "conf/eccv/LiSML22",
                    "DOI": "10.1007/978-3-031-19800-7_30",
                    "CorpusId": 253518543
                },
                "corpusId": 253518543,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/0aaa0a28754a68df9444c0dba83b523b8568724c",
                "title": "DRCNet: Dynamic Image Restoration Contrastive Network",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2146326219",
                        "name": "Fei Li"
                    },
                    {
                        "authorId": "2121272448",
                        "name": "Lingfeng Shen"
                    },
                    {
                        "authorId": "2054302841",
                        "name": "Yang Mi"
                    },
                    {
                        "authorId": "2109653343",
                        "name": "Zhenbo Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The results on the first row are obtained from the paper [1], the second row represents our results on a similar patch.",
                "In the original paper [1], an encoder-decoder architecture consisting a residual spatial-adaptive block, namely RSAB, is proposed for removing spatially-variant and channel-dependent noise while processing larger regions in each step by utilizing deformable convolutions.",
                "Compared methods: CBM3D [18], CDnCNN-B [19], CBDNet [4], PD [3], RIDNet [5], SADNet [1].",
                "The results on the first two rows are obtained from the paper [1], the third row represents our results on the same image."
            ],
            "isInfluential": true,
            "intents": [
                "result",
                "methodology"
            ],
            "citingPaper": {
                "paperId": "23642bc2730402171c04a630fb5bea9eadc97de0",
                "externalIds": {
                    "CorpusId": 235416296
                },
                "corpusId": 235416296,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/23642bc2730402171c04a630fb5bea9eadc97de0",
                "title": "[Re] Spatial-Adaptive Network for Single Image Denoising",
                "abstract": "In this study, we present our results and experience during replicating the paper titled \u201dSpatial-Adaptive Network for Single Image Denoising\u201d. This paper proposes novel spatial-adaptive denoising architecture for efficient noise removal by leveraging the deformable convolutions to adapt spatial information (i.e. edges and textures). We have implemented the model from scratch in PyTorch framework, and then have conducted real and synthetic noise experiments on the corresponding datasets. We have achieved to reproduce the results qualitatively and quantitatively.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "102831815",
                        "name": "\u015e. Mente\u015f"
                    },
                    {
                        "authorId": "1388060209",
                        "name": "Furkan Kinli"
                    },
                    {
                        "authorId": "41079271",
                        "name": "B. \u00d6zcan"
                    },
                    {
                        "authorId": "2460659",
                        "name": "Mustafa Furkan K\u0131ra\u00e7"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "isInfluential": false,
            "intents": [],
            "citingPaper": {
                "paperId": "110e9a5422ccec9cf365694e5d5ada3558e72f43",
                "externalIds": {
                    "DBLP": "conf/prcv/BaiLYLZ21",
                    "DOI": "10.1007/978-3-030-88010-1_22",
                    "CorpusId": 239497189
                },
                "corpusId": 239497189,
                "publicationVenue": {
                    "id": "9fb7bad5-c082-4f14-b867-e34e5a11d443",
                    "name": "Chinese Conference on Pattern Recognition and Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "PRCV",
                        "Chin Conf Pattern Recognit Comput Vis"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/110e9a5422ccec9cf365694e5d5ada3558e72f43",
                "title": "ODE-Inspired Image Denoiser: An End-to-End Dynamical Denoising Network",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2141377310",
                        "name": "Yu Bai"
                    },
                    {
                        "authorId": "152524968",
                        "name": "Meiqin Liu"
                    },
                    {
                        "authorId": "145670268",
                        "name": "Chao Yao"
                    },
                    {
                        "authorId": "49043799",
                        "name": "Chunyu Lin"
                    },
                    {
                        "authorId": "2129511552",
                        "name": "Yao Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In particular, convolutional neural networks (CNN) can learn the hierarchy of complex image features, so that a variety of CNN-based methods have been developed for denoising (Zhang et al., 2017b; Chang et al., 2020), deraining (Wei et al.",
                "For the denoising, we compared our method with CBM3D (Dabov et al., 2007), DnCNN (Zhang et al., 2017a), FFDNet (Zhang et al., 2018b), IRCNN (Zhang et al., 2017b), DHDN (Park et al., 2019), and SADNet (Chang et al., 2020).",
                "\u2026can learn the hierarchy of complex image features, so that a variety of CNN-based methods have been developed for denoising (Zhang et al., 2017b; Chang et al., 2020), deraining (Wei et al., 2019; Ren et al., 2019), deblurring (Nah et al., 2017; Kupyn et al., 2019), deblocking (Li et al., 2020b;\u2026"
            ],
            "isInfluential": false,
            "intents": [
                "methodology",
                "background"
            ],
            "citingPaper": {
                "paperId": "c74f11e1fea331d7b9715c8fab7c9f1d35fe829e",
                "externalIds": {
                    "CorpusId": 254685224
                },
                "corpusId": 254685224,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c74f11e1fea331d7b9715c8fab7c9f1d35fe829e",
                "title": "P RIVACY - PRESERVING T ASK -A GNOSTIC V ISION T RANSFORMER FOR I MAGE P ROCESSING",
                "abstract": "We validate the performance of TAViT on multiple image processing tasks. Experimental results show that our multi-task distributed learning framework using the alternating training strategy outperforms the end-to-end learning of each individual task thanks to the decomposition of the task-agnostic Transformer body and task-speci\ufb01c networks. This suggests that our framework is a promising approach for learning multiple tasks with distributed privacy-sensitive data. In sum, our contributions",
                "year": 2021,
                "authors": []
            }
        }
    ]
}