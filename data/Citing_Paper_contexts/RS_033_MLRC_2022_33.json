{
    "offset": 0,
    "data": [
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "98861eea0f600ef106ea896893413131c1cd6a67",
                "externalIds": {
                    "DOI": "10.3390/info14100520",
                    "CorpusId": 262592874
                },
                "corpusId": 262592874,
                "publicationVenue": {
                    "id": "9c4992a4-b84a-42c0-97de-25e7c9412739",
                    "name": "Information",
                    "issn": "2078-2489",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-181420",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-181420",
                        "https://www.mdpi.com/journal/information"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/98861eea0f600ef106ea896893413131c1cd6a67",
                "title": "Can Triplet Loss Be Used for Multi-Label Few-Shot Classification? A Case Study",
                "abstract": "Few-shot learning is a deep learning subfield that is the focus of research nowadays. This paper addresses the research question of whether a triplet-trained Siamese network, initially designed for multi-class classification, can effectively handle multi-label classification. We conducted a case study to identify any limitations in its application. The experiments were conducted on a dataset containing Hungarian legal decisions of administrative agencies in tax matters belonging to a major legal content provider. We also tested how different Siamese embeddings compare on classifying a previously non-existing label on a binary and a multi-label setting. We found that triplet-trained Siamese networks can be applied to perform classification but with a sampling restriction during training. We also found that the overlap between labels affects the results negatively. The few-shot model, seeing only ten examples for each label, provided competitive results compared to models trained on tens of thousands of court decisions using tf-idf vectorization and logistic regression.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "40610474",
                        "name": "G. Cs\u00e1nyi"
                    },
                    {
                        "authorId": "2127193505",
                        "name": "Ren\u00e1t\u00f3 V\u00e1gi"
                    },
                    {
                        "authorId": "2148363070",
                        "name": "Andrea Megyeri"
                    },
                    {
                        "authorId": "2246102052",
                        "name": "Anna F\u00fcl\u00f6p\u00a0"
                    },
                    {
                        "authorId": "2132037834",
                        "name": "D\u00e1niel Nagy"
                    },
                    {
                        "authorId": "2127194051",
                        "name": "J\u00e1nos P\u00e1l Vad\u00e1sz"
                    },
                    {
                        "authorId": "2148377303",
                        "name": "Istv\u00e1n \u00dcveges"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Recent studies [20, 35] about the verbalizer have proposed one-to-many mapping with similar words from the external knowledge base, e."
            ],
            "citingPaper": {
                "paperId": "a97e10228662b0159692a64edc69e2acf5394318",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-09363",
                    "ArXiv": "2308.09363",
                    "DOI": "10.48550/arXiv.2308.09363",
                    "CorpusId": 261030524
                },
                "corpusId": 261030524,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a97e10228662b0159692a64edc69e2acf5394318",
                "title": "Open-vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models",
                "abstract": "Video Question Answering (VideoQA) is a challenging task that entails complex multi-modal reasoning. In contrast to multiple-choice VideoQA which aims to predict the answer given several options, the goal of open-ended VideoQA is to answer questions without restricting candidate answers. However, the majority of previous VideoQA models formulate open-ended VideoQA as a classification task to classify the video-question pairs into a fixed answer set, i.e., closed-vocabulary, which contains only frequent answers (e.g., top-1000 answers). This leads the model to be biased toward only frequent answers and fail to generalize on out-of-vocabulary answers. We hence propose a new benchmark, Open-vocabulary Video Question Answering (OVQA), to measure the generalizability of VideoQA models by considering rare and unseen answers. In addition, in order to improve the model's generalization power, we introduce a novel GNN-based soft verbalizer that enhances the prediction on rare and unseen answers by aggregating the information from their similar words. For evaluation, we introduce new baselines by modifying the existing (closed-vocabulary) open-ended VideoQA models and improve their performances by further taking into account rare and unseen answers. Our ablation studies and qualitative analyses demonstrate that our GNN-based soft verbalizer further improves the model performance, especially on rare and unseen answers. We hope that our benchmark OVQA can serve as a guide for evaluating the generalizability of VideoQA models and inspire future research. Code is available at https://github.com/mlvlab/OVQA.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2136072008",
                        "name": "Dohwan Ko"
                    },
                    {
                        "authorId": "2232661859",
                        "name": "Ji Soo Lee"
                    },
                    {
                        "authorId": "2217959054",
                        "name": "M. Choi"
                    },
                    {
                        "authorId": "2232599760",
                        "name": "Jaewon Chu"
                    },
                    {
                        "authorId": "2149159287",
                        "name": "Jihwan Park"
                    },
                    {
                        "authorId": null,
                        "name": "Hyunwoo J. Kim"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Flan-T5 (Chung et al., 2022) is an improved upstream model scaled to thousands of tasks (Wang et al., 2022b).",
                "Wang et al. (2022a) additionally ensure that label tokens belong only to a single class."
            ],
            "citingPaper": {
                "paperId": "1d98e3de197c56ff89754fb4423418d8df5e931f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-12576",
                    "ArXiv": "2305.12576",
                    "DOI": "10.48550/arXiv.2305.12576",
                    "CorpusId": 258832375
                },
                "corpusId": 258832375,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1d98e3de197c56ff89754fb4423418d8df5e931f",
                "title": "Automated Few-shot Classification with Instruction-Finetuned Language Models",
                "abstract": "A particularly successful class of approaches for few-shot learning combines language models with prompts -- hand-crafted task descriptions that complement data samples. However, designing prompts by hand for each task commonly requires domain knowledge and substantial guesswork. We observe, in the context of classification tasks, that instruction finetuned language models exhibit remarkable prompt robustness, and we subsequently propose a simple method to eliminate the need for handcrafted prompts, named AuT-Few. This approach consists of (i) a prompt retrieval module that selects suitable task instructions from the instruction-tuning knowledge base, and (ii) the generation of two distinct, semantically meaningful, class descriptions and a selection mechanism via cross-validation. Over $12$ datasets, spanning $8$ classification tasks, we show that AuT-Few outperforms current state-of-the-art few-shot learning methods. Moreover, AuT-Few is the best ranking method across datasets on the RAFT few-shot benchmark. Notably, these results are achieved without task-specific handcrafted prompts on unseen tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "134617539",
                        "name": "Rami Aly"
                    },
                    {
                        "authorId": "2110332219",
                        "name": "Xingjian Shi"
                    },
                    {
                        "authorId": "3002019",
                        "name": "Kaixiang Lin"
                    },
                    {
                        "authorId": "2085709",
                        "name": "Aston Zhang"
                    },
                    {
                        "authorId": "145771261",
                        "name": "A. Wilson"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "For example, using top-level predictions to refine prompts of bottom levels can surpass soft prompts and hard prompts (Wang et al., 2022b).",
                "Compared with fine-tuning, prompt tuning may have a better generalization on various tasks due to the aligned nature of language descriptions and answer semantics, e.g., classification problems (Gao et al., 2021; Wang et al., 2022a)."
            ],
            "citingPaper": {
                "paperId": "1badaf6065aba7b9c3eb6a7a059ad499acecbdd2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-03973",
                    "ArXiv": "2305.03973",
                    "DOI": "10.48550/arXiv.2305.03973",
                    "CorpusId": 258557852
                },
                "corpusId": 258557852,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/1badaf6065aba7b9c3eb6a7a059ad499acecbdd2",
                "title": "DiscoPrompt: Path Prediction Prompt Tuning for Implicit Discourse Relation Recognition",
                "abstract": "Implicit Discourse Relation Recognition (IDRR) is a sophisticated and challenging task to recognize the discourse relations between the arguments with the absence of discourse connectives. The sense labels for each discourse relation follow a hierarchical classification scheme in the annotation process (Prasad et al., 2008), forming a hierarchy structure. Most existing works do not well incorporate the hierarchy structure but focus on the syntax features and the prior knowledge of connectives in the manner of pure text classification. We argue that it is more effective to predict the paths inside the hierarchical tree (e.g.,\"Comparison ->Contrast ->however\") rather than flat labels (e.g., Contrast) or connectives (e.g., however). We propose a prompt-based path prediction method to utilize the interactive information and intrinsic senses among the hierarchy in IDRR. This is the first work that injects such structure information into pre-trained language models via prompt tuning, and the performance of our solution shows significant and consistent improvement against competitive baselines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2216598559",
                        "name": "Chunkit Chan"
                    },
                    {
                        "authorId": "2146075375",
                        "name": "Xin Liu"
                    },
                    {
                        "authorId": "2109077713",
                        "name": "Cheng Jiayang"
                    },
                    {
                        "authorId": "2118274188",
                        "name": "Zihan Li"
                    },
                    {
                        "authorId": "1809614",
                        "name": "Yangqiu Song"
                    },
                    {
                        "authorId": "9413717",
                        "name": "Ginny Y. Wong"
                    },
                    {
                        "authorId": "1768070",
                        "name": "S. See"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "AutoL (Gao et al., 2021) and AMuLaP (Wang et al., 2022) are both automatic label words searching methods utilizing few-shot examples.",
                ", 2021) and AMuLaP (Wang et al., 2022) are both automatic label words searching methods utilizing few-shot examples.",
                ", 2021); AMuLaP: method in (Wang et al., 2022); Majority: majority class."
            ],
            "citingPaper": {
                "paperId": "24cc83df913c752bd7e38a5acffeb7be6c17f657",
                "externalIds": {
                    "ACL": "2023.acl-long.869",
                    "DBLP": "journals/corr/abs-2212-06950",
                    "ArXiv": "2212.06950",
                    "DOI": "10.48550/arXiv.2212.06950",
                    "CorpusId": 254636531
                },
                "corpusId": 254636531,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/24cc83df913c752bd7e38a5acffeb7be6c17f657",
                "title": "Pre-trained Language Models Can be Fully Zero-Shot Learners",
                "abstract": "How can we extend a pre-trained model to many language understanding tasks, without labeled or additional unlabeled data? Pre-trained language models (PLMs) have been effective for a wide range of NLP tasks. However, existing approaches either require fine-tuning on downstream labeled datasets or manually constructing proper prompts. In this paper, we propose nonparametric prompting PLM (NPPrompt) for fully zero-shot language understanding. Unlike previous methods, NPPrompt uses only pre-trained language models and does not require any labeled data or additional raw corpus for further fine-tuning, nor does it rely on humans to construct a comprehensive set of prompt label words. We evaluate NPPrompt against previous major few-shot and zero-shot learning methods on diverse NLP tasks: including text classification, text entailment, similar text retrieval, paraphrasing, and multiple-choice question answering. Experimental results demonstrate that our NPPrompt outperforms the previous best fully zero-shot method by big margins, with absolute gains of 12.8% in accuracy on text classification and 15.6% on the GLUE benchmark. Our source code is available at https://anonymous.4open. science/r/NPPrompt.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "150345512",
                        "name": "Xuandong Zhao"
                    },
                    {
                        "authorId": "51152981",
                        "name": "Siqi Ouyang"
                    },
                    {
                        "authorId": "2118173308",
                        "name": "Zhiguo Yu"
                    },
                    {
                        "authorId": "2145208873",
                        "name": "Ming-li Wu"
                    },
                    {
                        "authorId": "47681096",
                        "name": "Lei Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "However, most previous works focus injecting knowledge into prompt on single-label multi-class classification task (Hu et al., 2022; Wang et al., 2022a; Ye et al., 2022)."
            ],
            "citingPaper": {
                "paperId": "0b80389b6ec62e0c03c822c2748f7c41d859bcb7",
                "externalIds": {
                    "ArXiv": "2210.03304",
                    "DBLP": "journals/corr/abs-2210-03304",
                    "DOI": "10.48550/arXiv.2210.03304",
                    "CorpusId": 252762110,
                    "PubMed": "36848298"
                },
                "corpusId": 252762110,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/0b80389b6ec62e0c03c822c2748f7c41d859bcb7",
                "title": "Knowledge Injected Prompt Based Fine-tuning for Multi-label Few-shot ICD Coding",
                "abstract": "Automatic International Classification of Diseases (ICD) coding aims to assign multiple ICD codes to a medical note with average length of 3,000+ tokens. This task is challenging due to a high-dimensional space of multi-label assignment (tens of thousands of ICD codes) and the long-tail challenge: only a few codes (common diseases) are frequently assigned while most codes (rare diseases) are infrequently assigned. This study addresses the long-tail challenge by adapting a prompt-based fine-tuning technique with label semantics, which has been shown to be effective under few-shot setting. To further enhance the performance in medical domain, we propose a knowledge-enhanced longformer by injecting three domain-specific knowledge: hierarchy, synonym, and abbreviation with additional pretraining using contrastive learning. Experiments on MIMIC-III-full, a benchmark dataset of code assignment, show that our proposed method outperforms previous state-of-the-art method in 14.5% in marco F1 (from 10.3 to 11.8, P<0.001). To further test our model on few-shot setting, we created a new rare diseases coding dataset, MIMIC-III-rare50, on which our model improves marco F1 from 17.1 to 30.4 and micro F1 from 17.2 to 32.6 compared to previous method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48598711",
                        "name": "Zhichao Yang"
                    },
                    {
                        "authorId": "2051628335",
                        "name": "Shufan Wang"
                    },
                    {
                        "authorId": "3440700",
                        "name": "Bhanu Pratap Singh Rawat"
                    },
                    {
                        "authorId": "2052388703",
                        "name": "Avijit Mitra"
                    },
                    {
                        "authorId": "2119120474",
                        "name": "Hong Yu"
                    }
                ]
            }
        }
    ]
}