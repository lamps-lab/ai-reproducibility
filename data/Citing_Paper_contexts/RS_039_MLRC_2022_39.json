{
    "offset": 0,
    "data": [
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0b8a76d08598b1f61bcb7c69e30aa61d1328566d",
                "externalIds": {
                    "DOI": "10.1007/s10489-023-04928-3",
                    "CorpusId": 261801900
                },
                "corpusId": 261801900,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/0b8a76d08598b1f61bcb7c69e30aa61d1328566d",
                "title": "Fairness-aware fake news mitigation using counter information propagation",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2928447",
                        "name": "A. Saxena"
                    },
                    {
                        "authorId": "2240240414",
                        "name": "Cristina Guti\u00e9rrez Bierbooms"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9fde7a5d491e25d2ff040e2f2f2517fa5b753e85",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-13821",
                    "ArXiv": "2308.13821",
                    "DOI": "10.48550/arXiv.2308.13821",
                    "CorpusId": 261245374
                },
                "corpusId": 261245374,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9fde7a5d491e25d2ff040e2f2f2517fa5b753e85",
                "title": "A Survey of Imbalanced Learning on Graphs: Problems, Techniques, and Future Directions",
                "abstract": "Graphs represent interconnected structures prevalent in a myriad of real-world scenarios. Effective graph analytics, such as graph learning methods, enables users to gain profound insights from graph data, underpinning various tasks including node classification and link prediction. However, these methods often suffer from data imbalance, a common issue in graph data where certain segments possess abundant data while others are scarce, thereby leading to biased learning outcomes. This necessitates the emerging field of imbalanced learning on graphs, which aims to correct these data distribution skews for more accurate and representative learning outcomes. In this survey, we embark on a comprehensive review of the literature on imbalanced learning on graphs. We begin by providing a definitive understanding of the concept and related terminologies, establishing a strong foundational understanding for readers. Following this, we propose two comprehensive taxonomies: (1) the problem taxonomy, which describes the forms of imbalance we consider, the associated tasks, and potential solutions; (2) the technique taxonomy, which details key strategies for addressing these imbalances, and aids readers in their method selection process. Finally, we suggest prospective future directions for both problems and techniques within the sphere of imbalanced learning on graphs, fostering further innovation in this critical area.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2163529904",
                        "name": "Zemin Liu"
                    },
                    {
                        "authorId": "1527090156",
                        "name": "Yuan N. Li"
                    },
                    {
                        "authorId": "150244356",
                        "name": "Nan-Fang Chen"
                    },
                    {
                        "authorId": "2155616330",
                        "name": "Qian Wang"
                    },
                    {
                        "authorId": "2019961",
                        "name": "Bryan Hooi"
                    },
                    {
                        "authorId": "143603418",
                        "name": "Bin He"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "533ea56eb1e99db390536228bcb1c5259e59f3a6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-09596",
                    "ArXiv": "2308.09596",
                    "DOI": "10.48550/arXiv.2308.09596",
                    "CorpusId": 261030987
                },
                "corpusId": 261030987,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/533ea56eb1e99db390536228bcb1c5259e59f3a6",
                "title": "Disparity, Inequality, and Accuracy Tradeoffs in Graph Neural Networks for Node Classification",
                "abstract": "Graph neural networks (GNNs) are increasingly used in critical human applications for predicting node labels in attributed graphs. Their ability to aggregate features from nodes' neighbors for accurate classification also has the capacity to exacerbate existing biases in data or to introduce new ones towards members from protected demographic groups. Thus, it is imperative to quantify how GNNs may be biased and to what extent their harmful effects may be mitigated. To this end, we propose two new GNN-agnostic interventions namely, (i) PFR-AX which decreases the separability between nodes in protected and non-protected groups, and (ii) PostProcess which updates model predictions based on a blackbox policy to minimize differences between error rates across demographic groups. Through a large set of experiments on four datasets, we frame the efficacies of our approaches (and three variants) in terms of their algorithmic fairness-accuracy tradeoff and benchmark our results against three strong baseline interventions on three state-of-the-art GNN models. Our results show that no single intervention offers a universally optimal tradeoff, but PFR-AX and PostProcess provide granular control and improve model confidence when correctly predicting positive outcomes for nodes in protected groups.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "32009009",
                        "name": "Arpit Merchant"
                    },
                    {
                        "authorId": "2082412452",
                        "name": "Carlos Castillo"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "Crosswalk [Khajehnejad et al. 2021] \u2713 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717 \u2713 \u2717 \u2713 \u2717 \u2713 \u2717 \u2713"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f0606e2d975cda134392877e4064924a13915301",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-03929",
                    "ArXiv": "2307.03929",
                    "DOI": "10.48550/arXiv.2307.03929",
                    "CorpusId": 259501889
                },
                "corpusId": 259501889,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f0606e2d975cda134392877e4064924a13915301",
                "title": "Fairness-Aware Graph Neural Networks: A Survey",
                "abstract": "Graph Neural Networks (GNNs) have become increasingly important due to their representational power and state-of-the-art predictive performance on many fundamental learning tasks. Despite this success, GNNs suffer from fairness issues that arise as a result of the underlying graph data and the fundamental aggregation mechanism that lies at the heart of the large class of GNN models. In this article, we examine and categorize fairness techniques for improving the fairness of GNNs. Previous work on fair GNN models and techniques are discussed in terms of whether they focus on improving fairness during a preprocessing step, during training, or in a post-processing phase. Furthermore, we discuss how such techniques can be used together whenever appropriate, and highlight the advantages and intuition as well. We also introduce an intuitive taxonomy for fairness evaluation metrics including graph-level fairness, neighborhood-level fairness, embedding-level fairness, and prediction-level fairness metrics. In addition, graph datasets that are useful for benchmarking the fairness of GNN models are summarized succinctly. Finally, we highlight key open problems and challenges that remain to be addressed.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2198208547",
                        "name": "April Chen"
                    },
                    {
                        "authorId": "2066337266",
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "authorId": "2867343",
                        "name": "Namyong Park"
                    },
                    {
                        "authorId": "30440868",
                        "name": "Puja Trivedi"
                    },
                    {
                        "authorId": "2153607948",
                        "name": "Yu Wang"
                    },
                    {
                        "authorId": "2117903209",
                        "name": "Tong Yu"
                    },
                    {
                        "authorId": "2109571021",
                        "name": "Sungchul Kim"
                    },
                    {
                        "authorId": "2462276",
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "authorId": "144741751",
                        "name": "Nesreen K. Ahmed"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dea420ed4c5da45475438eee5673f5a64f6ec0f5",
                "externalIds": {
                    "DBLP": "journals/ijon/SangW23",
                    "DOI": "10.1016/j.neucom.2023.126621",
                    "CorpusId": 260267597
                },
                "corpusId": 260267597,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/dea420ed4c5da45475438eee5673f5a64f6ec0f5",
                "title": "Graph convolution with topology refinement for Automatic Reinforcement Learning",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2137831830",
                        "name": "Jianghui Sang"
                    },
                    {
                        "authorId": "2203795674",
                        "name": "Yongli Wang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "537cc3142ed46b88bcf861cc3aed656ff5322c3e",
                "externalIds": {
                    "DOI": "10.1109/icassp49357.2023.10094834",
                    "CorpusId": 258538600
                },
                "corpusId": 258538600,
                "publicationVenue": {
                    "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
                    "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Acoust Speech Signal Process",
                        "IEEE Int Conf Acoust Speech Signal Process",
                        "ICASSP",
                        "International Conference on Acoustics, Speech, and Signal Processing"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
                },
                "url": "https://www.semanticscholar.org/paper/537cc3142ed46b88bcf861cc3aed656ff5322c3e",
                "title": "Dynamic Fair Node Representation Learning",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1411389353",
                        "name": "O. D. Kose"
                    },
                    {
                        "authorId": "1798830",
                        "name": "Yanning Shen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Along this research path \u2013 learning node representations for fair IM \u2013 the works that are closest to ours are [16, 17].",
                "The follow-upwork [16] revisits generic algorithms for learning node representations (e.",
                "The same scalability limits are exhibited by the two state-of-the-art methods on fair IM that do rely on node representation [16, 17], yet learned only from the social connectivity.",
                "Compared to [16, 17], we revisit IM with fairness under a more flexible formulation, by algorithmic solutions that are applicable to arbitrary sets of sensitive attributes and to large, realistic datasets.",
                "Crosswalk [16] is a randomwalk based graph representation method, which enhances fairness by re-weighting the edges between nodes from different groups.",
                "The limitations of [16, 17] are threefold: (i) as they exploit the social connectivity, they incur a high computation cost and cannot reasonably scale; in particular, training the adversarial neural network in [17] grows exponentially with the number of users in the social network, (ii) they assume that the probability of diffusion between a pair of nodes is constant and follows the Markovian assumption, and (iii) they are not generic, i."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "365a5f12a555348818264559e2653e108027c262",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-01587",
                    "ArXiv": "2306.01587",
                    "DOI": "10.1145/3580305.3599847",
                    "CorpusId": 259063763
                },
                "corpusId": 259063763,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/365a5f12a555348818264559e2653e108027c262",
                "title": "Influence Maximization with Fairness at Scale",
                "abstract": "In this paper, we revisit the problem of influence maximization with fairness, which aims to select k influential nodes to maximise the spread of information in a network, while ensuring that selected sensitive user attributes (e.g., gender, location, origin, race, etc.) are fairly affected, i.e., are proportionally similar between the original network and the affected users. Recent studies on this problem focused only on extremely small networks, hence the challenge remains on how to achieve a scalable solution, applicable to networks with millions or billions of nodes. We propose an approach that is based on learning node representations (embeddings) for fair spread from diffusion cascades, instead of the social connectivity, and in this way we can deal with very large graphs. We propose two data-driven approaches: (a) fairness-based participant sampling (FPS), and (b) fairness as context (FAC). Spread related user features, such as the probability of diffusing information to others, are derived from the historical information cascades, using a deep neural network. The extracted features are then used in selecting influencers that maximize the influence spread, while being also fair with respect to the chosen sensitive attributes. In FPS, fairness and cascade length information are considered independently in the decision-making process, while FAC considers these information facets jointly and takes into account correlations between them. The proposed algorithms are generic and represent the first policy-driven solutions that can be applied to arbitrary sets of sensitive attributes at scale. We evaluate the performance of our solutions on a real-world public dataset (Sina Weibo) and on a hybrid real-synthetic dataset (Digg), which exhibit all the facets that we exploit, namely diffusion network, diffusion traces, and user profiles. These experiments show that our methods outperform the state-the-art solutions in terms of spread, fairness, and scalability.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2150671661",
                        "name": "Yuting Feng"
                    },
                    {
                        "authorId": "34945460",
                        "name": "A. Patel"
                    },
                    {
                        "authorId": "2772242",
                        "name": "B. Cautis"
                    },
                    {
                        "authorId": "2507979",
                        "name": "H. Vahabi"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f4e18d4aefc1ee0f88f0bb09fa5c17c74aad767e",
                "externalIds": {
                    "ArXiv": "2304.01391",
                    "DBLP": "journals/corr/abs-2304-01391",
                    "DOI": "10.48550/arXiv.2304.01391",
                    "CorpusId": 257921453
                },
                "corpusId": 257921453,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f4e18d4aefc1ee0f88f0bb09fa5c17c74aad767e",
                "title": "Counterfactual Learning on Graphs: A Survey",
                "abstract": "Graph-structured data are pervasive in the real-world such as social networks, molecular graphs and transaction networks. Graph neural networks (GNNs) have achieved great success in representation learning on graphs, facilitating various downstream tasks. However, GNNs have several drawbacks such as lacking interpretability, can easily inherit the bias of the training data and cannot model the casual relations. Recently, counterfactual learning on graphs has shown promising results in alleviating these drawbacks. Various graph counterfactual learning approaches have been proposed for counterfactual fairness, explainability, link prediction and other applications on graphs. To facilitate the development of this promising direction, in this survey, we categorize and comprehensively review papers on graph counterfactual learning. We divide existing methods into four categories based on research problems studied. For each category, we provide background and motivating examples, a general framework summarizing existing works and a detailed review of these works. We point out promising future research directions at the intersection of graph-structured data, counterfactual learning, and real-world applications. To offer a comprehensive view of resources for future studies, we compile a collection of open-source implementations, public datasets, and commonly-used evaluation metrics. This survey aims to serve as a ``one-stop-shop'' for building a unified understanding of graph counterfactual learning categories and current resources. We also maintain a repository for papers and resources and will keep updating the repository https://github.com/TimeLovercc/Awesome-Graph-Causal-Learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2149465392",
                        "name": "Zhimeng Guo"
                    },
                    {
                        "authorId": "33664431",
                        "name": "Teng Xiao"
                    },
                    {
                        "authorId": "1682418",
                        "name": "C. Aggarwal"
                    },
                    {
                        "authorId": "2146672392",
                        "name": "Hui Liu"
                    },
                    {
                        "authorId": "2116430057",
                        "name": "Suhang Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[29] serves as a seminal work for fairness-aware random walk-based studies [30]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "27cd652b6ab1cc4a02d16c16fca2f5bdd0bb6ef9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-14591",
                    "ArXiv": "2303.14591",
                    "DOI": "10.48550/arXiv.2303.14591",
                    "CorpusId": 257767038
                },
                "corpusId": 257767038,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/27cd652b6ab1cc4a02d16c16fca2f5bdd0bb6ef9",
                "title": "FairGAT: Fairness-aware Graph Attention Networks",
                "abstract": "Graphs can facilitate modeling various complex systems such as gene networks and power grids, as well as analyzing the underlying relations within them. Learning over graphs has recently attracted increasing attention, particularly graph neural network-based (GNN) solutions, among which graph attention networks (GATs) have become one of the most widely utilized neural network structures for graph-based tasks. Although it is shown that the use of graph structures in learning results in the amplification of algorithmic bias, the influence of the attention design in GATs on algorithmic bias has not been investigated. Motivated by this, the present study first carries out a theoretical analysis in order to demonstrate the sources of algorithmic bias in GAT-based learning for node classification. Then, a novel algorithm, FairGAT, that leverages a fairness-aware attention design is developed based on the theoretical findings. Experimental results on real-world networks demonstrate that FairGAT improves group fairness measures while also providing comparable utility to the fairness-aware baselines for node classification and link prediction.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1825768830",
                        "name": "\u00d6yk\u00fc Deniz K\u00f6se"
                    },
                    {
                        "authorId": "1798830",
                        "name": "Yanning Shen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[21] propose CrossWalk, a method that enhances fairness in graph algorithms by biasing random walks to cross group boundaries."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0bd934dec43bba4a10e31907c2dc0dbbfe334e6a",
                "externalIds": {
                    "ArXiv": "2212.12040",
                    "DBLP": "journals/corr/abs-2212-12040",
                    "DOI": "10.48550/arXiv.2212.12040",
                    "CorpusId": 255096503
                },
                "corpusId": 255096503,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0bd934dec43bba4a10e31907c2dc0dbbfe334e6a",
                "title": "Graph Learning with Localized Neighborhood Fairness",
                "abstract": "Learning fair graph representations for downstream applications is becoming increasingly important, but existing work has mostly focused on improving fairness at the global level by either modify-ing the graph structure or objective function without taking into account the local neighborhood of a node. In this work, we formally introduce the notion of neighborhood fairness and develop a computational framework for learning such locally fair embeddings. We argue that the notion of neighborhood fairness is more appropriate since GNN-based models operate at the local neighborhood level of a node. Our neighborhood fairness framework has two main components that are \ufb02exible for learning fair graph representations from arbitrary data: the \ufb01rst aims to construct fair neighborhoodsfor any arbitrary node in a graph and the second enables adaptionof these fair neighborhoodsto bettercapture certain application or data-dependent constraints, such as allowing neigh-borhoodstobe more biased towards certain attributesorneighbors in the graph. Furthermore, while link prediction has been exten-sively studied, we are the \ufb01rst to investigate the graph representation learning task of fair link classi\ufb01cation. We demonstrate the e\ufb00ectiveness of the proposed neighborhood fairness framework for a variety of graph machine learning tasks including fair link prediction, link classi\ufb01cation, and learning fair graph embeddings. Notably, our approach achieves not only better fairness but also increases the accuracy in the majority of cases across a wide variety of graphs, problem settings, and metrics.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2198208547",
                        "name": "April Chen"
                    },
                    {
                        "authorId": "2066337266",
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "authorId": "1793409",
                        "name": "Nedim Lipka"
                    },
                    {
                        "authorId": "1890683",
                        "name": "J. Hoffswell"
                    },
                    {
                        "authorId": "51192588",
                        "name": "G. Chan"
                    },
                    {
                        "authorId": "30518075",
                        "name": "Shunan Guo"
                    },
                    {
                        "authorId": "2176108906",
                        "name": "E. Koh"
                    },
                    {
                        "authorId": "2109571021",
                        "name": "Sungchul Kim"
                    },
                    {
                        "authorId": "144741751",
                        "name": "Nesreen K. Ahmed"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[15] developed Crosswalk, the key idea is to bias random walks to cross group boundaries, which enhances fairness.",
                "To address the fairness issues, most existing works learn a fair node or graph representation by optimizing adjacency matrices [18], [31], adversarial training [8], or fairness-oriented DeepWalk [15], [24]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a03f43a8987a148b7f3b599e8aa70ab9feeb8b64",
                "externalIds": {
                    "DBLP": "conf/bigdataconf/LiuZX22",
                    "DOI": "10.1109/BigData55660.2022.10020318",
                    "CorpusId": 256312270
                },
                "corpusId": 256312270,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/a03f43a8987a148b7f3b599e8aa70ab9feeb8b64",
                "title": "Trade less Accuracy for Fairness and Trade-off Explanation for GNN",
                "abstract": "Graphs are widely found in social network analysis and e-commerce, where Graph Neural Networks (GNNs) are the state-of the-art model. GNNs can be biased due to sensitive attributes and network topology. With existing work that learns a fair node representation or adjacency matrix, achieving a strong guarantee of group fairness while preserving prediction accuracy is still challenging, with the fairness-accuracy trade-off remaining obscure to human decision-makers. We first define and analyze a novel upper bound of group fairness to optimize the adjacency matrix for fairness without significantly h arming prediction accuracy. To understand the nuance of fairness-accuracy tradeoff, we further propose macroscopic and microscopic explanation methods to reveal the trade-offs and the space that one can exploit. The macroscopic explanation method is based on stratified sampling and linear programming to deterministically explain the dynamics of the group fairness and prediction accuracy. Driving down to the microscopic level, we propose a path-based explanation that reveals how network topology leads to the tradeoff. On seven graph datasets, we demonstrate the novel upper bound can achieve more efficient fairness-accuracy trade-offs and the intuitiveness of the explanation methods can clearly pinpoint where the trade-off is improved.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2144399347",
                        "name": "Yazheng Liu"
                    },
                    {
                        "authorId": "2108286275",
                        "name": "Xi Zhang"
                    },
                    {
                        "authorId": "3131378",
                        "name": "Sihong Xie"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "First, some works adapt existing unsupervised graph embedding approaches for fairness (Rahman et al., 2019; Khajehnejad et al., 2021), but it is challenging to accommodate all such models."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b511aaa7dc9c1190a0ab371e8023cec05705c273",
                "externalIds": {
                    "ArXiv": "2211.09925",
                    "DBLP": "journals/corr/abs-2211-09925",
                    "DOI": "10.48550/arXiv.2211.09925",
                    "CorpusId": 253708354
                },
                "corpusId": 253708354,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b511aaa7dc9c1190a0ab371e8023cec05705c273",
                "title": "FairMILE: A Multi-Level Framework for Fair and Scalable Graph Representation Learning",
                "abstract": "Graph representation learning models have been deployed for making decisions in multiple high-stakes scenarios. It is therefore critical to ensure that these models are fair. Prior research has shown that graph neural networks can inherit and reinforce the bias present in graph data. Researchers have begun to examine ways to mitigate the bias in such models. However, existing efforts are restricted by their inefficiency, limited applicability, and the constraints they place on sensitive attributes. To address these issues, we present FairMILE a general framework for fair and scalable graph representation learning. FairMILE is a multi-level framework that allows contemporary unsupervised graph embedding methods to scale to large graphs in an agnostic manner. FairMILE learns both fair and high-quality node embeddings where the fairness constraints are incorporated in each phase of the framework. Our experiments across two distinct tasks demonstrate that FairMILE can learn node representations that often achieve superior fairness scores and high downstream performance while significantly outperforming all the baselines in terms of efficiency.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3129825",
                        "name": "Yuntian He"
                    },
                    {
                        "authorId": "1711136",
                        "name": "Saket Gurukar"
                    },
                    {
                        "authorId": "2739353",
                        "name": "Srinivas Parthasarathy"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Both of these are widely used by the research community to assess the fairness of machine learning models in graph data [1], [8], [11], [13], [16], [24], [44].",
                "The remaining parameters are the same as stated in their papers [19], [24], [33].",
                "CrossWalk [24]: This method enhances the fairness of various graph algorithms applied to node embeddings, including influence maximization, link prediction, and node classification.",
                "The first group of research works focuses on the design of an embedding mechanism that would be fair for the sensitive attribute [8], [24], [33].",
                "Recent studies on the fairness of GNNs and Node2Vec have also reported similar issues [8], [24], [33]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "1964fb0aa184d84d0cd82f8efc5787027b3cb0bd",
                "externalIds": {
                    "DBLP": "conf/bigdataconf/MansoorAAKHK22",
                    "ArXiv": "2211.00783",
                    "DOI": "10.1109/BigData55660.2022.10020694",
                    "CorpusId": 253255265
                },
                "corpusId": 253255265,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1964fb0aa184d84d0cd82f8efc5787027b3cb0bd",
                "title": "Impact Of Missing Data Imputation On The Fairness And Accuracy Of Graph Node Classifiers",
                "abstract": "Analysis of the fairness of machine learning (ML) algorithms has attracted many researchers\u2019 interest. Several studies have shown that ML methods produce a bias toward different groups, which limits the applicability of ML models in many applications, such as crime rate prediction. The data used for ML may have missing values, which, if not appropriately handled, are known to further harmfully affect fairness. To address this issue, many imputation methods have been proposed to deal with missing data. However, research on the effect of missing data imputation on fairness is still rather limited. In this paper, we analyze the impact of imputation on fairness in the context of graph data (node attributes) using different embedding and neural network methods. Extensive experiments on six datasets demonstrate several issues of fairness in graph node classification when dealing with missing data and various imputation techniques. We find that the choice of the imputation method affects both fairness and accuracy. Our results provide valuable insights into fairness ML over graph data and how to handle missingness in graphs efficiently.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9419338",
                        "name": "Haris Mansoor"
                    },
                    {
                        "authorId": "150290745",
                        "name": "Sarwan Ali"
                    },
                    {
                        "authorId": "2068848354",
                        "name": "Shafiq Alam"
                    },
                    {
                        "authorId": "2115772281",
                        "name": "Muhammad Asad Khan"
                    },
                    {
                        "authorId": "1798642",
                        "name": "U. Hassan"
                    },
                    {
                        "authorId": "7640859",
                        "name": "Imdadullah Khan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "On query-/user-induced recommendation networks such as those on Amazon and YouTube, structural bias hinders the discovery of diversified content, reducing serendipity (Ge et al. 2010; Kotkov et al. 2016; Anagnostopoulos et al. 2020).",
                "On query/user-induced recommendation networks such as those on Amazon and YouTube, structural bias hinders the discovery of diversified content, reducing serendipity (Ge et al. 2010; Kotkov et al. 2016; Anagnostopoulos et al. 2020).",
                "Recommender systems may also reduce serendipity (Ge et al. 2010; Kotkov et al. 2016; Anagnostopoulos et al. 2020), i.e., the possibility of \u201cstomping\u201d on content/users expressing different opinions.",
                "Recommender systems may also reduce serendipity (Ge et al. 2010; Kotkov et al. 2016; Anagnostopoulos et al. 2020), i."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "c448724e93478bfa8bc6ea9836bdc19d5468708e",
                "externalIds": {
                    "DBLP": "journals/datamine/HaddadanMRU22",
                    "DOI": "10.1007/s10618-022-00875-8",
                    "CorpusId": 252686986
                },
                "corpusId": 252686986,
                "publicationVenue": {
                    "id": "d263025a-9eaf-443f-9bbf-72377e8d22a6",
                    "name": "Data mining and knowledge discovery",
                    "type": "journal",
                    "alternate_names": [
                        "Data Mining and Knowledge Discovery",
                        "Data Min Knowl Discov",
                        "Data min knowl discov"
                    ],
                    "issn": "1384-5810",
                    "url": "https://www.springer.com/computer/database+management+&+information+retrieval/journal/10618",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10618",
                        "http://www.springer.com/computer/database+management+&+information+retrieval/journal/10618"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c448724e93478bfa8bc6ea9836bdc19d5468708e",
                "title": "Reducing polarization and increasing diverse navigability in graphs by inserting edges and swapping edge weights",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3393743",
                        "name": "Shahrzad Haddadan"
                    },
                    {
                        "authorId": "1505833240",
                        "name": "Cristina Menghini"
                    },
                    {
                        "authorId": "3019872",
                        "name": "Matteo Riondato"
                    },
                    {
                        "authorId": "1735099",
                        "name": "E. Upfal"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "[88] also proposed a reweighting method that can be applied to any random walk",
                "CR \u2013 [167] [167] \u2013 \u2013 [167] \u2013 IM [58] [7, 10, 20, 56, 88, 89, 125, 153, 154, 162, 165] [7, 10, 20, 56, 58, 88, 89, 125, 153, 154, 162, 165] \u2013 \u2013 [7, 10, 20, 56, 58, 88, 89, 125, 153, 154, 162, 165] \u2013",
                "IBM \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 CD \u2013 [88] [88, 108] \u2013 \u2013 [88] [108]",
                "Research Topic Individual fairness Group Fairness Feature-Aware FeatureBlind PreProcessing In-Processing PostProcessing LP [96, 124] [88, 96, 104, 124, 132, 134] [88, 96, 104, 124, 132, 134] \u2013 [96] [88, 104, 124, 134] [104, 132]",
                "Several works, including [56, 88, 104, 108, 124], have highlighted unfairness in different SNA algorithms, i.",
                "[88] used disparity to compare the fairness of node classification methods."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "b97f74c200206e87f0ca3e97c71813bb66293651",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2209-01678",
                    "ArXiv": "2209.01678",
                    "DOI": "10.48550/arXiv.2209.01678",
                    "CorpusId": 252089559
                },
                "corpusId": 252089559,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b97f74c200206e87f0ca3e97c71813bb66293651",
                "title": "FairSNA: Algorithmic Fairness in Social Network Analysis",
                "abstract": "In recent years, designing fairness-aware methods has received much attention in various domains, including machine learning, natural language processing, and information retrieval. However, understanding structural bias and inequalities in social networks and designing fairness-aware methods for various research problems in social network analysis (SNA) have not received much attention. In this work, we highlight how the structural bias of social networks impacts the fairness of different SNA methods. We further discuss fairness aspects that should be considered while proposing network structure-based solutions for different SNA problems, such as link prediction, influence maximization, centrality ranking, and community detection. This paper clearly highlights that very few works have considered fairness and bias while proposing solutions; even these works are mainly focused on some research topics, such as link prediction, influence maximization, and PageRank. However, fairness has not yet been addressed for other research topics, such as influence blocking and community detection. We review state-of-the-art for different research topics in SNA, including the considered fairness constraints, their limitations, and our vision. This paper also covers evaluation metrics, available datasets, and synthetic network generating models used in such studies. Finally, we highlight various open research directions that require researchers' attention to bridge the gap between fairness and SNA.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2928447",
                        "name": "A. Saxena"
                    },
                    {
                        "authorId": "2165966989",
                        "name": "George Fletcher"
                    },
                    {
                        "authorId": "1691997",
                        "name": "Mykola Pechenizkiy"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "CrossWalk aims to address the shortcomings of Fairwalk by assigning more weights to both edges that connect nodes Manuscript submitted to ACM\nA Survey on Fairness for Machine Learning on Graphs 13\non the boundary of the protected groups, and edges connecting nodes from different groups.",
                "In the same spirit, CrossWalk [34] is based on a re-weighting procedure for building the random walks and can therefore be used with any random walk based algorithms including DeepWalk [59] and Node2vec [26], to name a few.",
                "Method Reference Code Pre-processing In training Post-processing Keywords FairOT [41] Github \u2713 Optimal Transport, Laplacian regularization FairDrop [65] Github \u2713 Edge Drop, Homophily UGE [70] NA \u2713 Structural generative graph model FairWalk [60] Github\u2217 \u2713 Random walk CrossWalk [34] Github \u2713 Random walk DeBayes [11] Github \u2713 Conditional Network Embeddings, Bayeisan prior FIPR [12] Github \u2713 I-Projection regularizer CFC [10] Github \u2713 Adversarial Learning, Compositional filtering FLIP [50] Github \u2713 Adversarial Learning, Modularity DKGE [21] NA \u2713 Adversarial, Knowledge graphs FairGNN [17] Github \u2713 GNNs, Adversarial Learning FairAdj [45] Github \u2713 Graph Neural Networks MONET [58] Github \u2713 GNNs, metadata NIFTY [3] Github \u2713 GNNs, augmented views, stability InFoRM [31] Github \u2713 \u2713 \u2713"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "59033c5ce8e3b87bfcb86aa237747c5367586400",
                "externalIds": {
                    "ArXiv": "2205.05396",
                    "DBLP": "journals/corr/abs-2205-05396",
                    "DOI": "10.48550/arXiv.2205.05396",
                    "CorpusId": 248693458
                },
                "corpusId": 248693458,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/59033c5ce8e3b87bfcb86aa237747c5367586400",
                "title": "A Survey on Fairness for Machine Learning on Graphs",
                "abstract": "Nowadays, the analysis of complex phenomena modeled by graphs plays a crucial role in many real-world application domains where decisions can have a strong societal impact. However, numerous studies and papers have recently revealed that machine learning models could lead to potential disparate treatment between individuals and unfair outcomes. In that context, algorithmic contributions for graph mining are not spared by the problem of fairness and present some specific challenges related to the intrinsic nature of graphs: (1) graph data is non-IID, and this assumption may invalidate many existing studies in fair machine learning, (2) suited metric definitions to assess the different types of fairness with relational data and (3) algorithmic challenge on the difficulty of finding a good trade-off between model accuracy and fairness. This survey is the first one dedicated to fairness for relational data. It aims to present a comprehensive review of state-of-the-art techniques in fairness on graph mining and identify the open challenges and future trends. In particular, we start by presenting several sensible application domains and the associated graph mining tasks with a focus on edge prediction and node classification in the sequel. We also recall the different metrics proposed to evaluate potential bias at different levels of the graph mining process; then we provide a comprehensive overview of recent contributions in the domain of fair machine learning for graphs, that we classify into pre-processing, in-processing and post-processing models. We also propose to describe existing graph data, synthetic and real-world benchmarks. Finally, we present in detail five potential promising directions to advance research in studying algorithmic fairness on graphs.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2007596562",
                        "name": "Manvi Choudhary"
                    },
                    {
                        "authorId": "1947597",
                        "name": "Charlotte Laclau"
                    },
                    {
                        "authorId": "1725600",
                        "name": "C. Largeron"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "C1 Facebook\u2217 [63] group, individual 1,034 26,749 224 gender (2) [5], [24], [37], [39], [44], [73], [78], [80] [83], [87], [101], [128], [146], [171] Pokec\u2217 [142] group 1,632,803 30,622,564 59 region (2), gender (2) [38], [40], [50], [67], [82], [110] Twitter\u2217 [107] group 81,306 1,768,149 1,364 political opinion (2) [77], [83], [149] Lastfm [27] group, provider 49,900 518,647 gender (2), age (3) [117], [159], [164] Oklahoma97 [126] group 3,111 73,230 8 gender (2) [91] UNC28 [126] group 4,018 65,287 8 gender (2) [91] Google+ [107] popularity 4,938 547,923 5 [101] Epinion [144] popularity 8,806 157,887 [1], [174] Filmtrust [58] provider 3,579 35,494 [99] Ciao [144] popularity 7,317 85,205 [174]",
                "3) Rebalancing in Applications: Recommender Systems: Upsampling is a common rebalancing approach in recommender systems.",
                "Information Flow-Based Rebalancing: Information flowbased rebalancing techniques are commonly adopted to achieve fair influence maximization.",
                "[77] proposed CrossWalk, which extends the rebalancing range to the whole walk by assigning larger transition probabilities to nodes that are closer to the sensitive groups\u2019 topological peripheries.",
                "Node Sampling/Generation-Based Rebalancing: Rebalancing can also be achieved via node sampling or node generation.",
                "Other Applications: Rebalancing has been adopted to fulfill fairness in real-world applications other than recommender systems.",
                "1) Improving Group Fairness: Edge/Path-Based Rebalancing: A number of works adopt the rebalancing strategy to promote group fairness based on the edges or paths in the input network data.",
                "Rebalancing can also be leveraged to improve user fairness.",
                "Rebalancing aims to reduce the distribution difference of certain properties (e.g., the appearance rate of a node in random walks and frequency of recommendations of an item) between advantaged and disadvantaged nodes.",
                "Group Optimization with Regularization [71] Recommendation Optimization with Regularization [67] Node classification Optimization with Regularization [173] Node classification Optimization with Regularization [3] Node classification Optimization with Regularization [24] Link prediction Optimization with Regularization [167] Recommendation Optimization with Regularization [88] Node classification Optimization with Regularization [44] Node classification Optimization with Regularization [110] Node classification Optimization with Regularization [50] Node classification Optimization with Regularization [175] Recommendation Optimization with Regularization [52] Recommendation Optimization with Regularization [156] Link prediction Optimization with Regularization [83] Recommendation Optimization with Regularization [87] Link prediction Optimization with Regularization [21] Recommendation Optimization with Constraint(s) [80] Graph clustering Optimization with Constraint(s) [171] Node classification Optimization with Constraint(s) [45] Influence maximization Optimization with Constraint(s) [123] Influence maximization Optimization with Constraint(s) [124] Influence maximization Optimization with Constraint(s) [5] Influence maximization Rebalancing [128] Link prediction Rebalancing [25] Link prediction Rebalancing [171] Node classification Rebalancing [46] Recommendation Rebalancing [122] Recommendation Rebalancing [149] Node ranking Rebalancing [77] Influence maximization, link prediction, and node classification Rebalancing [146] Influence maximization Rebalancing [138] Influence maximization Rebalancing [148] Influence maximization Rebalancing [82] Node classification Rebalancing [37] Link prediction Adversarial [18] Recommendation Adversarial [38] Node classification Adversarial [78] Influence Maximization Adversarial [164] Recommendation Adversarial [159] Recommendation Adversarial [78] Influence maximization Edge rewiring [91] Node classification Edge rewiring [82] Node classification Edge rewiring [65] Topology debiasing Edge rewiring [40] Node classification Edge rewiring [82] Node classification Edge rewiring [136] Node classification Orthogonal projection [114] Node classification and recommendation Orthogonal projection [113] Node classification and recommendation Orthogonal projection [171] Node classification"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "8e11c4f14567d1257d4c8d8ac4e14446e1b99c36",
                "externalIds": {
                    "ArXiv": "2204.09888",
                    "DBLP": "journals/corr/abs-2204-09888",
                    "DOI": "10.1109/TKDE.2023.3265598",
                    "CorpusId": 248300164
                },
                "corpusId": 248300164,
                "publicationVenue": {
                    "id": "c6840156-ee10-4d78-8832-7f8909811576",
                    "name": "IEEE Transactions on Knowledge and Data Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Knowl Data Eng"
                    ],
                    "issn": "1041-4347",
                    "url": "https://www.computer.org/web/tkde",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=69"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8e11c4f14567d1257d4c8d8ac4e14446e1b99c36",
                "title": "Fairness in Graph Mining: A Survey",
                "abstract": "Graph mining algorithms have been playing a significant role in myriad fields over the years. However, despite their promising performance on various graph analytical tasks, most of these algorithms lack fairness considerations. As a consequence, they could lead to discrimination towards certain populations when exploited in human-centered applications. Recently, algorithmic fairness has been extensively studied in graph-based applications. In contrast to algorithmic fairness on independent and identically distributed (i.i.d.) data, fairness in graph mining has exclusive backgrounds, taxonomies, and fulfilling techniques. In this survey, we provide a comprehensive and up-to-date introduction of existing literature under the context of fair graph mining. Specifically, we propose a novel taxonomy of fairness notions on graphs, which sheds light on their connections and differences. We further present an organized summary of existing techniques that promote fairness in graph mining. Finally, we discuss current research challenges and open questions, aiming at encouraging cross-breeding ideas and further advances.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "123918726",
                        "name": "Yushun Dong"
                    },
                    {
                        "authorId": "2157405959",
                        "name": "Jing Ma"
                    },
                    {
                        "authorId": "2117075272",
                        "name": "Song Wang"
                    },
                    {
                        "authorId": "2127380428",
                        "name": "Chen Chen"
                    },
                    {
                        "authorId": "1737121128",
                        "name": "Jundong Li"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-08570",
                    "ArXiv": "2204.08570",
                    "DOI": "10.48550/arXiv.2204.08570",
                    "CorpusId": 248239981
                },
                "corpusId": 248239981,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cfb35f8c18fbc5baa453280ecd0aa8148bbba659",
                "title": "A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability",
                "abstract": "Graph Neural Networks (GNNs) have made rapid developments in the recent years. Due to their great ability in modeling graph-structured data, GNNs are vastly used in various applications, including high-stakes scenarios such as financial analysis, traffic predictions, and drug discovery. Despite their great potential in benefiting humans in the real world, recent study shows that GNNs can leak private information, are vulnerable to adversarial attacks, can inherit and magnify societal bias from training data and lack interpretability, which have risk of causing unintentional harm to the users and society. For example, existing works demonstrate that attackers can fool the GNNs to give the outcome they desire with unnoticeable perturbation on training graph. GNNs trained on social networks may embed the discrimination in their decision process, strengthening the undesirable societal bias. Consequently, trustworthy GNNs in various aspects are emerging to prevent the harm from GNN models and increase the users' trust in GNNs. In this paper, we give a comprehensive survey of GNNs in the computational aspects of privacy, robustness, fairness, and explainability. For each aspect, we give the taxonomy of the related methods and formulate the general frameworks for the multiple categories of trustworthy GNNs. We also discuss the future research directions of each aspect and connections between these aspects to help achieve trustworthiness.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152961073",
                        "name": "Enyan Dai"
                    },
                    {
                        "authorId": "1999191869",
                        "name": "Tianxiang Zhao"
                    },
                    {
                        "authorId": "1643792176",
                        "name": "Huaisheng Zhu"
                    },
                    {
                        "authorId": "2150636336",
                        "name": "Jun Xu"
                    },
                    {
                        "authorId": "2149465392",
                        "name": "Zhimeng Guo"
                    },
                    {
                        "authorId": "2146672392",
                        "name": "Hui Liu"
                    },
                    {
                        "authorId": "1736632",
                        "name": "Jiliang Tang"
                    },
                    {
                        "authorId": "2893721",
                        "name": "Suhang Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[100] propose a re-weighting approach for generating the random walks, however, they assign more weights to the links that connect nodes from different groups to provide a higher chance of discovery in extreme cases that Rahman et al.",
                "Repairing Graph [106, 151] Learn Unbiased Embeddings [29, 100, 140]"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0368102a9510ab93ab500322dbc7709b0a8a6e1b",
                "externalIds": {
                    "ArXiv": "2203.11852",
                    "DOI": "10.1145/3588433",
                    "CorpusId": 257558858
                },
                "corpusId": 257558858,
                "publicationVenue": {
                    "id": "7b2adce0-d53f-49d6-8784-b0645604fe62",
                    "name": "ACM Computing Surveys",
                    "type": "journal",
                    "alternate_names": [
                        "ACM Comput Surv"
                    ],
                    "issn": "0360-0300",
                    "url": "http://www.acm.org/pubs/surveys/",
                    "alternate_urls": [
                        "http://portal.acm.org/csur",
                        "https://csur.acm.org/",
                        "http://csur.acm.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0368102a9510ab93ab500322dbc7709b0a8a6e1b",
                "title": "Representation Bias in Data: A Survey on Identification and Resolution Techniques",
                "abstract": "Data-driven algorithms are only as good as the data they work with, while datasets, especially social data, often fail to represent minorities adequately. Representation Bias in data can happen due to various reasons, ranging from historical discrimination to selection and sampling biases in the data acquisition and preparation methods. Given that \u201cbias in, bias out,\u201d one cannot expect AI-based solutions to have equitable outcomes for societal applications, without addressing issues such as representation bias. While there has been extensive study of fairness in machine learning models, including several review papers, bias in the data has been less studied. This article reviews the literature on identifying and resolving representation bias as a feature of a dataset, independent of how consumed later. The scope of this survey is bounded to structured (tabular) and unstructured (e.g., image, text, graph) data. It presents taxonomies to categorize the studied techniques based on multiple design dimensions and provides a side-by-side comparison of their properties. There is still a long way to fully address representation bias issues in data. The authors hope that this survey motivates researchers to approach these challenges in the future by observing existing work within their respective domains.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1927032",
                        "name": "N. Shahbazi"
                    },
                    {
                        "authorId": "2117033724",
                        "name": "Yin Lin"
                    },
                    {
                        "authorId": "1717283",
                        "name": "Abolfazl Asudeh"
                    },
                    {
                        "authorId": "145531067",
                        "name": "H. Jagadish"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "\u2026typical accuracy, AUROC, F1-score, average precision as well as true and negative false rate have been widely used to assess predictive performance [Khajehnejad et al., 2021; Spinelli et al., 2021; Zeng et al., 2021; Ma et al., 2022; Tang et al., 2020b; Palowitch and Perozzi, 2020] while stability\u2026",
                "In terms of evaluation, in addition to the proposed graph fairness notions discussed in Section 4, the typical accuracy, AUROC, F1-score, average precision as well as true and negative false rate have been widely used to assess predictive performance [Khajehnejad et al., 2021; Spinelli et al., 2021; Zeng et al., 2021; Ma et al., 2022; Tang et al., 2020b; Palowitch and Perozzi, 2020] while stability [Agarwal et al."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "919ceada39bf7fec30965485ef1eb73f68afe2f8",
                "externalIds": {
                    "ArXiv": "2202.07170",
                    "CorpusId": 256697668
                },
                "corpusId": 256697668,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/919ceada39bf7fec30965485ef1eb73f68afe2f8",
                "title": "Fairness Amidst Non-IID Graph Data: Current Achievements and Future Directions",
                "abstract": "The importance of understanding and correcting algorithmic bias in machine learning (ML) has led to an increase in research on fairness in ML, which typically assumes that the underlying data is independent and identically distributed (IID). However, in reality, data is often represented using non-IID graph structures that capture connections among individual units. To address bias in ML systems, it is crucial to bridge the gap between the traditional fairness literature designed for IID data and the ubiquity of non-IID graph data. In this survey, we review such recent advance in fairness amidst non-IID graph data and identify datasets and evaluation metrics available for future research. We also point out the limitations of existing work as well as promising future directions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2027662138",
                        "name": "Wenbin Zhang"
                    },
                    {
                        "authorId": "2239443126",
                        "name": "Shimei Pan"
                    },
                    {
                        "authorId": "1788230",
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "authorId": "1733716",
                        "name": "T. Walsh"
                    },
                    {
                        "authorId": "1898068",
                        "name": "Jeremy C. Weiss"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Various methods have been proposed in this direction, including adversarial learning based approaches [2, 3, 6, 12, 21, 33], bayesian based approach [4], statistical based approaches [5, 16, 19, 23, 27, 35] and others [20, 22]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9361dfb0f38d80dadf373b6cda02d44b8ce43867",
                "externalIds": {
                    "DBLP": "conf/cikm/KangT21",
                    "DOI": "10.1145/3459637.3482030",
                    "CorpusId": 240230880
                },
                "corpusId": 240230880,
                "publicationVenue": {
                    "id": "7431ff67-91dc-41fa-b322-1b1ca657025f",
                    "name": "International Conference on Information and Knowledge Management",
                    "type": "conference",
                    "alternate_names": [
                        "Conference on Information and Knowledge Management",
                        "Conf Inf Knowl Manag",
                        "Int Conf Inf Knowl Manag",
                        "CIKM"
                    ],
                    "url": "http://www.cikm.org/"
                },
                "url": "https://www.semanticscholar.org/paper/9361dfb0f38d80dadf373b6cda02d44b8ce43867",
                "title": "Fair Graph Mining",
                "abstract": "In today's increasingly connected world, graph mining plays a pivotal role in many real-world application domains, including social network analysis, recommendations, marketing and financial security. Tremendous efforts have been made to develop a wide range of computational models. However, recent studies have revealed that many widely-applied graph mining models could suffer from potential discrimination. Fairness on graph mining aims to develop strategies in order to mitigate bias introduced/amplified during the mining process. The unique challenges of enforcing fairness on graph mining include (1) theoretical challenge on non-IID nature of graph data, which may invalidate the basic assumption behind many existing studies in fair machine learning, and (2) algorithmic challenge on the dilemma of balancing model accuracy and fairness. This tutorial aims to (1) present a comprehensive review of state-of-the-art techniques in fairness on graph mining and (2) identify the open challenges and future trends. In particular, we start with reviewing the background, problem definitions, unique challenges and related problems; then we will focus on an in-depth overview of (1) recent techniques in enforcing group fairness, individual fairness and other fairness notions in the context of graph mining, and (2) future directions in studying algorithmic fairness on graphs. We believe this tutorial could be attractive to researchers and practitioners in areas including data mining, artificial intelligence, social science and beneficial to a plethora of real-world application domains.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2111625448",
                        "name": "Jian Kang"
                    },
                    {
                        "authorId": "8163721",
                        "name": "Hanghang Tong"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "However, only a few studies [19] investigate the unfairness caused by the graph structure without knowing a sensitive feature."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "86c4bb73bcc10a17faee13034a1742008cb001df",
                "externalIds": {
                    "ArXiv": "2106.15535",
                    "DBLP": "conf/nips/MaDM21",
                    "CorpusId": 235669723
                },
                "corpusId": 235669723,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/86c4bb73bcc10a17faee13034a1742008cb001df",
                "title": "Subgroup Generalization and Fairness of Graph Neural Networks",
                "abstract": "Despite enormous successful applications of graph neural networks (GNNs), theoretical understanding of their generalization ability, especially for node-level tasks where data are not independent and identically-distributed (IID), has been sparse. The theoretical investigation of the generalization performance is beneficial for understanding fundamental issues (such as fairness) of GNN models and designing better learning methods. In this paper, we present a novel PAC-Bayesian analysis for GNNs under a non-IID semi-supervised learning setup. Moreover, we analyze the generalization performances on different subgroups of unlabeled nodes, which allows us to further study an accuracy-(dis)parity-style (un)fairness of GNNs from a theoretical perspective. Under reasonable assumptions, we demonstrate that the distance between a test subgroup and the training set can be a key factor affecting the GNN performance on that subgroup, which calls special attention to the training node selection for fair learning. Experiments across multiple GNN models and datasets support our theoretical results.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47793019",
                        "name": "Jiaqi Ma"
                    },
                    {
                        "authorId": "2153368600",
                        "name": "Junwei Deng"
                    },
                    {
                        "authorId": "1743469",
                        "name": "Q. Mei"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Crosswalk [33] is also a random walk-based embedding method, but, unlike Fairwalk, extends the range of the weighting including multi-hop neighbors.",
                "Most of the bias related work focuses on link prediction [32,23,24,49], node classification [33] and recommendation [36,8,45,10] tasks.",
                "NC REC LP EA Direct Group [33] [36,8,45,10] [32,23,24,36,8,45,33,10,49] [21,31]"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b21471bf20222466daae14136cd358304a1957ba",
                "externalIds": {
                    "DBLP": "conf/esws/FanourakisECKPS23",
                    "DOI": "10.1007/978-3-031-33455-9_5",
                    "CorpusId": 258911291
                },
                "corpusId": 258911291,
                "publicationVenue": {
                    "id": "c7bde2ee-6ad5-49c7-9498-a01e46c162eb",
                    "name": "Extended Semantic Web Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Eur Semantic Web Conf",
                        "Ext Semantic Web Conf",
                        "European Semantic Web Conference",
                        "ESWC"
                    ],
                    "url": "https://link.springer.com/conference/esws"
                },
                "url": "https://www.semanticscholar.org/paper/b21471bf20222466daae14136cd358304a1957ba",
                "title": "Structural Bias in Knowledge Graphs for the Entity Alignment Task",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "20596647",
                        "name": "N. Fanourakis"
                    },
                    {
                        "authorId": "3226264",
                        "name": "Vasilis Efthymiou"
                    },
                    {
                        "authorId": "3009404",
                        "name": "V. Christophides"
                    },
                    {
                        "authorId": "2078908",
                        "name": "D. Kotzinos"
                    },
                    {
                        "authorId": "1781993",
                        "name": "E. Pitoura"
                    },
                    {
                        "authorId": "12619004",
                        "name": "K. Stefanidis"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cc15128e6d2291b86b2264317a15e79f73993330",
                "externalIds": {
                    "CorpusId": 263765539
                },
                "corpusId": 263765539,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cc15128e6d2291b86b2264317a15e79f73993330",
                "title": "[Re] CrossWalk: Fairness-enhanced Node Representation Learning",
                "abstract": "Scope of Reproducibility \u2014 This work aims to reproduce the findings of the paper \u201cCross\u2010 Walk: Fairness\u2010enhanced Node Representation Learning\u201d [1] by investigating the two main claims made by the authors about CrossWalk, which suggest that (i) CrossWalk enhances fairness in three graph algorithms, while only suffering from small decreases in performance, and that (ii) CrossWalk preserves the necessary structural properties of the graph while reducing disparity.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2256403448",
                        "name": "Luca Pantea"
                    },
                    {
                        "authorId": "2199328174",
                        "name": "Andrei Blahovici"
                    },
                    {
                        "authorId": "2256275271",
                        "name": "Koustuv Sinha"
                    },
                    {
                        "authorId": "1452678770",
                        "name": "Maurits J. R. Bleeker"
                    },
                    {
                        "authorId": "81679142",
                        "name": "Samarth Bhargav"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Other than the individual and group level parity based graph fairness [Spinelli et al., 2021; Khajehnejad et al., 2021; Buyl and Bie, 2021], graph causal reasoning fairness has been investigated, particularly graph counterfactual fairness."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "50be62b588d079737a9c9c8500c2957ac6c6a703",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-07170",
                    "CorpusId": 246863892
                },
                "corpusId": 246863892,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/50be62b588d079737a9c9c8500c2957ac6c6a703",
                "title": "Fairness Amidst Non-IID Graph Data: A Literature Review",
                "abstract": "Fairness in machine learning (ML), the process to understand and correct algorithmic bias, has gained increasing attention with numerous literature being carried out, commonly assume the underlying data is independent and identically distributed (IID). On the other hand, graphs are a ubiquitous data structure to capture connections among individual units and is non-IID by nature. It is therefore of great importance to bridge the traditional fairness literature designed on IID data and ubiquitous non-IID graph representations to tackle bias in ML systems. In this survey, we review such recent advance in fairness amidst non-IID graph data and identify datasets and evaluation metrics available for future research. We also point out the limitations of existing work as well as promising future directions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2027662138",
                        "name": "Wenbin Zhang"
                    },
                    {
                        "authorId": "1898068",
                        "name": "Jeremy C. Weiss"
                    },
                    {
                        "authorId": "1788230",
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "authorId": "1733716",
                        "name": "T. Walsh"
                    }
                ]
            }
        }
    ]
}