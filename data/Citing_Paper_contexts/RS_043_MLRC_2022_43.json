{
    "offset": 0,
    "data": [
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "8ffd6ec9c51576c84e1e04dac17c3652fe564037",
                "externalIds": {
                    "ArXiv": "2309.14975",
                    "CorpusId": 262822580
                },
                "corpusId": 262822580,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8ffd6ec9c51576c84e1e04dac17c3652fe564037",
                "title": "Low-Cost Exoskeletons for Learning Whole-Arm Manipulation in the Wild",
                "abstract": "While humans can use parts of their arms other than the hands for manipulations like gathering and supporting, whether robots can effectively learn and perform the same type of operations remains relatively unexplored. As these manipulations require joint-level control to regulate the complete poses of the robots, we develop AirExo, a low-cost, adaptable, and portable dual-arm exoskeleton, for teleoperation and demonstration collection. As collecting teleoperated data is expensive and time-consuming, we further leverage AirExo to collect cheap in-the-wild demonstrations at scale. Under our in-the-wild learning framework, we show that with only 3 minutes of the teleoperated demonstrations, augmented by diverse and extensive in-the-wild data collected by AirExo, robots can learn a policy that is comparable to or even better than one learned from teleoperated demonstrations lasting over 20 minutes. Experiments demonstrate that our approach enables the model to learn a more general and robust policy across the various stages of the task, enhancing the success rates in task completion even with the presence of disturbances. Project website: https://airexo.github.io/",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2152115958",
                        "name": "Hongjie Fang"
                    },
                    {
                        "authorId": "122851212",
                        "name": "Haoshu Fang"
                    },
                    {
                        "authorId": "21595671",
                        "name": "Yiming Wang"
                    },
                    {
                        "authorId": "120532949",
                        "name": "Jieji Ren"
                    },
                    {
                        "authorId": "47740650",
                        "name": "Jing Chen"
                    },
                    {
                        "authorId": "2228276343",
                        "name": "Ruo Zhang"
                    },
                    {
                        "authorId": "39899748",
                        "name": "Weiming Wang"
                    },
                    {
                        "authorId": "1830034",
                        "name": "Cewu Lu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "T-DEX [13] BC-BeT [62] Tactile Only Image + Tactile Reward AVI [14] TAVI Peg Insertion 2/10 0/10 6/10 6/10 6/10 8/10 Sponge Flipping 1/10 0/10 8/10 4/10 3/10 8/10 Eraser Turning 2/10 0/10 0/10 2/10 0/10 5/10 Bowl Unstacking 1/10 0/10 5/10 0/10 3/10 9/10 Plier Picking 0/10 0/10 4/10 4/10 6/10 7/10 Mint Opening 4/10 0/10 0/10 5/10 1/10 7/10 Avg.",
                "2) BC-BeT [62]: We implement and run a state-of-theart behavior cloning method Behavior Transformers.",
                "We see that BC-BeT is unable to complete any of the tasks, quickly going out of distribution and failing to recover."
            ],
            "citingPaper": {
                "paperId": "334d6aa9b6e85d3ed3fb13daf5c2aeda89ee2b27",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-12300",
                    "ArXiv": "2309.12300",
                    "DOI": "10.48550/arXiv.2309.12300",
                    "CorpusId": 262084312
                },
                "corpusId": 262084312,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/334d6aa9b6e85d3ed3fb13daf5c2aeda89ee2b27",
                "title": "See to Touch: Learning Tactile Dexterity through Visual Incentives",
                "abstract": "Equipping multi-fingered robots with tactile sensing is crucial for achieving the precise, contact-rich, and dexterous manipulation that humans excel at. However, relying solely on tactile sensing fails to provide adequate cues for reasoning about objects' spatial configurations, limiting the ability to correct errors and adapt to changing situations. In this paper, we present Tactile Adaptation from Visual Incentives (TAVI), a new framework that enhances tactile-based dexterity by optimizing dexterous policies using vision-based rewards. First, we use a contrastive-based objective to learn visual representations. Next, we construct a reward function using these visual representations through optimal-transport based matching on one human demonstration. Finally, we use online reinforcement learning on our robot to optimize tactile-based policies that maximize the visual reward. On six challenging tasks, such as peg pick-and-place, unstacking bowls, and flipping slender objects, TAVI achieves a success rate of 73% using our four-fingered Allegro robot hand. The increase in performance is 108% higher than policies using tactile and vision-based rewards and 135% higher than policies without tactile observational input. Robot videos are best viewed on our project website: https://see-to-touch.github.io/.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2143167646",
                        "name": "Irmak Guzey"
                    },
                    {
                        "authorId": "2243500130",
                        "name": "Yinlong Dai"
                    },
                    {
                        "authorId": "2153473632",
                        "name": "Ben Evans"
                    },
                    {
                        "authorId": "2127604",
                        "name": "Soumith Chintala"
                    },
                    {
                        "authorId": "34026610",
                        "name": "Lerrel Pinto"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "9572fa00e594862f1704b90f9f218069cf83af30",
                "externalIds": {
                    "ArXiv": "2309.10175",
                    "DBLP": "journals/corr/abs-2309-10175",
                    "DOI": "10.48550/arXiv.2309.10175",
                    "CorpusId": 262054598
                },
                "corpusId": 262054598,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9572fa00e594862f1704b90f9f218069cf83af30",
                "title": "One ACT Play: Single Demonstration Behavior Cloning with Action Chunking Transformers",
                "abstract": "Learning from human demonstrations (behavior cloning) is a cornerstone of robot learning. However, most behavior cloning algorithms require a large number of demonstrations to learn a task, especially for general tasks that have a large variety of initial conditions. Humans, however, can learn to complete tasks, even complex ones, after only seeing one or two demonstrations. Our work seeks to emulate this ability, using behavior cloning to learn a task given only a single human demonstration. We achieve this goal by using linear transforms to augment the single demonstration, generating a set of trajectories for a wide range of initial conditions. With these demonstrations, we are able to train a behavior cloning agent to successfully complete three block manipulation tasks. Additionally, we developed a novel addition to the temporal ensembling method used by action chunking agents during inference. By incorporating the standard deviation of the action predictions into the ensembling method, our approach is more robust to unforeseen changes in the environment, resulting in significant performance improvements.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2186052477",
                        "name": "Abraham George"
                    },
                    {
                        "authorId": "3614493",
                        "name": "A. Farimani"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "ab8279ad4a607930ab16f11d2670ab4e3cbb153a",
                "externalIds": {
                    "ArXiv": "2309.10150",
                    "DBLP": "journals/corr/abs-2309-10150",
                    "DOI": "10.48550/arXiv.2309.10150",
                    "CorpusId": 262054345
                },
                "corpusId": 262054345,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ab8279ad4a607930ab16f11d2670ab4e3cbb153a",
                "title": "Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions",
                "abstract": "In this work, we present a scalable reinforcement learning method for training multi-task policies from large offline datasets that can leverage both human demonstrations and autonomously collected data. Our method uses a Transformer to provide a scalable representation for Q-functions trained via offline temporal difference backups. We therefore refer to the method as Q-Transformer. By discretizing each action dimension and representing the Q-value of each action dimension as separate tokens, we can apply effective high-capacity sequence modeling techniques for Q-learning. We present several design decisions that enable good performance with offline RL training, and show that Q-Transformer outperforms prior offline RL algorithms and imitation learning techniques on a large diverse real-world robotic manipulation task suite. The project's website and videos can be found at https://q-transformer.github.io",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2527420",
                        "name": "Yevgen Chebotar"
                    },
                    {
                        "authorId": "144579461",
                        "name": "Q. Vuong"
                    },
                    {
                        "authorId": "17818078",
                        "name": "A. Irpan"
                    },
                    {
                        "authorId": "1944801",
                        "name": "Karol Hausman"
                    },
                    {
                        "authorId": "144956443",
                        "name": "F. Xia"
                    },
                    {
                        "authorId": "2161346119",
                        "name": "Yao Lu"
                    },
                    {
                        "authorId": "1488785534",
                        "name": "Aviral Kumar"
                    },
                    {
                        "authorId": "10909315",
                        "name": "Tianhe Yu"
                    },
                    {
                        "authorId": "1505793452",
                        "name": "Alexander Herzog"
                    },
                    {
                        "authorId": "31719101",
                        "name": "Karl Pertsch"
                    },
                    {
                        "authorId": "2161342233",
                        "name": "K. Gopalakrishnan"
                    },
                    {
                        "authorId": "46920727",
                        "name": "Julian Ibarz"
                    },
                    {
                        "authorId": "7624658",
                        "name": "Ofir Nachum"
                    },
                    {
                        "authorId": "34365421",
                        "name": "S. Sontakke"
                    },
                    {
                        "authorId": "2196524735",
                        "name": "Grecia Salazar"
                    },
                    {
                        "authorId": "2195355151",
                        "name": "Huong Tran"
                    },
                    {
                        "authorId": "2195951533",
                        "name": "Jodilyn Peralta"
                    },
                    {
                        "authorId": "2161386250",
                        "name": "Clayton Tan"
                    },
                    {
                        "authorId": "2064295888",
                        "name": "D. Manjunath"
                    },
                    {
                        "authorId": "2243236877",
                        "name": "Jaspiar Singht"
                    },
                    {
                        "authorId": "2196524598",
                        "name": "Brianna Zitkovich"
                    },
                    {
                        "authorId": "2175779811",
                        "name": "Tomas Jackson"
                    },
                    {
                        "authorId": "2251957",
                        "name": "Kanishka Rao"
                    },
                    {
                        "authorId": "46881670",
                        "name": "Chelsea Finn"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "Inspired by the recent cross-pollination of natural language processing (NLP) techniques in offline RL (Chen et al., 2021; Janner et al., 2021; Shafiullah et al., 2022), we take a different approach.",
                "Prior work has leveraged these ideas in similar contexts (Janner et al., 2021; Shafiullah et al., 2022; Jiang et al., 2022) and we follow suit.",
                ", 2021), Behavior Transformer (Shafiullah et al., 2022), and TAP (Jiang et al.",
                "Architectures from NLP have made their way into Offline RL (Chen et al., 2021; Janner et al., 2021; Shafiullah et al., 2022), but as we have demonstrated, there is a trove of further techniques to explore.",
                "Following prior work (Dadashi et al., 2021; Shafiullah et al., 2022), we discretize the action space and use a modified byte-pair encoding (BPE) scheme (Gage, 1994; Sennrich et al.",
                "Methods like Trajectory Transformer (Janner et al., 2021), Behavior Transformer (Shafiullah et al., 2022), and TAP (Jiang et al., 2022) perform supervised learning on top of trajectory data using a discrete action-space derived from the data.",
                "Discretization removes resolution from the action space, which will be detrimental in settings like fast locomotion that require the full range, but this may potentially be fixed by a residual correction (Shafiullah et al., 2022)."
            ],
            "citingPaper": {
                "paperId": "cd6d2eca8c03ae9bc75c5e8d55e36be1dc088893",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-04459",
                    "ArXiv": "2309.04459",
                    "DOI": "10.48550/arXiv.2309.04459",
                    "CorpusId": 261660568
                },
                "corpusId": 261660568,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cd6d2eca8c03ae9bc75c5e8d55e36be1dc088893",
                "title": "Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning",
                "abstract": "Exploration in sparse-reward reinforcement learning is difficult due to the requirement of long, coordinated sequences of actions in order to achieve any reward. Moreover, in continuous action spaces there are an infinite number of possible actions, which only increases the difficulty of exploration. One class of methods designed to address these issues forms temporally extended actions, often called skills, from interaction data collected in the same domain, and optimizes a policy on top of this new action space. Typically such methods require a lengthy pretraining phase, especially in continuous action spaces, in order to form the skills before reinforcement learning can begin. Given prior evidence that the full range of the continuous action space is not required in such tasks, we propose a novel approach to skill-generation with two components. First we discretize the action space through clustering, and second we leverage a tokenization technique borrowed from natural language processing to generate temporally extended actions. Such a method outperforms baselines for skill-generation in several challenging sparse-reward domains, and requires orders-of-magnitude less computation in skill-generation and online rollouts.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "50393053",
                        "name": "David Yunis"
                    },
                    {
                        "authorId": "2238921746",
                        "name": "Justin Jung"
                    },
                    {
                        "authorId": "2238627927",
                        "name": "Falcon Dai"
                    },
                    {
                        "authorId": "1733702",
                        "name": "Matthew R. Walter"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Among these approaches, we find that action-clustering-based approaches (BeT [68]) for multi-task settings, perform significantly worse.",
                "BeT [68]: We modify the Behavior Transformer architecture with language conditioning and train it in a multi-task manner."
            ],
            "citingPaper": {
                "paperId": "148e95859248878a0695a31ef6165614a01df631",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-01918",
                    "ArXiv": "2309.01918",
                    "DOI": "10.48550/arXiv.2309.01918",
                    "CorpusId": 261518421
                },
                "corpusId": 261518421,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/148e95859248878a0695a31ef6165614a01df631",
                "title": "RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking",
                "abstract": "The grand aim of having a single robot that can manipulate arbitrary objects in diverse settings is at odds with the paucity of robotics datasets. Acquiring and growing such datasets is strenuous due to manual efforts, operational costs, and safety challenges. A path toward such an universal agent would require a structured framework capable of wide generalization but trained within a reasonable data budget. In this paper, we develop an efficient system (RoboAgent) for training universal agents capable of multi-task manipulation skills using (a) semantic augmentations that can rapidly multiply existing datasets and (b) action representations that can extract performant policies with small yet diverse multi-modal datasets without overfitting. In addition, reliable task conditioning and an expressive policy architecture enable our agent to exhibit a diverse repertoire of skills in novel situations specified using language commands. Using merely 7500 demonstrations, we are able to train a single agent capable of 12 unique skills, and demonstrate its generalization over 38 tasks spread across common daily activities in diverse kitchen scenes. On average, RoboAgent outperforms prior methods by over 40% in unseen situations while being more sample efficient and being amenable to capability improvements and extensions through fine-tuning. Videos at https://robopen.github.io/",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51113848",
                        "name": "Homanga Bharadhwaj"
                    },
                    {
                        "authorId": "2215216078",
                        "name": "Jay Vakil"
                    },
                    {
                        "authorId": "2237724790",
                        "name": "Mohit Sharma"
                    },
                    {
                        "authorId": "2117767136",
                        "name": "Abhi Gupta"
                    },
                    {
                        "authorId": "2757335",
                        "name": "Shubham Tulsiani"
                    },
                    {
                        "authorId": "2237996121",
                        "name": "Vikash Kumar"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[19] for multimodal behavior cloning: the set of all action vectors present in the training split is partitioned into K clusters using k-means, and each action a is then decomposed as the sum of a cluster center and an offset, i.",
                "The first term in the expression above is the focal loss function [20, 19], where ptrue denotes the probability output by the softmax layer for the ground truth cluster at timestep i."
            ],
            "citingPaper": {
                "paperId": "5ef87dda04569d89559db9ac83833433416fa66e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-13278",
                    "ArXiv": "2308.13278",
                    "DOI": "10.48550/arXiv.2308.13278",
                    "CorpusId": 261214553
                },
                "corpusId": 261214553,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5ef87dda04569d89559db9ac83833433416fa66e",
                "title": "Integrating LLMs and Decision Transformers for Language Grounded Generative Quality-Diversity",
                "abstract": "Quality-Diversity is a branch of stochastic optimization that is often applied to problems from the Reinforcement Learning and control domains in order to construct repertoires of well-performing policies/skills that exhibit diversity with respect to a behavior space. Such archives are usually composed of a finite number of reactive agents which are each associated to a unique behavior descriptor, and instantiating behavior descriptors outside of that coarsely discretized space is not straight-forward. While a few recent works suggest solutions to that issue, the trajectory that is generated is not easily customizable beyond the specification of a target behavior descriptor. We propose to jointly solve those problems in environments where semantic information about static scene elements is available by leveraging a Large Language Model to augment the repertoire with natural language descriptions of trajectories, and training a policy conditioned on those descriptions. Thus, our method allows a user to not only specify an arbitrary target behavior descriptor, but also provide the model with a high-level textual prompt to shape the generated trajectory. We also propose an LLM-based approach to evaluating the performance of such generative agents. Furthermore, we develop a benchmark based on simulated robot navigation in a 2d maze that we use for experimental validation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "30095335",
                        "name": "Achkan Salehi"
                    },
                    {
                        "authorId": "1765955",
                        "name": "S. Doncieux"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "6a375be82efc01ec4ed73334655935a56ba82d38",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-12952",
                    "ArXiv": "2308.12952",
                    "DOI": "10.48550/arXiv.2308.12952",
                    "CorpusId": 261100981
                },
                "corpusId": 261100981,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6a375be82efc01ec4ed73334655935a56ba82d38",
                "title": "BridgeData V2: A Dataset for Robot Learning at Scale",
                "abstract": "We introduce BridgeData V2, a large and diverse dataset of robotic manipulation behaviors designed to facilitate research on scalable robot learning. BridgeData V2 contains 60,096 trajectories collected across 24 environments on a publicly available low-cost robot. BridgeData V2 provides extensive task and environment variability, leading to skills that can generalize across environments, domains, and institutions, making the dataset a useful resource for a broad range of researchers. Additionally, the dataset is compatible with a wide variety of open-vocabulary, multi-task learning methods conditioned on goal images or natural language instructions. In our experiments, we train 6 state-of-the-art imitation learning and offline reinforcement learning methods on our dataset, and find that they succeed on a suite of tasks requiring varying amounts of generalization. We also demonstrate that the performance of these methods improves with more data and higher capacity models, and that training on a greater variety of skills leads to improved generalization. By publicly sharing BridgeData V2 and our pre-trained models, we aim to accelerate research in scalable robot learning methods. Project page at https://rail-berkeley.github.io/bridgedata",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2029241116",
                        "name": "Homer Walke"
                    },
                    {
                        "authorId": "2069483822",
                        "name": "Kevin Black"
                    },
                    {
                        "authorId": "2233425761",
                        "name": "Abraham Lee"
                    },
                    {
                        "authorId": "2159987907",
                        "name": "Moo Jin Kim"
                    },
                    {
                        "authorId": "117791840",
                        "name": "Maximilian Du"
                    },
                    {
                        "authorId": "1382713388",
                        "name": "Chongyi Zheng"
                    },
                    {
                        "authorId": "145914976",
                        "name": "Tony Zhao"
                    },
                    {
                        "authorId": "2163582335",
                        "name": "Philippe Hansen-Estruch"
                    },
                    {
                        "authorId": "144579461",
                        "name": "Q. Vuong"
                    },
                    {
                        "authorId": "2162736405",
                        "name": "Andre Wang He"
                    },
                    {
                        "authorId": "1823943196",
                        "name": "Vivek Myers"
                    },
                    {
                        "authorId": "145213709",
                        "name": "Kuan Fang"
                    },
                    {
                        "authorId": "46881670",
                        "name": "Chelsea Finn"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "f20d906fcae5c2475e00f14cdd87c1ebacdcffcb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-14326",
                    "ArXiv": "2307.14326",
                    "DOI": "10.48550/arXiv.2307.14326",
                    "CorpusId": 260164917
                },
                "corpusId": 260164917,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f20d906fcae5c2475e00f14cdd87c1ebacdcffcb",
                "title": "Waypoint-Based Imitation Learning for Robotic Manipulation",
                "abstract": "While imitation learning methods have seen a resurgent interest for robotic manipulation, the well-known problem of compounding errors continues to afflict behavioral cloning (BC). Waypoints can help address this problem by reducing the horizon of the learning problem for BC, and thus, the errors compounded over time. However, waypoint labeling is underspecified, and requires additional human supervision. Can we generate waypoints automatically without any additional human supervision? Our key insight is that if a trajectory segment can be approximated by linear motion, the endpoints can be used as waypoints. We propose Automatic Waypoint Extraction (AWE) for imitation learning, a preprocessing module to decompose a demonstration into a minimal set of waypoints which when interpolated linearly can approximate the trajectory up to a specified error threshold. AWE can be combined with any BC algorithm, and we find that AWE can increase the success rate of state-of-the-art algorithms by up to 25% in simulation and by 4-28% on real-world bimanual manipulation tasks, reducing the decision making horizon by up to a factor of 10. Videos and code are available at https://lucys0.github.io/awe/",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2117039498",
                        "name": "Lu Shi"
                    },
                    {
                        "authorId": "50465276",
                        "name": "Archit Sharma"
                    },
                    {
                        "authorId": "145914976",
                        "name": "Tony Zhao"
                    },
                    {
                        "authorId": "46881670",
                        "name": "Chelsea Finn"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "af6d0ba799213cbbcbfceb1fb9b78d2858486308",
                "externalIds": {
                    "ArXiv": "2307.14535",
                    "DBLP": "journals/corr/abs-2307-14535",
                    "DOI": "10.48550/arXiv.2307.14535",
                    "CorpusId": 260203080
                },
                "corpusId": 260203080,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/af6d0ba799213cbbcbfceb1fb9b78d2858486308",
                "title": "Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition",
                "abstract": "We present a framework for robot skill acquisition, which 1) efficiently scale up data generation of language-labelled robot data and 2) effectively distills this data down into a robust multi-task language-conditioned visuo-motor policy. For (1), we use a large language model (LLM) to guide high-level planning, and sampling-based robot planners (e.g. motion or grasp samplers) for generating diverse and rich manipulation trajectories. To robustify this data-collection process, the LLM also infers a code-snippet for the success condition of each task, simultaneously enabling the data-collection process to detect failure and retry as well as the automatic labeling of trajectories with success/failure. For (2), we extend the diffusion policy single-task behavior-cloning approach to multi-task settings with language conditioning. Finally, we propose a new multi-task benchmark with 18 tasks across five domains to test long-horizon behavior, common-sense reasoning, tool-use, and intuitive physics. We find that our distilled policy successfully learned the robust retrying behavior in its data collection procedure, while improving absolute success rates by 33.2% on average across five domains. Code, data, and additional qualitative results are available on https://www.cs.columbia.edu/~huy/scalingup/.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "9091886",
                        "name": "Huy Ha"
                    },
                    {
                        "authorId": "47686265",
                        "name": "Peter R. Florence"
                    },
                    {
                        "authorId": "3340170",
                        "name": "Shuran Song"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "c34f420af1564434a6a6b1576e8f6a34c001b925",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-09955",
                    "ArXiv": "2307.09955",
                    "DOI": "10.48550/arXiv.2307.09955",
                    "CorpusId": 259982636
                },
                "corpusId": 259982636,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c34f420af1564434a6a6b1576e8f6a34c001b925",
                "title": "XSkill: Cross Embodiment Skill Discovery",
                "abstract": "Human demonstration videos are a widely available data source for robot learning and an intuitive user interface for expressing desired behavior. However, directly extracting reusable robot manipulation skills from unstructured human videos is challenging due to the big embodiment difference and unobserved action parameters. To bridge this embodiment gap, this paper introduces XSkill, an imitation learning framework that 1) discovers a cross-embodiment representation called skill prototypes purely from unlabeled human and robot manipulation videos, 2) transfers the skill representation to robot actions using conditional diffusion policy, and finally, 3) composes the learned skill to accomplish unseen tasks specified by a human prompt video. Our experiments in simulation and real-world environments show that the discovered skill prototypes facilitate both skill transfer and composition for unseen tasks, resulting in a more general and scalable imitation learning framework. The benchmark, code, and qualitative results are on https://xskill.cs.columbia.edu/",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2110683030",
                        "name": "Mengda Xu"
                    },
                    {
                        "authorId": "74498275",
                        "name": "Zhenjia Xu"
                    },
                    {
                        "authorId": "46859937",
                        "name": "Cheng Chi"
                    },
                    {
                        "authorId": "1956361",
                        "name": "M. Veloso"
                    },
                    {
                        "authorId": "3340170",
                        "name": "Shuran Song"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "b54798dd87b442aaa6888a1502200e9fe7a190ab",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-08927",
                    "ArXiv": "2307.08927",
                    "DOI": "10.48550/arXiv.2307.08927",
                    "CorpusId": 259951009
                },
                "corpusId": 259951009,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/b54798dd87b442aaa6888a1502200e9fe7a190ab",
                "title": "Multi-Stage Cable Routing through Hierarchical Imitation Learning",
                "abstract": "We study the problem of learning to perform multi-stage robotic manipulation tasks, with applications to cable routing, where the robot must route a cable through a series of clips. This setting presents challenges representative of complex multi-stage robotic manipulation scenarios: handling deformable objects, closing the loop on visual perception, and handling extended behaviors consisting of multiple steps that must be executed successfully to complete the entire task. In such settings, learning individual primitives for each stage that succeed with a high enough rate to perform a complete temporally extended task is impractical: if each stage must be completed successfully and has a non-negligible probability of failure, the likelihood of successful completion of the entire task becomes negligible. Therefore, successful controllers for such multi-stage tasks must be able to recover from failure and compensate for imperfections in low-level controllers by smartly choosing which controllers to trigger at any given time, retrying, or taking corrective action as needed. To this end, we describe an imitation learning system that uses vision-based policies trained from demonstrations at both the lower (motor control) and the upper (sequencing) level, present a system for instantiating this method to learn the cable routing task, and perform evaluations showing great performance in generalizing to very challenging clip placement variations. Supplementary videos, datasets, and code can be found at https://sites.google.com/view/cablerouting.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2591708",
                        "name": "Jianlan Luo"
                    },
                    {
                        "authorId": "2223881088",
                        "name": "Charles Xu"
                    },
                    {
                        "authorId": "3468192",
                        "name": "Xinyang Geng"
                    },
                    {
                        "authorId": "2184667723",
                        "name": "Gilbert Feng"
                    },
                    {
                        "authorId": "145213709",
                        "name": "Kuan Fang"
                    },
                    {
                        "authorId": "2106350498",
                        "name": "L. Tan"
                    },
                    {
                        "authorId": "1745219",
                        "name": "S. Schaal"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "There has been success addressing multi-modality by performing classification over a discretized version of the search space [8, 42, 44, 46, 47, 49], but these methods are typically less precise."
            ],
            "citingPaper": {
                "paperId": "5419479e05f312abc78ea54eab030936d66d4e32",
                "externalIds": {
                    "ArXiv": "2307.04751",
                    "DBLP": "journals/corr/abs-2307-04751",
                    "DOI": "10.48550/arXiv.2307.04751",
                    "CorpusId": 259501343
                },
                "corpusId": 259501343,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5419479e05f312abc78ea54eab030936d66d4e32",
                "title": "Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal Rearrangement",
                "abstract": "We propose a system for rearranging objects in a scene to achieve a desired object-scene placing relationship, such as a book inserted in an open slot of a bookshelf. The pipeline generalizes to novel geometries, poses, and layouts of both scenes and objects, and is trained from demonstrations to operate directly on 3D point clouds. Our system overcomes challenges associated with the existence of many geometrically-similar rearrangement solutions for a given scene. By leveraging an iterative pose de-noising training procedure, we can fit multi-modal demonstration data and produce multi-modal outputs while remaining precise and accurate. We also show the advantages of conditioning on relevant local geometric features while ignoring irrelevant global structure that harms both generalization and precision. We demonstrate our approach on three distinct rearrangement tasks that require handling multi-modality and generalization over object shape and pose in both simulation and the real world. Project website, code, and videos: https://anthonysimeonov.github.io/rpdiff-multi-modal/",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "14993397",
                        "name": "A. Simeonov"
                    },
                    {
                        "authorId": "47989608",
                        "name": "Ankit Goyal"
                    },
                    {
                        "authorId": "2033958",
                        "name": "Lucas Manuelli"
                    },
                    {
                        "authorId": "1485124622",
                        "name": "Lin Yen-Chen"
                    },
                    {
                        "authorId": "2221849610",
                        "name": "Alina Sarmiento"
                    },
                    {
                        "authorId": "2157311710",
                        "name": "Alberto Rodriguez"
                    },
                    {
                        "authorId": "33932184",
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "authorId": "145197953",
                        "name": "D. Fox"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "Learning robot manipulation from demonstrations Many recent work train multi-task manipulation policies that leverage Transformer architectures [1, 2, 3, 5, 23, 24] to predict robot actions from video input and language instructions."
            ],
            "citingPaper": {
                "paperId": "7275318008bad0c141d3516a25903b37014590a1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-17817",
                    "ArXiv": "2306.17817",
                    "DOI": "10.48550/arXiv.2306.17817",
                    "CorpusId": 259308821
                },
                "corpusId": 259308821,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7275318008bad0c141d3516a25903b37014590a1",
                "title": "Act3D: Infinite Resolution Action Detection Transformer for Robotic Manipulation",
                "abstract": "3D perceptual representations are well suited for robot manipulation as they easily encode occlusions and simplify spatial reasoning. Many manipulation tasks require high spatial precision in end-effector pose prediction, typically demanding high-resolution 3D perceptual grids that are computationally expensive to process. As a result, most manipulation policies operate directly in 2D, foregoing 3D inductive biases. In this paper, we propose Act3D, a manipulation policy Transformer that casts 6-DoF keypose prediction as 3D detection with adaptive spatial computation. It takes as input 3D feature clouds unprojected from one or more camera views, iteratively samples 3D point grids in free space in a coarse-to-fine manner, featurizes them using relative spatial attention to the physical feature cloud, and selects the best feature point for end-effector pose prediction. Act3D sets a new state-of-the-art in RLbench, an established manipulation benchmark. Our model achieves 10% absolute improvement over the previous SOTA 2D multi-view policy on 74 RLbench tasks and 22% absolute improvement with 3x less compute over the previous SOTA 3D policy. In thorough ablations, we show the importance of relative spatial attention, large-scale vision-language pre-trained 2D backbones, and weight tying across coarse-to-fine attentions. Code and videos are available at our project site: https://act3d.github.io/.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "81588783",
                        "name": "Th\u00e9ophile Gervet"
                    },
                    {
                        "authorId": "2060151696",
                        "name": "Zhou Xian"
                    },
                    {
                        "authorId": "3070188",
                        "name": "N. Gkanatsios"
                    },
                    {
                        "authorId": "1705557",
                        "name": "Katerina Fragkiadaki"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "In future work, we will extend recently proposed alternative approaches for handling multimodality such as Behavior Transformers [30] and Diffusion Policies [34] to the IFL setting and compare them to IIFL.",
                "Shafiullah et al. [30] propose Behavior Transformers, a technique that applies the multi-token prediction of Transformer neural networks [31] to imitation learning.",
                "[30] propose Behavior Transformers, a technique that applies the multi-token prediction of Transformer neural networks [31] to imitation learning."
            ],
            "citingPaper": {
                "paperId": "e43973851681ad7f0e3c49d4e6808a87d6b0e26b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-15228",
                    "ArXiv": "2306.15228",
                    "DOI": "10.48550/arXiv.2306.15228",
                    "CorpusId": 259262205
                },
                "corpusId": 259262205,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e43973851681ad7f0e3c49d4e6808a87d6b0e26b",
                "title": "IIFL: Implicit Interactive Fleet Learning from Heterogeneous Human Supervisors",
                "abstract": "Imitation learning has been applied to a range of robotic tasks, but can struggle when (1) robots encounter edge cases that are not represented in the training data (distribution shift) or (2) the human demonstrations are heterogeneous: taking different paths around an obstacle, for instance (multimodality). Interactive fleet learning (IFL) mitigates distribution shift by allowing robots to access remote human teleoperators during task execution and learn from them over time, but is not equipped to handle multimodality. Recent work proposes Implicit Behavior Cloning (IBC), which is able to represent multimodal demonstrations using energy-based models (EBMs). In this work, we propose addressing both multimodality and distribution shift with Implicit Interactive Fleet Learning (IIFL), the first extension of implicit policies to interactive imitation learning (including the single-robot, single-human setting). IIFL quantifies uncertainty using a novel application of Jeffreys divergence to EBMs. While IIFL is more computationally expensive than explicit methods, results suggest that IIFL achieves 4.5x higher return on human effort in simulation experiments and an 80% higher success rate in a physical block pushing task over (Explicit) IFL, IBC, and other baselines when human supervision is heterogeneous.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "40348019",
                        "name": "G. Datta"
                    },
                    {
                        "authorId": "1387872831",
                        "name": "Ryan Hoque"
                    },
                    {
                        "authorId": "2186302682",
                        "name": "Anrui Gu"
                    },
                    {
                        "authorId": "2419277",
                        "name": "Eugen Solowjow"
                    },
                    {
                        "authorId": "144344283",
                        "name": "Ken Goldberg"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "[39] Nur Muhammad Shafiullah, Zichen Cui, Ariuntuya Arty Altanzaya, and Lerrel Pinto.",
                "This approach has been shown to scale favorably to large models and multi-task settings [36], at times exceeding the performance of large-scale multi-task imitation learning with transformers [37, 38, 39]."
            ],
            "citingPaper": {
                "paperId": "5bac7d00035bc1e246a34f9ee3152b290f97bb92",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-14892",
                    "ArXiv": "2306.14892",
                    "DOI": "10.48550/arXiv.2306.14892",
                    "CorpusId": 259262142
                },
                "corpusId": 259262142,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5bac7d00035bc1e246a34f9ee3152b290f97bb92",
                "title": "Supervised Pretraining Can Learn In-Context Reinforcement Learning",
                "abstract": "Large transformer models trained on diverse datasets have shown a remarkable ability to learn in-context, achieving high few-shot performance on tasks they were not explicitly trained to solve. In this paper, we study the in-context learning capabilities of transformers in decision-making problems, i.e., reinforcement learning (RL) for bandits and Markov decision processes. To do so, we introduce and study Decision-Pretrained Transformer (DPT), a supervised pretraining method where the transformer predicts an optimal action given a query state and an in-context dataset of interactions, across a diverse set of tasks. This procedure, while simple, produces a model with several surprising capabilities. We find that the pretrained transformer can be used to solve a range of RL problems in-context, exhibiting both exploration online and conservatism offline, despite not being explicitly trained to do so. The model also generalizes beyond the pretraining distribution to new tasks and automatically adapts its decision-making strategies to unknown structure. Theoretically, we show DPT can be viewed as an efficient implementation of Bayesian posterior sampling, a provably sample-efficient RL algorithm. We further leverage this connection to provide guarantees on the regret of the in-context algorithm yielded by DPT, and prove that it can learn faster than algorithms used to generate the pretraining data. These results suggest a promising yet simple path towards instilling strong in-context decision-making abilities in transformers.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "120703894",
                        "name": "Jonathan Lee"
                    },
                    {
                        "authorId": "14484808",
                        "name": "A. Xie"
                    },
                    {
                        "authorId": "3124110",
                        "name": "Aldo Pacchiano"
                    },
                    {
                        "authorId": "2232505",
                        "name": "Yash Chandak"
                    },
                    {
                        "authorId": "46881670",
                        "name": "Chelsea Finn"
                    },
                    {
                        "authorId": "7624658",
                        "name": "Ofir Nachum"
                    },
                    {
                        "authorId": "2563117",
                        "name": "E. Brunskill"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "9dbb39eccbcd31b8f6b4ff0a2c96f61a7c34e54b",
                "externalIds": {
                    "ArXiv": "2306.11335",
                    "DBLP": "journals/corr/abs-2306-11335",
                    "DOI": "10.48550/arXiv.2306.11335",
                    "CorpusId": 259202791
                },
                "corpusId": 259202791,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9dbb39eccbcd31b8f6b4ff0a2c96f61a7c34e54b",
                "title": "RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with Progressive Reasoning Tasks",
                "abstract": "Recently, the advent of pre-trained large-scale language models (LLMs) like ChatGPT and GPT-4 have significantly advanced the machine's natural language understanding capabilities. This breakthrough has allowed us to seamlessly integrate these open-source LLMs into a unified robot simulator environment to help robots accurately understand and execute human natural language instructions. To this end, in this work, we introduce a realistic robotic manipulation simulator and build a Robotic Manipulation with Progressive Reasoning Tasks (RM-PRT) benchmark on this basis. Specifically, the RM-PRT benchmark builds a new high-fidelity digital twin scene based on Unreal Engine 5, which includes 782 categories, 2023 objects, and 15K natural language instructions generated by ChatGPT for a detailed evaluation of robot manipulation. We propose a general pipeline for the RM-PRT benchmark that takes as input multimodal prompts containing natural language instructions and automatically outputs actions containing the movement and position transitions. We set four natural language understanding tasks with progressive reasoning levels and evaluate the robot's ability to understand natural language instructions in two modes of adsorption and grasping. In addition, we also conduct a comprehensive analysis and comparison of the differences and advantages of 10 different LLMs in instruction understanding and generation quality. We hope the new simulator and benchmark will facilitate future research on language-guided robotic manipulation. Project website: https://necolizer.github.io/RM-PRT/ .",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51056374",
                        "name": "Pengzhen Ren"
                    },
                    {
                        "authorId": "47969138",
                        "name": "Kaiwen Zhang"
                    },
                    {
                        "authorId": "2220886900",
                        "name": "Hetao Zheng"
                    },
                    {
                        "authorId": "46947005",
                        "name": "Zixuan Li"
                    },
                    {
                        "authorId": "2153698189",
                        "name": "Yuhang Wen"
                    },
                    {
                        "authorId": "94228656",
                        "name": "Fengda Zhu"
                    },
                    {
                        "authorId": "2220606052",
                        "name": "Mas Ma"
                    },
                    {
                        "authorId": "2153397698",
                        "name": "Xiaodan Liang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "To address this limitation, future research could explore integrating SGR into methods that possess multi-modal modeling capabilities [83, 84]."
            ],
            "citingPaper": {
                "paperId": "bf2de6a33a582be5d60adc9a07bdb9cec0cb5140",
                "externalIds": {
                    "ArXiv": "2306.10474",
                    "DBLP": "journals/corr/abs-2306-10474",
                    "DOI": "10.48550/arXiv.2306.10474",
                    "CorpusId": 259203512
                },
                "corpusId": 259203512,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/bf2de6a33a582be5d60adc9a07bdb9cec0cb5140",
                "title": "A Universal Semantic-Geometric Representation for Robotic Manipulation",
                "abstract": "Robots rely heavily on sensors, especially RGB and depth cameras, to perceive and interact with the world. RGB cameras record 2D images with rich semantic information while missing precise spatial information. On the other side, depth cameras offer critical 3D geometry data but capture limited semantics. Therefore, integrating both modalities is crucial for learning representations for robotic perception and control. However, current research predominantly focuses on only one of these modalities, neglecting the benefits of incorporating both. To this end, we present Semantic-Geometric Representation (SGR), a universal perception module for robotics that leverages the rich semantic information of large-scale pre-trained 2D models and inherits the merits of 3D spatial reasoning. Our experiments demonstrate that SGR empowers the agent to successfully complete a diverse range of simulated and real-world robotic manipulation tasks, outperforming state-of-the-art methods significantly in both single-task and multi-task settings. Furthermore, SGR possesses the unique capability to generalize to novel semantic attributes, setting it apart from the other methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2146325895",
                        "name": "Tong Zhang"
                    },
                    {
                        "authorId": "2149297811",
                        "name": "Yingdong Hu"
                    },
                    {
                        "authorId": "2220292004",
                        "name": "Hanchen Cui"
                    },
                    {
                        "authorId": "2158087618",
                        "name": "Hang Zhao"
                    },
                    {
                        "authorId": "48146895",
                        "name": "Yang Gao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "from various sources and present strong multi-modalities [30]."
            ],
            "citingPaper": {
                "paperId": "1f188c2386ee715944cd529bf01cdad846a888a3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-20081",
                    "ArXiv": "2305.20081",
                    "DOI": "10.48550/arXiv.2305.20081",
                    "CorpusId": 258987691
                },
                "corpusId": 258987691,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1f188c2386ee715944cd529bf01cdad846a888a3",
                "title": "Efficient Diffusion Policies for Offline Reinforcement Learning",
                "abstract": "Offline reinforcement learning (RL) aims to learn optimal policies from offline datasets, where the parameterization of policies is crucial but often overlooked. Recently, Diffsuion-QL significantly boosts the performance of offline RL by representing a policy with a diffusion model, whose success relies on a parametrized Markov Chain with hundreds of steps for sampling. However, Diffusion-QL suffers from two critical limitations. 1) It is computationally inefficient to forward and backward through the whole Markov chain during training. 2) It is incompatible with maximum likelihood-based RL algorithms (e.g., policy gradient methods) as the likelihood of diffusion models is intractable. Therefore, we propose efficient diffusion policy (EDP) to overcome these two challenges. EDP approximately constructs actions from corrupted ones at training to avoid running the sampling chain. We conduct extensive experiments on the D4RL benchmark. The results show that EDP can reduce the diffusion policy training time from 5 days to 5 hours on gym-locomotion tasks. Moreover, we show that EDP is compatible with various offline RL algorithms (TD3, CRR, and IQL) and achieves new state-of-the-art on D4RL by large margins over previous methods. Our code is available at https://github.com/sail-sg/edp.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2064621368",
                        "name": "Bingyi Kang"
                    },
                    {
                        "authorId": "2125110703",
                        "name": "Xiao Ma"
                    },
                    {
                        "authorId": "144369497",
                        "name": "Chao Du"
                    },
                    {
                        "authorId": "19201674",
                        "name": "Tianyu Pang"
                    },
                    {
                        "authorId": "2186749683",
                        "name": "Shuicheng Yan"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "GAIL [26], BeT [52]) might help alleviate this issue.",
                "Some forms of distributional BC (e.g. GAIL [26], BeT [52]) might help alleviate this issue.",
                "More recent work have explored the use of multi-modal transformers [27] to fit large amounts of demonstration data [49, 52, 56]."
            ],
            "citingPaper": {
                "paperId": "32ff7e5ea4ef146cc63fdee23af1cc47e89af095",
                "externalIds": {
                    "ArXiv": "2305.19240",
                    "DBLP": "journals/corr/abs-2305-19240",
                    "DOI": "10.48550/arXiv.2305.19240",
                    "CorpusId": 258967721
                },
                "corpusId": 258967721,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/32ff7e5ea4ef146cc63fdee23af1cc47e89af095",
                "title": "NetHack is Hard to Hack",
                "abstract": "Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning. Our investigations produce a state-of-the-art neural agent that surpasses previous fully neural policies by 127% in offline settings and 25% in online settings on median game score. However, we also demonstrate that mere scaling is insufficient to bridge the performance gap with the best symbolic models or even the top human players.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2047734290",
                        "name": "Ulyana Piterbarg"
                    },
                    {
                        "authorId": "34026610",
                        "name": "Lerrel Pinto"
                    },
                    {
                        "authorId": "2276554",
                        "name": "R. Fergus"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", 2022), robot learning and behavior cloning from noisy demonstrations (Shafiullah et al., 2022; Cui et al., 2022), learning from multiple cameras (Seo et al."
            ],
            "citingPaper": {
                "paperId": "f11044596cf2eaf59f83d82b8167b16ba6a08617",
                "externalIds": {
                    "ArXiv": "2305.16554",
                    "DBLP": "journals/corr/abs-2305-16554",
                    "DOI": "10.48550/arXiv.2305.16554",
                    "CorpusId": 258947729
                },
                "corpusId": 258947729,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/f11044596cf2eaf59f83d82b8167b16ba6a08617",
                "title": "Emergent Agentic Transformer from Chain of Hindsight Experience",
                "abstract": "Large transformer models powered by diverse data and model scale have dominated natural language modeling and computer vision and pushed the frontier of multiple AI areas. In reinforcement learning (RL), despite many efforts into transformer-based policies, a key limitation, however, is that current transformer-based policies cannot learn by directly combining information from multiple sub-optimal trials. In this work, we address this issue using recently proposed chain of hindsight to relabel experience, where we train a transformer on a sequence of trajectory experience ascending sorted according to their total rewards. Our method consists of relabelling target return of each trajectory to the maximum total reward among in sequence of trajectories and training an autoregressive model to predict actions conditioning on past states, actions, rewards, target returns, and task completion tokens, the resulting model, Agentic Transformer (AT), can learn to improve upon itself both at training and test time. As we show on D4RL and ExoRL benchmarks, to the best our knowledge, this is the first time that a simple transformer-based model performs competitively with both temporal-difference and imitation-learning-based approaches, even from sub-optimal data. Our Agentic Transformer also shows a promising scaling trend that bigger models consistently improve results.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2143855835",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "Transformer-based policy architectures such as Gato [12], PerAct [55], VIMA [56], RT-1 [10], Dasari and Gupta [57], and Behavior Transformer [58] have demonstrated impressive results across a range of robotic manipulation tasks, yet make use of discretization of the input",
                "Transformer-based policy architectures such as Gato [12], PerAct [40], VIMA [41], RT-1 [10], Dasari and Gupta [42], and Behavior Transformer [43] have demonstrated impressive results across a range of robotic manipulation tasks, yet make use of discretization of the input observations and output actions, limiting their applicability to tasks requiring precise manipulation.",
                "Additionally, we compare against Behavior Transformer (BeT) [43], which discretizes the dataset into clusters using K-Means and uses a Transformer model to predict a cluster center and an offset, in order to handle multi-modal data.",
                "Additionally, we compare against Behavior Transformer (BeT) [58], which discretizes the dataset into clusters using K-Means and uses a Transformer model to predict a cluster center and an offset, in order to handle multi-"
            ],
            "citingPaper": {
                "paperId": "82feec0425350e097e8003ba07ab807a34a9457d",
                "externalIds": {
                    "ArXiv": "2305.16309",
                    "DBLP": "journals/corr/abs-2305-16309",
                    "DOI": "10.48550/arXiv.2305.16309",
                    "CorpusId": 258887633
                },
                "corpusId": 258887633,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/82feec0425350e097e8003ba07ab807a34a9457d",
                "title": "Imitating Task and Motion Planning with Visuomotor Transformers",
                "abstract": "Imitation learning is a powerful tool for training robot manipulation policies, allowing them to learn from expert demonstrations without manual programming or trial-and-error. However, common methods of data collection, such as human supervision, scale poorly, as they are time-consuming and labor-intensive. In contrast, Task and Motion Planning (TAMP) can autonomously generate large-scale datasets of diverse demonstrations. In this work, we show that the combination of large-scale datasets generated by TAMP supervisors and flexible Transformer models to fit them is a powerful paradigm for robot manipulation. To that end, we present a novel imitation learning system called OPTIMUS that trains large-scale visuomotor Transformer policies by imitating a TAMP agent. OPTIMUS introduces a pipeline for generating TAMP data that is specifically curated for imitation learning and can be used to train performant transformer-based policies. In this paper, we present a thorough study of the design decisions required to imitate TAMP and demonstrate that OPTIMUS can solve a wide variety of challenging vision-based manipulation tasks with over 70 different objects, ranging from long-horizon pick-and-place tasks, to shelf and articulated object manipulation, achieving 70 to 80% success rates. Video results and code at https://mihdalal.github.io/optimus/",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "35904540",
                        "name": "Murtaza Dalal"
                    },
                    {
                        "authorId": "49686756",
                        "name": "Ajay Mandlekar"
                    },
                    {
                        "authorId": "1834058",
                        "name": "Caelan Reed Garrett"
                    },
                    {
                        "authorId": "34653454",
                        "name": "Ankur Handa"
                    },
                    {
                        "authorId": "145124475",
                        "name": "R. Salakhutdinov"
                    },
                    {
                        "authorId": "145197953",
                        "name": "D. Fox"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "Autoregressive model [Vaswani et al., 2017, Brown et al., 2020] represents the policy as the distribution of action, where it considers the distribution of the whole trajectory [Reed et al., 2022, Shafiullah et al., 2022]."
            ],
            "citingPaper": {
                "paperId": "fe9fe9f15f24fbbb19b62bcd9a3418511a699b84",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-13122",
                    "ArXiv": "2305.13122",
                    "DOI": "10.48550/arXiv.2305.13122",
                    "CorpusId": 258832463
                },
                "corpusId": 258832463,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fe9fe9f15f24fbbb19b62bcd9a3418511a699b84",
                "title": "Policy Representation via Diffusion Probability Model for Reinforcement Learning",
                "abstract": "Popular reinforcement learning (RL) algorithms tend to produce a unimodal policy distribution, which weakens the expressiveness of complicated policy and decays the ability of exploration. The diffusion probability model is powerful to learn complicated multimodal distributions, which has shown promising and potential applications to RL. In this paper, we formally build a theoretical foundation of policy representation via the diffusion probability model and provide practical implementations of diffusion policy for online model-free RL. Concretely, we character diffusion policy as a stochastic process, which is a new approach to representing a policy. Then we present a convergence guarantee for diffusion policy, which provides a theory to understand the multimodality of diffusion policy. Furthermore, we propose the DIPO which is an implementation for model-free online RL with DIffusion POlicy. To the best of our knowledge, DIPO is the first algorithm to solve model-free online RL problems with the diffusion model. Finally, extensive empirical results show the effectiveness and superiority of DIPO on the standard continuous control Mujoco benchmark.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "150196660",
                        "name": "Long Yang"
                    },
                    {
                        "authorId": "2218293583",
                        "name": "Zhixiong Huang"
                    },
                    {
                        "authorId": "2218119181",
                        "name": "Fenghao Lei"
                    },
                    {
                        "authorId": "2218100325",
                        "name": "Yucun Zhong"
                    },
                    {
                        "authorId": "46286308",
                        "name": "Yiming Yang"
                    },
                    {
                        "authorId": "47967033",
                        "name": "Cong Fang"
                    },
                    {
                        "authorId": "2992234",
                        "name": "Shiting Wen"
                    },
                    {
                        "authorId": "2218029625",
                        "name": "Binbin Zhou"
                    },
                    {
                        "authorId": "33383055",
                        "name": "Zhouchen Lin"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "In particular, behavioral cloning has made significant progress due to modeling advances (e.g. LSTM-GMM [4], Transformers [5], Diffusion [3]).",
                "LSTM-GMM [4], Transformers [5], Diffusion [3])."
            ],
            "citingPaper": {
                "paperId": "fa1f0356f1c1374528ed170f2c99ea78481d071b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-12171",
                    "ArXiv": "2305.12171",
                    "DOI": "10.48550/arXiv.2305.12171",
                    "CorpusId": 258832509
                },
                "corpusId": 258832509,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/fa1f0356f1c1374528ed170f2c99ea78481d071b",
                "title": "Diffusion Co-Policy for Synergistic Human-Robot Collaborative Tasks",
                "abstract": "Modeling multimodal human behavior accurately has been a key barrier to increasing the level of interaction between human and robot, particularly for collaborative tasks. Our key insight is that the predictive accuracy of human behaviors on physical tasks is bottlenecked by the model for methods involving human behavior prediction. We present a method for training denoising diffusion probabilistic models on a dataset of collaborative human-human demonstrations and conditioning on past human partner actions to plan sequences of robot actions that synergize well with humans during test time. We demonstrate the method outperforms other state-of-art learning methods on human-robot table-carrying, a continuous state-action task, in both simulation and real settings with a human in the loop. Moreover, we qualitatively highlight compelling robot behaviors that arise during evaluations that demonstrate evidence of true human-robot collaboration, including mutual adaptation, shared task understanding, leadership switching, learned partner behaviors, and low levels of wasteful interaction forces arising from dissent. Project page coming soon.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2054604255",
                        "name": "Eley Ng"
                    },
                    {
                        "authorId": "2145253971",
                        "name": "Ziang Liu"
                    },
                    {
                        "authorId": "25219190",
                        "name": "Monroe Kennedy"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "7572bf46bd1d895c92f367b2b46c205cfeb2e967",
                "externalIds": {
                    "ArXiv": "2305.02968",
                    "DBLP": "journals/corr/abs-2305-02968",
                    "DOI": "10.48550/arXiv.2305.02968",
                    "CorpusId": 258480255
                },
                "corpusId": 258480255,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/7572bf46bd1d895c92f367b2b46c205cfeb2e967",
                "title": "Masked Trajectory Models for Prediction, Representation, and Control",
                "abstract": "We introduce Masked Trajectory Models (MTM) as a generic abstraction for sequential decision making. MTM takes a trajectory, such as a state-action sequence, and aims to reconstruct the trajectory conditioned on random subsets of the same trajectory. By training with a highly randomized masking pattern, MTM learns versatile networks that can take on different roles or capabilities, by simply choosing appropriate masks at inference time. For example, the same MTM network can be used as a forward dynamics model, inverse dynamics model, or even an offline RL agent. Through extensive experiments in several continuous control tasks, we show that the same MTM network -- i.e. same weights -- can match or outperform specialized networks trained for the aforementioned capabilities. Additionally, we find that state representations learned by MTM can significantly accelerate the learning speed of traditional RL algorithms. Finally, in offline RL benchmarks, we find that MTM is competitive with specialized offline RL algorithms, despite MTM being a generic self-supervised learning method without any explicit RL components. Code is available at https://github.com/facebookresearch/mtm",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108864104",
                        "name": "Philipp Wu"
                    },
                    {
                        "authorId": "2905057",
                        "name": "Arjun Majumdar"
                    },
                    {
                        "authorId": "2059203883",
                        "name": "Kevin Stone"
                    },
                    {
                        "authorId": "1491144944",
                        "name": "Yixin Lin"
                    },
                    {
                        "authorId": "2080746",
                        "name": "Igor Mordatch"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    },
                    {
                        "authorId": "19275599",
                        "name": "A. Rajeswaran"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "BeT [49] also leverages Transformers as the architecture, but with key differences: (1) no action chunking: the model predicts one action given the history of observations; and (2) the image observations are pre-processed by a separately trained frozen",
                "Many works have then sought to improve BC, for example by incorporating history with various architectures [39, 49, 26, 7], using a different training objective [17, 42], and including regularization [46]."
            ],
            "citingPaper": {
                "paperId": "91eb20f923ea3b0246868902aef4e9bea572b800",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-13705",
                    "ArXiv": "2304.13705",
                    "DOI": "10.48550/arXiv.2304.13705",
                    "CorpusId": 258331658
                },
                "corpusId": 258331658,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/91eb20f923ea3b0246868902aef4e9bea572b800",
                "title": "Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware",
                "abstract": "Fine manipulation tasks, such as threading cable ties or slotting a battery, are notoriously difficult for robots because they require precision, careful coordination of contact forces, and closed-loop visual feedback. Performing these tasks typically requires high-end robots, accurate sensors, or careful calibration, which can be expensive and difficult to set up. Can learning enable low-cost and imprecise hardware to perform these fine manipulation tasks? We present a low-cost system that performs end-to-end imitation learning directly from real demonstrations, collected with a custom teleoperation interface. Imitation learning, however, presents its own challenges, particularly in high-precision domains: errors in the policy can compound over time, and human demonstrations can be non-stationary. To address these challenges, we develop a simple yet novel algorithm, Action Chunking with Transformers (ACT), which learns a generative model over action sequences. ACT allows the robot to learn 6 difficult tasks in the real world, such as opening a translucent condiment cup and slotting a battery with 80-90% success, with only 10 minutes worth of demonstrations. Project website: https://tonyzhaozh.github.io/aloha/",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145914976",
                        "name": "Tony Zhao"
                    },
                    {
                        "authorId": "2109446216",
                        "name": "Vikash Kumar"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    },
                    {
                        "authorId": "46881670",
                        "name": "Chelsea Finn"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Transformer-based architectures were also researched as a policy class for task-agnostic behavior learning [6, 35].",
                "Most prior work tries to deal with this challenge, by combining generative models, such as Variational Autoencoders (VAEs) [12, 25, 32] and Generative Pretrained Transformer (GPTs) [6, 35], with additional models and networks to explicitly encode multimodality or hierarchy.",
                "\u2022 Conditional-Behavior Transformer (C-BeT) is a GPTlike transformer-based policy, that predicts discrete action labels together with a continuous offset vector to learn multimodal behavior [35, 6]."
            ],
            "citingPaper": {
                "paperId": "1334a47e8f4e4ffd04ff534329d76a5e5cc16f46",
                "externalIds": {
                    "ArXiv": "2304.02532",
                    "DBLP": "conf/rss/ReussLJL23",
                    "DOI": "10.48550/arXiv.2304.02532",
                    "CorpusId": 257952177
                },
                "corpusId": 257952177,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1334a47e8f4e4ffd04ff534329d76a5e5cc16f46",
                "title": "Goal-Conditioned Imitation Learning using Score-based Diffusion Policies",
                "abstract": "We propose a new policy representation based on score-based diffusion models (SDMs). We apply our new policy representation in the domain of Goal-Conditioned Imitation Learning (GCIL) to learn general-purpose goal-specified policies from large uncurated datasets without rewards. Our new goal-conditioned policy architecture\"$\\textbf{BE}$havior generation with $\\textbf{S}$c$\\textbf{O}$re-based Diffusion Policies\"(BESO) leverages a generative, score-based diffusion model as its policy. BESO decouples the learning of the score model from the inference sampling process, and, hence allows for fast sampling strategies to generate goal-specified behavior in just 3 denoising steps, compared to 30+ steps of other diffusion based policies. Furthermore, BESO is highly expressive and can effectively capture multi-modality present in the solution space of the play data. Unlike previous methods such as Latent Plans or C-Bet, BESO does not rely on complex hierarchical policies or additional clustering for effective goal-conditioned behavior learning. Finally, we show how BESO can even be used to learn a goal-independent policy from play-data using classifier-free guidance. To the best of our knowledge this is the first work that a) represents a behavior policy based on such a decoupled SDM b) learns an SDM based policy in the domain of GCIL and c) provides a way to simultaneously learn a goal-dependent and a goal-independent policy from play-data. We evaluate BESO through detailed simulation and show that it consistently outperforms several state-of-the-art goal-conditioned imitation learning methods on challenging benchmarks. We additionally provide extensive ablation studies and experiments to demonstrate the effectiveness of our method for goal-conditioned behavior generation. Demonstrations and Code are available at https://intuitive-robots.github.io/beso-website/",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2165874245",
                        "name": "Moritz Reuss"
                    },
                    {
                        "authorId": "40978026",
                        "name": "M. Li"
                    },
                    {
                        "authorId": "2026993876",
                        "name": "Xiaogang Jia"
                    },
                    {
                        "authorId": "2931067",
                        "name": "Rudolf Lioutikov"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                ", 2021), Behavior Transformer (BeT) (Shafiullah et al., 2022), MaskDP (Liu et al.",
                "\u2026one-shot imitation learning, (Lynch et al., 2020; Singh et al., 2020) explore behavior priors from demos, (Chen et al., 2021; Liu et al., 2022; Janner et al., 2021; Shafiullah et al., 2022; Ajay et al., 2022; Janner et al., 2022) examine different sequence modeling strategies for policy learning.",
                "Note that multimodality is a related but orthogonal issue (Shafiullah et al., 2022), i.",
                "Despite the recent progress (Chen et al., 2021; Florence et al., 2022; Shafiullah et al., 2022; Liu et al., 2022; Ajay et al., 2022), it remains extremely challenging to solve lowlevel control tasks such as contact-rich object manipulations by IL in a scalable manner.",
                "While theoretically sound, it is shown (Shafiullah et al., 2022) to be less practical for non-Markovian implicit models, and later explicit models outperform it.",
                "Note that multimodality is a related but orthogonal issue (Shafiullah et al., 2022), i.e., when a unimodal estimate of the (continuous) action distribution leads to a significantly worse return.",
                ", 2020) explore behavior priors from demos, (Chen et al., 2021; Liu et al., 2022; Janner et al., 2021; Shafiullah et al., 2022; Ajay et al., 2022; Janner et al., 2022) examine different sequence modeling strategies for policy learning.",
                "Namely, Decision Transformer (DT) (Chen et al., 2021), Behavior Transformer (BeT) (Shafiullah et al., 2022), MaskDP (Liu et al., 2022) and Decision Diffuser (DD) (Ajay et al., 2022)."
            ],
            "citingPaper": {
                "paperId": "e1bd151a3f670fd0f77580702fe7a85dc78a41cb",
                "externalIds": {
                    "ArXiv": "2304.00776",
                    "DBLP": "journals/corr/abs-2304-00776",
                    "DOI": "10.48550/arXiv.2304.00776",
                    "CorpusId": 257912725
                },
                "corpusId": 257912725,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e1bd151a3f670fd0f77580702fe7a85dc78a41cb",
                "title": "Chain-of-Thought Predictive Control",
                "abstract": "We study generalizable policy learning from demonstrations for complex low-level control tasks (e.g., contact-rich object manipulations). We propose an imitation learning method that incorporates the idea of temporal abstraction and the planning capabilities from Hierarchical RL (HRL) in a novel and effective manner. As a step towards decision foundation models, our design can utilize scalable, albeit highly sub-optimal, demonstrations. Specifically, we find certain short subsequences of the demos, i.e. the chain-of-thought (CoT), reflect their hierarchical structures by marking the completion of subgoals in the tasks. Our model learns to dynamically predict the entire CoT as coherent and structured long-term action guidance and consistently outperforms typical two-stage subgoal-conditioned policies. On the other hand, such CoT facilitates generalizable policy learning as they exemplify the decision patterns shared among demos (even those with heavy noises and randomness). Our method, Chain-of-Thought Predictive Control (CoTPC), significantly outperforms existing ones on challenging low-level manipulation tasks from scalable yet highly sub-optimal demos.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2072783179",
                        "name": "Zhiwei Jia"
                    },
                    {
                        "authorId": "32324034",
                        "name": "Fangchen Liu"
                    },
                    {
                        "authorId": "2059023339",
                        "name": "Vineet Thumuluri"
                    },
                    {
                        "authorId": "2119322767",
                        "name": "Ling Chen"
                    },
                    {
                        "authorId": "18036051",
                        "name": "Zhiao Huang"
                    },
                    {
                        "authorId": "2093560213",
                        "name": "H. Su"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "aee25ec357bfb8b4422e04630bc46c8f5a03a695",
                "externalIds": {
                    "DBLP": "journals/sensors/HanMSC23",
                    "PubMedCentral": "10098871",
                    "DOI": "10.3390/s23073762",
                    "CorpusId": 258020497,
                    "PubMed": "37050822"
                },
                "corpusId": 258020497,
                "publicationVenue": {
                    "id": "3dbf084c-ef47-4b74-9919-047b40704538",
                    "name": "Italian National Conference on Sensors",
                    "type": "conference",
                    "alternate_names": [
                        "SENSORS",
                        "IEEE Sens",
                        "Ital National Conf Sens",
                        "IEEE Sensors",
                        "Sensors"
                    ],
                    "issn": "1424-8220",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-142001",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-142001",
                        "http://www.mdpi.com/journal/sensors",
                        "https://www.mdpi.com/journal/sensors"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/aee25ec357bfb8b4422e04630bc46c8f5a03a695",
                "title": "A Survey on Deep Reinforcement Learning Algorithms for Robotic Manipulation",
                "abstract": "Robotic manipulation challenges, such as grasping and object manipulation, have been tackled successfully with the help of deep reinforcement learning systems. We give an overview of the recent advances in deep reinforcement learning algorithms for robotic manipulation tasks in this review. We begin by outlining the fundamental ideas of reinforcement learning and the parts of a reinforcement learning system. The many deep reinforcement learning algorithms, such as value-based methods, policy-based methods, and actor\u2013critic approaches, that have been suggested for robotic manipulation tasks are then covered. We also examine the numerous issues that have arisen when applying these algorithms to robotics tasks, as well as the various solutions that have been put forth to deal with these issues. Finally, we highlight several unsolved research issues and talk about possible future directions for the subject.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "71202632",
                        "name": "Dong-Ki Han"
                    },
                    {
                        "authorId": "2048111901",
                        "name": "Beni Mulyana"
                    },
                    {
                        "authorId": "1712346",
                        "name": "V. Stankovi\u0107"
                    },
                    {
                        "authorId": "2111143576",
                        "name": "Samuel Cheng"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Indicatively, a recent paper on Behaviour Transformers [17] showcased the benefits of the method on a toy navigation task that only took a simple multi layer perceptron to solve."
            ],
            "citingPaper": {
                "paperId": "99c6784e928e0b2352c8fddfb2111a3aa90a55e1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-06035",
                    "ArXiv": "2304.06035",
                    "DOI": "10.48550/arXiv.2304.06035",
                    "CorpusId": 258108216
                },
                "corpusId": 258108216,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/99c6784e928e0b2352c8fddfb2111a3aa90a55e1",
                "title": "Choose Your Weapon: Survival Strategies for Depressed AI Academics",
                "abstract": "Are you an AI researcher at an academic institution? Are you anxious you are not coping with the current pace of AI advancements? Do you feel you have no (or very limited) access to the computational and human resources required for an AI research breakthrough? You are not alone; we feel the same way. A growing number of AI academics can no longer find the means and resources to compete at a global scale. This is a somewhat recent phenomenon, but an accelerating one, with private actors investing enormous compute resources into cutting edge AI research. Here, we discuss what you can do to stay competitive while remaining an academic. We also briefly discuss what universities and the private sector could do improve the situation, if they are so inclined. This is not an exhaustive list of strategies, and you may not agree with all of them, but it serves to start a discussion.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1810053",
                        "name": "J. Togelius"
                    },
                    {
                        "authorId": "1686193",
                        "name": "Georgios N. Yannakakis"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Recently, Shafiullah et al. (2022) proposed the behavior transformer which employs a minGPT transformer (Brown et al., 2020) to predict targets by decomposing them into cluster centers and residual offsets.",
                "Moreover, we consider energy-based models for behavior learning (IBC) (Florence et al., 2022) and the recently proposed behavior transformer (BeT) (Shafiullah et al., 2022).",
                ", 2022) and the recently proposed behavior transformer (BeT) (Shafiullah et al., 2022)."
            ],
            "citingPaper": {
                "paperId": "c1b247c76c71153726c50d8109e300aee09ec3fa",
                "externalIds": {
                    "ArXiv": "2303.15349",
                    "DBLP": "journals/corr/abs-2303-15349",
                    "DOI": "10.48550/arXiv.2303.15349",
                    "CorpusId": 257766392
                },
                "corpusId": 257766392,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c1b247c76c71153726c50d8109e300aee09ec3fa",
                "title": "Information Maximizing Curriculum: A Curriculum-Based Approach for Training Mixtures of Experts",
                "abstract": "Mixtures of Experts (MoE) are known for their ability to learn complex conditional distributions with multiple modes. However, despite their potential, these models are challenging to train and often tend to produce poor performance, explaining their limited popularity. Our hypothesis is that this under-performance is a result of the commonly utilized maximum likelihood (ML) optimization, which leads to mode averaging and a higher likelihood of getting stuck in local maxima. We propose a novel curriculum-based approach to learning mixture models in which each component of the MoE is able to select its own subset of the training data for learning. This approach allows for independent optimization of each component, resulting in a more modular architecture that enables the addition and deletion of components on the fly, leading to an optimization less susceptible to local optima. The curricula can ignore data-points from modes not represented by the MoE, reducing the mode-averaging problem. To achieve a good data coverage, we couple the optimization of the curricula with a joint entropy objective and optimize a lower bound of this objective. We evaluate our curriculum-based approach on a variety of multimodal behavior learning tasks and demonstrate its superiority over competing methods for learning MoE models and conditional generative models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "41073370",
                        "name": "Denis Blessing"
                    },
                    {
                        "authorId": "2212496620",
                        "name": "Onur Celik"
                    },
                    {
                        "authorId": "2026993876",
                        "name": "Xiaogang Jia"
                    },
                    {
                        "authorId": "2165874245",
                        "name": "Moritz Reuss"
                    },
                    {
                        "authorId": "40978026",
                        "name": "M. Li"
                    },
                    {
                        "authorId": "2931067",
                        "name": "Rudolf Lioutikov"
                    },
                    {
                        "authorId": "26599977",
                        "name": "G. Neumann"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Given a set of demonstrations, offline imitation methods such as Behavior Cloning (BC) use supervised learning to learn a policy that outputs actions similar to the expert data and have been used extensively in robotics [57, 21, 63, 80, 43]."
            ],
            "citingPaper": {
                "paperId": "dae9be0f0d815b53b46974377a0edf9169a99f3f",
                "externalIds": {
                    "ArXiv": "2303.12076",
                    "DBLP": "journals/corr/abs-2303-12076",
                    "DOI": "10.48550/arXiv.2303.12076",
                    "CorpusId": 257636836
                },
                "corpusId": 257636836,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/dae9be0f0d815b53b46974377a0edf9169a99f3f",
                "title": "Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play",
                "abstract": "Teaching dexterity to multi-fingered robots has been a longstanding challenge in robotics. Most prominent work in this area focuses on learning controllers or policies that either operate on visual observations or state estimates derived from vision. However, such methods perform poorly on fine-grained manipulation tasks that require reasoning about contact forces or about objects occluded by the hand itself. In this work, we present T-Dex, a new approach for tactile-based dexterity, that operates in two phases. In the first phase, we collect 2.5 hours of play data, which is used to train self-supervised tactile encoders. This is necessary to bring high-dimensional tactile readings to a lower-dimensional embedding. In the second phase, given a handful of demonstrations for a dexterous task, we learn non-parametric policies that combine the tactile observations with visual ones. Across five challenging dexterous tasks, we show that our tactile-based dexterity models outperform purely vision and torque-based models by an average of 1.7X. Finally, we provide a detailed analysis on factors critical to T-Dex including the importance of play data, architectures, and representation learning.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2212471096",
                        "name": "Irmak G\u00fczey"
                    },
                    {
                        "authorId": "2153473632",
                        "name": "Ben Evans"
                    },
                    {
                        "authorId": "2127604",
                        "name": "Soumith Chintala"
                    },
                    {
                        "authorId": "34026610",
                        "name": "Lerrel Pinto"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "75192f2abd5cc76b9cae1e7920a164f446a2200e",
                "externalIds": {
                    "ArXiv": "2303.08135",
                    "DBLP": "journals/corr/abs-2303-08135",
                    "DOI": "10.48550/arXiv.2303.08135",
                    "CorpusId": 257505038
                },
                "corpusId": 257505038,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/75192f2abd5cc76b9cae1e7920a164f446a2200e",
                "title": "Manipulate by Seeing: Creating Manipulation Controllers from Pre-Trained Representations",
                "abstract": "The field of visual representation learning has seen explosive growth in the past years, but its benefits in robotics have been surprisingly limited so far. Prior work uses generic visual representations as a basis to learn (task-specific) robot action policies (e.g., via behavior cloning). While the visual representations do accelerate learning, they are primarily used to encode visual observations. Thus, action information has to be derived purely from robot data, which is expensive to collect! In this work, we present a scalable alternative where the visual representations can help directly infer robot actions. We observe that vision encoders express relationships between image observations as distances (e.g., via embedding dot product) that could be used to efficiently plan robot behavior. We operationalize this insight and develop a simple algorithm for acquiring a distance function and dynamics predictor, by fine-tuning a pre-trained representation on human collected video sequences. The final method is able to substantially outperform traditional robot learning baselines (e.g., 70% success v.s. 50% for behavior cloning on pick-place) on a suite of diverse real-world manipulation tasks. It can also generalize to novel objects, without using any robot demonstrations during train time. For visualizations of the learned policies please check: https://agi-labs.github.io/manipulate-by-seeing/.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2966240",
                        "name": "Jianren Wang"
                    },
                    {
                        "authorId": "36076404",
                        "name": "S. Dasari"
                    },
                    {
                        "authorId": "2193493900",
                        "name": "Mohan Kumar Srirama"
                    },
                    {
                        "authorId": "2757335",
                        "name": "Shubham Tulsiani"
                    },
                    {
                        "authorId": "2117767136",
                        "name": "Abhi Gupta"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "Similarly, the autoregressive sequence modeling objective from Equation 11 can also be instantiated to model behavioral priors [Shafiullah et al. 2022], resulting in a policy that can depend on the history of interaction \u03c0 (at |st , \u03c4<t ).",
                "Similarly, the autoregressive sequence modeling objective from Equation 11 can also be instantiated to model behavioral priors [Shafiullah et al. 2022], resulting in a policy that can depend on the history of interaction \ud835\udf0b (\ud835\udc4e\ud835\udc61 |\ud835\udc60\ud835\udc61 , \ud835\udf0f<\ud835\udc61 ).",
                "Inspired by the scaling success of transformers, generalist agents modeling sequences of diverse behaviors have been developed for simulated tasks [Shafiullah et al. 2022], over 40 Atari games [Lee et al. 2022], over 700 real-world robot tasks [Brohan et al. 2022], and over 600 distinct tasks with\u2026",
                "Inspired by the scaling success of transformers, generalist agents modeling sequences of diverse behaviors have been developed for simulated tasks [Shafiullah et al. 2022], over 40 Atari games [Lee et al."
            ],
            "citingPaper": {
                "paperId": "2ebd5df74980a37370b0bcdf16deff958289c041",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-04129",
                    "ArXiv": "2303.04129",
                    "DOI": "10.48550/arXiv.2303.04129",
                    "CorpusId": 257378587
                },
                "corpusId": 257378587,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2ebd5df74980a37370b0bcdf16deff958289c041",
                "title": "Foundation Models for Decision Making: Problems, Methods, and Opportunities",
                "abstract": "Foundation models pretrained on diverse data at scale have demonstrated extraordinary capabilities in a wide range of vision and language tasks. When such models are deployed in real world environments, they inevitably interface with other entities and agents. For example, language models are often used to interact with human beings through dialogue, and visual perception models are used to autonomously navigate neighborhood streets. In response to these developments, new paradigms are emerging for training foundation models to interact with other agents and perform long-term reasoning. These paradigms leverage the existence of ever-larger datasets curated for multimodal, multitask, and generalist interaction. Research at the intersection of foundation models and decision making holds tremendous promise for creating powerful new systems that can interact effectively across a diverse range of applications such as dialogue, autonomous driving, healthcare, education, and robotics. In this manuscript, we examine the scope of foundation models for decision making, and provide conceptual tools and technical background for understanding the problem space and exploring new research directions. We review recent approaches that ground foundation models in practical decision making applications through a variety of methods such as prompting, conditional generative modeling, planning, optimal control, and reinforcement learning, and discuss common challenges and open problems in the field.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47569072",
                        "name": "Sherry Yang"
                    },
                    {
                        "authorId": "7624658",
                        "name": "Ofir Nachum"
                    },
                    {
                        "authorId": "15394275",
                        "name": "Yilun Du"
                    },
                    {
                        "authorId": "119640649",
                        "name": "Jason Wei"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    },
                    {
                        "authorId": "50319359",
                        "name": "D. Schuurmans"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology",
                "result"
            ],
            "isInfluential": true,
            "contexts": [
                "Time-series diffusion transformer To reduce the oversmoothing effect in CNN models [49], we introduce a novel transformer-based DDPM which adopts the transformer architecture from minGPT [42] for action prediction.",
                "We systematically evaluate Diffusion Policy on 12 tasks from 4 benchmarks [12, 15, 29, 42].",
                "Diffusion Policy learns to approach the contact point equally likely from left or right, while LSTM-GMM [29] and IBC [12] exhibit bias toward one side and BET [42] cannot commit to one mode.",
                "The baseline methods we evaluate, however, work best with velocity control (and this is reflected in the literature where most existing work reports using velocitycontrol action spaces [29, 42, 60, 13, 28, 27]).",
                "Prior work attempts to address this challenge by exploring different action representations (Fig 1 a) \u2013 using mixtures of Gaussians [29], categorical representations of quantized actions [42], or by switching the the policy representation (Fig 1 b) \u2013 from explicit to implicit to better capture multi-modal distributions [12, 56].",
                "We systematically evaluate Diffusion Policy across 12 tasks from 4 different benchmarks [12, 15, 29, 42] under the behavior cloning formulation.",
                "3) Multimodal Block Pushing: adapted from BET [42], this task tests the policy\u2019s ability to model multimodal action distributions by pushing two blocks into two squares in any order.",
                "Similarly, BC-RNN and BET would have difficulty specifying the number of modes that exist in the action distribution (needed for GMM or kmeans steps).",
                "In contrast, both LSTM-GMM [29] and IBC [12] are biased toward one mode, while BET [42] fails to commit to a single mode due to its lack of temporal action consistency.",
                "However, suppose each action in the sequence is predicted as independent multimodal distributions (as done in BCRNN and BET).",
                "This surprising result stands in contrast to the majority of recent behavior cloning work that generally relies on velocity control [29, 42, 60, 13, 28, 27].",
                "We present the best-performing for each baseline method on each benchmark from all possible sources \u2013 our reproduced result (LSTM-GMM) or original number reported in the paper (BET, IBC).",
                "The challenge of modeling multi-modal distribution in human demonstrations has been widely discussed in behavior cloning literature [12, 42, 29]."
            ],
            "citingPaper": {
                "paperId": "bdba3bd30a49ea4c5b20b43dbd8f0eb59e9d80e2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-04137",
                    "ArXiv": "2303.04137",
                    "DOI": "10.48550/arXiv.2303.04137",
                    "CorpusId": 257378658
                },
                "corpusId": 257378658,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bdba3bd30a49ea4c5b20b43dbd8f0eb59e9d80e2",
                "title": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion",
                "abstract": "This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details will be publicly available.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "46859937",
                        "name": "Cheng Chi"
                    },
                    {
                        "authorId": "2480008",
                        "name": "S. Feng"
                    },
                    {
                        "authorId": "15394275",
                        "name": "Yilun Du"
                    },
                    {
                        "authorId": "74498275",
                        "name": "Zhenjia Xu"
                    },
                    {
                        "authorId": "2090529",
                        "name": "Eric A. Cousineau"
                    },
                    {
                        "authorId": "2302757",
                        "name": "B. Burchfiel"
                    },
                    {
                        "authorId": "3340170",
                        "name": "Shuran Song"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "[58] Nur Muhammad Mahi Shafiullah, Zichen Jeff Cui, Ariuntuya Altanzaya, and Lerrel Pinto.",
                "BC has also been applied to tasks with a multimodal action distribution [21, 58, 13]."
            ],
            "citingPaper": {
                "paperId": "96a19cd83080d70e10f3e6fd5af327a1263f20d6",
                "externalIds": {
                    "ArXiv": "2303.01497",
                    "DBLP": "journals/corr/abs-2303-01497",
                    "DOI": "10.48550/arXiv.2303.01497",
                    "CorpusId": 257279829
                },
                "corpusId": 257279829,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/96a19cd83080d70e10f3e6fd5af327a1263f20d6",
                "title": "Teach a Robot to FISH: Versatile Imitation from One Minute of Demonstrations",
                "abstract": "While imitation learning provides us with an efficient toolkit to train robots, learning skills that are robust to environment variations remains a significant challenge. Current approaches address this challenge by relying either on large amounts of demonstrations that span environment variations or on handcrafted reward functions that require state estimates. Both directions are not scalable to fast imitation. In this work, we present Fast Imitation of Skills from Humans (FISH), a new imitation learning approach that can learn robust visual skills with less than a minute of human demonstrations. Given a weak base-policy trained by offline imitation of demonstrations, FISH computes rewards that correspond to the\"match\"between the robot's behavior and the demonstrations. These rewards are then used to adaptively update a residual policy that adds on to the base-policy. Across all tasks, FISH requires at most twenty minutes of interactive learning to imitate demonstrations on object configurations that were not seen in the demonstrations. Importantly, FISH is constructed to be versatile, which allows it to be used across robot morphologies (e.g. xArm, Allegro, Stretch) and camera configurations (e.g. third-person, eye-in-hand). Our experimental evaluations on 9 different tasks show that FISH achieves an average success rate of 93%, which is around 3.8x higher than prior state-of-the-art methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51445278",
                        "name": "Siddhant Haldar"
                    },
                    {
                        "authorId": "1518270974",
                        "name": "Jyothish Pari"
                    },
                    {
                        "authorId": "81526981",
                        "name": "A. Rai"
                    },
                    {
                        "authorId": "34026610",
                        "name": "Lerrel Pinto"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "However, BC learns from decorrelated sampled state-action pairs, and often fails to capture the temporal structure of the task and the global information of expert demonstrations (Codevilla et al., 2019; Shafiullah et al., 2022)."
            ],
            "citingPaper": {
                "paperId": "e134ec1e834b0d56945916cdc02df653dc4e175d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-13335",
                    "ArXiv": "2302.13335",
                    "DOI": "10.48550/arXiv.2302.13335",
                    "CorpusId": 257219662
                },
                "corpusId": 257219662,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e134ec1e834b0d56945916cdc02df653dc4e175d",
                "title": "Diffusion Model-Augmented Behavioral Cloning",
                "abstract": "Imitation learning addresses the challenge of learning by observing an expert's demonstrations without access to reward signals from environments. Most existing imitation learning methods that do not require interacting with environments either model the expert distribution as the conditional probability p(a|s) (e.g., behavioral cloning, BC) or the joint probability p(s, a) (e.g., implicit behavioral cloning). Despite its simplicity, modeling the conditional probability with BC usually struggles with generalization. While modeling the joint probability can lead to improved generalization performance, the inference procedure can be time-consuming and it often suffers from manifold overfitting. This work proposes an imitation learning framework that benefits from modeling both the conditional and joint probability of the expert distribution. Our proposed diffusion model-augmented behavioral cloning (DBC) employs a diffusion model trained to model expert behaviors and learns a policy to optimize both the BC loss (conditional) and our proposed diffusion model loss (joint). DBC outperforms baselines in various continuous control tasks in navigation, robot arm manipulation, dexterous manipulation, and locomotion. We design additional experiments to verify the limitations of modeling either the conditional probability or the joint probability of the expert distribution as well as compare different generative models.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2210254136",
                        "name": "Hsiang-Chun Wang"
                    },
                    {
                        "authorId": "2118435475",
                        "name": "Shangcheng Chen"
                    },
                    {
                        "authorId": "2109374418",
                        "name": "Shao-Hua Sun"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "contexts": [
                "Noticeably, there are other approaches used for improving sample efficiency for imitation learning, such as grounding action on discrete observation space [23, 44, 50, 51]."
            ],
            "citingPaper": {
                "paperId": "a4701fadfd92683f2df4245d3ea873f1df61a71a",
                "externalIds": {
                    "ArXiv": "2302.12422",
                    "DBLP": "journals/corr/abs-2302-12422",
                    "DOI": "10.48550/arXiv.2302.12422",
                    "CorpusId": 257205825
                },
                "corpusId": 257205825,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a4701fadfd92683f2df4245d3ea873f1df61a71a",
                "title": "MimicPlay: Long-Horizon Imitation Learning by Watching Human Play",
                "abstract": "Imitation Learning from human demonstrations is a promising paradigm to teach robots manipulation skills in the real world, but learning complex long-horizon tasks often requires an unattainable amount of demonstrations. To reduce the high data requirement, we resort to human play data - video sequences of people freely interacting with the environment using their hands. We hypothesize that even with different morphologies, human play data contain rich and salient information about physical interactions that can readily facilitate robot policy learning. Motivated by this, we introduce a hierarchical learning framework named MimicPlay that learns latent plans from human play data to guide low-level visuomotor control trained on a small number of teleoperated demonstrations. With systematic evaluations of 14 long-horizon manipulation tasks in the real world, we show that MimicPlay dramatically outperforms state-of-the-art imitation learning methods in task success rate, generalization ability, and robustness to disturbances. More details and video results could be found at https://mimic-play.github.io",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2405565",
                        "name": "Chen Wang"
                    },
                    {
                        "authorId": "3275727",
                        "name": "Linxi (Jim) Fan"
                    },
                    {
                        "authorId": "2282025",
                        "name": "Jiankai Sun"
                    },
                    {
                        "authorId": "2657185",
                        "name": "Ruohan Zhang"
                    },
                    {
                        "authorId": "48004138",
                        "name": "Li Fei-Fei"
                    },
                    {
                        "authorId": "2068265",
                        "name": "Danfei Xu"
                    },
                    {
                        "authorId": "2117748",
                        "name": "Yuke Zhu"
                    },
                    {
                        "authorId": "47627049",
                        "name": "Anima Anandkumar"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "\u2026able to: 1) more accurately model complex action distributions (as illustrated in Figure 1); 2) significantly outperform state-of-the-art methods (Shafiullah et al., 2022) on a simulated robotic benchmark; and 3) scale to modelling human gameplay in Counter-Strike: Global Offensive - a modern,\u2026",
                "One of our experiments uses the set up from Shafiullah et al. (2022), allowing us to compare to their reported results, including Behaviour Transformers (BeT): the K-mean+residual combined with a large 6-layer transformer, and previous 10 observations as history; Implicit BC: the official\u2026",
                "By using diffusion models for BC we are able to: 1) more accurately model complex action distributions (as illustrated in Figure 1); 2) significantly outperform state-of-the-art methods (Shafiullah et al., 2022) on a simulated robotic benchmark; and 3) scale to modelling human gameplay in Counter-Strike: Global Offensive - a modern, 3D gaming environment recently proposed as a platform for imitation learning research (Pearce and Zhu, 2022).",
                "Shafiullah et al. (2022) extended K-Means by learning an observationdependent residual that is added to the bin\u2019s center and optimised via MSE.",
                "A further two baselines can be considered strong, more complex methods, namely K-means+Residual: as with K-means, but additionally learns a continuous residual on top of each bin prediction, trained via MSE, which was the core innovation of Behaviour Transformers (BeT) (Shafiullah et al., 2022); and EBM: a generative energy-based model trained with a contrastive loss, proposed in (Florence et al.",
                "The difference might be explained by the larger network used by Shafiullah et al. (2022) \u2013 6 layers, and an observation history of 10 steps.",
                "\u2026learns a continuous residual on top of each bin prediction, trained via MSE, which was the core innovation of Behaviour Transformers (BeT) (Shafiullah et al., 2022); and EBM: a generative energy-based model trained with a contrastive loss, proposed in (Florence et al., 2022) \u2013 full\u2026",
                "However, as our goal is to learn the full distribution of demonstrations, we instead follow the setup introduced by Shafiullah et al. (2022), which ignores any goal conditioning and aims to train an agent that can recover the full set of demonstrating policies.",
                "We made extensive efforts to bring performance of K-means+residual inline with that reported in Shafiullah et al. (2022).",
                "Shafiullah et al. (2022) proposed a transformer based BC method that implements the KMeans+Residual approach discussed in Sec.",
                "Each episode lasted 280 timesteps as in Shafiullah et al. (2022) \u2013 98% of humans completed their assigned four tasks within this time."
            ],
            "citingPaper": {
                "paperId": "b43330013a5abcccd366d71f2f66c493c790abc6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2301-10677",
                    "ArXiv": "2301.10677",
                    "DOI": "10.48550/arXiv.2301.10677",
                    "CorpusId": 256231177
                },
                "corpusId": 256231177,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/b43330013a5abcccd366d71f2f66c493c790abc6",
                "title": "Imitating Human Behaviour with Diffusion Models",
                "abstract": "Diffusion models have emerged as powerful generative models in the text-to-image domain. This paper studies their application as observation-to-action models for imitating human behaviour in sequential environments. Human behaviour is stochastic and multimodal, with structured correlations between action dimensions. Meanwhile, standard modelling choices in behaviour cloning are limited in their expressiveness and may introduce bias into the cloned policy. We begin by pointing out the limitations of these choices. We then propose that diffusion models are an excellent fit for imitating human behaviour, since they learn an expressive distribution over the joint action space. We introduce several innovations to make diffusion models suitable for sequential environments; designing suitable architectures, investigating the role of guidance, and developing reliable sampling strategies. Experimentally, diffusion models closely match human demonstrations in a simulated robotic control task and a modern 3D gaming environment.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1720750695",
                        "name": "Tim Pearce"
                    },
                    {
                        "authorId": "36054740",
                        "name": "Tabish Rashid"
                    },
                    {
                        "authorId": "3469155",
                        "name": "A. Kanervisto"
                    },
                    {
                        "authorId": "2064459380",
                        "name": "David Bignell"
                    },
                    {
                        "authorId": "9492808",
                        "name": "Mingfei Sun"
                    },
                    {
                        "authorId": "2099584262",
                        "name": "Raluca Georgescu"
                    },
                    {
                        "authorId": "9164659",
                        "name": "Sergio Valcarcel Macua"
                    },
                    {
                        "authorId": "2202716298",
                        "name": "Shan Zheng Tan"
                    },
                    {
                        "authorId": "1990422",
                        "name": "I. Momennejad"
                    },
                    {
                        "authorId": "1380228856",
                        "name": "Katja Hofmann"
                    },
                    {
                        "authorId": "1693696",
                        "name": "Sam Devlin"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "638b5c76d96e32f54475a8327a9c68e0167156a9",
                "externalIds": {
                    "ArXiv": "2301.03044",
                    "DBLP": "journals/corr/abs-2301-03044",
                    "DOI": "10.48550/arXiv.2301.03044",
                    "CorpusId": 255546304
                },
                "corpusId": 255546304,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/638b5c76d96e32f54475a8327a9c68e0167156a9",
                "title": "A Survey on Transformers in Reinforcement Learning",
                "abstract": "Transformer has been considered the dominating neural architecture in NLP and CV, mostly under supervised settings. Recently, a similar surge of using Transformers has appeared in the domain of reinforcement learning (RL), but it is faced with unique design choices and challenges brought by the nature of RL. However, the evolution of Transformers in RL has not yet been well unraveled. In this paper, we seek to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2159213989",
                        "name": "Wenzhe Li"
                    },
                    {
                        "authorId": "2199828096",
                        "name": "Hao Luo"
                    },
                    {
                        "authorId": "41123614",
                        "name": "Zichuan Lin"
                    },
                    {
                        "authorId": "2111387140",
                        "name": "Chongjie Zhang"
                    },
                    {
                        "authorId": "2265693",
                        "name": "Zongqing Lu"
                    },
                    {
                        "authorId": "2055648566",
                        "name": "Deheng Ye"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "[66] presented a behavior transformer (BeT), which is able to learn behaviors from distributionally multimodal data.",
                "Autonomous Driving SPLT [65] Disentangling the policy and world models ICML 2022 BeT [66] Modeling unlabeled demonstration data with multiple modes NeurIPS 2022 TransFuser [67] Fusion of intermediate features of the front view and LiDAR CVPR 2021 InterFuse [68] Safety-enhanced framework with multi-modal, view sensors CoRL 2022",
                "In particular, the BeT divides a continuous actions into two parts: a categorical\nvariable denoting an \"action center\" by k-means [240] and a corresponding \"residual action\"; it then uses the transformer to map each observation to a categorical distribution over k discrete action bins with the focal loss [241]:\nLfocal(pt) = \u2212(1\u2212 pt)\u03b3 log(pt).",
                "To train models that can naively learn multimodal policy behaviors, Shafiullah et al. [66] presented a behavior transformer (BeT), which is able to learn behaviors from distributionally multimodal data.",
                "(32)\nAn extra head is used to predict the offset with a loss akin to the masked multitask loss [242]:\nMT-Loss(a, (\u3008a\u0302(j)i \u3009) k j=1) = k\u2211 j=1 I[bac = j] \u00b7 ||\u3008a\u3009 \u2212 \u3008a\u0302(j)\u3009||22,\n(33) where I[] denotes the Iverson bracket, ensuring that the loss is only incurred from the ground-truth class of action a. Experiments conducted on CARLA showed that the BeT is able to cover all the modes of demonstration data."
            ],
            "citingPaper": {
                "paperId": "060cee8411181e8151ab1e3212b81528accd9b8b",
                "externalIds": {
                    "ArXiv": "2212.14164",
                    "CorpusId": 260442424
                },
                "corpusId": 260442424,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/060cee8411181e8151ab1e3212b81528accd9b8b",
                "title": "On Transforming Reinforcement Learning by Transformer: The Development Trajectory",
                "abstract": "Transformer, originally devised for natural language processing, has also attested significant success in computer vision. Thanks to its super expressive power, researchers are investigating ways to deploy transformers to reinforcement learning (RL) and the transformer-based models have manifested their potential in representative RL benchmarks. In this paper, we collect and dissect recent advances on transforming RL by transformer (transformer-based RL or TRL), in order to explore its development trajectory and future trend. We group existing developments in two categories: architecture enhancement and trajectory optimization, and examine the main applications of TRL in robotic manipulation, text-based games, navigation and autonomous driving. For architecture enhancement, these methods consider how to apply the powerful transformer structure to RL problems under the traditional RL framework, which model agents and environments much more precisely than deep RL methods, but they are still limited by the inherent defects of traditional RL algorithms, such as bootstrapping and\"deadly triad\". For trajectory optimization, these methods treat RL problems as sequence modeling and train a joint state-action model over entire trajectories under the behavior cloning framework, which are able to extract policies from static datasets and fully use the long-sequence modeling capability of the transformer. Given these advancements, extensions and challenges in TRL are reviewed and proposals about future direction are discussed. We hope that this survey can provide a detailed introduction to TRL and motivate future research in this rapidly developing field.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2176837980",
                        "name": "Shengchao Hu"
                    },
                    {
                        "authorId": "2144035454",
                        "name": "Li Shen"
                    },
                    {
                        "authorId": "2910574",
                        "name": "Ya Zhang"
                    },
                    {
                        "authorId": "2223152252",
                        "name": "Yixin Chen"
                    },
                    {
                        "authorId": "2135519749",
                        "name": "Dacheng Tao"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d",
                "externalIds": {
                    "ArXiv": "2212.06817",
                    "DBLP": "journals/corr/abs-2212-06817",
                    "DOI": "10.48550/arXiv.2212.06817",
                    "CorpusId": 254591260
                },
                "corpusId": 254591260,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d",
                "title": "RT-1: Robotics Transformer for Real-World Control at Scale",
                "abstract": "By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project's website and videos can be found at robotics-transformer1.github.io",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "118025075",
                        "name": "Anthony Brohan"
                    },
                    {
                        "authorId": "2161343011",
                        "name": "Noah Brown"
                    },
                    {
                        "authorId": "2196517336",
                        "name": "Justice Carbajal"
                    },
                    {
                        "authorId": "2527420",
                        "name": "Yevgen Chebotar"
                    },
                    {
                        "authorId": "2196517328",
                        "name": "Joseph Dabis"
                    },
                    {
                        "authorId": "46881670",
                        "name": "Chelsea Finn"
                    },
                    {
                        "authorId": "2161342233",
                        "name": "K. Gopalakrishnan"
                    },
                    {
                        "authorId": "1944801",
                        "name": "Karol Hausman"
                    },
                    {
                        "authorId": "1505793452",
                        "name": "Alexander Herzog"
                    },
                    {
                        "authorId": "2726592",
                        "name": "Jasmine Hsu"
                    },
                    {
                        "authorId": "46920727",
                        "name": "Julian Ibarz"
                    },
                    {
                        "authorId": "2704814",
                        "name": "Brian Ichter"
                    },
                    {
                        "authorId": "17818078",
                        "name": "A. Irpan"
                    },
                    {
                        "authorId": "2175779811",
                        "name": "Tomas Jackson"
                    },
                    {
                        "authorId": "2161341920",
                        "name": "Sally Jesmonth"
                    },
                    {
                        "authorId": "2052368480",
                        "name": "Nikhil J. Joshi"
                    },
                    {
                        "authorId": "144885996",
                        "name": "Ryan C. Julian"
                    },
                    {
                        "authorId": "48313860",
                        "name": "Dmitry Kalashnikov"
                    },
                    {
                        "authorId": "2161342687",
                        "name": "Yuheng Kuang"
                    },
                    {
                        "authorId": "2057988112",
                        "name": "Isabel Leal"
                    },
                    {
                        "authorId": "2145145412",
                        "name": "Kuang-Huei Lee"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    },
                    {
                        "authorId": "2161346119",
                        "name": "Yao Lu"
                    },
                    {
                        "authorId": "51225879",
                        "name": "U. Malla"
                    },
                    {
                        "authorId": "2064295888",
                        "name": "D. Manjunath"
                    },
                    {
                        "authorId": "2080746",
                        "name": "Igor Mordatch"
                    },
                    {
                        "authorId": "7624658",
                        "name": "Ofir Nachum"
                    },
                    {
                        "authorId": "2057314286",
                        "name": "Carolina Parada"
                    },
                    {
                        "authorId": "2195951533",
                        "name": "Jodilyn Peralta"
                    },
                    {
                        "authorId": "2125121323",
                        "name": "Emily Perez"
                    },
                    {
                        "authorId": "31719101",
                        "name": "Karl Pertsch"
                    },
                    {
                        "authorId": "2161342191",
                        "name": "Jornell Quiambao"
                    },
                    {
                        "authorId": "2251957",
                        "name": "Kanishka Rao"
                    },
                    {
                        "authorId": "1766489",
                        "name": "M. Ryoo"
                    },
                    {
                        "authorId": "2196524735",
                        "name": "Grecia Salazar"
                    },
                    {
                        "authorId": "2840758",
                        "name": "P. Sanketi"
                    },
                    {
                        "authorId": "2196510642",
                        "name": "Kevin Sayed"
                    },
                    {
                        "authorId": "2196040785",
                        "name": "Jaspiar Singh"
                    },
                    {
                        "authorId": "34365421",
                        "name": "S. Sontakke"
                    },
                    {
                        "authorId": "2056868723",
                        "name": "Austin Stone"
                    },
                    {
                        "authorId": "2161386250",
                        "name": "Clayton Tan"
                    },
                    {
                        "authorId": "2195355151",
                        "name": "Huong Tran"
                    },
                    {
                        "authorId": "2657155",
                        "name": "Vincent Vanhoucke"
                    },
                    {
                        "authorId": "2195627101",
                        "name": "Steve Vega"
                    },
                    {
                        "authorId": "144579461",
                        "name": "Q. Vuong"
                    },
                    {
                        "authorId": "144956443",
                        "name": "F. Xia"
                    },
                    {
                        "authorId": "9961095",
                        "name": "Ted Xiao"
                    },
                    {
                        "authorId": "2153917744",
                        "name": "Peng Xu"
                    },
                    {
                        "authorId": "3068504",
                        "name": "Sichun Xu"
                    },
                    {
                        "authorId": "10909315",
                        "name": "Tianhe Yu"
                    },
                    {
                        "authorId": "2196524598",
                        "name": "Brianna Zitkovich"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "401 Other works have tackled policy learning in much more complex settings like a simulated realistic 402 looking kitchen with several objects, but assume ground-truth simulator state observations instead 403 of visual inputs [31, 32]."
            ],
            "citingPaper": {
                "paperId": "9ffc8f7b3fbd5e609f609b1c20206129f22b4eb7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-05711",
                    "ArXiv": "2212.05711",
                    "DOI": "10.48550/arXiv.2212.05711",
                    "CorpusId": 254183982
                },
                "corpusId": 254183982,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9ffc8f7b3fbd5e609f609b1c20206129f22b4eb7",
                "title": "CACTI: A Framework for Scalable Multi-Task Multi-Scene Visual Imitation Learning",
                "abstract": "Large-scale training have propelled significant progress in various sub-fields of AI such as computer vision and natural language processing. However, building robot learning systems at a comparable scale remains challenging. To develop robots that can perform a wide range of skills and adapt to new scenarios, efficient methods for collecting vast and diverse amounts of data on physical robot systems are required, as well as the capability to train high-capacity policies using such datasets. In this work, we propose a framework for scaling robot learning, with specific focus on multi-task and multi-scene manipulation in kitchen environments, both in simulation and in the real world. Our proposed framework, CACTI, comprises four stages that separately handle: data collection, data augmentation, visual representation learning, and imitation policy training, to enable scalability in robot learning . We make use of state-of-the-art generative models as part of the data augmentation stage, and use pre-trained out-of-domain visual representations to improve training efficiency. Experimental results demonstrate the effectiveness of our approach. On a real robot setup, CACTI enables efficient training of a single policy that can perform 10 manipulation tasks involving kitchen objects, and is robust to varying layouts of distractors. In a simulated kitchen environment, CACTI trains a single policy to perform 18 semantic tasks across 100 layout variations for each individual task. We will release the simulation task benchmark and augmented datasets in both real and simulated environments to facilitate future research.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2126966292",
                        "name": "Zhao Mandi"
                    },
                    {
                        "authorId": "51113848",
                        "name": "Homanga Bharadhwaj"
                    },
                    {
                        "authorId": "12887111",
                        "name": "V. Moens"
                    },
                    {
                        "authorId": "2110601402",
                        "name": "Shuran Song"
                    },
                    {
                        "authorId": "19275599",
                        "name": "A. Rajeswaran"
                    },
                    {
                        "authorId": "2109446216",
                        "name": "Vikash Kumar"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "contexts": [
                "\u2026Chen et al. (2021a) and Janner et al. (2021) consider offline RL as supervised sequential modeling problem and following works achieve impressive success (Reed et al., 2022; Lee et al., 2022; Furuta et al., 2022; Xu et al., 2022; Shafiullah et al., 2022; Zheng et al., 2022; Paster et al., 2022).",
                "(2021) consider offline RL as supervised sequential modeling problem and following works achieve impressive success (Reed et al., 2022; Lee et al., 2022; Furuta et al., 2022; Xu et al., 2022; Shafiullah et al., 2022; Zheng et al., 2022; Paster et al., 2022)."
            ],
            "citingPaper": {
                "paperId": "8745c5b9522c11818418f64fdc880894faeaed16",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-14296",
                    "ArXiv": "2211.14296",
                    "DOI": "10.48550/arXiv.2211.14296",
                    "CorpusId": 254018220
                },
                "corpusId": 254018220,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8745c5b9522c11818418f64fdc880894faeaed16",
                "title": "A System for Morphology-Task Generalization via Unified Representation and Behavior Distillation",
                "abstract": "The rise of generalist large-scale models in natural language and vision has made us expect that a massive data-driven approach could achieve broader generalization in other domains such as continuous control. In this work, we explore a method for learning a single policy that manipulates various forms of agents to solve various tasks by distilling a large amount of proficient behavioral data. In order to align input-output (IO) interface among multiple tasks and diverse agent morphologies while preserving essential 3D geometric relations, we introduce morphology-task graph, which treats observations, actions and goals/task in a unified graph representation. We also develop MxT-Bench for fast large-scale behavior generation, which supports procedural generation of diverse morphology-task combinations with a minimal blueprint and hardware-accelerated simulator. Through efficient representation and architecture selection on MxT-Bench, we find out that a morphology-task graph representation coupled with Transformer architecture improves the multi-task performances compared to other baselines including recent discrete tokenization, and provides better prior knowledge for zero-shot transfer or sample efficiency in downstream multi-task imitation learning. Our work suggests large diverse offline datasets, unified IO representation, and policy representation and architecture selection through supervised learning form a promising approach for studying and advancing morphology-task generalization.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2052903664",
                        "name": "Hiroki Furuta"
                    },
                    {
                        "authorId": "1715282",
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "authorId": "2153732825",
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "authorId": "2046135",
                        "name": "S. Gu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", 2022), robot learning and behavior cloning from noisy demonstrations (Shafiullah et al., 2022; Cui et al., 2022), and language-conditioned RL (Guhur et al.",
                "\u2026learning (Chen et al., 2021a; Reed et al., 2022), vision-language navigation (Chen et al., 2021b; Shah et al., 2022), robot learning and behavior cloning from noisy demonstrations (Shafiullah et al., 2022; Cui et al., 2022), and language-conditioned RL (Guhur et al., 2022; Shridhar et al., 2022a)."
            ],
            "citingPaper": {
                "paperId": "172cb3ca0b283eb1381fa840d51003d3ec0465d1",
                "externalIds": {
                    "ArXiv": "2210.13431",
                    "CorpusId": 257766739
                },
                "corpusId": 257766739,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/172cb3ca0b283eb1381fa840d51003d3ec0465d1",
                "title": "Instruction-Following Agents with Multimodal Transformer",
                "abstract": "Humans are excellent at understanding language and vision to accomplish a wide range of tasks. In contrast, creating general instruction-following embodied agents remains a difficult challenge. Prior work that uses pure language-only models lack visual grounding, making it difficult to connect language instructions with visual observations. On the other hand, methods that use pre-trained multimodal models typically come with divided language and visual representations, requiring designing specialized network architecture to fuse them together. We propose a simple yet effective model for robots to solve instruction-following tasks in vision-based environments. Our \\ours method consists of a multimodal transformer that encodes visual observations and language instructions, and a transformer-based policy that predicts actions based on encoded representations. The multimodal transformer is pre-trained on millions of image-text pairs and natural language text, thereby producing generic cross-modal representations of observations and instructions. The transformer-based policy keeps track of the full history of observations and actions, and predicts actions autoregressively. Despite its simplicity, we show that this unified transformer model outperforms all state-of-the-art pre-trained or trained-from-scratch methods in both single-task and multi-task settings. Our model also shows better model scalability and generalization ability than prior work.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2143855835",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "87068304",
                        "name": "Lisa Lee"
                    },
                    {
                        "authorId": "3436470",
                        "name": "Kimin Lee"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "Our work is most closely related to Shafiullah et al. (2022) as we build on their transformer architecture, while our unimodal baseline is a variant of Chen et al. (2021) that learns outcome conditioned instead of reward conditioned policy.",
                "As a result, we choose Behavior Transformers (BeT) (Shafiullah et al., 2022) as our generative architecture base as it can learn action generation with multiple modes.",
                "B.2 HYPERPARAMETERS LIST:\nWe present the C-BeT hyperparameters in Table 6 below, which were mostly using the default hyperparameters in the original Shafiullah et al. (2022) paper:\nThe shared hyperparameters are in Table 7.",
                "We use Behavior Transformers from Shafiullah et al. (2022) as our backbone architecture, building our conditional algorithm on top of it.",
                "Behavior Transformers (BeT): BeT (Shafiullah et al., 2022) is a multi-modal behavior cloning model designed particularly for tackling play-like behavior datasets.",
                "To produce a distribution over continuous actions instead of discrete tokens, C-BeT augments standard text generation transformers with the action discretization introduced in Behavior Transformers (BeT) (Shafiullah et al., 2022).",
                "Transformers for behavior learning: Our work follows earlier notable works in using transformers to learn a behavior model from an offline dataset, such as Chen et al. (2021); Janner et al. (2021); Shafiullah et al. (2022)."
            ],
            "citingPaper": {
                "paperId": "9b5f4aab169fba588e214c010345232053f8ae76",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-10047",
                    "ArXiv": "2210.10047",
                    "DOI": "10.48550/arXiv.2210.10047",
                    "CorpusId": 252968170
                },
                "corpusId": 252968170,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/9b5f4aab169fba588e214c010345232053f8ae76",
                "title": "From Play to Policy: Conditional Behavior Generation from Uncurated Robot Data",
                "abstract": "While large-scale sequence modeling from offline data has led to impressive performance gains in natural language and image generation, directly translating such ideas to robotics has been challenging. One critical reason for this is that uncurated robot demonstration data, i.e. play data, collected from non-expert human demonstrators are often noisy, diverse, and distributionally multi-modal. This makes extracting useful, task-centric behaviors from such data a difficult generative modeling problem. In this work, we present Conditional Behavior Transformers (C-BeT), a method that combines the multi-modal generation ability of Behavior Transformer with future-conditioned goal specification. On a suite of simulated benchmark tasks, we find that C-BeT improves upon prior state-of-the-art work in learning from play data by an average of 45.7%. Further, we demonstrate for the first time that useful task-centric behaviors can be learned on a real-world robot purely from play data without any task labels or reward information. Robot videos are best viewed on our project website: https://play-to-policy.github.io",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2172045711",
                        "name": "Zichen Jeff Cui"
                    },
                    {
                        "authorId": "2108824771",
                        "name": "Yibin Wang"
                    },
                    {
                        "authorId": "84146411",
                        "name": "Nur Muhammad (Mahi) Shafiullah"
                    },
                    {
                        "authorId": "34026610",
                        "name": "Lerrel Pinto"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "05c4bfd0b16fd0ab0e72e0866d0a5bfec5ad7ded",
                "externalIds": {
                    "DBLP": "conf/rss/KumarSENYFL23",
                    "ArXiv": "2210.05178",
                    "DOI": "10.15607/RSS.2023.XIX.019",
                    "CorpusId": 252816019
                },
                "corpusId": 252816019,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/05c4bfd0b16fd0ab0e72e0866d0a5bfec5ad7ded",
                "title": "Pre-Training for Robots: Offline RL Enables Learning New Tasks in a Handful of Trials",
                "abstract": "Progress in deep learning highlights the tremendous potential of utilizing diverse robotic datasets for attaining effective generalization and makes it enticing to consider leveraging broad datasets for attaining robust generalization in robotic learning as well. However, in practice, we often want to learn a new skill in a new environment that is unlikely to be contained in the prior data. Therefore we ask: how can we leverage existing diverse offline datasets in combination with small amounts of task-specific data to solve new tasks, while still enjoying the generalization benefits of training on large amounts of data? In this paper, we demonstrate that end-to-end offline RL can be an effective approach for doing this, without the need for any representation learning or vision-based pre-training. We present pre-training for robots (PTR), a framework based on offline RL that attempts to effectively learn new tasks by combining pre-training on existing robotic datasets with rapid fine-tuning on a new task, with as few as 10 demonstrations. PTR utilizes an existing offline RL method, conservative Q-learning (CQL), but extends it to include several crucial design decisions that enable PTR to actually work and outperform a variety of prior methods. To our knowledge, PTR is the first RL method that succeeds at learning new tasks in a new domain on a real WidowX robot with as few as 10 task demonstrations, by effectively leveraging an existing dataset of diverse multi-task robot data collected in a variety of toy kitchens. We also demonstrate that PTR can enable effective autonomous fine-tuning and improvement in a handful of trials, without needing any demonstrations. An accompanying overview video can be found in the supplementary material and at thi URL: https://sites.google.com/view/ptr-final/",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1488785534",
                        "name": "Aviral Kumar"
                    },
                    {
                        "authorId": "2111007256",
                        "name": "Anika Singh"
                    },
                    {
                        "authorId": "27535721",
                        "name": "F. Ebert"
                    },
                    {
                        "authorId": "2211096149",
                        "name": "Mitsuhiko Nakamoto"
                    },
                    {
                        "authorId": "3800238",
                        "name": "Yanlai Yang"
                    },
                    {
                        "authorId": "46881670",
                        "name": "Chelsea Finn"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "25425e299101b13ec2872417a14f961f4f8aa18e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-03094",
                    "ArXiv": "2210.03094",
                    "DOI": "10.48550/arXiv.2210.03094",
                    "CorpusId": 252735175
                },
                "corpusId": 252735175,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/25425e299101b13ec2872417a14f961f4f8aa18e",
                "title": "VIMA: General Robot Manipulation with Multimodal Prompts",
                "abstract": "Prompt-based learning has emerged as a successful paradigm in natural language processing, where a single general-purpose language model can be instructed to perform any task specified by input prompts. Yet task specification in robotics comes in various forms, such as imitating one-shot demonstrations, following language instructions, and reaching visual goals. They are often considered different tasks and tackled by specialized models. We show that a wide spectrum of robot manipulation tasks can be expressed with multimodal prompts, interleaving textual and visual tokens. Accordingly, we develop a new simulation benchmark that consists of thousands of procedurally-generated tabletop tasks with multimodal prompts, 600K+ expert trajectories for imitation learning, and a four-level evaluation protocol for systematic generalization. We design a transformer-based robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively. VIMA features a recipe that achieves strong model scalability and data efficiency. It outperforms alternative designs in the hardest zero-shot generalization setting by up to $2.9\\times$ task success rate given the same training data. With $10\\times$ less training data, VIMA still performs $2.7\\times$ better than the best competing variant. Code and video demos are available at https://vimalabs.github.io/",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2171112793",
                        "name": "Yunfan Jiang"
                    },
                    {
                        "authorId": "25445698",
                        "name": "Agrim Gupta"
                    },
                    {
                        "authorId": "5630943",
                        "name": "Zichen Zhang"
                    },
                    {
                        "authorId": "96374437",
                        "name": "Guanzhi Wang"
                    },
                    {
                        "authorId": "1768148923",
                        "name": "Yongqiang Dou"
                    },
                    {
                        "authorId": "2187067176",
                        "name": "Yanjun Chen"
                    },
                    {
                        "authorId": "48004138",
                        "name": "Li Fei-Fei"
                    },
                    {
                        "authorId": "47627049",
                        "name": "Anima Anandkumar"
                    },
                    {
                        "authorId": "2117748",
                        "name": "Yuke Zhu"
                    },
                    {
                        "authorId": "3275727",
                        "name": "Linxi (Jim) Fan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "35f8eb09776abebad8a963b59978d673ab97301a",
                "externalIds": {
                    "ArXiv": "2209.13046",
                    "CorpusId": 256389549
                },
                "corpusId": 256389549,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/35f8eb09776abebad8a963b59978d673ab97301a",
                "title": "Understanding Hindsight Goal Relabeling from a Divergence Minimization Perspective",
                "abstract": "Hindsight goal relabeling has become a foundational technique in multi-goal reinforcement learning (RL). The essential idea is that any trajectory can be seen as a sub-optimal demonstration for reaching its final state. Intuitively, learning from those arbitrary demonstrations can be seen as a form of imitation learning (IL). However, the connection between hindsight goal relabeling and imitation learning is not well understood. In this paper, we propose a novel framework to understand hindsight goal relabeling from a divergence minimization perspective. Recasting the goal reaching problem in the IL framework not only allows us to derive several existing methods from first principles, but also provides us with the tools from IL to improve goal reaching algorithms. Experimentally, we find that under hindsight relabeling, Q-learning outperforms behavioral cloning (BC). Yet, a vanilla combination of both hurts performance. Concretely, we see that the BC loss only helps when selectively applied to actions that get the agent closer to the goal according to the Q-function. Our framework also explains the puzzling phenomenon wherein a reward of (-1, 0) results in significantly better performance than a (0, 1) reward for goal reaching.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48571502",
                        "name": "Lunjun Zhang"
                    },
                    {
                        "authorId": "3275284",
                        "name": "Bradly C. Stadie"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                "However, as offline datasets are often collected by a mixture of policies, the true behavior policy may exhibit strong multi-modalities, skewness, or dependencies between different action dimensions, which cannot be well modeled by diagonal Gaussian policies (Shafiullah et al., 2022)."
            ],
            "citingPaper": {
                "paperId": "14e5cf62fe3974f97c4a5b9b16079b50dd5f4606",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-06193",
                    "ArXiv": "2208.06193",
                    "DOI": "10.48550/arXiv.2208.06193",
                    "CorpusId": 251554821
                },
                "corpusId": 251554821,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/14e5cf62fe3974f97c4a5b9b16079b50dd5f4606",
                "title": "Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning",
                "abstract": "Offline reinforcement learning (RL), which aims to learn an optimal policy using a previously collected static dataset, is an important paradigm of RL. Standard RL methods often perform poorly in this regime due to the function approximation errors on out-of-distribution actions. While a variety of regularization methods have been proposed to mitigate this issue, they are often constrained by policy classes with limited expressiveness that can lead to highly suboptimal solutions. In this paper, we propose representing the policy as a diffusion model, a recent class of highly-expressive deep generative models. We introduce Diffusion Q-learning (Diffusion-QL) that utilizes a conditional diffusion model to represent the policy. In our approach, we learn an action-value function and we add a term maximizing action-values into the training loss of the conditional diffusion model, which results in a loss that seeks optimal actions that are near the behavior policy. We show the expressiveness of the diffusion model-based policy, and the coupling of the behavior cloning and policy improvement under the diffusion model both contribute to the outstanding performance of Diffusion-QL. We illustrate the superiority of our method compared to prior works in a simple 2D bandit example with a multimodal behavior policy. We then show that our method can achieve state-of-the-art performance on the majority of the D4RL benchmark tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108160372",
                        "name": "Zhendong Wang"
                    },
                    {
                        "authorId": "2323922",
                        "name": "Jonathan J. Hunt"
                    },
                    {
                        "authorId": "38026572",
                        "name": "Mingyuan Zhou"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "contexts": [
                ", 2022), robot learning and behavior cloning from noisy demonstrations (Shafiullah et al., 2022; Cui et al., 2022), and language-conditioned RL (Guhur et al.",
                "\u2026learning (Chen et al., 2021a; Reed et al., 2022), vision-language navigation (Chen et al., 2021b; Shah et al., 2022), robot learning and behavior cloning from noisy demonstrations (Shafiullah et al., 2022; Cui et al., 2022), and language-conditioned RL (Guhur et al., 2022; Shridhar et al., 2022a)."
            ],
            "citingPaper": {
                "paperId": "af3a6deac815c7e41a58b15be74e5e7a1066b899",
                "externalIds": {
                    "CorpusId": 256416555
                },
                "corpusId": 256416555,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/af3a6deac815c7e41a58b15be74e5e7a1066b899",
                "title": "InstructRL: Simple yet Effective Instruction-Following Agents with Multimodal Transformer",
                "abstract": "Humans are excellent at understanding language and vision to accomplish a wide range of tasks. In contrast, creating general instruction-following embodied agents remains a dif\ufb01cult challenge. Prior work that uses pure language-only models lack visual grounding, making it dif\ufb01cult to connect language instructions with visual observations. On the other hand, methods that use pre-trained multimodal models typically come with divided language and visual representations, requiring designing specialized network architecture to fuse them together. We propose a simple yet effective model for robots to solve instruction-following tasks in vision-based environments. Our Instruct RL method consists of a multimodal transformer that encodes visual observations and language instructions, and a transformer-based policy that predicts actions based on encoded representations. The multimodal transformer is pre-trained on millions of image-text pairs and natural language text, thereby producing generic cross-modal representations of observations and instructions. The transformer-based policy keeps track of the full history of observations and actions, and predicts actions autoregressively. Despite its simplicity, we show that this uni\ufb01ed transformer model outperforms all state-of-the-art pre-trained or trained-from-scratch methods in both single-task and multi-task settings. Our model also shows better model scalability and generalization ability than prior work.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2143855835",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "87068304",
                        "name": "Lisa Lee"
                    },
                    {
                        "authorId": "3436470",
                        "name": "Kimin Lee"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "5b06056345034e1559ef8680190cdccc79a2196d",
                "externalIds": {
                    "DBLP": "conf/icml/JiangG0WDC0AZF23",
                    "CorpusId": 260844398
                },
                "corpusId": 260844398,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/5b06056345034e1559ef8680190cdccc79a2196d",
                "title": "VIMA: Robot Manipulation with Multimodal Prompts",
                "abstract": "Prompt-based learning has emerged as a successful paradigm in natural language processing, where a single general-purpose language model can be instructed to perform any task specified by input prompts. Yet task specification in robotics comes in various forms, such as imitating one-shot demonstrations, following language instructions, and reaching visual goals. They are often considered different tasks and tackled by specialized models. We show that a wide spectrum of robot manipulation tasks can be expressed with multimodal prompts , interleaving textual and visual tokens. Accordingly, we develop a new simulation benchmark that consists of thousands of procedurally-generated tabletop tasks with multimodal prompts, 600K+ expert trajectories for imitation learning, and a four-level evaluation protocol for systematic generalization. We de-sign a transformer-based robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively. VIMA features a recipe that achieves strong model scalability and data efficiency. It outperforms alternative designs in the hardest zero-shot generalization setting by up to 2 . 9 \u00d7 task success rate given the same training data. With 10 \u00d7 less training data, VIMA still performs 2 . 7 \u00d7 better than the best competing variant. Code and video demos are available at vimalabs.github.io .",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2171112793",
                        "name": "Yunfan Jiang"
                    },
                    {
                        "authorId": "25445698",
                        "name": "Agrim Gupta"
                    },
                    {
                        "authorId": "5630943",
                        "name": "Zichen Zhang"
                    },
                    {
                        "authorId": "96374437",
                        "name": "Guanzhi Wang"
                    },
                    {
                        "authorId": "1768148923",
                        "name": "Yongqiang Dou"
                    },
                    {
                        "authorId": "2187067176",
                        "name": "Yanjun Chen"
                    },
                    {
                        "authorId": "2146326206",
                        "name": "Fei-Fei Li"
                    },
                    {
                        "authorId": "47627049",
                        "name": "Anima Anandkumar"
                    },
                    {
                        "authorId": "2117748",
                        "name": "Yuke Zhu"
                    },
                    {
                        "authorId": "3275727",
                        "name": "Linxi (Jim) Fan"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [],
            "citingPaper": {
                "paperId": "462f0e65fa5ce0bc27241abba519f8a4bf40e879",
                "externalIds": {
                    "CorpusId": 262647964
                },
                "corpusId": 262647964,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/462f0e65fa5ce0bc27241abba519f8a4bf40e879",
                "title": "Learning to Discern: Imitating Heterogeneous Human Demonstrations with Preference and Representation Learning",
                "abstract": ": Practical Imitation Learning (IL) systems rely on large human demon-1 stration datasets for successful policy learning. However, challenges lie in main-2 taining the quality of collected data and addressing the suboptimal nature of some 3 demonstrations, which can compromise the overall dataset quality and hence the 4 learning outcome. Furthermore, the intrinsic heterogeneity in human behavior can 5 produce equally successful but disparate demonstrations, further exacerbating the 6 challenge of discerning demonstration quality. To address these challenges, this 7 paper introduces Learning to Discern (L2D), an imitation learning framework for 8 learning from demonstrations with diverse quality and style. Given a small batch 9 of demonstrations with sparse quality labels, we learn a latent representation for 10 temporally-embeded trajectory segments. Preference learning in this latent space 11 trains a quality evaluator that generalizes to new demonstrators exhibiting differ-12 ent styles. Empirically, we show that L2D can effectively assess and learn from 13 varying demonstrations, thereby leading to improved policy performance across a 14 range of tasks in both simulation and on a physical robot. 15",
                "year": 2023,
                "authors": []
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "Prior approaches utilize such existing data by running imitation learning (IL) (Young et al., 2020; Ebert et al., 2021; Shafiullah et al., 2022) or by using representation learning (Nair et al.",
                "\u2026via representation learning (Mandlekar et al., 2020; Yang & Nachum, 2021; Yang et al., 2021; Nair et al., 2022; He et al., 2021), as well as other methods for learning from human demonstrations, such as behavioral cloning methods with expressive policy architectures (Shafiullah et al., 2022).",
                ", 2021), as well as other methods for learning from human demonstrations, such as behavioral cloning methods with expressive policy architectures (Shafiullah et al., 2022).",
                "Prior approaches utilize such existing data by running imitation learning (IL) (Young et al., 2020; Ebert et al., 2021; Shafiullah et al., 2022) or by using representation learning (Nair et al., 2022) methods for pre-training and then fine-tuning with imitation learning.",
                "The sencond approach was with the BeT Architecture from Shafiullah et al. (2022).",
                "Note that PTR outperforms all other baselines including BC (finetune), BC with more expressive policy classes (BeT (Shafiullah et al., 2022), Auto-regressive), representation learning methods (Nair et al."
            ],
            "citingPaper": {
                "paperId": "2d39ce0421b85f0b5525ac0f282ea2a6599553f3",
                "externalIds": {
                    "CorpusId": 254222798
                },
                "corpusId": 254222798,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2d39ce0421b85f0b5525ac0f282ea2a6599553f3",
                "title": "P RE -T RAINING FOR R OBOTS : L EVERAGING D IVERSE M ULTITASK D ATA VIA O FFLINE RL",
                "abstract": "Recent progress in deep learning highlights the tremendous potential of utilizing diverse datasets for achieving effective generalization and makes it enticing to consider leveraging broad datasets for attaining more robust generalization in robotic learning as well. However, in practice we likely will want to learn a new skill in a new environment that is unlikely to be contained in the prior data. Therefore we ask: how can we leverage existing diverse offline datasets in combination with small amounts of task-specific data to solve new tasks, while still enjoying the generalization benefits of training on large amounts of data? In this paper, we demonstrate that end-to-end offline RL can be an effective approach for doing this, without the need for any representation learning or vision-based pre-training. We present pre-training for robots (PTR), a framework based on offline RL that attempts to effectively learn new tasks by combining pre-training on existing robotic datasets with rapid fine-tuning on a new task, with as a few as 10 demonstrations. At its core, PTR applies an existing offline RL method such as conservative Qlearning (CQL), but extends it to include several crucial design decisions that enable PTR to actually work and outperform a variety of prior methods. To the best of our knowledge, PTR is the first offline RL method that succeeds at learning new tasks in a new domain on a real WidowX robot with as few as 10 task demonstrations, by effectively leveraging an existing dataset of diverse multi-task robot data collected in a variety of toy kitchens. We present an accompanying overview video at this Anonymous URl: https://www.youtube.com/watch?v= yAWgyLJD5lY&ab_channel=PTRICLR.",
                "year": 2022,
                "authors": []
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "isInfluential": true,
            "contexts": [
                "Autonomous Driving SPLT [65] Disentangling the policy and world models ICML 2022 BeT [66] Modeling unlabeled demonstration data with multiple modes NeurIPS 2022 TransFuser [67] Fusion of intermediate features of the front view and LiDAR CVPR 2021 InterFuse [68] Safety-enhanced framework with multi-modal, view sensors CoRL 2022",
                "[66] present Behavior Transformers (BeT), which is able to learn behaviors from distributionally multi-modal data.",
                "In order to train models that can naively learn multimodal policy behavior, Shafiullah et al. [66] present Behavior Transformers (BeT), which is able to learn behaviors from distributionally multi-modal data.",
                "In particular, BeT divides the continuous actions into two parts: a categorical variable denoting an \"action center\" by k-means [240] and a corresponding \"residual action\", then uses the transformer to map each observation to a categorical distribution over k discrete action bins with Focal loss [241]:\nLfocal(pt) = \u2212(1\u2212 pt)\u03b3 log(pt), (32)\nand an extra head to predict the offset with a loss akin to the masked multi-task loss [242]:\nMT-Loss(a, (\u3008a\u0302(j)i \u3009) k j=1) = k\u2211 j=1 I[bac = j] \u00b7 ||\u3008a\u3009 \u2212 \u3008a\u0302(j)\u3009||22,\n(33) where the I[] denotes the Iverson bracket, ensuring the loss is only incurred from the ground truth class of action a. Experiments on CARLA show that BeT is able to cover all the modes of demonstration data."
            ],
            "citingPaper": {
                "paperId": "e3d833eeae571fb9f269206634863c1f33cae6e3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-14164",
                    "DOI": "10.48550/arXiv.2212.14164",
                    "CorpusId": 255340560
                },
                "corpusId": 255340560,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e3d833eeae571fb9f269206634863c1f33cae6e3",
                "title": "On Transforming Reinforcement Learning by Transformer: The Development Trajectory",
                "abstract": "\u2014Transformer, originally devised for natural language processing, has also attested signi\ufb01cant success in computer vision. Thanks to its super expressive power, researchers are investigating ways to deploy transformers to reinforcement learning (RL) and the transformer-based models have manifested their potential in representative RL benchmarks. In this paper, we collect and dissect recent advances on transforming RL by transformer (transformer-based RL or TRL), in order to explore its development trajectory and future trend. We group existing developments in two categories: architecture enhancement and trajectory optimization, and examine the main applications of TRL in robotic manipulation, text-based games, navigation and autonomous driving. For architecture enhancement, these methods consider how to apply the powerful transformer structure to RL problems under the traditional RL framework, which model agents and environments much more precisely than deep RL methods, but they are still limited by the inherent defects of traditional RL algorithms, such as bootstrapping and \u201cdeadly triad\u201d. For trajectory optimization, these methods treat RL problems as sequence modeling and train a joint state-action model over entire trajectories under the behavior cloning framework, which are able to extract policies from static datasets and fully use the long-sequence modeling capability of the transformer. Given these advancements, extensions and challenges in TRL are reviewed and proposals about future direction are discussed. We hope that this survey can provide a detailed introduction to TRL and motivate future research in this rapidly developing \ufb01eld.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2176837980",
                        "name": "Shengchao Hu"
                    },
                    {
                        "authorId": "2144035454",
                        "name": "Li Shen"
                    },
                    {
                        "authorId": "2910574",
                        "name": "Ya Zhang"
                    },
                    {
                        "authorId": "2193529485",
                        "name": "Yixin Chen"
                    },
                    {
                        "authorId": "2135519749",
                        "name": "Dacheng Tao"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                ", 2022), robot learning and behavior cloning from noisy demonstrations (Shafiullah et al., 2022; Cui et al., 2022), and language-conditioned RL (Guhur et al."
            ],
            "citingPaper": {
                "paperId": "1fa589c76e14492dca7a544d58628c2a4dc55264",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-13431",
                    "DOI": "10.48550/arXiv.2210.13431",
                    "CorpusId": 253098249
                },
                "corpusId": 253098249,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1fa589c76e14492dca7a544d58628c2a4dc55264",
                "title": "Instruction-Following Agents with Jointly Pre-Trained Vision-Language Models",
                "abstract": "Humans are excellent at understanding language and vision to accomplish a wide range of tasks. In contrast, creating general instruction-following embodied agents remains a dif\ufb01cult challenge. Prior work that uses pure language-only models lack visual grounding, making it dif\ufb01cult to connect language instructions with visual observations. On the other hand, methods that use pre-trained vision-language models typically come with divided language and visual representations, requiring designing specialized network architecture to fuse them together. We propose a simple yet effective model for robots to solve instruction-following tasks in vision-based environments. Our Instruct RL method consists of a multimodal transformer that encodes visual observations and language instructions, and a policy transformer that predicts actions based on encoded representations. The multimodal transformer is pre-trained on millions of image-text pairs and natural language text, thereby producing generic cross-modal representations of observations and instructions. The policy transformer keeps track of the full history of observations and actions, and predicts actions autoregressively. We show that this uni\ufb01ed transformer model outperforms all state-of-the-art pre-trained or trained-from-scratch methods in both single-task and multi-task settings. Our model also shows better model scalability and generalization ability than prior work 1 . outperforms CLIP-RL in most categories. These results demonstrate the transferability of jointly pre-trained vision-language models to diverse multi-task settings.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2143856672",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "87068304",
                        "name": "Lisa Lee"
                    },
                    {
                        "authorId": "3436470",
                        "name": "Kimin Lee"
                    },
                    {
                        "authorId": "1689992",
                        "name": "P. Abbeel"
                    }
                ]
            }
        },
        {
            "intents": [],
            "isInfluential": false,
            "contexts": [
                "Note that PTR outperforms all other baselines including BC (finetune), BC with more expressive policy classes (BeT (Shafiullah et al., 2022), Auto-regressive), representation learning methods (Nair et al.",
                ", 2021), as well as other methods for learning from human demonstrations, such as behavioral cloning methods with expressive policy architectures (Shafiullah et al., 2022).",
                "Prior approaches utilize such existing data by running imitation learning (IL) (Young et al., 2020; Ebert et al., 2021; Shafiullah et al., 2022) or by using representation learning (Nair et al."
            ],
            "citingPaper": {
                "paperId": "32b0dbf9e13fe75b181be910c5e31b47941327ae",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-05178",
                    "DOI": "10.48550/arXiv.2210.05178",
                    "CorpusId": 262465543
                },
                "corpusId": 262465543,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/32b0dbf9e13fe75b181be910c5e31b47941327ae",
                "title": "Pre-Training for Robots: Offline RL Enables Learning New Tasks from a Handful of Trials",
                "abstract": "We pre-training for robots (PTR), a on of\ufb02ine RL attempts to effectively learn new tasks by combining pre-training on existing robotic datasets with \ufb01ne-tuning on a new task, with",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1488785534",
                        "name": "Aviral Kumar"
                    },
                    {
                        "authorId": "2111007256",
                        "name": "Anika Singh"
                    },
                    {
                        "authorId": "27535721",
                        "name": "F. Ebert"
                    },
                    {
                        "authorId": "3800238",
                        "name": "Yanlai Yang"
                    },
                    {
                        "authorId": "46881670",
                        "name": "Chelsea Finn"
                    },
                    {
                        "authorId": "1736651",
                        "name": "S. Levine"
                    }
                ]
            }
        }
    ]
}