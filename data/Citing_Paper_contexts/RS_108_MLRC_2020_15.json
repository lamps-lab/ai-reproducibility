{
    "offset": 0,
    "data": [
        {
            "contexts": [
                "EmbedKGQA (Saxena et al., 2020) has three modules: Question Embedding Module, Knowledge Embedding Module, and Answer Selection Module; the latter selects the final answer based on the first two modules.",
                "Other models compare entity embeddings from KG with question embeddings (Saxena et al., 2020) or entity embeddings extracted from the question (Razzhigaev et al."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "26ed1957395dddb62c9d73df9bcdc65316fabedc",
                "externalIds": {
                    "ArXiv": "2310.02166",
                    "CorpusId": 263608480
                },
                "corpusId": 263608480,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/26ed1957395dddb62c9d73df9bcdc65316fabedc",
                "title": "Large Language Models Meet Knowledge Graphs to Answer Factoid Questions",
                "abstract": "Recently, it has been shown that the incorporation of structured knowledge into Large Language Models significantly improves the results for a variety of NLP tasks. In this paper, we propose a method for exploring pre-trained Text-to-Text Language Models enriched with additional information from Knowledge Graphs for answering factoid questions. More specifically, we propose an algorithm for subgraphs extraction from a Knowledge Graph based on question entities and answer candidates. Then, we procure easily interpreted information with Transformer-based models through the linearization of the extracted subgraphs. Final re-ranking of the answer candidates with the extracted information boosts Hits@1 scores of the pre-trained text-to-text language models by 4-6%.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2253458749",
                        "name": "Mikhail Salnikov"
                    },
                    {
                        "authorId": "2253468414",
                        "name": "Hai Le"
                    },
                    {
                        "authorId": "2253455445",
                        "name": "Prateek Rajput"
                    },
                    {
                        "authorId": "38650969",
                        "name": "Irina Nikishina"
                    },
                    {
                        "authorId": "2253460898",
                        "name": "Pavel Braslavski"
                    },
                    {
                        "authorId": "2253465552",
                        "name": "Valentin Malykh"
                    },
                    {
                        "authorId": "2253459158",
                        "name": "Alexander Panchenko"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b47e96762351b2dbf7e863ece4640df6194bcc0c",
                "externalIds": {
                    "ArXiv": "2310.01061",
                    "CorpusId": 263605944
                },
                "corpusId": 263605944,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b47e96762351b2dbf7e863ece4640df6194bcc0c",
                "title": "Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning",
                "abstract": "Large language models (LLMs) have demonstrated impressive reasoning abilities in complex tasks. However, they lack up-to-date knowledge and experience hallucinations during reasoning, which can lead to incorrect reasoning processes and diminish their performance and trustworthiness. Knowledge graphs (KGs), which capture vast amounts of facts in a structured format, offer a reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM reasoning methods only treat KGs as factual knowledge bases and overlook the importance of their structural information for reasoning. In this paper, we propose a novel method called reasoning on graphs (RoG) that synergizes LLMs with KGs to enable faithful and interpretable reasoning. Specifically, we present a planning-retrieval-reasoning framework, where RoG first generates relation paths grounded by KGs as faithful plans. These plans are then used to retrieve valid reasoning paths from the KGs for LLMs to conduct faithful reasoning. Furthermore, RoG not only distills knowledge from KGs to improve the reasoning ability of LLMs through training but also allows seamless integration with any arbitrary LLMs during inference. Extensive experiments on two benchmark KGQA datasets demonstrate that RoG achieves state-of-the-art performance on KG reasoning tasks and generates faithful and interpretable reasoning results.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2238130759",
                        "name": "Linhao Luo"
                    },
                    {
                        "authorId": "2256011160",
                        "name": "Yuan-Fang Li"
                    },
                    {
                        "authorId": "2561045",
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "authorId": "2254047333",
                        "name": "Shirui Pan"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9d2165b57a6fa0c82e3e5dba864b098ef56ad8bf",
                "externalIds": {
                    "DOI": "10.1016/j.compag.2023.108180",
                    "CorpusId": 261590633
                },
                "corpusId": 261590633,
                "publicationVenue": {
                    "id": "80fdf70e-8520-4bb7-b387-3abebc9970b7",
                    "name": "Computers and Electronics in Agriculture",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Electron Agric"
                    ],
                    "issn": "0168-1699",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/503304/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/01681699"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9d2165b57a6fa0c82e3e5dba864b098ef56ad8bf",
                "title": "KisanQRS: A deep learning-based automated query-response system for agricultural decision-making",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2238081119",
                        "name": "Mohammad Zia Ur Rehman"
                    },
                    {
                        "authorId": "2238288711",
                        "name": "Devraj Raghuvanshi"
                    },
                    {
                        "authorId": "2238143338",
                        "name": "Nagendra Kumar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Such retrievaland-reasoning approaches aim to reduce the search space, and have proven their superiority over directly reasoning on the whole knowledge graph (Chen et al., 2019; Saxena et al., 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3decb3626224e58f32453c74ccbc17bd54b32823",
                "externalIds": {
                    "ArXiv": "2309.16540",
                    "CorpusId": 263142055
                },
                "corpusId": 263142055,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3decb3626224e58f32453c74ccbc17bd54b32823",
                "title": "Unsupervised Fact Verification by Language Model Distillation",
                "abstract": "Unsupervised fact verification aims to verify a claim using evidence from a trustworthy knowledge base without any kind of data annotation. To address this challenge, algorithms must produce features for every claim that are both semantically meaningful, and compact enough to find a semantic alignment with the source information. In contrast to previous work, which tackled the alignment problem by learning over annotated corpora of claims and their corresponding labels, we propose SFAVEL (Self-supervised Fact Verification via Language Model Distillation), a novel unsupervised framework that leverages pre-trained language models to distil self-supervised features into high-quality claim-fact alignments without the need for annotations. This is enabled by a novel contrastive loss function that encourages features to attain high-quality claim and evidence alignments whilst preserving the semantic relationships across the corpora. Notably, we present results that achieve a new state-of-the-art on the standard FEVER fact verification benchmark (+8% accuracy) with linear evaluation.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "41021385",
                        "name": "A. Bazaga"
                    },
                    {
                        "authorId": "2248203742",
                        "name": "Pietro Lio"
                    },
                    {
                        "authorId": "2297068",
                        "name": "G. Micklem"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following [30], we prune the KG to contain only relations mentioned in the questions and the triples within 2 hops of mentioned entities."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "30f0abb793772c15f2cdfec97c994685348177c1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2309-11206",
                    "ArXiv": "2309.11206",
                    "DOI": "10.48550/arXiv.2309.11206",
                    "CorpusId": 262054223
                },
                "corpusId": 262054223,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/30f0abb793772c15f2cdfec97c994685348177c1",
                "title": "Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering",
                "abstract": "Despite their competitive performance on knowledge-intensive tasks, large language models (LLMs) still have limitations in memorizing all world knowledge especially long tail knowledge. In this paper, we study the KG-augmented language model approach for solving the knowledge graph question answering (KGQA) task that requires rich world knowledge. Existing work has shown that retrieving KG knowledge to enhance LLMs prompting can significantly improve LLMs performance in KGQA. However, their approaches lack a well-formed verbalization of KG knowledge, i.e., they ignore the gap between KG representations and textual representations. To this end, we propose an answer-sensitive KG-to-Text approach that can transform KG knowledge into well-textualized statements most informative for KGQA. Based on this approach, we propose a KG-to-Text enhanced LLMs framework for solving the KGQA task. Experiments on several KGQA benchmarks show that the proposed KG-to-Text augmented LLMs approach outperforms previous KG-augmented LLMs approaches regarding answer accuracy and usefulness of knowledge statements.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "50118263",
                        "name": "Yike Wu"
                    },
                    {
                        "authorId": "2056375411",
                        "name": "Nan Hu"
                    },
                    {
                        "authorId": "2065970582",
                        "name": "Sheng Bi"
                    },
                    {
                        "authorId": "1730054",
                        "name": "G. Qi"
                    },
                    {
                        "authorId": "143702207",
                        "name": "J. Ren"
                    },
                    {
                        "authorId": "2242906489",
                        "name": "Anhuan Xie"
                    },
                    {
                        "authorId": "2243575849",
                        "name": "Wei Song"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ea07b65d2032b070886ffd6f6a9a7ee87a510b1e",
                "externalIds": {
                    "DBLP": "conf/ht/GounakisMT23",
                    "DOI": "10.1145/3603163.3609067",
                    "CorpusId": 261512086
                },
                "corpusId": 261512086,
                "publicationVenue": {
                    "id": "ec4efbef-0722-428c-bdfa-603ece1d0829",
                    "name": "ACM Conference on Hypertext & Social Media",
                    "type": "conference",
                    "alternate_names": [
                        "HT",
                        "ACM Conf Hypertext  Soc Media"
                    ],
                    "url": "http://www.acm.org/sigweb/"
                },
                "url": "https://www.semanticscholar.org/paper/ea07b65d2032b070886ffd6f6a9a7ee87a510b1e",
                "title": "Evaluating a Radius-based Pipeline for Question Answering over Cultural (CIDOC-CRM based) Knowledge Graphs",
                "abstract": "CIDOC-CRM is an event-based international standard for cultural documentation that has been widely used for offering semantic interoperability in the Cultural Heritage (CH) domain. Although there are several Knowledge Graphs (KGs) expressed by using CIDOC-CRM, the task of Question Answering (QA) has not been studied over such graphs. For this reason, in this paper we propose and evaluate a Radius-based QA pipeline over CIDOC-CRM KGs for single-entity factoid questions. In particular, we propose a generic QA pipeline that comprises several models and methods, including a keyword search model for recognizing the entity of the question (and linking it to the KG), methods that are based on path expansion for constructing subgraphs of different radius (i.e., path lengths) starting from the recognized entity, i.e., for being used as a context, and pre-trained neural models (based on BERT) for answering the question using the mentioned context. Moreover, since there are no available benchmarks over CIDOC-CRM KGs, we construct (by using a real KG) an evaluation benchmark having 10,000 questions, i.e., 5,000 single-entity factoid, 2,500 comparative and 2,500 confirmation questions. For evaluating the QA pipeline, we use the 5,000 single-entity factoid questions. Concerning the results, the QA pipeline achieves satisfactory results both in the entity recognition step (78% accuracy) and in the QA process (51% F1 score).",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2237630436",
                        "name": "Nikos Gounakis"
                    },
                    {
                        "authorId": "2081511",
                        "name": "M. Mountantonakis"
                    },
                    {
                        "authorId": "1801959",
                        "name": "Yannis Tzitzikas"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e3e9cd6b550ef2ce61110f1ea2fffce131106616",
                "externalIds": {
                    "DBLP": "journals/kbs/Cui0HH023",
                    "DOI": "10.1016/j.knosys.2023.110760",
                    "CorpusId": 259616318
                },
                "corpusId": 259616318,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e3e9cd6b550ef2ce61110f1ea2fffce131106616",
                "title": "Path-based multi-hop reasoning over knowledge graph for answering questions via adversarial reinforcement learning",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2093917610",
                        "name": "Hai Cui"
                    },
                    {
                        "authorId": "2068928996",
                        "name": "Tao Peng"
                    },
                    {
                        "authorId": "2093920100",
                        "name": "Ridong Han"
                    },
                    {
                        "authorId": "2111758731",
                        "name": "Jiayu Han"
                    },
                    {
                        "authorId": "2118467569",
                        "name": "Lu Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "98eadc68ea2c1cf762cc657fef68d675a50d856c",
                "externalIds": {
                    "DBLP": "journals/tamd/ZhangLZXYC23",
                    "DOI": "10.1109/TCDS.2022.3198272",
                    "CorpusId": 251708766
                },
                "corpusId": 251708766,
                "publicationVenue": {
                    "id": "f35f148a-0a3c-45db-b610-3d89e09ddf21",
                    "name": "IEEE Transactions on Cognitive and Developmental Systems",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Cogn Dev Syst"
                    ],
                    "issn": "2379-8920",
                    "url": "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7274989"
                },
                "url": "https://www.semanticscholar.org/paper/98eadc68ea2c1cf762cc657fef68d675a50d856c",
                "title": "Query Path Generation via Bidirectional Reasoning for Multihop Question Answering From Knowledge Bases",
                "abstract": "Multihop question answering from knowledge bases (KBQA) is a hot research topic in natural language processing. Recently, the graph neural network-based (GNN-based) methods have achieved promising results as the KB can be organized as a knowledge graph (KG). However, they often suffered from the sparsity of the KG which was detrimental to the structure encoding and reasoning capabilities of GNN. Specifically, a KG is a sparse graph linked by directed relations and previous studies have paid scant attention to the directional characteristic of relations in the KG, limiting the patterns of relation path that GNN-based approaches could resolve. This study proposes a bidirectional recurrent GNN (BRGNN) to tackle these difficulties. To model the bidirectional information of relations, all adjacent relations of an entity are grouped by their directions, and they are separately aggregated into the entity representation in outward and inward directions. For the reasoning process, BRGNN simultaneously considers the neighbor relations in both directions to cover more patterns of relation paths and improve the recall of answers. Extensive experiments on three benchmarks: WebQuestionsSP, ComplexWebQuestions, and MetaQA, verify that BRGNN can answer more questions by taking into account the directional information, and it is competitive to all state-of-the-art approaches.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2181035097",
                        "name": "Geng Zhang"
                    },
                    {
                        "authorId": "2108457408",
                        "name": "Jin Liu"
                    },
                    {
                        "authorId": "143652253",
                        "name": "Guangyou Zhou"
                    },
                    {
                        "authorId": "3408236",
                        "name": "Zhiwen Xie"
                    },
                    {
                        "authorId": "144922494",
                        "name": "Xiao Yu"
                    },
                    {
                        "authorId": "2149528254",
                        "name": "Xiaohui Cui"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cc36df656947ff7bd3c8028770bfbf031860e6af",
                "externalIds": {
                    "DOI": "10.1016/j.comnet.2023.110014",
                    "CorpusId": 261634548
                },
                "corpusId": 261634548,
                "publicationVenue": {
                    "id": "f8b7e518-b406-437e-84da-b0948622c1c9",
                    "name": "Computer Networks",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Netw",
                        "Comput  Netw",
                        "Computer & Network"
                    ],
                    "issn": "1389-1286",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/505606/description#description",
                    "alternate_urls": [
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/505606/description",
                        "http://www.sciencedirect.com/science/journal/13891286"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/cc36df656947ff7bd3c8028770bfbf031860e6af",
                "title": "Prompt-WNQA: A prompt-based complex question answering for wireless network over knowledge graph",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2238992320",
                        "name": "Pei Liu"
                    },
                    {
                        "authorId": "2238721022",
                        "name": "Bing Qian"
                    },
                    {
                        "authorId": "2239070428",
                        "name": "Qi Sun"
                    },
                    {
                        "authorId": "2238911959",
                        "name": "Longgang Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Currently, knowledge graphs play an important role in many artificial intelligence tasks, such as entity recognition [8], semantic parsing, text classification, document summarization, subject indexing, intelligent recommendation [9][10], information extraction [11], and knowledge question answering [12]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cecf1f72ba62c4fc1f2c4b8c7fa12281682057db",
                "externalIds": {
                    "DOI": "10.14733/cadaps.2024.s7.256-269",
                    "CorpusId": 261481950
                },
                "corpusId": 261481950,
                "publicationVenue": {
                    "id": "60c64cae-4b62-4053-99b2-d374d22ae9e7",
                    "name": "Computer-Aided Design and Applications",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Des Appl",
                        "Computer-aided Design and Applications"
                    ],
                    "issn": "1686-4360",
                    "url": "http://www.cadanda.com/",
                    "alternate_urls": [
                        "http://www.tandfonline.com/loi/tcad20"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/cecf1f72ba62c4fc1f2c4b8c7fa12281682057db",
                "title": "Knowledge Graph Representation Learning based on Entity Semantic Distance Classification",
                "abstract": ". In recent years, significant progress has been made in knowledge graph representation learning, which has shown promising results in knowledge computing applications such as relation extraction and knowledge reasoning. However, the unbalanced distribution of relations and entities in knowledge graphs leads to low training efficiency. To address this issue, this paper proposes a novel knowledge representation learning method based on entity distance classification. This method classifies entities based on their semantic distance on a specific relational plane, and employs different training strategies to increase the training opportunities for entities with low semantic distance differentiation. Moreover, the loss function is adjusted by introducing different residual weights, which allows for the allocation of different training opportunities to negative samples. The effectiveness of our approach is demonstrated by comparing it with mainstream knowledge representation models on various benchmark datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2237440632",
                        "name": "Yi Zhang"
                    },
                    {
                        "authorId": "3237325",
                        "name": "Wanhua Cao"
                    },
                    {
                        "authorId": "2157144739",
                        "name": "Juntao Liu"
                    },
                    {
                        "authorId": "2237409236",
                        "name": "Yuanbin Wang"
                    },
                    {
                        "authorId": "2134887850",
                        "name": "Ziyun Rao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b2c0327ab007aa25a78b04df25783a15d9d94d57",
                "externalIds": {
                    "DOI": "10.1016/j.knosys.2023.110996",
                    "CorpusId": 262026782
                },
                "corpusId": 262026782,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b2c0327ab007aa25a78b04df25783a15d9d94d57",
                "title": "A contrastive framework for enhancing Knowledge Graph Question Answering: Alleviating exposure bias",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2105634495",
                        "name": "Huifang Du"
                    },
                    {
                        "authorId": "2242965788",
                        "name": "Xixie Zhang"
                    },
                    {
                        "authorId": "2243853151",
                        "name": "Meng Wang"
                    },
                    {
                        "authorId": "2144861665",
                        "name": "Yunwen Chen"
                    },
                    {
                        "authorId": "2082352772",
                        "name": "Daqi Ji"
                    },
                    {
                        "authorId": "2243249350",
                        "name": "Jun Ma"
                    },
                    {
                        "authorId": "2256769434",
                        "name": "Haofen Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We mainly compare SKP with multiple state-of-the-art baselines that use embedding-based method for KBQA as follow: GraftNet [31] , PullNet [30] , Bert-KBQA [34] , EmbedKGQA [28] , NSM [14] , SR-NSM [43] , EmQL [29] , KGT5 [27] , CBRSUBG [7] , UniK-QA [7] , DeCAF [39] , DPR [19]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a67d3b32cf8c35dd6b19beff096a712ff19edac1",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-14436",
                    "ArXiv": "2308.14436",
                    "DOI": "10.48550/arXiv.2308.14436",
                    "CorpusId": 261245623
                },
                "corpusId": 261245623,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a67d3b32cf8c35dd6b19beff096a712ff19edac1",
                "title": "Bridging the KB-Text Gap: Leveraging Structured Knowledge-aware Pre-training for KBQA",
                "abstract": "Knowledge Base Question Answering (KBQA) aims to answer natural language questions with factual information such as entities and relations in KBs. However, traditional Pre-trained Language Models (PLMs) are directly pre-trained on large-scale natural language corpus, which poses challenges for them in understanding and representing complex subgraphs in structured KBs. To bridge the gap between texts and structured KBs, we propose a Structured Knowledge-aware Pre-training method (SKP). In the pre-training stage, we introduce two novel structured knowledge-aware tasks, guiding the model to effectively learn the implicit relationship and better representations of complex subgraphs. In downstream KBQA task, we further design an efficient linearization strategy and an interval attention mechanism, which assist the model to better encode complex subgraphs and shield the interference of irrelevant subgraphs during reasoning respectively. Detailed experiments and analyses on WebQSP verify the effectiveness of SKP, especially the significant improvement in subgraph retrieval (+4.08% H@10).",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51490462",
                        "name": "Guanting Dong"
                    },
                    {
                        "authorId": "2127717326",
                        "name": "Rumei Li"
                    },
                    {
                        "authorId": "2592528",
                        "name": "Sirui Wang"
                    },
                    {
                        "authorId": "2108474448",
                        "name": "Yupeng Zhang"
                    },
                    {
                        "authorId": "2069503881",
                        "name": "Yunsen Xian"
                    },
                    {
                        "authorId": "1753096",
                        "name": "Weiran Xu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "23efe9b99b5f0e79d7dbd4e3bfcf1c2d8b23c1ff",
                "externalIds": {
                    "PubMedCentral": "10500657",
                    "DOI": "10.1021/acsomega.3c05114",
                    "CorpusId": 261198535,
                    "PubMed": "37720754"
                },
                "corpusId": 261198535,
                "publicationVenue": {
                    "id": "d516f81c-009b-445d-b748-c827efa137d3",
                    "name": "ACS Omega",
                    "alternate_names": [
                        "AC Omega"
                    ],
                    "issn": "2470-1343",
                    "url": "https://epub.uni-regensburg.de/36538/",
                    "alternate_urls": [
                        "https://pubs.acs.org/journal/acsodf"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/23efe9b99b5f0e79d7dbd4e3bfcf1c2d8b23c1ff",
                "title": "Marie and BERT\u2014A Knowledge Graph Embedding Based Question Answering System for Chemistry",
                "abstract": "This paper presents a novel knowledge graph question answering (KGQA) system for chemistry, which is implemented on hybrid knowledge graph embeddings, aiming to provide fact-oriented information retrieval for chemistry-related research and industrial applications. Unlike other existing designs, the system operates on multiple embedding spaces, which use various embedding methods and queries the embedding spaces in parallel. With the answers returned from multiple embedding spaces, the system leverages a score alignment model to adjust the answer scores and rerank the answers. Further, the system implements an algorithm to derive implicit multihop relations to handle the complexities of deep ontologies and improve multihop question answering. The system also implements a BERT-based bidirectional entity-linking model to enhance the robustness and accuracy of the entity-linking module. The system uses a joint numerical embedding model to efficiently handle numerical filtering questions. Further, it can invoke semantic agents to perform dynamic calculations autonomously. Finally, the KGQA system handles numerous chemical reaction mechanisms using semantic parsing supported by a Linked Data Fragment server. This paper evaluates the accuracy of each module within the KGQA system with a chemistry question data set.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "11055975",
                        "name": "Xiaochi Zhou"
                    },
                    {
                        "authorId": "2116579602",
                        "name": "Shaocong Zhang"
                    },
                    {
                        "authorId": "2233892947",
                        "name": "Mehal Agarwal"
                    },
                    {
                        "authorId": "93404234",
                        "name": "J. Akroyd"
                    },
                    {
                        "authorId": "3037931",
                        "name": "S. Mosbach"
                    },
                    {
                        "authorId": "2062455872",
                        "name": "Markus Kraft"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ec101a7eff56e2a90bc0143f734fb282be757a5f",
                "externalIds": {
                    "ArXiv": "2308.13676",
                    "DBLP": "journals/corr/abs-2308-13676",
                    "DOI": "10.48550/arXiv.2308.13676",
                    "CorpusId": 261242776
                },
                "corpusId": 261242776,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ec101a7eff56e2a90bc0143f734fb282be757a5f",
                "title": "Rethinking Language Models as Symbolic Knowledge Graphs",
                "abstract": "Symbolic knowledge graphs (KGs) play a pivotal role in knowledge-centric applications such as search, question answering and recommendation. As contemporary language models (LMs) trained on extensive textual data have gained prominence, researchers have extensively explored whether the parametric knowledge within these models can match up to that present in knowledge graphs. Various methodologies have indicated that enhancing the size of the model or the volume of training data enhances its capacity to retrieve symbolic knowledge, often with minimal or no human supervision. Despite these advancements, there is a void in comprehensively evaluating whether LMs can encompass the intricate topological and semantic attributes of KGs, attributes crucial for reasoning processes. In this work, we provide an exhaustive evaluation of language models of varying sizes and capabilities. We construct nine qualitative benchmarks that encompass a spectrum of attributes including symmetry, asymmetry, hierarchy, bidirectionality, compositionality, paths, entity-centricity, bias and ambiguity. Additionally, we propose novel evaluation metrics tailored for each of these attributes. Our extensive evaluation of various LMs shows that while these models exhibit considerable potential in recalling factual information, their ability to capture intricate topological and semantic traits of KGs remains significantly constrained. We note that our proposed evaluation metrics are more reliable in evaluating these abilities than the existing metrics. Lastly, some of our benchmarks challenge the common notion that larger LMs (e.g., GPT-4) universally outshine their smaller counterparts (e.g., BERT).",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51035076",
                        "name": "Vishwas Mruthyunjaya"
                    },
                    {
                        "authorId": "1713436",
                        "name": "Pouya Pezeshkpour"
                    },
                    {
                        "authorId": "1842532",
                        "name": "Estevam Hruschka"
                    },
                    {
                        "authorId": "29995869",
                        "name": "Nikita Bhutani"
                    }
                ]
            }
        },
        {
            "contexts": [
                "They often support intelligent systems, such as recommendation system [4], question answering [5], and text generation [6]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "391c4675c9e2e3fccfead3d6fad53a05d0ed399f",
                "externalIds": {
                    "DOI": "10.1109/ICIEA58696.2023.10241455",
                    "CorpusId": 261711982
                },
                "corpusId": 261711982,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/391c4675c9e2e3fccfead3d6fad53a05d0ed399f",
                "title": "Persona and Contextual Semantic Embeddings for Entity Alignment",
                "abstract": "Entity alignment is to identify entities from different KGs that are equivalent in the real world. The complexity of neighborhood relations and structural heterogeneity remain major obstacles to entity alignment. To tackle the challenges, our proposed PCSE utilizes three components to form a new framework that projects persona embedding of the entity on the hyperplane based on the translation model and graph attention network. It employs BERT to obtain the name and description semantics of the entity and applies a message propagation mechanism to aggregate the context information. This strategy can produce a cognitive expression of the entity, which abstracts the whole picture of it. The experimental results on three datasets demonstrate that our proposed model outperforms existing baselines.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2239723414",
                        "name": "Chang Lu"
                    },
                    {
                        "authorId": "2239405959",
                        "name": "Hongtao Zhou"
                    },
                    {
                        "authorId": "2240413388",
                        "name": "Housheng Su"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f5facba24a7d39de8a9efc994cf22b0b1b4c94ee",
                "externalIds": {
                    "DBLP": "journals/fcsc/ZhangHD24",
                    "DOI": "10.1007/s11704-022-2336-6",
                    "CorpusId": 260812695
                },
                "corpusId": 260812695,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f5facba24a7d39de8a9efc994cf22b0b1b4c94ee",
                "title": "Meta-path reasoning of knowledge graph for commonsense question answering",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108160055",
                        "name": "Miao Zhang"
                    },
                    {
                        "authorId": "2118328604",
                        "name": "Tingting He"
                    },
                    {
                        "authorId": "144964056",
                        "name": "M. Dong"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0da13d3a8b5d80c62ade5aa5be1a7beba2c06468",
                "externalIds": {
                    "ACL": "2023.inlg-main.8",
                    "ArXiv": "2308.06488",
                    "DBLP": "journals/corr/abs-2308-06488",
                    "DOI": "10.48550/arXiv.2308.06488",
                    "CorpusId": 260887250
                },
                "corpusId": 260887250,
                "publicationVenue": {
                    "id": "8648a277-d0ec-4691-9eed-399b31ff9860",
                    "name": "International Conference on Natural Language Generation",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Nat Lang Gener",
                        "INLG"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1613"
                },
                "url": "https://www.semanticscholar.org/paper/0da13d3a8b5d80c62ade5aa5be1a7beba2c06468",
                "title": "Generating Faithful Text From a Knowledge Graph with Noisy Reference Text",
                "abstract": "Knowledge Graph (KG)-to-Text generation aims at generating fluent natural-language text that accurately represents the information of a given knowledge graph. While significant progress has been made in this task by exploiting the power of pre-trained language models (PLMs) with appropriate graph structure-aware modules, existing models still fall short of generating faithful text, especially when the ground-truth natural-language text contains additional information that is not present in the graph. In this paper, we develop a KG-to-text generation model that can generate faithful natural-language text from a given graph, in the presence of noisy reference text. Our framework incorporates two core ideas: Firstly, we utilize contrastive learning to enhance the model\u2019s ability to differentiate between faithful and hallucinated information in the text, thereby encouraging the decoder to generate text that aligns with the input graph. Secondly, we empower the decoder to control the level of hallucination in the generated text by employing a controllable text generation technique. We evaluate our model\u2019s performance through the standard quantitative metrics as well as a ChatGPT-based quantitative and qualitative analysis. Our evaluation demonstrates the superior performance of our model over state-of-the-art KG-to-text models on faithfulness.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2466293",
                        "name": "Tahsina Hashem"
                    },
                    {
                        "authorId": "2154869314",
                        "name": "Weiqing Wang"
                    },
                    {
                        "authorId": "2129412",
                        "name": "D. Wijaya"
                    },
                    {
                        "authorId": "2152557920",
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "authorId": "152244300",
                        "name": "Yuan-Fang Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f02bd75d6eeec8bb3c76877a31c597a1891d37de",
                "externalIds": {
                    "ArXiv": "2308.06512",
                    "DBLP": "journals/corr/abs-2308-06512",
                    "DOI": "10.48550/arXiv.2308.06512",
                    "CorpusId": 260887839
                },
                "corpusId": 260887839,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f02bd75d6eeec8bb3c76877a31c597a1891d37de",
                "title": "HyperFormer: Enhancing Entity and Relation Interaction for Hyper-Relational Knowledge Graph Completion",
                "abstract": "Hyper-relational knowledge graphs (HKGs) extend standard knowledge graphs by associating attribute-value qualifiers to triples, which effectively represent additional fine-grained information about its associated triple. Hyper-relational knowledge graph completion (HKGC) aims at inferring unknown triples while considering its qualifiers. Most existing approaches to HKGC exploit a global-level graph structure to encode hyper-relational knowledge into the graph convolution message passing process. However, the addition of multi-hop information might bring noise into the triple prediction process. To address this problem, we propose HyperFormer, a model that considers local-level sequential information, which encodes the content of the entities, relations and qualifiers of a triple. More precisely, HyperFormer is composed of three different modules: an entity neighbor aggregator module allowing to integrate the information of the neighbors of an entity to capture different perspectives of it; a relation qualifier aggregator module to integrate hyper-relational knowledge into the corresponding relation to refine the representation of relational content; a convolution-based bidirectional interaction module based on a convolutional operation, capturing pairwise bidirectional interactions of entity-relation, entity-qualifier, and relation-qualifier. realize the depth perception of the content related to the current statement. Furthermore, we introduce a Mixture-of-Experts strategy into the feed-forward layers of HyperFormer to strengthen its representation capabilities while reducing the amount of model parameters and computation. Extensive experiments on three well-known datasets with four different conditions demonstrate HyperFormer's effectiveness. Datasets and code are available at https://github.com/zhiweihu1103/HKGC-HyperFormer.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2111297694",
                        "name": "Zhiwei Hu"
                    },
                    {
                        "authorId": "2218767257",
                        "name": "V'ictor Guti'errez-Basulto"
                    },
                    {
                        "authorId": "2164103794",
                        "name": "Zhiliang Xiang"
                    },
                    {
                        "authorId": "2160218813",
                        "name": "Ru Li"
                    },
                    {
                        "authorId": "9416872",
                        "name": "Jeff Z. Pan"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3fe510be29851b8b2414de8c7032c62e6e30f2dd",
                "externalIds": {
                    "DOI": "10.1007/s10489-023-04849-1",
                    "CorpusId": 260859204
                },
                "corpusId": 260859204,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3fe510be29851b8b2414de8c7032c62e6e30f2dd",
                "title": "Multi-hop question answering over incomplete knowledge graph with abstract conceptual evidence",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2160911411",
                        "name": "Qibo Sun"
                    },
                    {
                        "authorId": "2165165357",
                        "name": "Chunhong Zhang"
                    },
                    {
                        "authorId": "2111374382",
                        "name": "Zheng Hu"
                    },
                    {
                        "authorId": "2230028864",
                        "name": "Zhihong Jin"
                    },
                    {
                        "authorId": "119883348",
                        "name": "Jibin Yu"
                    },
                    {
                        "authorId": "2230025733",
                        "name": "Liping Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e9bab7f2340aa3b8d0c6fc4ea34c2cd147894822",
                "externalIds": {
                    "DOI": "10.1117/12.2689748",
                    "CorpusId": 260776656
                },
                "corpusId": 260776656,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e9bab7f2340aa3b8d0c6fc4ea34c2cd147894822",
                "title": "Multi-view consistency for multi-hop knowledge base question answering",
                "abstract": "The task of Knowledge Base Question Answering (KBQA) is to answer a question in natural language over a Knowledge Base. And multi-hop KBQA aims to reason over multiple hops of facts in KB to answer a complex question. Step-wised reasoning has been an important schema to solve multi-hop KBQA. But previous approaches suffer from lacking reasoning paths, causing models may answer in an incorrect way. To address the issue, we present a novel approach to enhance the KBQA model by leveraging consistency between different views of the data, with few intermediate-relation-labeled data. Previous retrieval-based methods proceeded by utilizing the data view of (question, intermediate entities, answer entities). In our method, we introduce the data view of (question, intermediate relations) and enhance the KBQA model through the consistency of different data views. Concretely, we first implement a question-to-intermediate relations(Q2R) model to obtain intermediate relations\u2019 distributions. By utilizing a pretrained text generation model, it performs well using a small part of relation-labeled data. Then we devise a map function to map distributions of intermediate entities to distributions of intermediate. Finally, a constraint that metrics the consistency between the intermediate path distributions obtained from the Q2R model and the original KBQA model is constructed to enhance the KBQA model. Experiments over three datasets of multi-hop KBQA are conducted, and the results demonstrate the effectiveness of our method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47118929",
                        "name": "Xin Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, some works [22,23] used knowledge graph embedding to deal with question answering.",
                "We conducted experiments using the WebQuestion dataset and introduced the proposed component into two existing models: EmbedKGQA [22] and TransferNet [43].",
                "Thanks to the shared utilization of knowledge graph embeddings, which enhances the models\u2019 ability to capture semantic relationships and facilitate reasoning capabilities, the introduced component exhibits enhanced effectiveness when integrated into our model and EmbedKGQA.",
                "In EmbedKGQA, we incorporated the results of the relation-learning module into the inference module.",
                "It is worth noting that both our proposed model and EmbedKGQA leverage knowledge graph embeddings."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "01a865f8199d70a9ae359aaeff9302a76423a056",
                "externalIds": {
                    "DOI": "10.3390/electronics12153363",
                    "CorpusId": 260689874
                },
                "corpusId": 260689874,
                "publicationVenue": {
                    "id": "ccd8e532-73c6-414f-bc91-271bbb2933e2",
                    "name": "Electronics",
                    "type": "journal",
                    "issn": "1450-5843",
                    "alternate_issns": [
                        "2079-9292",
                        "0883-4989"
                    ],
                    "url": "http://www.electronics.etfbl.net/",
                    "alternate_urls": [
                        "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-247562",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-247562",
                        "https://www.mdpi.com/journal/electronics"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/01a865f8199d70a9ae359aaeff9302a76423a056",
                "title": "Improving Question Answering over Knowledge Graphs with a Chunked Learning Network",
                "abstract": "The objective of knowledge graph question answering is to assist users in answering questions by utilizing the information stored within the graph. Users are not required to comprehend the underlying data structure. This is a difficult task because, on the one hand, correctly understanding the semantics of a problem is difficult for machines. On the other hand, the growing knowledge graph will inevitably lead to information retrieval errors. Specifically, the question-answering task has three difficulties: word abbreviation, object complement, and entity ambiguity. An object complement means that different entities share the same predicate, and entity ambiguity means that words have different meanings in different contexts. To solve these problems, we propose a novel method named the Chunked Learning Network. It uses different models according to different scenarios to obtain a vector representation of the topic entity and relation in the question. The answer entity representation that yields the closest fact triplet, according to a joint distance metric, is returned as the answer. For sentences with an object complement, we use dependency parsing to construct dependency relationships between words to obtain more accurate vector representations. Experiments demonstrate the effectiveness of our method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2143356770",
                        "name": "Zicheng Zuo"
                    },
                    {
                        "authorId": "144702363",
                        "name": "Zhenfang Zhu"
                    },
                    {
                        "authorId": "1587665827",
                        "name": "Wenqing Wu"
                    },
                    {
                        "authorId": "2157137336",
                        "name": "Wenling Wang"
                    },
                    {
                        "authorId": "2178346377",
                        "name": "Jiangtao Qi"
                    },
                    {
                        "authorId": "2202648393",
                        "name": "Linghui Zhong"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1c7e5b1cf26560ed0400579358dd57006fa2bd63",
                "externalIds": {
                    "DBLP": "conf/kdd/LiuT23",
                    "DOI": "10.1145/3580305.3599564",
                    "CorpusId": 260500247
                },
                "corpusId": 260500247,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/1c7e5b1cf26560ed0400579358dd57006fa2bd63",
                "title": "Knowledge Graph Reasoning and Its Applications",
                "abstract": "The use of knowledge graphs has gained significant traction in a wide variety of applications, ranging from recommender systems and question answering to fact checking. By leveraging the wealth of information contained within knowledge graphs, it is possible to greatly enhance various downstream tasks, making reasoning over knowledge graphs an area of increasing interest. However, despite its popularity, knowledge graph reasoning remains a challenging problem. The first major challenge of knowledge graph reasoning lies in the nature of knowledge graphs themselves. Most knowledge graphs are incomplete, meaning that they may not capture all the relevant knowledge required for reasoning. As a result, reasoning on incomplete knowledge graphs can be difficult. Additionally, real-world knowledge graphs often evolve over time, which presents an additional challenge. The second challenge of knowledge graph reasoning pertains to the input data. In some KG reasoning applications, users may be unfamiliar with the background knowledge graph, leading to the possibility of asking ambiguous questions that can make KG reasoning tasks more challenging. Moreover, some applications require iterative reasoning, where users ask several related questions in sequence, further increasing the complexity of the task. The third challenge of knowledge graph reasoning concerns the algorithmic aspect. Due to the varied properties of relations in knowledge graphs, such as transitivity, symmetry, and asymmetry, designing an all-round KG reasoning model that fits all these properties can be challenging. Furthermore, most KG reasoning models tend to focus on solving a specific problem, lacking the generalization ability required to apply to other tasks. This tutorial aims to comprehensively review different aspects of knowledge graph reasoning applications and highlight open challenges and future directions. It is intended to benefit researchers and practitioners in the fields of data mining, artificial intelligence, and social science.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1505803281",
                        "name": "Lihui Liu"
                    },
                    {
                        "authorId": "2058143613",
                        "name": "H. Tong"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8230be675982e6b25755cde789a329ce666b0d9a",
                "externalIds": {
                    "DOI": "10.1016/j.eswa.2023.121086",
                    "CorpusId": 260523019
                },
                "corpusId": 260523019,
                "publicationVenue": {
                    "id": "987139ae-a65d-49bb-aaf6-fb764dc40b19",
                    "name": "Expert systems with applications",
                    "type": "journal",
                    "alternate_names": [
                        "Expert syst appl",
                        "Expert Systems With Applications",
                        "Expert Syst Appl"
                    ],
                    "issn": "0957-4174",
                    "url": "https://www.journals.elsevier.com/expert-systems-with-applications/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/expert-systems-with-applications",
                        "http://www.sciencedirect.com/science/journal/09574174"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8230be675982e6b25755cde789a329ce666b0d9a",
                "title": "Incorporating global-local neighbors with Gaussian mixture embedding for few-shot knowledge graph completion",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "30219510",
                        "name": "Penghui Xie"
                    },
                    {
                        "authorId": "143652253",
                        "name": "Guangyou Zhou"
                    },
                    {
                        "authorId": "2108457408",
                        "name": "Jin Liu"
                    },
                    {
                        "authorId": "1683391",
                        "name": "J. Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2017] are employed to initialize entity and relation embeddings to help answer a question in the task of KGQA [Saxena et al., 2020].",
                ", 2020] and EmbedKGQA [Saxena et al., 2020]; and (III) temporal KG embedding-based models, including CronKGQA [Saxena et al."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4620e0d491e1fbd1b96cb9435ab950a8d9229030",
                "externalIds": {
                    "DBLP": "conf/ijcai/LiuLLGLWW0FG23",
                    "DOI": "10.24963/ijcai.2023/571",
                    "CorpusId": 260858507
                },
                "corpusId": 260858507,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/4620e0d491e1fbd1b96cb9435ab950a8d9229030",
                "title": "Local and Global: Temporal Question Answering via Information Fusion",
                "abstract": "Many models that leverage knowledge graphs (KGs) have recently demonstrated remarkable success in question answering (QA) tasks. In the real world, many facts contained in KGs are time-constrained thus temporal KGQA has received increasing attention. Despite the fruitful efforts of previous models in temporal KGQA, they still have several limitations. (I) They neither emphasize the graph structural information between entities in KGs nor explicitly utilize a multi-hop relation path through graph neural networks to enhance answer prediction. (II) They adopt pre-trained language models (LMs) to obtain question representations, focusing merely on the global information related to the question while not highlighting the local information of the entities in KGs. To address these limitations, we introduce a novel model that simultaneously explores both Local information and Global information for the task of temporal KGQA (LGQA). Specifically, we first introduce an auxiliary task in the temporal KG embedding procedure to make timestamp embeddings time-order aware. Then, we design information fusion layers that effectively incorporate local and global information to deepen question understanding. We conduct extensive experiments on two benchmarks, and LGQA significantly outperforms previous state-of-the-art models, especially in difficult questions. Moreover, LGQA can generate interpretable and trustworthy predictions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108833052",
                        "name": "Yonghao Liu"
                    },
                    {
                        "authorId": "2113983954",
                        "name": "Di Liang"
                    },
                    {
                        "authorId": "31289209",
                        "name": "Meng Li"
                    },
                    {
                        "authorId": "1720285",
                        "name": "Fausto Giunchiglia"
                    },
                    {
                        "authorId": "2108691206",
                        "name": "Ximing Li"
                    },
                    {
                        "authorId": "2592528",
                        "name": "Sirui Wang"
                    },
                    {
                        "authorId": "2144356294",
                        "name": "Wei Wu"
                    },
                    {
                        "authorId": "2230223542",
                        "name": "Lan Huang"
                    },
                    {
                        "authorId": "2190042195",
                        "name": "Xiaoyue Feng"
                    },
                    {
                        "authorId": "144479376",
                        "name": "Renchu Guan"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5134829ce8ecd73c16b21df6affeb6be964f55ac",
                "externalIds": {
                    "DOI": "10.1016/j.eswa.2023.121267",
                    "CorpusId": 261442519
                },
                "corpusId": 261442519,
                "publicationVenue": {
                    "id": "987139ae-a65d-49bb-aaf6-fb764dc40b19",
                    "name": "Expert systems with applications",
                    "type": "journal",
                    "alternate_names": [
                        "Expert syst appl",
                        "Expert Systems With Applications",
                        "Expert Syst Appl"
                    ],
                    "issn": "0957-4174",
                    "url": "https://www.journals.elsevier.com/expert-systems-with-applications/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/expert-systems-with-applications",
                        "http://www.sciencedirect.com/science/journal/09574174"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5134829ce8ecd73c16b21df6affeb6be964f55ac",
                "title": "Tensor decompositions for temporal knowledge graph completion with time perspective",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1740485640",
                        "name": "Jinfa Yang"
                    },
                    {
                        "authorId": "1738732",
                        "name": "Xianghua Ying"
                    },
                    {
                        "authorId": "6658208",
                        "name": "Yongjie Shi"
                    },
                    {
                        "authorId": "2158522878",
                        "name": "Bowei Xing"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0c429d383810e11ccc09184f6cf227520fb8ae12",
                "externalIds": {
                    "DBLP": "journals/jbi/DangPN23",
                    "DOI": "10.1016/j.jbi.2023.104460",
                    "CorpusId": 260396941,
                    "PubMed": "37532000"
                },
                "corpusId": 260396941,
                "publicationVenue": {
                    "id": "f9827422-a381-440c-a8a4-e5e50415934e",
                    "name": "Journal of Biomedical Informatics",
                    "type": "journal",
                    "alternate_names": [
                        "J Biomed Informatics"
                    ],
                    "issn": "1532-0464",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/622857/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/journal-of-biomedical-informatics/",
                        "http://www.sciencedirect.com/science/journal/15320464",
                        "http://www.journals.elsevier.com/journal-of-biomedical-informatics/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0c429d383810e11ccc09184f6cf227520fb8ae12",
                "title": "GENA: A knowledge graph for nutrition and mental health",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2066227852",
                        "name": "Linh Dang"
                    },
                    {
                        "authorId": "1520539533",
                        "name": "U. Phan"
                    },
                    {
                        "authorId": "48182862",
                        "name": "Nhung T. H. Nguyen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "1 Weuse the underlying knowledge graph as well as the QA data provided by [27].",
                "The underlying knowledge graphs were provided by [4], [9], and [27].",
                "EmbedKGQA [27] is a framework that uses two separate pre-trained models, i."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0958ffa68716d35dbda61bacd66712b07cacab42",
                "externalIds": {
                    "DBLP": "conf/sigir/AtifKD23",
                    "DOI": "10.1145/3539618.3591698",
                    "CorpusId": 259949829
                },
                "corpusId": 259949829,
                "publicationVenue": {
                    "id": "8dce23a9-44e0-4381-a39e-2acc1edff700",
                    "name": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                    "type": "conference",
                    "alternate_names": [
                        "International ACM SIGIR Conference on Research and Development in Information Retrieval",
                        "Int ACM SIGIR Conf Res Dev Inf Retr",
                        "SIGIR",
                        "Annu Int ACM SIGIR Conf Res Dev Inf Retr"
                    ],
                    "url": "http://www.acm.org/sigir/"
                },
                "url": "https://www.semanticscholar.org/paper/0958ffa68716d35dbda61bacd66712b07cacab42",
                "title": "BeamQA: Multi-hop Knowledge Graph Question Answering with Sequence-to-Sequence Prediction and Beam Search",
                "abstract": "Knowledge Graph Question Answering (KGQA) is a task that aims to answer natural language queries by extracting facts from a knowledge graph. Current state-of-the-art techniques for KGQA rely on text-based information from graph entity and relations labels, as well as external textual corpora. By reasoning over multiple edges in the graph, these can accurately rank and return the most relevant entities. However, one of the limitations of these methods is that they cannot handle the inherent incompleteness of real-world knowledge graphs and may lead to inaccurate answers due to missing edges. To address this issue, recent advances in graph representation learning have led to the development of systems that can use link prediction techniques to handle missing edges probabilistically, allowing the system to reason with incomplete information. However, existing KGQA frameworks that use such techniques often depend on learning a transformation from the query representation to the graph embedding space, which requires access to a large training dataset. We present BeamQA, an approach that overcomes these limitations by combining a sequence-to-sequence prediction model with beam search execution in the embedding space. Our model uses a pre-trained large language model and synthetic question generation. Our experiments demonstrate the effectiveness of BeamQA when compared to other KGQA methods on two knowledge graph question-answering datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1491789788",
                        "name": "Farah Atif"
                    },
                    {
                        "authorId": "2223742749",
                        "name": "Ola El Khatib"
                    },
                    {
                        "authorId": "1695677",
                        "name": "D. Difallah"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "23f9f5f9fa0dee1e82a02ca493238011806b73d7",
                "externalIds": {
                    "DOI": "10.1007/s10489-023-04588-3",
                    "CorpusId": 259921223
                },
                "corpusId": 259921223,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/23f9f5f9fa0dee1e82a02ca493238011806b73d7",
                "title": "A transformer framework for generating context-aware knowledge graph paths",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "152288639",
                        "name": "Pei-Chi Lo"
                    },
                    {
                        "authorId": "2084632198",
                        "name": "E. Lim"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These methods align document and questions to the background KB (i.e., Wikidata5M) and perform the knowledge reasoning on the background KB. EmbedKGQA (Saxena et al., 2020) converts documents and questions into vectors in the embedding space of the background KB and performs the knowledge reasoning with operations on the embedding vector, where we use ComplEx (Trouillon et al., 2016).",
                "\u2026methods align document and questions to the background KB (i.e., Wikidata5M) and perform the knowledge reasoning on the background KB. EmbedKGQA (Saxena et al., 2020) converts documents and questions into vectors in the embedding space of the background KB and performs the knowledge reasoning\u2026",
                "EmbedKGQA (Saxena et al., 2020) converts documents and questions into vectors in the embedding space of the background KB and performs the knowledge reasoning with operations on the embedding vector, where we use ComplEx (Trouillon et al."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "42b850269b3619978f65d8d78a3c7e8640b99984",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2307-03115",
                    "ArXiv": "2307.03115",
                    "DOI": "10.48550/arXiv.2307.03115",
                    "CorpusId": 259360839
                },
                "corpusId": 259360839,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/42b850269b3619978f65d8d78a3c7e8640b99984",
                "title": "KoRC: Knowledge oriented Reading Comprehension Benchmark for Deep Text Understanding",
                "abstract": "Deep text understanding, which requires the connections between a given document and prior knowledge beyond its text, has been highlighted by many benchmarks in recent years. However, these benchmarks have encountered two major limitations. On the one hand, most of them require human annotation of knowledge, which leads to limited knowledge coverage. On the other hand, they usually use choices or spans in the texts as the answers, which results in narrow answer space. To overcome these limitations, we build a new challenging benchmark named KoRc in this paper. Compared with previous benchmarks, KoRC has two advantages, i.e., broad knowledge coverage and flexible answer format. Specifically, we utilize massive knowledge bases to guide annotators or large language models (LLMs) to construct knowledgable questions. Moreover, we use labels in knowledge bases rather than spans or choices as the final answers. We test state-of-the-art models on KoRC and the experimental results show that the strongest baseline only achieves 68.3% and 30.0% F1 measure in the in-distribution and out-of-distribution test set, respectively. These results indicate that deep text understanding is still an unsolved challenge. The benchmark dataset, leaderboard, and baseline methods are released in https://github.com/THU-KEG/KoRC.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1423719712",
                        "name": "Zijun Yao"
                    },
                    {
                        "authorId": "2211723524",
                        "name": "Yan-Tie Liu"
                    },
                    {
                        "authorId": "48574888",
                        "name": "Xin Lv"
                    },
                    {
                        "authorId": "1712738522",
                        "name": "S. Cao"
                    },
                    {
                        "authorId": "2116034394",
                        "name": "Jifan Yu"
                    },
                    {
                        "authorId": "2055765060",
                        "name": "Lei Hou"
                    },
                    {
                        "authorId": "2133353675",
                        "name": "Juanzi Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cf3fbbd9817829f67dda5d19c983460ff4712de9",
                "externalIds": {
                    "DOI": "10.1007/s10489-023-04723-0",
                    "CorpusId": 259625732
                },
                "corpusId": 259625732,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cf3fbbd9817829f67dda5d19c983460ff4712de9",
                "title": "SEPAKE: a structure-enhanced and position-aware knowledge embedding framework for knowledge graph completion",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47730694",
                        "name": "Mei Yu"
                    },
                    {
                        "authorId": "1958789165",
                        "name": "Tingxu Jiang"
                    },
                    {
                        "authorId": "2155521320",
                        "name": "Jian Yu"
                    },
                    {
                        "authorId": "2466436",
                        "name": "Mankun Zhao"
                    },
                    {
                        "authorId": "2171053434",
                        "name": "Jiujiang Guo"
                    },
                    {
                        "authorId": "50367281",
                        "name": "Ming Yang"
                    },
                    {
                        "authorId": "2805420",
                        "name": "Ruiguo Yu"
                    },
                    {
                        "authorId": "50079136",
                        "name": "Xuewei Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fe5924a91f74d22aae4bc4cdf5f4dbcf692c4a3b",
                "externalIds": {
                    "DBLP": "journals/kbs/PanLZH23",
                    "DOI": "10.1016/j.knosys.2023.110787",
                    "CorpusId": 259531933
                },
                "corpusId": 259531933,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fe5924a91f74d22aae4bc4cdf5f4dbcf692c4a3b",
                "title": "Incorporating logic rules with textual representations for interpretable knowledge graph reasoning",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1452348450",
                        "name": "Yudai Pan"
                    },
                    {
                        "authorId": "2155648333",
                        "name": "Jun Liu"
                    },
                    {
                        "authorId": "2145417611",
                        "name": "Lingling Zhang"
                    },
                    {
                        "authorId": "121240779",
                        "name": "Y. Huang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "51605a6ea4d9867e9a2db10e9f2d88f20b4e9cbd",
                "externalIds": {
                    "DBLP": "journals/kbs/WangLLSLJ23",
                    "DOI": "10.1016/j.knosys.2023.110810",
                    "CorpusId": 260052267
                },
                "corpusId": 260052267,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/51605a6ea4d9867e9a2db10e9f2d88f20b4e9cbd",
                "title": "Hic-KGQA: Improving multi-hop question answering over knowledge graph via hypergraph and inference chain",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2110124127",
                        "name": "Jingchao Wang"
                    },
                    {
                        "authorId": "2124849776",
                        "name": "Weimin Li"
                    },
                    {
                        "authorId": "2213896033",
                        "name": "Fangfang Liu"
                    },
                    {
                        "authorId": "2168001382",
                        "name": "Bin Sheng"
                    },
                    {
                        "authorId": "2157220291",
                        "name": "Wei Liu"
                    },
                    {
                        "authorId": "2072796607",
                        "name": "Qun Jin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "85c45b434e97902c179826f7b91d8095a95c751e",
                "externalIds": {
                    "DOI": "10.1016/j.knosys.2023.110820",
                    "CorpusId": 260148425
                },
                "corpusId": 260148425,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/85c45b434e97902c179826f7b91d8095a95c751e",
                "title": "Predict, pretrained, select and answer: Interpretable and scalable complex question answering over knowledge bases",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2149215129",
                        "name": "Xing Cao"
                    },
                    {
                        "authorId": "2118113853",
                        "name": "Yun Liu"
                    },
                    {
                        "authorId": "2075375889",
                        "name": "Feng Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Saxena A[2] and others use KG embedding to predict the links in the knowledge map, embed the entities and problems in the knowledge map into vectors, and select the triple with the highest score according to the scoring function when reasoning."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5d0ac66787e42f8da277b266cef22aabbdc5ddad",
                "externalIds": {
                    "DOI": "10.1117/12.2683290",
                    "CorpusId": 259263056
                },
                "corpusId": 259263056,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/5d0ac66787e42f8da277b266cef22aabbdc5ddad",
                "title": "Logical reasoning over knowledge graphs in vector space",
                "abstract": "Due to the incompleteness of knowledge map, the existing knowledge map is usually missing, and the task of knowledge reasoning is to predict the missing facts according to a large number of information such as entities and relationships. In order to satisfy the interpretability and accuracy, this paper proposes a multi-hop reasoning method combining first-order logic and graph embedded vector, which has the mathematical rigor of logical reasoning and the high accuracy of embedded vector. For the specified query, it is transformed into a first-order logical operation representation, and the query dependency graph is instantiated, and the reverse sampling from the root node to the anchor point in the instantiation process ensures that the query must have an answer. According to the query calculation diagram combined with logical operators, reasoning is based on embedded method, which avoids explicitly traversing KG and obtains the query answer.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2143720046",
                        "name": "Jin Wang"
                    },
                    {
                        "authorId": "7524277",
                        "name": "Yongzhong Huang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b9457dfeb95fba0bf2e081dbde50f971cbfb5d7e",
                "externalIds": {
                    "DOI": "10.1117/12.2684740",
                    "CorpusId": 259749070
                },
                "corpusId": 259749070,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b9457dfeb95fba0bf2e081dbde50f971cbfb5d7e",
                "title": "Multi-hop knowledge base question answering by case retrieval",
                "abstract": "Multi-hop Knowledge Base Question Answering (KBQA) aims to find the answer entities in a Knowledge Base (KB) that are multiple hops away from the topic entities mentioned in a question. The key challenge of multi-hop KBQA is the significant semantic gap between Natural Language (NL) and the relation paths in KB. Recently, the Case-Based Reasoning (CBR) approach has been successfully applied to KBQA, which first retrieves other similar questions from the training set and then referencing their solutions (e.g., reasoning path) to derive a solution for given question. However, existing CBR-based methods are hard to provide correct solutions while the gold reasoning paths are unavailable. Moreover, they are limited to the assumption that the reasoning path required to answer a question is repeats for similar questions, thus suffer from the generalization issue. To address above problems, we propose a novel framework, which consider both the semantic matching between NL question and KB paths, as well as the matching between KB paths of similar question and given question, to alleviate the inconsistency between NL and KB. Specifically, it uses above two matching mechanisms to generate more accurate solutions for training questions and then reuses it to rank the candidate reasoning paths of given question. To cope with the generalization issue mentioned above, our method adopts different ranking algorithm according to different NL similarity. Experiments over two benchmarks demonstrate the effectiveness of our approach, which achieves state of the art on MetaQA and new state of the art results on PathQuestion-Large.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2144127740",
                        "name": "Jingcheng Liu"
                    },
                    {
                        "authorId": "2155393420",
                        "name": "Ting Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some researches [12], [13], [15]\u2013[17] on the mutual attention between questions and candidate answers are considered as the potential solution for this issue."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "21c85e3018061f23116ddf66c848c0cdeb8dbff0",
                "externalIds": {
                    "DBLP": "conf/ijcnn/LuoSXWW23",
                    "DOI": "10.1109/IJCNN54540.2023.10191339",
                    "CorpusId": 260386517
                },
                "corpusId": 260386517,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/21c85e3018061f23116ddf66c848c0cdeb8dbff0",
                "title": "Improving Complex Knowledge Base Question Answering with Relation-Aware Subgraph Retrieval and Reasoning Network",
                "abstract": "Complex Knowledge Base Question Answering aims to answer a complex question over a Knowledge Base. A mainstream solution is based on information retrieval, which usually extracts a pivotal subgraph from entire Knowledge Base to locate candidate answers, and then determines the plausible answers with semantic matching between candidate answers and the question. However, such a paradigm can have two critical problems: 1) Complex Knowledge Base Question Answering can be sensitive to the subgraph, since a small subgraph may exclude the answers, while a large one may introduce a lot of noise; 2) directly deriving answers with semantic matching neglects the global topology in the Knowledge Base, which may limit the capability in answer reasoning. To tackle above challenges, we propose the Relation-Aware Subgraph Retrieval and Reasoning Network, where relations are emphasized to construct subgraphs and answer reasoning. Specifically, we present a Relation-Aware Subgraph Retrieval (RASR) method to initialize and prune subgraphs with the guidance of relation semantics. To compre-hensively understand the complex correlations between the question and candidate answers, we put forward a Relation-Aware Reasoning Network (RARN), which contains a text reasoning module focusing on the semantics understanding of the question and a graph reasoning module focusing on mining the topology between the topic entities and the answers. Experiments on two classical benchmark datasets show that our reasoning model outperforms the state-of-the-art results of Information Retrieval models. What's more, data statistical analysis on the subgraphs demonstrates the effectiveness of our proposed subgraph retrieval method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2061548489",
                        "name": "Dan Luo"
                    },
                    {
                        "authorId": "2054250919",
                        "name": "Jiawei Sheng"
                    },
                    {
                        "authorId": "46485352",
                        "name": "Hongbo Xu"
                    },
                    {
                        "authorId": "2108740151",
                        "name": "Lihong Wang"
                    },
                    {
                        "authorId": "1690603",
                        "name": "Bin Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Large-scale knowledge graphs (KGs) such as YAGO [1], Wikidata [2] and WordNet [3] are valuable resources for many natural language processing applications including recommendation system [4], [5], information extraction [6], [7], question answering [8], [9] and so on."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "24f30f0cb83af01b480a907afae889ce6ee79012",
                "externalIds": {
                    "DBLP": "conf/ijcnn/LiYLZ23",
                    "DOI": "10.1109/IJCNN54540.2023.10191162",
                    "CorpusId": 260386195
                },
                "corpusId": 260386195,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/24f30f0cb83af01b480a907afae889ce6ee79012",
                "title": "TransD-based Multi-hop Meta Learning for Few-shot Knowledge Graph Completion",
                "abstract": "Few-shot knowledge graph completion (FKGC), which aims to infer missing facts about a relation from only a few reference triples, has recently attracted great attention. The core of solving the FKGC task is to learn a vector representation for each few-shot relation using the corresponding entity represen-tations. To this end, existing models generally enhance entity representations with their direct neighbors. However, a large number of entities have few direct neighbors. Hence, encoding only direct neighborhood is insufficient to obtain satisfactory en-tity representations. In addition, current models typically utilize static embeddings to represent entities, ignoring their diverse semantics, i.e., an entity may show distinct semantics within different few-shot relations. To address these issues, we propose a new FKGC framework, namely TransD-based Multi-hop Meta Learning (TDML). TDML consists of three main components: a multi-hop neighbor encoder to enhance entity representations by aggregating heterogeneous multi-hop neighbors, a transformer encoder to generate the relation meta representations, and a TransD-based relation representation updater that allows each entity to exhibit relation-specific semantics and tune the relation meta representations. Extensive experiments on two public datasets demonstrate that our model outperforms state-of-the-art FKGC methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2226450667",
                        "name": "Jindi Li"
                    },
                    {
                        "authorId": "2088157995",
                        "name": "Kui Yu"
                    },
                    {
                        "authorId": "2111353835",
                        "name": "Yuling Li"
                    },
                    {
                        "authorId": "2145061788",
                        "name": "Yuhong Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "representations play a critical role in downstream tasks such as question answering [27] and recommendation systems [28]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ad509c66120acd88faae5c17bb907ca1a3b7d8fa",
                "externalIds": {
                    "DBLP": "conf/ijcnn/YuFZZZYY23",
                    "DOI": "10.1109/IJCNN54540.2023.10191061",
                    "CorpusId": 260385216
                },
                "corpusId": 260385216,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/ad509c66120acd88faae5c17bb907ca1a3b7d8fa",
                "title": "MASZSL: A Multi-Block Attention-Based Description Generative Adversarial Network for Knowledge Graph Zero-Shot Relational Learning",
                "abstract": "In the real world, the Knowledge Graph(KG) is dynamic and new entities are added at any time. Therefore, open-world Knowledge Graph Completion(KGC) was proposed to approach new-added entities, but previous approaches often introduced too much noise when introducing external text resources for new entities. To alleviate this problem, knowledge graph zero-shot relational learning (KGZSL) has recently attracted more attention. Generative Adversarial Networks (GANs) are frequently used in KGZSL to connect existing relation descriptions to the domain of knowledge graphs. However, these methods ignore the impact of existing entities on embeddings for unidentified relational representations and in-stead concentrate on examining the connection between relational textual texts and knowledge network structures. In this work, we propose a multi-block attention framework using relation Description Generative Adversarial Networks (desGAN) jointing KG and text representation and address model collapse and training stability problems in previous studies. The core idea of our method is to obtain the background knowledge graph information and the relation representation through and multi-block attention layer and the desGAN, then a connection between the structured KG semantic space and the unstructured text semantic space of the new entity is established, forcing the entity pair to be closer to their real relation. Experimental results on the knowledge graph zeroshot relational learning dataset demonstrate that our MASZSL has a faster convergence speed and achieves state-of-the-art performance on this task.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "47730694",
                        "name": "Mei Yu"
                    },
                    {
                        "authorId": "2226521665",
                        "name": "Pengtao Fan"
                    },
                    {
                        "authorId": "2466436",
                        "name": "Mankun Zhao"
                    },
                    {
                        "authorId": "2108186393",
                        "name": "Wenbin Zhang"
                    },
                    {
                        "authorId": "152621421",
                        "name": "Yue Zhao"
                    },
                    {
                        "authorId": "50367281",
                        "name": "Ming Yang"
                    },
                    {
                        "authorId": "2155521320",
                        "name": "Jian Yu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8fcf6a522f516b42469d57f343b596e7dbd9755d",
                "externalIds": {
                    "DBLP": "conf/ijcnn/WenSLLFW23",
                    "DOI": "10.1109/IJCNN54540.2023.10191152",
                    "CorpusId": 260387389
                },
                "corpusId": 260387389,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/8fcf6a522f516b42469d57f343b596e7dbd9755d",
                "title": "Schema Item Matters in Knowledge Base Question Answering",
                "abstract": "Knowledge base question answering is a challenging task that aims to answer questions by querying knowledge bases. Recently, state-of-the-art methods tend to include an enumerator module and a ranker module. They first enumerate by searching the knowledge base and then rank the candidates to select the target logical form. However, these methods sometimes fail to cover the candidates which involve more complex combinations. A recent solution to this issue is to add a generator module after the ranker to generate the uncovered target logical form. However, the enumerator and ranker always discard partial ground truth schema items. Consequently, the lack of them in the generator input results in the failure to generate the target logical form. To address this problem, we present a novel framework, SIMQA, to reuse the neglected schema items, i.e., classes and relations. Specifically, we adopt a matcher module to select the most related schema items for the given question, and feed them to the generator. On this basis, we propose a novel generation model based on contrastive learning to force the model to focus on the supplemental schema items. Experiment results on GRAILQA and WEBQSP datasets demonstrate the highly competitive performance of the proposed method, and verify that schema item matters in KBQA.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2226445226",
                        "name": "Zhe Wen"
                    },
                    {
                        "authorId": "84109730",
                        "name": "Q. Si"
                    },
                    {
                        "authorId": "144981131",
                        "name": "Zheng Lin"
                    },
                    {
                        "authorId": "2145497933",
                        "name": "Hua Liu"
                    },
                    {
                        "authorId": "143655088",
                        "name": "Peng Fu"
                    },
                    {
                        "authorId": "2154491752",
                        "name": "Weiping Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Question answering in knowledge graphs requires reasoning over multiple edges to arrive at the right answer and should have a tractable path to explain the new behavior [4,5]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7b9ce347f9e5df4fb9546d59bb524a1a3c0578a1",
                "externalIds": {
                    "DBLP": "journals/information/AgrawalB023",
                    "DOI": "10.3390/info14060336",
                    "CorpusId": 259236810
                },
                "corpusId": 259236810,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7b9ce347f9e5df4fb9546d59bb524a1a3c0578a1",
                "title": "Auction-Based Learning for Question Answering over Knowledge Graphs",
                "abstract": "Knowledge graphs are graph-based data models which can represent real-time data that is constantly growing with the addition of new information. The question-answering systems over knowledge graphs (KGQA) retrieve answers to a natural language question from the knowledge graph. Most existing KGQA systems use static knowledge bases for offline training. After deployment, they fail to learn from unseen new entities added to the graph. There is a need for dynamic algorithms which can adapt to the evolving graphs and give interpretable results. In this research work, we propose using new auction algorithms for question answering over knowledge graphs. These algorithms can adapt to changing environments in real-time, making them suitable for offline and online training. An auction algorithm computes paths connecting an origin node to one or more destination nodes in a directed graph and uses node prices to guide the search for the path. The prices are initially assigned arbitrarily and updated dynamically based on defined rules. The algorithm navigates the graph from the high-price to the low-price nodes. When new nodes and edges are dynamically added or removed in an evolving knowledge graph, the algorithm can adapt by reusing the prices of existing nodes and assigning arbitrary prices to the new nodes. For subsequent related searches, the \u201clearned\u201d prices provide the means to \u201ctransfer knowledge\u201d and act as a \u201cguide\u201d: to steer it toward the lower-priced nodes. Our approach reduces the search computational effort by 60% in our experiments, thus making the algorithm computationally efficient. The resulting path given by the algorithm can be mapped to the attributes of entities and relations in knowledge graphs to provide an explainable answer to the query. We discuss some applications for which our method can be used.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145363324",
                        "name": "Garima Agrawal"
                    },
                    {
                        "authorId": "1786249",
                        "name": "D. Bertsekas"
                    },
                    {
                        "authorId": "2155337763",
                        "name": "Huan Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Knowledge graph question answering (KGQA) aims to find answers to natural language questions based on the structured facts stored in knowledge graphs [237], [238]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9e8b7b0d4c628c12b6a65ab56ac5f33a35eff2e6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-08302",
                    "ArXiv": "2306.08302",
                    "DOI": "10.48550/arXiv.2306.08302",
                    "CorpusId": 259165563
                },
                "corpusId": 259165563,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9e8b7b0d4c628c12b6a65ab56ac5f33a35eff2e6",
                "title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
                "abstract": "Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2153326034",
                        "name": "Shirui Pan"
                    },
                    {
                        "authorId": "1382578305",
                        "name": "Linhao Luo"
                    },
                    {
                        "authorId": "46395829",
                        "name": "Yufei Wang"
                    },
                    {
                        "authorId": "2127380138",
                        "name": "Chen Chen"
                    },
                    {
                        "authorId": "2163833353",
                        "name": "Jiapu Wang"
                    },
                    {
                        "authorId": "1748808",
                        "name": "Xindong Wu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026neural semantic parsingbased methods (Yih et al., 2015; Bao et al., 2016; Luo et al., 2018), information retrieval-based methods (Sun et al., 2018; Saxena et al., 2020; Yasunaga\net al., 2021), and differentiable KG-based methods (Cohen et al., 2020; Saffari et al., 2021; Sen et al., 2021), which,\u2026"
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "233e48a00e5172819b36d3834cd0f4335b7ed595",
                "externalIds": {
                    "ArXiv": "2306.04136",
                    "ACL": "2023.nlrse-1.7",
                    "DBLP": "journals/corr/abs-2306-04136",
                    "DOI": "10.48550/arXiv.2306.04136",
                    "CorpusId": 259095910
                },
                "corpusId": 259095910,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/233e48a00e5172819b36d3834cd0f4335b7ed595",
                "title": "Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering",
                "abstract": "Large Language Models (LLMs) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training. However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive. To this end, we propose to augment the knowledge directly in the input of LLMs. Specifically, we first retrieve the relevant facts to the input question from the knowledge graph based on semantic similarities between the question and its associated facts. After that, we prepend the retrieved facts to the input question in the form of the prompt, which is then forwarded to LLMs to generate the answer. Our framework, Knowledge-Augmented language model PromptING (KAPING), requires no model training, thus completely zero-shot. We validate the performance of our KAPING framework on the knowledge graph question answering task, that aims to answer the user\u2019s question based on facts over a knowledge graph, on which ours outperforms relevant zero-shot baselines by up to 48% in average, across multiple LLMs of various sizes.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "90765684",
                        "name": "Jinheon Baek"
                    },
                    {
                        "authorId": "8129718",
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "authorId": "1741702",
                        "name": "Amir Saffari"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We improve EmbedKGQA by using our pre-training and re-training framework to learn KG embeddings.",
                "Our QA method improves EmbedKGQA by replacing its KG embeddings learned by ComplEx [55] with our transferable embeddings.",
                "Our MuKGE could also be used here to learn embeddings with the knowledge transferred from other background KGs. EmbedKGQA uses RoBERTa [31] to represent natural questions.",
                "In our experiment, we chose thewidely usedmulti-hopQA dataset WebQuestionsSP [70] following EmbedKGQA.",
                "We follow EmbedKGQA [40], the first embedding-based model for multi-hop KGQA, in this experiment.",
                "The two modules can be processed step-by-step [40], or simultaneously optimized to take advantage of their deep interactions [75].",
                "Following EmbedKGQA and NSM, we also use the relation pruning strategy to reduce the space of candidate entities by removing irrelevant relations.",
                "NSM introduces a more complex reasoning method for KGQA, whereas our method transfers useful knowledge to improve the simple method EmbedKGQA with more expressive KG embeddings that can benefit KG-related downstream tasks.",
                "To further investigate the effect of KG incompleteness on QA performance, following EmbedKGQA, we use FB4QA in two settings, i.e., Half-KG and Full-KG.",
                "To ensure a fair comparison, other QA modules remain the same as those in EmbedKGQA.",
                "We follow EmbedKGQA [40] to develop our QA method.",
                "We choose EmbedKGQA as a baseline.",
                "EmbedKGQA learns embeddings for the target KG using TransE [5] or ComplEx [55].",
                ", link prediction in one KG [5], and multi-hop KG question answering [40].",
                "Our QA method, denoted by EmbedKGQA + MuKGE, outperforms EmbedKGQA.",
                "Multi-hop KGQA predicts the answer entity from a target KG given a natural language query [40].",
                "Following EmbedKGQA and \ud835\udf07KG, we use the extracted subset of Freebase as the target KG.",
                "EmbedKGQA learns a mapping between the embeddings of a question and its answer entity.",
                "EmbedKGQA is the first embedding-based method for multi-hop KGQA."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "d558c9c90b7f14d16c894014af4b4ccff6fd65cb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2306-02679",
                    "ArXiv": "2306.02679",
                    "DOI": "10.1145/3580305.3599397",
                    "CorpusId": 259076156
                },
                "corpusId": 259076156,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/d558c9c90b7f14d16c894014af4b4ccff6fd65cb",
                "title": "Joint Pre-training and Local Re-training: Transferable Representation Learning on Multi-source Knowledge Graphs",
                "abstract": "In this paper, we present the \"joint pre-training and local re-training'' framework for learning and applying multi-source knowledge graph (KG) embeddings. We are motivated by the fact that different KGs contain complementary information to improve KG embeddings and downstream tasks. We pre-train a large teacher KG embedding model over linked multi-source KGs and distill knowledge to train a student model for a task-specific KG. To enable knowledge transfer across different KGs, we use entity alignment to build a linked subgraph for connecting the pre-trained KGs and the target KG. The linked subgraph is re-trained for three-level knowledge distillation from the teacher to the student, i.e., feature knowledge distillation, network knowledge distillation, and prediction knowledge distillation, to generate more expressive embeddings. The teacher model can be reused for different target KGs and tasks without having to train from scratch. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our framework.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "144381296",
                        "name": "Zequn Sun"
                    },
                    {
                        "authorId": "50535588",
                        "name": "Jiacheng Huang"
                    },
                    {
                        "authorId": "2144383422",
                        "name": "Jing-Rong Lin"
                    },
                    {
                        "authorId": "1500399728",
                        "name": "Xiaozhou Xu"
                    },
                    {
                        "authorId": "2109402489",
                        "name": "Qijin Chen"
                    },
                    {
                        "authorId": "145066190",
                        "name": "Wei Hu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Since knowledge graphs are often sparse with many missing links, this poses additional challenges, especially increasing the need for multi-hop reasoning (Saxena et al., 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c15e57285a25c5fb941269bb55d91dce628c2ba1",
                "externalIds": {
                    "ACL": "2023.tacl-1.40",
                    "DOI": "10.1162/tacl_a_00569",
                    "CorpusId": 259335338
                },
                "corpusId": 259335338,
                "publicationVenue": {
                    "id": "f613fad9-da81-41c5-841d-e2c01b5d33cb",
                    "name": "International Conference on Topology, Algebra and Categories in Logic",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Topol Algebra Category Log"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c15e57285a25c5fb941269bb55d91dce628c2ba1",
                "title": "OpenFact: Factuality Enhanced Open Knowledge Extraction",
                "abstract": "We focus on the factuality property during the extraction of an OpenIE corpus named OpenFact, which contains more than 12 million high-quality knowledge triplets. We break down the factuality property into two important aspects\u2014expressiveness and groundedness\u2014and we propose a comprehensive framework to handle both aspects. To enhance expressiveness, we formulate each knowledge piece in OpenFact based on a semantic frame. We also design templates, extra constraints, and adopt human efforts so that most OpenFact triplets contain enough details. For groundedness, we require the main arguments of each triplet to contain linked Wikidata1 entities. A human evaluation suggests that the OpenFact triplets are much more accurate and contain denser information compared to OPIEC-Linked (Gashteovski et al., 2019), one recent high-quality OpenIE corpus grounded to Wikidata. Further experiments on knowledge base completion and knowledge base question answering show the effectiveness of OpenFact over OPIEC-Linked as supplementary knowledge to Wikidata as the major KG.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "50258954",
                        "name": "Linfeng Song"
                    },
                    {
                        "authorId": "1754106063",
                        "name": "Ante Wang"
                    },
                    {
                        "authorId": "34741133",
                        "name": "Xiaoman Pan"
                    },
                    {
                        "authorId": "49723569",
                        "name": "Hongming Zhang"
                    },
                    {
                        "authorId": "41190054",
                        "name": "Dian Yu"
                    },
                    {
                        "authorId": "50496698",
                        "name": "Lifeng Jin"
                    },
                    {
                        "authorId": "2013337",
                        "name": "Haitao Mi"
                    },
                    {
                        "authorId": "34739384",
                        "name": "Jinsong Su"
                    },
                    {
                        "authorId": "39939186",
                        "name": "Yue Zhang"
                    },
                    {
                        "authorId": "2111505433",
                        "name": "Dong Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Saxena et al.[11] proposed the EmbedKGQA model, which was the first time to use KG embeddings to represent the triplet embedding vectors."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5ec8e96b8b3b0fdabe3e2484dca745d73f45ff9c",
                "externalIds": {
                    "DBLP": "conf/cscwd/MaY023",
                    "DOI": "10.1109/CSCWD57460.2023.10152692",
                    "CorpusId": 259235866
                },
                "corpusId": 259235866,
                "publicationVenue": {
                    "id": "a966c5e8-76dc-45c0-942a-3c7e41ac9b1a",
                    "name": "International Conference on Computer Supported Cooperative Work in Design",
                    "type": "conference",
                    "alternate_names": [
                        "Computer Supported Cooperative Work in Design",
                        "Int Conf Comput Support Cooperative Work Des",
                        "CSCWD",
                        "Comput Support Cooperative Work Des"
                    ],
                    "url": "http://www.cscwd.org/"
                },
                "url": "https://www.semanticscholar.org/paper/5ec8e96b8b3b0fdabe3e2484dca745d73f45ff9c",
                "title": "BERT-based Question Answering using Knowledge Graph Embeddings in Nuclear Power Domain",
                "abstract": "In order to improve the resource utilization rate of existing nuclear power data and promote workers to efficiently obtain the operation information of nuclear power units and assist them in fault diagnosis and maintenance decision-making, this paper constructs a knowledge graph question answering (KGQA) dataset in the field of nuclear power. The BEm-KGQA model based on the pre-trained language model and knowledge graph embedding method was proposed. Our model learns the embedded representation of the knowledge graph through BERT and fine-tunes the BERT model. In the question embedding stage, it learns the embedded representation of the question based on the fine-tuned BERT model. Through experiments, we demonstrate the effectiveness of the method over other models. In addition, this paper implements a nuclear power question answering system. Based on the question answering system, employees can learn about unit information and efficiently obtain information on unusual operating events of nuclear power.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2220491219",
                        "name": "Zuyang Ma"
                    },
                    {
                        "authorId": "2047542417",
                        "name": "Kaihong Yan"
                    },
                    {
                        "authorId": "2154853698",
                        "name": "Hongwei Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "EmbedKGQA [125] learns representation of a multi-hop NLQ in the KG embedding space first by using RoBERTa (robustly optimized BERT pretraining), followed by fully connected linear layers with ReLU activation, and finally projecting onto the KG em-",
                "More recently, [55, 26, 113, 125, 79, 124] propose methods to answer NLQs over KGs in an end-to-end"
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1f07a41978599c86c0bd7df35ab17b0c95f54ab8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-14485",
                    "ArXiv": "2305.14485",
                    "DOI": "10.1145/3615952.3615956",
                    "CorpusId": 258865927
                },
                "corpusId": 258865927,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1f07a41978599c86c0bd7df35ab17b0c95f54ab8",
                "title": "Knowledge Graphs Querying",
                "abstract": "Knowledge graphs (KGs) such as DBpedia, Freebase, YAGO, Wikidata, and NELL were constructed to store large-scale, real-world facts as (subject, predicate, object) triples - that can also be modeled as a graph, where a node (a subject or an object) represents an entity with attributes, and a directed edge (a predicate) is a relationship between two entities. Querying KGs is critical in web search, question answering (QA), semantic search, personal assistants, fact checking, and recommendation. While significant progress has been made on KG construction and curation, thanks to deep learning recently we have seen a surge of research on KG querying and QA. The objectives of our survey are two-fold. First, research on KG querying has been conducted by several communities, such as databases, data mining, semantic web, machine learning, information retrieval, and natural language processing (NLP), with different focus and terminologies; and also in diverse topics ranging from graph databases, query languages, join algorithms, graph patterns matching, to more sophisticated KG embedding and natural language questions (NLQs). We aim at uniting different interdisciplinary topics and concepts that have been developed for KG querying. Second, many recent advances on KG and query embedding, multimodal KG, and KG-QA come from deep learning, IR, NLP, and computer vision domains. We identify important challenges of KG querying that received less attention by graph databases, and by the DB community in general, e.g., incomplete KG, semantic matching, multimodal data, and NLQs. We conclude by discussing interesting opportunities for the data management community, for instance, KG as a unified data model and vector-based query processing.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108514592",
                        "name": "Arijit Khan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For KGQA, we select KV-Mem (Miller et al., 2016), GragtNet (Sun et al., 2018), EmbedKGQA (Saxena et al., 2020), NSM (He et al., 2021), and UniKGQA (Jiang et al., 2022b).",
                ", 2018), EmbedKGQA (Saxena et al., 2020), NSM (He et al."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6d3d07aa1be601bd38289bb89f9ad2fa6a2c36e7",
                "externalIds": {
                    "ArXiv": "2305.09645",
                    "DBLP": "journals/corr/abs-2305-09645",
                    "DOI": "10.48550/arXiv.2305.09645",
                    "CorpusId": 258714753
                },
                "corpusId": 258714753,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6d3d07aa1be601bd38289bb89f9ad2fa6a2c36e7",
                "title": "StructGPT: A General Framework for Large Language Model to Reason over Structured Data",
                "abstract": "In this paper, we study how to improve the zero-shot reasoning ability of large language models~(LLMs) over structured data in a unified way. Inspired by the study on tool augmentation for LLMs, we develop an \\emph{Iterative Reading-then-Reasoning~(IRR)} approach for solving question answering tasks based on structured data, called \\textbf{StructGPT}. In our approach, we construct the specialized function to collect relevant evidence from structured data (\\ie \\emph{reading}), and let LLMs concentrate the reasoning task based on the collected information (\\ie \\emph{reasoning}). Specially, we propose an \\emph{invoking-linearization-generation} procedure to support LLMs in reasoning on the structured data with the help of the external interfaces. By iterating this procedures with provided interfaces, our approach can gradually approach the target answer to a given query. Extensive experiments conducted on three types of structured data demonstrate the effectiveness of our approach, which can significantly boost the performance of ChatGPT and achieve comparable performance against the full-data supervised-tuning baselines. Our codes and data are publicly available at~\\url{https://github.com/RUCAIBox/StructGPT}.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2118240359",
                        "name": "Jinhao Jiang"
                    },
                    {
                        "authorId": "1423651904",
                        "name": "Kun Zhou"
                    },
                    {
                        "authorId": "2198280871",
                        "name": "Zican Dong"
                    },
                    {
                        "authorId": "1657563745",
                        "name": "Keming Ye"
                    },
                    {
                        "authorId": "2542603",
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "authorId": "153693432",
                        "name": "Ji-rong Wen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ed69a8f2319b4829d4972087383ddb0e20d25590",
                "externalIds": {
                    "DOI": "10.3390/app13106104",
                    "CorpusId": 258772587
                },
                "corpusId": 258772587,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ed69a8f2319b4829d4972087383ddb0e20d25590",
                "title": "MKBQA: Question Answering over Knowledge Graph Based on Semantic Analysis and Priority Marking Method",
                "abstract": "In the field of question answering-based knowledge graphs, due to the complexity of the construction of knowledge graphs, a domain-specific knowledge graph often cannot contain some common-sense knowledge, which makes it impossible to answer questions that involve common-sense and domain knowledge at the same time. Therefore, this study proposes a knowledge graph-based question answering method in the computer science domain, which facilitates obtaining complete answers in this domain. In order to solve the problem of natural language problems being difficult to match with structured knowledge, a series of logic rules are first designed to convert natural language into triples of the question. Then, a semantic query expansion strategy based on WordNet is proposed and a priority marking algorithm is proposed to mark the order of triples of the question. Finally, when a question triple corresponds to multiple triples in the knowledge graph, it can be solved by the proposed SimCSE-based similarity method. The designed logic rules can deal with each type of question in a targeted manner according to the different question words and can effectively transform the question text into question triples. In addition, the proposed priority marking algorithm can effectively mark the order in the triple of the question. MKBQA can answer not only computer science-related questions but also extended open domain questions. In practical applications, answering a domain question often cannot rely solely on one knowledge graph. It is necessary to combine domain knowledge and common-sense knowledge. The MKBQA method provides a new idea and can be easily migrated from the field of computer science to other fields. Experiment results on real-world data sets show that, as compared to baselines, our method achieves significant improvements to question answering and can combine common-sense and domain-specific knowledge graphs to give a more complete answer.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2217729140",
                        "name": "Xiang Wang"
                    },
                    {
                        "authorId": "2110468226",
                        "name": "Yanchao Li"
                    },
                    {
                        "authorId": "2108931541",
                        "name": "Huiyong Wang"
                    },
                    {
                        "authorId": "2217526466",
                        "name": "Menglong Lv"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026information extraction, has enjoyed widespread empirical success and can provide backend support for various NLP tasks, such as question answering (Saxena et al., 2020; Shang et al., 2022; Zhang et al., 2022a), commonsense reasoning (Yasunaga et al., 2021; Zhang et al., 2022d) etc. Traditional\u2026",
                "Knowledge Graph Construction (KGC), typically through information extraction, has enjoyed widespread empirical success and can provide backend support for various NLP tasks, such as question answering (Saxena et al., 2020; Shang et al., 2022; Zhang et al., 2022a), commonsense reasoning (Yasunaga et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c324d4c662f17ce2e68acfbb736e7b7c9041ffdb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-08703",
                    "ArXiv": "2305.08703",
                    "DOI": "10.48550/arXiv.2305.08703",
                    "CorpusId": 258686618
                },
                "corpusId": 258686618,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c324d4c662f17ce2e68acfbb736e7b7c9041ffdb",
                "title": "Schema-adaptable Knowledge Graph Construction",
                "abstract": "Conventional Knowledge Graph Construction (KGC) approaches typically follow the static information extraction paradigm with a closed set of pre-defined schema. As a result, such approaches fall short when applied to dynamic scenarios or domains, whereas a new type of knowledge emerges. This necessitates a system that can handle evolving schema automatically to extract information for KGC. To address this need, we propose a new task called schema-adaptable KGC, which aims to continually extract entity, relation, and event based on a dynamically changing schema graph without re-training. We first split and convert existing datasets based on three principles to build a benchmark, i.e., horizontal schema expansion, vertical schema expansion, and hybrid schema expansion; then investigate the schema-adaptable performance of several well-known approaches such as Text2Event, TANL, UIE and GPT-3.5. We further propose a simple yet effective baseline dubbed AdaKGC, which contains schema-enriched prefix instructor and schema-conditioned dynamic decoding to better handle evolving schema. Comprehensive experimental results illustrate that AdaKGC can outperform baselines but still have room for improvement. We hope the proposed work can deliver benefits to the community. Code and datasets will be available in https://github.com/zjunlp/AdaKGC.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "40463748",
                        "name": "Hongbin Ye"
                    },
                    {
                        "authorId": "2217228727",
                        "name": "Honghao Gui"
                    },
                    {
                        "authorId": "2152775219",
                        "name": "Xin Xu"
                    },
                    {
                        "authorId": "2144200945",
                        "name": "Huajun Chen"
                    },
                    {
                        "authorId": "2608639",
                        "name": "Ningyu Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026computational linguistics community, due to its ability to improve the completeness and fairness of KGs, and enhance a wide range of knowledge-driven downstream applications like question-answering (Saxena et al., 2020; Chen et al., 2021) and dialogue systems (Liu et al., 2021; Xu et al., 2019c).",
                "EA task has received a lot of attentions in the computational linguistics community, due to its ability to improve the completeness and fairness of KGs, and enhance a wide range of knowledge-driven downstream applications like question-answering (Saxena et al., 2020; Chen et al., 2021) and dialogue systems (Liu et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d2029c972967b4da21ae3bbe4e202f9c814bd792",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-06574",
                    "ArXiv": "2305.06574",
                    "DOI": "10.48550/arXiv.2305.06574",
                    "CorpusId": 258615656
                },
                "corpusId": 258615656,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/d2029c972967b4da21ae3bbe4e202f9c814bd792",
                "title": "A Fused Gromov-Wasserstein Framework for Unsupervised Knowledge Graph Entity Alignment",
                "abstract": "Entity alignment is the task of identifying corresponding entities across different knowledge graphs (KGs). Although recent embedding-based entity alignment methods have shown significant advancements, they still struggle to fully utilize KG structural information. In this paper, we introduce FGWEA, an unsupervised entity alignment framework that leverages the Fused Gromov-Wasserstein (FGW) distance, allowing for a comprehensive comparison of entity semantics and KG structures within a joint optimization framework. To address the computational challenges associated with optimizing FGW, we devise a three-stage progressive optimization algorithm. It starts with a basic semantic embedding matching, proceeds to approximate cross-KG structural and relational similarity matching based on iterative updates of high-confidence entity links, and ultimately culminates in a global structural comparison between KGs. We perform extensive experiments on four entity alignment datasets covering 14 distinct KGs across five languages. Without any supervision or hyper-parameter tuning, FGWEA surpasses 21 competitive baselines, including cutting-edge supervised entity alignment methods. Our code is available at https://github.com/squareRoot3/FusedGW-Entity-Alignment.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "66239746",
                        "name": "Jianheng Tang"
                    },
                    {
                        "authorId": "8599669",
                        "name": "Kangfei Zhao"
                    },
                    {
                        "authorId": "2196671991",
                        "name": "Jia Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In the data analysis phase, a deep-learning model derived from EmbedKGQA [5] is employed for the KGQA task."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3201581c3f56fe2690570eb31072bf67fa54fb18",
                "externalIds": {
                    "DBLP": "conf/iotdi/ZhangX23a",
                    "DOI": "10.1145/3576842.3589161",
                    "CorpusId": 258333935
                },
                "corpusId": 258333935,
                "publicationVenue": {
                    "id": "40d9ed4d-6f8b-46bb-a613-9969f48c9c60",
                    "name": "International Conference on Internet-of-Things Design and Implementation",
                    "type": "conference",
                    "alternate_names": [
                        "IoTDI",
                        "Int Conf Internet-of-things Des Implement"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3201581c3f56fe2690570eb31072bf67fa54fb18",
                "title": "A Knowledge Graph Question Answering Approach to IoT Forensics",
                "abstract": "Internet of Things (IoT) forensics has been a particularly challenging task for forensic practitioners due to the heterogeneity of IoT environments as well as the complexity and volume of IoT data. With the advent of artificial intelligence, question-answering (QA) systems have emerged as a potential solution for users to access sophisticated forensic knowledge and data. In this light, we present a novel IoT forensics framework that employs knowledge graph question answering (KGQA). Our framework enables investigators to access forensic artifacts and cybersecurity knowledge using natural language questions facilitated by a deep-learning-powered KGQA model. The proposed framework demonstrates high efficacy in answering natural language questions over the experimental IoT forensic knowledge graph.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2109970280",
                        "name": "Ruipeng Zhang"
                    },
                    {
                        "authorId": "2087679456",
                        "name": "Mengjun Xie"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We also report the published methods in the leaderboard including KVMNet (Miller et al., 2016), SRN (Qiu et al., 2020), EmbedKGQA (Saxena et al., 2020), RGCN (Schlichtkrull et al., 2018), GraphQ IR (Nie et al., 2022).",
                "IR-based methods (Miller et al., 2016; Saxena et al., 2020) follow a retrieve-and-generate paradigm: retrieve a question-specific graph and directly generate answer with text encoder-decoder.",
                ", 2020), EmbedKGQA (Saxena et al., 2020), RGCN (Schlichtkrull et al."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ad55fef5ecf6f75912cb4dbb12408c0a10ca490b",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-03356",
                    "ArXiv": "2305.03356",
                    "DOI": "10.48550/arXiv.2305.03356",
                    "CorpusId": 258546873
                },
                "corpusId": 258546873,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ad55fef5ecf6f75912cb4dbb12408c0a10ca490b",
                "title": "From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser for Complex Question Answering over Knowledge Base",
                "abstract": "Parsing questions into executable logical forms has showed impressive results for knowledge-base question answering (KBQA). However, complex KBQA is a more challenging task that requires to perform complex multi-step reasoning. Recently, a new semantic parser called KoPL has been proposed to explicitly model the reasoning processes, which achieved the state-of-the-art on complex KBQA. In this paper, we further explore how to unlock the reasoning ability of semantic parsers by a simple proposed parse-execute-refine paradigm. We refine and improve the KoPL parser by demonstrating the executed intermediate reasoning steps to the KBQA model. We show that such simple strategy can significantly improve the ability of complex reasoning. Specifically, we propose three components: a parsing stage, an execution stage and a refinement stage, to enhance the ability of complex reasoning. The parser uses the KoPL to generate the transparent logical forms. Then, the execution stage aligns and executes the logical forms over knowledge base to obtain intermediate reasoning processes. Finally, the intermediate step-by-step reasoning processes are demonstrated to the KBQA model in the refinement stage. With the explicit reasoning processes, it is much easier to answer the complex questions. Experiments on benchmark dataset shows that the proposed PER-KBQA performs significantly better than the stage-of-the-art baselines on the complex KBQA.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2187751363",
                        "name": "Wangzhen Guo"
                    },
                    {
                        "authorId": "2191083713",
                        "name": "Linyin Luo"
                    },
                    {
                        "authorId": "2356867",
                        "name": "Hanjiang Lai"
                    },
                    {
                        "authorId": "2111610090",
                        "name": "Jian Yin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Similarly, traditional embedding methods, i.e., KV-Mem, EmbedKGQA, and GraftNet, also perform better in simple questions than in complex ones.",
                "Besides, KV-Mem (Miller et al., 2016), EmbedKGQA (Saxena et al., 2020), GraftNet (Sun et al., 2018), PullNet (Sun et al., 2019), ReTraCk (Chen et al., 2021) and BiNSM (He et al., 2021) are all IR-based methods, which are also the focus of our comparison.",
                ", 2016), EmbedKGQA (Saxena et al., 2020), GraftNet (Sun et al."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7c771706727310dcab8566690e0057c6d52dfdfb",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-02118",
                    "ArXiv": "2305.02118",
                    "DOI": "10.48550/arXiv.2305.02118",
                    "CorpusId": 258461160
                },
                "corpusId": 258461160,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/7c771706727310dcab8566690e0057c6d52dfdfb",
                "title": "Pay More Attention to Relation Exploration for Knowledge Base Question Answering",
                "abstract": "Knowledge base question answering (KBQA) is a challenging task that aims to retrieve correct answers from large-scale knowledge bases. Existing attempts primarily focus on entity representation and final answer reasoning, which results in limited supervision for this task. Moreover, the relations, which empirically determine the reasoning path selection, are not fully considered in recent advancements. In this study, we propose a novel framework, RE-KBQA, that utilizes relations in the knowledge base to enhance entity representation and introduce additional supervision. We explore guidance from relations in three aspects, including (1) distinguishing similar entities by employing a variational graph auto-encoder to learn relation importance; (2) exploring extra supervision by predicting relation distributions as soft labels with a multi-task scheme; (3) designing a relation-guided re-ranking algorithm for post-processing. Experimental results on two benchmark datasets demonstrate the effectiveness and superiority of our framework, improving the F1 score by 5.7% from 40.5 to 46.3 on CWQ and 5.8% from 62.8 to 68.5 on WebQSP, better or on par with state-of-the-art methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2112402733",
                        "name": "Yong Cao"
                    },
                    {
                        "authorId": "2108363412",
                        "name": "Xianzhi Li"
                    },
                    {
                        "authorId": "2144319462",
                        "name": "Huiwen Liu"
                    },
                    {
                        "authorId": "1617947323",
                        "name": "Wenzhen Dai"
                    },
                    {
                        "authorId": "2143503026",
                        "name": "Shuai Chen"
                    },
                    {
                        "authorId": "37722675",
                        "name": "Bin Wang"
                    },
                    {
                        "authorId": "2108558000",
                        "name": "Min Chen"
                    },
                    {
                        "authorId": "2086349",
                        "name": "Daniel Hershcovich"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0139e689add40a61c9454674edac4e93702aa5fc",
                "externalIds": {
                    "ArXiv": "2305.01750",
                    "DBLP": "conf/acl/LiMZGSC23",
                    "ACL": "2023.acl-long.385",
                    "DOI": "10.48550/arXiv.2305.01750",
                    "CorpusId": 258461017
                },
                "corpusId": 258461017,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/0139e689add40a61c9454674edac4e93702aa5fc",
                "title": "Few-shot In-context Learning on Knowledge Base Question Answering",
                "abstract": "Question answering over knowledge bases is considered a difficult problem due to the challenge of generalizing to a wide variety of possible natural language questions. Additionally, the heterogeneity of knowledge base schema items between different knowledge bases often necessitates specialized training for different knowledge base question-answering (KBQA) datasets. To handle questions over diverse KBQA datasets with a unified training-free framework, we propose KB-BINDER, which for the first time enables few-shot in-context learning over KBQA tasks. Firstly, KB-BINDER leverages large language models like Codex to generate logical forms as the draft for a specific question by imitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledge base to bind the generated draft to an executable one with BM25 score matching. The experimental results on four public heterogeneous KBQA datasets show that KB-BINDER can achieve a strong performance with only a few in-context demonstrations. Especially on GraphQA and 3-hop MetaQA, KB-BINDER can even outperform the state-of-the-art trained models. On GrailQA and WebQSP, our model is also on par with other fully-trained models. We believe KB-BINDER can serve as an important baseline for future research. We plan to release all the code and data. Our code is available at https://github.com/ltl3A87/KB-BINDER.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2157466550",
                        "name": "Tianle Li"
                    },
                    {
                        "authorId": "2461713",
                        "name": "Xueguang Ma"
                    },
                    {
                        "authorId": "14697077",
                        "name": "Alex Zhuang"
                    },
                    {
                        "authorId": "2022231256",
                        "name": "Yu Gu"
                    },
                    {
                        "authorId": "1758652",
                        "name": "Yu Su"
                    },
                    {
                        "authorId": "2928777",
                        "name": "Wenhu Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "aa35bf542164d0467577ff68fc7d1d554aba58ab",
                "externalIds": {
                    "DBLP": "journals/ipm/CuiPHZBL23",
                    "DOI": "10.1016/j.ipm.2023.103283",
                    "CorpusId": 256217944
                },
                "corpusId": 256217944,
                "publicationVenue": {
                    "id": "37f5b9b7-f828-4ae1-a174-45b538cbd4e4",
                    "name": "Information Processing & Management",
                    "type": "journal",
                    "alternate_names": [
                        "Inf Process Manag",
                        "Inf Process  Manag",
                        "Information Processing and Management"
                    ],
                    "issn": "0306-4573",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/244/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/information-processing-and-management/",
                        "http://www.sciencedirect.com/science/journal/03064573",
                        "http://www.journals.elsevier.com/information-processing-and-management/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/aa35bf542164d0467577ff68fc7d1d554aba58ab",
                "title": "Reinforcement learning with dynamic completion for answering multi-hop questions over incomplete knowledge graph",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2093917610",
                        "name": "Hai Cui"
                    },
                    {
                        "authorId": "2068928731",
                        "name": "T. Peng"
                    },
                    {
                        "authorId": "2093920100",
                        "name": "Ridong Han"
                    },
                    {
                        "authorId": "2175454135",
                        "name": "Beibei Zhu"
                    },
                    {
                        "authorId": "2202477161",
                        "name": "Haijia Bi"
                    },
                    {
                        "authorId": "2118467569",
                        "name": "Lu Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The yellow part corresponds to the method EmbedKGQA [20] 2.",
                "Similar to [20], we approximate \u00cd \ufffd \ufffd\ufffd (\ufffd |\ufffd, \ufffd\ufffd ) as \u2211 log \ufffd\ufffd (\ufffd |\ufffd, \ufffd\ufffd ) \u221d \ufffd \u2217 |\ufffd\ufffd \u2229 \ufffd\ufffd |,",
                "The yellow part shows the architecture of EmbedKGQA [20].",
                "Following the same setup as in [20], we evaluate the accuracy using the Hit@1 metrics which is the fraction of times a correct answer was retrieved within the top-1 positions.",
                "EmbedKGQA [20] embeds both the input question and entities in the knowledge graph to points in the embedding space and fnds answers according to their embedding similarity.",
                "\u2022 EmbedKGQA [20] conducts multi-hop reasoning through matching pre-trained entity embeddings with question embedding obtained from RoBERTa [5].",
                ", [20], [16], directly learn an embedding from the natural language sentence and search answers in the embedding space."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "11341a0bc9e0a9e84c930aa2bf5049413e576aad",
                "externalIds": {
                    "DBLP": "conf/www/LiuCDYT23",
                    "DOI": "10.1145/3543507.3583316",
                    "CorpusId": 258333817
                },
                "corpusId": 258333817,
                "publicationVenue": {
                    "id": "e07422f9-c065-40c3-a37b-75e98dce79fe",
                    "name": "The Web Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Web Conf",
                        "WWW"
                    ],
                    "url": "http://www.iw3c2.org/"
                },
                "url": "https://www.semanticscholar.org/paper/11341a0bc9e0a9e84c930aa2bf5049413e576aad",
                "title": "Knowledge Graph Question Answering with Ambiguous Query",
                "abstract": "Knowledge graph question answering aims to identify answers of the query according to the facts in the knowledge graph. In the vast majority of the existing works, the input queries are considered perfect and can precisely express the user\u2019s query intention. However, in reality, input queries might be ambiguous and elusive which only contain a limited amount of information. Directly answering these ambiguous queries may yield unwanted answers and deteriorate user experience. In this paper, we propose PReFNet which focuses on answering ambiguous queries with pseudo relevance feedback on knowledge graphs. In order to leverage the hidden (pseudo) relevance information existed in the results that are initially returned from a given query, PReFNet treats the top-k returned candidate answers as a set of most relevant answers, and uses variational Bayesian inference to infer user\u2019s query intention. To boost the quality of the inferred queries, a neighborhood embedding based VGAE model is used to prune inferior inferred queries. The inferred high quality queries will be returned to the users to help them search with ease. Moreover, all the high-quality candidate nodes will be re-ranked according to the inferred queries. The experiment results show that our proposed method can recommend high-quality query graphs to users and improve the question answering accuracy.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1505803281",
                        "name": "Lihui Liu"
                    },
                    {
                        "authorId": "2215477428",
                        "name": "Yuzhong Chen"
                    },
                    {
                        "authorId": "40308435",
                        "name": "Mahashweta Das"
                    },
                    {
                        "authorId": "2145058012",
                        "name": "Hao Yang"
                    },
                    {
                        "authorId": "2058143613",
                        "name": "H. Tong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "2 Multi-hop KGQA Recent eforts have been devoted to providing answers by reasoning over a knowledge graph [11, 13? ], mainly including both semantic parsing and information retrieval methods[16, 24]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8be9e2ad4696eac6168d3d0ed490f124eb100078",
                "externalIds": {
                    "DBLP": "conf/www/DongZHDTJ23",
                    "DOI": "10.1145/3543507.3583376",
                    "CorpusId": 258333655
                },
                "corpusId": 258333655,
                "publicationVenue": {
                    "id": "e07422f9-c065-40c3-a37b-75e98dce79fe",
                    "name": "The Web Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Web Conf",
                        "WWW"
                    ],
                    "url": "http://www.iw3c2.org/"
                },
                "url": "https://www.semanticscholar.org/paper/8be9e2ad4696eac6168d3d0ed490f124eb100078",
                "title": "Hierarchy-Aware Multi-Hop Question Answering over Knowledge Graphs",
                "abstract": "Knowledge graphs (KGs) have been widely used to enhance complex question answering (QA). To understand complex questions, existing studies employ language models (LMs) to encode contexts. Despite the simplicity, they neglect the latent relational information among question concepts and answers in KGs. While question concepts ubiquitously present hyponymy at the semantic level, e.g., mammals and animals, this feature is identically reflected in the hierarchical relations in KGs, e.g., a_type_of. Therefore, we are motivated to explore comprehensive reasoning by the hierarchical structures in KGs to help understand questions. However, it is non-trivial to reason over tree-like structures compared with chained paths. Moreover, identifying appropriate hierarchies relies on expertise. To this end, we propose HamQA, a novel Hierarchy-aware multi-hop Question Answering framework on knowledge graphs, to effectively align the mutual hierarchical information between question contexts and KGs. The entire learning is conducted in Hyperbolic space, inspired by its advantages of embedding hierarchical structures. Specifically, (i) we design a context-aware graph attentive network to capture context information. (ii) Hierarchical structures are continuously preserved in KGs by minimizing the Hyperbolic geodesic distances. The comprehensive reasoning is conducted to jointly train both components and provide a top-ranked candidate as an optimal answer. We achieve a higher ranking than the state-of-the-art multi-hop baselines on the official OpenBookQA leaderboard with an accuracy of 85%.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2171902940",
                        "name": "Junnan Dong"
                    },
                    {
                        "authorId": "2172162773",
                        "name": "Qinggang Zhang"
                    },
                    {
                        "authorId": "2171663135",
                        "name": "Xiao Huang"
                    },
                    {
                        "authorId": "108237152",
                        "name": "Keyu Duan"
                    },
                    {
                        "authorId": "9747941",
                        "name": "Qiaoyu Tan"
                    },
                    {
                        "authorId": "2215502313",
                        "name": "Zhimeng Jiang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[5] proposes a way to answer Natural Language questions using KG with the help of graph alignment, neural networks and NLP to generate answers for simple and multihop ques tions."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4f64a764135cbb3ff0c4e56555c335881ab422cb",
                "externalIds": {
                    "DOI": "10.1109/ICICT57646.2023.10134047",
                    "CorpusId": 259028311
                },
                "corpusId": 259028311,
                "publicationVenue": {
                    "id": "9181819e-530e-4408-9917-93c5f58d7fce",
                    "name": "International Congress on Information and Communication Technology",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Inf Comput Technol",
                        "International Conference on Inventive Computation Technologies",
                        "ICICT",
                        "Int Congr Inf Commun Technol",
                        "Int Conf Inven Comput Technol",
                        "International Conference on Issues and Challenges in Intelligent Computing Techniques",
                        "International Conference on Information and Computer Technologies",
                        "Int Conf Issue Chall Intell Comput Tech"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4f64a764135cbb3ff0c4e56555c335881ab422cb",
                "title": "Question Answering System using Knowledge Graphs",
                "abstract": "A question answering system aims to answer the asked question with relevant responses thus sufficing the re-quested query asked in natural language by responding in the same language. Knowledge Graph Question Answering (KGQA) aims to answer questions asked by the user on a paragraph from a knowledge graph (KG). A strongly connected KG is essential in picking out answers for the requested question. This is because the KG is traversed to select the answer. A well connected KG thus provides a relevant answer. The knowledge graph is built by identifying the subject, the object and the relation for every sentence in the input text or knowledge base. Questions are processed to identify the source-relation-target triples which are then matched with that of the triples forming the KG. The challenge is in extracting the entities and relations between them to create the KG. The model's performance is directly proportional to the strength of the KG. Hence, the presence of a well connected KG provides great accuracy while a poorly connected one would break the system. The proposed model is tested on a Multi RC dataset. Multi RC is a dataset for multi hop question answering that includes short paragraphs and multi-sentence questions. This allows catering to both single hop and multi hop questions. The primary objective was to build a question answering system with the ability to answer multi hop questions together with an efficient response time through the usage of knowledge graphs. A novel approach has been employed where natural language questions are processed into key-value pairs, by leveraging python modules whose dependencies aid in parts of speech tagging in the English language thereby mapping back to the data entities present in the KG to retrieve the correct answer.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2219407345",
                        "name": "Spurthy Skandan"
                    },
                    {
                        "authorId": "2218777519",
                        "name": "Susheen Kanungo"
                    },
                    {
                        "authorId": "2219405385",
                        "name": "Shreyas Devaraj"
                    },
                    {
                        "authorId": "2118972418",
                        "name": "Sahil Gupta"
                    },
                    {
                        "authorId": "25529403",
                        "name": "Surabhi Narayan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "They play an important role in many knowledge-driven downstream tasks [1], including information extraction [2], [3], program analysis [4], question answering [5], [6], medical diagnosis [7] etc."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1d44fcb84732019d1df479fe88f1e9e5b12b4bfd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-10297",
                    "ArXiv": "2304.10297",
                    "DOI": "10.48550/arXiv.2304.10297",
                    "CorpusId": 258236277
                },
                "corpusId": 258236277,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1d44fcb84732019d1df479fe88f1e9e5b12b4bfd",
                "title": "SARF: Aliasing Relation Assisted Self-Supervised Learning for Few-shot Relation Reasoning",
                "abstract": "Few-shot relation reasoning on knowledge graphs (FS-KGR) aims to infer long-tail data-poor relations, which has drawn increasing attention these years due to its practicalities. The pre-training of previous methods needs to manually construct the meta-relation set, leading to numerous labor costs. Self-supervised learning (SSL) is treated as a solution to tackle the issue, but still at an early stage for FS-KGR task. Moreover, most of the existing methods ignore leveraging the beneficial information from aliasing relations (AR), i.e., data-rich relations with similar contextual semantics to the target data-poor relation. Therefore, we proposed a novel Self-Supervised Learning model by leveraging Aliasing Relations to assist FS-KGR, termed SARF. Concretely, four main components are designed in our model, i.e., SSL reasoning module, AR-assisted mechanism, fusion module, and scoring function. We first generate the representation of the co-occurrence patterns in a generative manner. Meanwhile, the representations of aliasing relations are learned to enhance reasoning in the AR-assist mechanism. Besides, multiple strategies, i.e., simple summation and learnable fusion, are offered for representation fusion. Finally, the generated representation is used for scoring. Extensive experiments on three few-shot benchmarks demonstrate that SARF achieves state-of-the-art performance compared with other methods in most cases.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "13325191",
                        "name": "Lingyuan Meng"
                    },
                    {
                        "authorId": "2024445866",
                        "name": "K. Liang"
                    },
                    {
                        "authorId": "2165556493",
                        "name": "Bin Xiao"
                    },
                    {
                        "authorId": "2516087",
                        "name": "Sihang Zhou"
                    },
                    {
                        "authorId": "2119034129",
                        "name": "Yue Liu"
                    },
                    {
                        "authorId": "2152969291",
                        "name": "Meng Liu"
                    },
                    {
                        "authorId": "2154476091",
                        "name": "Xihong Yang"
                    },
                    {
                        "authorId": "2130021053",
                        "name": "Xinwang Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In addition, the literature [24, 25] also includes a simple question answering and multihop question answering system framework based on knowledge graph embedding, but the author has patented the method."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6b296ff56e316240e8eeeb138dcef9ccbb8baee0",
                "externalIds": {
                    "DOI": "10.1186/s40494-023-00922-7",
                    "CorpusId": 258183726
                },
                "corpusId": 258183726,
                "publicationVenue": {
                    "id": "ddc90473-29ee-46d9-afe3-16c9331e7f3c",
                    "name": "Heritage Science",
                    "type": "journal",
                    "alternate_names": [
                        "Heritage Sci"
                    ],
                    "issn": "2050-7445",
                    "url": "http://www.heritagesciencejournal.com/",
                    "alternate_urls": [
                        "https://heritagesciencejournal.springeropen.com/",
                        "https://heritagesciencejournal.springeropen.com",
                        "http://www.heritagesciencejournal.com/content"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/6b296ff56e316240e8eeeb138dcef9ccbb8baee0",
                "title": "The application of knowledge graphs in the Chinese cultural field: the ancient capital culture of Beijing",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2064457158",
                        "name": "B. Bai"
                    },
                    {
                        "authorId": "2098364822",
                        "name": "Wenjun Hou"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "85bee437c0ffbd543925c56b1def016cf9a69bea",
                "externalIds": {
                    "DOI": "10.3390/electronics12081905",
                    "CorpusId": 258231185
                },
                "corpusId": 258231185,
                "publicationVenue": {
                    "id": "ccd8e532-73c6-414f-bc91-271bbb2933e2",
                    "name": "Electronics",
                    "type": "journal",
                    "issn": "1450-5843",
                    "alternate_issns": [
                        "2079-9292",
                        "0883-4989"
                    ],
                    "url": "http://www.electronics.etfbl.net/",
                    "alternate_urls": [
                        "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-247562",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-247562",
                        "https://www.mdpi.com/journal/electronics"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/85bee437c0ffbd543925c56b1def016cf9a69bea",
                "title": "Multi-Hop Knowledge Graph Question Answer Method Based on Relation Knowledge Enhancement",
                "abstract": "Multi-hop knowledge graph question answer (KGQA) is a challenging task because it requires reasoning over multiple edges of the knowledge graph (KG) to arrive at the right answer. However, KGs are often incomplete with many missing links, posing additional challenges for multi-hop KGQA. Recent research on multi-hop KGQA attempted to deal with KG sparsity with relevant external texts. In our work, we propose a multi-hop KGQA model based on relation knowledge enhancement (RKE-KGQA), which fuses both label and text relations through global attention for relation knowledge augmentation. It is well known that the relation between entities can be represented by labels in the knowledge graph or texts in the text corpus, and multi-hop KGQA needs to jump across different entities through relations. First, we assign an activation probability to each entity, then calculate a score for the enhancement relation, and then transfer the score through the activated relations and, finally, obtain the answer. We carry out extensive experiments on three datasets and demonstrate that RKE-KGQA achieves the outperformance result.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2214867499",
                        "name": "Tianbin Wang"
                    },
                    {
                        "authorId": "26977212",
                        "name": "Ruiyang Huang"
                    },
                    {
                        "authorId": "2187454043",
                        "name": "Huansha Wang"
                    },
                    {
                        "authorId": "35678872",
                        "name": "Hongxin Zhi"
                    },
                    {
                        "authorId": "2214841577",
                        "name": "Hongji Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Another way is to directly learn knowledge graph embeddings and then integrate the learned entity and relation embeddings into the QA pipeline [69, 109, 146]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "965e0d4bfe8097baab1947fc23263ae790620e23",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-06136",
                    "ArXiv": "2304.06136",
                    "DOI": "10.48550/arXiv.2304.06136",
                    "CorpusId": 258108416
                },
                "corpusId": 258108416,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/965e0d4bfe8097baab1947fc23263ae790620e23",
                "title": "AGI for Agriculture",
                "abstract": "Artificial General Intelligence (AGI) is poised to revolutionize a variety of sectors, including healthcare, finance, transportation, and education. Within healthcare, AGI is being utilized to analyze clinical medical notes, recognize patterns in patient data, and aid in patient management. Agriculture is another critical sector that impacts the lives of individuals worldwide. It serves as a foundation for providing food, fiber, and fuel, yet faces several challenges, such as climate change, soil degradation, water scarcity, and food security. AGI has the potential to tackle these issues by enhancing crop yields, reducing waste, and promoting sustainable farming practices. It can also help farmers make informed decisions by leveraging real-time data, leading to more efficient and effective farm management. This paper delves into the potential future applications of AGI in agriculture, such as agriculture image processing, natural language processing (NLP), robotics, knowledge graphs, and infrastructure, and their impact on precision livestock and precision crops. By leveraging the power of AGI, these emerging technologies can provide farmers with actionable insights, allowing for optimized decision-making and increased productivity. The transformative potential of AGI in agriculture is vast, and this paper aims to highlight its potential to revolutionize the industry.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3073046",
                        "name": "G. Lu"
                    },
                    {
                        "authorId": "2153702893",
                        "name": "Sheng Li"
                    },
                    {
                        "authorId": "40626717",
                        "name": "Gengchen Mai"
                    },
                    {
                        "authorId": "2214298401",
                        "name": "Jin Sun"
                    },
                    {
                        "authorId": "2181182",
                        "name": "Dajiang Zhu"
                    },
                    {
                        "authorId": "152869339",
                        "name": "L. Chai"
                    },
                    {
                        "authorId": "2214271343",
                        "name": "Haijian Sun"
                    },
                    {
                        "authorId": "2214283393",
                        "name": "Xianqiao Wang"
                    },
                    {
                        "authorId": "29944950",
                        "name": "Haixing Dai"
                    },
                    {
                        "authorId": "49229465",
                        "name": "Ning Liu"
                    },
                    {
                        "authorId": "2115800524",
                        "name": "R. Xu"
                    },
                    {
                        "authorId": "2128924201",
                        "name": "Daniel J Petti"
                    },
                    {
                        "authorId": "2145402599",
                        "name": "Changying Li"
                    },
                    {
                        "authorId": "2115345993",
                        "name": "Tianming Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cfe32482a4827ebffd8d59f1b80c0bc9fb4697c6",
                "externalIds": {
                    "DOI": "10.1109/ICCEA58433.2023.10135342",
                    "CorpusId": 259028830
                },
                "corpusId": 259028830,
                "publicationVenue": {
                    "id": "0a4491c0-572b-484b-b6d1-191b75b000fe",
                    "name": "International Conference Civil Engineering and Architecture",
                    "type": "conference",
                    "alternate_names": [
                        "ICCEA",
                        "Int Conf Civ Eng Archit"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/cfe32482a4827ebffd8d59f1b80c0bc9fb4697c6",
                "title": "Improving temporal question answering using temporal knowledge graph embedding",
                "abstract": "Knowledge graph question answering is an important research direction for question answering tasks. In recent years, there has been an increasing amount of research on knowledge graph question answering, but temporal knowledge graph question answering is still a relatively unexplored area. In question answering tasks, many natural language questions have explicit or implicit temporal constraints, and most existing research methods lack the temporal awareness to deal with complex temporal questions. To address these challenges, this paper proposes a question answering model based on temporal knowledge graph embedding (TKGETQA). The model uses TKG embeddings to root a question in the entities, relations and time horizons it references, and perceives temporal information from the question to improve the accuracy of answer prediction. Experiments on the dataset CronQuestions in this paper show that the TKGETQA model exhibits better results compared to existing temporal knowledge graph question answering approaches.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2124773762",
                        "name": "Jiabao Chen"
                    },
                    {
                        "authorId": "1485668568",
                        "name": "Yongquan Fan"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", question answering [48,30,12] and natural language generation [2,22]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ef686901a5fa131e522e4d0cea39997aa8f627ec",
                "externalIds": {
                    "DBLP": "conf/pkdd/DingWLMT23",
                    "ArXiv": "2304.00613",
                    "DOI": "10.48550/arXiv.2304.00613",
                    "CorpusId": 257913402
                },
                "corpusId": 257913402,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ef686901a5fa131e522e4d0cea39997aa8f627ec",
                "title": "Improving Few-Shot Inductive Learning on Temporal Knowledge Graphs using Confidence-Augmented Reinforcement Learning",
                "abstract": "Temporal knowledge graph completion (TKGC) aims to predict the missing links among the entities in a temporal knwoledge graph (TKG). Most previous TKGC methods only consider predicting the missing links among the entities seen in the training set, while they are unable to achieve great performance in link prediction concerning newly-emerged unseen entities. Recently, a new task, i.e., TKG few-shot out-of-graph (OOG) link prediction, is proposed, where TKGC models are required to achieve great link prediction performance concerning newly-emerged entities that only have few-shot observed examples. In this work, we propose a TKGC method FITCARL that combines few-shot learning with reinforcement learning to solve this task. In FITCARL, an agent traverses through the whole TKG to search for the prediction answer. A policy network is designed to guide the search process based on the traversed path. To better address the data scarcity problem in the few-shot setting, we introduce a module that computes the confidence of each candidate action and integrate it into the policy for action selection. We also exploit the entity concept information with a novel concept regularizer to boost model performance. Experimental results show that FITCARL achieves stat-of-the-art performance on TKG few-shot OOG link prediction.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2046761003",
                        "name": "Zifeng Ding"
                    },
                    {
                        "authorId": "2181618414",
                        "name": "Jingpei Wu"
                    },
                    {
                        "authorId": "2203330037",
                        "name": "Zong-Xun Li"
                    },
                    {
                        "authorId": "10684484",
                        "name": "Yunpu Ma"
                    },
                    {
                        "authorId": "1742501819",
                        "name": "Volker Tresp"
                    }
                ]
            }
        },
        {
            "contexts": [
                "EmbedKGQA (Saxena et al. 2020) Knowledge graph embedding-based multi-hop question answering"
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "42b323b6df79e49c9bf5cee2a91398a7fa3d594d",
                "externalIds": {
                    "ArXiv": "2303.13948",
                    "DBLP": "journals/corr/abs-2303-13948",
                    "PubMedCentral": "10068207",
                    "DOI": "10.1007/s10462-023-10465-9",
                    "CorpusId": 257757244,
                    "PubMed": "37362886"
                },
                "corpusId": 257757244,
                "publicationVenue": {
                    "id": "ea8553fe-2467-4367-afee-c4deb3754820",
                    "name": "Artificial Intelligence Review",
                    "type": "journal",
                    "alternate_names": [
                        "Artif Intell Rev"
                    ],
                    "issn": "0269-2821",
                    "url": "https://link.springer.com/journal/10462"
                },
                "url": "https://www.semanticscholar.org/paper/42b323b6df79e49c9bf5cee2a91398a7fa3d594d",
                "title": "Knowledge Graphs: Opportunities and Challenges",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1726114642",
                        "name": "Ciyuan Peng"
                    },
                    {
                        "authorId": "2143633281",
                        "name": "Feng Xia"
                    },
                    {
                        "authorId": "2149932639",
                        "name": "Mehdi Naseriparsa"
                    },
                    {
                        "authorId": "2052329",
                        "name": "Francesco Osborne"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a64167fcaa7a487575c6479510e57795afc9974e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-13284",
                    "ArXiv": "2303.13284",
                    "DOI": "10.48550/arXiv.2303.13284",
                    "CorpusId": 257687289
                },
                "corpusId": 257687289,
                "publicationVenue": {
                    "id": "c7bde2ee-6ad5-49c7-9498-a01e46c162eb",
                    "name": "Extended Semantic Web Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Eur Semantic Web Conf",
                        "Ext Semantic Web Conf",
                        "European Semantic Web Conference",
                        "ESWC"
                    ],
                    "url": "https://link.springer.com/conference/esws"
                },
                "url": "https://www.semanticscholar.org/paper/a64167fcaa7a487575c6479510e57795afc9974e",
                "title": "GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph Question Answering",
                "abstract": "In this work, we present an end-to-end Knowledge Graph Question Answering (KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text pre-trained language model. The model takes a question in natural language as input and produces a simpler form of the intended SPARQL query. In the simpler form, the model does not directly produce entity and relation IDs. Instead, it produces corresponding entity and relation labels. The labels are grounded to KG entity and relation IDs in a subsequent step. To further improve the results, we instruct the model to produce a truncated version of the KG embedding for each entity. The truncated KG embedding enables a finer search for disambiguation purposes. We find that T5 is able to learn the truncated KG embeddings without any change of loss function, improving KGQA performance. As a result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata datasets on end-to-end KGQA over Wikidata.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "35635012",
                        "name": "Debayan Banerjee"
                    },
                    {
                        "authorId": "83623712",
                        "name": "Pranav Ajit Nair"
                    },
                    {
                        "authorId": "2370666",
                        "name": "Ricardo Usbeck"
                    },
                    {
                        "authorId": "66911936",
                        "name": "Chris Biemann"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some works [14, 16, 31, 41, 55, 65] use knowledge in KGs in an out-of-the-box manner.",
                "EmbedKGQA[41] uses trained ComplEx [50] embeddings to support the answer selection in question-answering systems."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c7f1c0330abe3506d98b4262899a60c9dc5db2b8",
                "externalIds": {
                    "DBLP": "conf/www/ZhangZCGHXSC23",
                    "ArXiv": "2303.03922",
                    "DOI": "10.1145/3543507.3583301",
                    "CorpusId": 257378588
                },
                "corpusId": 257378588,
                "publicationVenue": {
                    "id": "e07422f9-c065-40c3-a37b-75e98dce79fe",
                    "name": "The Web Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Web Conf",
                        "WWW"
                    ],
                    "url": "http://www.iw3c2.org/"
                },
                "url": "https://www.semanticscholar.org/paper/c7f1c0330abe3506d98b4262899a60c9dc5db2b8",
                "title": "Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer",
                "abstract": "Knowledge graphs (KG) are essential background knowledge providers in many tasks. When designing models for KG-related tasks, one of the key tasks is to devise the Knowledge Representation and Fusion (KRF) module that learns the representation of elements from KGs and fuses them with task representations. While due to the difference of KGs and perspectives to be considered during fusion across tasks, duplicate and ad hoc KRF modules design are conducted among tasks. In this paper, we propose a novel knowledge graph pretraining model KGTransformer that could serve as a uniform KRF module in diverse KG-related tasks. We pretrain KGTransformer with three self-supervised tasks with sampled sub-graphs as input. For utilization, we propose a general prompt-tuning mechanism regarding task data as a triple prompt to allow flexible interactions between task KGs and task data. We evaluate pretrained KGTransformer on three tasks, triple classification, zero-shot image classification, and question answering. KGTransformer consistently achieves better results than specifically designed task models. Through experiments, we justify that the pretrained KGTransformer could be used off the shelf as a general and effective KRF module across KG-related tasks. The code and datasets are available at https://github.com/zjukg/KGTransformer.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2155281129",
                        "name": "Wen Zhang"
                    },
                    {
                        "authorId": "2145160040",
                        "name": "Yushan Zhu"
                    },
                    {
                        "authorId": "48622851",
                        "name": "Mingyang Chen"
                    },
                    {
                        "authorId": "152683218",
                        "name": "Yuxia Geng"
                    },
                    {
                        "authorId": "2108692967",
                        "name": "Yufen Huang"
                    },
                    {
                        "authorId": "2156549998",
                        "name": "Yajing Xu"
                    },
                    {
                        "authorId": "2174740491",
                        "name": "Wenting Song"
                    },
                    {
                        "authorId": "14499025",
                        "name": "Hua-zeng Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For KGQA, these models include graph embeddings [9], [10], graph convolutional networks [11], and Deep Neural Approaches [12]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d0a49ac255d81e3c9d9173b8f804b038c908a2ed",
                "externalIds": {
                    "DBLP": "conf/iccae/MSAKCD23",
                    "DOI": "10.1109/ICCAE56788.2023.10111327",
                    "CorpusId": 258465609
                },
                "corpusId": 258465609,
                "publicationVenue": {
                    "id": "0a723d41-98f7-4257-a452-51655d56f982",
                    "name": "International Conference on Computer and Automation Engineering",
                    "type": "conference",
                    "alternate_names": [
                        "ICCAE",
                        "Int Conf Comput Autom Eng"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d0a49ac255d81e3c9d9173b8f804b038c908a2ed",
                "title": "A Dataset and Multi-task Multi-view Approach for Question-Answering with the Dual Perspectives of Text and Knowledge",
                "abstract": "Question-answering (QA) systems are important tools for extracting information from large datasets and providing accurate and relevant answers to user queries. Two of the most widely studied and built QA systems are Natural Language Question Answering (NLQA) and Knowledge Graph Question Answering (KGQA). NLQA relies on sequence learning algorithms, which have limitations on the length of input they can handle, while KGQA relies on the Subject-Predicate-Object (SPO) tuple representation of data, which may not always be available in the knowledge graph. In this paper, we present a novel approach for addressing these challenges by utilizing the structural information from the Knowledge Graph (KG) and the semantic information from the Natural Language Context. Due to the lack of a dataset to enable this approach, we propose the creation of a multi-view dataset - MTL-QA, specifically designed for multi-task learning. We also present a multi-task learning approach to jointly train NLQA and KGQA models and demonstrate the effectiveness on the proposed MTL-QA dataset.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2217700144",
                        "name": "MS Adithya"
                    },
                    {
                        "authorId": "2107701836",
                        "name": "M. Ahmed"
                    },
                    {
                        "authorId": "2216009752",
                        "name": "Mihir Madhusudan Kestur"
                    },
                    {
                        "authorId": "2216016216",
                        "name": "A. S. Chaithanya"
                    },
                    {
                        "authorId": "2187142562",
                        "name": "Bhaskarjyothi Das"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "817a8fcc05ef1117ace38ecc41702f0a8438a3ed",
                "externalIds": {
                    "ArXiv": "2303.02206",
                    "CorpusId": 257364865
                },
                "corpusId": 257364865,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/817a8fcc05ef1117ace38ecc41702f0a8438a3ed",
                "title": "Domain Specific Question Answering Over Knowledge Graphs Using Logical Programming and Large Language Models",
                "abstract": "Answering questions over domain-specific graphs requires a tailored approach due to the limited number of relations and the specific nature of the domain. Our approach integrates classic logical programming languages into large language models (LLMs), enabling the utilization of logical reasoning capabilities to tackle the KGQA task. By representing the questions as Prolog queries, which are readable and near close to natural language in representation, we facilitate the generation of programmatically derived answers. To validate the effectiveness of our approach, we evaluate it using a well-known benchmark dataset, MetaQA. Our experimental results demonstrate that our method achieves accurate identification of correct answer entities for all test questions, even when trained on a small fraction of annotated data. Overall, our work presents a promising approach to addressing question answering over domain-specific graphs, offering an explainable and robust solution by incorporating logical programming languages.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2210793915",
                        "name": "Navid Madani"
                    },
                    {
                        "authorId": "1748081",
                        "name": "R. Srihari"
                    },
                    {
                        "authorId": "3306556",
                        "name": "K. Joseph"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8ef722df973f16a88795cbe8178821d06684de81",
                "externalIds": {
                    "DBLP": "journals/ipm/BiNZHMZ0W23",
                    "DOI": "10.1016/j.ipm.2022.103242",
                    "CorpusId": 255050487
                },
                "corpusId": 255050487,
                "publicationVenue": {
                    "id": "37f5b9b7-f828-4ae1-a174-45b538cbd4e4",
                    "name": "Information Processing & Management",
                    "type": "journal",
                    "alternate_names": [
                        "Inf Process Manag",
                        "Inf Process  Manag",
                        "Information Processing and Management"
                    ],
                    "issn": "0306-4573",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/244/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/information-processing-and-management/",
                        "http://www.sciencedirect.com/science/journal/03064573",
                        "http://www.journals.elsevier.com/information-processing-and-management/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8ef722df973f16a88795cbe8178821d06684de81",
                "title": "Boosting question answering over knowledge graph with reward integration and policy evaluation under weak supervision",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "48486326",
                        "name": "Xin Bi"
                    },
                    {
                        "authorId": "2064596283",
                        "name": "H. Nie"
                    },
                    {
                        "authorId": "2197928314",
                        "name": "Guoliang Zhang"
                    },
                    {
                        "authorId": "2136117186",
                        "name": "Lei Hu"
                    },
                    {
                        "authorId": "150350246",
                        "name": "Yuliang Ma"
                    },
                    {
                        "authorId": "2749153",
                        "name": "Xiangguo Zhao"
                    },
                    {
                        "authorId": "14886336",
                        "name": "Ye Yuan"
                    },
                    {
                        "authorId": "8349792",
                        "name": "Guoren Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Our two base-size models outperform EmbedKGQA by approximately 6% hits@1, an English-as-pivot baseline that utilizes RoBERTabase and the KB embedding ComplEx (Trouillon et al., 2016).",
                "0 License for EmbedKGQA, BSD2-Clause License for GraftNet, and Apache-2.",
                "For information extraction style, we select EmbedKGQA (Saxena et al., 2020), GraftNet (Sun et al.",
                "Following previous works (Sun et al., 2018; Saxena et al., 2020), we further prune it to contain only those relations that are mentioned in the dataset.",
                "For information extraction style, we select EmbedKGQA (Saxena et al., 2020), GraftNet (Sun et al., 2018), NSM (with its teacherstudent variant, He et al., 2021), all of which re-\nquire no annotation of structured KB queries, as our method does.",
                "Following previous works (Sun et al., 2018; Saxena et al., 2020; He et al., 2021), we use the golden topic entities for a fair comparison with the baselines.",
                "\u2026in KBQA generally fall into two main paradigms, either the information extraction style (Miller et al., 2016; Sun et al., 2018; Xu et al., 2019; Saxena et al., 2020; He et al., 2021; Shi et al., 2021) or the semantic parsing style (Yih et al., 2015; Lan and Jiang, 2020; Ye et al., 2022; Gu and\u2026",
                "Following previous works (Saxena et al., 2020; He et al., 2021), we use hits@1 as the evaluation metric.",
                "Specifically, we first identify the topic entity from the given question, link it to the KB, and extract its n-order neighbors to construct a KB subgraph, following traditional monolingual KBQA methods (Saxena et al., 2020; He et al., 2021).",
                "With only 10% of the training data, i.e., 310 instances, our models reach over 62% hits@1, comparable with EmbedKGQA trained with full training data.",
                "KBQA Recent efforts in KBQA generally fall into two main paradigms, either the information extraction style (Miller et al., 2016; Sun et al., 2018; Xu et al., 2019; Saxena et al., 2020; He et al., 2021; Shi et al., 2021) or the semantic parsing style (Yih et al."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "c3eeaf7744b7f6aa7bbe4cddbb210d4fb957b31e",
                "externalIds": {
                    "ACL": "2023.findings-eacl.185",
                    "ArXiv": "2302.13241",
                    "DBLP": "journals/corr/abs-2302-13241",
                    "DOI": "10.48550/arXiv.2302.13241",
                    "CorpusId": 257219401
                },
                "corpusId": 257219401,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/c3eeaf7744b7f6aa7bbe4cddbb210d4fb957b31e",
                "title": "Cross-Lingual Question Answering over Knowledge Base as Reading Comprehension",
                "abstract": "Although many large-scale knowledge bases (KBs) claim to contain multilingual information, their support for many non-English languages is often incomplete. This incompleteness gives birth to the task of cross-lingual question answering over knowledge base (xKBQA), which aims to answer questions in languages different from that of the provided KB. One of the major challenges facing xKBQA is the high cost of data annotation, leading to limited resources available for further exploration. Another challenge is mapping KB schemas and natural language expressions in the questions under cross-lingual settings. In this paper, we propose a novel approach for xKBQA in a reading comprehension paradigm. We convert KB subgraphs into passages to narrow the gap between KB schemas and questions, which enables our model to benefit from recent advances in multilingual pre-trained language models (MPLMs) and cross-lingual machine reading comprehension (xMRC). Specifically, we use MPLMs, with considerable knowledge of cross-lingual mappings, for cross-lingual reading comprehension. Existing high-quality xMRC datasets can be further utilized to finetune our model, greatly alleviating the data scarcity issue in xKBQA. Extensive experiments on two xKBQA datasets in 12 languages show that our approach outperforms various baselines and achieves strong few-shot and zero-shot performance. Our dataset and code are released for further research.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2111574159",
                        "name": "Chen Zhang"
                    },
                    {
                        "authorId": "7827757",
                        "name": "Yuxuan Lai"
                    },
                    {
                        "authorId": "2115387922",
                        "name": "Yansong Feng"
                    },
                    {
                        "authorId": "145781166",
                        "name": "Xingyu Shen"
                    },
                    {
                        "authorId": "144549196",
                        "name": "Haowei Du"
                    },
                    {
                        "authorId": "144060462",
                        "name": "Dongyan Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", T-EaE-add and T-EaE-replace), EmbedKGQA [13] and CronKGQA [5].",
                "Knowledge graph embedding (KGE) [6, 7, 8] aims to embed entities and relationships into a low-dimensional continuous vector space, thus facilitating downstream tasks like knowledge graph completion [9, 10], relation extraction and classification [11, 12, 13] and semantic matching [14, 15, 16, 17]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4aed81cf00b09893dfb18e0089fa2773a558278b",
                "externalIds": {
                    "ArXiv": "2302.12529",
                    "DBLP": "journals/corr/abs-2302-12529",
                    "DOI": "10.48550/arXiv.2302.12529",
                    "CorpusId": 257205845
                },
                "corpusId": 257205845,
                "publicationVenue": {
                    "id": "0d6f7fba-7092-46b3-8039-93458dba736b",
                    "name": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Acoust Speech Signal Process",
                        "IEEE Int Conf Acoust Speech Signal Process",
                        "ICASSP",
                        "International Conference on Acoustics, Speech, and Signal Processing"
                    ],
                    "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000002"
                },
                "url": "https://www.semanticscholar.org/paper/4aed81cf00b09893dfb18e0089fa2773a558278b",
                "title": "Time-aware Multiway Adaptive Fusion Network for Temporal Knowledge Graph Question Answering",
                "abstract": "Knowledge graphs (KGs) have received increasing attention due to its wide applications on natural language processing. However, its use case on temporal question answering (QA) has not been well-explored. Most of existing methods are developed based on pre-trained language models, which might not be capable to learn \\emph{temporal-specific} presentations of entities in terms of temporal KGQA task. To alleviate this problem, we propose a novel \\textbf{T}ime-aware \\textbf{M}ultiway \\textbf{A}daptive (\\textbf{TMA}) fusion network. Inspired by the step-by-step reasoning behavior of humans. For each given question, TMA first extracts the relevant concepts from the KG, and then feeds them into a multiway adaptive module to produce a \\emph{temporal-specific} representation of the question. This representation can be incorporated with the pre-trained KG embedding to generate the final prediction. Empirical results verify that the proposed model achieves better performance than the state-of-the-art models in the benchmark dataset. Notably, the Hits@1 and Hits@10 results of TMA on the CronQuestions dataset's complex questions are absolutely improved by 24\\% and 10\\% compared to the best-performing baseline. Furthermore, we also show that TMA employing an adaptive fusion mechanism can provide interpretability by analyzing the proportion of information in question representations.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108833052",
                        "name": "Yonghao Liu"
                    },
                    {
                        "authorId": "2113983954",
                        "name": "Di Liang"
                    },
                    {
                        "authorId": "36595248",
                        "name": "Fang Fang"
                    },
                    {
                        "authorId": "2592528",
                        "name": "Sirui Wang"
                    },
                    {
                        "authorId": "2144356294",
                        "name": "Wei Wu"
                    },
                    {
                        "authorId": "2195212886",
                        "name": "Rui Jiang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent research has demonstrated that in addition to the input, adding other relevant knowledge to the model can improve the model\u2019s performance to variable degrees, such as reading comprehension [10], text classifcation [11], natural language inference [12], knowledge acquisition [13], and question answer [14]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8d03e0f4065bd13ae51dea9726441649360ea60a",
                "externalIds": {
                    "PubMedCentral": "9908338",
                    "DOI": "10.1155/2023/2088698",
                    "CorpusId": 256567406,
                    "PubMed": "36777631"
                },
                "corpusId": 256567406,
                "publicationVenue": {
                    "id": "8bbffbf0-5dfc-4485-b4b1-d177fd330b21",
                    "name": "Evidence-Based Complementary and Alternative Medicine",
                    "type": "journal",
                    "alternate_names": [
                        "Evidence-based Complement Altern Med",
                        "Evidence-based Complementary and Alternative Medicine"
                    ],
                    "issn": "1741-427X",
                    "url": "https://www.hindawi.com/journals/ecam/",
                    "alternate_urls": [
                        "http://www.hindawi.com/journals/ecam/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8d03e0f4065bd13ae51dea9726441649360ea60a",
                "title": "Combining the External Medical Knowledge Graph Embedding to Improve the Performance of Syndrome Differentiation Model",
                "abstract": "The electronic medical records (EMRs) of traditional Chinese medicine (TCM) include a wealth of TCM knowledge and syndrome diagnosis information, which is crucial for improving the quality of TCM auxiliary decision-making. In practical diagnosis, one disease corresponds to one syndrome, posing considerable hurdles for the informatization of TCM. The purpose of this work was to create an end-to-end TCM diagnostic model, and the knowledge graph (KG) created in this article is used to improve the model's information and realize auxiliary decision-making for TCM disorders. We approached auxiliary decision-making for syndrome differentiation in this article as a multilabel classification task and presented a knowledge-based decision support model for syndrome differentiation (KDSD). Specifically, we created a KG based on TCM features (TCMKG), supplementing the textual representation of medical data with embedded information. Finally, we proposed fusing medical text with KG entity representation (F-MT-KER) to get prediction results using a linear output layer. After obtaining the vector representation of the medical record text using the BERT model, the vector representation of various KG embedded models can provide additional hidden information to a certain extent. Experimental results show that our method improves by 1% (P@1) on the syndrome differentiation auxiliary decision task compared to the baseline model BERT. The usage of EMRs can aid TCM development more efficiently. With the help of entity level representation, character level representation, and model fusion, the multilabel classification method based on the pretraining model and KG can better simulate the TCM syndrome differentiation of the complex cases.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "50540226",
                        "name": "Qing Ye"
                    },
                    {
                        "authorId": "2152903765",
                        "name": "Rui Yang"
                    },
                    {
                        "authorId": "2116908665",
                        "name": "Chun-lei Cheng"
                    },
                    {
                        "authorId": "2204364310",
                        "name": "Lin Peng"
                    },
                    {
                        "authorId": "2154134662",
                        "name": "Yong Lan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "There are not a large number of reciprocal triples in the subset, which can more accurately evaluate the performance of the model [19]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ba59ceb9b65342dab914abb3f007dca7a00195c4",
                "externalIds": {
                    "DOI": "10.1109/DSDE58527.2023.00025",
                    "CorpusId": 260742973
                },
                "corpusId": 260742973,
                "publicationVenue": {
                    "id": "5f2cd087-8b1f-4fb9-9a9a-fde57d3cc8aa",
                    "name": "Data Storage and Data Engineering",
                    "type": "conference",
                    "alternate_names": [
                        "Data Storage Data Eng",
                        "DSDE"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=763"
                },
                "url": "https://www.semanticscholar.org/paper/ba59ceb9b65342dab914abb3f007dca7a00195c4",
                "title": "Optimization Model of Knowledge Graph Reasoning Process Based on Generative Adversarial Network",
                "abstract": "The knowledge map for most of the real world is incomplete, that is, there are problems of missing real facts and containing false facts. In recent years, most of the work, such as ConvE and TransE, reasoned the knowledge map by querying the implicit knowledge related to rules or based on the path, but the inference process was affected by large-scale long-distance and complex relationships, which led to the lack of interpretability and low training efficiency of the reasoning process. This paper proposed the optimization method of the reasoning process of the knowledge map based on the confrontation network, GAPO, The R-GCN auxiliary network is introduced into the GAN network to generate mixed data with high confidence as far as possible during the period of generating negative sample data, so as to improve the discriminator's ability to distinguish true and false triple facts. At the same time, reinforcement learning algorithm is introduced to treat the reasoning process of knowledge atlas as state space, and hierarchical information is used to ensure the reliability and authenticity of the reasoning link. The obtained hierarchical information data improves the interpretability of the reasoning process to a certain extent. The experiment shows that GAPO model has better performance than ConvE and TransE in reasoning, which proves that it is effective.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2229583211",
                        "name": "Yongyang Wang"
                    },
                    {
                        "authorId": "1768366",
                        "name": "Yongguo Han"
                    },
                    {
                        "authorId": "2176092697",
                        "name": "Jing Liao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To a great extent, researchers focus on the synergy of Knowledge Graph and Deep Learning (Miller et al., 2016a; Saxena et al., 2020, 2022)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "507e6c5e750703f147aa2f06fb651f72bdbee216",
                "externalIds": {
                    "ACL": "2022.icon-main.26",
                    "ArXiv": "2301.04013",
                    "DBLP": "journals/corr/abs-2301-04013",
                    "DOI": "10.48550/arXiv.2301.04013",
                    "CorpusId": 255570200
                },
                "corpusId": 255570200,
                "publicationVenue": {
                    "id": "8a6e871b-6c73-419c-98a8-27e437270a12",
                    "name": "ICON",
                    "type": "conference",
                    "alternate_names": [
                        "ICNLP",
                        "Int conf nat lang process",
                        "International conference natural language processing",
                        "Int Conf Nat Lang Process",
                        "TAL",
                        "IEEE International Conference on Networks",
                        "IEEE Int Conf Netw",
                        "International Conference on Natural Language Processing"
                    ],
                    "issn": "1361-8113",
                    "url": "http://www.icohtec.org/publications-icon.html",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/conhome/1000494/all-proceedings",
                        "http://www.wikicfp.com/cfp/program?id=1360",
                        "http://www.jstor.org/journal/icon"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/507e6c5e750703f147aa2f06fb651f72bdbee216",
                "title": "There is No Big Brother or Small Brother:Knowledge Infusion in Language Models for Link Prediction and Question Answering",
                "abstract": "The integration of knowledge graphs with deep learning is thriving in improving the performance of various natural language processing (NLP) tasks. In this paper, we focus on knowledge-infused link prediction and question answering using language models, T5, and BLOOM across three domains:Aviation, Movie, and Web. In this context, we infuse knowledge in large and small language models and study their performance, and find the performance to be similar. For the link prediction task on the Aviation Knowledge Graph, we obtain a 0.2 hits@1 score using T5-small, T5-base, T5-large, and BLOOM. Using template-based scripts, we create a set of 1 million synthetic factoid QA pairs in the aviation domain from National Transportation Safety Board (NTSB) reports. On our curated QA pairs, the three models of T5 achieve a 0.7 hits@1 score. We validate our findings with the paired student t test and Cohen\u2019s kappa scores. For link prediction on Aviation Knowledge Graph using T5-small and T5-large, we obtain a Cohen\u2019s kappa score of 0.76, showing substantial agreement between the models. Thus, we infer that small language models perform similar to large language models with the infusion of knowledge.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2199840769",
                        "name": "Ankush Agarwal"
                    },
                    {
                        "authorId": "2199837969",
                        "name": "Sakharam Gawade"
                    },
                    {
                        "authorId": "2199840241",
                        "name": "Sachin Channabasavarajendra"
                    },
                    {
                        "authorId": "145532184",
                        "name": "P. Bhattacharyya"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "06b3dfdc0f1c02ed8b14c85aa6a25d26bb3fabfb",
                "externalIds": {
                    "DOI": "10.3934/mbe.2023228",
                    "CorpusId": 255705477,
                    "PubMed": "36896529"
                },
                "corpusId": 255705477,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/06b3dfdc0f1c02ed8b14c85aa6a25d26bb3fabfb",
                "title": "Dual-process system based on mixed semantic fusion for Chinese medical knowledge-based question answering.",
                "abstract": "Chinese medical knowledge-based question answering (cMed-KBQA) is a vital component of the intelligence question-answering assignment. Its purpose is to enable the model to comprehend questions and then deduce the proper answer from the knowledge base. Previous methods solely considered how questions and knowledge base paths were represented, disregarding their significance. Due to entity and path sparsity, the performance of question and answer cannot be effectively enhanced. To address this challenge, this paper presents a structured methodology for the cMed-KBQA based on the cognitive science dual systems theory by synchronizing an observation stage (System 1) and an expressive reasoning stage (System 2). System 1 learns the question's representation and queries the associated simple path. Then System 2 retrieves complicated paths for the question from the knowledge base by using the simple path provided by System 1. Specifically, System 1 is implemented by the entity extraction module, entity linking module, simple path retrieval module, and simple path-matching model. Meanwhile, System 2 is performed by using the complex path retrieval module and complex path-matching model. The public CKBQA2019 and CKBQA2020 datasets were extensively studied to evaluate the suggested technique. Using the metric average F1-score, our model achieved 78.12% on CKBQA2019 and 86.60% on CKBQA2020.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2118662979",
                        "name": "Meiling Wang"
                    },
                    {
                        "authorId": "2146255",
                        "name": "Xiaohai He"
                    },
                    {
                        "authorId": "2191386549",
                        "name": "Zhao Zhang"
                    },
                    {
                        "authorId": "49480298",
                        "name": "Luping Liu"
                    },
                    {
                        "authorId": "3334291",
                        "name": "L. Qing"
                    },
                    {
                        "authorId": "47909531",
                        "name": "Yan Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some researches adopt KG embedding to solve the sparsity problem [6].",
                "EmbedKGQA[6]uses an embedding-based approach to perform multi-hop reasoning by matching pre-trained entity embeddings with problem embeddings obtained by RoBERTa[22].",
                "Some researches use KG embedding models for link prediction to improve the performance of multi-hop KBQA[6] , however, the incompleteness of the knowledge graph will lead to the occurrence of super nodes, resulting in the model not being well generalized.",
                "The incomplete knowledge graphs is simulated by randomly deleting half of triples in the knowledge graph[6], and then experimenting with different baseline models on it, as shown in table 3."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "cf4b5f00a1a0425cc4b9a9e246175a87e849359f",
                "externalIds": {
                    "DOI": "10.1088/1742-6596/2424/1/012027",
                    "CorpusId": 255871907
                },
                "corpusId": 255871907,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/cf4b5f00a1a0425cc4b9a9e246175a87e849359f",
                "title": "TrEKBQA:Traversing Knowledge Graph Embedding for Multi-hop Knowledge Base Question Answering",
                "abstract": "Recent research apply KG embedding to multi-hop Knowledge Base Question Answering(KBQA) to predict missing links, however, it is often affected by the skewed distribution of nodes in the knowledge graph, resulting in poor generalization of the model. Therefore, we propose a method TrEKBQA based on traversing the knowledge graph embedding space for multi-hop KBQA, which performs path traversal in the KG embedding space instead of KG itself for link prediction to complete the knowledge graph, thus improving the accuracy of multi-hop KBQA.TrEKBQA model complex relationships using correlations between individual links and longer paths connecting the same pair of entities to traverse the KG embedding space to mitigate the effects of biased distribution of nodes and improve the performance of link prediction. In the pre-processing process, TrEKBQA uses the PRN algorithm to extract subgraphs related to the problem entity to reduce the number of target entities. Through experiments on multiple benchmark datasets, we demonstrate the effectiveness of TrEKBQA on KBQA tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2553072",
                        "name": "Xiujin Shi"
                    },
                    {
                        "authorId": "2117230219",
                        "name": "Jun Hu"
                    },
                    {
                        "authorId": "2201040379",
                        "name": "Naiwen Sun"
                    },
                    {
                        "authorId": "2116696147",
                        "name": "Shoujian Yu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bbe727e84ad87a55224664ca3d423834f3774f21",
                "externalIds": {
                    "DBLP": "journals/isci/LiMYZYLDY23",
                    "DOI": "10.1016/j.ins.2023.01.113",
                    "CorpusId": 256480324
                },
                "corpusId": 256480324,
                "publicationVenue": {
                    "id": "e46002a1-d7a6-4681-aae9-36bc3a6a1f93",
                    "name": "Information Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Information Scientist",
                        "Inf Sci"
                    ],
                    "issn": "0020-0255",
                    "alternate_issns": [
                        "0020-0263"
                    ],
                    "url": "http://www.sciencedirect.com/science/journal/00200255"
                },
                "url": "https://www.semanticscholar.org/paper/bbe727e84ad87a55224664ca3d423834f3774f21",
                "title": "A structure-enhanced generative adversarial network for knowledge graph zero-shot relational learning",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "50079136",
                        "name": "Xuewei Li"
                    },
                    {
                        "authorId": "49304761",
                        "name": "Jinming Ma"
                    },
                    {
                        "authorId": "2155521320",
                        "name": "Jian Yu"
                    },
                    {
                        "authorId": "2466436",
                        "name": "Mankun Zhao"
                    },
                    {
                        "authorId": "47730694",
                        "name": "Mei Yu"
                    },
                    {
                        "authorId": "2115371085",
                        "name": "Hongwei Liu"
                    },
                    {
                        "authorId": "2190053827",
                        "name": "Weiping Ding"
                    },
                    {
                        "authorId": "2805420",
                        "name": "Ruiguo Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These KGs have been applied in various NLP tasks and achieved excellent results, such as semantic parsing [6], knowledge answering [16] and named entity disambiguation [24]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f122d3e499001c8e49c6974f1d0bbafa2dfcc7b4",
                "externalIds": {
                    "DBLP": "conf/mlnlp/LiuWZD22",
                    "DOI": "10.1145/3578741.3578815",
                    "CorpusId": 257366749
                },
                "corpusId": 257366749,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f122d3e499001c8e49c6974f1d0bbafa2dfcc7b4",
                "title": "MAKE:Knowledge Graph Embedding via Multi-Attention neural network",
                "abstract": "Knowledge graph embedding is a popular method to solve the incompleteness of knowledge graph. At present, research on knowledge graph embedding based on neural network has achieved remarkable results, but most models ignore the influence of the correlation among with subject entity and relation and object entity within triple. Existing attention-based models take into account the effect of correlation, but perform moderately. In this paper, we propose a multi-attention neural network-based embedding model, named MAKE, which utilizes a novel multi-attention mechanism to generate feature maps of triples by computing correlations within triple. To fully exploit the performance of the multi-attention mechanism, MAKE uses a trainable batch normalization method and a novel composite loss function to improve the model learning ability. Evaluation results on FB15K-237 and WN18RR standard datasets show that our MAKE achieves better performance than previous state-of-the-art knowledge graph embedding models.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2183432560",
                        "name": "Denghui Liu"
                    },
                    {
                        "authorId": "2108837681",
                        "name": "Yanna Wang"
                    },
                    {
                        "authorId": "2109495370",
                        "name": "Zili Zhou"
                    },
                    {
                        "authorId": "2174507",
                        "name": "Zhaoan Dong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "While our work is motivated by these, the nature of unanswerable questions is very different for KBs compared to unstructured contexts.",
                "Prior work on QA\nover incomplete KBs has explored algorithms for dropping facts from KBs (Saxena et al., 2020; Thai et al., 2022).",
                "There is work on improving accuracy of QA over incomplete KBs (Thai et al., 2022; Saxena et al., 2020), but these do not address answerability.",
                "Since these generate logical forms, we expect these to be more robust to data level incompleteness than purely retrieval-based approaches (Saxena et al., 2020; Das et al., 2021; Zhang et al., 2022; Mitra et al., 2022; Wang et al., 2022b).",
                "This problem arises for KBs due to various reasons, including schema-level and data-level incompleteness of KBs (Min et al., 2013), limited KB scope, questions with false premises, etc.",
                "Retrieval based approaches (Saxena et al., 2020; Zhang et al., 2022; Mitra et al., 2022; Wang et al., 2022b; Das et al., 2022) learn to identify paths in the KB starting from entities mentioned in the question, and then score and analyze these paths to directly retrieve the answer.",
                "Retrieval based approaches (Saxena et al., 2020; Zhang et al., 2022; Mitra et al., 2022; Wang et al., 2022b; Das et al., 2022) learn to identify paths in the KB starting from entities mentioned in the question, and then",
                "The problem of natural language question answering over knowledge bases (KBQA) has received a lot of interest in recent years (Saxena et al., 2020; Zhang et al., 2022; Mitra et al., 2022; Wang et al., 2022b; Das et al., 2022; Cao et al., 2022c; Ye et al., 2022; Chen et al., 2021; Das et al., 2021)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "5182d7994d3712e9390c2642c90f79f9814c6b4d",
                "externalIds": {
                    "ArXiv": "2212.10189",
                    "DBLP": "conf/acl/PatidarFSVBM23",
                    "ACL": "2023.acl-long.576",
                    "DOI": "10.48550/arXiv.2212.10189",
                    "CorpusId": 254877654
                },
                "corpusId": 254877654,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/5182d7994d3712e9390c2642c90f79f9814c6b4d",
                "title": "Do I have the Knowledge to Answer? Investigating Answerability of Knowledge Base Questions",
                "abstract": "When answering natural language questions over knowledge bases, missing facts, incomplete schema and limited scope naturally lead to many questions being unanswerable. While answerability has been explored in other QA settings, it has not been studied for QA over knowledge bases (KBQA). We create GrailQAbility, a new benchmark KBQA dataset with unanswerability, by first identifying various forms of KB incompleteness that make questions unanswerable, and then systematically adapting GrailQA (a popular KBQA dataset with only answerable questions). Experimenting with three state-of-the-art KBQA models, we find that all three models suffer a drop in performance even after suitable adaptation for unanswerable questions. In addition, these often detect unanswerability for wrong reasons and find specific forms of unanswerability particularly difficult to handle. This underscores the need for further research in making KBQA systems robust to unanswerability.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "34823900",
                        "name": "Mayur Patidar"
                    },
                    {
                        "authorId": "2144286398",
                        "name": "Avinash Kumar Singh"
                    },
                    {
                        "authorId": "1382097940",
                        "name": "Prayushi Faldu"
                    },
                    {
                        "authorId": "3213990",
                        "name": "L. Vig"
                    },
                    {
                        "authorId": "145963427",
                        "name": "Indrajit Bhattacharya"
                    },
                    {
                        "authorId": "2674444",
                        "name": "Mausam"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "600a11a9113e6d87c77c4d4d6ccd7008b6fa1a12",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-09724",
                    "ArXiv": "2212.09724",
                    "DOI": "10.48550/arXiv.2212.09724",
                    "CorpusId": 254853846
                },
                "corpusId": 254853846,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/600a11a9113e6d87c77c4d4d6ccd7008b6fa1a12",
                "title": "A Retrieve-and-Read Framework for Knowledge Graph Link Prediction",
                "abstract": "Knowledge graph (KG) link prediction aims to infer new facts based on existing facts in the KG. Recent studies have shown that using the graph neighborhood of a node via graph neural networks (GNNs) provides more useful information compared to just using the query information. Conventional GNNs for KG link prediction follow the standard message-passing paradigm on the entire KG, which leads to over-smoothing of representations and also limits their scalability. On a large scale, it becomes computationally expensive to aggregate useful information from the entire KG for inference. To address the limitations of existing KG link prediction frameworks, we propose a novel retrieve-and-read framework, which first retrieves a relevant subgraph context for the query and then jointly reasons over the context and the query with a high-capacity reader. As part of our exemplar instantiation for the new framework, we propose a novel Transformer-based GNN as the reader, which incorporates graph-based attention structure and cross-attention between query and context for deep fusion. This design enables the model to focus on salient context information relevant to the query. Empirical results on two standard KG link prediction datasets demonstrate the competitive performance of the proposed method.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "7591930",
                        "name": "Vardaan Pahuja"
                    },
                    {
                        "authorId": "7425689",
                        "name": "Boshi Wang"
                    },
                    {
                        "authorId": "3422205",
                        "name": "Hugo Latapie"
                    },
                    {
                        "authorId": "2089885442",
                        "name": "Jayanth Srinivasa"
                    },
                    {
                        "authorId": "1758652",
                        "name": "Yu Su"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other retrieval methods use graph-based knowledge along with transformer models to find multi-hop reasoning paths [20] [21]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2012877f69aa01ad5d8c7283154c14a84aeaf7ad",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2302-06854",
                    "ArXiv": "2302.06854",
                    "DOI": "10.1109/BigData55660.2022.10020725",
                    "CorpusId": 256317712
                },
                "corpusId": 256317712,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2012877f69aa01ad5d8c7283154c14a84aeaf7ad",
                "title": "Large-Scale Knowledge Synthesis and Complex Information Retrieval from Biomedical Documents",
                "abstract": "Recent advances in the healthcare industry have led to an abundance of unstructured data, making it challenging to perform tasks such as efficient and accurate information retrieval at scale. Our work offers an all-in-one scalable solution for extracting and exploring complex information from large-scale research documents, which would otherwise be tedious. First, we briefly explain our knowledge synthesis process to extract helpful information from unstructured text data of research documents. Then, on top of the knowledge extracted from the documents, we perform complex information retrieval using three major components- Paragraph Retrieval, Triplet Retrieval from Knowledge Graphs, and Complex Question Answering (QA). These components combine lexical and semantic-based methods to retrieve paragraphs and triplets and perform faceted refinement for filtering these search results. The complexity of biomedical queries and documents necessitates using a QA system capable of handling queries more complex than factoid queries, which we evaluate qualitatively on the COVID-19 Open Research Dataset (CORD-19) to demonstrate the effectiveness and value-add.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2184792648",
                        "name": "Shreya Saxena"
                    },
                    {
                        "authorId": "87374744",
                        "name": "Rajini Sangani"
                    },
                    {
                        "authorId": "2072410693",
                        "name": "Siva Prasad"
                    },
                    {
                        "authorId": "2202988716",
                        "name": "Shubham Kumar"
                    },
                    {
                        "authorId": "74689878",
                        "name": "Mihir Athale"
                    },
                    {
                        "authorId": "2127376427",
                        "name": "Rohan Awhad"
                    },
                    {
                        "authorId": "1419986651",
                        "name": "Vishal Vaddina"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We focus our evaluation on EmbedKGQA [23], an approach that combines graph embeddings and reasoning using graph traversal.",
                "The EmbedKGQA framework has two training stages: (i) train the knowledge graph embedding on the link prediction task; (ii) train the model for QA by leveraging the pretrained embeddings."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dcb17d546ce8aec11174bafad2fb3d913e6b2e98",
                "externalIds": {
                    "DBLP": "conf/bigdataconf/YuMD22",
                    "DOI": "10.1109/BigData55660.2022.10021012",
                    "CorpusId": 256320302
                },
                "corpusId": 256320302,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/dcb17d546ce8aec11174bafad2fb3d913e6b2e98",
                "title": "CrunchQA: A Synthetic Dataset for Question Answering over Crunchbase Knowledge Graph",
                "abstract": "The digital transformation in the finance and enterprise sector has been driven by the advances made in big data and artificial intelligence technologies. For instance, data integration enables businesses to make better decisions by consolidating and mining heterogeneous data repositories. In particular, knowledge graphs (KGs) are used to facilitate the integration of disparate data sources and can be utilized to answer complex queries. This work proposes a new dataset for question-answering on knowledge graphs (KGQA) to reflect the challenges we identified in real-world applications which are not covered by existing benchmarks, namely, multi-hop constraints, numeric and literal embeddings, ranking, reification, and hyper-relations. To build the dataset, we create a new Knowledge Graph from the Crunchbase database using a lightweight schema to support high-quality entity embeddings in large graphs. Next, we create a Question Answering dataset based on natural language question generation using predefined multiple-hop templates and paraphrasing. Finally, we conduct extensive experiments with state-of-the-art KGQA models and compare their performance on CrunchQA. The results show that the existing models do not perform well, for example, on multi-hop constrained queries. Hence, CrunchQA can be used as a challenging benchmark dataset for future KGQA reasoning models. The dataset and scripts are available on the project repository. 1",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2008878271",
                        "name": "Lifan Yu"
                    },
                    {
                        "authorId": "151151266",
                        "name": "Nadya Abdel Madjid"
                    },
                    {
                        "authorId": "1695677",
                        "name": "D. Difallah"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Depending on the technical paradigm, they can be divided into the traditional neural network model based on key-value pair modeling [11], and the emerging graph neural network model[12].",
                "In contrast, the reinforcement learning-based SRN and the pre-trained modelbased word vector matching EmbedKGQA possess strong antiinterference capability.",
                "BOLDED FONT INDICATES THE BEST PERFORMING METHOD\nModels Hits@1\nGraftNet 73.2\nSRN 74.1 EmbedKGQA 74.4\nNSM 78.7\nIR-MH 80.1\nRESULTS ANALYSIS: Table 2 shows the comparison findings of several multi-hop knowledge base approaches Q&A. Table 2 presents a comparison of experimental data, it can be observed that GraftNet performs the worst, which may be because the knowledge graph of the integrated energy service domain for which this paper is oriented contains very sufficient domain knowledge, in other words, the retrieved subgraphs contain a large number of candidate entities, the quantity of noisy entities is enormous compared to the original problem leading to interference in the performance of the graph based convolutional neural network model.",
                "(4) EmbedKGQA[12]: Multi-hop inference by matching the pre-processed entity codes in the knowledge graph with the question codes obtained from RoBERTa."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "0e2300459bec181bde3dbfe81074c199e0adc38a",
                "externalIds": {
                    "DBLP": "conf/icsai/ZhangSWZWL22",
                    "DOI": "10.1109/ICSAI57119.2022.10005492",
                    "CorpusId": 255599003
                },
                "corpusId": 255599003,
                "publicationVenue": {
                    "id": "8a31511d-7523-41b6-8d81-565373827b19",
                    "name": "International Conference on Systems and Informatics",
                    "type": "conference",
                    "alternate_names": [
                        "ICSAI",
                        "Int Conf Syst Informatics"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0e2300459bec181bde3dbfe81074c199e0adc38a",
                "title": "Multi-hop Knowledge Base Q&A in Integrated Energy Services Based on Intermediate Reasoning Attention",
                "abstract": "Knowledge base with multiple hops quizzing aims to discover the subject entity in a question at a distance from the knowledge base\u2019s answer entity for multiple hops. The lack of supervised signals for the intermediate phases of multi-hop inference, which leaves a model only able to get input on the final output, is a significant difficulty for the study, where the inference instructions for the intermediate steps cannot be effectively optimized and the forward propagation of inference states is weakened. Most of the existing research approaches use global attention to motivate the model to learn the inference instructions of each hop, which has been shown to fail to achieve effective performance in weakly supervised tasks. To address this challenge, this paper proposes an intermediate inference attention mechanism to handle multi-hop knowledge base quizzing tasks. Inspired by the human execution of multi-hop quizzing where each hop question is influenced by the previous hop answer, in this approach, the model pays more attention to the inference state generated by the previous hop inference instruction when generating each hop inference instruction, prompting a close interaction between the inference state of the intermediate step and the inference instruction, and providing effective attentional feedback for the optimization of the intermediate step inference instruction. On the KBQA dataset in the integrated energy service domain, which is self-constructed in this research, we conduct comprehensive comparison experiments. The findings suggest that the technique we provided achieves optimum performance in this study.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108185821",
                        "name": "Wenbin Zhang"
                    },
                    {
                        "authorId": "1931391",
                        "name": "Jiaju She"
                    },
                    {
                        "authorId": "2202919679",
                        "name": "Yingqiu Wang"
                    },
                    {
                        "authorId": "2200056085",
                        "name": "Meng Zhao"
                    },
                    {
                        "authorId": "2154459373",
                        "name": "Yi Wang"
                    },
                    {
                        "authorId": "2200401753",
                        "name": "Chao Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "texts, relation extraction (RE) [1, 2] has served as an auxiliary technology for some downstream tasks, such as knowledge construction [3, 4] and question answering [5, 6]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4df1af4b1d25373c77c51e724cc80716b36aecb0",
                "externalIds": {
                    "DBLP": "journals/nca/WenZZ23",
                    "DOI": "10.1007/s00521-022-08051-1",
                    "CorpusId": 254443346
                },
                "corpusId": 254443346,
                "publicationVenue": {
                    "id": "702e18c0-c8c6-4800-a398-42aa159394d1",
                    "name": "Neural computing & applications (Print)",
                    "type": "journal",
                    "alternate_names": [
                        "Neural comput  appl (print",
                        "Neural Comput Appl",
                        "Neural Computing and Applications"
                    ],
                    "issn": "0941-0643",
                    "url": "https://link.springer.com/journal/521"
                },
                "url": "https://www.semanticscholar.org/paper/4df1af4b1d25373c77c51e724cc80716b36aecb0",
                "title": "Improving distant supervision relation extraction with entity-guided enhancement feature",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2075349460",
                        "name": "Haixu Wen"
                    },
                    {
                        "authorId": "2145144877",
                        "name": "Xinhua Zhu"
                    },
                    {
                        "authorId": "2109006376",
                        "name": "Lanfang Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Such a retrieval-and-reasoning paradigm has shown superiority over directly reasoning on the entire KG (Chen et al., 2019; Saxena et al., 2020).",
                "Second, TransferNet performs better than GraftNet, EmbedKGQA, and NSM with the same retrieval method.",
                "We consider the following baselines for performance comparison: (1) reasoning-focused methods: KV-Mem (Miller et al., 2016), GraftNet (Sun et al., 2018), EmbedKGQA (Saxena et al., 2020), NSM (He et al., 2021), TransferNet (Shi et al., 2021); (2) retrieval-augmented methods: PullNet (Sun et al., 2019), SR+NSM (Zhang et al., 2022), SR+NSM+E2E (Zhang et al., 2022).",
                "\u2026following baselines for performance comparison: (1) reasoning-focused methods: KV-Mem (Miller et al., 2016), GraftNet (Sun et al., 2018), EmbedKGQA (Saxena et al., 2020), NSM (He et al., 2021), TransferNet (Shi et al., 2021); (2) retrieval-augmented methods: PullNet (Sun et al., 2019), SR+NSM\u2026",
                "\u2022 EmbedKGQA (Saxena et al., 2020) reformulates the multi-hop reasoning of GraftNet as a link prediction task by matching pre-trained entity embeddings with question representations from a PLM.\n\u2022 NSM (He et al., 2021) first conducts retrieval following GraftNet and then adapt the neural state machine (Hudson & Manning, 2019) used in visual reasoning for multi-hop reasoning on the KG.\n\u2022 TransferNet (Shi et al., 2021) first conducts retrieval following GraftNet and then performs the multi-hop reasoning on a KG or a text-formed relation graph in a transparent framework.",
                ", 2018), EmbedKGQA (Saxena et al., 2020), NSM (He et al.",
                "\u2022 EmbedKGQA (Saxena et al., 2020) reformulates the multi-hop reasoning of GraftNet as a link prediction task by matching pre-trained entity embeddings with question representations from a PLM.\n\u2022 NSM (He et al., 2021) first conducts retrieval following GraftNet and then adapt the neural state\u2026"
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "2d01da2c9ece0969d6ec56d22f78caf57050fc03",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2212-00959",
                    "ArXiv": "2212.00959",
                    "DOI": "10.48550/arXiv.2212.00959",
                    "CorpusId": 254221022
                },
                "corpusId": 254221022,
                "publicationVenue": {
                    "id": "939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                    "name": "International Conference on Learning Representations",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Learn Represent",
                        "ICLR"
                    ],
                    "url": "https://iclr.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/2d01da2c9ece0969d6ec56d22f78caf57050fc03",
                "title": "UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph",
                "abstract": "Multi-hop Question Answering over Knowledge Graph~(KGQA) aims to find the answer entities that are multiple hops away from the topic entities mentioned in a natural language question on a large-scale Knowledge Graph (KG). To cope with the vast search space, existing work usually adopts a two-stage approach: it first retrieves a relatively small subgraph related to the question and then performs the reasoning on the subgraph to find the answer entities accurately. Although these two stages are highly related, previous work employs very different technical solutions for developing the retrieval and reasoning models, neglecting their relatedness in task essence. In this paper, we propose UniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and reasoning in both model architecture and parameter learning. For model architecture, UniKGQA consists of a semantic matching module based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs. For parameter learning, we design a shared pre-training task based on question-relation matching for both retrieval and reasoning models, and then propose retrieval- and reasoning-oriented fine-tuning strategies. Compared with previous studies, our approach is more unified, tightly relating the retrieval and reasoning stages. Extensive experiments on three benchmark datasets have demonstrated the effectiveness of our method on the multi-hop KGQA task. Our codes and data are publicly available at~\\url{https://github.com/RUCAIBox/UniKGQA}.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118240359",
                        "name": "Jinhao Jiang"
                    },
                    {
                        "authorId": "1423651904",
                        "name": "Kun Zhou"
                    },
                    {
                        "authorId": "2542603",
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "authorId": "153693432",
                        "name": "Ji-rong Wen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "EmbedKGQA [15] improves the KBQA task by combining the idea of knowledge completion and matching the pre-trained entity embedding with the question embedding for multi-hop reasoning.",
                "To address this challenge, researchers supplement knowledge sources with auxiliary information [12] [13] [14] [15] [16]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "602adf22f43cced28636e0e130ad30643f0f0f21",
                "externalIds": {
                    "DOI": "10.1109/IUCC-CIT-DSCI-SmartCNS57392.2022.00067",
                    "CorpusId": 257958957
                },
                "corpusId": 257958957,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/602adf22f43cced28636e0e130ad30643f0f0f21",
                "title": "Knowledge Base Question Answering via Structured Query Generation using Question domain",
                "abstract": "The core problem of Knowledge Base Question Answering(KBQA) is to find queries from user questions to knowledge bases. Specifically, natural language questions need to be transformed into structured queries before associating with knowledge bases, and the answers can be found from the knowledge graph using the structured queries. We found structured queries in similar question domains tend to have repetitive reasoning steps. Also, humans often use cases identical to the question and information from these cases to assist in answering new questions. Hence, we propose a new KBQA framework based on similar question domains. We separately design the inference information retriever module to extract cases with a similar structure to the question and the relation information retriever module to narrow the scope of reasoning relation extraction. Finally, we used the retrieved inference cases and relation candidate sets as auxiliary information and generated an executable Knowledge-oriented Programming Language(KoPL) through the program generation module. Experiments have shown that the model can handle complex question answering and has a strong reasoning ability. Our methodology has resulted in new state-of-the-art performance on WebQSP and CWQ datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2213533932",
                        "name": "Jiecheng Li"
                    },
                    {
                        "authorId": "153001545",
                        "name": "Zizhen Peng"
                    },
                    {
                        "authorId": "145629492",
                        "name": "Xiaoying Zhu"
                    },
                    {
                        "authorId": "2070276430",
                        "name": "Keda Lu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[15] use a linear combination of relation scores and ComplEx scores to find answer entities.",
                "[15], the question is encoded to a vector using the RoBERTa.",
                "[15] KG embedding Calculate the embedded vector similarity between the question and the answer to obtain the optimal answer and improve the lack of information in KG Lack of application of a priori knowledge and poor interpretability",
                "At present, most of the IR-based method [14] [15] [16] [17] [21] are studied for multi-hop questions, and how to fully",
                "[15] used the ComplEx method to embed KG into the complex space to capture comprehensive feature information to obtain head entity, relation, and vector representation of candidate answers."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "01a6a841a73f9966b647ecc9aece1bbeb58bd7b7",
                "externalIds": {
                    "DOI": "10.1109/CAC57257.2022.10055934",
                    "CorpusId": 257516438
                },
                "corpusId": 257516438,
                "publicationVenue": {
                    "id": "bfa6e440-6762-47f0-b1b9-2a43c02d9f62",
                    "name": "ACM Cloud and Autonomic Computing Conference",
                    "type": "conference",
                    "alternate_names": [
                        "ACM Cloud Auton Comput Conf",
                        "Chinese Automation Congress",
                        "Chin Autom Congr",
                        "CAC"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/01a6a841a73f9966b647ecc9aece1bbeb58bd7b7",
                "title": "A Survey on Information Retrieval Method for Knowledge Graph Complex Question Answering",
                "abstract": "Knowledge graph question answering (KGQA) uses structured knowledge in the knowledge graph (KG) to answer natural language questions. Currently, simple question answering has been well solved, but the performance of complex question answering needs to be improved. In this survey, we first present the background knowledge about complex question answering based on KG. Then the information retrieval-based (IR-based) method is introduced. We describe each procedure in the IR-based method in detail and investigate the related tools. Finally, the work is summarized and future research is envisioned based on the problems of current the IR-based method directions. With this survey, we aim to provide novices in the field of KGQA with an entry point to a suitable the IR-based method, where each step of the method can be understood.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1760554",
                        "name": "Junbiao Pang"
                    },
                    {
                        "authorId": "48378957",
                        "name": "Yongheng Zhang"
                    },
                    {
                        "authorId": "2211055767",
                        "name": "Jiaxin Deng"
                    },
                    {
                        "authorId": "48386589",
                        "name": "Xiao-qing Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ce2f15287d8b0f48e263f4f4942bb842f37d5b24",
                "externalIds": {
                    "DBLP": "journals/idt/KhobragadeG22",
                    "DOI": "10.3233/idt-210103",
                    "CorpusId": 254330217
                },
                "corpusId": 254330217,
                "publicationVenue": {
                    "id": "15cb2d28-fc6a-42d0-a385-e285baee6f03",
                    "name": "International Journal of Intelligent Decision Technologies",
                    "type": "journal",
                    "alternate_names": [
                        "Int J Intell Decis Technol",
                        "Intelligent Decision Technologies",
                        "Intell Decis Technol"
                    ],
                    "issn": "1872-4981",
                    "url": "http://content.iospress.com/journals/intelligent-decision-technologies",
                    "alternate_urls": [
                        "https://www.iospress.nl/journal/intelligent-decision-technologies/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ce2f15287d8b0f48e263f4f4942bb842f37d5b24",
                "title": "Study and analysis of various link predictions in knowledge graph: A challenging overview",
                "abstract": "Knowledge Graph (KG) is the network which contains some topic-based entities, called nodes, and the associated information among the entities. Here, the concept in the knowledge graph is denoted by the tuple relationship, such that the \u27e8entity1, predicate, entity2\u27e9. The entity in the knowledge graph is the abstract concepts based on the particular object, namely the organization, dataset, people, and some associated documentation. The big issue in the KG is that it consists of some incomplete information. The missing details can be identified by employing the knowledge graph completion (KGC) solution. KGC is the same as the link prediction concepts in the knowledge graphs. However, this concept is more complex that it does not predict the link relationship among the nodes but also the diversified information from the link relations. Hence this survey analyzes different methods of link prediction techniques, and this review provides a detailed review of 50 research papers concentrating on various methods, like embedding-based methods, deep learning methods, knowledge acquisition methods, ranking methods, and representation learning methods. The analysis is carried out with respect to the survey based on the publication year, research techniques, performance measures, dataset, toolset and achievement of the research methodologies. Also, the problems in the methods are explained in the research gaps and issues. Furthermore, the future extent of these research works is done based on the limitations identified from the existing research methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "97851193",
                        "name": "A. Khobragade"
                    },
                    {
                        "authorId": "2926835",
                        "name": "S. Ghumbre"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f80ee5510c9b8259250013887e141b0556bb5464",
                "externalIds": {
                    "DBLP": "journals/jsan/ChenLLLWHW22",
                    "DOI": "10.3390/jsan11040078",
                    "CorpusId": 253862877
                },
                "corpusId": 253862877,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f80ee5510c9b8259250013887e141b0556bb5464",
                "title": "An Overview of Knowledge Graph Reasoning: Key Technologies and Applications",
                "abstract": "In recent years, with the rapid development of Internet technology and applications, the scale of Internet data has exploded, which contains a significant amount of valuable knowledge. The best methods for the organization, expression, calculation, and deep analysis of this knowledge have attracted a great deal of attention. The knowledge graph has emerged as a rich and intuitive way to express knowledge. Knowledge reasoning based on knowledge graphs is one of the current research hot spots in knowledge graphs and has played an important role in wireless communication networks, intelligent question answering, and other applications. Knowledge graph-oriented knowledge reasoning aims to deduce new knowledge or identify wrong knowledge from existing knowledge. Different from traditional knowledge reasoning, knowledge reasoning methods oriented to knowledge graphs are more diversified due to the concise, intuitive, flexible, and rich knowledge expression forms in knowledge graphs. Based on the basic concepts of knowledge graphs and knowledge graph reasoning, this paper introduces the latest research progress in knowledge graph-oriented knowledge reasoning methods in recent years. Specifically, according to different reasoning methods, knowledge graph reasoning includes rule-based reasoning, distributed representation-based reasoning, neural network-based reasoning, and mixed reasoning. These methods are summarized in detail, and the future research directions and prospects of knowledge reasoning based on knowledge graphs are discussed and prospected.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2192226606",
                        "name": "Yonghong Chen"
                    },
                    {
                        "authorId": "2182276242",
                        "name": "Hao Li"
                    },
                    {
                        "authorId": "2145573242",
                        "name": "Han Li"
                    },
                    {
                        "authorId": "2192177046",
                        "name": "Wenhao Liu"
                    },
                    {
                        "authorId": "1409786102",
                        "name": "Yirui Wu"
                    },
                    {
                        "authorId": "2182284005",
                        "name": "Qian Huang"
                    },
                    {
                        "authorId": "2112899620",
                        "name": "Shaohua Wan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "KGs have already been widely used in a series of downstream tasks, e.g., question answering [Saxena et al., 2020, Ding et al., 2022b] and recommender systems [Wang et al., 2019c,a]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "576a7f578c56626f5e764096e65848c98a25e24d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-08169",
                    "ArXiv": "2211.08169",
                    "DOI": "10.48550/arXiv.2211.08169",
                    "CorpusId": 253523543
                },
                "corpusId": 253523543,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/576a7f578c56626f5e764096e65848c98a25e24d",
                "title": "Few-Shot Inductive Learning on Temporal Knowledge Graphs using Concept-Aware Information",
                "abstract": "Knowledge graph completion (KGC) aims to predict the missing links among knowledge graph (KG) entities. Though various methods have been developed for KGC, most of them can only deal with the KG entities seen in the training set and cannot perform well in predicting links concerning novel entities in the test set. Similar problem exists in temporal knowledge graphs (TKGs), and no previous temporal knowledge graph completion (TKGC) method is developed for modeling newly-emerged entities. Compared to KGs, TKGs require temporal reasoning techniques for modeling, which naturally increases the difficulty in dealing with novel, yet unseen entities. In this work, we focus on the inductive learning of unseen entities' representations on TKGs. We propose a few-shot out-of-graph (OOG) link prediction task for TKGs, where we predict the missing entities from the links concerning unseen entities by employing a meta-learning framework and utilizing the meta-information provided by only few edges associated with each unseen entity. We construct three new datasets for TKG few-shot OOG link prediction, and we propose a model that mines the concept-aware information among entities. Experimental results show that our model achieves superior performance on all three datasets and our concept-aware modeling component demonstrates a strong effect.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2046761003",
                        "name": "Zifeng Ding"
                    },
                    {
                        "authorId": "2181618414",
                        "name": "Jingpei Wu"
                    },
                    {
                        "authorId": "2147293727",
                        "name": "Bailan He"
                    },
                    {
                        "authorId": "10684484",
                        "name": "Yunpu Ma"
                    },
                    {
                        "authorId": "2188573511",
                        "name": "Zhen Han"
                    },
                    {
                        "authorId": "1742501819",
                        "name": "Volker Tresp"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In [13], authors have used KG embeddings to overcome missing link challenge in KGQA."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0c007e000eaf59e86788b45b70879612cde2e8ee",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-05351",
                    "ArXiv": "2211.05351",
                    "DOI": "10.48550/arXiv.2211.05351",
                    "CorpusId": 253447269
                },
                "corpusId": 253447269,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/0c007e000eaf59e86788b45b70879612cde2e8ee",
                "title": "Biomedical Multi-hop Question Answering Using Knowledge Graph Embeddings and Language Models",
                "abstract": "Biomedical knowledge graphs (KG) are heterogenous networks consisting of biological entities as nodes and relations between them as edges. These entities and relations are extracted from millions of research papers and unified in a single resource. The goal of biomedical multi-hop question-answering over knowledge graph (KGQA) is to help biologist and scientist to get valuable insights by asking questions in natural language. Relevant answers can be found by first understanding the question and then querying the KG for right set of nodes and relationships to arrive at an answer. To model the question, language models such as RoBERTa and BioBERT are used to understand context from natural language question. One of the challenges in KGQA is missing links in the KG. Knowledge graph embeddings (KGE) help to overcome this problem by encoding nodes and edges in a dense and more efficient way. In this paper, we use a publicly available KG called Hetionet which is an integrative network of biomedical knowledge assembled from 29 different databases of genes, compounds, diseases, and more. We have enriched this KG dataset by creating a multi-hop biomedical question-answering dataset in natural language for testing the biomedical multi-hop question-answering system and this dataset will be made available to the research community. The major contribution of this research is an integrated system that combines language models with KG embeddings to give highly relevant answers to free-form questions asked by biologists in an intuitive interface. Biomedical multi-hop question-answering system is tested on this data and results are highly encouraging.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "30167519",
                        "name": "Dattaraj Rao"
                    },
                    {
                        "authorId": "2053439130",
                        "name": "Shraddha Mane"
                    },
                    {
                        "authorId": "34737180",
                        "name": "M. Paliwal"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Due to their effectiveness in storing and representing factual knowledge, they have been successfully applied in question answering [91, 132], recommendation system [95, 151], information retrieval [34, 118] and other domain-specific applications [58, 68]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "aa78c309264eb223166e3dacbce43e08c39a2027",
                "externalIds": {
                    "ArXiv": "2211.03536",
                    "DBLP": "journals/corr/abs-2211-03536",
                    "DOI": "10.48550/arXiv.2211.03536",
                    "CorpusId": 253384318
                },
                "corpusId": 253384318,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/aa78c309264eb223166e3dacbce43e08c39a2027",
                "title": "Knowledge Graph Embedding: A Survey from the Perspective of Representation Spaces",
                "abstract": "Knowledge graph embedding (KGE) is a increasingly popular technique that aims to represent entities and relations of knowledge graphs into low-dimensional semantic spaces for a wide spectrum of applications such as link prediction, knowledge reasoning and knowledge completion. In this paper, we provide a systematic review of existing KGE techniques based on representation spaces. Particularly, we build a fine-grained classification to categorise the models based on three mathematical perspectives of the representation spaces: (1) Algebraic perspective, (2) Geometric perspective, and (3) Analytical perspective. We introduce the rigorous definitions of fundamental mathematical spaces before diving into KGE models and their mathematical properties. We further discuss different KGE methods over the three categories, as well as summarise how spatial advantages work over different embedding needs. By collating the experimental results from downstream tasks, we also explore the advantages of mathematical space in different scenarios and the reasons behind them. We further state some promising research directions from a representation space perspective, with which we hope to inspire researchers to design their KGE models as well as their related applications with more consideration of their mathematical space properties.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1406166829",
                        "name": "Jiahang Cao"
                    },
                    {
                        "authorId": "1384241384",
                        "name": "Jinyuan Fang"
                    },
                    {
                        "authorId": "3451645",
                        "name": "Zaiqiao Meng"
                    },
                    {
                        "authorId": "3279808",
                        "name": "Shangsong Liang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The main QA model is based on EmbedKGQA [15],\nwhere it first learns the embedding of the Knowledge\nGraphs, the question, and the head entities.",
                "While the original WebQSP dataset [17] contains a full knowledge base, WebQSP-50 was constructed by [15] randomly omitting half of the entities to emulate a sparse knowledge base.",
                "The main QA model is based on EmbedKGQA [15], where it first learns the embedding of the Knowledge Graphs, the question, and the head entities.",
                "Table 1 shows experimental results (HITS@1 score) comparing the original EmbedKGQA model with the WebQSP-50 datasets and the modified model with WebQSP-50-TH and RBP datasets.",
                "Other embedding schemes utilize deep learning networks to learn the embedding, such as ConvE [14] which learns a scoring function of the head, tail, and relation using Convolutional Neural Networks, and InteractE [15], an improved version of ConvE by incorporating feature interaction into the network."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "490c2f17e5d2303c3f0fd93a2a726a4fdd0978cd",
                "externalIds": {
                    "DOI": "10.1109/iSAI-NLP56921.2022.9960247",
                    "CorpusId": 254098444
                },
                "corpusId": 254098444,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/490c2f17e5d2303c3f0fd93a2a726a4fdd0978cd",
                "title": "Question Answering over Knowledge Graphs for Thai Retail Banking Products",
                "abstract": "Question Answering over Knowledge Graphs (KGQA) extracts the answer entity directly from the graph, given a natural language question, offering scalability to applications that need to readily provide information to the end users, such as chatbots. Nevertheless, KGQA specifically designed for Knowledge Graphs in Thai has not yet been well investigated. In this paper, we adapt multi-hop KGQA using Graph Embedding to handle Thai dataset while being able to extract answer entities that do not have explicit relation to the head node. We also construct a Thai Knowledge Graph with the ontology based on retail banking products. The model achieves a HITS @ 1 score of 80.8 on our annotated dataset. The results confirm that, aside from reaching multi-hop answers, using Graph Embedding in KGQA helps improve the overall score, especially in sparse Knowledge Graphs. Moreover, augmenting the training questions to include more entities in the graph can further help increase the performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2192834409",
                        "name": "Wirit Khongcharoen"
                    },
                    {
                        "authorId": "151479615",
                        "name": "Chanatip Saetia"
                    },
                    {
                        "authorId": "3468441",
                        "name": "Tawunrat Chalothorn"
                    },
                    {
                        "authorId": "97176787",
                        "name": "P. Buabthong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "EmbedKGQA [40] employs Roberta to encodes complex questions as relation vectors eq and uses the ComplEx score to determine the answer entity.",
                "[40] first used knowledge base embeddings for KBQA."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "78aff11e32b33ccd8752a4206fa4c499616180b3",
                "externalIds": {
                    "DBLP": "journals/nca/CaoZS23",
                    "DOI": "10.1007/s00521-022-07965-0",
                    "CorpusId": 253375888
                },
                "corpusId": 253375888,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/78aff11e32b33ccd8752a4206fa4c499616180b3",
                "title": "Improving and evaluating complex question answering over knowledge bases by constructing strongly supervised data",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2149215129",
                        "name": "Xing Cao"
                    },
                    {
                        "authorId": "2982407",
                        "name": "Yingsi Zhao"
                    },
                    {
                        "authorId": "2114338318",
                        "name": "Bo Shen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "lationships between various entities, providing rich semantic background knowledge for various natural language processing (NLP) tasks, such as natural language representation [1], question answering [2], image captioning [3], and text classification [4]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5c61dc694400c7d4c46c93be79420d2a9c601594",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-02849",
                    "ArXiv": "2211.02849",
                    "DOI": "10.48550/arXiv.2211.02849",
                    "CorpusId": 253383970
                },
                "corpusId": 253383970,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/5c61dc694400c7d4c46c93be79420d2a9c601594",
                "title": "Coarse-to-fine Knowledge Graph Domain Adaptation based on Distantly-supervised Iterative Training",
                "abstract": "Modern supervised learning neural network models require a large amount of manually labeled data, which makes the construction of domain-specific knowledge graphs time-consuming and labor-intensive. In parallel, although there has been much research on named entity recognition and relation extraction based on distantly supervised learning, constructing a domain-specific knowledge graph from large collections of textual data without manual annotations is still an urgent problem to be solved. In response, we propose an integrated framework for adapting and re-learning knowledge graphs from one coarse domain (biomedical) to a finer-define domain (oncology). In this framework, we apply distant-supervision on cross-domain knowledge graph adaptation. Consequently, no manual data annotation is required to train the model. We introduce a novel iterative training strategy to facilitate the discovery of domain-specific named entities and triples. Experimental results indicate that the proposed framework can perform domain adaptation and construction of knowledge graph efficiently.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2190106647",
                        "name": "Homgmin Cai"
                    },
                    {
                        "authorId": "2036578588",
                        "name": "Wenxiong Liao"
                    },
                    {
                        "authorId": "2145977326",
                        "name": "Zheng Liu"
                    },
                    {
                        "authorId": null,
                        "name": "Xiaoke Huang"
                    },
                    {
                        "authorId": "2108362387",
                        "name": "Yiyang Zhang"
                    },
                    {
                        "authorId": "2190108365",
                        "name": "Siqi Ding"
                    },
                    {
                        "authorId": "2153702893",
                        "name": "Sheng Li"
                    },
                    {
                        "authorId": "1762919",
                        "name": "Quanzheng Li"
                    },
                    {
                        "authorId": "2115345993",
                        "name": "Tianming Liu"
                    },
                    {
                        "authorId": "47057650",
                        "name": "Xiang Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f1eaefeac8057a41af87ada287bacf383f8fde9f",
                "externalIds": {
                    "DBLP": "journals/isci/CuiPXHHL23",
                    "DOI": "10.1016/j.ins.2022.11.042",
                    "CorpusId": 253663325
                },
                "corpusId": 253663325,
                "publicationVenue": {
                    "id": "e46002a1-d7a6-4681-aae9-36bc3a6a1f93",
                    "name": "Information Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Information Scientist",
                        "Inf Sci"
                    ],
                    "issn": "0020-0255",
                    "alternate_issns": [
                        "0020-0263"
                    ],
                    "url": "http://www.sciencedirect.com/science/journal/00200255"
                },
                "url": "https://www.semanticscholar.org/paper/f1eaefeac8057a41af87ada287bacf383f8fde9f",
                "title": "Incorporating anticipation embedding into reinforcement learning framework for multi-hop knowledge graph question answering",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2093917610",
                        "name": "Hai Cui"
                    },
                    {
                        "authorId": "2068928731",
                        "name": "T. Peng"
                    },
                    {
                        "authorId": "2174842474",
                        "name": "Feng Xiao"
                    },
                    {
                        "authorId": "2111758731",
                        "name": "Jiayu Han"
                    },
                    {
                        "authorId": "2093920100",
                        "name": "Ridong Han"
                    },
                    {
                        "authorId": "2118467569",
                        "name": "Lu Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We use the underlying knowledge graph provided by [5] as well as the QA data.",
                "3 \u2013 follows the overall embedding-based KGQA framework introduced in [5] with its four main blocks:",
                "In the case of WebQuestionsSP, we use the underlying knowledge graphs provided by [5].",
                "More recently [5] proposed EmbedKGQA, a framework with two training (i) train the knowledge graph embedding on the link prediction task; (ii) train the model for QA by leveraging the pretrained embeddings.",
                "c) Embedding-based KGQA: In the embedding-based KGQA approach [5], the questions are mapped into a vector eq in the complex space C together with the entities and relations embeddings.",
                "To address this limitation our work builds on an embedding-based KGQA framework (EmbedKGQA [5]) and uses heperbolic representation to further tackle the sparsity and presence of hierarchical structures in the KG."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "bc5271c98d1d1fa70a443bd1803cbf98f55a86e5",
                "externalIds": {
                    "DBLP": "conf/icdm/MadjidKGD22",
                    "DOI": "10.1109/ICDM54844.2022.00041",
                    "CorpusId": 256463882
                },
                "corpusId": 256463882,
                "publicationVenue": {
                    "id": "67d15a94-d523-4b5f-be58-03fe2ef9dcfb",
                    "name": "Industrial Conference on Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Ind Conf Data Min",
                        "ICDM"
                    ],
                    "url": "http://www.data-mining-forum.de/"
                },
                "url": "https://www.semanticscholar.org/paper/bc5271c98d1d1fa70a443bd1803cbf98f55a86e5",
                "title": "HyperKGQA: Question Answering over Knowledge Graphs using Hyperbolic Representation Learning",
                "abstract": "Knowledge Graph Question Answering (KGQA) models enable users to acquire entity-based answers from a Knowledge Graph by asking natural language questions (NLQs) without the need to learn a specialized graph query language or knowing the underlying schema of the knowledge graph. This work investigates hyperbolic graph representation learning methods to effectively and efficiently represent knowledge base items and natural questions. Our system, HyperKGQA, proposes a technique that embeds the knowledge graph in a hyperbolic manifold, then learns an adaptive transformation of pre-trained sentence representations into the space of entities and relations. Finally, a post-processing step refines the ranking of the candidate answers by computing the relevance score of the set of relations and the question. An extensive set of experiments conducted on two datasets shows that our method outperforms the current state-of-the-art models when reasoning over sparse graphs to answer multi-hop questions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "151151266",
                        "name": "Nadya Abdel Madjid"
                    },
                    {
                        "authorId": "2203830217",
                        "name": "Ola El Khatib"
                    },
                    {
                        "authorId": "2204074526",
                        "name": "Shuang Gao"
                    },
                    {
                        "authorId": "1695677",
                        "name": "D. Difallah"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d0b0a06945bcc9eb0af4f05841f93e2da122763c",
                "externalIds": {
                    "DOI": "10.2118/211855-ms",
                    "CorpusId": 253248715
                },
                "corpusId": 253248715,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d0b0a06945bcc9eb0af4f05841f93e2da122763c",
                "title": "Expert System for Question Answering on Anomalous Events and Mitigation Strategies Using Bidirectional Transformers and Knowledge Graphs",
                "abstract": "\n Daily drilling reports provide vital information for well planning as they capture anomalous events and mitigation measures during drilling operations. Previous works predominantly focus on search frameworks for information retrieval from these reports. However, the context between searches is lost, preventing users from narrowing down to the exact answer. Here, we present a transformer-based closed domain conversational agent for longer dialogues to guide users to contextual information for anomalous drilling events through natural language.\n Automated text extraction, cleaning and validation tasks are initially performed to resolve data quality issues prior to language modeling on a validated data set. Subsequently, a knowledge-based graph is created by node embedding using entity extractions and by learning the semantic-level relationships between entity nodes such as well names and events. Further, conversational agents are trained on the knowledge graphs for natural dialogue generation using neural machine translation models. Here, users\u2019 questions are translated into a query in a structured language that is evaluated directly over the knowledge graph in order to generate the desired answers.\n The workflow was tested on an asset with multiple wells experiencing several anomalous events during drilling such as stuck pipe, circulation losses and kicks. The end-to-end workflow was tested on its ability to retrieve anomalous events and present mitigation measures in the aforementioned data set based on the descriptions input by survey participants. Performance on the anomaly extraction, attribute mapping and mitigation performance were evaluated through F1 scores. A significantly high F1 score was recorded for anomaly extraction. This is predominantly driven by high precision due to explicit modeling of the reports as a knowledge graph. In addition to testing the workflow end to end, we tested the knowledge graph representation in isolation. For this, ranking metrics and triple classification with negative samples were used for the evaluation. The adjusted mean rank index was close to one, indicating high performance. Structured querying on the knowledge graphs also showed high accuracy for classifying anomalous events in the drilling report.\n The work described in this paper automates the end-to-end workflow for building an expert system for answering questions about anomalous events and mitigation strategies using daily drilling reports. Our novel approach using a knowledge graph with a transformer-based conversational agent enables users to perform detailed interactive investigation of anomalous events observed in daily drilling reports and create mitigation strategies. The workflow also allows for incorporating prior domain knowledge from drilling experts.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2144618224",
                        "name": "Jaijith Sreekantan"
                    },
                    {
                        "authorId": "2189454342",
                        "name": "Chad Hutchison"
                    },
                    {
                        "authorId": "2189454314",
                        "name": "Pratuat Amatya"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These methods either rely on KG embeddings (Saxena et al., 2020; Ren et al., 2021) or on side information, such as text corpus (Sun et al.",
                "EmbedKGQA (Saxena et al., 2020) utilizes KG pre-trained embeddings (Trouillon et al., 2016) to improve multi-hop reasoning.",
                "EmbedKGQA (Saxena et al., 2020) utilizes KG pre-trained embeddings (Trouillon et al.",
                "These methods either rely on KG embeddings (Saxena et al., 2020; Ren et al., 2021) or on side information, such as text corpus (Sun et al., 2018, 2019; Xiong et al., 2019; Han et al., 2020), to infer missing information."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "543cefea7ad2a94c6bb2d8f0d0d6384e1547a9e7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-13650",
                    "ArXiv": "2210.13650",
                    "DOI": "10.48550/arXiv.2210.13650",
                    "CorpusId": 253107873
                },
                "corpusId": 253107873,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/543cefea7ad2a94c6bb2d8f0d0d6384e1547a9e7",
                "title": "ReaRev: Adaptive Reasoning for Question Answering over Knowledge Graphs",
                "abstract": "Knowledge Graph Question Answering (KGQA) involves retrieving entities as answers from a Knowledge Graph (KG) using natural language queries. The challenge is to learn to reason over question-relevant KG facts that traverse KG entities and lead to the question answers. To facilitate reasoning, the question is decoded into instructions, which are dense question representations used to guide the KG traversals. However, if the derived instructions do not exactly match the underlying KG information, they may lead to reasoning under irrelevant context. Our method, termed ReaRev, introduces a new way to KGQA reasoning with respect to both instruction decoding and execution. To improve instruction decoding, we perform reasoning in an adaptive manner, where KG-aware information is used to iteratively update the initial instructions. To improve instruction execution, we emulate breadth-first search (BFS) with graph neural networks (GNNs). The BFS strategy treats the instructions as a set and allows our method to decide on their execution order on the fly. Experimental results on three KGQA benchmarks demonstrate the ReaRev's effectiveness compared with previous state-of-the-art, especially when the KG is incomplete or when we tackle complex questions. Our code is publicly available at https://github.com/cmavro/ReaRev_KGQA.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1944251405",
                        "name": "Costas Mavromatis"
                    },
                    {
                        "authorId": "50877490",
                        "name": "G. Karypis"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Entity Retriever We perform entity retrieval following a standard pipeline with three steps, i.e., mention detection, candidate generation, and entity disambiguation (Shen et al., 2021)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "dbc6622b8c70bc1a3cc290fdb166229d80ec8f83",
                "externalIds": {
                    "DBLP": "conf/emnlp/ShuYLKMQL22",
                    "ACL": "2022.emnlp-main.555",
                    "ArXiv": "2210.12925",
                    "DOI": "10.48550/arXiv.2210.12925",
                    "CorpusId": 253098671
                },
                "corpusId": 253098671,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/dbc6622b8c70bc1a3cc290fdb166229d80ec8f83",
                "title": "TIARA: Multi-grained Retrieval for Robust Question Answering over Large Knowledge Base",
                "abstract": "Pre-trained language models (PLMs) have shown their effectiveness in multiple scenarios. However, KBQA remains challenging, especially regarding coverage and generalization settings. This is due to two main factors: i) understanding the semantics of both questions and relevant knowledge from the KB; ii) generating executable logical forms with both semantic and syntactic correctness. In this paper, we present a new KBQA model, TIARA, which addresses those issues by applying multi-grained retrieval to help the PLM focus on the most relevant KB context, viz., entities, exemplary logical forms, and schema items. Moreover, constrained decoding is used to control the output space and reduce generation errors. Experiments over important benchmarks demonstrate the effectiveness of our approach. TIARA outperforms previous SOTA, including those using PLMs or oracle entity annotations, by at least 4.1 and 1.1 F1 points on GrailQA and WebQuestionsSP, respectively. Specifically on GrailQA, TIARA outperforms previous models in all categories, with an improvement of 4.7 F1 points in zero-shot generalization.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1406331721",
                        "name": "Yiheng Shu"
                    },
                    {
                        "authorId": "2139425204",
                        "name": "Zhiwei Yu"
                    },
                    {
                        "authorId": "2110539208",
                        "name": "Yuhan Li"
                    },
                    {
                        "authorId": "2047947436",
                        "name": "B\u00f6rje F. Karlsson"
                    },
                    {
                        "authorId": "2168066",
                        "name": "Tingting Ma"
                    },
                    {
                        "authorId": "1887019",
                        "name": "Yuzhong Qu"
                    },
                    {
                        "authorId": "50554693",
                        "name": "Chin-Yew Lin"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "774255e00ac21e1d6a9ac530a6789c026808fd96",
                "externalIds": {
                    "PubMedCentral": "9585073",
                    "DOI": "10.1038/s41598-022-20752-0",
                    "CorpusId": 252996475,
                    "PubMed": "36266295"
                },
                "corpusId": 252996475,
                "publicationVenue": {
                    "id": "f99f77b7-b1b6-44d3-984a-f288e9884b9b",
                    "name": "Scientific Reports",
                    "type": "journal",
                    "alternate_names": [
                        "Sci Rep"
                    ],
                    "issn": "2045-2322",
                    "url": "http://www.nature.com/srep/",
                    "alternate_urls": [
                        "http://www.nature.com/srep/index.html"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/774255e00ac21e1d6a9ac530a6789c026808fd96",
                "title": "An automatic hypothesis generation for plausible linkage between xanthium and diabetes",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9324684",
                        "name": "A. F. Syafiandini"
                    },
                    {
                        "authorId": "2188258329",
                        "name": "Gyuri Song"
                    },
                    {
                        "authorId": "2188245843",
                        "name": "Yu-ri Ahn"
                    },
                    {
                        "authorId": "2154889736",
                        "name": "Heeyoung Kim"
                    },
                    {
                        "authorId": "2110310577",
                        "name": "Min Song"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Saxena A et al [8] alleviated the data incompleteness problem faced by multi-hop quizzing by performing link prediction based on knowledge graph embedding models to have multi-hop reasoning capabilities that can be used on complex long paths."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "70aaa4650f944fa060c470d1ccee031b0fd0cfa3",
                "externalIds": {
                    "DOI": "10.1117/12.2656576",
                    "CorpusId": 253162126
                },
                "corpusId": 253162126,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/70aaa4650f944fa060c470d1ccee031b0fd0cfa3",
                "title": "Improve embedded knowledge graph multi-hop question answering based on relational path",
                "abstract": "Multi-hop Knowledge Graph Question Answering (KGQA) requires reasoning about multi-hop inference relations between topic entities and answers on the knowledge graph(KG) and returning correct answers. The difficulty of obtaining the implied inference relations in multi-hop questions described in natural language and the sparse knowledge graph bring challenges to the multi-hop KGQA. In this paper, we propose an embedded knowledge graph multi-hop Q&A model based on relational paths, which exploits the relational chains in the knowledge graph and the semantic similarity of multiple questions to improve the accuracy of multi-hop KGQA task. The comparative experiments prove that our method significantly outperforms the state-of-the-art counterparts. Comprehensive ablation experiments also validate the effectiveness of our approach on multi-hop KGQA tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2153662798",
                        "name": "Y. Niu"
                    },
                    {
                        "authorId": "2117772704",
                        "name": "Yun Jiang"
                    },
                    {
                        "authorId": "2189033385",
                        "name": "Qingda Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2013) to multi-hop complex questions requiring multi-fact reasoning (Sun et al., 2019; Saxena et al., 2020).",
                "It is considered a more challenging problem than KGQA (Bhutani et al., 2019; Saxena et al., 2020), where questions are typically about persistent, non-temporal facts (e.",
                "Most KGQA systems have focused on answering questions from simple (i.e., 1-hop fact-based questions) (Berant et al., 2013) to multi-hop complex questions requiring multi-fact reasoning (Sun et al., 2019; Saxena et al., 2020).",
                "It is considered a more challenging problem than KGQA (Bhutani et al., 2019; Saxena et al., 2020), where questions are typically about persistent, non-temporal facts (e.g., place of birth), with only a small portion of the questions requiring any temporal reasoning (Jia et al., 2018a)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "831b350b6f665379c819a2c0535f17d7205cea0e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-06281",
                    "ArXiv": "2210.06281",
                    "ACL": "2023.eacl-main.150",
                    "DOI": "10.48550/arXiv.2210.06281",
                    "CorpusId": 252846570
                },
                "corpusId": 252846570,
                "publicationVenue": {
                    "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
                    "name": "Conference of the European Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Eur Chapter Assoc Comput Linguistics",
                        "EACL"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/eacl/"
                },
                "url": "https://www.semanticscholar.org/paper/831b350b6f665379c819a2c0535f17d7205cea0e",
                "title": "TwiRGCN: Temporally Weighted Graph Convolution for Question Answering over Temporal Knowledge Graphs",
                "abstract": "Recent years have witnessed interest in Temporal Question Answering over Knowledge Graphs (TKGQA), resulting in the development of multiple methods. However, these are highly engineered, thereby limiting their generalizability, and they do not automatically discover relevant parts of the KG during multi-hop reasoning. Relational graph convolutional networks (RGCN) provide an opportunity to address both of these challenges \u2013 we explore this direction in the paper. Specifically, we propose a novel, intuitive and interpretable scheme to modulate the messages passed through a KG edge during convolution based on the relevance of its associated period to the question. We also introduce a gating device to predict if the answer to a complex temporal question is likely to be a KG entity or time and use this prediction to guide our scoring mechanism. We evaluate the resulting system, which we call TwiRGCN, on a recent challenging dataset for multi-hop complex temporal QA called TimeQuestions. We show that TwiRGCN significantly outperforms state-of-the-art models on this dataset across diverse question types. Interestingly, TwiRGCN improves accuracy by 9\u201310 percentage points for the most difficult ordinal and implicit question types.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144921674",
                        "name": "Aditya Sharma"
                    },
                    {
                        "authorId": "46961776",
                        "name": "Apoorv Saxena"
                    },
                    {
                        "authorId": "2124123754",
                        "name": "Chitrank Gupta"
                    },
                    {
                        "authorId": "2470890",
                        "name": "Seyed Mehran Kazemi"
                    },
                    {
                        "authorId": "2406435",
                        "name": "Partha P. Talukdar"
                    },
                    {
                        "authorId": "2197703",
                        "name": "Soumen Chakrabarti"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018), question answering (Huang et al., 2019; Saxena et al., 2020), and natural language generation (Liu et al.",
                "\u2026being actively used to inject structured knowledge into target system in various fields such as recommendation (Wang et al., 2019; Xu et al., 2020; Zhang et al., 2018), question answering (Huang et al., 2019; Saxena et al., 2020), and natural language generation (Liu et al., 2021; Wu et al., 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c43dba06605a967e72c356175a11171a6315f411",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-06242",
                    "ArXiv": "2210.06242",
                    "DOI": "10.48550/arXiv.2210.06242",
                    "CorpusId": 252846469
                },
                "corpusId": 252846469,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c43dba06605a967e72c356175a11171a6315f411",
                "title": "Entity Aware Negative Sampling with Auxiliary Loss of False Negative Prediction for Knowledge Graph Embedding",
                "abstract": "Knowledge graph (KG) embedding is widely used in many downstream applications using KGs. Generally, since KGs contain only ground truth triples, it is necessary to construct arbitrary negative samples for representation learning of KGs. Recently, various methods for sampling high-quality negatives have been studied because the quality of negative triples has great effect on KG embedding. In this paper, we propose a novel method called Entity Aware Negative Sampling (EANS), which is able to sample negative entities resemble to positive one by adopting Gaussian distribution to the aligned entity index space. Additionally, we introduce auxiliary loss for false negative prediction that can alleviate the impact of the sampled false negative triples. The proposed method can generate high-quality negative samples regardless of negative sample size and effectively mitigate the influence of false negative samples. The experimental results on standard benchmarks show that our EANS outperforms existing the state-of-the-art methods of negative sampling on several knowledge graph embedding models. Moreover, the proposed method achieves competitive performance even when the number of negative samples is limited to only one.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2176642586",
                        "name": "Sang-hyun Je"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In addition, there are some other KBQA methods [33, 34]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "62b5cbb3144ccf9e345aa0e012979ac084648f73",
                "externalIds": {
                    "PubMedCentral": "9581621",
                    "DOI": "10.1155/2022/4734179",
                    "CorpusId": 252893108,
                    "PubMed": "36275972"
                },
                "corpusId": 252893108,
                "publicationVenue": {
                    "id": "f32b7322-b69c-4e63-801d-8f50784ef778",
                    "name": "Computational Intelligence and Neuroscience",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Intell Neurosci"
                    ],
                    "issn": "1687-5265",
                    "url": "https://www.hindawi.com/journals/cin/"
                },
                "url": "https://www.semanticscholar.org/paper/62b5cbb3144ccf9e345aa0e012979ac084648f73",
                "title": "Edge-Aware Graph Neural Network for Multi-Hop Path Reasoning over Knowledge Base",
                "abstract": "Multi-hop path reasoning over knowledge base aims at finding answer entities for an input question by walking along a path of triples from graph structure data, which is a crucial branch in the knowledge base question answering (KBQA) research field. Previous studies rely on deep neural networks to simulate the way humans solve multi-hop questions, which do not consider the latent relation information contained in connected edges, and lack of measuring the correlation between specific relations and the input question. To address these challenges, we propose an edge-aware graph neural network for multi-hop path reasoning task. First, a query node is directly added to the candidate subgraph retrieved from the knowledge base, which constructs what we term a query graph. This graph construction strategy makes it possible to enhance the information flow between the question and the nodes for the subsequent message passing steps. Second, question-related information contained in the relations is added to the entity node representations during graph updating; meanwhile, the relation representations are updated. Finally, the attention mechanism is used to weight the contribution from neighbor nodes so that only the information of neighbor nodes related to the query can be injected into new node representations. Experimental results on MetaQA and PathQuestion-Large (PQL) benchmarks demonstrate that the proposed model achieves higher Hit@1 and F1 scores than the baseline methods by a large margin. Moreover, ablation studies show that both the graph construction and the graph update algorithm contribute to performance improvement.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108133955",
                        "name": "Yanan Zhang"
                    },
                    {
                        "authorId": "2152163772",
                        "name": "Li Jin"
                    },
                    {
                        "authorId": "2108748852",
                        "name": "Xiaoyu Li"
                    },
                    {
                        "authorId": "2187845243",
                        "name": "Honqi Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5c4bece777bbc7c52cdd4bbb6e222163f6a580dd",
                "externalIds": {
                    "ArXiv": "2210.01425",
                    "DBLP": "conf/aaai/NieS0DH00LZ23",
                    "DOI": "10.1609/aaai.v37i11.26572",
                    "CorpusId": 254247336
                },
                "corpusId": 254247336,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/5c4bece777bbc7c52cdd4bbb6e222163f6a580dd",
                "title": "Unveiling the Black Box of PLMs with Semantic Anchors: Towards Interpretable Neural Semantic Parsing",
                "abstract": "The recent prevalence of pretrained language models (PLMs) has dramatically shifted the paradigm of semantic parsing, where the mapping from natural language utterances to structured logical forms is now formulated as a Seq2Seq task. Despite the promising performance, previous PLM-based approaches often suffer from hallucination problems due to their negligence of the structural information contained in the sentence, which essentially constitutes the key semantics of the logical forms. Furthermore, most works treat PLM as a black box in which the generation process of the target logical form is hidden beneath the decoder modules, which greatly hinders the model's intrinsic interpretability. To address these two issues, we propose to incorporate the current PLMs with a hierarchical decoder network. By taking the first-principle structures as the semantic anchors, we propose two novel intermediate supervision tasks, namely Semantic Anchor Extraction and Semantic Anchor Alignment, for training the hierarchical decoders and probing the model intermediate representations in a self-adaptive manner alongside the fine-tuning process. We conduct intensive experiments on several semantic parsing benchmarks and demonstrate that our approach can consistently outperform the baselines. More importantly, by analyzing the intermediate representations of the hierarchical decoders, our approach also makes a huge step toward the interpretability of PLMs in the domain of semantic parsing.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "115361209",
                        "name": "L. Nie"
                    },
                    {
                        "authorId": "2175443520",
                        "name": "Jiu Sun"
                    },
                    {
                        "authorId": "2108975906",
                        "name": "Yanlin Wang"
                    },
                    {
                        "authorId": "12723949",
                        "name": "Lun Du"
                    },
                    {
                        "authorId": "2055765060",
                        "name": "Lei Hou"
                    },
                    {
                        "authorId": "2133353675",
                        "name": "Juanzi Li"
                    },
                    {
                        "authorId": "2109750123",
                        "name": "Shi Han"
                    },
                    {
                        "authorId": "2140415600",
                        "name": "Dongmei Zhang"
                    },
                    {
                        "authorId": "2467444",
                        "name": "Jidong Zhai"
                    }
                ]
            }
        },
        {
            "contexts": [
                "EMBEDKGQA (Saxena et al., 2020) is a method that incorporates pre-trained knowledge graph embeddings into a KGQA model.",
                "EMBEDKGQA (Saxena et al., 2020) is a method that incorporates pre-trained knowledge",
                "\u2026to question answering include knowledge graph (KG) based methods, which use structured data to find the correct answer (Miller et al., 2016; Saxena et al., 2020); machine reading comprehension methods, which extract answers from input documents (Rajpurkar et al., 2016; Kwiatkowski et al.,\u2026",
                "Approaches to question answering include knowledge graph (KG) based methods, which use structured data to find the correct answer (Miller et al., 2016; Saxena et al., 2020); machine reading comprehension methods, which extract answers from input documents (Rajpurkar et al."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "f8770b2097e9c197a4d60253d696bd117d7ec883",
                "externalIds": {
                    "ACL": "2022.coling-1.138",
                    "DBLP": "conf/coling/SenAS22",
                    "ArXiv": "2210.01613",
                    "DOI": "10.48550/arXiv.2210.01613",
                    "CorpusId": 252693442
                },
                "corpusId": 252693442,
                "publicationVenue": {
                    "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
                    "name": "International Conference on Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Linguistics",
                        "COLING"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/coling/"
                },
                "url": "https://www.semanticscholar.org/paper/f8770b2097e9c197a4d60253d696bd117d7ec883",
                "title": "Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering",
                "abstract": "We introduce Mintaka, a complex, natural, and multilingual dataset designed for experimenting with end-to-end question-answering models. Mintaka is composed of 20,000 question-answer pairs collected in English, annotated with Wikidata entities, and translated into Arabic, French, German, Hindi, Italian, Japanese, Portuguese, and Spanish for a total of 180,000 samples. Mintaka includes 8 types of complex questions, including superlative, intersection, and multi-hop questions, which were naturally elicited from crowd workers. We run baselines over Mintaka, the best of which achieves 38% hits@1 in English and 31% hits@1 multilingually, showing that existing models have room for improvement. We release Mintaka at https://github.com/amazon-research/mintaka.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "50650468",
                        "name": "Priyanka Sen"
                    },
                    {
                        "authorId": "8129718",
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "authorId": "1741702",
                        "name": "Amir Saffari"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9da3d394037dc40a25230e6666cfe2ef82055cb3",
                "externalIds": {
                    "DOI": "10.1109/IAEAC54830.2022.9929997",
                    "CorpusId": 253423978
                },
                "corpusId": 253423978,
                "publicationVenue": {
                    "id": "7eadf6d0-2d35-410f-ad53-2635d1f830d7",
                    "name": "IEEE Advanced Information Technology, Electronic and Automation Control Conference",
                    "type": "conference",
                    "alternate_names": [
                        "IAEAC",
                        "IEEE Adv Inf Technol Electron Autom Control Conf"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9da3d394037dc40a25230e6666cfe2ef82055cb3",
                "title": "A new Knowledge Inference Approach Based on Multi-headed Attention Mechanism",
                "abstract": "To address the problem of sparse relationships between entities in the current knowledge graph and the increase in complexity and computational effort in knowledge inference, this paper proposes a knowledge inference method based on a multi-headed attention mechanism - the MAtt-BiGRU model. For a given triad, the path information is encoded by a two-way gated recurrent neural network, and the confidence of the model is calculated by aggregating it with the candidate paths with the help of the multi-headed attention mechanism, while the inference ability of the model is enhanced by dynamic negative sample training. Numerical experiments are conducted on the MAtt-BiGRU model using large knowledge graph datasets NELL-995 and FB15k-237. The experimental results show that the proposed MAtt-BiGRU model has better results in Mean Average Precision (MAP) and Mean Reciprocal Rank(MRR) performance evaluation metrics when compared with existing commonly used path ranking algorithms such as Path Ranking Algorithm(PRA) and Path-RNN, and the proposed model can more accurately reason about the relations between entities.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "25479930",
                        "name": "Yichao Cai"
                    },
                    {
                        "authorId": "2167041572",
                        "name": "Qingyu Yang"
                    },
                    {
                        "authorId": "2180456614",
                        "name": "Wei Chen"
                    },
                    {
                        "authorId": "2180516852",
                        "name": "Ge Wang"
                    },
                    {
                        "authorId": "46999350",
                        "name": "Taian Liu"
                    },
                    {
                        "authorId": "2110789461",
                        "name": "Xinying Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "994a0686d067005c51b08ce4abe6bf2f8540411d",
                "externalIds": {
                    "DBLP": "journals/eswa/MaMZMLZ23",
                    "DOI": "10.1016/j.eswa.2022.119013",
                    "CorpusId": 252949417
                },
                "corpusId": 252949417,
                "publicationVenue": {
                    "id": "987139ae-a65d-49bb-aaf6-fb764dc40b19",
                    "name": "Expert systems with applications",
                    "type": "journal",
                    "alternate_names": [
                        "Expert syst appl",
                        "Expert Systems With Applications",
                        "Expert Syst Appl"
                    ],
                    "issn": "0957-4174",
                    "url": "https://www.journals.elsevier.com/expert-systems-with-applications/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/expert-systems-with-applications",
                        "http://www.sciencedirect.com/science/journal/09574174"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/994a0686d067005c51b08ce4abe6bf2f8540411d",
                "title": "PANC: Prototype Augmented Neighbor Constraint instance completion in knowledge graphs",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47375371",
                        "name": "Ruixin Ma"
                    },
                    {
                        "authorId": "2143993602",
                        "name": "Yunlong Ma"
                    },
                    {
                        "authorId": "2108881320",
                        "name": "Hongyan Zhang"
                    },
                    {
                        "authorId": "2165282300",
                        "name": "Biao Mei"
                    },
                    {
                        "authorId": "2178362344",
                        "name": "Guangyue Lv"
                    },
                    {
                        "authorId": "144010790",
                        "name": "Liang Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ec6f16196a90cfc93b1ad0cc42c10fdde92bc4fd",
                "externalIds": {
                    "DBLP": "journals/ijon/WuZQWZL23",
                    "DOI": "10.1016/j.neucom.2022.10.023",
                    "CorpusId": 252956909
                },
                "corpusId": 252956909,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ec6f16196a90cfc93b1ad0cc42c10fdde92bc4fd",
                "title": "A dynamic graph expansion network for multi-hop knowledge base question answering",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1587665827",
                        "name": "Wenqing Wu"
                    },
                    {
                        "authorId": "144702363",
                        "name": "Zhenfang Zhu"
                    },
                    {
                        "authorId": "2178346377",
                        "name": "Jiangtao Qi"
                    },
                    {
                        "authorId": "2157137336",
                        "name": "Wenling Wang"
                    },
                    {
                        "authorId": "2119056403",
                        "name": "Guangyuan Zhang"
                    },
                    {
                        "authorId": "2137323",
                        "name": "Peiyu Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d5f428e6c7ae295fa940abfd729d92e8e55098eb",
                "externalIds": {
                    "DBLP": "journals/tai/TuWLJC22",
                    "DOI": "10.1109/TAI.2022.3149234",
                    "CorpusId": 246668203
                },
                "corpusId": 246668203,
                "publicationVenue": {
                    "id": "3c27e831-750f-45bc-9914-2148a5259eba",
                    "name": "IEEE Transactions on Artificial Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Artif Intell"
                    ],
                    "issn": "2691-4581",
                    "url": "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=9078688"
                },
                "url": "https://www.semanticscholar.org/paper/d5f428e6c7ae295fa940abfd729d92e8e55098eb",
                "title": "Context- and Sentiment-Aware Networks for Emotion Recognition in Conversation",
                "abstract": "Emotion recognition in conversation (ERC) has promising potential in many fields, such as recommendation systems, man\u2013machine interaction, and medical care. In contrast to other emotion identification tasks, conversation is essentially a process of dynamic interaction in which people often convey emotional messages relying on context and common-sense knowledge. In this article, we propose a context- and sentiment-aware framework, termed Sentic GAT, to solve this challenge. In Sentic GAT, common-sense knowledge is dynamically represented by the context- and sentiment-aware graph attention mechanism based on sentimental consistency, and context information is captured by the dialogue transformer (DT) with hierarchical multihead attention (HMAT), where HMAT is used to obtain the dependency of historical utterances on themselves and other utterances for better context representation. Additionally, we explore a contrastive loss to discriminate context-free and context-sensitive utterances in emotion identification to enhance context representation in straightforward conversations that directly express ideas. The experimental results show that context and sentimental information can promote the representation of common-sense knowledge, and the intra- and inter-dependency of contextual utterances effectively improve the performance of Sentic GAT. Moreover, our Sentic GAT using emotional intensity outperforms the most advanced model on the tested datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "65981454",
                        "name": "Geng Tu"
                    },
                    {
                        "authorId": "9481569",
                        "name": "Jintao Wen"
                    },
                    {
                        "authorId": "2155881308",
                        "name": "Cheng Liu"
                    },
                    {
                        "authorId": "2087582676",
                        "name": "Dazhi Jiang"
                    },
                    {
                        "authorId": "49943757",
                        "name": "E. Cambria"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3b5c311ac38a03458614bfb411aee114214939e4",
                "externalIds": {
                    "DBLP": "conf/ictai/WangLGZ22",
                    "DOI": "10.1109/ICTAI56018.2022.00074",
                    "CorpusId": 258219651
                },
                "corpusId": 258219651,
                "publicationVenue": {
                    "id": "ba1e9488-a629-4abe-a0c2-ec2c79c91616",
                    "name": "IEEE International Conference on Tools with Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Tool Artif Intell",
                        "ICTAI",
                        "IEEE Int Conf Tool Artif Intell",
                        "International Conference on Tools with Artificial Intelligence"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1488"
                },
                "url": "https://www.semanticscholar.org/paper/3b5c311ac38a03458614bfb411aee114214939e4",
                "title": "Path-aware Multi-hop Question Answering Over Knowledge Graph Embedding",
                "abstract": "Question answering over knowledge graph (KGQA) aims at answering questions posed over the knowledge graph (KG). Multi-hop KGQA requires multi-hop reasoning on KG to achieve the correct answer. Unfortunately, KGs are usually incomplete with many missing links, which poses additional challenges to KGQA. KG embedding-based KGQA methods have recently been proposed as a way to overcome this limitation. However, existing KG embedding-based KGQA methods fail to take full advantage of semantic correlations between questions and paths. Furthermore, their inference process is not easily explainable. To address these challenges, we propose a novel path-aware multi-hop KGQA model (PA-KGQA), which can fully capture semantic correlations between the paths and the questions in a feature-interactive manner. Specifically, we introduce a case-enhanced path retriever to evaluate the importance of paths between topic entities and candidate answer entities, and then propose an interactive convolutional neural network (ICNN) to model the interactions between paths and questions for mining richer correlation features. Experiments show that PA-KGQA achieves state-of-the-art results on multiple benchmark datasets and is explainable.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2110124127",
                        "name": "Jingchao Wang"
                    },
                    {
                        "authorId": "2154838642",
                        "name": "Weimin Li"
                    },
                    {
                        "authorId": "49813610",
                        "name": "Yixing Guo"
                    },
                    {
                        "authorId": "2143744718",
                        "name": "Xiaokang Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[33] propose a effective method to deal with multi-hop KGQA through sparse Knowledge graphs."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3ca8392fe6cd7ee556ed4da41042ef05ea06aebc",
                "externalIds": {
                    "DBLP": "journals/www/NingZLL23",
                    "PubMedCentral": "9523637",
                    "DOI": "10.1007/s11280-022-01100-8",
                    "CorpusId": 252645190,
                    "PubMed": "36196376"
                },
                "corpusId": 252645190,
                "publicationVenue": {
                    "id": "9cf86359-a093-42b4-8bc0-6fd56465c360",
                    "name": "World wide web (Bussum)",
                    "type": "journal",
                    "alternate_names": [
                        "World Wide Web",
                        "World wide web (bussum"
                    ],
                    "issn": "1386-145X",
                    "url": "https://link.springer.com/journal/11280"
                },
                "url": "https://www.semanticscholar.org/paper/3ca8392fe6cd7ee556ed4da41042ef05ea06aebc",
                "title": "EAGS: An extracting auxiliary knowledge graph model in multi-turn dialogue generation",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2144249976",
                        "name": "Bo Ning"
                    },
                    {
                        "authorId": "2051631542",
                        "name": "Deji Zhao"
                    },
                    {
                        "authorId": "100683845",
                        "name": "Xinyi Liu"
                    },
                    {
                        "authorId": "2108255847",
                        "name": "Guan-yu Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "aab8187522ec5d1e2cc5f392f5e948b85fc569f1",
                "externalIds": {
                    "DBLP": "journals/apin/CuiPBHHL23",
                    "DOI": "10.1007/s10489-022-04127-6",
                    "CorpusId": 252550919
                },
                "corpusId": 252550919,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/aab8187522ec5d1e2cc5f392f5e948b85fc569f1",
                "title": "Stepwise relation prediction with dynamic reasoning network for multi-hop knowledge graph question answering",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2093917610",
                        "name": "Hai Cui"
                    },
                    {
                        "authorId": "2068928731",
                        "name": "T. Peng"
                    },
                    {
                        "authorId": "145303560",
                        "name": "Tie Bao"
                    },
                    {
                        "authorId": "2093920100",
                        "name": "Ridong Han"
                    },
                    {
                        "authorId": "2111758731",
                        "name": "Jiayu Han"
                    },
                    {
                        "authorId": "2118467569",
                        "name": "Lu Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "KGs have drawn great research attention from the academia (Lin et al., 2021; Ji et al., 2022) and have been widely used to enhance downstream applications such as question answering (Saxena et al., 2020; Qiu et al., 2020) and recommendation systems (Anelli et al., 2021; Zhou et al., 2020).",
                ", 2022) and have been widely used to enhance downstream applications such as question answering (Saxena et al., 2020; Qiu et al., 2020) and recommendation systems (Anelli et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a8741a1228550ca528ccd4e036928855f8786820",
                "externalIds": {
                    "ACL": "2022.coling-1.181",
                    "DBLP": "conf/coling/CaiMMYZL22",
                    "ArXiv": "2209.09677",
                    "DOI": "10.48550/arXiv.2209.09677",
                    "CorpusId": 252383242
                },
                "corpusId": 252383242,
                "publicationVenue": {
                    "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
                    "name": "International Conference on Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Linguistics",
                        "COLING"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/coling/"
                },
                "url": "https://www.semanticscholar.org/paper/a8741a1228550ca528ccd4e036928855f8786820",
                "title": "A Simple Temporal Information Matching Mechanism for Entity Alignment between Temporal Knowledge Graphs",
                "abstract": "Entity alignment (EA) aims to find entities in different knowledge graphs (KGs) that refer to the same object in the real world. Recent studies incorporate temporal information to augment the representations of KGs. The existing methods for EA between temporal KGs (TKGs) utilize a time-aware attention mechanisms to incorporate relational and temporal information into entity embeddings. The approaches outperform the previous methods by using temporal information. However, we believe that it is not necessary to learn the embeddings of temporal information in KGs since most TKGs have uniform temporal representations. Therefore, we propose a simple GNN model combined with a temporal information matching mechanism, which achieves better performance with less time and fewer parameters. Furthermore, since alignment seeds are difficult to label in real-world applications, we also propose a method to generate unsupervised alignment seeds via the temporal information of TKG. Extensive experiments on public datasets indicate that our supervised method significantly outperforms the previous methods and the unsupervised one has competitive performance.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2190715271",
                        "name": "Lianshang Cai"
                    },
                    {
                        "authorId": "49723181",
                        "name": "Xin Mao"
                    },
                    {
                        "authorId": "2153935133",
                        "name": "Meirong Ma"
                    },
                    {
                        "authorId": "1491238705",
                        "name": "Haonan Yuan"
                    },
                    {
                        "authorId": "1490772341",
                        "name": "Jianchao Zhu"
                    },
                    {
                        "authorId": "49284832",
                        "name": "Man Lan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[48] employed the RoBERTa pretraining model to encode complex questions as a relation vector eq , forming a triple with a head entity eh and a tail entity et , and used the ComplEx score [49] to find the answer entity.",
                "EmbedKGQA(2020) [48] employs RoBERTa to encode a complex question as a relation vector eq , which forms a triple with the head entity and tail entity, and uses the ComplEx score to determine the answer entity."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4ef6e990040e84d4830bf39f90989278e8f92219",
                "externalIds": {
                    "DBLP": "journals/apin/CaoL23",
                    "DOI": "10.1007/s10489-022-04123-w",
                    "CorpusId": 252428561
                },
                "corpusId": 252428561,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4ef6e990040e84d4830bf39f90989278e8f92219",
                "title": "ReLMKG: reasoning with pre-trained language models and knowledge graphs for complex question answering",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2149215129",
                        "name": "Xing Cao"
                    },
                    {
                        "authorId": "2118113853",
                        "name": "Yun Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Unlike other knowledge-based multi-hop QA datasets (Welbl et al., 2018; Talmor and Berant, 2018; Saxena et al., 2020) that restrict the final answer to the content",
                "Unlike other knowledge-based multi-hop QA datasets (Welbl et al., 2018; Talmor and Berant, 2018; Saxena et al., 2020) that restrict the final answer to the content of explicit knowledge bases, all QA pairs in the HotpotQA are collected from Wikipedia."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "432b1611029cb9c8ff7a632bcef0f47f0b879004",
                "externalIds": {
                    "ArXiv": "2209.06923",
                    "DBLP": "journals/corr/abs-2209-06923",
                    "ACL": "2022.coling-1.154",
                    "DOI": "10.48550/arXiv.2209.06923",
                    "CorpusId": 252280663
                },
                "corpusId": 252280663,
                "publicationVenue": {
                    "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
                    "name": "International Conference on Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Linguistics",
                        "COLING"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/coling/"
                },
                "url": "https://www.semanticscholar.org/paper/432b1611029cb9c8ff7a632bcef0f47f0b879004",
                "title": "Prompt-based Conservation Learning for Multi-hop Question Answering",
                "abstract": "Multi-hop question answering (QA) requires reasoning over multiple documents to answer a complex question and provide interpretable supporting evidence. However, providing supporting evidence is not enough to demonstrate that a model has performed the desired reasoning to reach the correct answer. Most existing multi-hop QA methods fail to answer a large fraction of sub-questions, even if their parent questions are answered correctly. In this paper, we propose the Prompt-based Conservation Learning (PCL) framework for multi-hop QA, which acquires new knowledge from multi-hop QA tasks while conserving old knowledge learned on single-hop QA tasks, mitigating forgetting. Specifically, we first train a model on existing single-hop QA tasks, and then freeze this model and expand it by allocating additional sub-networks for the multi-hop QA task. Moreover, to condition pre-trained language models to stimulate the kind of reasoning required for specific multi-hop questions, we learn soft prompts for the novel sub-networks to perform type-specific reasoning. Experimental results on the HotpotQA benchmark show that PCL is competitive for multi-hop QA and retains good performance on the corresponding single-hop sub-questions, demonstrating the efficacy of PCL in mitigating knowledge loss by forgetting.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2244794",
                        "name": "Zhenyun Deng"
                    },
                    {
                        "authorId": "2116513959",
                        "name": "Yonghua Zhu"
                    },
                    {
                        "authorId": "2144353312",
                        "name": "Yang Chen"
                    },
                    {
                        "authorId": "2089733563",
                        "name": "Qianqian Qi"
                    },
                    {
                        "authorId": "2819135",
                        "name": "M. Witbrock"
                    },
                    {
                        "authorId": "29838575",
                        "name": "P. Riddle"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We utilize two evaluation metrics Hits@1 and F1 that are widely applied in the previous work [20,19,18,17].",
                "EmbedKGQA[17] utilizes pre-trained knowledge embedding to predict answer",
                "Following [17], we prune the knowledge graph to contain the entities within 2 hops away from the mentioned entity."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "44915a445531b5487e39a8d8b1b8983e3037abc3",
                "externalIds": {
                    "DBLP": "conf/nlpcc/DuHZZ22",
                    "ArXiv": "2209.03005",
                    "DOI": "10.48550/arXiv.2209.03005",
                    "CorpusId": 252111070
                },
                "corpusId": 252111070,
                "publicationVenue": {
                    "id": "640a5acb-a481-4a3e-a751-1eb880600a99",
                    "name": "Natural Language Processing and Chinese Computing",
                    "type": "conference",
                    "alternate_names": [
                        "NLPCC",
                        "Nat Lang Process Chin Comput"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/44915a445531b5487e39a8d8b1b8983e3037abc3",
                "title": "Knowledge-enhanced Iterative Instruction Generation and Reasoning for Knowledge Base Question Answering",
                "abstract": ". Multi-hop Knowledge Base Question Answering(KBQA) aims to \ufb01nd the answer entity in a knowledge base which is several hops from the topic entity mentioned in the question. Existing Retrieval-based approaches \ufb01rst generate instructions from the question and then use them to guide the multi-hop reasoning on the knowledge graph. As the instructions are \ufb01xed during the whole reasoning procedure and the knowledge graph is not considered in instruction generation, the model cannot revise its mistake once it predicts an intermediate entity incorrectly. To han-dle this, we propose KBIGER ( K nowledge B ase I terative Instruction GE nerating and R easoning), a novel and e\ufb03cient approach to generate the instructions dynamically with the help of reasoning graph. Instead of generating all the instructions before reasoning, we take the ( k \u2212 1)-th reasoning graph into consideration to build the k -th instruction. In this way, the model could check the prediction from the graph and generate new instructions to revise the incorrect prediction of intermediate entities. We do experiments on two multi-hop KBQA benchmarks and outperform the existing approaches, becoming the new-state-of-the-art. Further experiments show our method does detect the incorrect prediction of intermediate entities and has the ability to revise such errors.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144549196",
                        "name": "Haowei Du"
                    },
                    {
                        "authorId": "2007771781",
                        "name": "Quzhe Huang"
                    },
                    {
                        "authorId": "2111574159",
                        "name": "Chen Zhang"
                    },
                    {
                        "authorId": "144060462",
                        "name": "Dongyan Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "35a5124883012be53e1505bb5f9943222a7c048e",
                "externalIds": {
                    "ACL": "2022.coling-1.156",
                    "ArXiv": "2209.00870",
                    "DBLP": "journals/corr/abs-2209-00870",
                    "DOI": "10.48550/arXiv.2209.00870",
                    "CorpusId": 252070626
                },
                "corpusId": 252070626,
                "publicationVenue": {
                    "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
                    "name": "International Conference on Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Linguistics",
                        "COLING"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/coling/"
                },
                "url": "https://www.semanticscholar.org/paper/35a5124883012be53e1505bb5f9943222a7c048e",
                "title": "Exploiting Hybrid Semantics of Relation Paths for Multi-hop Question Answering over Knowledge Graphs",
                "abstract": "Answering natural language questions on knowledge graphs (KGQA) remains a great challenge in terms of understanding complex questions via multi-hop reasoning. Previous efforts usually exploit large-scale entity-related text corpus or knowledge graph (KG) embeddings as auxiliary information to facilitate answer selection. However, the rich semantics implied in off-the-shelf relation paths between entities is far from well explored. This paper proposes improving multi-hop KGQA by exploiting relation paths\u2019 hybrid semantics. Specifically, we integrate explicit textual information and implicit KG structural features of relation paths based on a novel rotate-and-scale entity link prediction framework. Extensive experiments on three existing KGQA datasets demonstrate the superiority of our method, especially in multi-hop scenarios. Further investigation confirms our method\u2019s systematical coordination between questions and relation paths to identify answer entities.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2184030467",
                        "name": "Zile Qiao"
                    },
                    {
                        "authorId": "145235143",
                        "name": "Wei Ye"
                    },
                    {
                        "authorId": "2118942397",
                        "name": "Tong Zhang"
                    },
                    {
                        "authorId": "8046305",
                        "name": "Tong Mo"
                    },
                    {
                        "authorId": "2139261376",
                        "name": "Weiping Li"
                    },
                    {
                        "authorId": "1705434",
                        "name": "Shikun Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "eac236677a39c8434961c1b67ff6ee543535be71",
                "externalIds": {
                    "DBLP": "journals/kbs/FanCW22",
                    "DOI": "10.1016/j.knosys.2022.109857",
                    "CorpusId": 252129335
                },
                "corpusId": 252129335,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/eac236677a39c8434961c1b67ff6ee543535be71",
                "title": "Knowledge base question answering via path matching",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "9399859",
                        "name": "C. Fan"
                    },
                    {
                        "authorId": "2190905328",
                        "name": "Wentong Chen"
                    },
                    {
                        "authorId": "2109034826",
                        "name": "Yuexin Wu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "talof-China) [22], for instance, Yago [25], Wikidata [2] and DBPedia [13] are representative KG projects."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0d51c4b1ade3b4e0c1dee53849fdc5def6798f92",
                "externalIds": {
                    "DBLP": "journals/nca/PengJ22",
                    "DOI": "10.1007/s00521-022-07688-2",
                    "CorpusId": 251713643
                },
                "corpusId": 251713643,
                "publicationVenue": {
                    "id": "702e18c0-c8c6-4800-a398-42aa159394d1",
                    "name": "Neural computing & applications (Print)",
                    "type": "journal",
                    "alternate_names": [
                        "Neural comput  appl (print",
                        "Neural Comput Appl",
                        "Neural Computing and Applications"
                    ],
                    "issn": "0941-0643",
                    "url": "https://link.springer.com/journal/521"
                },
                "url": "https://www.semanticscholar.org/paper/0d51c4b1ade3b4e0c1dee53849fdc5def6798f92",
                "title": "Dynamic memory supported dialog generation model based on commonsense knowledge graph",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40659308",
                        "name": "Dunlu Peng"
                    },
                    {
                        "authorId": "2182278143",
                        "name": "Yunjie Jiang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In many knowledge-centric artificial intelligence (AI) applications, such as question answering (Huang et al., 2019; Saxena et al., 2020), information extraction (Hoffmann et al., 2011; Daiber et al., 2013), and recommendation (Wang et al., 2019; Xian et al., 2019), KG plays an important role as it\u2026",
                "In many knowledge-centric artificial intelligence (AI) applications, such as question answering (Huang et al., 2019; Saxena et al., 2020), information extraction (Hoffmann et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5456aef84577e61bc9118464bdc520e0d0983981",
                "externalIds": {
                    "ACL": "2023.acl-long.591",
                    "DBLP": "journals/corr/abs-2208-09137",
                    "ArXiv": "2208.09137",
                    "DOI": "10.48550/arXiv.2208.09137",
                    "CorpusId": 251710245
                },
                "corpusId": 251710245,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/5456aef84577e61bc9118464bdc520e0d0983981",
                "title": "GreenKGC: A Lightweight Knowledge Graph Completion Method",
                "abstract": "Knowledge graph completion (KGC) aims to discover missing relationships between entities in knowledge graphs (KGs). Most prior KGC work focuses on learning embeddings for entities and relations through a simple score function. Yet, a higher-dimensional embedding space is usually required for a better reasoning capability, which leads to larger model size and hinders applicability to real-world problems (e.g., large-scale KGs or mobile/edge computing). A lightweight modularized KGC solution, called GreenKGC, is proposed in this work to address this issue. GreenKGC consists of three modules: representation learning, feature pruning, and decision learning, to extract discriminant KG features and make accurate predictions on missing relationships using classifiers and negative sampling. Experimental results demonstrate that, in low dimensions, GreenKGC can outperform SOTA methods in most datasets. In addition, low-dimensional GreenKGC can achieve competitive or even better performance against high-dimensional models with a much smaller model size.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "6853732",
                        "name": "Yun Cheng Wang"
                    },
                    {
                        "authorId": "46216042",
                        "name": "Xiou Ge"
                    },
                    {
                        "authorId": "144461647",
                        "name": "Bin Wang"
                    },
                    {
                        "authorId": "9363144",
                        "name": "C.-C. Jay Kuo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following the definition in [20], we assume that all the answer entities exist in the knowledge graph and each question in multi-hop KBQA only contains a single topic entity vQ \u2208 V and vQ is given.",
                "KV-Mem [16] and EmbedKGQA [20] are embedding and deep learning-based methods that use deep learning networks to embed the question into a point in the embedding space and find answers according to a similarity function.",
                "We note that some work [20] treats the knowledge graph completion task as a single-hop knowledge graph question answering task due to their interchangeable properties.",
                "For multi-hop question answering, Pullnet [21] first extracts question specific subgraphs, and then performs multi-hop reasoning on the extracted subgraph via graph neural networks to find answers, EmbedKGQA [20] uses a pre-trained BERT model to map natural language questions to relation embeddings and finds answers by ComplEx [24].",
                "\u2022 EmbedKGQA [20] conducts multi-hop reasoning through matching pre-trained entity embeddings with question embedding obtained from RoBERTa.",
                "Following the standard setup in KGQA [20], we evaluate the accuracy using the Hits@1 metrics.",
                "Despite the potential close relationship betweenmulti-hop KGQA and KGC tasks [20], existing works usually treat them as two separate tasks without considering their reciprocal benefits."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "105a4a3fe84154ef0812686204dd824a9f04ec40",
                "externalIds": {
                    "DBLP": "conf/kdd/LiuDXXT22",
                    "DOI": "10.1145/3534678.3539289",
                    "CorpusId": 251518441
                },
                "corpusId": 251518441,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/105a4a3fe84154ef0812686204dd824a9f04ec40",
                "title": "Joint Knowledge Graph Completion and Question Answering",
                "abstract": "Knowledge graph reasoning plays a pivotal role in many real-world applications, such as network alignment, computational fact-checking, recommendation, and many more. Among these applications, knowledge graph completion (KGC) and multi-hop question answering over knowledge graph (Multi-hop KGQA) are two representative reasoning tasks. In the vast majority of the existing works, the two tasks are considered separately with different models or algorithms. However, we envision that KGC and Multi-hop KGQA are closely related to each other. Therefore, the two tasks will benefit from each other if they are approached adequately. In this work, we propose a neural model named BiNet to jointly handle KGC and multi-hop KGQA, and formulate it as a multi-task learning problem. Specifically, our proposed model leverages a shared embedding space and an answer scoring module, which allows the two tasks to automatically share latent features and learn the interactions between natural language question decoder and answer scoring module. Compared to the existing methods, the proposed BiNet model addresses both multi-hop KGQA and KGC tasks simultaneously with superior performance. Experiment results show that BiNet outperforms state-of-the-art methods on a wide range of KGQA and KGC benchmark datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1505803281",
                        "name": "Lihui Liu"
                    },
                    {
                        "authorId": "22607329",
                        "name": "Boxin Du"
                    },
                    {
                        "authorId": "1684988",
                        "name": "Jiejun Xu"
                    },
                    {
                        "authorId": "35846319",
                        "name": "Yinglong Xia"
                    },
                    {
                        "authorId": "2058143613",
                        "name": "H. Tong"
                    }
                ]
            }
        },
        {
            "contexts": [
                "KGQA & TKGQA Baselines For EmbedKGQA, we use the trained ComplEx representations as its supporting KG information.",
                "We also consider one KGQA method EmbedKGQA [25], and two TKGQA methods, i.e., CronKGQA [24] and TempoQR [20] as baselines.",
                "We use the EmbedKGQA and CronKGQA implementation provided in the repository of CronKGQA17.",
                "We also consider one KGQA method EmbedKGQA [25], and two TKGQA methods, i.",
                "We run EmbedKGQA on top of the KG representations trained with ComplEx on ICEWS21, and run TKGQA baselines on top of the TKG representations trained with TComplEx.",
                "We observe that EmbedKGQA achieves a better performance than BERT and RoBERTa, showing that employing KG representations helps TKGQA."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "4f393ff98e8b4db9b2d06d7e82e32b6589907326",
                "externalIds": {
                    "ArXiv": "2208.06501",
                    "CorpusId": 259991877
                },
                "corpusId": 259991877,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4f393ff98e8b4db9b2d06d7e82e32b6589907326",
                "title": "ForecastTKGQuestions: A Benchmark for Temporal Question Answering and Forecasting over Temporal Knowledge Graphs",
                "abstract": "Question answering over temporal knowledge graphs (TKGQA) has recently found increasing interest. TKGQA requires temporal reasoning techniques to extract the relevant information from temporal knowledge bases. The only existing TKGQA dataset, i.e., CronQuestions, consists of temporal questions based on the facts from a fixed time period, where a temporal knowledge graph (TKG) spanning the same period can be fully used for answer inference, allowing the TKGQA models to use even the future knowledge to answer the questions based on the past facts. In real-world scenarios, however, it is also common that given the knowledge until now, we wish the TKGQA systems to answer the questions asking about the future. As humans constantly seek plans for the future, building TKGQA systems for answering such forecasting questions is important. Nevertheless, this has still been unexplored in previous research. In this paper, we propose a novel task: forecasting question answering over temporal knowledge graphs. We also propose a large-scale TKGQA benchmark dataset, i.e., ForecastTKGQuestions, for this task. It includes three types of questions, i.e., entity prediction, yes-no, and fact reasoning questions. For every forecasting question in our dataset, QA models can only have access to the TKG information before the timestamp annotated in the given question for answer inference. We find that the state-of-the-art TKGQA methods perform poorly on forecasting questions, and they are unable to answer yes-no questions and fact reasoning questions. To this end, we propose ForecastTKGQA, a TKGQA model that employs a TKG forecasting module for future inference, to answer all three types of questions. Experimental results show that ForecastTKGQA outperforms recent TKGQA methods on the entity prediction questions, and it also shows great effectiveness in answering the other two types of questions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2046761003",
                        "name": "Zifeng Ding"
                    },
                    {
                        "authorId": "2203330037",
                        "name": "Zong-Xun Li"
                    },
                    {
                        "authorId": "2036741466",
                        "name": "Ruoxia Qi"
                    },
                    {
                        "authorId": "2181618414",
                        "name": "Jingpei Wu"
                    },
                    {
                        "authorId": "2147293727",
                        "name": "Bailan He"
                    },
                    {
                        "authorId": "10684484",
                        "name": "Yunpu Ma"
                    },
                    {
                        "authorId": "144861196",
                        "name": "Zhao Meng"
                    },
                    {
                        "authorId": "2224083544",
                        "name": "Shuo Chen"
                    },
                    {
                        "authorId": "2072387342",
                        "name": "Ruotong Liao"
                    },
                    {
                        "authorId": "2223193538",
                        "name": "Zhen Han"
                    },
                    {
                        "authorId": "1742501819",
                        "name": "Volker Tresp"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "60824f4ba36f3c0dc812b335188aa5c27bf454bd",
                "externalIds": {
                    "DBLP": "journals/kbs/ChenYFHJZW22",
                    "DOI": "10.1016/j.knosys.2022.109576",
                    "CorpusId": 251262376
                },
                "corpusId": 251262376,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/60824f4ba36f3c0dc812b335188aa5c27bf454bd",
                "title": "Staged query graph generation based on answer type for question answering over knowledge base",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2194754978",
                        "name": "Haoyuan Chen"
                    },
                    {
                        "authorId": "2167813037",
                        "name": "Fei Ye"
                    },
                    {
                        "authorId": "2167853610",
                        "name": "Yuankai Fan"
                    },
                    {
                        "authorId": "2240008",
                        "name": "Zhenying He"
                    },
                    {
                        "authorId": "3261435",
                        "name": "Yinan Jing"
                    },
                    {
                        "authorId": "2152981229",
                        "name": "Kai Zhang"
                    },
                    {
                        "authorId": "38871997",
                        "name": "X. S. Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Saxena and others [9] first used embedding in multi-hop KBQA, which aims to enhance the multi-hop reasoning ability of the model."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1bbc14f4d28c28849664a36ed7fbee65c9f79337",
                "externalIds": {
                    "DOI": "10.1109/MLCCIM55934.2022.00016",
                    "CorpusId": 253880831
                },
                "corpusId": 253880831,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1bbc14f4d28c28849664a36ed7fbee65c9f79337",
                "title": "Multi-hop Question Answering Model Based on Chinese Open Domain Knowledge Graph",
                "abstract": "Existing question answering systems based on Chinese knowledge graph mainly model the single-hop problem, that is, the answer can be obtained by querying a single triplet. In this paper, a multi-hop question answering system of Chinese open domain knowledge graph is modeled, which mainly includes a named entity recognition module and a question classification module based on ERNIE model, a multi-hop relationship prediction module based on seq2seq model, and an entity linking module based on multi-feature fusion. The multi-hop question answering in CKBQA field is realized by pipeline. The experimental results show that the F1-score of the system is 0.62 on CCKS2021 dataset, and the experimental results are competitive.It should be noted that the Chinese examples in the text have been translated into English descriptions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2192206214",
                        "name": "Bin Lang"
                    },
                    {
                        "authorId": "2144398760",
                        "name": "Yufeng Ma"
                    },
                    {
                        "authorId": "2163664335",
                        "name": "Yongjin Zhang"
                    },
                    {
                        "authorId": "2192206942",
                        "name": "Da Lei"
                    },
                    {
                        "authorId": "2112734156",
                        "name": "Changjun Fan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "EmbedKGQA [12] first embeds questions into the complex space, and then uses relation information in the candidate answer set to select the answer.",
                "[12] is the first to apply KG embedding to multi-hop QA.",
                "[12] proposed embedding the question in the KG semantic space and using the embedding scoring function to find the candidate answers."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "8482c8e00f4a55a8e8bbdaf40cb5ce8b581b7ced",
                "externalIds": {
                    "DBLP": "journals/apin/GuoWZLX23",
                    "DOI": "10.1007/s10489-022-03927-0",
                    "CorpusId": 251152309
                },
                "corpusId": 251152309,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/8482c8e00f4a55a8e8bbdaf40cb5ce8b581b7ced",
                "title": "A knowledge inference model for question answering on an incomplete knowledge graph",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2179528947",
                        "name": "Qimeng Guo"
                    },
                    {
                        "authorId": "2167552914",
                        "name": "Xue Wang"
                    },
                    {
                        "authorId": "144702363",
                        "name": "Zhenfang Zhu"
                    },
                    {
                        "authorId": "2137323",
                        "name": "Peiyu Liu"
                    },
                    {
                        "authorId": "2457650",
                        "name": "Liancheng Xu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Both our method and EmbedKGQA in the Full-KG setting achieve better accuracy than the corresponding result in the Half-KG setting.",
                "Following EmbedKGQA, we limit the KG to a subset of Freebase that contains all relational triples within 2-hops of any entity specified in the WebQuestionsSP questions.",
                "We follow EmbedKGQA [32], a recent popular embedding-based KGQA method, to build a QA pipeline with our \u03bcKG.",
                "The typical pipeline of using KG embeddings to answer natural language questions [32] is learning to align the question representation (encoded by a pre-trained language model like BERT [13]) with the answer entity\u2019s embedding (encoded by a KG embedding model like ComplEx [44]).",
                "This is because our learned embeddings of FB4QA can benefit from the background KG, and thus are more expressive than those in EmbedKGQA.",
                "EmbedKGQA consists of three modules.",
                "The KG embedding model used in EmbedKGQA is ComplEx.",
                "To study the effect of KG sparsity on QA performance, following EmbedKGQA, the FB4QA is used for two settings: Half-FB4QA and Full-FB4QA.",
                "For a fair comparison, we keep other modules in our pipeline the same as those in EmbedKGQA.",
                "We follow EmbedKGQA [32], a recent popular embedding-based KGQA method, to build a QA pipeline with our \u00b5KG.",
                "We can see from the table that EmbedKGQA + Wikidata outperforms the baseline EmbedKGQA in all three settings."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "11f63ce1444897929af304891ae24b0b3a95004e",
                "externalIds": {
                    "DBLP": "conf/semweb/LuoSH22",
                    "ArXiv": "2207.11442",
                    "DOI": "10.48550/arXiv.2207.11442",
                    "CorpusId": 251041000
                },
                "corpusId": 251041000,
                "publicationVenue": {
                    "id": "efa3ff7a-4d96-44a1-a022-a683408919b6",
                    "name": "International Workshop on the Semantic Web",
                    "type": "conference",
                    "alternate_names": [
                        "SemWeb",
                        "Int Workshop Semantic Web"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/11f63ce1444897929af304891ae24b0b3a95004e",
                "title": "\u03bcKG: A Library for Multi-source Knowledge Graph Embeddings and Applications",
                "abstract": "This paper presents $\\mu\\text{KG}$, an open-source Python library for representation learning over knowledge graphs. $\\mu\\text{KG}$ supports joint representation learning over multi-source knowledge graphs (and also a single knowledge graph), multiple deep learning libraries (PyTorch and TensorFlow2), multiple embedding tasks (link prediction, entity alignment, entity typing, and multi-source link prediction), and multiple parallel computing modes (multi-process and multi-GPU computing). It currently implements 26 popular knowledge graph embedding models and supports 16 benchmark datasets. $\\mu\\text{KG}$ provides advanced implementations of embedding techniques with simplified pipelines of different tasks. It also comes with high-quality documentation for ease of use. $\\mu\\text{KG}$ is more comprehensive than existing knowledge graph embedding libraries. It is useful for a thorough comparison and analysis of various embedding models and tasks. We show that the jointly learned embeddings can greatly help knowledge-powered downstream tasks, such as multi-hop knowledge graph question answering. We will stay abreast of the latest developments in the related fields and incorporate them into $\\mu\\text{KG}$.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2115606367",
                        "name": "Xin Luo"
                    },
                    {
                        "authorId": "2109745316",
                        "name": "Zequn Sun"
                    },
                    {
                        "authorId": "145066190",
                        "name": "Wei Hu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To alleviate the above problem, other types of works [8], [31] aim to leverage large-scale pre-trained KG embeddings to alleviate the influence of incomplete KGs, which may recover some valuable knowledge using the triple representation learning such as TransE [9].",
                ", Memory Network [41], KEQA [17] and EmbedKGQA [31].",
                "It is worth mentioning that our method and EmbedKGQA both apply the TransE pre-trained embeddings to the KG\u2019s initialization\nphase.",
                "EmbedKGQA [31] improves over the prior state-of-the-art in incomplete KG, which embeds the question and whole entities into the high dimensional space for solving multi-hop KGQA task over sparse KG.",
                "EmbedKGQA [31] improves over the prior state-of-the-art in incomplete KG, which embeds the question and whole entities into the high dimensional space for solving multi-hop KGQA task over sparse KG. PullNet [4] achieves state-of-theart performance, which uses an iterative process to construct a question-specific subgraph that contains information relevant to the question.",
                "Other types of work focus on learning latent representations implicitly, and then produce the final answer by measuring the similarity between the user\u2019s query and golden answer, e.g., Memory Network [41], KEQA [17] and EmbedKGQA [31]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "b2159e6ad8efb3e057e7d8f1d9e9ea1f5a3b5697",
                "externalIds": {
                    "DBLP": "conf/ijcnn/ZanWZZWZWGW22",
                    "DOI": "10.1109/IJCNN55064.2022.9892700",
                    "CorpusId": 252625996
                },
                "corpusId": 252625996,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/b2159e6ad8efb3e057e7d8f1d9e9ea1f5a3b5697",
                "title": "Complex Question Answering over Incomplete Knowledge Graph as N-ary Link Prediction",
                "abstract": "The Question Answering over Knowledge Graph (KGQA) task seeks entities (answers) from the Knowledge Graph (KG) in order to answer natural language questions. In practice, KG is often incomplete, with numerous missing links and nodes. With such an incomplete KG, it is tricky to use the semantics inside the KG to get the golden answers, particularly for complex questions. Some current efforts concentrate on using external corpora to overcome KG sparsity; however, identifying and obtaining the corpora is challenging. Other types of work aim to leverage the pre-trained embeddings to resolve the issue but perform slightly worse on complex questions involving numerous triple facts in KG. To address the aforementioned problems, we present a framework CAPKGQA, which transforms Complex KGQA into an n-Ary link Prediction task capable of explicitly modeling complex questions. Furthermore, previous methods also suffer from incomplete KG throughout the candidate answer generation phase. Therefore, we devise an embedding-based retrieval strategy to extract more reliable candidate answers from incomplete KG. Extensive experiments reveal that our approach beats the state-of-the-art models on incomplete and complex KGQA tasks by a significant margin.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2134434187",
                        "name": "Daoguang Zan"
                    },
                    {
                        "authorId": "2592528",
                        "name": "Sirui Wang"
                    },
                    {
                        "authorId": "2108854068",
                        "name": "Hongzhi Zhang"
                    },
                    {
                        "authorId": "1423651904",
                        "name": "Kun Zhou"
                    },
                    {
                        "authorId": "2144356294",
                        "name": "Wei Wu"
                    },
                    {
                        "authorId": "2542603",
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "authorId": "2168285051",
                        "name": "Bingchao Wu"
                    },
                    {
                        "authorId": "36923691",
                        "name": "Bei Guan"
                    },
                    {
                        "authorId": "2108097250",
                        "name": "Yongji Wang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c282ffa48e3a1636c22351839e7a2e1148b8fae3",
                "externalIds": {
                    "DBLP": "conf/ijcnn/LiCD22",
                    "DOI": "10.1109/IJCNN55064.2022.9892550",
                    "CorpusId": 252626021
                },
                "corpusId": 252626021,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/c282ffa48e3a1636c22351839e7a2e1148b8fae3",
                "title": "Multi-hop Question Answering with Knowledge Graph Embedding in a Similar Semantic Space",
                "abstract": "Multi-hop Question Answering using the knowledge graph (KG) as a data source requires subject entities and relations that are obtained from natural language questions; the answers are then obtained by reasoning through multiple triples in the KG. However, even large KGs are incomplete, and the reasoning often fails to obtain the correct answer due to the lack of relations in the KG. Recently, researchers have proposed introducing KG embedding into the multi-hop knowledge graph question answering (KGQA) field to solve the incompleteness of the KG. However, these methods embed the question and the KG into different semantic spaces, and it is difficult to obtain the correct answers. Furthermore, due to the limitation of the question embedding sequence, the contribution of each word to the question semantics cannot be distinguished. To overcome the above problems, this paper proposes an effective multi-hop KGQA model, TIPNet, using relation embeddings in knowledge graph triples, which uses the idea of translation models to narrow the semantic spatial distance between question embeddings and KG embeddings. At the same time, TIP weighting technology is proposed to distinguish the semantic contribution of words to the question. To validate the performance of TIPNet, experiments were conducted on the WebQSP and CWQ datasets, and the model reached advanced levels under both KG-full and KG-half settings.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49515895",
                        "name": "Fengying Li"
                    },
                    {
                        "authorId": "2186638985",
                        "name": "Mingdong Chen"
                    },
                    {
                        "authorId": "1716563",
                        "name": "Rongsheng Dong"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", to predict missing facts in an incomplete KG [15], for drug discovery in a biomedical KG [14], for question answering [18,19], or visual relationship detection [2]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d73c719e34acacd663446e34d5161d49d7356413",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2207-04979",
                    "ArXiv": "2207.04979",
                    "DOI": "10.48550/arXiv.2207.04979",
                    "CorpusId": 250426050
                },
                "corpusId": 250426050,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d73c719e34acacd663446e34d5161d49d7356413",
                "title": "Start Small, Think Big: On Hyperparameter Optimization for Large-Scale Knowledge Graph Embeddings",
                "abstract": "Knowledge graph embedding (KGE) models are an effective and popular approach to represent and reason with multi-relational data. Prior studies have shown that KGE models are sensitive to hyperparameter settings, however, and that suitable choices are dataset-dependent. In this paper, we explore hyperparameter optimization (HPO) for very large knowledge graphs, where the cost of evaluating individual hyperparameter configurations is excessive. Prior studies often avoided this cost by using various heuristics; e.g., by training on a subgraph or by using fewer epochs. We systematically discuss and evaluate the quality and cost savings of such heuristics and other low-cost approximation techniques. Based on our findings, we introduce GraSH, an efficient multi-fidelity HPO algorithm for large-scale KGEs that combines both graph and epoch reduction techniques and runs in multiple rounds of increasing fidelities. We conducted an experimental study and found that GraSH obtains state-of-the-art results on large graphs at a low cost (three complete training runs in total).",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2008201285",
                        "name": "Adrian Kochsiek"
                    },
                    {
                        "authorId": "2175650669",
                        "name": "Fritz Niesel"
                    },
                    {
                        "authorId": "1777107",
                        "name": "Rainer Gemulla"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f5f8fa8f573aa02d8c3edc2b962144c890f9b98a",
                "externalIds": {
                    "DBLP": "conf/sigir/Chakrabarti22",
                    "DOI": "10.1145/3477495.3532679",
                    "CorpusId": 250340278
                },
                "corpusId": 250340278,
                "publicationVenue": {
                    "id": "8dce23a9-44e0-4381-a39e-2acc1edff700",
                    "name": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                    "type": "conference",
                    "alternate_names": [
                        "International ACM SIGIR Conference on Research and Development in Information Retrieval",
                        "Int ACM SIGIR Conf Res Dev Inf Retr",
                        "SIGIR",
                        "Annu Int ACM SIGIR Conf Res Dev Inf Retr"
                    ],
                    "url": "http://www.acm.org/sigir/"
                },
                "url": "https://www.semanticscholar.org/paper/f5f8fa8f573aa02d8c3edc2b962144c890f9b98a",
                "title": "Deep Knowledge Graph Representation Learning for Completion, Alignment, and Question Answering",
                "abstract": "A knowledge graph (KG) has nodes and edges representing entities and relations. KGs are central to search and question answering (QA), yet research on deep/neural representation of KGs, as well as deep QA, have moved largely to AI, ML and NLP communities. The goal of this tutorial is to give IR researchers a thorough update on the best practices of neural KG representation and inference from AI, ML and NLP communities, and then explore how KG representation research in the IR community can be better driven by the needs of search, passage retrieval, and QA. In this tutorial, we will study the most widely-used public KGs, important properties of their relations, types and entities, best-practice deep representations of KG elements and how they support or cannot support such properties, loss formulations and learning methods for KG completion and inference, the representation of time in temporal KGs, alignment across multiple KGs, possibly in different languages, and the use and benefits of deep KG representations in QA applications.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2197703",
                        "name": "Soumen Chakrabarti"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1ed91a88cfb55aa8e1b2a2145c7fff7c644bb284",
                "externalIds": {
                    "ArXiv": "2207.00719",
                    "DBLP": "journals/corr/abs-2207-00719",
                    "DOI": "10.48550/arXiv.2207.00719",
                    "CorpusId": 250264454
                },
                "corpusId": 250264454,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1ed91a88cfb55aa8e1b2a2145c7fff7c644bb284",
                "title": "Syntax Controlled Knowledge Graph-to-Text Generation with Order and Semantic Consistency",
                "abstract": "The knowledge graph (KG) stores a large amount of structural knowledge, while it is not easy for direct human understanding. Knowledge graph-to-text (KG-to-text) generation aims to generate easy-to-understand sentences from the KG, and at the same time, maintains semantic consistency between generated sentences and the KG. Existing KG-to-text generation methods phrase this task as a sequence-to-sequence generation task with linearized KG as input and consider the consistency issue of the generated texts and KG through a simple selection between decoded sentence word and KG node word at each time step. However, the linearized KG order is commonly obtained through a heuristic search without data-driven optimization. In this paper, we optimize the knowledge description order prediction under the order supervision extracted from the caption and further enhance the consistency of the generated sentences and KG through syntactic and semantic regularization. We incorporate the Part-of-Speech (POS) syntactic tags to constrain the positions to copy words from the KG and employ a semantic context scoring function to evaluate the semantic fitness for each word in its local context when decoding each word in the generated sentence. Extensive experiments are conducted on two datasets, WebNLG and DART, and achieve state-of-the-art performances.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108457928",
                        "name": "Jin Liu"
                    },
                    {
                        "authorId": "2174801025",
                        "name": "Chongfeng Fan"
                    },
                    {
                        "authorId": "2153433487",
                        "name": "Feng Zhou"
                    },
                    {
                        "authorId": "46485395",
                        "name": "Huijuan Xu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "EmbedKGQA [11] regard multi-hop KGQA task as link prediction and search for answer entity based on question embedding and knowledge embeddings, which mitigates the problem of KG incompleteness and can predict answer in unlimited neighbors.",
                "Additionally, a schema that defines a useful, high-level structure of a KG has been neglected in the current multi-hop KGQA tasks [11].",
                "As mentioned in the related work, EmbedKGQA [11] is a good work considering multi-hop reasoning.",
                "Embedding based methods [11, 28] measure the similarity between question embeddings and candidate answer embeddings to get the right answer.",
                "We compare our model with two state-of-the-art models, including EmbedKGQA [11] and TransferNet [28]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "2873585141204e7174dce0bee3cf504259c24273",
                "externalIds": {
                    "DBLP": "journals/dint/DuLWCY22",
                    "DOI": "10.1162/dint_a_00154",
                    "CorpusId": 251470739
                },
                "corpusId": 251470739,
                "publicationVenue": {
                    "id": "f80a2e54-1332-48bf-9634-4b2f77fca809",
                    "name": "Data Intelligence",
                    "type": "journal",
                    "alternate_names": [
                        "Data Intell"
                    ],
                    "issn": "2641-435X",
                    "url": "http://www.data-intelligence-journal.org/",
                    "alternate_urls": [
                        "https://www.mitpressjournals.org/loi/dint"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2873585141204e7174dce0bee3cf504259c24273",
                "title": "COKG-QA: Multi-hop Question Answering over COVID-19 Knowledge Graphs",
                "abstract": "Abstract COVID-19 evolves rapidly and an enormous number of people worldwide desire instant access to COVID-19 information such as the overview, clinic knowledge, vaccine, prevention measures, and COVID-19 mutation. Question answering (QA) has become the mainstream interaction way for users to consume the ever-growing information by posing natural language questions. Therefore, it is urgent and necessary to develop a QA system to offer consulting services all the time to relieve the stress of health services. In particular, people increasingly pay more attention to complex multi-hop questions rather than simple ones during the lasting pandemic, but the existing COVID-19 QA systems fail to meet their complex information needs. In this paper, we introduce a novel multi-hop QA system called COKG-QA, which reasons over multiple relations over large-scale COVID-19 Knowledge Graphs to return answers given a question. In the field of question answering over knowledge graph, current methods usually represent entities and schemas based on some knowledge embedding models and represent questions using pre-trained models. While it is convenient to represent different knowledge (i.e., entities and questions) based on specified embeddings, an issue raises that these separate representations come from heterogeneous vector spaces. We align question embeddings with knowledge embeddings in a common semantic space by a simple but effective embedding projection mechanism. Furthermore, we propose combining entity embeddings with their corresponding schema embeddings which served as important prior knowledge, to help search for the correct answer entity of specified types. In addition, we derive a large multi-hop Chinese COVID-19 dataset (called COKG-DATA for remembering) for COKG-QA based on the linked knowledge graph OpenKG-COVID19 launched by OpenKG\u2460, including comprehensive and representative information about COVID-19. COKG-QA achieves quite competitive performance in the 1-hop and 2-hop data while obtaining the best result with significant improvements in the 3-hop. And it is more efficient to be used in the QA system for users. Moreover, the user study shows that the system not only provides accurate and interpretable answers but also is easy to use and comes with smart tips and suggestions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2105634495",
                        "name": "Huifang Du"
                    },
                    {
                        "authorId": "2181007685",
                        "name": "Zhongwen Le"
                    },
                    {
                        "authorId": "21606013",
                        "name": "Haofen Wang"
                    },
                    {
                        "authorId": "2144861665",
                        "name": "Yunwen Chen"
                    },
                    {
                        "authorId": "2151483623",
                        "name": "Jing Yu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f9b3501ec711c7d782287decb53092bd6c532a13",
                "externalIds": {
                    "DOI": "10.1117/12.2637109",
                    "CorpusId": 249724456
                },
                "corpusId": 249724456,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f9b3501ec711c7d782287decb53092bd6c532a13",
                "title": "Graph attention network with edge weights for question answering over knowledge graph",
                "abstract": "Question Answering over the Knowledge Graphs (KGQA) has attracted extensive attention. The graph neural network can represent the dependent information of the KG, so it is well applied to the KGQA. But most of the KGQA approaches based on graph neural networks model question sentences and candidate answer entities separately. And the influences among questions, relations, and structure are not fully utilized when learning entity representations. To solve these problems, it is proposed that a question answering method based on graph attention network with edge weight to enhance the question relevance of entity representation. For the relationship in the extracted candidate answer subgraph, the Roberta is used to calculate the question\u2019s semantic similarity to be the edge weight. A graph attention network is relied on to fuse the pre-trained entity embeddings and edge weight information for node updates to obtain candidate answer representations. The experimental results show that our proposed model has certain advantages compared with some other benchmark methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108920510",
                        "name": "Junzhe Li"
                    },
                    {
                        "authorId": "70421537",
                        "name": "Xia Hou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Nevertheless, some approaches like [85] can cover this shortcoming.",
                "Some approaches that utilize knowledge graph embedding can be more efficient in handling this weakness and handling complex reasoning [85].",
                "8 Example of finding the answer for question What are the genres of movies written by Louis Mellis? [85]",
                "The evaluation of the [92] was without using text data [85]",
                "In [85], the researcher proposed an EmbedKGQA system that employs knowledge graph embeddings for answering multi-hop questions."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "2ff06d5041b91663f28922b83e06cf22f2232c08",
                "externalIds": {
                    "DBLP": "journals/apin/EtezadiS23",
                    "DOI": "10.1007/s10489-022-03732-9",
                    "CorpusId": 249439927
                },
                "corpusId": 249439927,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2ff06d5041b91663f28922b83e06cf22f2232c08",
                "title": "The state of the art in open domain complex question answering: a survey",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2049030484",
                        "name": "Romina Etezadi"
                    },
                    {
                        "authorId": "2567327",
                        "name": "M. Shamsfard"
                    }
                ]
            }
        },
        {
            "contexts": [
                "As the candidate subgraph contains limited facts, several works in [19,20] tried to deal with the incompletion."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4b63c3739a09686758d3b28b6595ce3257227bdb",
                "externalIds": {
                    "DBLP": "conf/iscas/WangLL22",
                    "DOI": "10.1109/ISCAS48785.2022.9937823",
                    "CorpusId": 253461652
                },
                "corpusId": 253461652,
                "publicationVenue": {
                    "id": "9bc219ae-a4dc-4241-8e1a-0552f9ee9ef7",
                    "name": "International Symposium on Circuits and Systems",
                    "type": "conference",
                    "alternate_names": [
                        "ISCAS",
                        "Int Symp Circuit Syst"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4b63c3739a09686758d3b28b6595ce3257227bdb",
                "title": "A Question Embedding-based Method to Enrich Features for Knowledge Base Question Answering",
                "abstract": "Knowledge base question answering (KBQA) has become an important manner to access large-scale knowledge bases. A major challenge in this task is to obtain the answer in a large search space, whereas the dataset contains limited training data. Previous works mainly concentrated on narrowing down the search space but failed to deal with the limited training data well. To address this situation, we propose a novel method adopting few-shot learning to enable the KBQA model to infer more precisely with limited samples. First, we design a question embedding model to retrieve historical questions similar to the target question. Information of the golden path, which can obtain the right answers from the knowledge base, is adopted to train the retrieval model. Then, we utilize prior features from the retrieved questions in an effective way to make the KBQA model search answers more concisely. Experimental results on benchmark datasets have demonstrated the effectiveness of our approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "153316152",
                        "name": "Xin Wang"
                    },
                    {
                        "authorId": "2190508289",
                        "name": "Meng Lin"
                    },
                    {
                        "authorId": "2151674461",
                        "name": "Qian-Ying Lu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "faff441c3b17abb7ff70f019dcb8b9e38cce70f6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-06456",
                    "ArXiv": "2205.06456",
                    "DOI": "10.48550/arXiv.2205.06456",
                    "CorpusId": 248798617
                },
                "corpusId": 248798617,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/faff441c3b17abb7ff70f019dcb8b9e38cce70f6",
                "title": "Simple and Effective Relation-based Embedding Propagation for Knowledge Representation Learning",
                "abstract": "Relational graph neural networks have garnered particular attention to encode graph context in knowledge graphs (KGs). Although they achieved competitive performance on small KGs, how to efficiently and effectively utilize graph context for large KGs remains an open problem. To this end, we propose the Relation-based Embedding Propagation (REP) method. It is a post-processing technique to adapt pre-trained KG embeddings with graph context. As relations in KGs are directional, we model the incoming head context and the outgoing tail context separately. Accordingly, we design relational context functions with no external parameters. Besides, we use averaging to aggregate context information, making REP more computation-efficient. We theoretically prove that such designs can avoid information distortion during propagation. Extensive experiments also demonstrate that REP has significant scalability while improving or maintaining prediction quality. Particularly, it averagely brings about 10% relative improvement to triplet-based embedding methods on OGBL-WikiKG2 and takes 5%-83% time to achieve comparable results as the state-of-the-art GC-OTE.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109616592",
                        "name": "Huijuan Wang"
                    },
                    {
                        "authorId": "29267165",
                        "name": "Siming Dai"
                    },
                    {
                        "authorId": "116236876",
                        "name": "Weiyue Su"
                    },
                    {
                        "authorId": "2064918759",
                        "name": "Hui Zhong"
                    },
                    {
                        "authorId": "2117590680",
                        "name": "Zeyang Fang"
                    },
                    {
                        "authorId": "2151325127",
                        "name": "Zhengjie Huang"
                    },
                    {
                        "authorId": "144588144",
                        "name": "Shi Feng"
                    },
                    {
                        "authorId": "2145422062",
                        "name": "Zeyu Chen"
                    },
                    {
                        "authorId": "2117103617",
                        "name": "Yu Sun"
                    },
                    {
                        "authorId": "3046102",
                        "name": "Dianhai Yu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In addition, it is also important to search for an optimal HP configuration when adopting KG embedding methods to the real-world applications (Bordes et al., 2014; Zhang et al., 2016; Saxena et al., 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c6af2a9bb8588e720ae2c4f42e3bbe6f52f42c6e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2205-02460",
                    "ArXiv": "2205.02460",
                    "DOI": "10.48550/arXiv.2205.02460",
                    "CorpusId": 248524797
                },
                "corpusId": 248524797,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c6af2a9bb8588e720ae2c4f42e3bbe6f52f42c6e",
                "title": "KGTuner: Efficient Hyper-parameter Search for Knowledge Graph Learning",
                "abstract": "While hyper-parameters (HPs) are important for knowledge graph (KG) learning, existing methods fail to search them efficiently. To solve this problem, we first analyze the properties of different HPs and measure the transfer ability from small subgraph to the full graph. Based on the analysis, we propose an efficient two-stage search algorithm KGTuner, which efficiently explores HP configurations on small subgraph at the first stage and transfers the top-performed configurations for fine-tuning on the large full graph at the second stage. Experiments show that our method can consistently find better HPs than the baseline algorithms within the same time budget, which achieves {9.1\\%} average relative improvement for four embedding models on the large-scale KGs in open graph benchmark.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48379419",
                        "name": "Yongqi Zhang"
                    },
                    {
                        "authorId": "1768392566",
                        "name": "Zhanke Zhou"
                    },
                    {
                        "authorId": "3259992",
                        "name": "Quanming Yao"
                    },
                    {
                        "authorId": "2154403289",
                        "name": "Yong Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3486dc8a89e1d45880a08b5fc98ffaa5c6592336",
                "externalIds": {
                    "DBLP": "journals/ipm/ZhangWZZH22",
                    "DOI": "10.1016/j.ipm.2022.102933",
                    "CorpusId": 247989646
                },
                "corpusId": 247989646,
                "publicationVenue": {
                    "id": "37f5b9b7-f828-4ae1-a174-45b538cbd4e4",
                    "name": "Information Processing & Management",
                    "type": "journal",
                    "alternate_names": [
                        "Inf Process Manag",
                        "Inf Process  Manag",
                        "Information Processing and Management"
                    ],
                    "issn": "0306-4573",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/244/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/information-processing-and-management/",
                        "http://www.sciencedirect.com/science/journal/03064573",
                        "http://www.journals.elsevier.com/information-processing-and-management/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3486dc8a89e1d45880a08b5fc98ffaa5c6592336",
                "title": "ARL: An adaptive reinforcement learning framework for complex question answering over knowledge base",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2107962491",
                        "name": "Qixuan Zhang"
                    },
                    {
                        "authorId": "2161585177",
                        "name": "Xinyi Weng"
                    },
                    {
                        "authorId": "143652253",
                        "name": "Guangyou Zhou"
                    },
                    {
                        "authorId": "39939149",
                        "name": "Yi Zhang"
                    },
                    {
                        "authorId": "1683391",
                        "name": "J. Huang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8ed2cd0a5c1aa44c4fd16bcf3bac7d2d02b27758",
                "externalIds": {
                    "DOI": "10.1016/j.knosys.2022.108943",
                    "CorpusId": 248586866
                },
                "corpusId": 248586866,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/8ed2cd0a5c1aa44c4fd16bcf3bac7d2d02b27758",
                "title": "Learning causal representations for multi-hop question answering over knowledge graphs",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2106576284",
                        "name": "Y. Sui"
                    },
                    {
                        "authorId": "2113511419",
                        "name": "Shanshan Feng"
                    },
                    {
                        "authorId": "2143675206",
                        "name": "Huaxiang Zhang"
                    },
                    {
                        "authorId": "2125080511",
                        "name": "Jiangxia Cao"
                    },
                    {
                        "authorId": "144159009",
                        "name": "Liang Hu"
                    },
                    {
                        "authorId": "3461677",
                        "name": "N. Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b79faa2ba7352830ad8b9b3e3c037bd54020cc05",
                "externalIds": {
                    "DBLP": "journals/ws/LiWZ22",
                    "DOI": "10.1016/j.websem.2022.100723",
                    "CorpusId": 249109773
                },
                "corpusId": 249109773,
                "publicationVenue": {
                    "id": "78d61825-c1d6-4e7f-ab9d-1525db71105c",
                    "name": "Journal of Web Semantics",
                    "type": "journal",
                    "alternate_names": [
                        "J Web Semant"
                    ],
                    "issn": "1570-8268",
                    "url": "http://www.elsevier.com/locate/websem",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/15708268",
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/671322/description"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/b79faa2ba7352830ad8b9b3e3c037bd54020cc05",
                "title": "Translational relation embeddings for multi-hop knowledge base question answering",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1814227458",
                        "name": "Ziyan Li"
                    },
                    {
                        "authorId": "21606013",
                        "name": "Haofen Wang"
                    },
                    {
                        "authorId": "2108126306",
                        "name": "Wenqiang Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e77078d5ddcaf5c76fe1c291c2b7cbc7518afc23",
                "externalIds": {
                    "DBLP": "journals/tcbb/DuYLZJ23",
                    "DOI": "10.1109/TCBB.2022.3171388",
                    "CorpusId": 248432501,
                    "PubMed": "35486563"
                },
                "corpusId": 248432501,
                "publicationVenue": {
                    "id": "dc4a9aad-72db-4530-a183-eaa4bf1d4490",
                    "name": "IEEE/ACM Transactions on Computational Biology & Bioinformatics",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE/ACM Trans Comput Biology  Bioinform",
                        "IEEE/ACM Trans Comput Biology Bioinform",
                        "IEEE/ACM Transactions on Computational Biology and Bioinformatics"
                    ],
                    "issn": "1545-5963",
                    "url": "https://ieeexplore.ieee.org/servlet/opac?punumber=8857",
                    "alternate_urls": [
                        "http://www.computer.org/portal/web/tcbb/home"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e77078d5ddcaf5c76fe1c291c2b7cbc7518afc23",
                "title": "Improving Biomedical Question Answering by Data Augmentation and Model Weighting",
                "abstract": "Biomedical Question Answering aims to extract an answer to the given question from a biomedical context. Due to the strong professionalism of specific domain, it's more difficult to build large-scale datasets for specific domain question answering. Existing methods are limited by the lack of training data, and the performance is not as good as in open-domain settings, especially degrading when facing to the adversarial sample. We try to resolve the above issues. First, effective data augmentation strategies are adopted to improve the model training, including slide window, summarization and round-trip translation. Second, we propose a model weighting strategy for the final answer prediction in biomedical domain, which combines the advantage of two models, open-domain model QANet and BioBERT pre-trained in biomedical domain data. Finally, we give adversarial training to reinforce the robustness of the model. The public biomedical dataset collected from PubMed provided by BioASQ challenge is used to evaluate our approach. The results show that the model performance has been improved significantly compared to the single model and other models participated in BioASQ challenge. It can learn richer semantic expression from data augmentation and adversarial samples, which is beneficial to solve more complex question answering problems in biomedical domain.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3626995",
                        "name": "Yongping Du"
                    },
                    {
                        "authorId": "2152481834",
                        "name": "Jingya Yan"
                    },
                    {
                        "authorId": "2155710822",
                        "name": "Yuxuan Lu"
                    },
                    {
                        "authorId": "2109819801",
                        "name": "Yiliang Zhao"
                    },
                    {
                        "authorId": "2149171103",
                        "name": "Xingnan Jin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026methods (Yu et al. 2017; Gupta, Chinnakotla, and Shrivastava 2018; Chen, Wu, and Zaki 2019; Petrochuk and Zettlemoyer 2018; Zhao et al. 2019; Saxena, Tripathi, and Talukdar 2020) obtain relevant candidate answers according to the topic entity and then rank the answers to obtain the final\u2026"
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "37f03c4210ede0efcdd7abab5dbfccaf2af66c87",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-12662",
                    "ArXiv": "2204.12662",
                    "DOI": "10.48550/arXiv.2204.12662",
                    "CorpusId": 248405695
                },
                "corpusId": 248405695,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/37f03c4210ede0efcdd7abab5dbfccaf2af66c87",
                "title": "Better Query Graph Selection for Knowledge Base Question Answering",
                "abstract": "This paper presents a novel approach based on semantic parsing to improve the performance of Knowledge Base Question Answering (KBQA). Specifically, we focus on how to select an optimal query graph from a candidate set so as to retrieve the answer from knowledge base (KB). In our approach, we first propose to linearize the query graph into a sequence, which is used to form a sequence pair with the question. It allows us to use mature sequence modeling, such as BERT, to encode the sequence pair. Then we use a ranking method to sort candidate query graphs. In contrast to the previous studies, our approach can efficiently model semantic interactions between the graph and the question as well as rank the candidate graphs from a global view. The experimental results show that our system achieves the top performance on ComplexQuestions and the second best performance on WebQuestions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2148363313",
                        "name": "Yonghui Jia"
                    },
                    {
                        "authorId": "2463750",
                        "name": "Wenliang Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Besides, knowledge base embeddings are also used to improve multi-hop question answering and achieve success [18]."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4ecd4bc5a70b659e9ff248fcf76ce8235bba8c0f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-12808",
                    "ArXiv": "2204.12808",
                    "DOI": "10.48550/arXiv.2204.12808",
                    "CorpusId": 248406183
                },
                "corpusId": 248406183,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4ecd4bc5a70b659e9ff248fcf76ce8235bba8c0f",
                "title": "A Method of Query Graph Reranking for Knowledge Base Question Answering",
                "abstract": ". This paper presents a novel reranking method to better choose the optimal query graph, a sub-graph of knowledge graph, to retrieve the answer for an input question in Knowledge Base Question Answering (KBQA). Existing methods su\ufb00er from a severe problem that there is a signi\ufb01cant gap between top-1 performance and the oracle score of top-n results. To address this problem, our method divides the choosing procedure into two steps: query graph ranking and query graph reranking. In the \ufb01rst step, we provide top-n query graphs for each question. Then we propose to rerank the top-n query graphs by combining with the information of answer type. Experimental results on two widely used datasets show that our proposed method achieves the best results on the WebQuestions dataset and the second best on the ComplexQuestions dataset.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2148363313",
                        "name": "Yonghui Jia"
                    },
                    {
                        "authorId": "2463750",
                        "name": "Wenliang Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Inference algorithms can be divided into rule mining (Gala\u0301rraga et al. 2013; Lao, Mitchell, and Cohen 2011), reinforcement learning (Xiong, Hoang, and Wang 2017), knowledge representation learning (Saxena, Tripathi, and Talukdar 2020; Bordes et al. 2013), etc."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4f053d45ccb6ea6f102705313ff5a797b6ee389a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2204-11394",
                    "ArXiv": "2204.11394",
                    "DOI": "10.48550/arXiv.2204.11394",
                    "CorpusId": 248377528
                },
                "corpusId": 248377528,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/4f053d45ccb6ea6f102705313ff5a797b6ee389a",
                "title": "Financial data analysis application via multi-strategy text processing",
                "abstract": "Maintaining financial system stability is critical to economic development, and early identification of risks and opportunities is essential. The financial industry contains a wide variety of data, such as financial statements, customer information, stock trading data, news, etc. Massive heterogeneous data calls for intelligent algorithms for machines to process and understand. This paper mainly focuses on the stock trading data and news about China A-share companies. We present a financial data analysis application, Financial Quotient Porter, designed to combine textual and numerical data by using a multi-strategy data mining approach. Additionally, we present our efforts and plans in deep learning financial text processing application scenarios using natural language processing (NLP) and knowledge graph (KG) technologies. Based on KG technology, risks and opportunities can be identified from heterogeneous data. NLP technology can be used to extract entities, relations, and events from unstructured text, and analyze market sentiment. Experimental results show market sentiments towards a company and an industry, as well as news-level associations between companies.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "3465702",
                        "name": "Hongyin Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026on multiple evidences of a knowledge graph, and has been broadly used in various downstream tasks such as question answering (Lin et al., 2019; Saxena et al., 2020; Han et al., 2020b,a; Yadati et al., 2021), or knowledge-enhanced text generation (Liu et al., 2019; Moon et al., 2019; Ji et\u2026",
                "tion answering (Lin et al., 2019; Saxena et al., 2020; Han et al., 2020b,a; Yadati et al., 2021), or knowledge-enhanced text generation (Liu et al."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8c9d8a23b15f8cc43b4e06c7dadbb66a3cab3cf3",
                "externalIds": {
                    "ArXiv": "2204.10448",
                    "DBLP": "journals/corr/abs-2204-10448",
                    "ACL": "2022.acl-long.29",
                    "DOI": "10.48550/arXiv.2204.10448",
                    "CorpusId": 248366595
                },
                "corpusId": 248366595,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/8c9d8a23b15f8cc43b4e06c7dadbb66a3cab3cf3",
                "title": "Hypergraph Transformer: Weakly-Supervised Multi-hop Reasoning for Knowledge-based Visual Question Answering",
                "abstract": "Knowledge-based visual question answering (QA) aims to answer a question which requires visually-grounded external knowledge beyond image content itself. Answering complex questions that require multi-hop reasoning under weak supervision is considered as a challenging problem since i) no supervision is given to the reasoning process and ii) high-order semantics of multi-hop knowledge facts need to be captured. In this paper, we introduce a concept of hypergraph to encode high-level semantics of a question and a knowledge base, and to learn high-order associations between them. The proposed model, Hypergraph Transformer, constructs a question hypergraph and a query-aware knowledge hypergraph, and infers an answer by encoding inter-associations between two hypergraphs and intra-associations in both hypergraph itself. Extensive experiments on two knowledge-based visual QA and two knowledge-based textual QA demonstrate the effectiveness of our method, especially for multi-hop reasoning problem. Our source code is available at https://github.com/yujungheo/kbvqa-public.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "15353659",
                        "name": "Y. Heo"
                    },
                    {
                        "authorId": "143693759",
                        "name": "Eun-Sol Kim"
                    },
                    {
                        "authorId": "2115124026",
                        "name": "Woo Suk Choi"
                    },
                    {
                        "authorId": "1692756",
                        "name": "Byoung-Tak Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "40a6fa6fb8236ec7ae85834576b357fbc9ccec23",
                "externalIds": {
                    "ArXiv": "2204.08554",
                    "DBLP": "journals/corr/abs-2204-08554",
                    "DOI": "10.48550/arXiv.2204.08554",
                    "CorpusId": 248240020
                },
                "corpusId": 248240020,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/40a6fa6fb8236ec7ae85834576b357fbc9ccec23",
                "title": "CBR-iKB: A Case-Based Reasoning Approach for Question Answering over Incomplete Knowledge Bases",
                "abstract": "Knowledge bases (KBs) are often incomplete and constantly changing in practice. Yet, in many question answering applications coupled with knowledge bases, the sparse nature of KBs is often overlooked. To this end, we propose a case-based reasoning approach, CBR-iKB, for knowledge base question answering (KBQA) with incomplete-KB as our main focus. Our method ensembles decisions from multiple reasoning chains with a novel nonparametric reasoning algorithm. By design, CBR-iKB can seamlessly adapt to changes in KBs without any task-specific training or fine-tuning. Our method achieves 100% accuracy on MetaQA and establishes new state-of-the-art on multiple benchmarks. For instance, CBR-iKB achieves an accuracy of 70% on WebQSP under the incomplete-KB setting, outperforming the existing state-of-the-art method by 22.3%.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2064894364",
                        "name": "Dung Ngoc Thai"
                    },
                    {
                        "authorId": "30090681",
                        "name": "Srinivas Ravishankar"
                    },
                    {
                        "authorId": "145749443",
                        "name": "I. Abdelaziz"
                    },
                    {
                        "authorId": "2046903043",
                        "name": "Mudit Chaudhary"
                    },
                    {
                        "authorId": "2689774",
                        "name": "Nandana Mihindukulasooriya"
                    },
                    {
                        "authorId": "2138053379",
                        "name": "Tahira Naseem"
                    },
                    {
                        "authorId": "143863023",
                        "name": "R. Das"
                    },
                    {
                        "authorId": "2123450380",
                        "name": "P. Kapanipathi"
                    },
                    {
                        "authorId": "2297836",
                        "name": "Achille Fokoue"
                    },
                    {
                        "authorId": "143753639",
                        "name": "A. McCallum"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Given a question q, following previous works (Saxena et al., 2020; Chen et al., 2020; Cai et al., 2021) we assume the topic entity of q has been obtained by preprocessing.",
                "Since Freebase has more than 338,580,000 triples, for ease of experimentation we use a light version provided by Saxena et al. (2020).",
                "EmbedKGQA achieves a good performance on MetaQA, but a relatively lower performance on WSP.",
                "4) EmbedKGQA (Saxena et al., 2020), which proposes a knowledge embedding method for Complex KGQA.",
                "Saxena et al. (2020) predicts the answers by utilizing the KG embedding model."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "30fd58106f4123428bb287ac43d888e9965ab176",
                "externalIds": {
                    "ACL": "2022.coling-1.135",
                    "DBLP": "conf/coling/LiJ22",
                    "ArXiv": "2204.10194",
                    "DOI": "10.48550/arXiv.2204.10194",
                    "CorpusId": 248299925
                },
                "corpusId": 248299925,
                "publicationVenue": {
                    "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
                    "name": "International Conference on Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Linguistics",
                        "COLING"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/coling/"
                },
                "url": "https://www.semanticscholar.org/paper/30fd58106f4123428bb287ac43d888e9965ab176",
                "title": "Semantic Structure Based Query Graph Prediction for Question Answering over Knowledge Graph",
                "abstract": "Building query graphs from natural language questions is an important step in complex question answering over knowledge graph (Complex KGQA). In general, a question can be correctly answered if its query graph is built correctly and the right answer is then retrieved by issuing the query graph against the KG. Therefore, this paper focuses on query graph generation from natural language questions. Existing approaches for query graph generation ignore the semantic structure of a question, resulting in a large number of noisy query graph candidates that undermine prediction accuracies. In this paper, we define six semantic structures from common questions in KGQA and develop a novel Structure-BERT to predict the semantic structure of a question. By doing so, we can first filter out noisy candidate query graphs by the predicted semantic structures, and then rank the remaining candidates with a BERT-based ranking model. Extensive experiments on two popular benchmarks MetaQA and WebQuestionsSP (WSP) demonstrate the effectiveness of our method as compared to state-of-the-arts.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "47629178",
                        "name": "Mingchen Li"
                    },
                    {
                        "authorId": "2163096053",
                        "name": "Jonathan Shihao Ji"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2013 EmbedKGQA (2020) [17] took KGQA as a link prediction task and incorporated KGE with Bert to predict the answer.",
                "Thereby, KGQA becomes an important topic and attracts much attention recently [4,7,17,19,20,24].",
                "A series of work [7, 12, 17] have been performed in KGE to learn the low-dimensional representations of entities and relations in a KG as follows.",
                "These learned vectors have been employed to complete many KGQAs efficiently because they can predict missing links between entities [7,12,17].",
                "[17] took KGQA as a link prediction task and incorporated ComplEx, a KGE method, to help predict the answer."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "e8e10fd5c29a235cd5c5bc18e2bd9186ba60d042",
                "externalIds": {
                    "ArXiv": "2203.13570",
                    "DBLP": "journals/corr/abs-2203-13570",
                    "DOI": "10.48550/arXiv.2203.13570",
                    "CorpusId": 244903701
                },
                "corpusId": 244903701,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e8e10fd5c29a235cd5c5bc18e2bd9186ba60d042",
                "title": "Improving Question Answering over Knowledge Graphs Using Graph Summarization",
                "abstract": "Question Answering (QA) systems over Knowledge Graphs (KGs) (KGQA) automatically answer natural language questions using triples contained in a KG. The key idea is to represent questions and entities of a KG as low-dimensional embeddings. Previous KGQAs have attempted to represent entities using Knowledge Graph Embedding (KGE) and Deep Learning (DL) methods. However, KGEs are too shallow to capture the expressive features and DL methods process each triple independently. Recently, Graph Convolutional Network (GCN) has shown to be excellent in providing entity embeddings. However, using GCNs to KGQAs is inefficient because GCNs treat all relations equally when aggregating neighbourhoods. Also, a problem could occur when using previous KGQAs: in most cases, questions often have an uncertain number of answers. To address the above issues, we propose a graph summarization technique using Recurrent Convolutional Neural Network (RCNN) and GCN. The combination of GCN and RCNN ensures that the embeddings are propagated together with the relations relevant to the question, and thus better answers. The proposed graph summarization technique can be used to tackle the issue that KGQAs cannot answer questions with an uncertain number of answers. In this paper, we demonstrated the proposed technique on the most common type of questions, which is single-relation questions. Experiments have demonstrated that the proposed graph summarization technique using RCNN and GCN can provide better results when compared to the GCN. The proposed graph summarization technique significantly improves the recall of actual answers when the questions have an uncertain number of answers.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118155464",
                        "name": "Sirui Li"
                    },
                    {
                        "authorId": "2227610887",
                        "name": "Kok Kai Wong"
                    },
                    {
                        "authorId": "2635255",
                        "name": "Dengya Zhu"
                    },
                    {
                        "authorId": "1866074",
                        "name": "L. Fung"
                    }
                ]
            }
        },
        {
            "contexts": [
                "YAGO [1], NELL [2], and Wikidata [3] have been widely applied in various downstream applications, such as question & answering [4], semantic search [5], and information extraction [6]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "88a81681f061b22d4420d6af47a9b593b3a0889a",
                "externalIds": {
                    "ArXiv": "2203.11639",
                    "DBLP": "journals/corr/abs-2203-11639",
                    "DOI": "10.48550/arXiv.2203.11639",
                    "CorpusId": 247596600
                },
                "corpusId": 247596600,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/88a81681f061b22d4420d6af47a9b593b3a0889a",
                "title": "Learning Relation-Specific Representations for Few-shot Knowledge Graph Completion",
                "abstract": "Recent years have witnessed increasing interest in few-shot knowledge graph completion (FKGC), which aims to infer unseen query triples for a few-shot relation using a few reference triples about the relation. The primary focus of existing FKGC methods lies in learning relation representations that can reflect the common information shared by the query and reference triples. To this end, these methods learn entity-pair representations from the direct neighbors of head and tail entities, and then aggregate the representations of reference entity pairs. However, the entity-pair representations learned only from direct neighbors may have low expressiveness when the involved entities have sparse direct neighbors or share a common local neighborhood with other entities. Moreover, merely modeling the semantic information of head and tail entities is insufficient to accurately infer their relational information especially when they have multiple relations. To address these issues, we propose a Relation-Specific Context Learning (RSCL) framework, which exploits graph contexts of triples to learn global and local relation-specific representations for few-shot relations. Specifically, we first extract graph contexts for each triple, which can provide long-term entity-relation dependencies. To encode the extracted graph contexts, we then present a hierarchical attention network to capture contextualized information of triples and highlight valuable local neighborhood information of entities. Finally, we design a hybrid attention aggregator to evaluate the likelihood of the query triples at the global and local levels. Experimental results on two public datasets demonstrate that RSCL outperforms state-of-the-art FKGC methods.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2111353835",
                        "name": "Yuling Li"
                    },
                    {
                        "authorId": "46330674",
                        "name": "Kui Yu"
                    },
                    {
                        "authorId": "2145061788",
                        "name": "Yuhong Zhang"
                    },
                    {
                        "authorId": "2145502658",
                        "name": "Xindong Wu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "38db9934e871c564da4923005470a3adf755b36e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-11054",
                    "ArXiv": "2203.11054",
                    "DOI": "10.48550/arXiv.2203.11054",
                    "CorpusId": 247593728
                },
                "corpusId": 247593728,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/38db9934e871c564da4923005470a3adf755b36e",
                "title": "Targeted Extraction of Temporal Facts from Textual Resources for Improved Temporal Question Answering over Knowledge Bases",
                "abstract": "Knowledge Base Question Answering (KBQA) systems have the goal of answering complex natural language questions by reasoning over relevant facts retrieved from Knowledge Bases (KB). One of the major challenges faced by these systems is their inability to retrieve all relevant facts due to factors such as incomplete KB and entity/relation linking errors. In this paper, we address this particular challenge for systems handling a specific category of questions called temporal questions, where answer derivation involve reasoning over facts asserting point/intervals of time for various events. We propose a novel approach where a targeted temporal fact extraction technique is used to assist KBQA whenever it fails to retrieve temporal facts from the KB. We use $\\lambda$-expressions of the questions to logically represent the component facts and the reasoning steps needed to derive the answer. This allows us to spot those facts that failed to get retrieved from the KB and generate textual queries to extract them from the textual resources in an open-domain question answering fashion. We evaluated our approach on a benchmark temporal question answering dataset considering Wikidata and Wikipedia respectively as the KB and textual resource. Experimental results show a significant $\\sim$30\\% relative improvement in answer accuracy, demonstrating the effectiveness of our approach.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2147325257",
                        "name": "Nithish Kannen"
                    },
                    {
                        "authorId": "2027112872",
                        "name": "Udit Sharma"
                    },
                    {
                        "authorId": "2965855",
                        "name": "S. Neelam"
                    },
                    {
                        "authorId": "50564082",
                        "name": "Dinesh Khandelwal"
                    },
                    {
                        "authorId": "3192316",
                        "name": "S. Ikbal"
                    },
                    {
                        "authorId": "9621738",
                        "name": "Hima P. Karanam"
                    },
                    {
                        "authorId": "143666446",
                        "name": "L. V. Subramaniam"
                    }
                ]
            }
        },
        {
            "contexts": [
                "These include PullNet (Sun et al., 2019a), EmQL (Sun et al., 2021), EmbedKGQA (Saxena et al., 2020) and LEGO (Ren et al., 2021).",
                "KGT5 performance only marginally improves when pretrained on full KG compared to 50% KG, and lags far behind both EmbedKGQA (a ComplEx-based method) as well as CBR-KGQA (a semantic parsing method that uses (NL-query, SPARQL-query) parallel data).",
                "\u2026KGQA approaches on incomplete KGs, but combining KGEs with the QA pipeline is a non-trivial task; models that attempt to do this often work on only limited query types (Huang et al. 2019; Sun et al. 2021; Saxena et al. 2020) or require multistage training and inference pipelines (Ren et al., 2021).",
                "3 QA methods which leverage KGEs outperform traditional KGQA approaches on incomplete KGs, but combining KGEs with the QA pipeline is a non-trivial task; models that attempt to do this often work on only limited query types (Huang et al. 2019; Sun et al. 2021; Saxena et al. 2020) or require multistage training and inference pipelines (Ren et al.",
                "These methods attempt to overcome KG incompleteness using KG embeddings (Huang et al. 2019; Saxena et al. 2020; Sun et al. 2021; Ren et al. 2021).",
                ", 2021), EmbedKGQA (Saxena et al., 2020) and LEGO (Ren et al.",
                "EmbedKGQA (Saxena et al., 2020) is capable of multi-hop question answering but is unable to deal with questions involving more than one entity."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "6d40db49cec2a543e01a4ef651f053ae935274fc",
                "externalIds": {
                    "DBLP": "conf/acl/SaxenaKG22",
                    "ACL": "2022.acl-long.201",
                    "ArXiv": "2203.10321",
                    "DOI": "10.48550/arXiv.2203.10321",
                    "CorpusId": 247595094
                },
                "corpusId": 247595094,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/6d40db49cec2a543e01a4ef651f053ae935274fc",
                "title": "Sequence-to-Sequence Knowledge Graph Completion and Question Answering",
                "abstract": "Knowledge graph embedding (KGE) models represent each entity and relation of a knowledge graph (KG) with low-dimensional embedding vectors. These methods have recently been applied to KG link prediction and question answering over incomplete KGs (KGQA). KGEs typically create an embedding for each entity in the graph, which results in large model sizes on real-world graphs with millions of entities. For downstream tasks these atomic entity representations often need to be integrated into a multi stage pipeline, limiting their utility. We show that an off-the-shelf encoder-decoder Transformer model can serve as a scalable and versatile KGE model obtaining state-of-the-art results for KG link prediction and incomplete KG question answering. We achieve this by posing KG link prediction as a sequence-to-sequence task and exchange the triple scoring approach taken by prior KGE methods with autoregressive decoding. Such a simple but powerful method reduces the model size up to 98% compared to conventional KGE models while keeping inference time tractable. After finetuning this model on the task of KGQA over incomplete KGs, our approach outperforms baselines on multiple large-scale datasets without extensive hyperparameter tuning.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46961776",
                        "name": "Apoorv Saxena"
                    },
                    {
                        "authorId": "2008201285",
                        "name": "Adrian Kochsiek"
                    },
                    {
                        "authorId": "1777107",
                        "name": "Rainer Gemulla"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020), which integrates entity knowledge into a transformer-based language model and has been used for TKGQA (Saxena et al., 2020).",
                "Inspired by work on relational KGQA (Huang et al., 2019; Saxena et al., 2020), where knowledge graph embeddings (Dasgupta et al.",
                "\u2022 T-EaE-add/replacement (Saxena et al., 2021) are two modifications of KG enhanced language model EaE (F\u00e9vry et al., 2020), which integrates entity knowledge into a transformer-based language model and has been used for TKGQA (Saxena et al., 2020).",
                "Inspired by work on relational KGQA (Huang et al., 2019; Saxena et al., 2020), where knowledge graph embeddings (Dasgupta et al., 2018; Garc\u00eda-Dur\u00e1n et al., 2018; Goel et al., 2020; Wu et al., 2020; Lacroix et al., 2020) learned independently of question answering are used as input to KGQA models,\u2026",
                "\u2022 EmbedKGQA (Saxena et al., 2020) is the first method to use KG embeddings for the multi-hop KGQA task.",
                "We select several recent SOTA TKGQA models as our baselines as follow:\n\u2022 EmbedKGQA (Saxena et al., 2020) is the first method to use KG embeddings for the multi-hop KGQA task."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "3469127ffee73eca466bd86e4bfdfa8e8c71107f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2203-00255",
                    "ArXiv": "2203.00255",
                    "ACL": "2022.acl-long.552",
                    "DOI": "10.48550/arXiv.2203.00255",
                    "CorpusId": 247187995
                },
                "corpusId": 247187995,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/3469127ffee73eca466bd86e4bfdfa8e8c71107f",
                "title": "Improving Time Sensitivity for Question Answering over Temporal Knowledge Graphs",
                "abstract": "Question answering over temporal knowledge graphs (KGs) efficiently uses facts contained in a temporal KG, which records entity relations and when they occur in time, to answer natural language questions (e.g., \u201cWho was the president of the US before Obama?\u201d). These questions often involve three time-related challenges that previous work fail to adequately address: 1) questions often do not specify exact timestamps of interest (e.g., \u201cObama\u201d instead of 2000); 2) subtle lexical differences in time relations (e.g., \u201cbefore\u201d vs \u201cafter\u201d); 3) off-the-shelf temporal KG embeddings that previous work builds on ignore the temporal order of timestamps, which is crucial for answering temporal-order related questions. In this paper, we propose a time-sensitive question answering (TSQA) framework to tackle these problems. TSQA features a timestamp estimation module to infer the unwritten timestamp from the question. We also employ a time-sensitive KG encoder to inject ordering information into the temporal KG embeddings that TSQA is based on. With the help of techniques to reduce the search space for potential answers, TSQA significantly outperforms the previous state of the art on a new benchmark for question answering over temporal KGs, especially achieving a 32% (absolute) error reduction on complex questions that require multiple steps of reasoning over facts in the temporal KG.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2062997349",
                        "name": "Chao Shang"
                    },
                    {
                        "authorId": "2148300",
                        "name": "Guangtao Wang"
                    },
                    {
                        "authorId": "2121382930",
                        "name": "Peng Qi"
                    },
                    {
                        "authorId": "30768523",
                        "name": "Jing Huang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1f7649b2850ae7a0af02a14bc476185d5b02eda2",
                "externalIds": {
                    "DBLP": "journals/kbs/BiNZZYW22",
                    "DOI": "10.1016/j.knosys.2022.108515",
                    "CorpusId": 247307665
                },
                "corpusId": 247307665,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1f7649b2850ae7a0af02a14bc476185d5b02eda2",
                "title": "Unrestricted multi-hop reasoning network for interpretable question answering over knowledge graph",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48486326",
                        "name": "Xin Bi"
                    },
                    {
                        "authorId": "2064596283",
                        "name": "H. Nie"
                    },
                    {
                        "authorId": "2108217443",
                        "name": "Xiyu Zhang"
                    },
                    {
                        "authorId": "2749153",
                        "name": "Xiangguo Zhao"
                    },
                    {
                        "authorId": "14886336",
                        "name": "Ye Yuan"
                    },
                    {
                        "authorId": "8349792",
                        "name": "Guoren Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "We compare with embeddingbased KBQA models, in which EmbedKGQA (Saxena et al., 2020) directly optimizes the triplet (topic Table 1: Data statistics.",
                "Embeddingbased methods embed entities and rank them based on their relevance to the question, where the entities are extracted from the whole KB (Miller et al., 2016; Saxena et al., 2020) or restricted in a subgraph (Chen et al.",
                "Among these models, KV-Mem and EmbedKGQA retrieve the answers from the global key-value memory built on the KB or the original whole KB, which enjoys high recall but suffers from many noisy entities.",
                "We compare with embeddingbased KBQA models, in which EmbedKGQA (Saxena et al., 2020) directly optimizes the triplet (topic\nentity, question, answer) based on their direct embeddings.",
                "\u2026retrieve a question-relevant subgraph and then perform reasoning on it (He et al., 2021; Sun et al., 2018, 2019) reduce the reasoning space, showing superiority compared with reasoning on the whole KB (Chen et al., 2019a; Saxena et al., 2020; Xu et al., 2019) (Cf. Table 2 for empirical proof).",
                "Embeddingbased methods embed entities and rank them based on their relevance to the question, where the entities are extracted from the whole KB (Miller et al., 2016; Saxena et al., 2020) or restricted in a subgraph (Chen et al., 2019a; He et al., 2021; Sun et al., 2018; Zhang et al., 2018).",
                ", 2018, 2019) reduce the reasoning space, showing superiority compared with reasoning on the whole KB (Chen et al., 2019a; Saxena et al., 2020; Xu et al., 2019) (Cf."
            ],
            "intents": [
                "methodology",
                "result"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "767ceaade8a2cfb186f20091e50c7d03641e0caa",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-13296",
                    "ACL": "2022.acl-long.396",
                    "ArXiv": "2202.13296",
                    "DOI": "10.18653/v1/2022.acl-long.396",
                    "CorpusId": 247158305
                },
                "corpusId": 247158305,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/767ceaade8a2cfb186f20091e50c7d03641e0caa",
                "title": "Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering",
                "abstract": "Recent works on knowledge base question answering (KBQA) retrieve subgraphs for easier reasoning. The desired subgraph is crucial as a small one may exclude the answer but a large one might introduce more noises. However, the existing retrieval is either heuristic or interwoven with the reasoning, causing reasoning on the partial subgraphs, which increases the reasoning bias when the intermediate supervision is missing. This paper proposes a trainable subgraph retriever (SR) decoupled from the subsequent reasoning process, which enables a plug-and-play framework to enhance any subgraph-oriented KBQA model. Extensive experiments demonstrate SR achieves significantly better retrieval and QA performance than existing retrieval methods. Via weakly supervised pre-training as well as the end-to-end fine-tuning, SR achieves new state-of-the-art performance when combined with NSM (He et al., 2021), a subgraph-oriented reasoner, for embedding-based KBQA methods. Codes and datasets are available online (https://github.com/RUCKBReasoning/SubgraphRetrievalKBQA)",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2155700347",
                        "name": "Jing Zhang"
                    },
                    {
                        "authorId": "2108046717",
                        "name": "Xiaokang Zhang"
                    },
                    {
                        "authorId": "81962214",
                        "name": "Jifan Yu"
                    },
                    {
                        "authorId": "152226504",
                        "name": "Jian Tang"
                    },
                    {
                        "authorId": "2148911975",
                        "name": "Jie Tang"
                    },
                    {
                        "authorId": "1625473962",
                        "name": "Cuiping Li"
                    },
                    {
                        "authorId": "2191043357",
                        "name": "Hong Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d3466694d7ba0ad9ecd4526dd3dcefb8161a3356",
                "externalIds": {
                    "ACL": "2022.findings-acl.202",
                    "DBLP": "conf/acl/LiZH0H22",
                    "ArXiv": "2202.13352",
                    "DOI": "10.18653/v1/2022.findings-acl.202",
                    "CorpusId": 247158769
                },
                "corpusId": 247158769,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/d3466694d7ba0ad9ecd4526dd3dcefb8161a3356",
                "title": "HiCLRE: A Hierarchical Contrastive Learning Framework for Distantly Supervised Relation Extraction",
                "abstract": "Distant supervision assumes that any sentence containing the same entity pairs reflects identical relationships. Previous works of distantly supervised relation extraction (DSRE) task generally focus on sentence-level or bag-level de-noising techniques independently, neglecting the explicit interaction with cross levels. In this paper, we propose a hierarchical contrastive learning Framework for Distantly Supervised relation extraction (HiCLRE) to reduce noisy sentences, which integrate the global structural information and local fine-grained interaction. Specifically, we propose a three-level hierarchical learning framework to interact with cross levels, generating the de-noising context-aware representations via adapting the existing multi-head self-attention, named Multi-Granularity Recontextualization. Meanwhile, pseudo positive samples are also provided in the specific level for contrastive learning via a dynamic gradient-based data augmentation strategy, named Dynamic Gradient Adversarial Perturbation. Experiments demonstrate that HiCLRE significantly outperforms strong baselines in various mainstream DSRE datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2156707742",
                        "name": "Dongyang Li"
                    },
                    {
                        "authorId": "2146342371",
                        "name": "Taolin Zhang"
                    },
                    {
                        "authorId": "2091493779",
                        "name": "Nan Hu"
                    },
                    {
                        "authorId": "50097294",
                        "name": "Chengyu Wang"
                    },
                    {
                        "authorId": "143644849",
                        "name": "Xiaofeng He"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018) have been widely used in many knowledge-intensive applications, including question answering (Sun et al., 2020; Saxena et al., 2020), dialogue systems (Yang et al.",
                "\u2026(Lehmann et al., 2015) and NELL (Mitchell et al., 2018) have been widely used in many knowledge-intensive applications, including question answering (Sun et al., 2020; Saxena et al., 2020), dialogue systems (Yang et al., 2020; Zhou et al., 2018) and recommender systems (Wang et al., 2021, 2019a)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "009263dec4026507d5809b14881f833c80b74cbc",
                "externalIds": {
                    "ACL": "2022.acl-long.205",
                    "ArXiv": "2202.13785",
                    "DBLP": "journals/corr/abs-2202-13785",
                    "DOI": "10.18653/v1/2022.acl-long.205",
                    "CorpusId": 247158115
                },
                "corpusId": 247158115,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/009263dec4026507d5809b14881f833c80b74cbc",
                "title": "CAKE: A Scalable Commonsense-Aware Framework For Multi-View Knowledge Graph Completion",
                "abstract": "Knowledge graphs store a large number of factual triples while they are still incomplete, inevitably. The previous knowledge graph completion (KGC) models predict missing links between entities merely relying on fact-view data, ignoring the valuable commonsense knowledge. The previous knowledge graph embedding (KGE) techniques suffer from invalid negative sampling and the uncertainty of fact-view link prediction, limiting KGC\u2019s performance. To address the above challenges, we propose a novel and scalable Commonsense-Aware Knowledge Embedding (CAKE) framework to automatically extract commonsense from factual triples with entity concepts. The generated commonsense augments effective self-supervision to facilitate both high-quality negative sampling (NS) and joint commonsense and fact-view link prediction. Experimental results on the KGC task demonstrate that assembling our framework could enhance the performance of the original KGE models, and the proposed commonsense-aware NS module is superior to other NS techniques. Besides, our proposed framework could be easily adaptive to various KGE models and explain the predicted results.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2052804171",
                        "name": "Guanglin Niu"
                    },
                    {
                        "authorId": "2151287667",
                        "name": "Bo Li"
                    },
                    {
                        "authorId": "3126972",
                        "name": "Yongfei Zhang"
                    },
                    {
                        "authorId": "1796257902",
                        "name": "Shi Pu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent approaches to semantic parsing [35, 20] uses powerful neural models and achieve strong performance.",
                "Many KBQA methods usually consider few hops of edges around entities as the query subgraph [31, 35] leading to query-independent and (often) large subgraphs, because of the presence of hub nodes in large KBs.",
                "Many KBQA methods usually consider few hops of edges around entities as the query subgraph (Neelakantan et al., 2015; Saxena et al., 2020) leading to query-independent and (often) large subgraphs, because of the presence of hub nodes in large KBs.",
                "For other datasets, the underlying KB is the full Freebase KB containing over 45\nModel MetaQA WebQSP 1-hop 2-hop 3-hop\nKVMemNN (Miller et al., 2016) 95.8 25.1 10.1 46.7 GraftNet (Sun et al., 2018) 97.0 94.8 77.7 66.4 PullNet (Sun et al., 2019a) 97.0 99.9 91.4 68.1 SRN (Qiu et al., 2020b) 97.0 95.1 75.2 - ReifKB (Cohen et al., 2020) 96.2 81.1 72.3 52.7 EmbedKGQA (Saxena et al., 2020) 97.5 98.8 94.8 66.6 NSM (He et al., 2021) 97.2 99.9 98.9 74.3 CBR-SUBG (Ours) 97.1 99.8 99.3 72.1\nTable 2: Performance on WebQSP and MetaQA benchmarks.\nmillion entities (nodes) and 3 billion facts (edges).",
                "Followup KBQA works (Saxena et al., 2020; He et al., 2021, inter-alia) use the query-specific graphs provided by GraftNet from their open-source code and do not provide a mechanism to gather query-specific subgraphs.",
                "Recent approaches to semantic parsing (Saxena et al., 2020; He et al., 2021) uses powerful neural models and achieve strong performance.",
                "\u2026PullNet (Sun et al., 2019a) 97.0 99.9 91.4 68.1 SRN (Qiu et al., 2020b) 97.0 95.1 75.2 - ReifKB (Cohen et al., 2020) 96.2 81.1 72.3 52.7 EmbedKGQA (Saxena et al., 2020) 97.5 98.8 94.8 66.6 NSM (He et al., 2021) 97.2 99.9 98.9 74.3 CBR-SUBG (Ours) 97.1 99.8 99.3 72.1\nTable 2: Performance on WebQSP\u2026"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "d7f9bc1acde978072050b6fe2dfdb237f94e480e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2202-10610",
                    "ArXiv": "2202.10610",
                    "CorpusId": 247026028
                },
                "corpusId": 247026028,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d7f9bc1acde978072050b6fe2dfdb237f94e480e",
                "title": "Knowledge Base Question Answering by Case-based Reasoning over Subgraphs",
                "abstract": "Question answering (QA) over knowledge bases (KBs) is challenging because of the diverse, essentially unbounded, types of reasoning patterns needed. However, we hypothesize in a large KB, reasoning patterns required to answer a query type reoccur for various entities in their respective subgraph neighborhoods. Leveraging this structural similarity between local neighborhoods of different subgraphs, we introduce a semiparametric model (CBR-SUBG) with (i) a nonparametric component that for each query, dynamically retrieves other similar $k$-nearest neighbor (KNN) training queries along with query-specific subgraphs and (ii) a parametric component that is trained to identify the (latent) reasoning patterns from the subgraphs of KNN queries and then apply them to the subgraph of the target query. We also propose an adaptive subgraph collection strategy to select a query-specific compact subgraph, allowing us to scale to full Freebase KB containing billions of facts. We show that CBR-SUBG can answer queries requiring subgraph reasoning patterns and performs competitively with the best models on several KBQA benchmarks. Our subgraph collection strategy also produces more compact subgraphs (e.g. 55\\% reduction in size for WebQSP while increasing answer recall by 4.85\\%)\\footnote{Code, model, and subgraphs are available at \\url{https://github.com/rajarshd/CBR-SUBG}}.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "143863023",
                        "name": "R. Das"
                    },
                    {
                        "authorId": "36851489",
                        "name": "Ameya Godbole"
                    },
                    {
                        "authorId": "12500726",
                        "name": "Ankita Rajaram Naik"
                    },
                    {
                        "authorId": "2155649956",
                        "name": "Elliot Tower"
                    },
                    {
                        "authorId": "3422908",
                        "name": "Robin Jia"
                    },
                    {
                        "authorId": "1771307",
                        "name": "M. Zaheer"
                    },
                    {
                        "authorId": "2548384",
                        "name": "Hannaneh Hajishirzi"
                    },
                    {
                        "authorId": "143753639",
                        "name": "A. McCallum"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recently, there is a wide range of research work about question generation over knowledge graphs [2, 12, 27]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "097688d43af6ed3ab74bd198b30a543b15f06f96",
                "externalIds": {
                    "DBLP": "conf/wsdm/ShenCCZX22",
                    "DOI": "10.1145/3488560.3498431",
                    "CorpusId": 246828666
                },
                "corpusId": 246828666,
                "publicationVenue": {
                    "id": "ea38228f-6ed3-4222-a3ce-d963d8cc9516",
                    "name": "Web Search and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Web Search Data Min",
                        "WSDM"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=3158"
                },
                "url": "https://www.semanticscholar.org/paper/097688d43af6ed3ab74bd198b30a543b15f06f96",
                "title": "Diversified Query Generation Guided by Knowledge Graph",
                "abstract": "Relevant articles recommendation plays an important role in online news platforms. Directly displaying recalled articles by a search engine lacks a deep understanding of the article contents. Generating clickable queries, on the other hand, summarizes an article in various aspects, which can be henceforth utilized to better connect relevant articles. Most existing approaches for generating article queries, however, do not consider the diversity of queries or whether they are appealing enough, which are essential for boosting user experience and platform drainage. To this end, we propose a Knowledge-Enhanced Diversified QuerY Generator (KEDY), which leverages an external knowledge graph (KG) as guidance. We diversify the query generation with the information of semantic neighbors of the entities in articles. We further constrain the diversification process with entity popularity knowledge to build appealing queries that users may be more interested in. The information within KG is propagated towards more popular entities with popularity-guided graph attention. We collect a news-query dataset from the search logs of a real-world search engine. Extensive experiments demonstrate our proposed KEDY can generate more diversified and insightful related queries than several strong baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2111114976",
                        "name": "Xi Shen"
                    },
                    {
                        "authorId": "5040052",
                        "name": "Jiangjie Chen"
                    },
                    {
                        "authorId": "2108182762",
                        "name": "Jiaze Chen"
                    },
                    {
                        "authorId": "2154454736",
                        "name": "Chun Zeng"
                    },
                    {
                        "authorId": "2116642640",
                        "name": "Yanghua Xiao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "0e general framework defines a scoring function for triples (h, l, ?) in KG and constrains them so that the score of the correct triple is greater than that of the wrong triples [12]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7c2d49dacca00f6657b7b636d4a7c7fc59b1dcee",
                "externalIds": {
                    "PubMedCentral": "8853781",
                    "DOI": "10.1155/2022/8693937",
                    "CorpusId": 246761370,
                    "PubMed": "35186106"
                },
                "corpusId": 246761370,
                "publicationVenue": {
                    "id": "8bbffbf0-5dfc-4485-b4b1-d177fd330b21",
                    "name": "Evidence-Based Complementary and Alternative Medicine",
                    "type": "journal",
                    "alternate_names": [
                        "Evidence-based Complement Altern Med",
                        "Evidence-based Complementary and Alternative Medicine"
                    ],
                    "issn": "1741-427X",
                    "url": "https://www.hindawi.com/journals/ecam/",
                    "alternate_urls": [
                        "http://www.hindawi.com/journals/ecam/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7c2d49dacca00f6657b7b636d4a7c7fc59b1dcee",
                "title": "Decision-Making System for the Diagnosis of Syndrome Based on Traditional Chinese Medicine Knowledge Graph",
                "abstract": "The clinical informatization of traditional Chinese medicine (TCM) focuses on serving users and assisting in diagnosis. The rules of clinical knowledge play an important role in improving the TCM informatization service. However, many rules are difficult to find because of the complexity of the data in the current TCM syndrome prediction. Therefore, we proposed an end-to-end model, called Decision-making System for the Diagnosis of Syndrome (DSDS), which is based on the knowledge graph (KG) of TCM. This paper introduces the link prediction for the diagnosis of syndrome by dismantling medical records into multiple symptoms. In addition, based on the symptoms and predicted syndromes, the most relevant syndrome could be determined by the scoring and voting method in this paper. The results show that the accuracy of DSDS is 80.6%.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2152903765",
                        "name": "Rui Yang"
                    },
                    {
                        "authorId": "50540226",
                        "name": "Qing Ye"
                    },
                    {
                        "authorId": "2116908665",
                        "name": "Chun-lei Cheng"
                    },
                    {
                        "authorId": "48692260",
                        "name": "Suhua Zhang"
                    },
                    {
                        "authorId": "2154134662",
                        "name": "Yong Lan"
                    },
                    {
                        "authorId": "2154118785",
                        "name": "Jing Zou"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "66a0c4ff7fcb77be775aea842f11eea125dc3d40",
                "externalIds": {
                    "DBLP": "journals/ijon/HaoCWWC22",
                    "DOI": "10.1016/j.neucom.2022.02.008",
                    "CorpusId": 246657084
                },
                "corpusId": 246657084,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/66a0c4ff7fcb77be775aea842f11eea125dc3d40",
                "title": "Motif-based memory networks for complex-factoid question answering",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "145586380",
                        "name": "Z. Hao"
                    },
                    {
                        "authorId": "2155463899",
                        "name": "Junhao Chen"
                    },
                    {
                        "authorId": "2070075811",
                        "name": "Wen Wen"
                    },
                    {
                        "authorId": "2152566465",
                        "name": "Biao Wu"
                    },
                    {
                        "authorId": "1856374",
                        "name": "Ruichu Cai"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "89343e742ac946b7a1e4e29bc15e4c82dd9749ba",
                "externalIds": {
                    "DBLP": "journals/ijon/NiuLZSSLP22",
                    "DOI": "10.1016/j.neucom.2022.02.011",
                    "CorpusId": 246671483
                },
                "corpusId": 246671483,
                "publicationVenue": {
                    "id": "df12d289-f447-47d3-8846-75e39de3ab57",
                    "name": "Neurocomputing",
                    "type": "journal",
                    "issn": "0925-2312",
                    "url": "http://www.elsevier.com/locate/neucom",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/09252312"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/89343e742ac946b7a1e4e29bc15e4c82dd9749ba",
                "title": "Joint semantics and data-driven path representation for knowledge graph reasoning",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2052804171",
                        "name": "Guanglin Niu"
                    },
                    {
                        "authorId": "2151287667",
                        "name": "Bo Li"
                    },
                    {
                        "authorId": "2108026046",
                        "name": "Yongfei Zhang"
                    },
                    {
                        "authorId": "20587739",
                        "name": "Yongpan Sheng"
                    },
                    {
                        "authorId": "2151458697",
                        "name": "Chuan Shi"
                    },
                    {
                        "authorId": "1756333",
                        "name": "Jingyang Li"
                    },
                    {
                        "authorId": "3290437",
                        "name": "Shiliang Pu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Structured knowledge encoding methods have been explored in many prior studies (Bogin et al., 2019; Lin et al., 2019; Yin et al., 2020a; Herzig et al., 2020; Agarwal et al., 2020; Saxena et al., 2020; Yasunaga and Liang, 2020; Yasunaga et al., 2021; Oguz et al., 2021)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "53c0abe83fe9b4fdaf2208295d8504fcf5241694",
                "externalIds": {
                    "DBLP": "conf/emnlp/XieW0ZSYWZYWZWL22",
                    "ArXiv": "2201.05966",
                    "ACL": "2022.emnlp-main.39",
                    "DOI": "10.18653/v1/2022.emnlp-main.39",
                    "CorpusId": 246016124
                },
                "corpusId": 246016124,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/53c0abe83fe9b4fdaf2208295d8504fcf5241694",
                "title": "UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models",
                "abstract": "Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests, such as semantic parsing over databases and question answering over knowledge bases. Since the inputs and outputs of SKG tasks are heterogeneous, they have been studied separately by different communities, which limits systematic and compatible research on SKG. In this paper, we overcome this limitation by proposing the UnifiedSKG framework, which unifies 21 SKG tasks into a text-to-text format, aiming to promote systematic SKG research, instead of being exclusive to a single task, domain, or dataset. We use UnifiedSKG to benchmark T5 with different sizes and show that T5, with simple modifications when necessary, achieves state-of-the-art performance on almost all of the 21 tasks. We further demonstrate that multi-task prefix-tuning improves the performance on most tasks, largely improving the overall performance. UnifiedSKG also facilitates the investigation of zero-shot and few-shot learning, and we show that T0, GPT-3, and Codex struggle in zero-shot and few-shot learning for SKG. We also use UnifiedSKG to conduct a series of controlled experiments on structured knowledge encoding variants across SKG tasks. UnifiedSKG is easily extensible to more tasks, and it is open-sourced at https://github.com/hkunlp/unifiedskg.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2057038673",
                        "name": "Tianbao Xie"
                    },
                    {
                        "authorId": "114621402",
                        "name": "Chen Henry Wu"
                    },
                    {
                        "authorId": "2055357805",
                        "name": "Peng Shi"
                    },
                    {
                        "authorId": "51011000",
                        "name": "Ruiqi Zhong"
                    },
                    {
                        "authorId": "11869783",
                        "name": "Torsten Scholak"
                    },
                    {
                        "authorId": "19168196",
                        "name": "Michihiro Yasunaga"
                    },
                    {
                        "authorId": "30340989",
                        "name": "Chien-Sheng Wu"
                    },
                    {
                        "authorId": "1606040932",
                        "name": "Ming Zhong"
                    },
                    {
                        "authorId": "38253388",
                        "name": "Pengcheng Yin"
                    },
                    {
                        "authorId": "8729431",
                        "name": "Sida I. Wang"
                    },
                    {
                        "authorId": "3428769",
                        "name": "Victor Zhong"
                    },
                    {
                        "authorId": "2118640406",
                        "name": "Bailin Wang"
                    },
                    {
                        "authorId": "2155795167",
                        "name": "Chengzu Li"
                    },
                    {
                        "authorId": "2143195008",
                        "name": "Connor Boyle"
                    },
                    {
                        "authorId": "33981736",
                        "name": "Ansong Ni"
                    },
                    {
                        "authorId": "3366595",
                        "name": "Ziyu Yao"
                    },
                    {
                        "authorId": "9215251",
                        "name": "Dragomir R. Radev"
                    },
                    {
                        "authorId": "2054594326",
                        "name": "Caiming Xiong"
                    },
                    {
                        "authorId": "47648549",
                        "name": "Lingpeng Kong"
                    },
                    {
                        "authorId": "15176410",
                        "name": "Rui Zhang"
                    },
                    {
                        "authorId": "144365875",
                        "name": "Noah A. Smith"
                    },
                    {
                        "authorId": "1982950",
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "authorId": "48881008",
                        "name": "Tao Yu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bd08bb0e1b6ac0815fc5b24ac1bf2fdd4d4d0e52",
                "externalIds": {
                    "DBLP": "journals/nca/ZhaoLHB22",
                    "DOI": "10.1007/s00521-021-06736-7",
                    "CorpusId": 247620361
                },
                "corpusId": 247620361,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bd08bb0e1b6ac0815fc5b24ac1bf2fdd4d4d0e52",
                "title": "Improving question answering over incomplete knowledge graphs with relation prediction",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "46350730",
                        "name": "Fen Zhao"
                    },
                    {
                        "authorId": "152998396",
                        "name": "Yinguo Li"
                    },
                    {
                        "authorId": "2100467251",
                        "name": "Jie Hou"
                    },
                    {
                        "authorId": "2075398132",
                        "name": "Ling Bai"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9f97fac44d10700eeb7682ac800cd427cff942e1",
                "externalIds": {
                    "DBLP": "journals/kbs/ZhangZHT22",
                    "DOI": "10.1016/j.knosys.2022.108252",
                    "CorpusId": 246346197
                },
                "corpusId": 246346197,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9f97fac44d10700eeb7682ac800cd427cff942e1",
                "title": "Improving complex knowledge base question answering via structural information learning",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2155383644",
                        "name": "Jinhao Zhang"
                    },
                    {
                        "authorId": "50081620",
                        "name": "Lizong Zhang"
                    },
                    {
                        "authorId": "2064059144",
                        "name": "Bei Hui"
                    },
                    {
                        "authorId": "2106775523",
                        "name": "Ling Tian"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a185997a4f04571d65760068def9666b02357d05",
                "externalIds": {
                    "DBLP": "journals/eswa/LeiHZSZ21",
                    "MAG": "3193535092",
                    "DOI": "10.1016/J.ESWA.2021.115708",
                    "CorpusId": 238694099
                },
                "corpusId": 238694099,
                "publicationVenue": {
                    "id": "987139ae-a65d-49bb-aaf6-fb764dc40b19",
                    "name": "Expert systems with applications",
                    "type": "journal",
                    "alternate_names": [
                        "Expert syst appl",
                        "Expert Systems With Applications",
                        "Expert Syst Appl"
                    ],
                    "issn": "0957-4174",
                    "url": "https://www.journals.elsevier.com/expert-systems-with-applications/",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/expert-systems-with-applications",
                        "http://www.sciencedirect.com/science/journal/09574174"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a185997a4f04571d65760068def9666b02357d05",
                "title": "Is the suggested food your desired?: Multi-modal recipe recommendation with demand-based knowledge graph",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "9456092",
                        "name": "Zhenfeng Lei"
                    },
                    {
                        "authorId": "48262654",
                        "name": "A. Haq"
                    },
                    {
                        "authorId": "19229562",
                        "name": "Adnan Zeb"
                    },
                    {
                        "authorId": "2134340252",
                        "name": "M. Suzauddola"
                    },
                    {
                        "authorId": "38487214",
                        "name": "Defu Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Saxena et al [10] used knowledge base embedding in link prediction to solve the sparsity problem of graphs.",
                "Saxena et al [10] used knowledge base embedding in link prediction to solve the sparsity problem"
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "508264922f2531960475ec5020f71da3442aead5",
                "externalIds": {
                    "DBLP": "conf/icit/LiLS21",
                    "DOI": "10.1145/3512576.3512620",
                    "CorpusId": 248087437
                },
                "corpusId": 248087437,
                "publicationVenue": {
                    "id": "6c1a00cb-4b59-44d6-a887-bfd7c5b3768f",
                    "name": "International Conference on Industrial Technology",
                    "type": "conference",
                    "alternate_names": [
                        "ICIT",
                        "Int Conf Ind Technol",
                        "IEEE Int Conf Integr Technol",
                        "IEEE International Conference on Integration Technology"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/508264922f2531960475ec5020f71da3442aead5",
                "title": "CypherQA: Question-answering method based on Attribute Knowledge Graph",
                "abstract": "In knowledge-based question answering(KBQA), most research adopts the question template matching, which faces with challenges such as unclear entity boundaries and difficult path inference when solving complex questions. In this paper, we propose a KBQA solution based on attribute graph. It extracts the mentions in text to recognize relations and entities, and transforms it into a slot-filling Cypher statement to query the answer. Meanwhile, we design a two-layer network based on a structural attention mechanism to optimize entity boundary identification. The solution provides new ideas of relation recognition for answering complex questions over attribute knowledge graph. Experimental results show that the proposed approach achieves promising performance on both CCKS2019 public dataset and the self-built vertical domain dataset.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2144121673",
                        "name": "Chenqi Li"
                    },
                    {
                        "authorId": "2162072414",
                        "name": "Xiangqun Lu"
                    },
                    {
                        "authorId": "2198730",
                        "name": "Kai Shuang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ab15163d8c57fa5706d399c8fb62e489d57b22b7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-06109",
                    "ArXiv": "2112.06109",
                    "CorpusId": 245124446
                },
                "corpusId": 245124446,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ab15163d8c57fa5706d399c8fb62e489d57b22b7",
                "title": "Injecting Numerical Reasoning Skills into Knowledge Base Question Answering Models",
                "abstract": "Embedding-based methods are popular for Knowledge Base Question Answering (KBQA), but few current models have numerical reasoning skills and thus struggle to answer ordinal constrained questions. This paper proposes a new embedding-based KBQA framework which particularly takes numerical reasoning into account. We present NumericalTransformer on top of NSM, a state-of-the-art embedding-based KBQA model, to create NT-NSM. To enable better training, we propose two pre-training tasks with explicit numerical-oriented loss functions on two generated training datasets and a template-based data augmentation method for enriching ordinal constrained QA dataset. Extensive experiments on KBQA benchmarks demonstrate that with the help of our training algorithm, NT-NSM is empowered with numerical reasoning skills and substantially outperforms the baselines in answering ordinal constrained questions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2150671688",
                        "name": "Yu Feng"
                    },
                    {
                        "authorId": "2155700347",
                        "name": "Jing Zhang"
                    },
                    {
                        "authorId": "2108046717",
                        "name": "Xiaokang Zhang"
                    },
                    {
                        "authorId": "2978364",
                        "name": "Lemao Liu"
                    },
                    {
                        "authorId": "1625473962",
                        "name": "Cuiping Li"
                    },
                    {
                        "authorId": "2118061616",
                        "name": "Hong Chen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1f836ccdd422f35c1e2e470b1bcc7201b863cc63",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-05785",
                    "ArXiv": "2112.05785",
                    "DOI": "10.1609/aaai.v36i5.20526",
                    "CorpusId": 245124416
                },
                "corpusId": 245124416,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/1f836ccdd422f35c1e2e470b1bcc7201b863cc63",
                "title": "TempoQR: Temporal Question Reasoning over Knowledge Graphs",
                "abstract": "Knowledge Graph Question Answering (KGQA) involves retrieving facts from a Knowledge Graph (KG) using natural language queries. A KG is a curated set of facts consisting of entities linked by relations. Certain facts include also temporal information forming a Temporal KG (TKG). Although many natural questions involve explicit or implicit time constraints, question answering (QA) over TKGs has been a relatively unexplored area. Existing solutions are mainly designed for simple temporal questions that can be answered directly by a single TKG fact.\n This paper puts forth a comprehensive embedding-based framework for answering complex questions over TKGs. Our method termed temporal question reasoning (TempoQR) exploits TKG embeddings to ground the question to the specific entities and time scope it refers to. It does so by augmenting the question embeddings with context, entity and time-aware information by employing three specialized modules. The first computes a textual representation of a given question, the second combines it with the entity embeddings for entities involved in the question, and the third generates question-specific time embeddings. Finally, a transformer-based encoder learns to fuse the generated temporal information with the question representation, which is used for answer predictions. Extensive experiments show that TempoQR improves accuracy by 25--45 percentage points on complex temporal questions over state-of-the-art approaches and it generalizes better to unseen question types.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1944251405",
                        "name": "Costas Mavromatis"
                    },
                    {
                        "authorId": "2145157804",
                        "name": "Prasanna Lakkur Subramanyam"
                    },
                    {
                        "authorId": "40043851",
                        "name": "V. N. Ioannidis"
                    },
                    {
                        "authorId": "2121390172",
                        "name": "Soji Adeshina"
                    },
                    {
                        "authorId": "48575315",
                        "name": "Phillip Howard"
                    },
                    {
                        "authorId": "2145154001",
                        "name": "Tetiana Grinberg"
                    },
                    {
                        "authorId": "2058623624",
                        "name": "Nagib Hakim"
                    },
                    {
                        "authorId": "50877490",
                        "name": "G. Karypis"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020), solve multi-hop question answering(Saxena et al., 2020; Fang et al., 2020), and so on."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "83c4b8fcef3c214cfae45818a5b4b637372f3083",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2112-02732",
                    "ACL": "2022.naacl-main.372",
                    "ArXiv": "2112.02732",
                    "DOI": "10.18653/v1/2022.naacl-main.372",
                    "CorpusId": 244909056
                },
                "corpusId": 244909056,
                "publicationVenue": {
                    "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
                    "name": "North American Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "North Am Chapter Assoc Comput Linguistics",
                        "NAACL"
                    ],
                    "url": "https://www.aclweb.org/portal/naacl"
                },
                "url": "https://www.semanticscholar.org/paper/83c4b8fcef3c214cfae45818a5b4b637372f3083",
                "title": "JointLK: Joint Reasoning with Language Models and Knowledge Graphs for Commonsense Question Answering",
                "abstract": "Existing KG-augmented models for commonsense question answering primarily focus on designing elaborate Graph Neural Networks (GNNs) to model knowledge graphs (KGs). However, they ignore (i) the effectively fusing and reasoning over question context representations and the KG representations, and (ii) automatically selecting relevant nodes from the noisy KGs during reasoning. In this paper, we propose a novel model, JointLK, which solves the above limitations through the joint reasoning of LM and GNN and the dynamic KGs pruning mechanism. Specifically, JointLK performs joint reasoning between LM and GNN through a novel dense bidirectional attention module, in which each question token attends on KG nodes and each KG node attends on question tokens, and the two modal representations fuse and update mutually by multi-step interactions. Then, the dynamic pruning module uses the attention weights generated by joint reasoning to prune irrelevant KG nodes recursively. We evaluate JointLK on the CommonsenseQA and OpenBookQA datasets, and demonstrate its improvements to the existing LM and LM+KG models, as well as its capability to perform interpretable reasoning.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3124812",
                        "name": "Yueqing Sun"
                    },
                    {
                        "authorId": "117637793",
                        "name": "Qi Shi"
                    },
                    {
                        "authorId": "49231145",
                        "name": "Le Qi"
                    },
                    {
                        "authorId": "2153636092",
                        "name": "Yu Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "edge that support many intelligent applications and services such as question answering (Saxena et al. 2020; Mai et al. 2019b), voice assistant (e.",
                "\u2026or commercial KGs provide structured data and factual knowledge that support many intelligent applications and services such as question answering (Saxena et al. 2020; Mai et al. 2019b), voice assistant (e.g., Apple Siri, Amazon Alex, Google Assistant), search (e.g., Google Search, Bing Search,\u2026"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "30898fe9c0e6420ec44a307f460ee87896c2ccd8",
                "externalIds": {
                    "ArXiv": "2112.00970",
                    "DBLP": "journals/corr/abs-2112-00970",
                    "DOI": "10.1007/s41651-021-00097-4",
                    "CorpusId": 244799183
                },
                "corpusId": 244799183,
                "publicationVenue": {
                    "id": "a9e0bd7c-a663-4479-996d-a3b393cf3e41",
                    "name": "Journal of Geovisualization and Spatial Analysis",
                    "type": "journal",
                    "alternate_names": [
                        "J Geovisualization Spat Anal"
                    ],
                    "issn": "2509-8829",
                    "url": "https://link.springer.com/journal/41651"
                },
                "url": "https://www.semanticscholar.org/paper/30898fe9c0e6420ec44a307f460ee87896c2ccd8",
                "title": "Narrative Cartography with Knowledge Graphs",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "40626717",
                        "name": "Gengchen Mai"
                    },
                    {
                        "authorId": "145329464",
                        "name": "Weiming Huang"
                    },
                    {
                        "authorId": "2112830444",
                        "name": "Ling Cai"
                    },
                    {
                        "authorId": "2070268166",
                        "name": "Rui Zhu"
                    },
                    {
                        "authorId": "2135307417",
                        "name": "Ni Lao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a65115054cfde1d6352309ecb1343fc0efe2b194",
                "externalIds": {
                    "DBLP": "journals/kbs/ZhaoFZYCXZL22",
                    "DOI": "10.1016/j.knosys.2021.107909",
                    "CorpusId": 245032580
                },
                "corpusId": 245032580,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a65115054cfde1d6352309ecb1343fc0efe2b194",
                "title": "EIGAT: Incorporating global information in local attention for knowledge representation learning",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "97522134",
                        "name": "Yu Zhao"
                    },
                    {
                        "authorId": "31227667",
                        "name": "Huali Feng"
                    },
                    {
                        "authorId": "2153072255",
                        "name": "Han Zhou"
                    },
                    {
                        "authorId": "2145435595",
                        "name": "Yanruo Yang"
                    },
                    {
                        "authorId": "2143791763",
                        "name": "Xingyan Chen"
                    },
                    {
                        "authorId": "3360722",
                        "name": "Ruobing Xie"
                    },
                    {
                        "authorId": "1799525",
                        "name": "Fuzhen Zhuang"
                    },
                    {
                        "authorId": "2117895423",
                        "name": "Qing Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "63962805dc43c5fa0a6f082b6fca1f38505dcc60",
                "externalIds": {
                    "DBLP": "journals/ws/SunLCQ22",
                    "DOI": "10.1016/j.websem.2021.100698",
                    "CorpusId": 245482383
                },
                "corpusId": 245482383,
                "publicationVenue": {
                    "id": "78d61825-c1d6-4e7f-ab9d-1525db71105c",
                    "name": "Journal of Web Semantics",
                    "type": "journal",
                    "alternate_names": [
                        "J Web Semant"
                    ],
                    "issn": "1570-8268",
                    "url": "http://www.elsevier.com/locate/websem",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/15708268",
                        "http://www.elsevier.com/wps/find/journaldescription.cws_home/671322/description"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/63962805dc43c5fa0a6f082b6fca1f38505dcc60",
                "title": "Skeleton parsing for complex question answering over knowledge bases",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108535039",
                        "name": "Yawei Sun"
                    },
                    {
                        "authorId": "2209967390",
                        "name": "Peng-fei Li"
                    },
                    {
                        "authorId": "144592996",
                        "name": "Gong Cheng"
                    },
                    {
                        "authorId": "1887019",
                        "name": "Yuzhong Qu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The EmbedKGQA model proposed by Saxena A et al[1]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "69c29712af3f3b410c0224e4e68ea181c28257a6",
                "externalIds": {
                    "DOI": "10.1109/CECIT53797.2021.00184",
                    "CorpusId": 247857460
                },
                "corpusId": 247857460,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/69c29712af3f3b410c0224e4e68ea181c28257a6",
                "title": "Research on Prediction of Link Embedding in Maritime Knowledge Graph",
                "abstract": "The knowledge graph is essentially a multi-relational network, which uses a structured method to store the knowledge system of the corresponding relationships between entities in the real world. It is a structured representation of the real world and brings new technical means to the knowledge representation of the maritime world. However, the data is often incomplete, missing many nodes or links, because they may be only a small part of all based on credible facts. We can solve this problem by predicting missing links. Recent studies on multi-hop KGQA try to use relevant external text to deal with KG sparsity, this method still has relatively large limitations. In another study, a KG embedding method has been proposed to reduce KG sparsity by performing missing link prediction.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "39336958",
                        "name": "Peng Liu"
                    },
                    {
                        "authorId": "2161022494",
                        "name": "Feng Chen"
                    },
                    {
                        "authorId": "2157405452",
                        "name": "Jingping Ma"
                    },
                    {
                        "authorId": "2161719955",
                        "name": "Jiahao Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[30] directly computes knowledge embeddings on the whole retrieved KSG, which is computationally intensive.",
                "EmbedKGQA [30] directly matched pretrained entity KG embeddings with question embedding, which is computationally intensive."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "de75305ab74c60d2bd19a5bfef22d3fef752cc5f",
                "externalIds": {
                    "ArXiv": "2111.10541",
                    "DBLP": "conf/ijcnlp/GaoWHWXL22",
                    "ACL": "2022.aacl-main.7",
                    "CorpusId": 244478146
                },
                "corpusId": 244478146,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/de75305ab74c60d2bd19a5bfef22d3fef752cc5f",
                "title": "Graph-augmented Learning to Rank for Querying Large-scale Knowledge Graph",
                "abstract": "Knowledge graph question answering (KGQA) based on information retrieval aims to answer a question by retrieving answer from a large-scale knowledge graph. Most existing methods first roughly retrieve the knowledge subgraphs (KSG) that may contain candidate answer, and then search for the exact answer in the KSG. However, the KSG may contain thousands of candidate nodes since the knowledge graph involved in querying is often of large scale, thus decreasing the performance of answer selection. To tackle this problem, we first propose to partition the retrieved KSG to several smaller sub-KSGs via a new subgraph partition algorithm and then present a graph-augmented learning to rank model to select the top-ranked sub-KSGs from them. Our proposed model combines a novel subgraph matching networks to capture global interactions in both question and subgraphs and an Enhanced Bilateral Multi-Perspective Matching model to capture local interactions. Finally, we apply an answer selection model on the full KSG and the top-ranked sub-KSGs respectively to validate the effectiveness of our proposed graph-augmented learning to rank method. The experimental results on multiple benchmark datasets have demonstrated the effectiveness of our approach.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "116267999",
                        "name": "Hanning Gao"
                    },
                    {
                        "authorId": "3008832",
                        "name": "Lingfei Wu"
                    },
                    {
                        "authorId": "2152319586",
                        "name": "Po Hu"
                    },
                    {
                        "authorId": "143628849",
                        "name": "Zhihua Wei"
                    },
                    {
                        "authorId": "2392383",
                        "name": "Fangli Xu"
                    },
                    {
                        "authorId": "2064502840",
                        "name": "Bowei Long"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "920598a31d3e6284129ad24ffcc8d957a88068d0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-07658",
                    "ArXiv": "2111.07658",
                    "CorpusId": 244117403
                },
                "corpusId": 244117403,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/920598a31d3e6284129ad24ffcc8d957a88068d0",
                "title": "Calculating Question Similarity is Enough: A New Method for KBQA Tasks",
                "abstract": "Knowledge Base Question Answering (KBQA) aims to answer natural language questions with the help of an external knowledge base. The core idea is to find the link between the internal knowledge behind questions and known triples of the knowledge base. Traditional KBQA task pipelines contain several steps, including entity recognition, entity linking, answering selection, etc. In this kind of pipeline methods, errors in any procedure will inevitably propagate to the final prediction. To address this challenge, this paper proposes a Corpus Generation - Retrieve Method (CGRM) with Pre-training Language Model (PLM) for the KBQA task. The major novelty lies in the design of the new method, wherein our approach, the knowledge enhanced T5 (kT5) model aims to generate natural language QA pairs based on Knowledge Graph triples and directly solve the QA by retrieving the synthetic dataset. The new method can extract more information about the entities from PLM to improve accuracy and simplify the processes. We test our method on NLPCC-ICCPOL 2016 KBQA dataset, and the results show that our method improves the performance of KBQA and the out straight-forward method is competitive with the state-of-the-art.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1907469334",
                        "name": "Hanyu Zhao"
                    },
                    {
                        "authorId": "3411347",
                        "name": "Shaoqing Yuan"
                    },
                    {
                        "authorId": "2124048409",
                        "name": "Jiahong Leng"
                    },
                    {
                        "authorId": "2119600282",
                        "name": "X. Pan"
                    },
                    {
                        "authorId": "2142449761",
                        "name": "Guoqiang Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "EmbedKGQA (Saxena et al., 2020) and GraftNet are two approaches that directly ranks across entities in the knowledge base to predict an answer, by leveraging either KG embeddings from Knowledge Base Completion (KBC); or creating a unified graph from KB and text.",
                "KGQA (Saxena et al., 2020) is the state-of-the-art KBQA system on MetaQA and WebQSP datasets, (8) PullNet (Sun et al.",
                "EmbedKGQA (Saxena, Tripathi, and Talukdar 2020) and GraftNet are two such approaches that directly ranks across entities in the knowledge base to predict an answer, by leveraging either KG embeddings from Knowledge Base Completion (KBC); or creating a unified graph from KB and text.",
                "On the other hand, GraftNet (Sun et al. 2018) and EmbedKGQA (Saxena, Tripathi, and Talukdar 2020) demonstrated their ability to generalize over multiple KGs by demonstrating state-of-the-art performance on MetaQA (Wikimovies) as well as WebQSP (Freebase).",
                "On the other hand, MaSP is a multi-task end-to-end learning approach that focuses of dialog-based KGQA setup.",
                "The two techniques, however, are highly sensitive to the training data; failing to generalize in terms of relation compositionality within a KG. EmbedKGQA and GraftNet show significant drops (between 23-50",
                "Finally, it is unclear how to transfer KBC embedding-based approaches such as EmbedKGQA across KGs since the learnt KG embeddings are tightly coupled with the specific KG in question.",
                "This is most clearly observed in EmbedKGQA, as well as GraftNet-KB, though the use of text (GraftNet-Text and GraftNet-Both) does alleviate this issue.",
                "(5) EmbedKGQA (Saxena, Tripathi, and Talukdar 2020) is the state-of-the-art KBQA system on MetaQA and WebQSP datasets, (6) PullNet (Sun, Bedrax-Weiss, and Cohen 2019) is recent approach evaluated on MetaQA and WebQSP datasets, (7) GraftNet (Sun et al. 2018) infuses both text and KG into a heterogeneous graph and uses GCN for question answering, and (8) EmQL (Sun et al. 2020) is a query embedding approach that was successfully integrated into a KBQA system and evaluated on WebQSP and MetaQA datasets.",
                ", 2018) and EmbedKGQA (Saxena et al., 2020) demonstrated their",
                "We then trained STaG-QA, EmbedKGQA and GraftNet on the new reduced training set and tested the performance\non our new development sets (seen and unseen)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "b2a7b03a3c403b7c7d01a5f52dfdbd845323ab66",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-05825",
                    "ArXiv": "2111.05825",
                    "DOI": "10.18653/v1/2022.findings-emnlp.408",
                    "CorpusId": 243938317
                },
                "corpusId": 243938317,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/b2a7b03a3c403b7c7d01a5f52dfdbd845323ab66",
                "title": "A Two-Stage Approach towards Generalization in Knowledge Base Question Answering",
                "abstract": "Most existing approaches for Knowledge Base Question Answering (KBQA) focus on a specific underlying knowledge base either because of inherent assumptions in the approach, or because evaluating it on a different knowledge base requires non-trivial changes. However, many popular knowledge bases share similarities in their underlying schemas that can be leveraged to facilitate generalization across knowledge bases. To achieve this generalization, we introduce a KBQA framework based on a 2-stage architecture that explicitly separates semantic parsing from the knowledge base interaction, facilitating transfer learning across datasets and knowledge graphs. We show that pretraining on datasets with a different underlying knowledge base can nevertheless provide significant performance gains and reduce sample complexity. Our approach achieves comparable or state-of-the-art performance for LC-QuAD (DBpedia), WebQSP (Freebase), SimpleQuestions (Wikidata) and MetaQA (Wikimovies-KG).",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "30090681",
                        "name": "Srinivas Ravishankar"
                    },
                    {
                        "authorId": "2139248433",
                        "name": "June Thai"
                    },
                    {
                        "authorId": "145749443",
                        "name": "I. Abdelaziz"
                    },
                    {
                        "authorId": "2139896353",
                        "name": "Nandana Mihidukulasooriya"
                    },
                    {
                        "authorId": "2138053379",
                        "name": "Tahira Naseem"
                    },
                    {
                        "authorId": "2123450380",
                        "name": "P. Kapanipathi"
                    },
                    {
                        "authorId": "2139887382",
                        "name": "Gaetano Rossilleo"
                    },
                    {
                        "authorId": "2297836",
                        "name": "Achille Fokoue"
                    }
                ]
            }
        },
        {
            "contexts": [
                "EmbedKGQA [165] Freebase; MetaQA-KG [213] Simple and Complex KG Embedding based method",
                "The EmbedKGQA methodology has been shown to be effective on multihop questions of different hop lengths and also on incomplete KGs i.e. KGs with high number of missing links/relations.",
                "EmbedKGQA, similar to KEQA, finds embeddings (in complex dense multidimensional space)\nof nodes and relations in the KG using ComplEx [181].",
                "In order to answer multihop questions using KG embeddings, methods such as EmbedKGQA [165] have been developed."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "7c5064305b8f8add57f211e185cdfeb9d68e6e31",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2111-05937",
                    "ArXiv": "2111.05937",
                    "CorpusId": 243986045
                },
                "corpusId": 243986045,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/7c5064305b8f8add57f211e185cdfeb9d68e6e31",
                "title": "Recent Advances in Automated Question Answering In Biomedical Domain",
                "abstract": "The objective of automated Question Answering (QA) systems is to provide answers to user queries in a time efficient manner. The answers are usually found in either databases (or knowledge bases) or a collection of documents commonly referred to as the corpus. In the past few decades there has been a proliferation of acquisition of knowledge and consequently there has been an exponential growth in new scientific articles in the field of biomedicine. Therefore, it has become difficult to keep track of all the information in the domain, even for domain experts. With the improvements in commercial search engines, users can type in their queries and get a small set of documents most relevant for answering their query, as well as relevant snippets from the documents in some cases. However, it may be still tedious and time consuming to manually look for the required information or answers. This has necessitated the development of efficient QA systems which aim to find exact and precise answers to user provided natural language questions in the domain of biomedicine. In this paper, we introduce the basic methodologies used for developing general domain QA systems, followed by a thorough investigation of different aspects of biomedical QA systems, including benchmark datasets and several proposed approaches, both using structured databases and collection of texts. We also explore the limitations of current systems and explore potential avenues for further advancement.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "39886598",
                        "name": "K. D. Baksi"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Knowledge Graph The process of utilizing KG Database [5] to perform QA bots can be categorized into five steps:\n\u2022 User input a medical question\n\u2022 Extract keywords from the question using pre-defined matching words.",
                "For example, Saxena et al address the problem of complex KGQA coupling KG embeddings with a question embedding vector in their EmbedKGQA [5], Liu Huangyong at Institute of Software, Chinese Academy of Sciences have constructed a Chinese-versioned medical QA bot using a certain scale medical domain knowledge map centered on disease and established a knowledge graph to complete automatic questionanswering and analysis services [6].",
                "Knowledge Graph The process of utilizing KG Database [5] to perform QA bots can be categorized into five steps: \u2022 User input a medical question"
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "bdc92de8d38cc6867b80ca19475fff9f2d37fbcb",
                "externalIds": {
                    "DOI": "10.1088/1742-6596/2078/1/012048",
                    "CorpusId": 243984080
                },
                "corpusId": 243984080,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bdc92de8d38cc6867b80ca19475fff9f2d37fbcb",
                "title": "A Medical Service Application Based on 3D-CNN and Knowledge Graph",
                "abstract": "Remote medical prognosis application is a category of medical tests tool designed to collect patients\u2019 body conditions and offer diagnosis results synchronously. However, most online applications are predicated on a simple chat bot which typically redirect patients to other online medical websites, which undermines the user experience and may prompt useless information for their reference. To tackle these issues, this paper proposed a medical prognosis application with deep learning techniques for a more responsive and intelligent medical prognosis procedure. This application can be break down into three parts-lung cancer detection, a database-supporting medical QA bot and a Hierarchical Bidirectional LSTM model (HBDA). A 3D-CNN model is built for the lung cancer detection, with a sequence of sliced CT images as inputs and outputs a probability scaler for tumor indications. A knowledge graph is applied in the medical QA bot implementation and the HBDA model is designed for semantic segmentation in order to better capture users\u2019 intention in medical questions. For the performance of the medical prognosis, since we have limited computer memory, the 3D-CNN didn\u2019t perform very well on detecting tumor conditions in the CT images with accuracy at around 70%. The knowledge graph-based medical QA bot intelligently recognize the underlying pattern in patients\u2019 question and delivered decent medical response. The HBDA model performs well on distinguish the similarities and disparities between various medical questions, reaching accuracy at 90%. These results shed light for the feasibility of utilizing deep learning techniques such as 3D-CNN, Knowledge Graph, and Hierarchical Bi-directional LSTM to simulate the medical prognosis process.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "152320827",
                        "name": "Jiasheng Ni"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "32d61e4dfb039e126c1b54cae7bac0d33ffcd51f",
                "externalIds": {
                    "DBLP": "journals/aei/ZhouHGLPZSB21",
                    "DOI": "10.1016/j.aei.2021.101441",
                    "CorpusId": 242072273
                },
                "corpusId": 242072273,
                "publicationVenue": {
                    "id": "ec497fa8-833a-4d68-873a-539c20989c22",
                    "name": "Advanced Engineering Informatics",
                    "type": "journal",
                    "alternate_names": [
                        "Adv Eng Informatics"
                    ],
                    "issn": "1474-0346",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/622240/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/14740346"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/32d61e4dfb039e126c1b54cae7bac0d33ffcd51f",
                "title": "An end-to-end tabular information-oriented causality event evolutionary knowledge graph for manufacturing documents",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118870182",
                        "name": "Bin Zhou"
                    },
                    {
                        "authorId": "2064244474",
                        "name": "B. Hua"
                    },
                    {
                        "authorId": "2118713039",
                        "name": "X. Gu"
                    },
                    {
                        "authorId": "1390856745",
                        "name": "Yuqian Lu"
                    },
                    {
                        "authorId": "2143986469",
                        "name": "Tao Peng"
                    },
                    {
                        "authorId": "2149515768",
                        "name": "Yu Zheng"
                    },
                    {
                        "authorId": "9375754",
                        "name": "Xingwang Shen"
                    },
                    {
                        "authorId": "2072742582",
                        "name": "Jinsong Bao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The literature [1-4] extracts entities and their relational features from questions and later matches the relevant answers in knowledge base, which mostly consists of questions and answer pairs."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bb4ac62d83c55ab0295f9c087431d6b6af958701",
                "externalIds": {
                    "DOI": "10.1088/1742-6596/2078/1/012017",
                    "CorpusId": 243984147
                },
                "corpusId": 243984147,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bb4ac62d83c55ab0295f9c087431d6b6af958701",
                "title": "Structured Knowledge Base Q&A System Based on TorchServe Deployment",
                "abstract": "Structured tabular data are widely used in various information systems, especially with the development of big data technology, making it more difficult to query on these complex data. SQL facilitates the query on structured tabular, however, the mastery of SQL has a certain threshold for most non-expert users. Therefore, in order to facilitate ordinary users to quickly obtain the required information in complex structured data, we design and implement a Q&A system for structured knowledge. First, we make a detailed distinction between Q&A scenarios for structured data and design different approaches, respectively. Then, we introduce deep learning models in system algorithm layer to enhance the generalization ability. Finally, the TorchServe framework is used to optimize system deployment and improve system performance using batch inference. The experimental results show that the prototype system has certain generalization ability and also has some advantages in performance compared with traditional methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2100406437",
                        "name": "Xia Ye"
                    },
                    {
                        "authorId": "2152936873",
                        "name": "Ruiheng Liu"
                    },
                    {
                        "authorId": "2100915868",
                        "name": "Zengying Yue"
                    }
                ]
            }
        },
        {
            "contexts": [
                "EmbedKGQA (Saxena et al., 2020) expends KEQA for multi-relational KBQA by calculating the answer score from the leaned question embedding and the entity embeddings.",
                "Inspired by the KG embedding technique for KG completion, EmbedKGQA (Saxena et al., 2020) employs the pre-trained entity embeddings by KGE and the question embedding to calculate the score of each candidate answer entity.",
                "We compare PKEEQA with the state-of-the-art multi-relational KBQA baselines, including VRN (Zhang et al., 2018), KVMem (Miller et al., 2016), GraftNet (Sun et al., 2018), PullNet (Sun et al., 2019a) and EmbedKGQA (Saxena et al., 2020)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6f4b7f6185b859478655bada08da13e6a843b67f",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2110-15622",
                    "ArXiv": "2110.15622",
                    "CorpusId": 240288552
                },
                "corpusId": 240288552,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6f4b7f6185b859478655bada08da13e6a843b67f",
                "title": "Path-Enhanced Multi-Relational Question Answering with Knowledge Graph Embeddings",
                "abstract": "The multi-relational Knowledge Base Question Answering (KBQA) system performs multi-hop reasoning over the knowledge graph (KG) to achieve the answer. Recent approaches attempt to introduce the knowledge graph embedding (KGE) technique to handle the KG incompleteness but only consider the triple facts and neglect the significant semantic correlation between paths and multi-relational questions. In this paper, we propose a Path and Knowledge Embedding-Enhanced multi-relational Question Answering model (PKEEQA), which leverages multi-hop paths between entities in the KG to evaluate the ambipolar correlation between a path embedding and a multi-relational question embedding via a customizable path representation mechanism, benefiting for achieving more accurate answers from the perspective of both the triple facts and the extra paths. Experimental results illustrate that PKEEQA improves KBQA models' performance for multi-relational question answering with explainability to some extent derived from paths.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2052804171",
                        "name": "Guanglin Niu"
                    },
                    {
                        "authorId": "2154900133",
                        "name": "Yang Li"
                    },
                    {
                        "authorId": "1672552269",
                        "name": "Chengguang Tang"
                    },
                    {
                        "authorId": "2111217841",
                        "name": "Zhongkai Hu"
                    },
                    {
                        "authorId": "2145002676",
                        "name": "Shibin Yang"
                    },
                    {
                        "authorId": "2152926434",
                        "name": "Peng Li"
                    },
                    {
                        "authorId": "121899912",
                        "name": "Chengyu Wang"
                    },
                    {
                        "authorId": "2144221991",
                        "name": "Hao Wang"
                    },
                    {
                        "authorId": "2152147863",
                        "name": "Jian Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Knowledge graphs form the backbone of many AI systems across a wide range of domains: recommender systems [27, 28], question answering [21, 23] and commonsense reasoning [12, 14]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cff582c6d7a28c48e0d37ac3c10ea9a8c3fc4e78",
                "externalIds": {
                    "ArXiv": "2110.14890",
                    "DBLP": "conf/kdd/RenDDCZLS22",
                    "DOI": "10.1145/3534678.3539405",
                    "CorpusId": 240070688
                },
                "corpusId": 240070688,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/cff582c6d7a28c48e0d37ac3c10ea9a8c3fc4e78",
                "title": "SMORE: Knowledge Graph Completion and Multi-hop Reasoning in Massive Knowledge Graphs",
                "abstract": "Knowledge graphs (KGs) capture knowledge in the form of head--relation--tail triples and are a crucial component in many AI systems. There are two important reasoning tasks on KGs: (1) single-hop knowledge graph completion, which involves predicting individual links in the KG; and (2), multi-hop reasoning, where the goal is to predict which KG entities satisfy a given logical query. Embedding-based methods solve both tasks by first computing an embedding for each entity and relation, then using them to form predictions. However, existing scalable KG embedding frameworks only support single-hop knowledge graph completion and cannot be applied to the more challenging multi-hop reasoning task. Here we present Scalable Multi-hOp REasoning (SMORE), the first general framework for both single-hop and multi-hop reasoning in KGs. Using a single machine SMORE can perform multi-hop reasoning in Freebase KG (86M entities, 338M edges), which is 1,500x larger than previously considered KGs. The key to SMORE's runtime performance is a novel bidirectional rejection sampling that achieves a square root reduction of the complexity of online training data generation. Furthermore, SMORE exploits asynchronous scheduling, overlapping CPU-based data sampling, GPU-based embedding computation, and frequent CPU--GPU IO. SMORE increases throughput (i.e., training speed) over prior multi-hop KG frameworks by 2.2x with minimal GPU memory requirements (2GB for training 400-dim embeddings on 86M-node Freebase) and achieves near linear speed-up with the number of GPUs. Moreover, on the simpler single-hop knowledge graph completion task SMORE achieves comparable or even better runtime performance to state-of-the-art frameworks on both single GPU and multi-GPU settings.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "40046694",
                        "name": "Hongyu Ren"
                    },
                    {
                        "authorId": "2791430",
                        "name": "H. Dai"
                    },
                    {
                        "authorId": "144445933",
                        "name": "Bo Dai"
                    },
                    {
                        "authorId": "1425082935",
                        "name": "Xinyun Chen"
                    },
                    {
                        "authorId": "65855107",
                        "name": "Denny Zhou"
                    },
                    {
                        "authorId": "1702139",
                        "name": "J. Leskovec"
                    },
                    {
                        "authorId": "50319359",
                        "name": "D. Schuurmans"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d91dae75e7d3a13aad9a6815d6cbdf9a42f897e2",
                "externalIds": {
                    "DBLP": "conf/nips/AtzeniBL21",
                    "ArXiv": "2110.14266",
                    "CorpusId": 239998351
                },
                "corpusId": 239998351,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/d91dae75e7d3a13aad9a6815d6cbdf9a42f897e2",
                "title": "SQALER: Scaling Question Answering by Decoupling Multi-Hop and Logical Reasoning",
                "abstract": "State-of-the-art approaches to reasoning and question answering over knowledge graphs (KGs) usually scale with the number of edges and can only be applied effectively on small instance-dependent subgraphs. In this paper, we address this issue by showing that multi-hop and more complex logical reasoning can be accomplished separately without losing expressive power. Motivated by this insight, we propose an approach to multi-hop reasoning that scales linearly with the number of relation types in the graph, which is usually significantly smaller than the number of edges or nodes. This produces a set of candidate solutions that can be provably refined to recover the solution to the original problem. Our experiments on knowledge-based question answering show that our approach solves the multi-hop MetaQA dataset, achieves a new state-of-the-art on the more challenging WebQuestionsSP, is orders of magnitude more scalable than competitive approaches, and can achieve compositional generalization out of the training distribution.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "32946276",
                        "name": "Mattia Atzeni"
                    },
                    {
                        "authorId": "1685405",
                        "name": "Jasmina Bogojeska"
                    },
                    {
                        "authorId": "1966031",
                        "name": "Andreas Loukas"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "50a00638b8fb2037bf8d06ef7b1f52ae4299f8a1",
                "externalIds": {
                    "DBLP": "series/synthesis/2021Roy",
                    "DOI": "10.2200/s0113ed1v01y202109icr076",
                    "CorpusId": 240191541
                },
                "corpusId": 240191541,
                "publicationVenue": {
                    "id": "ab88597d-3d07-40b0-8dc9-7c34c7faf1c3",
                    "name": "Synthesis Lectures on Information Concepts Retrieval and Services",
                    "type": "journal",
                    "alternate_names": [
                        "Synthesis Lectures on Information Concepts, Retrieval, and Services",
                        "Synth Lect Inf Concept Retr Serv"
                    ],
                    "issn": "1947-945X",
                    "url": "https://www.morganclaypool.com/toc/icr/1/1"
                },
                "url": "https://www.semanticscholar.org/paper/50a00638b8fb2037bf8d06ef7b1f52ae4299f8a1",
                "title": "Question Answering for the Curated Web: Tasks and Methods in QA over Knowledge Bases and Text Collections",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "40075164",
                        "name": "Rishiraj Saha Roy"
                    },
                    {
                        "authorId": "39775488",
                        "name": "Avishek Anand"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Multi-hop reasoning over knowledge graphs (KGs)\u2014which aims to find answer entities of given queries using knowledge from KGs\u2014has attracted great attention from both academia and industry recently [28, 26, 18]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0e31660210a4f683fe22acd5bc53f737d765f1f3",
                "externalIds": {
                    "ArXiv": "2110.13715",
                    "DBLP": "conf/nips/ZhangWCJW21",
                    "CorpusId": 239885601
                },
                "corpusId": 239885601,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/0e31660210a4f683fe22acd5bc53f737d765f1f3",
                "title": "ConE: Cone Embeddings for Multi-Hop Reasoning over Knowledge Graphs",
                "abstract": "Query embedding (QE) -- which aims to embed entities and first-order logical (FOL) queries in low-dimensional spaces -- has shown great power in multi-hop reasoning over knowledge graphs. Recently, embedding entities and queries with geometric shapes becomes a promising direction, as geometric shapes can naturally represent answer sets of queries and logical relationships among them. However, existing geometry-based models have difficulty in modeling queries with negation, which significantly limits their applicability. To address this challenge, we propose a novel query embedding model, namely Cone Embeddings (ConE), which is the first geometry-based QE model that can handle all the FOL operations, including conjunction, disjunction, and negation. Specifically, ConE represents entities and queries as Cartesian products of two-dimensional cones, where the intersection and union of cones naturally model the conjunction and disjunction operations. By further noticing that the closure of complement of cones remains cones, we design geometric complement operators in the embedding space for the negation operations. Experiments demonstrate that ConE significantly outperforms existing state-of-the-art methods on benchmark datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "122542593",
                        "name": "Zhanqiu Zhang"
                    },
                    {
                        "authorId": "2146041754",
                        "name": "Jie Wang"
                    },
                    {
                        "authorId": "2155288144",
                        "name": "Jiajun Chen"
                    },
                    {
                        "authorId": "1743600",
                        "name": "Shuiwang Ji"
                    },
                    {
                        "authorId": "144864333",
                        "name": "Feng Wu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2022 EmbedKGQA [22] is a KG embedding driving method for multi-hop KGQA which matches the pretrained entity embeddings with question embeddings generated from the transformer.",
                "Following [22], for all topic entities labeled in the original Freebase, He et al.",
                "This evaluating indicator is popular and publicly recognized and has been used in many recent KGQA works [22, 26, 37].",
                "Definition 1 (Multi-hop question) [2, 5, 22] If a natural language question involves more than one predicate between the topic entity and answer, then we believe the answer is multiple hops away from the topic entity in the KG.",
                "Thanks to its ability to simplify the manipulation while preserving the KG inherent structure, it can benefit a variety of downstream tasks to take the entire KG into consideration, such as entity alignment [45], relation prediction [20] and even KGQA work [22].",
                "[22] propose a novel framework, named EmbedKGQA, which leverages the pre-trained KG embeddings to enrich the learned entity and relation representations.",
                "WebQuestionsSP-tiny [22, 27, 36] is a relatively small dataset for training but relies on a large-scale KG (Freebase) whose entities\u2019 count is greater than 10 million.",
                "Motivated by the previous work EmbedKGQA [22], we show how our model leverages an end-to-end neural network that employs the KG entity and relation embeddings to provide complex questions with answers from the KG.",
                "Inspired by the competitive performance of previous work EmbedKGQA [22], we observe that the global relation knowledge and structure information preserved in KG embedding could potentially be used to resolve these issues efficiently and improve the overall accuracy of question answering.",
                "8% compared to EmbedKGQA [22] and an increase of 1."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "20c320ee75e33cafbf6f4ed3b5d200281cf92dec",
                "externalIds": {
                    "DBLP": "journals/datamine/JinZYTYL23",
                    "ArXiv": "2110.12679",
                    "DOI": "10.1007/s10618-022-00891-8",
                    "CorpusId": 239768515
                },
                "corpusId": 239768515,
                "publicationVenue": {
                    "id": "d263025a-9eaf-443f-9bbf-72377e8d22a6",
                    "name": "Data mining and knowledge discovery",
                    "type": "journal",
                    "alternate_names": [
                        "Data Mining and Knowledge Discovery",
                        "Data Min Knowl Discov",
                        "Data min knowl discov"
                    ],
                    "issn": "1384-5810",
                    "url": "https://www.springer.com/computer/database+management+&+information+retrieval/journal/10618",
                    "alternate_urls": [
                        "https://link.springer.com/journal/10618",
                        "http://www.springer.com/computer/database+management+&+information+retrieval/journal/10618"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/20c320ee75e33cafbf6f4ed3b5d200281cf92dec",
                "title": "Improving embedded knowledge graph multi-hop question answering by introducing relational chain reasoning",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2151509980",
                        "name": "Weiqiang Jin"
                    },
                    {
                        "authorId": "2200563019",
                        "name": "Biao Zhao"
                    },
                    {
                        "authorId": "2149413711",
                        "name": "Hang Yu"
                    },
                    {
                        "authorId": "2146848934",
                        "name": "Xi Tao"
                    },
                    {
                        "authorId": "23588003",
                        "name": "Ruiping Yin"
                    },
                    {
                        "authorId": "2146431814",
                        "name": "Guizhong Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3358f057b7139201797dc34f938b99a13f99ae90",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-14364",
                    "ArXiv": "2109.14364",
                    "DOI": "10.24432/C5D30N",
                    "CorpusId": 238215256
                },
                "corpusId": 238215256,
                "publicationVenue": {
                    "id": "c2abee38-d372-4e2a-ac1e-2cd22999a564",
                    "name": "Conference on Automated Knowledge Base Construction",
                    "type": "conference",
                    "alternate_names": [
                        "AKBC",
                        "Conf Autom Knowl Base Constr",
                        "Automated Knowledge Base Construction",
                        "Autom Knowl Base Constr"
                    ],
                    "url": "https://www.akbc.ws/"
                },
                "url": "https://www.semanticscholar.org/paper/3358f057b7139201797dc34f938b99a13f99ae90",
                "title": "Multilingual Fact Linking",
                "abstract": "Knowledge-intensive NLP tasks can benefit from linking natural language text with facts from a Knowledge Graph (KG). Although facts themselves are language-agnostic, the fact labels (i.e., language-specific representation of the fact) in the KG are often present only in a few languages. This makes it challenging to link KG facts to sentences in languages other than the limited set of languages. To address this problem, we introduce the task of Multilingual Fact Linking (MFL) where the goal is to link fact expressed in a sentence to corresponding fact in the KG, even when the fact label in the KG is not available in the language of the sentence. To facilitate research in this area, we present a new evaluation dataset, IndicLink. This dataset contains 11,293 linked WikiData facts and 6,429 sentences spanning English and six Indian languages. We propose a Retrieval+Generation model, ReFCoG, that can scale to millions of KG facts by combining Dual Encoder based retrieval with a Seq2Seq based generation model which is constrained to output only valid KG facts. ReFCoG outperforms standard Retrieval+Re-ranking models by 10.7 pts in Precision@1. In spite of this gain, the model achieves an overall score of 52.1, showing ample scope for improvement in the task.ReFCoG code and IndicLink data are available at https://github.com/SaiKeshav/mfl",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1500421125",
                        "name": "Keshav Kolluru"
                    },
                    {
                        "authorId": "2511068",
                        "name": "Mart\u00edn Rezk"
                    },
                    {
                        "authorId": "1441098631",
                        "name": "Pat Verga"
                    },
                    {
                        "authorId": "2058480371",
                        "name": "W. Cohen"
                    },
                    {
                        "authorId": "2408872",
                        "name": "P. Talukdar"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "190f71c6f835f3040cbe2be7f15b05b117dced9d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-14401",
                    "ArXiv": "2109.14401",
                    "ACL": "2021.emnlp-main.657",
                    "DOI": "10.18653/v1/2021.emnlp-main.657",
                    "CorpusId": 238215189
                },
                "corpusId": 238215189,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/190f71c6f835f3040cbe2be7f15b05b117dced9d",
                "title": "BiQUE: Biquaternionic Embeddings of Knowledge Graphs",
                "abstract": "Knowledge graph embeddings (KGEs) compactly encode multi-relational knowledge graphs (KGs). Existing KGE models rely on geometric operations to model relational patterns. Euclidean (circular) rotation is useful for modeling patterns such as symmetry, but cannot represent hierarchical semantics. In contrast, hyperbolic models are effective at modeling hierarchical relations, but do not perform as well on patterns on which circular rotation excels. It is crucial for KGE models to unify multiple geometric transformations so as to fully cover the multifarious relations in KGs. To do so, we propose BiQUE, a novel model that employs biquaternions to integrate multiple geometric transformations, viz., scaling, translation, Euclidean rotation, and hyperbolic rotation. BiQUE makes the best trade-offs among geometric operators during training, picking the best one (or their best combination) for each relation. Experiments on five datasets show BiQUE\u2019s effectiveness.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2148901481",
                        "name": "Jia Guo"
                    },
                    {
                        "authorId": "1677641386",
                        "name": "Stanley Kok"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(Saxena, Tripathi, and Talukdar 2020) encodes both text and entities, text based on language models and entities based on knowledge graph embeddings (Trouillon et al. 2016) and shows that text can help KBQA in an incomplete setting.",
                "\u2026into two groups: (a) Question-to-entities: where techniques output the answer entities from the knowledge graph ignoring the SPARQL query (Saxena, Tripathi, and Talukdar 2020; Sun et al. 2018; Vakulenko et al. 2019), and (b) semantic parsing based: where approaches output intermediate\u2026"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "76ddbcc982ba1ccb6128dfd5960a3ab081f703ff",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-13430",
                    "ArXiv": "2109.13430",
                    "CorpusId": 238198153
                },
                "corpusId": 238198153,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/76ddbcc982ba1ccb6128dfd5960a3ab081f703ff",
                "title": "SYGMA: System for Generalizable Modular Question Answering OverKnowledge Bases",
                "abstract": "Knowledge Base Question Answering (KBQA) tasks that in-volve complex reasoning are emerging as an important re-search direction. However, most KBQA systems struggle withgeneralizability, particularly on two dimensions: (a) acrossmultiple reasoning types where both datasets and systems haveprimarily focused on multi-hop reasoning, and (b) across mul-tiple knowledge bases, where KBQA approaches are specif-ically tuned to a single knowledge base. In this paper, wepresent SYGMA, a modular approach facilitating general-izability across multiple knowledge bases and multiple rea-soning types. Specifically, SYGMA contains three high levelmodules: 1) KB-agnostic question understanding module thatis common across KBs 2) Rules to support additional reason-ing types and 3) KB-specific question mapping and answeringmodule to address the KB-specific aspects of the answer ex-traction. We demonstrate effectiveness of our system by evalu-ating on datasets belonging to two distinct knowledge bases,DBpedia and Wikidata. In addition, to demonstrate extensi-bility to additional reasoning types we evaluate on multi-hopreasoning datasets and a new Temporal KBQA benchmarkdataset on Wikidata, namedTempQA-WD1, introduced in thispaper. We show that our generalizable approach has bettercompetetive performance on multiple datasets on DBpediaand Wikidata that requires both multi-hop and temporal rea-soning",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2965855",
                        "name": "S. Neelam"
                    },
                    {
                        "authorId": "2027112872",
                        "name": "Udit Sharma"
                    },
                    {
                        "authorId": "9621738",
                        "name": "Hima P. Karanam"
                    },
                    {
                        "authorId": "3192316",
                        "name": "S. Ikbal"
                    },
                    {
                        "authorId": "2123450380",
                        "name": "P. Kapanipathi"
                    },
                    {
                        "authorId": "145749443",
                        "name": "I. Abdelaziz"
                    },
                    {
                        "authorId": "2689774",
                        "name": "Nandana Mihindukulasooriya"
                    },
                    {
                        "authorId": "2145430350",
                        "name": "Young-Suk Lee"
                    },
                    {
                        "authorId": "2116935757",
                        "name": "Santosh K. Srivastava"
                    },
                    {
                        "authorId": "1999108",
                        "name": "Cezar Pendus"
                    },
                    {
                        "authorId": "2066641453",
                        "name": "Saswati Dana"
                    },
                    {
                        "authorId": "50252087",
                        "name": "Dinesh Garg"
                    },
                    {
                        "authorId": "2297836",
                        "name": "Achille Fokoue"
                    },
                    {
                        "authorId": "2127473246",
                        "name": "G. P. S. Bhargav"
                    },
                    {
                        "authorId": "50564082",
                        "name": "Dinesh Khandelwal"
                    },
                    {
                        "authorId": "30090681",
                        "name": "Srinivas Ravishankar"
                    },
                    {
                        "authorId": "2121386026",
                        "name": "Sairam Gurajada"
                    },
                    {
                        "authorId": "2141904540",
                        "name": "Maria Chang"
                    },
                    {
                        "authorId": "1400349389",
                        "name": "Rosario A. Uceda-Sosa"
                    },
                    {
                        "authorId": "1781292",
                        "name": "S. Roukos"
                    },
                    {
                        "authorId": "1703070",
                        "name": "Alexander G. Gray"
                    },
                    {
                        "authorId": "2129995696",
                        "name": "Guilherme LimaRyan Riegel"
                    },
                    {
                        "authorId": "1767482",
                        "name": "F. Luus"
                    },
                    {
                        "authorId": "143666446",
                        "name": "L. V. Subramaniam"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Huang et al. (2019) and Saxena et al. (2020) leveraged the knowledge graph embedding (KGE) to deal with the problem of sparse graphs.",
                "The KGQA module is based on the work of Embedded KGQA (Saxena et al., 2020).",
                "We describe the performance of Emily in addressing persona-related questions via Embedded KGQA (Saxena et al., 2020).",
                "Unlike Huang et al. (2019) focusing on simple questions, Saxena et al. (2020) proposed an \u201cEmbedKGQA\u201d model performing multi-hop KGQA over sparse KG."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "ec5032a0f7faeea81a63ee78b80fae96e6cf359a",
                "externalIds": {
                    "ArXiv": "2109.08875",
                    "DBLP": "journals/corr/abs-2109-08875",
                    "CorpusId": 237572036
                },
                "corpusId": 237572036,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ec5032a0f7faeea81a63ee78b80fae96e6cf359a",
                "title": "Emily: Developing An Emotion-affective Open-Domain Chatbot with Knowledge Graph-based Persona",
                "abstract": "In this paper, we describe approaches for developing Emily, an emotion-affective open-domain chatbot. Emily can perceive a user's negative emotion state and offer supports by positively converting the user's emotion states. This is done by finetuning a pretrained dialogue model upon data capturing dialogue contexts and desirable emotion states transition across turns. Emily can differentiate a general open-domain dialogue utterance with questions relating to personal information. By leveraging a question-answering approach based on knowledge graphs to handle personal information, Emily maintains personality consistency. We evaluate Emily against a few state-of-the-art open-domain chatbots and show the effects of the proposed approaches in emotion affecting and addressing personality inconsistency.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "46314360",
                        "name": "Weixuan Wang"
                    },
                    {
                        "authorId": "47298961",
                        "name": "Xiaoling Cai"
                    },
                    {
                        "authorId": "122854309",
                        "name": "Chongxuan Huang"
                    },
                    {
                        "authorId": "2145335027",
                        "name": "Haoran Wang"
                    },
                    {
                        "authorId": "2130373",
                        "name": "H. Lu"
                    },
                    {
                        "authorId": "2108797917",
                        "name": "Ximing Liu"
                    },
                    {
                        "authorId": "2067862988",
                        "name": "Wei Peng"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018), or iteratively built using learned models (Sun et al., 2019; Shi et al., 2021; Cohen et al., 2020; Saxena et al., 2020).",
                "\u2026for this task.548\nOne line of KBQA approaches first constructs a 549 query-specific subgraph with information retrieved 550 from the KB and then rank entity nodes to select 551 top entities as the answer (Sun et al., 2018, 2019; 552 Saxena et al., 2020; Cohen et al., 2020; Shi et al., 553 2021).",
                "One line of KBQA approaches tackles the problem by first constructing a query-specific subgraph with information retrieved from the knowledge base and then ranking entity nodes to select top entities as the answer (Sun et al., 2018, 2019; Saxena et al., 2020; Cohen et al., 2020; Shi et al., 2021)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9b2473c1d88f2300358dc8b11e826dc5ba81bbbc",
                "externalIds": {
                    "ArXiv": "2109.08678",
                    "DBLP": "journals/corr/abs-2109-08678",
                    "ACL": "2022.acl-long.417",
                    "DOI": "10.18653/v1/2022.acl-long.417",
                    "CorpusId": 237562927
                },
                "corpusId": 237562927,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/9b2473c1d88f2300358dc8b11e826dc5ba81bbbc",
                "title": "RNG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base Question Answering",
                "abstract": "Existing KBQA approaches, despite achieving strong performance on i.i.d. test data, often struggle in generalizing to questions involving unseen KB schema items. Prior ranking-based approaches have shown some success in generalization, but suffer from the coverage issue. We present RnG-KBQA, a Rank-and-Generate approach for KBQA, which remedies the coverage issue with a generation model while preserving a strong generalization capability. Our approach first uses a contrastive ranker to rank a set of candidate logical forms obtained by searching over the knowledge graph. It then introduces a tailored generation model conditioned on the question and the top-ranked candidates to compose the final logical form. We achieve new state-of-the-art results on GrailQA and WebQSP datasets. In particular, our method surpasses the prior state-of-the-art by a large margin on the GrailQA leaderboard. In addition, RnG-KBQA outperforms all prior approaches on the popular WebQSP benchmark, even including the ones that use the oracle entity linking. The experimental results demonstrate the effectiveness of the interplay between ranking and generation, which leads to the superior performance of our proposed approach across all settings with especially strong improvements in zero-shot generalization.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50183897",
                        "name": "Xi Ye"
                    },
                    {
                        "authorId": "3014143",
                        "name": "Semih Yavuz"
                    },
                    {
                        "authorId": "20851195",
                        "name": "Kazuma Hashimoto"
                    },
                    {
                        "authorId": "2118860628",
                        "name": "Yingbo Zhou"
                    },
                    {
                        "authorId": "2054594326",
                        "name": "Caiming Xiong"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f77d63d7014204bd8bb678708880a561ffe0fe42",
                "externalIds": {
                    "ACL": "2021.emnlp-main.694",
                    "DBLP": "conf/emnlp/SenOS21",
                    "ArXiv": "2109.05808",
                    "DOI": "10.18653/v1/2021.emnlp-main.694",
                    "CorpusId": 237491606
                },
                "corpusId": 237491606,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/f77d63d7014204bd8bb678708880a561ffe0fe42",
                "title": "Expanding End-to-End Question Answering on Differentiable Knowledge Graphs with Intersection",
                "abstract": "End-to-end question answering using a differentiable knowledge graph is a promising technique that requires only weak supervision, produces interpretable results, and is fully differentiable. Previous implementations of this technique (Cohen et al, 2020) have focused on single-entity questions using a relation following operation. In this paper, we propose a model that explicitly handles multiple-entity questions by implementing a new intersection operation, which identifies the shared elements between two sets of entities. We find that introducing intersection improves performance over a baseline model on two datasets, WebQuestionsSP (69.6% to 73.3% Hits@1) and ComplexWebQuestions (39.8% to 48.7% Hits@1), and in particular, improves performance on questions with multiple entities by over 14% on WebQuestionsSP and by 19% on ComplexWebQuestions.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "50650468",
                        "name": "Priyanka Sen"
                    },
                    {
                        "authorId": "1741702",
                        "name": "Amir Saffari"
                    },
                    {
                        "authorId": "2126500061",
                        "name": "Armin Oliya"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "15d643f4c27d373aa46f26a760051e76fde81dc2",
                "externalIds": {
                    "ArXiv": "2109.05817",
                    "ACL": "2021.emnlp-main.345",
                    "DBLP": "conf/emnlp/SaffariOSA21",
                    "DOI": "10.18653/v1/2021.emnlp-main.345",
                    "CorpusId": 237494509
                },
                "corpusId": 237494509,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/15d643f4c27d373aa46f26a760051e76fde81dc2",
                "title": "End-to-End Entity Resolution and Question Answering Using Differentiable Knowledge Graphs",
                "abstract": "Recently, end-to-end (E2E) trained models for question answering over knowledge graphs (KGQA) have delivered promising results using only a weakly supervised dataset. However, these models are trained and evaluated in a setting where hand-annotated question entities are supplied to the model, leaving the important and non-trivial task of entity resolution (ER) outside the scope of E2E learning. In this work, we extend the boundaries of E2E learning for KGQA to include the training of an ER component. Our model only needs the question text and the answer entities to train, and delivers a stand-alone QA model that does not require an additional ER component to be supplied during runtime. Our approach is fully differentiable, thanks to its reliance on a recent method for building differentiable KGs (Cohen et al., 2020). We evaluate our E2E trained model on two public datasets and show that it comes close to baseline models that use hand-annotated entities.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2126500061",
                        "name": "Armin Oliya"
                    },
                    {
                        "authorId": "1741702",
                        "name": "Amir Saffari"
                    },
                    {
                        "authorId": "50650468",
                        "name": "Priyanka Sen"
                    },
                    {
                        "authorId": "2126514646",
                        "name": "Tom Ayoola"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026explore the effectiveness of KG by either automatically constructing the graph\nusing named entity or semantic role labeling (Qiu et al., 2019; Bosselut et al., 2019; Fang et al., 2020; Chen et al., 2019) or resorting to existing KG (Saxena et al., 2020; Zhang et al., 2020; Yasunaga et al., 2021).",
                ", 2019) or resorting to existing KG (Saxena et al., 2020; Zhang et al., 2020; Yasunaga et al., 2021)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "368ffb89988eef9898440569e5378d2cb18cddd2",
                "externalIds": {
                    "DBLP": "conf/emnlp/XuDZ0L21",
                    "ArXiv": "2109.02905",
                    "DOI": "10.18653/v1/2021.findings-emnlp.99",
                    "CorpusId": 237433880
                },
                "corpusId": 237433880,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/368ffb89988eef9898440569e5378d2cb18cddd2",
                "title": "Exploiting Reasoning Chains for Multi-hop Science Question Answering",
                "abstract": "We propose a novel Chain Guided Retriever-reader ({\\tt CGR}) framework to model the reasoning chain for multi-hop Science Question Answering. Our framework is capable of performing explainable reasoning without the need of any corpus-specific annotations, such as the ground-truth reasoning chain, or human-annotated entity mentions. Specifically, we first generate reasoning chains from a semantic graph constructed by Abstract Meaning Representation of retrieved evidence facts. A \\textit{Chain-aware loss}, concerning both local and global chain information, is also designed to enable the generated chains to serve as distant supervision signals for training the retriever, where reinforcement learning is also adopted to maximize the utility of the reasoning chains. Our framework allows the retriever to capture step-by-step clues of the entire reasoning process, which is not only shown to be effective on two challenging multi-hop Science QA tasks, namely OpenBookQA and ARC-Challenge, but also favors explainability.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2218345185",
                        "name": "Weiwen Xu"
                    },
                    {
                        "authorId": "145843537",
                        "name": "Yang Deng"
                    },
                    {
                        "authorId": "2111118042",
                        "name": "Huihui Zhang"
                    },
                    {
                        "authorId": "2053327987",
                        "name": "Deng Cai"
                    },
                    {
                        "authorId": "1380007189",
                        "name": "W. Lam"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This can be generalized into a so-called multi-hop QA task [90, 91] where the topic (question) entity is known and the question is assumed to be a paraphrase of the multi-hop KG relation (there is an assumption that \u201cnephew\u201d is not directly a KG predicate).",
                "The prevalent paradigms in KG-QA focused on building explicit queries or logical forms that could be executed over the RDF triple store [1, 13, 35, 78, 92, 102], or using approximate graph search techniques after mapping question phrases to KG items [20, 54, 91, 106].",
                "To circumvent the brittleness of SPARQL for complex intents, an alternative direction has used approximate graph search without explicit queries (Uniqorn takes this philosophy) [20, 23, 63, 106, 122], where sometimes the entire KG is cast into an embedded space for multihop reasoning [54, 90, 91], or the answer derivation workflow is cast into a sequence-to-sequence model [15, 22, 101]."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f4b924fabfbd253cac4fab37045c282d98800b9c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-08614",
                    "ArXiv": "2108.08614",
                    "CorpusId": 237213582
                },
                "corpusId": 237213582,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/f4b924fabfbd253cac4fab37045c282d98800b9c",
                "title": "UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text",
                "abstract": "Question answering over knowledge graphs and other RDF data has been greatly advanced, with a number of good techniques providing crisp answers for natural language questions or telegraphic queries. Some of these systems incorporate textual sources as additional evidence for the answering process, but cannot compute answers that are present in text alone. Conversely, techniques from the IR and NLP communities have addressed QA over text, but such systems barely utilize semantic data and knowledge. This paper presents a method for complex questions that can seamlessly operate over a mixture of RDF datasets and text corpora, or individual sources, in a unified framework. Our method, called UNIQORN, builds a context graph on-the-fly, by retrieving question-relevant evidences from the RDF data and/or a text corpus, using fine-tuned BERT models. The resulting graph typically contains all question-relevant evidences but also a lot of noise. UNIQORN copes with this input by a graph algorithm for Group Steiner Trees, that identifies the best answer candidates in the context graph. Experimental results on several benchmarks of complex questions with multiple entities and relations, show that UNIQORN significantly outperforms state-of-the-art methods for heterogeneous QA. The graph-based methodology provides user-interpretable evidence for the complete answering process.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "34015686",
                        "name": "Soumajit Pramanik"
                    },
                    {
                        "authorId": "122367036",
                        "name": "Jesujoba Oluwadara Alabi"
                    },
                    {
                        "authorId": "40075164",
                        "name": "Rishiraj Saha Roy"
                    },
                    {
                        "authorId": "1751591",
                        "name": "G. Weikum"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The explicit multi-hop reasoning methods (MINERVA and SRN) are obviously weaker than implicit methods (MemNN, KVMemNN and EmbedKGQA), because they rely more on the structure of binary facts to build reasoning chains.",
                "QA on KG with missing links Current large-scale KGs are generally incomplete, so we hope that QA systems can also handle questions despite the KG being incomplete.",
                "In this work, we aim to explore the reasoning techniques suitable for n-ary question answering over KGs (i.e., n-ary KGQA).",
                "Current KGQA techniques are mainly designed to perform multihop reasoning on KGs (Das et al., 2018; Qiu et al., 2020; Saxena et al., 2020) for its high efficiency and interpretablity.",
                "The task of Question Answering over Knowledge Graphs (KGQA) has provided new avenues to the recent development of QA systems by utilizing advantages of KGs (Dubey et al., 2019; Huang et al., 2019; Yu et al., 2017; Zhang et al., 2021).",
                "The current popular datasets with their underlying KGs contain almost no n-ary facts.",
                "EmbedKGQA relaxes the requirement that entities and relations must be connected when constructing a reasoning chain, which makes it more flexible in dealing with n-ary facts.",
                "Similar to (Saxena et al., 2020), we learn a scoring function s(e; {w1, w2, ..., wn}) which calculates the semantic similarity between entity and a word sequence:\ns(e; {w1, w2, ..., wn}) = \u03c32(e >GRU(w1,w2, ...,wn)), (3)\nwhere \u03c32(\u00b7) is the activation function, e and wi are the embeddings of the\u2026",
                "Model WP-M WP-1F WP-2F WP-3F WC-M WC-1H WC-2H WC-C PQ-M PQ-2H PQ-3H\nMemNN (Sukhbaatar et al., 2015a) 32.9 34.2 39.6 12.5 52.4 71.6 55.5 73.3 86.8 89.5 79.2 KV-MemNN (Miller et al., 2016) 24.5 15.0 40.0 16.0 76.7 87.0 87.0 78.8 85.2 91.5 79.4 EmbedKGQA (Saxena et al., 2020) 26.4 35.4 22.3 3.5 52.5 59.6 79.0 52.0 36.7 51.0 30.6 IRN-weak (Zhou et al., 2018) - - - - 78.6 83.4 92.1 83.7 85.8 91.9 83.3 MINERVA (Das et al., 2018) 10.9 20.5 0.3 0.2 89.6 87.2 93.1 82.4 73.1 75.9 71.2 SRN (Qiu et al., 2020) 13.3 24.9 0.3 0.8 96.5 98.9 97.8 87.3 89.3 96.3 89.2\nFacTree 54.4 63.1 47.0 40.1 99.5 99.9 96.3 99.9 92.8 98.4 90.8\nFacTree significantly outperforms other methods both on n-ary KGQA and general binary KGQA.",
                "Some efforts were devoted to constructing implicit reasoning chains based on KG through memory network (Chen et al., 2019; Miller et al., 2016; Sukhbaatar et al., 2015b) or KG vector space (Bordes et al., 2014; He et al., 2021; Saxena et al., 2020).",
                "Similar to (Saxena et al., 2020), we learn a scoring function s(e; {w1, w2, .",
                "One can map a natural language (NL) question onto KGs to infer the correct answer.",
                "\u202634.2 39.6 12.5 52.4 71.6 55.5 73.3 86.8 89.5 79.2 KV-MemNN (Miller et al., 2016) 24.5 15.0 40.0 16.0 76.7 87.0 87.0 78.8 85.2 91.5 79.4 EmbedKGQA (Saxena et al., 2020) 26.4 35.4 22.3 3.5 52.5 59.6 79.0 52.0 36.7 51.0 30.6 IRN-weak (Zhou et al., 2018) - - - - 78.6 83.4 92.1 83.7 85.8 91.9 83.3\u2026"
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "4511c0448206f3170d2bf6e087d05700c3326fb2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-08297",
                    "ACL": "2022.findings-acl.66",
                    "ArXiv": "2108.08297",
                    "DOI": "10.18653/v1/2022.findings-acl.66",
                    "CorpusId": 237213618
                },
                "corpusId": 237213618,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/4511c0448206f3170d2bf6e087d05700c3326fb2",
                "title": "Fact-Tree Reasoning for N-ary Question Answering over Knowledge Graphs",
                "abstract": "Current Question Answering over Knowledge Graphs (KGQA) task mainly focuses on performing answer reasoning upon KGs with binary facts. However, it neglects the n-ary facts, which contain more than two entities. In this work, we highlight a more challenging but under-explored task: n-ary KGQA, i.e., answering n-ary facts questions upon n-ary KGs. Nevertheless, the multi-hop reasoning framework popular in binary KGQA task is not directly applicable on n-ary KGQA. We propose two feasible improvements: 1) upgrade the basic reasoning unit from entity or relation to fact, and 2) upgrade the reasoning structure from chain to tree. Therefore, we propose a novel fact-tree reasoning framework, FacTree, which integrates the above two upgrades. FacTree transforms the question into a fact tree and performs iterative fact reasoning on the fact tree to infer the correct answer. Experimental results on the n-ary KGQA dataset we constructed and two binary KGQA benchmarks demonstrate the effectiveness of FacTree compared with state-of-the-art methods.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1611287648",
                        "name": "Yao Zhang"
                    },
                    {
                        "authorId": "2158012487",
                        "name": "Peiyao Li"
                    },
                    {
                        "authorId": "46822058",
                        "name": "Hongru Liang"
                    },
                    {
                        "authorId": "1774986",
                        "name": "A. Jatowt"
                    },
                    {
                        "authorId": "2149231521",
                        "name": "Zhenglu Yang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Based on hypergraph convolutional networks (HGCN) [95], they encoded the sentences in the document and fused sentence representations into sentence-linked entity representations.",
                "[44] LSTM + dynamic instruction update key-value memory network cross entropy-based key-value memory network [59] neural network stepwise graph traversal reward-based variational algorithm [91] word embeddings + dynamic instruction update stepwise graph traversal reward-based traceable reasoning path [65] LSTM + dynamic instruction update graph neural network cross entropy-based heterogeneous update [66] LSTM + dynamic instruction update graph reader and text reader cross entropy-based knowledge-aware text reader [88] LSTM + dynamic instruction update graph neural network cross entropy-based graph retrieval module [92] LSTM + dynamic instruction update key-value memory network cross entropy-based stop strategy [93] BiLSTM + co-attention graph neural network cross entropy-based hypergraph; directed HGCN [90] Roberta KB embedding based triple scoring cross entropy-based pre-trained KB embeddings [89] BiLSTM + self-attention graph neural network cross entropy-based hypergraph; HGCN [85] BiGRU + step-aware representation stepwise graph traversal reward-based reward shaping [67] LSTM + self-attention graph neural network KL divergence-based teacher-student framework",
                "[90] utilized pre-trained knowledge base embeddings to address the incomplete KB issue as shown at the right side of Figure 7.",
                "complete KB Supplement KB with extra corpus via adding the question related documents into the subgraph as nodes [65], [88] or fusing extra textual information into entity representations [66], [89]; utilize the global knowledge to answer the questions by injecting global KB embeddings [90].",
                "Inspired by above work, Han et al. [84] proposed an interpretable model based on hypergraph convolutional networks (HGCN) to predict relation paths for explanation."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "7088e2b21e88b698ae5ed38008c46fa170c6e987",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2108-06688",
                    "ArXiv": "2108.06688",
                    "DOI": "10.1109/TKDE.2022.3223858",
                    "CorpusId": 237091715
                },
                "corpusId": 237091715,
                "publicationVenue": {
                    "id": "c6840156-ee10-4d78-8832-7f8909811576",
                    "name": "IEEE Transactions on Knowledge and Data Engineering",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Knowl Data Eng"
                    ],
                    "issn": "1041-4347",
                    "url": "https://www.computer.org/web/tkde",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=69"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7088e2b21e88b698ae5ed38008c46fa170c6e987",
                "title": "Complex Knowledge Base Question Answering: A Survey",
                "abstract": "Knowledge base question answering (KBQA) aims to answer a question over a knowledge base (KB). Early studies mainly focused on answering simple questions over KBs and achieved great success. However, their performances on complex questions are still far from satisfactory. Therefore, in recent years, researchers propose a large number of novel methods, which looked into the challenges of answering complex questions. In this survey, we review recent advances in KBQA with the focus on solving complex questions, which usually contain multiple subjects, express compound relations, or involve numerical operations. In detail, we begin with introducing the complex KBQA task and relevant background. Then, we present two mainstream categories of methods for complex KBQA, namely semantic parsing-based (SP-based) methods and information retrieval-based (IR-based) methods. Specifically, we illustrate their procedures with flow designs and discuss their difference and similarity. Next, we summarize the challenges that these two categories of methods encounter when answering complex questions, and explicate advanced solutions as well as techniques used in existing work. After that, we discuss the potential impact of pre-trained language models (PLMs) on complex KBQA. To help readers catch up with SOTA methods, we also provide a comprehensive evaluation and resource about complex KBQA task. Finally, we conclude and discuss several promising directions related to complex KBQA for future research.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3458560",
                        "name": "Yunshi Lan"
                    },
                    {
                        "authorId": "51149404",
                        "name": "Gaole He"
                    },
                    {
                        "authorId": "2118240359",
                        "name": "Jinhao Jiang"
                    },
                    {
                        "authorId": "144924128",
                        "name": "Jing Jiang"
                    },
                    {
                        "authorId": "2542603",
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "authorId": "153693432",
                        "name": "Ji-rong Wen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ebc64974e9e0021984a0158b3c04b60327730a88",
                "externalIds": {
                    "MAG": "3175795662",
                    "ACL": "2021.acl-demo.39",
                    "DBLP": "conf/acl/ChenLYLLJ21",
                    "DOI": "10.18653/v1/2021.acl-demo.39",
                    "CorpusId": 237698810
                },
                "corpusId": 237698810,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/ebc64974e9e0021984a0158b3c04b60327730a88",
                "title": "ReTraCk: A Flexible and Efficient Framework for Knowledge Base Question Answering",
                "abstract": "We present Retriever-Transducer-Checker (ReTraCk), a neural semantic parsing framework for large scale knowledge base question answering (KBQA). ReTraCk is designed as a modular framework to maintain high flexibility. It includes a retriever to retrieve relevant KB items efficiently, a transducer to generate logical form with syntax correctness guarantees and a checker to improve transduction procedure. ReTraCk is ranked at top1 overall performance on the GrailQA leaderboard and obtains highly competitive performance on the typical WebQuestionsSP benchmark. Our system can interact with users timely, demonstrating the efficiency of the proposed framework.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "23548372",
                        "name": "Shuang Chen"
                    },
                    {
                        "authorId": "1409707585",
                        "name": "Qian Liu"
                    },
                    {
                        "authorId": "2139425204",
                        "name": "Zhiwei Yu"
                    },
                    {
                        "authorId": "50554693",
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "authorId": "153249455",
                        "name": "Jian-Guang Lou"
                    },
                    {
                        "authorId": "144999037",
                        "name": "F. Jiang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Advances in NLP facilitate the development of various QA systems, such as Text-Based QA [14, 16, 20, 29, 37, 51], Knowledge-Based QA [7, 8, 10, 41], and Table-Based QA [21, 35, 57, 58]."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "74b4a26bb845936a684d512be7027f15d3060fef",
                "externalIds": {
                    "ArXiv": "2107.14420",
                    "CorpusId": 258685266
                },
                "corpusId": 258685266,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/74b4a26bb845936a684d512be7027f15d3060fef",
                "title": "Talk2Data: A Natural Language Interface for Exploratory Visual Analysis via Question Decomposition",
                "abstract": "Through a natural language interface (NLI) for exploratory visual analysis, users can directly\"ask\"analytical questions about the given tabular data. This process greatly improves user experience and lowers the technical barriers of data analysis. Existing techniques focus on generating a visualization from a concrete question. However, complex questions, requiring multiple data queries and visualizations to answer, are frequently asked in data exploration and analysis, which cannot be easily solved with the existing techniques. To address this issue, in this paper, we introduce Talk2Data, a natural language interface for exploratory visual analysis that supports answering complex questions. It leverages an advanced deep-learning model to resolve complex questions into a series of simple questions that could gradually elaborate on the users' requirements. To present answers, we design a set of annotated and captioned visualizations to represent the answers in a form that supports interpretation and narration. We conducted an ablation study and a controlled user study to evaluate Talk2Data's effectiveness and usefulness.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118270393",
                        "name": "Yi Guo"
                    },
                    {
                        "authorId": "2064971223",
                        "name": "Danqing Shi"
                    },
                    {
                        "authorId": "2151671022",
                        "name": "Mingjuan Guo"
                    },
                    {
                        "authorId": "2134152070",
                        "name": "Yanqiu Wu"
                    },
                    {
                        "authorId": "2152520175",
                        "name": "Qing Chen"
                    },
                    {
                        "authorId": "2059201609",
                        "name": "Nana Cao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "36a5c7b0f67151c1fdf203dcc2e0aaa76c3f4942",
                "externalIds": {
                    "DBLP": "conf/ijcnn/YangCLC21",
                    "DOI": "10.1109/IJCNN52387.2021.9533753",
                    "CorpusId": 237597606
                },
                "corpusId": 237597606,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/36a5c7b0f67151c1fdf203dcc2e0aaa76c3f4942",
                "title": "Cost-Effective Knowledge Graph Reasoning for Complex Factoid Questions",
                "abstract": "The task of reasoning over knowledge graph for factoid questions has received significant interest from the research community of natural language processing. Performing this task inevitably faces the issues of question complexity and reasoning efficiency. In this paper, we investigate modern reasoning approaches over knowledge graph to tackle complex factoid questions of diverse reasoning schemas with attractive speedup in computational efficiency. To this end, we propose two evidence retrieval strategies to generate concise and informative evidence graph of high semantic-relevance and factual coverage to the question. Then, we adopt DELFT, a graph neural networks based framework that takes the linguistic structure representation of a question and the evidence graph as input, to predict the answer by reasoning over the evidence graph. We evaluate the performance across several baselines in terms of effectiveness and efficiency on two real-world datasets, MOOCQA and MetaQA. The results show the superiority of message passing paradigm in delivering a robust reasoner with better answer quality and significantly improved computational efficiency.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2154724617",
                        "name": "Xia Yang"
                    },
                    {
                        "authorId": "2060384276",
                        "name": "Meng-Fen Chiang"
                    },
                    {
                        "authorId": "2180866493",
                        "name": "Wang-Chien Lee"
                    },
                    {
                        "authorId": "145882798",
                        "name": "Yi Chang"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "41281d90058a92d690ad8e28be6e9ac46d7d1406",
                "externalIds": {
                    "DBLP": "conf/ijcnn/DingYML21",
                    "DOI": "10.1109/IJCNN52387.2021.9533889",
                    "CorpusId": 237598858
                },
                "corpusId": 237598858,
                "publicationVenue": {
                    "id": "f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                    "name": "IEEE International Joint Conference on Neural Network",
                    "type": "conference",
                    "alternate_names": [
                        "IJCNN",
                        "IEEE Int Jt Conf Neural Netw",
                        "Int Jt Conf Neural Netw",
                        "International Joint Conference on Neural Network"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1573"
                },
                "url": "https://www.semanticscholar.org/paper/41281d90058a92d690ad8e28be6e9ac46d7d1406",
                "title": "A Multi -Role Graph Attention Network for Knowledge Graph Alignment",
                "abstract": "Entity alignment aims to automatically align entities which are equivalent in the real world from different knowledge graphs (KGs). The task is challenged by the diversity of KGs, the multi-relational graph structures and limited seed entity pairs. Most existing GNN-based models do not distinguish the types or the directions of relations while the existing translation-based models merely rely on local semantics. To tackle the challenges, we propose a Multi-Role Graph Attention network (MRGA), which incorporates local semantics in triples with global structural information in graphs and exploits rich relational information effectively. MRGA devises a GAT-based layer which regards each entity as a combination of roles that it plays in different triples. Multifaceted entity semantics (i.e. roles), associated with different triples, are aggregated to enrich entity embeddings. An additional attention mechanism is applied to capture the correlation in relations of different KGs. The model is evaluated on three publicly available datasets. The results show the superior performance of MRGA compared with baseline methods and demonstrate the effectiveness of each component in MRGA via ablation study.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2153731834",
                        "name": "Linyi Ding"
                    },
                    {
                        "authorId": "2152017571",
                        "name": "Weijie Yuan"
                    },
                    {
                        "authorId": "47356926",
                        "name": "Kui Meng"
                    },
                    {
                        "authorId": "150112803",
                        "name": "Gongshen Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1f20378d2820fdf1c1bb09ce22f739ab77b14e82",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-07842",
                    "ArXiv": "2107.07842",
                    "CorpusId": 236034048
                },
                "corpusId": 236034048,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1f20378d2820fdf1c1bb09ce22f739ab77b14e82",
                "title": "A Survey of Knowledge Graph Embedding and Their Applications",
                "abstract": "Knowledge Graph embedding provides a versatile technique for representing knowledge. These techniques can be used in a variety of applications such as completion of knowledge graph to predict missing information, recommender systems, question answering, query expansion, etc. The information embedded in Knowledge graph though being structured is challenging to consume in a real-world application. Knowledge graph embedding enables the real-world application to consume information to improve performance. Knowledge graph embedding is an active research area. Most of the embedding methods focus on structure-based information. Recent research has extended the boundary to include text-based information and image-based information in entity embedding. Efforts have been made to enhance the representation with context information. This paper introduces growth in the field of KG embedding from simple translation-based models to enrichment-based models. This paper includes the utility of the Knowledge graph in real-world applications.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2111928031",
                        "name": "Shivani Choudhary"
                    },
                    {
                        "authorId": "2119901835",
                        "name": "Tarun Luthra"
                    },
                    {
                        "authorId": "14879123",
                        "name": "Ashima Mittal"
                    },
                    {
                        "authorId": "2115745028",
                        "name": "Rajat Singh"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Embedding-based approaches learn low-dimensional vectors for both words and KGs, which allow inferring with those vectors for finding the answers (Chen et al., 2019; Saxena et al., 2020).",
                "For example, Saxena et al. (2020) proposed a system that can find answers from half masked KG based on question and knowledge graph embeddings."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e06ea5561deeb8896e3fffc07630a8361e82d4db",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-02040",
                    "ArXiv": "2107.02040",
                    "CorpusId": 235731915
                },
                "corpusId": 235731915,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/e06ea5561deeb8896e3fffc07630a8361e82d4db",
                "title": "A Knowledge-based Approach for Answering Complex Questions in Persian",
                "abstract": "Research on open-domain question answering (QA) has a long tradition. A challenge in this domain is answering complex questions (CQA) that require complex inference methods and large amounts of knowledge. In low resource languages, such as Persian, there are not many datasets for open-domain complex questions and also the language processing toolkits are not very accurate. In this paper, we propose a knowledge-based approach for answering Persian complex questions using Farsbase; the Persian knowledge graph, exploiting PeCoQ; the newly created complex Persian question dataset. In this work, we handle multi-constraint and multi-hop questions by building their set of possible corresponding logical forms. Then Multilingual-BERT is used to select the logical form that best describes the input complex question syntactically and semantically. The answer to the question is built from the answer to the logical form, extracted from the knowledge graph. Experiments show that our approach outperforms other approaches in Persian CQA.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2049030484",
                        "name": "Romina Etezadi"
                    },
                    {
                        "authorId": "2567327",
                        "name": "M. Shamsfard"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "068eb6b3797d5807b744d9326e7ebb50769e4b4d",
                "externalIds": {
                    "DBLP": "conf/acl/PappasA20",
                    "ArXiv": "2106.08908",
                    "ACL": "2021.acl-long.301",
                    "DOI": "10.18653/v1/2021.acl-long.301",
                    "CorpusId": 235446913
                },
                "corpusId": 235446913,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/068eb6b3797d5807b744d9326e7ebb50769e4b4d",
                "title": "A Neural Model for Joint Document and Snippet Ranking in Question Answering for Large Document Collections",
                "abstract": "Question answering (QA) systems for large document collections typically use pipelines that (i) retrieve possibly relevant documents, (ii) re-rank them, (iii) rank paragraphs or other snippets of the top-ranked documents, and (iv) select spans of the top-ranked snippets as exact answers. Pipelines are conceptually simple, but errors propagate from one component to the next, without later components being able to revise earlier decisions. We present an architecture for joint document and snippet ranking, the two middle stages, which leverages the intuition that relevant documents have good snippets and good snippets come from relevant documents. The architecture is general and can be used with any neural text relevance ranker. We experiment with two main instantiations of the architecture, based on POSIT-DRMM (PDRMM) and a BERT-based ranker. Experiments on biomedical data from BIOASQ show that our joint models vastly outperform the pipelines in snippet retrieval, the main goal for QA, with fewer trainable parameters, also remaining competitive in document retrieval. Furthermore, our joint PDRMM-based model is competitive with BERT-based models, despite using orders of magnitude fewer parameters. These claims are also supported by human evaluation on two test batches of BIOASQ. To test our key findings on another dataset, we modified the Natural Questions dataset so that it can also be used for document and snippet retrieval. Our joint PDRMM-based model again outperforms the corresponding pipeline in snippet retrieval on the modified Natural Questions dataset, even though it performs worse than the pipeline in document retrieval. We make our code and the modified Natural Questions dataset publicly available.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "36753496",
                        "name": "Dimitris Pappas"
                    },
                    {
                        "authorId": "1752430",
                        "name": "Ion Androutsopoulos"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Shedding light on the type of connections between entities, KGs are powerful to work with for numerous downstream tasks such as question-answering (Bordes et al., 2014; Hao et al., 2017; Saxena et al., 2020), recommendation system (Yu et al.",
                "\u2026between entities, KGs are powerful to work with for numerous downstream tasks such as question-answering (Bordes et al., 2014; Hao et al., 2017; Saxena et al., 2020), recommendation system (Yu et al., 2014; Zhang et al., 2016; Zhou et al., 2017), information retrieval (Lao and Cohen, 2010;\u2026"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ca763f4dedeaa932ea2732ed090da3df4a148449",
                "externalIds": {
                    "ACL": "2021.findings-acl.292",
                    "ArXiv": "2106.04311",
                    "DBLP": "journals/corr/abs-2106-04311",
                    "DOI": "10.18653/v1/2021.findings-acl.292",
                    "CorpusId": 235368186
                },
                "corpusId": 235368186,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/ca763f4dedeaa932ea2732ed090da3df4a148449",
                "title": "Hyperbolic Temporal Knowledge Graph Embeddings with Relational and Time Curvatures",
                "abstract": "Knowledge Graph (KG) completion has been excessively studied with a massive number of models proposed for the Link Prediction (LP) task. The main limitation of such models is their insensitivity to time. Indeed, the temporal aspect of stored facts is often ignored. To this end, more and more works consider time as a parameter to complete KGs. In this paper, we first demonstrate that, by simply increasing the number of negative samples, the recent AttH model can achieve competitive or even better performance than the state-of-the-art on Temporal KGs (TKGs), albeit its nontemporality. We further propose Hercules, a time-aware extension of AttH model, which defines the curvature of a Riemannian manifold as the product of both relation and time. Our experiments show that both Hercules and AttH achieve competitive or new state-of-the-art performances on ICEWS04 and ICEWS05-15 datasets. Therefore, one should raise awareness when learning TKGs representations to identify whether time truly boosts performances.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "46185285",
                        "name": "S\u00e9bastien Montella"
                    },
                    {
                        "authorId": "1388702112",
                        "name": "L. Rojas-Barahona"
                    },
                    {
                        "authorId": "1686239",
                        "name": "Johannes Heinecke"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026between test/train entities and test/train question paraphrases, leading to suspiciously high performance on baseline methods even with partial KG data (Saxena et al., 2020), which suggests that models that apparently perform well are not necessarily performing the desired reasoning over the KG.",
                "Recently, however, they have also been applied to the task of KGQA where they have been shown to increase performance the settings of both of complete and incomplete KGs (Saxena et al. 2020; Sun et al. 2020).",
                "\u2026dataset, we apply approaches based on deep language models (LM) alone, such as T5 (Raffel et al., 2020), BERT (Devlin et al., 2019), and KnowBERT (Peters et al., 2019), and also hybrid LM+KG embedding approaches, such as Entities-as-Experts (Fe\u0301vry et al., 2020) and EmbedKGQA (Saxena et al., 2020).",
                "We first apply EmbedKGQA (Saxena et al., 2020) directly to the task of Temporal KGQA.",
                "The issue has been seen in the MetaQA dataset as well, where there is significant overlap between test/train entities and test/train question paraphrases, leading to suspiciously high performance on baseline methods even with partial KG data (Saxena et al., 2020), which suggests that models that apparently perform well are not necessarily performing the desired reasoning over the KG."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "47fe46e561e3b270407b7bffa176e35291f165a7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2106-01515",
                    "ACL": "2021.acl-long.520",
                    "ArXiv": "2106.01515",
                    "DOI": "10.18653/v1/2021.acl-long.520",
                    "CorpusId": 235313508
                },
                "corpusId": 235313508,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/47fe46e561e3b270407b7bffa176e35291f165a7",
                "title": "Question Answering Over Temporal Knowledge Graphs",
                "abstract": "Temporal Knowledge Graphs (Temporal KGs) extend regular Knowledge Graphs by providing temporal scopes (start and end times) on each edge in the KG. While Question Answering over KG (KGQA) has received some attention from the research community, QA over Temporal KGs (Temporal KGQA) is a relatively unexplored area. Lack of broad coverage datasets has been another factor limiting progress in this area. We address this challenge by presenting CRONQUESTIONS, the largest known Temporal KGQA dataset, clearly stratified into buckets of structural complexity. CRONQUESTIONS expands the only known previous dataset by a factor of 340x. We find that various state-of-the-art KGQA methods fall far short of the desired performance on this new dataset. In response, we also propose CRONKGQA, a transformer-based solution that exploits recent advances in Temporal KG embeddings, and achieves performance superior to all baselines, with an increase of 120% in accuracy over the next best performing method. Through extensive experiments, we give detailed insights into the workings of CRONKGQA, as well as situations where significant further improvements appear possible. In addition to the dataset, we have released our code as well.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "46961776",
                        "name": "Apoorv Saxena"
                    },
                    {
                        "authorId": "40941894",
                        "name": "Soumen Chakrabarti"
                    },
                    {
                        "authorId": "2406435",
                        "name": "Partha P. Talukdar"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The dataset contains 2190 images sampled from the ILSVRC (Russakovsky et al., 2015) and the MSCOCO (Lin et al., 2014) datasets."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1826691df2f1ff87a12f944f6ef5583c9a6ceef4",
                "externalIds": {
                    "ACL": "2021.naacl-main.153",
                    "MAG": "3172726328",
                    "DBLP": "conf/naacl/RamnathSHY21",
                    "DOI": "10.18653/V1/2021.NAACL-MAIN.153",
                    "CorpusId": 235097383
                },
                "corpusId": 235097383,
                "publicationVenue": {
                    "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
                    "name": "North American Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "North Am Chapter Assoc Comput Linguistics",
                        "NAACL"
                    ],
                    "url": "https://www.aclweb.org/portal/naacl"
                },
                "url": "https://www.semanticscholar.org/paper/1826691df2f1ff87a12f944f6ef5583c9a6ceef4",
                "title": "Worldly Wise (WoW) - Cross-Lingual Knowledge Fusion for Fact-based Visual Spoken-Question Answering",
                "abstract": "Although Question-Answering has long been of research interest, its accessibility to users through a speech interface and its support to multiple languages have not been addressed in prior studies. Towards these ends, we present a new task and a synthetically-generated dataset to do Fact-based Visual Spoken-Question Answering (FVSQA). FVSQA is based on the FVQA dataset, which requires a system to retrieve an entity from Knowledge Graphs (KGs) to answer a question about an image. In FVSQA, the question is spoken rather than typed. Three sub-tasks are proposed: (1) speech-to-text based, (2) end-to-end, without speech-to-text as an intermediate component, and (3) cross-lingual, in which the question is spoken in a language different from that in which the KG is recorded. The end-to-end and cross-lingual tasks are the first to require world knowledge from a multi-relational KG as a differentiable layer in an end-to-end spoken language understanding task, hence the proposed reference implementation is called Worldly-Wise (WoW).WoW is shown to perform end-to-end cross-lingual FVSQA at same levels of accuracy across 3 languages - English, Hindi, and Turkish.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "152893002",
                        "name": "Kiran Ramnath"
                    },
                    {
                        "authorId": "2769735",
                        "name": "Leda Sari"
                    },
                    {
                        "authorId": "1399115926",
                        "name": "M. Hasegawa-Johnson"
                    },
                    {
                        "authorId": "2101317782",
                        "name": "C. Yoo"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2020a] or leverage KB embeddings [Saxena et al., 2020]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "f655b61b0929d02fa74392fe4fc2aedc801b4f47",
                "externalIds": {
                    "ArXiv": "2105.11644",
                    "DBLP": "journals/corr/abs-2105-11644",
                    "DOI": "10.24963/ijcai.2021/611",
                    "CorpusId": 235187102
                },
                "corpusId": 235187102,
                "publicationVenue": {
                    "id": "67f7f831-711a-43c8-8785-1e09005359b5",
                    "name": "International Joint Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Artif Intell",
                        "IJCAI"
                    ],
                    "url": "http://www.ijcai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/f655b61b0929d02fa74392fe4fc2aedc801b4f47",
                "title": "A Survey on Complex Knowledge Base Question Answering: Methods, Challenges and Solutions",
                "abstract": "Knowledge base question answering (KBQA) aims to answer a question over a knowledge base (KB). Recently, a large number of studies focus on semantically or syntactically complicated questions. In this paper, we elaborately summarize the typical challenges and solutions for complex KBQA. We begin with introducing the background about the KBQA task. Next, we present the two mainstream categories of methods for complex KBQA, namely semantic parsing-based (SP-based) methods and information retrieval-based (IR-based) methods. We then review the advanced methods comprehensively from the perspective of the two categories. Specifically, we explicate their solutions to the typical challenges. Finally, we conclude and discuss some promising directions for future research.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "3458560",
                        "name": "Yunshi Lan"
                    },
                    {
                        "authorId": "51149404",
                        "name": "Gaole He"
                    },
                    {
                        "authorId": "2118240359",
                        "name": "Jinhao Jiang"
                    },
                    {
                        "authorId": "144924128",
                        "name": "Jing Jiang"
                    },
                    {
                        "authorId": "2542603",
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "authorId": "153693432",
                        "name": "Ji-rong Wen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8f894c51cae3f5e4067b41b139cf1e9ba5598a4a",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-11776",
                    "ACL": "2021.findings-acl.90",
                    "ArXiv": "2105.11776",
                    "DOI": "10.18653/v1/2021.findings-acl.90",
                    "CorpusId": 235187342
                },
                "corpusId": 235187342,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/8f894c51cae3f5e4067b41b139cf1e9ba5598a4a",
                "title": "Dynamic Semantic Graph Construction and Reasoning for Explainable Multi-hop Science Question Answering",
                "abstract": "Knowledge retrieval and reasoning are two key stages in multi-hop question answering (QA) at web scale. Existing approaches suffer from low confidence when retrieving evidence facts to fill the knowledge gap and lack transparent reasoning process. In this paper, we propose a new framework to exploit more valid facts while obtaining explainability for multi-hop QA by dynamically constructing a semantic graph and reasoning over it. We employ Abstract Meaning Representation (AMR) as semantic graph representation. Our framework contains three new ideas: (a) {\\tt AMR-SG}, an AMR-based Semantic Graph, constructed by candidate fact AMRs to uncover any hop relations among question, answer and multiple facts. (b) A novel path-based fact analytics approach exploiting {\\tt AMR-SG} to extract active facts from a large fact pool to answer questions. (c) A fact-level relation modeling leveraging graph convolution network (GCN) to guide the reasoning process. Results on two scientific multi-hop QA datasets show that we can surpass recent approaches including those using additional knowledge graphs while maintaining high explainability on OpenBookQA and achieve a new state-of-the-art result on ARC-Challenge in a computationally practicable setting.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2218345185",
                        "name": "Weiwen Xu"
                    },
                    {
                        "authorId": "2111118042",
                        "name": "Huihui Zhang"
                    },
                    {
                        "authorId": "1724421",
                        "name": "Deng Cai"
                    },
                    {
                        "authorId": "144594306",
                        "name": "Wai Lam"
                    }
                ]
            }
        },
        {
            "contexts": [
                "The massive information contained in knowledge graph further promotes the research and application of KBQA.",
                "Notations Definitions\nG The knowledge graph\nE The entity set\nR The relation set\ne An entity in E\nr A relation in R\nh The head entity in a triplet\nt The tail entity in a triplet\n(h, r , t) An atomic fact\n\u03c8(Ee, Er , Ee) The scoring function X A natural language question\nx The token in X\ndent The size of entity embedding drel The size of relation embedding demb The size of token embedding dhid The size of hidden representation \u03a6V The key embedding matrix \u03a6K The value embedding matrix TC The candidate triplet set AC The candidate answer set\nKG embedding representation to advance the KBQA task.",
                "Historically, KBQA can be divided into two mainstreams [7].",
                "The IR-basedmethods treatKBQAas information retrieval problem by modeling questions and candidate answers with ranking algorithm.",
                "It can effectively improve the downstream tasks such as KG completion [30, 31], relation extraction [32] and KBQA [33].",
                "It can effectively improve the downstream tasks such as KG completion [3], relation extraction [27], and KBQA [20].",
                "The second branch treats KBQA as information retrieval problem, namely, information retrieval\nmethod (IR-based method).",
                "Question answering over knowledge base (KBQA) is one of the important technologies of intelligent human\u2013robot inter-\nB Quanjun Yin yinquanjun@nudt.edu.cn\nXinmeng Li xml.nudt@gmail.com\nMamoun Alazab mamoun.alazab@cdu.edu.au\nQian Li liqian9510@outlook.com\nKeping Yu keping.yu@aoni.waseda.jp\n1 College of Systems Engineering, National University of Defense Technology, Changsha 410000, China\n2 College of Engineering, IT and Environment, Charles Darwin University, Darwin, Australia\n3 Global Information and Telecommunication Institute, Waseda University, Tokyo 169-0072, Japan\naction.",
                "Saxena et al. [20] leveraged knowledge graph embedding to perform multi-hop KBQA."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "3a4d44ba181b954401ff2d167f66d5c7db6611a6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-13173",
                    "ArXiv": "2104.13173",
                    "DOI": "10.1007/s40747-021-00448-0",
                    "CorpusId": 233407786
                },
                "corpusId": 233407786,
                "publicationVenue": {
                    "id": "d30c9917-b233-46f4-a644-8e5cdf6d6c5e",
                    "name": "Complex & Intelligent Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Complex  Intell Syst"
                    ],
                    "issn": "2199-4536",
                    "url": "https://link.springer.com/journal/40747",
                    "alternate_urls": [
                        "http://link.springer.com/journal/40747"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3a4d44ba181b954401ff2d167f66d5c7db6611a6",
                "title": "Question-aware memory network for multi-hop question answering in human\u2013robot interaction",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2108184365",
                        "name": "Xinmeng Li"
                    },
                    {
                        "authorId": "2474250",
                        "name": "M. Alazab"
                    },
                    {
                        "authorId": "2117128193",
                        "name": "Qian Li"
                    },
                    {
                        "authorId": "47841301",
                        "name": "K. Yu"
                    },
                    {
                        "authorId": "39660976",
                        "name": "Quanjun Yin"
                    }
                ]
            }
        },
        {
            "contexts": [
                "KGs have been introduced to a variety of applications such as information extraction[7, 15], semantic search [1, 34], dialogue system [12, 37], and question answering[16, 26], to name ar X iv :2 10 4."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7a9b1d46eded598e1a04feb9ef605a698df2ddde",
                "externalIds": {
                    "DBLP": "conf/sigir/NiuLTGDLWSHS21",
                    "ArXiv": "2104.13095",
                    "DOI": "10.1145/3404835.3462925",
                    "CorpusId": 233407536
                },
                "corpusId": 233407536,
                "publicationVenue": {
                    "id": "8dce23a9-44e0-4381-a39e-2acc1edff700",
                    "name": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                    "type": "conference",
                    "alternate_names": [
                        "International ACM SIGIR Conference on Research and Development in Information Retrieval",
                        "Int ACM SIGIR Conf Res Dev Inf Retr",
                        "SIGIR",
                        "Annu Int ACM SIGIR Conf Res Dev Inf Retr"
                    ],
                    "url": "http://www.acm.org/sigir/"
                },
                "url": "https://www.semanticscholar.org/paper/7a9b1d46eded598e1a04feb9ef605a698df2ddde",
                "title": "Relational Learning with Gated and Attentive Neighbor Aggregator for Few-Shot Knowledge Graph Completion",
                "abstract": "Aiming at expanding few-shot relations' coverage in knowledge graphs (KGs), few-shot knowledge graph completion (FKGC) has recently gained more research interests. Some existing models employ a few-shot relation's multi-hop neighbor information to enhance its semantic representation. However, noise neighbor information might be amplified when the neighborhood is excessively sparse and no neighbor is available to represent the few-shot relation. Moreover, modeling and inferring complex relations of one-to-many (1-N), many-to-one (N-1), and many-to-many (N-N) by previous knowledge graph completion approaches requires high model complexity and a large amount of training instances. Thus, inferring complex relations in the few-shot scenario is difficult for FKGC models due to limited training instances. In this paper, we propose a few-shot relational learning with global-local framework to address the above issues. At the global stage, a novel gated and attentive neighbor aggregator is built for accurately integrating the semantics of a few-shot relation's neighborhood, which helps filtering the noise neighbors even if a KG contains extremely sparse neighborhoods. For the local stage, a meta-learning based TransH (MTransH) method is designed to model complex relations and train our model in a few-shot learning fashion. Extensive experiments show that our model outperforms the state-of-the-art FKGC approaches on the frequently-used benchmark datasets NELL-One and Wiki-One. Compared with the strong baseline model MetaR, our model achieves 5-shot FKGC performance improvements of 8.0% on NELL-One and 2.8% on Wiki-One by the metric Hits@10.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2052804171",
                        "name": "Guanglin Niu"
                    },
                    {
                        "authorId": "2154900133",
                        "name": "Yang Li"
                    },
                    {
                        "authorId": "1672552269",
                        "name": "Chengguang Tang"
                    },
                    {
                        "authorId": "9706609",
                        "name": "Ruiying Geng"
                    },
                    {
                        "authorId": "2023993410",
                        "name": "Jian Dai"
                    },
                    {
                        "authorId": "2145485081",
                        "name": "Qiao Liu"
                    },
                    {
                        "authorId": "2144221991",
                        "name": "Hao Wang"
                    },
                    {
                        "authorId": "2152147863",
                        "name": "Jian Sun"
                    },
                    {
                        "authorId": "2087380523",
                        "name": "Fei Huang"
                    },
                    {
                        "authorId": "2059080424",
                        "name": "Luo Si"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a90f4f298b7cc88ad8f30702825fe735eb788e73",
                "externalIds": {
                    "ArXiv": "2104.14757",
                    "DBLP": "conf/www/WangLP21",
                    "DOI": "10.1145/3442381.3450064",
                    "CorpusId": 233476212
                },
                "corpusId": 233476212,
                "publicationVenue": {
                    "id": "e07422f9-c065-40c3-a37b-75e98dce79fe",
                    "name": "The Web Conference",
                    "type": "conference",
                    "alternate_names": [
                        "Web Conf",
                        "WWW"
                    ],
                    "url": "http://www.iw3c2.org/"
                },
                "url": "https://www.semanticscholar.org/paper/a90f4f298b7cc88ad8f30702825fe735eb788e73",
                "title": "An Adversarial Transfer Network for Knowledge Representation Learning",
                "abstract": "Knowledge representation learning has received a lot of attention in the past few years. The success of existing methods heavily relies on the quality of knowledge graphs. The entities with few triplets tend to be learned with less expressive power. Fortunately, there are many knowledge graphs constructed from various sources, the representations of which could contain much information. We propose an adversarial embedding transfer network ATransN, which transfers knowledge from one or more teacher knowledge graphs to a target one through an aligned entity set without explicit data leakage. Specifically, we add soft constraints on aligned entity pairs and neighbours to the existing knowledge representation learning methods. To handle the problem of possible distribution differences between teacher and target knowledge graphs, we introduce an adversarial adaption module. The discriminator of this module evaluates the degree of consistency between the embeddings of an aligned entity pair. The consistency score is then used as the weights of soft constraints. It is not necessary to acquire the relations and triplets in teacher knowledge graphs because we only utilize the entity representations. Knowledge graph completion results show that ATransN achieves better performance against baselines without transfer on three datasets, CN3l, WK3l, and DWY100k. The ablation study demonstrates that ATransN can bring steady and consistent improvement in different settings. The extension of combining other knowledge graph embedding algorithms and the extension with three teacher graphs display the promising generalization of the adversarial transfer network.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2109616592",
                        "name": "Huijuan Wang"
                    },
                    {
                        "authorId": "2133436637",
                        "name": "Shuangyin Li"
                    },
                    {
                        "authorId": "153751474",
                        "name": "Rong Pan"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a07a94168608322600fd3cab54df1410b96852b6",
                "externalIds": {
                    "DBLP": "conf/emnlp/DasZTGPLTPM21",
                    "ACL": "2021.emnlp-main.755",
                    "ArXiv": "2104.08762",
                    "DOI": "10.18653/v1/2021.emnlp-main.755",
                    "CorpusId": 233296655
                },
                "corpusId": 233296655,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/a07a94168608322600fd3cab54df1410b96852b6",
                "title": "Case-based Reasoning for Natural Language Queries over Knowledge Bases",
                "abstract": "It is often challenging to solve a complex problem from scratch, but much easier if we can access other similar problems with their solutions \u2014 a paradigm known as case-based reasoning (CBR). We propose a neuro-symbolic CBR approach (CBR-KBQA) for question answering over large knowledge bases. CBR-KBQA consists of a nonparametric memory that stores cases (question and logical forms) and a parametric model that can generate a logical form for a new question by retrieving cases that are relevant to it. On several KBQA datasets that contain complex questions, CBR-KBQA achieves competitive performance. For example, on the CWQ dataset, CBR-KBQA outperforms the current state of the art by 11% on accuracy. Furthermore, we show that CBR-KBQA is capable of using new cases without any further training: by incorporating a few human-labeled examples in the case memory, CBR-KBQA is able to successfully generate logical forms containing unseen KB entities as well as relations.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "143863023",
                        "name": "R. Das"
                    },
                    {
                        "authorId": "1771307",
                        "name": "M. Zaheer"
                    },
                    {
                        "authorId": "2064894364",
                        "name": "Dung Ngoc Thai"
                    },
                    {
                        "authorId": "36851489",
                        "name": "Ameya Godbole"
                    },
                    {
                        "authorId": "3439053",
                        "name": "Ethan Perez"
                    },
                    {
                        "authorId": "1520031992",
                        "name": "Jay Yoon Lee"
                    },
                    {
                        "authorId": "2087344242",
                        "name": "Lizhen Tan"
                    },
                    {
                        "authorId": "1725498",
                        "name": "L. Polymenakos"
                    },
                    {
                        "authorId": "143753639",
                        "name": "A. McCallum"
                    }
                ]
            }
        },
        {
            "contexts": [
                "the dynamic organization of functional cortical modules in different cognitive processes [52, 53]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ba20dd2ae9354b6897476545219a98942583a099",
                "externalIds": {
                    "PubMedCentral": "10011502",
                    "ArXiv": "2104.07445",
                    "DOI": "10.1038/s42003-023-04580-0",
                    "CorpusId": 233240982,
                    "PubMed": "36914748"
                },
                "corpusId": 233240982,
                "publicationVenue": {
                    "id": "069e05e7-ca35-41d0-a8c7-bdc9ec6a82af",
                    "name": "Communications Biology",
                    "alternate_names": [
                        "Commun Biology"
                    ],
                    "issn": "2399-3642",
                    "url": "http://www.nature.com/commsbio/"
                },
                "url": "https://www.semanticscholar.org/paper/ba20dd2ae9354b6897476545219a98942583a099",
                "title": "Simulations approaching data: cortical slow waves in inferred models of the whole hemisphere of mouse",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "144690222",
                        "name": "C. Capone"
                    },
                    {
                        "authorId": "46353886",
                        "name": "Chiara De Luca"
                    },
                    {
                        "authorId": "9740812",
                        "name": "G. Bonis"
                    },
                    {
                        "authorId": "2633038",
                        "name": "E. Pastorelli"
                    },
                    {
                        "authorId": "47839325",
                        "name": "A. Mascaro"
                    },
                    {
                        "authorId": "152452082",
                        "name": "F. Resta"
                    },
                    {
                        "authorId": "145791663",
                        "name": "F. Pavone"
                    },
                    {
                        "authorId": "93598375",
                        "name": "P. Paolucci"
                    }
                ]
            }
        },
        {
            "contexts": [
                "EmbedKGQA (Saxena et al., 2020) takes KGQA as a link prediction task and incorporates knowledge graph embeddings (Bordes et al.",
                "Besides, there are many works specifically for only one graph form: For the label form, which is also known as \u201cKBQA\u201d or \u201cKGQA\u201d, existing methods fall into two categories: information retrieval (Miller et al., 2016; Xu et al., 2019; Zhao et al., 2019b; Saxena et al., 2020) and semantic parsing (Berant et al.",
                "Following (Saxena et al., 2020), we pruned the knowledge base to contain only mentioned predicates and within 2-hop triples of mentioned entities.",
                "\u2026is also known as \u201cKBQA\u201d or \u201cKGQA\u201d, existing methods fall into two categories: information retrieval (Miller et al., 2016; Xu et al., 2019; Zhao et al., 2019b; Saxena et al., 2020) and semantic parsing (Berant et al., 2013; Yih et al., 2015; Liang et al., 2017; Guo et al., 2018; Saha et al., 2019).",
                "EmbedKGQA (Saxena et al., 2020) takes KGQA as a link prediction task and incorporates knowledge graph embeddings (Bordes et al., 2013; Trouillon et al., 2016) to help predict the answer."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "53cffcd498d78b68c7c22f6fec5760be4c8a368c",
                "externalIds": {
                    "DBLP": "conf/emnlp/ShiC0LZ21",
                    "ACL": "2021.emnlp-main.341",
                    "ArXiv": "2104.07302",
                    "DOI": "10.18653/v1/2021.emnlp-main.341",
                    "CorpusId": 233240823
                },
                "corpusId": 233240823,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/53cffcd498d78b68c7c22f6fec5760be4c8a368c",
                "title": "TransferNet: An Effective and Transparent Framework for Multi-hop Question Answering over Relation Graph",
                "abstract": "Multi-hop Question Answering (QA) is a challenging task because it requires precise reasoning with entity relations at every step towards the answer. The relations can be represented in terms of labels in knowledge graph (e.g., spouse) or text in text corpus (e.g., they have been married for 26 years). Existing models usually infer the answer by predicting the sequential relation path or aggregating the hidden graph features. The former is hard to optimize, and the latter lacks interpretability. In this paper, we propose TransferNet, an effective and transparent model for multi-hop QA, which supports both label and text relations in a unified framework. TransferNet jumps across entities at multiple steps. At each step, it attends to different parts of the question, computes activated scores for relations, and then transfer the previous entity scores along activated relations in a differentiable way. We carry out extensive experiments on three datasets and demonstrate that TransferNet surpasses the state-of-the-art models by a large margin. In particular, on MetaQA, it achieves 100% accuracy in 2-hop and 3-hop questions. By qualitative analysis, we show that TransferNet has transparent and interpretable intermediate results.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2048406602",
                        "name": "Jiaxin Shi"
                    },
                    {
                        "authorId": "1712738522",
                        "name": "S. Cao"
                    },
                    {
                        "authorId": "145779862",
                        "name": "Lei Hou"
                    },
                    {
                        "authorId": "8549842",
                        "name": "Juan-Zi Li"
                    },
                    {
                        "authorId": "2119078220",
                        "name": "Hanwang Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018), question answering (Song et al., 2018; Saxena et al., 2020), information extraction (Liu et al.",
                "\u2026al., 2019), semantic role labeling (Marcheggiani and Titov, 2017), machine translation (Beck\net al., 2018), question answering (Song et al., 2018; Saxena et al., 2020), information extraction (Liu et al., 2018; Vashishth et al., 2018; Nguyen and Grishman, 2018; Sahu et al., 2019; Sun et al.,\u2026"
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "53800017056d8ea5883bd63119b76256bffd7e91",
                "externalIds": {
                    "ArXiv": "2104.06230",
                    "ACL": "2021.eacl-main.318",
                    "DBLP": "conf/eacl/ZhaoDF21",
                    "DOI": "10.18653/v1/2021.eacl-main.318",
                    "CorpusId": 233189646
                },
                "corpusId": 233189646,
                "publicationVenue": {
                    "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
                    "name": "Conference of the European Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Eur Chapter Assoc Comput Linguistics",
                        "EACL"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/eacl/"
                },
                "url": "https://www.semanticscholar.org/paper/53800017056d8ea5883bd63119b76256bffd7e91",
                "title": "GLaRA: Graph-based Labeling Rule Augmentation for Weakly Supervised Named Entity Recognition",
                "abstract": "Instead of using expensive manual annotations, researchers have proposed to train named entity recognition (NER) systems using heuristic labeling rules. However, devising labeling rules is challenging because it often requires a considerable amount of manual effort and domain expertise. To alleviate this problem, we propose GLARA, a graph-based labeling rule augmentation framework, to learn new labeling rules from unlabeled data. We first create a graph with nodes representing candidate rules extracted from unlabeled data. Then, we design a new graph neural network to augment labeling rules by exploring the semantic relations between rules. We finally apply the augmented rules on unlabeled data to generate weak labels and train a NER model using the weakly labeled data. We evaluate our method on three NER datasets and find that we can achieve an average improvement of +20% F1 score over the best baseline when given a small set of seed rules.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2004328594",
                        "name": "Xinyan Zhao"
                    },
                    {
                        "authorId": "47929135",
                        "name": "Haibo Ding"
                    },
                    {
                        "authorId": "143989610",
                        "name": "Z. Feng"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "31b1968a756c7f94e1910d919e26f763ac965071",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-04676",
                    "MAG": "3169172630",
                    "ArXiv": "2104.04676",
                    "ACL": "2021.naacl-main.187",
                    "DOI": "10.18653/v1/2021.naacl-main.187",
                    "CorpusId": 233209850
                },
                "corpusId": 233209850,
                "publicationVenue": {
                    "id": "01103732-3808-4930-b8e4-7e9e68d5c68d",
                    "name": "North American Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "North Am Chapter Assoc Comput Linguistics",
                        "NAACL"
                    ],
                    "url": "https://www.aclweb.org/portal/naacl"
                },
                "url": "https://www.semanticscholar.org/paper/31b1968a756c7f94e1910d919e26f763ac965071",
                "title": "Highly Efficient Knowledge Graph Embedding Learning with Orthogonal Procrustes Analysis",
                "abstract": "Knowledge Graph Embeddings (KGEs) have been intensively explored in recent years due to their promise for a wide range of applications. However, existing studies focus on improving the final model performance without acknowledging the computational cost of the proposed approaches, in terms of execution time and environmental impact. This paper proposes a simple yet effective KGE framework which can reduce the training time and carbon footprint by orders of magnitudes compared with state-of-the-art approaches, while producing competitive performance. We highlight three technical innovations: full batch learning via relational matrices, closed-form Orthogonal Procrustes Analysis for KGEs, and non-negative-sampling training. In addition, as the first KGE method whose entity embeddings also store full relation information, our trained models encode rich semantics and are highly interpretable. Comprehensive experiments and ablation studies involving 13 strong baselines and two standard datasets verify the effectiveness and efficiency of our algorithm.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "80372986",
                        "name": "Xutan Peng"
                    },
                    {
                        "authorId": "48390820",
                        "name": "Guanyi Chen"
                    },
                    {
                        "authorId": "2268783",
                        "name": "Chenghua Lin"
                    },
                    {
                        "authorId": "144324746",
                        "name": "Mark Stevenson"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "50bdbf2e8c73f66d09e5c8c093e7189dc6cac46f",
                "externalIds": {
                    "DBLP": "journals/cin/ChenMWMZ21",
                    "PubMedCentral": "7910065",
                    "DOI": "10.1155/2021/8810366",
                    "CorpusId": 232111898,
                    "PubMed": "33679967"
                },
                "corpusId": 232111898,
                "publicationVenue": {
                    "id": "f32b7322-b69c-4e63-801d-8f50784ef778",
                    "name": "Computational Intelligence and Neuroscience",
                    "type": "journal",
                    "alternate_names": [
                        "Comput Intell Neurosci"
                    ],
                    "issn": "1687-5265",
                    "url": "https://www.hindawi.com/journals/cin/"
                },
                "url": "https://www.semanticscholar.org/paper/50bdbf2e8c73f66d09e5c8c093e7189dc6cac46f",
                "title": "MTQA: Text-Based Multitype Question and Answer Reading Comprehension Model",
                "abstract": "Text-based multitype question answering is one of the research hotspots in the field of reading comprehension models. Multitype reading comprehension models have the characteristics of shorter time to propose, complex components of relevant corpus, and greater difficulty in model construction. There are relatively few research works in this field. Therefore, it is urgent to improve the model performance. In this paper, a text-based multitype question and answer reading comprehension model (MTQA) is proposed. The model is based on a multilayer transformer encoding and decoding structure. In the decoding structure, the headers of the answer type prediction decoding, fragment decoding, arithmetic decoding, counting decoding, and negation are added for the characteristics of multiple types of corpora. Meanwhile, high-performance ELECTRA checkpoints are employed, and secondary pretraining based on these checkpoints and an absolute loss function are designed to improve the model performance. The experimental results show that the performance of the proposed model on the DROP and QUOREF corpora is better than the best results of the current existing models, which proves that the proposed MTQA model has high feature extraction and relatively strong generalization capabilities.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2051898720",
                        "name": "Deguang Chen"
                    },
                    {
                        "authorId": "2715891",
                        "name": "Ziping Ma"
                    },
                    {
                        "authorId": "2110688496",
                        "name": "Lin Wei"
                    },
                    {
                        "authorId": "3208912",
                        "name": "Jinlin Ma"
                    },
                    {
                        "authorId": "2153095455",
                        "name": "Yanbin Zhu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(4) Knowledge Utilization, including knowledge base question answering (KBQA) [39], [40], intelligent retrieval, knowledge recommendation, etc."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0e701d5040b5c40f3ac1061dd13fd95a92e9e2f0",
                "externalIds": {
                    "ArXiv": "2102.02971",
                    "DBLP": "journals/corr/abs-2102-02971",
                    "DOI": "10.1109/ACCESS.2021.3068728",
                    "CorpusId": 231839612
                },
                "corpusId": 231839612,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0e701d5040b5c40f3ac1061dd13fd95a92e9e2f0",
                "title": "Metaknowledge Extraction Based on Multi-Modal Documents",
                "abstract": "The triplet-based knowledge in large-scale knowledge bases is most likely lacking in structural logic and problematic of conducting knowledge hierarchy. In this paper, we introduce the concept of metaknowledge to knowledge engineering research for the purpose of structural knowledge construction. Therefore, the Metaknowledge Extraction Framework and Document Structure Tree model are presented to extract and organize metaknowledge elements (titles, authors, abstracts, sections, paragraphs, etc.), so that it is feasible to extract the structural knowledge from multi-modal documents. Experiment results have proved the effectiveness of metaknowledge elements extraction by our framework. Meanwhile, detailed examples are given to demonstrate what exactly metaknowledge is and how to generate it. At the end of this paper, we propose and analyze the task flow of metaknowledge applications and the associations between knowledge and metaknowledge.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1471656895",
                        "name": "Shukan Liu"
                    },
                    {
                        "authorId": "47462777",
                        "name": "Ruilin Xu"
                    },
                    {
                        "authorId": "31173582",
                        "name": "Boying Geng"
                    },
                    {
                        "authorId": "2112590599",
                        "name": "Qiao Sun"
                    },
                    {
                        "authorId": "2055790799",
                        "name": "Li Duan"
                    },
                    {
                        "authorId": "2108054070",
                        "name": "Yiming Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "34ba905d57042bafc761e18284cfa29b6aaaa6e6",
                "externalIds": {
                    "MAG": "3118872581",
                    "DBLP": "conf/wsdm/HeL0ZW21",
                    "ArXiv": "2101.03737",
                    "DOI": "10.1145/3437963.3441753",
                    "CorpusId": 231572861
                },
                "corpusId": 231572861,
                "publicationVenue": {
                    "id": "ea38228f-6ed3-4222-a3ce-d963d8cc9516",
                    "name": "Web Search and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "Web Search Data Min",
                        "WSDM"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=3158"
                },
                "url": "https://www.semanticscholar.org/paper/34ba905d57042bafc761e18284cfa29b6aaaa6e6",
                "title": "Improving Multi-hop Knowledge Base Question Answering by Learning Intermediate Supervision Signals",
                "abstract": "Multi-hop Knowledge Base Question Answering (KBQA) aims to find the answer entities that are multiple hops away in the Knowl- edge Base (KB) from the entities in the question. A major challenge is the lack of supervision signals at intermediate steps. Therefore, multi-hop KBQA algorithms can only receive the feedback from the final answer, which makes the learning unstable or ineffective. To address this challenge, we propose a novel teacher-student approach for the multi-hop KBQA task. In our approach, the stu- dent network aims to find the correct answer to the query, while the teacher network tries to learn intermediate supervision signals for improving the reasoning capacity of the student network. The major novelty lies in the design of the teacher network, where we utilize both forward and backward reasoning to enhance the learning of intermediate entity distributions. By considering bidi- rectional reasoning, the teacher network can produce more reliable intermediate supervision signals, which can alleviate the issue of spurious reasoning. Extensive experiments on three benchmark datasets have demonstrated the effectiveness of our approach on the KBQA task.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51149404",
                        "name": "Gaole He"
                    },
                    {
                        "authorId": "3458560",
                        "name": "Yunshi Lan"
                    },
                    {
                        "authorId": "144924128",
                        "name": "Jing Jiang"
                    },
                    {
                        "authorId": "2542603",
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "authorId": "153693432",
                        "name": "Ji-rong Wen"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1bf4b8e867f929de1ec53ba7cf0f9d9ede58b11a",
                "externalIds": {
                    "ArXiv": "2012.15484",
                    "DBLP": "journals/corr/abs-2012-15484",
                    "CorpusId": 229923899
                },
                "corpusId": 229923899,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1bf4b8e867f929de1ec53ba7cf0f9d9ede58b11a",
                "title": "Seeing is Knowing! Fact-based Visual Question Answering using Knowledge Graph Embeddings",
                "abstract": "Fact-based Visual Question Answering (FVQA), a challenging variant of VQA, requires a QA-system to include facts from a diverse knowledge graph (KG) in its reasoning process to produce an answer. Large KGs, especially common-sense KGs, are known to be incomplete, i.e., not all non-existent facts are always incorrect. Therefore, being able to reason over incomplete KGs for QA is a critical requirement in real-world applications that has not been addressed extensively in the literature. We develop a novel QA architecture that allows us to reason over incomplete KGs, something current FVQA state-of-the-art (SOTA) approaches lack due to their critical reliance on fact retrieval. We use KG Embeddings, a technique widely used for KG completion, for the downstream task of FVQA. We also employ a new image representation technique we call 'Image-as-Knowledge' to enable this capability, alongside a simple one-step CoAttention mechanism to attend to text and image during QA. Our FVQA architecture is faster during inference time, being O(m), as opposed to existing FVQA SOTA methods which are O(N log N), where m = number of vertices, N = number of edges = O(m^2). KG embeddings are shown to hold complementary information to word embeddings: a combination of both metrics permits performance comparable to SOTA methods in the standard answer retrieval task, and significantly better (26% absolute) in the proposed missing-edge reasoning task.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "152893002",
                        "name": "Kiran Ramnath"
                    },
                    {
                        "authorId": "1399115926",
                        "name": "M. Hasegawa-Johnson"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Hu et al. [24] proposed a multi-type multispan network, which combines a multi-type answer predictor with a multi-span extraction method to enhance the MRC performance.",
                "QUESTION answering tasks such as Visual Question Answering (VQA) [1, 2] and Machine Reading Comprehension (MRC) [3, 4] have attracted the extensive interest of researchers, due to their numerous real-world applications such as intelligent assistants.",
                "We classify MRC methods into two categories: single-hop and multi-hop reasoning.",
                "Nie et al. [3] proposed a hierarchical pipeline model that reveals the importance of semantic retrieval to give general guidelines on the system design for MRC. Tang et al. [23] proposed a path-based graph convolutional network to perform multipath reasoning.",
                "In this section, we introduce the related works of three question answering tasks including TQA, MRC and VQA due to their similarities.",
                "MRC requires a machine to answer questions accurately given a textual context [18].",
                "Yuan et al. [20] reframed current static MRC environments as interactive and partially observed environments by restricting the context which a model observes at one time and used reinforcement learning to optimize the information-seeking agent.",
                "Different from VQA and MRC, TQA uses both text and diagram inputs in the context and the question, which makes it a non-trivial task.",
                "I. INTRODUCTION\nQUESTION answering tasks such as Visual QuestionAnswering (VQA) [1, 2] and Machine Reading Comprehension (MRC) [3, 4] have attracted the extensive interest of researchers, due to their numerous real-world applications such as intelligent assistants."
            ],
            "intents": [],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "9f0f40ce14effeff4482f804f1f49c7afca5302c",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2011-12662",
                    "ArXiv": "2011.12662",
                    "MAG": "3109028199",
                    "DOI": "10.1109/TNNLS.2023.3294991",
                    "CorpusId": 227162369,
                    "PubMed": "37486840"
                },
                "corpusId": 227162369,
                "publicationVenue": {
                    "id": "79c5a18d-0295-432c-aaa5-961d73de6d88",
                    "name": "IEEE Transactions on Neural Networks and Learning Systems",
                    "alternate_names": [
                        "IEEE Trans Neural Netw Learn Syst"
                    ],
                    "issn": "2162-237X",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5962385",
                    "alternate_urls": [
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9f0f40ce14effeff4482f804f1f49c7afca5302c",
                "title": "XTQA: Span-Level Explanations of the Textbook Question Answering",
                "abstract": "Textbook question answering (TQA) is the task of correctly answering diagram or nondiagram (ND) questions given large multimodal contexts consisting of abundant essays and diagrams. In real-world scenarios, an explainable TQA system plays a key role in deepening humans' understanding of learned knowledge. However, there is no work to investigate how to provide explanations currently. To address this issue, we devise a novel architecture toward span-level eXplanations for TQA (XTQA). In this article, spans are the combinations of sentences within a paragraph. The key idea is to consider the entire textual context of a lesson as candidate evidence and then use our proposed coarse-to-fine grained explanation extracting (EE) algorithm to narrow down the evidence scope and extract the span-level explanations with varying lengths for answering different questions. The EE algorithm can also be integrated into other TQA methods to make them explainable and improve the TQA performance. Experimental results show that XTQA obtains the best overall explanation result mean intersection over union (mIoU) of 52.38% on the first 300 questions of CK12-QA test splits, demonstrating the explainability of our method (ND: 150 and diagram: 150). The results also show that XTQA achieves the best TQA performance of 36.46% and 36.95% on the aforementioned splits. We have released our code in https://github.com/dr-majie/opentqa.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "49304622",
                        "name": "Jie Ma"
                    },
                    {
                        "authorId": "145694057",
                        "name": "Jun Liu"
                    },
                    {
                        "authorId": "2109012921",
                        "name": "Junjun Li"
                    },
                    {
                        "authorId": "50320297",
                        "name": "Q. Zheng"
                    },
                    {
                        "authorId": "3393799",
                        "name": "Qingyu Yin"
                    },
                    {
                        "authorId": "1708169071",
                        "name": "Jianlong Zhou"
                    },
                    {
                        "authorId": "121240779",
                        "name": "Y. Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In addition to singlerelation questions, EmbedKGQA [105] is proposed to deal Fig.",
                "EmbedKGQA selects the entity with the highest score as the answer.",
                "KEQA and EmbedKGQA cannot handle complex logical questions, because these queries involve set of operations resulting in multiple entities at each hop.",
                "In addition to singlerelation questions, EmbedKGQA [105] is proposed to deal\nwith the multi-hop relation questions."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "1383dcbae9eacbbd04d8a87bb20c5ccfc049c498",
                "externalIds": {
                    "ArXiv": "2010.05446",
                    "MAG": "3092702758",
                    "DBLP": "journals/corr/abs-2010-05446",
                    "CorpusId": 222290673
                },
                "corpusId": 222290673,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/1383dcbae9eacbbd04d8a87bb20c5ccfc049c498",
                "title": "Neural-Symbolic Reasoning on Knowledge Graphs",
                "abstract": "Knowledge graph reasoning is the fundamental component to support machine learning applications such as information extraction, information retrieval and recommendation. Since knowledge graph can be viewed as the discrete symbolic representations of knowledge, reasoning on knowledge graphs can naturally leverage the symbolic techniques. However, symbolic reasoning is intolerant of the ambiguous and noisy data. On the contrary, the recent advances of deep learning promote neural reasoning on knowledge graphs, which is robust to the ambiguous and noisy data, but lacks interpretability compared to symbolic reasoning. Considering the advantages and disadvantages of both methodologies, recent efforts have been made on combining the two reasoning methods. In this survey, we take a thorough look at the development of the symbolic reasoning, neural reasoning and the neural-symbolic reasoning on knowledge graphs. We survey two specific reasoning tasks, knowledge graph completion and question answering on knowledge graphs, and explain them in a unified reasoning framework. We also briefly discuss the future directions for knowledge graph reasoning.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2158144101",
                        "name": "Jing Zhang"
                    },
                    {
                        "authorId": "2152688058",
                        "name": "Bo Chen"
                    },
                    {
                        "authorId": "2145402325",
                        "name": "Lingxi Zhang"
                    },
                    {
                        "authorId": "1993985136",
                        "name": "Xirui Ke"
                    },
                    {
                        "authorId": "2113455170",
                        "name": "Haipeng Ding"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Many KGs in the realworld, such as Freebase [3], YAGO [36], WordNet [28] and NELL [29], often consist of millions or billions of facts1, have been established and conducted in a wide variety of real applications such as information extraction[17, 7], semantic search [1, 2], question answering[35, 18], and dialog system [14], to name a few."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c1f29715e20980a9a4e376dbc2c9db8f90ecac00",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2010-02602",
                    "ArXiv": "2010.02602",
                    "MAG": "3092090644",
                    "CorpusId": 222142147
                },
                "corpusId": 222142147,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c1f29715e20980a9a4e376dbc2c9db8f90ecac00",
                "title": "Joint Semantics and Data-Driven Path Representation for Knowledge Graph Inference",
                "abstract": "Inference on a large-scale knowledge graph (KG) is of great importance for KG applications like question answering. The path-based reasoning models can leverage much information over paths other than pure triples in the KG, which face several challenges: all the existing path-based methods are data-driven, lacking explainability for path representation. Besides, some methods either consider only relational paths or ignore the heterogeneity between entities and relations both contained in paths, which cannot capture the rich semantics of paths well. To address the above challenges, in this work, we propose a novel joint semantics and data-driven path representation that balances explainability and generalization in the framework of KG embedding. More specifically, we inject horn rules to obtain the condensed paths by the transparent and explainable path composition procedure. The entity converter is designed to transform the entities along paths into the representations in the semantic level similar to relations for reducing the heterogeneity between entities and relations, in which the KGs both with and without type information are considered. Our proposed model is evaluated on two classes of tasks: link prediction and path query answering task. The experimental results show that it has a significant performance gain over several different state-of-the-art baselines.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2052804171",
                        "name": "Guanglin Niu"
                    },
                    {
                        "authorId": "2151287667",
                        "name": "Bo Li"
                    },
                    {
                        "authorId": "3126972",
                        "name": "Yongfei Zhang"
                    },
                    {
                        "authorId": "20587739",
                        "name": "Yongpan Sheng"
                    },
                    {
                        "authorId": "144123161",
                        "name": "C. Shi"
                    },
                    {
                        "authorId": "1756333",
                        "name": "Jingyang Li"
                    },
                    {
                        "authorId": "3290437",
                        "name": "Shiliang Pu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Such KGE methods are conceptually simple and can be applied to tasks like factoid question answering (Saxena et al., 2020) and language modeling (Peters et al., 2019).",
                "Such KGE methods are conceptually simple and can be applied to tasks like factoid question answering (Saxena et al., 2020) and lan-"
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7e7499b47fe57033768f26ef98a3b644688eb2a2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2008-12813",
                    "MAG": "3082429057",
                    "ACL": "2021.emnlp-main.812",
                    "ArXiv": "2008.12813",
                    "DOI": "10.18653/v1/2021.emnlp-main.812",
                    "CorpusId": 221376974
                },
                "corpusId": 221376974,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/7e7499b47fe57033768f26ef98a3b644688eb2a2",
                "title": "HittER: Hierarchical Transformers for Knowledge Graph Embeddings",
                "abstract": "This paper examines the challenging problem of learning representations of entities and relations in a complex multi-relational knowledge graph. We propose HittER, a Hierarchical Transformer model to jointly learn Entity-relation composition and Relational contextualization based on a source entity\u2019s neighborhood. Our proposed model consists of two different Transformer blocks: the bottom block extracts features of each entity-relation pair in the local neighborhood of the source entity and the top block aggregates the relational information from outputs of the bottom block. We further design a masked entity prediction task to balance information from the relational context and the source entity itself. Experimental results show that HittER achieves new state-of-the-art results on multiple link prediction datasets. We additionally propose a simple approach to integrate HittER into BERT and demonstrate its effectiveness on two Freebase factoid question answering datasets.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "116881002",
                        "name": "Sanxing Chen"
                    },
                    {
                        "authorId": "2108860856",
                        "name": "Xiaodong Liu"
                    },
                    {
                        "authorId": "1800422",
                        "name": "Jianfeng Gao"
                    },
                    {
                        "authorId": "49097406",
                        "name": "Jian Jiao"
                    },
                    {
                        "authorId": "2124601065",
                        "name": "Ruofei Zhang"
                    },
                    {
                        "authorId": "2114167577",
                        "name": "Yangfeng Ji"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[16] assumed that such methods had limited coverage since it was not easy to get relevant corpus."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ec684fdc2030bba6c6f0f8ed1f21de0942c61234",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2007-13069",
                    "ArXiv": "2007.13069",
                    "MAG": "3045295485",
                    "CorpusId": 220793599
                },
                "corpusId": 220793599,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/ec684fdc2030bba6c6f0f8ed1f21de0942c61234",
                "title": "A Survey on Complex Question Answering over Knowledge Base: Recent Advances and Challenges",
                "abstract": "Question Answering (QA) over Knowledge Base (KB) aims to automatically answer natural language questions via well-structured relation information between entities stored in knowledge bases. In order to make KBQA more applicable in actual scenarios, researchers have shifted their attention from simple questions to complex questions, which require more KB triples and constraint inference. In this paper, we introduce the recent advances in complex QA. Besides traditional methods relying on templates and rules, the research is categorized into a taxonomy that contains two main branches, namely Information Retrieval-based and Neural Semantic Parsing-based. After describing the methods of these branches, we analyze directions for future research and introduce the models proposed by the Alime team.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2052628846",
                        "name": "Bin Fu"
                    },
                    {
                        "authorId": "12702898",
                        "name": "Yunqi Qiu"
                    },
                    {
                        "authorId": "1672552269",
                        "name": "Chengguang Tang"
                    },
                    {
                        "authorId": "2154900133",
                        "name": "Yang Li"
                    },
                    {
                        "authorId": "46493167",
                        "name": "Haiyang Yu"
                    },
                    {
                        "authorId": null,
                        "name": "Jian Sun"
                    }
                ]
            }
        },
        {
            "contexts": [
                "EmbedKGQA (Saxena et al., 2020), a state-of-the art model on MetaQA, which incorporates knowledge embeddings to improve the reasoning performance.",
                "\u2026al., 2020), and etc.) and then executes it against the KB and obtains the final answers; 2) information\nretrieval based methods (Miller et al., 2016; Saxena et al., 2020; Schlichtkrull et al., 2018; Zhang et al., 2018; Zhou et al., 2018; Qiu et al., 2020; Shi et al., 2021), which constructs a\u2026",
                "retrieval based methods (Miller et al., 2016; Saxena et al., 2020; Schlichtkrull et al., 2018; Zhang et al., 2018; Zhou et al., 2018; Qiu et al., 2020; Shi et al., 2021), which constructs a question-specific graph extracted from the KB and ranks all the entities in the extracted graph based on their relevance to the question.",
                "KBQA models typically follow a retrieve-and-rank paradigm, by constructing a question-specific graph extracted from the KB and ranks all the entities in the graph based on their relevance to the question (Miller et al., 2016; Saxena et al., 2020; Schlichtkrull et al., 2018; Zhang et al., 2018; Zhou et al., 2018; Qiu et al., 2020); or follow a parse-then-execute paradigm, by parsing a question to a query graph (Berant et al.",
                "\u2026question-specific graph extracted from the KB and ranks all the entities in the graph based on their relevance to the question (Miller et al., 2016; Saxena et al., 2020; Schlichtkrull et al., 2018; Zhang et al., 2018; Zhou et al., 2018; Qiu et al., 2020); or follow a parse-then-execute paradigm,\u2026"
            ],
            "intents": [],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "2aa1d4350e80613feed88d5a6337e79693f7aa57",
                "externalIds": {
                    "DBLP": "conf/acl/CaoSPNX0LHZ22",
                    "ACL": "2022.acl-long.422",
                    "ArXiv": "2007.03875",
                    "DOI": "10.18653/v1/2022.acl-long.422",
                    "CorpusId": 247362971
                },
                "corpusId": 247362971,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/2aa1d4350e80613feed88d5a6337e79693f7aa57",
                "title": "KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base",
                "abstract": "Complex question answering over knowledge base (Complex KBQA) is challenging because it requires various compositional reasoning capabilities, such as multi-hop inference, attribute comparison, set operation, etc. Existing benchmarks have some shortcomings that limit the development of Complex KBQA: 1) they only provide QA pairs without explicit reasoning processes; 2) questions are poor in diversity or scale. To this end, we introduce KQA Pro, a dataset for Complex KBQA including around 120K diverse natural language questions. We introduce a compositional and interpretable programming language KoPL to represent the reasoning process of complex questions. For each question, we provide the corresponding KoPL program and SPARQL query, so that KQA Pro can serve for both KBQA and semantic parsing tasks. Experimental results show that state-of-the-art KBQA methods cannot achieve promising results on KQA Pro as on current datasets, which suggests that KQA Pro is challenging and Complex KBQA requires further research efforts. We also treat KQA Pro as a diagnostic dataset for testing multiple reasoning skills, conduct a thorough evaluation of existing models and discuss further directions for Complex KBQA. Our codes and datasets can be obtained from https://github.com/shijx12/KQAPro_Baselines.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1712738522",
                        "name": "S. Cao"
                    },
                    {
                        "authorId": "2522647",
                        "name": "Jiaxin Shi"
                    },
                    {
                        "authorId": "3470231",
                        "name": "Liangming Pan"
                    },
                    {
                        "authorId": "115361209",
                        "name": "L. Nie"
                    },
                    {
                        "authorId": "2068340987",
                        "name": "Yutong Xiang"
                    },
                    {
                        "authorId": "145779862",
                        "name": "Lei Hou"
                    },
                    {
                        "authorId": "2133353675",
                        "name": "Juanzi Li"
                    },
                    {
                        "authorId": "1631386300",
                        "name": "Bin He"
                    },
                    {
                        "authorId": "2119078220",
                        "name": "Hanwang Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "EmbedKGQA [19] was the state-of-the-art model that applies KB embeddings ComplEx [23] in KBQA."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "8c38bffc058d558e7c734032ba63942865e05ae4",
                "externalIds": {
                    "MAG": "3098406703",
                    "DBLP": "conf/nips/SunAB0C20",
                    "CorpusId": 220425307
                },
                "corpusId": 220425307,
                "publicationVenue": {
                    "id": "d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                    "name": "Neural Information Processing Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Neural Inf Process Syst",
                        "NeurIPS",
                        "NIPS"
                    ],
                    "url": "http://neurips.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/8c38bffc058d558e7c734032ba63942865e05ae4",
                "title": "Faithful Embeddings for Knowledge Base Queries",
                "abstract": "The deductive closure of an ideal knowledge base (KB) contains exactly the logical queries that the KB can answer. However, in practice KBs are both incomplete and over-specified, failing to answer some queries that have real-world answers. \\emph{Query embedding} (QE) techniques have been recently proposed where KB entities and KB queries are represented jointly in an embedding space, supporting relaxation and generalization in KB inference. However, experiments in this paper show that QE systems may disagree with deductive reasoning on answers that do not require generalization or relaxation. We address this problem with a novel QE method that is more faithful to deductive reasoning, and show that this leads to better performance on complex queries to incomplete KBs. Finally we show that inserting this new QE module into a neural question-answering system leads to substantial improvements over the state-of-the-art.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "3456820",
                        "name": "Haitian Sun"
                    },
                    {
                        "authorId": "2112031035",
                        "name": "Andrew O. Arnold"
                    },
                    {
                        "authorId": "2113816700",
                        "name": "Tania Bedrax-Weiss"
                    },
                    {
                        "authorId": "2067616758",
                        "name": "Fernando Pereira"
                    },
                    {
                        "authorId": "50056360",
                        "name": "William W. Cohen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Figure 5 shows the system architecture proposed in Saxena et al. (2020). The system generates",
                "We describe the system in Saxena et al. (2020) in detail",
                "Saxena et al. (2020) and Huang et al. (2019) are two such works with very similar architectures.",
                "Figure 5 shows the system architecture proposed in Saxena et al. (2020).",
                "We describe the system in Saxena et al. (2020) in detail below."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "f5edab308b740b0486e13a0c3f7b27db47295707",
                "externalIds": {
                    "CorpusId": 259341333
                },
                "corpusId": 259341333,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/f5edab308b740b0486e13a0c3f7b27db47295707",
                "title": "Information Extraction with focus on Multi-hop Question Answering, Intent Detection and Slot Filling : A Survey",
                "abstract": "Information extraction (IE) is a fundamental and important problem in natural language processing. It covers a vast spectrum of tasks that have been the focus of research since the early days of natural language processing. This survey paper gives an overview of two information extraction problems - multi-hop question answering and intent detection and slot filling for dialogue state tracking. We explore the use of knowledge graph assisted deep learning models for answering multi-hop questions, which involve answering questions that require multiple steps of reasoning. For intent detection and slot filling, we explore two different types of problem statements. We first look at models for the case where intent and slot filling classes are predetermined, and the user queries are complete and independent of any ongoing dialogue. Subsequently, we discuss a versatile intent detection and slot filling system for dialogue state tracking, which can adapt to different class schemas. This system allows for handling varying types of intents and slot filling classes in dialogues from different domains. Lastly, we examine the challenge of continual learning in the context of dialogue state tracking. This involves training the model sequentially on multiple tasks while mitigating the issue of catastrophic forgetting, where the model\u2019s performance on previous tasks significantly deteriorates when learning new tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "49986335",
                        "name": "A. Kulkarni"
                    },
                    {
                        "authorId": "145532184",
                        "name": "P. Bhattacharyya"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Knowledge graphs (KGs) with plentiful structured semantic information have been widely used in various NLP applications such as question answering (Saxena et al., 2020; Ren et al., 2021), recommender systems (Wang et al., 2021a, 2022b) and information extraction (Hu et al., 2021; Zong et al., 2021).",
                "Knowledge graphs (KGs) with plentiful structured semantic information have been widely used in various NLP applications such as question answering (Saxena et al., 2020; Ren et al., 2021), recommender systems (Wang et al."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "14e650cb5578be5e9f88bc2c5f22cdf15e61eee6",
                "externalIds": {
                    "ACL": "2023.acl-long.586",
                    "DBLP": "conf/acl/TangZZZ23",
                    "DOI": "10.18653/v1/2023.acl-long.586",
                    "CorpusId": 259325842
                },
                "corpusId": 259325842,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/14e650cb5578be5e9f88bc2c5f22cdf15e61eee6",
                "title": "Multilingual Knowledge Graph Completion with Language-Sensitive Multi-Graph Attention",
                "abstract": "Multilingual Knowledge Graph Completion (KGC) aims to predict missing links with multilingual knowledge graphs. However, existing approaches suffer from two main drawbacks: (a) alignment dependency: the multilingual KGC is always realized with joint entity or relation alignment, which introduces additional alignment models and increases the complexity of the whole framework; (b) training inefficiency: the trained model will only be used for the completion of one target KG, although the data from all KGs are used simultaneously. To address these drawbacks, we propose a novel multilingual KGC framework with language-sensitive multi-graph attention such that the missing links on all given KGs can be inferred by a universal knowledge completion model. Specifically, we first build a relational graph neural network by sharing the embeddings of aligned nodes to transfer language-independent knowledge. Meanwhile, a language-sensitive multi-graph attention (LSMGA) is proposed to deal with the information inconsistency among different KGs. Experimental results show that our model achieves significant improvements on the DBP-5L and E-PKG datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "1996203445",
                        "name": "Rongchuan Tang"
                    },
                    {
                        "authorId": "2118833972",
                        "name": "Yang Zhao"
                    },
                    {
                        "authorId": "2423168",
                        "name": "Chengqing Zong"
                    },
                    {
                        "authorId": "2110631853",
                        "name": "Yu Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2022 EmbedKGQA (Saxena et al., 2020) is designed with static KGs."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a6199bd4d142823c938f0b7d3c42f68c06cb7632",
                "externalIds": {
                    "ACL": "2023.acl-long.637",
                    "DBLP": "conf/acl/ChenL023",
                    "DOI": "10.18653/v1/2023.acl-long.637",
                    "CorpusId": 259370736
                },
                "corpusId": 259370736,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/a6199bd4d142823c938f0b7d3c42f68c06cb7632",
                "title": "Multi-granularity Temporal Question Answering over Knowledge Graphs",
                "abstract": "Recently, question answering over temporal knowledge graphs (i.e., TKGQA) has been introduced and investigated, in quest of reasoning about dynamic factual knowledge. To foster research on TKGQA, a few datasets have been curated (e.g., CronQuestions and Complex-CronQuestions), and various models have been proposed based on these datasets. Nevertheless, existing efforts overlook the fact that real-life applications of TKGQA also tend to be complex in temporal granularity, i.e., the questions may concern mixed temporal granularities (e.g., both day and month). To overcome the limitation, in this paper, we motivate the notion of multi-granularity temporal question answering over knowledge graphs and present a large scale dataset for multi-granularity TKGQA, namely MultiTQ. To the best of our knowledge, MultiTQis among the first of its kind, and compared with existing datasets on TKGQA, MultiTQfeatures at least two desirable aspects\u2014ample relevant facts and multiple temporal granularities. It is expected to better reflect real-world challenges, and serve as a test bed for TKGQA models. In addition, we propose a competing baseline MultiQA over MultiTQ, which is experimentally demonstrated to be effective in dealing with TKGQA. The data and code are released at https://github.com/czy1999/MultiTQ.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2117098298",
                        "name": "Ziyang Chen"
                    },
                    {
                        "authorId": "9387602",
                        "name": "Jinzhi Liao"
                    },
                    {
                        "authorId": "2116710953",
                        "name": "Xiang Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Other techniques transform queries and KG triples to an embedding space (Saxena et al., 2020; Sun et al., 2020)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "98dff9953d982ef2289d277d7dca67d4372cd1fb",
                "externalIds": {
                    "ACL": "2023.sustainlp-1.15",
                    "DOI": "10.18653/v1/2023.sustainlp-1.15",
                    "CorpusId": 259833793
                },
                "corpusId": 259833793,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/98dff9953d982ef2289d277d7dca67d4372cd1fb",
                "title": "KGQA Without Retraining",
                "abstract": "Popular models for Knowledge Graph Question Answering (KGQA), including semantic parsing and End-to-End (E2E) models, decode into a constrained space of KG relations. Although E2E models accommodate novel entities at test-time, this constraint means they cannot access novel relations, requiring expensive and time-consuming retraining whenever a new relation is added to the KG. We propose KG-Flex, a new architecture for E2E KGQA that instead decodes into a continuous embedding space of relations, which enables use of novel relations at test-time. KG-Flex is the first to support KG updates with entirely novel triples, free of retraining, while still supporting end-to-end training with simple, weak supervision of (Q, A) pairs. Our architecture saves on time, energy, and data resources for retraining, yet we retain performance on standard benchmarks. We further demonstrate zero-shot use of novel relations, achieving up to 82% of base-line hit@1 on three QA datasets. KG-Flex can also fine-tune, requiring significantly shorter time than full retraining; fine-tuning on target data for 10% of full training increases hit@1 to 89-100% of baseline.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2075746248",
                        "name": "N. Mckenna"
                    },
                    {
                        "authorId": "50650468",
                        "name": "Priyanka Sen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "In the field of natural language processing, there are several attempts to solve extractive QA problems that can increase the ability to answer and reason like humans, such as HotpotQA dataset (Yang et al., 2018; Gao et al., 2021; Saxena et al., 2020).",
                "attempts to solve extractive QA problems that can increase the ability to answer and reason like humans, such as HotpotQA dataset (Yang et al., 2018; Gao et al., 2021; Saxena et al., 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a1c9f65be8914db32245e365d225ea29166a3f66",
                "externalIds": {
                    "DBLP": "conf/rep4nlp/Lee023",
                    "ACL": "2023.repl4nlp-1.7",
                    "DOI": "10.18653/v1/2023.repl4nlp-1.7",
                    "CorpusId": 259833830
                },
                "corpusId": 259833830,
                "publicationVenue": {
                    "id": "8b169440-4c13-4cf4-b3f9-1dc7c39dc888",
                    "name": "Workshop on Representation Learning for NLP",
                    "type": "conference",
                    "alternate_names": [
                        "RepL4NLP",
                        "Workshop Represent Learn NLP"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a1c9f65be8914db32245e365d225ea29166a3f66",
                "title": "Enhancing text comprehension for Question Answering with Contrastive Learning",
                "abstract": "Although Question Answering (QA) have advanced to the human-level language skills in NLP tasks, there is still a problem: the QA model gets confused when there are similar sentences or paragraphs. Existing studies focus on enhancing the text understanding of the candidate answers to improve the overall performance of the QA models. However, since these methods focus on re-ranking queries or candidate answers, they fail to resolve the confusion when many generated answers are similar to the expected answer. To address these issues, we propose a novel contrastive learning framework called ContrastiveQA that alleviates the confusion problem in answer extraction. We propose a supervised method where we generate positive and negative samples from the candidate answers and the given answer, respectively. We thus introduce ContrastiveQA, which uses contrastive learning with sampling data to reduce incorrect answers. Experimental results on four QA benchmarks show the effectiveness of the proposed method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2108343515",
                        "name": "Seungyeon Lee"
                    },
                    {
                        "authorId": "2109503505",
                        "name": "Minho Lee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026neural semantic parsingbased methods (Yih et al., 2015; Bao et al., 2016; Luo et al., 2018), information retrieval-based methods (Sun et al., 2018; Saxena et al., 2020; Yasunaga\net al., 2021), and differentiable KG-based methods (Cohen et al., 2020; Saffari et al., 2021; Sen et al., 2021), which,\u2026"
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "678cfca72b7de72b005f71207bc8a522ea9ac62c",
                "externalIds": {
                    "ACL": "2023.matching-1.7",
                    "DOI": "10.18653/v1/2023.matching-1.7",
                    "CorpusId": 260063238
                },
                "corpusId": 260063238,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/678cfca72b7de72b005f71207bc8a522ea9ac62c",
                "title": "Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering",
                "abstract": "Large Language Models (LLMs) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training. However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive. To this end, we propose to augment the knowledge directly in the input of LLMs. Specifically, we first retrieve the relevant facts to the input question from the knowledge graph based on semantic similarities between the question and its associated facts. After that, we prepend the retrieved facts to the input question in the form of the prompt, which is then forwarded to LLMs to generate the answer. Our framework, Knowledge-Augmented language model PromptING (KAPING), requires no model training, thus completely zero-shot. We validate the performance of our KAPING framework on the knowledge graph question answering task, that aims to answer the user\u2019s question based on facts over a knowledge graph, on which ours outperforms relevant zero-shot baselines by up to 48% in average, across multiple LLMs of various sizes.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "90765684",
                        "name": "Jinheon Baek"
                    },
                    {
                        "authorId": "8129718",
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "authorId": "1741702",
                        "name": "Amir Saffari"
                    }
                ]
            }
        },
        {
            "contexts": [
                "edu/rtw/kbbrowser/ see wide application in tasks such as questionanswering (Saxena et al., 2020; Das et al., 2017), recommendation (Zhang et al.",
                "\u2026relation-discovery\n2https://yago-knowledge.org/ 3http://rtw.ml.cmu.edu/rtw/kbbrowser/\nsee wide application in tasks such as questionanswering (Saxena et al., 2020; Das et al., 2017), recommendation (Zhang et al., 2016), and natural language inference (Peters et al., 2019)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b4c5718d04097b927de308543989cd0d5c8f65ee",
                "externalIds": {
                    "ACL": "2023.matching-1.5",
                    "DOI": "10.18653/v1/2023.matching-1.5",
                    "CorpusId": 260063120
                },
                "corpusId": 260063120,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b4c5718d04097b927de308543989cd0d5c8f65ee",
                "title": "Corpus-Based Task-Specific Relation Discovery",
                "abstract": "Relation extraction is a crucial language processing task for various downstream applications, including knowledge base completion, question answering, and summarization. Traditional relation-extraction techniques, however, rely on a predefined set of relations and model the extraction as a classification task. Consequently, such closed-world extraction methods are insufficient for inducing novel relations from a corpus. Unsupervised techniques like OpenIE, which extract  triples, generate relations that are too general for practical information extraction applications. In this work, we contribute the following: 1) We motivate and introduce a new task, corpus-based task-specific relation discovery. 2) We adapt existing data sources to create Wiki-Art, a novel dataset for task-specific relation discovery. 3) We develop a novel framework for relation discovery using zero-shot entity linking, prompting, and type-specific clustering. Our approach effectively connects unstructured text spans to their shared underlying relations, bridging the data-representation gap and significantly outperforming baselines on both quantitative and qualitative metrics. Our code and data are available in our GitHub repository.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "51046730",
                        "name": "Karthik Ramanan"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Some work have been proposed to utilize auxiliary information, such as extra question-related texts (Sun et al., 2019) and pre-trained KB embeddings (Saxena et al., 2020), which unfortunately could introduce noisy and misleading facts, not to mention the extra computational cost.",
                "We compare the proposed with different methods, including the baseline model (i.e. KDReader (Xiong et al., 2019)) and state-of-the-arts (i.e. PullNet (Sun et al., 2019), 2HR-DR (Han et al., 2020), EmbedKGQA (Saxena et al., 2020), and RecHyperNet (Yadati et al., 2021)).",
                "The IR approach, on the other hand, aims to perform semantic matching between topic entities from questions and candidate answers within the KB (Xiong et al., 2019; Sun et al., 2019; Saxena et al., 2020; Yadati et al.,\n2021).",
                "The IR approach, on the other hand, aims to perform semantic matching between topic entities from questions and candidate answers within the KB (Xiong et al., 2019; Sun et al., 2019; Saxena et al., 2020; Yadati et al., 2021).",
                "Followed by the work from (Xiong et al., 2019; Saxena et al., 2020), the low-resource KB settings have been constructed by down-sampling a percentage of facts in the background KB (we randomly retain a triple with probability of 0.",
                ", 2020), EmbedKGQA (Saxena et al., 2020), and RecHyperNet (Yadati et al.",
                "The work of (Saxena et al., 2020) utilized the pre-trained KB embeddings.",
                ", 2019) and pre-trained KB embeddings (Saxena et al., 2020), which unfortunately could introduce noisy and misleading facts, not to mention the extra computational cost.",
                "Followed by the work from (Xiong et al., 2019; Saxena et al., 2020), the low-resource KB settings have been constructed by down-sampling a percentage of facts in the background KB (we randomly retain a triple with probability of 0.1, 0.3, and 0.5)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "014f3e58e68898cd76162c554edd87b0b9ab55f7",
                "externalIds": {
                    "DBLP": "conf/naacl/LiuMHYYG22",
                    "DOI": "10.18653/v1/2022.findings-naacl.82",
                    "CorpusId": 250562672
                },
                "corpusId": 250562672,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/014f3e58e68898cd76162c554edd87b0b9ab55f7",
                "title": "Seeing the wood for the trees: a contrastive regularization method for the low-resource Knowledge Base Question Answering",
                "abstract": "Given a context knowledge base (KB) and a corresponding question, the Knowledge Base Question Answering task aims to retrieve correct answer entities from this KB. Despite sophisticated retrieval algorithms, the impact of the low-resource (incomplete) KB is not fully exploited, where contributing components ( i.e . key entities and/or relations) may be absent for question answering. To effectively address this problem, we propose a contrastive regularization based method, which is motivated by the learn - by - analogy capability from human readers. Speci\ufb01cally, the proposed work includes two major modules: the knowledge extension and sMoCo module. The former aims at exploiting the latent knowledge from the context KB and generating auxiliary information in the form of question-answer pairs. The later module utilizes those additional pairs and applies the contrastive regularization to learn informative representations, that making hard positive pairs attracted and hard negative pairs separated. Empirically, we achieved the state-of-the-art performance on the WebQuestionsSP dataset and the effectiveness of proposed modules is also evaluated.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1390977737",
                        "name": "Junping Liu"
                    },
                    {
                        "authorId": "2068869610",
                        "name": "Shi Mei"
                    },
                    {
                        "authorId": "2110048745",
                        "name": "Xinrong Hu"
                    },
                    {
                        "authorId": "2152174146",
                        "name": "Xun Yao"
                    },
                    {
                        "authorId": "2109794333",
                        "name": "Jack Yang"
                    },
                    {
                        "authorId": "2118269946",
                        "name": "Yi Guo"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Following (Saxena et al., 2020), we pruned the KB to contain only mentioned relations and within 2-hop triples of mentioned entities.",
                "\u2022 EmbedKGQA (Saxena et al., 2020) utilizes the link predict ability of KG embeddings (Bordes et al.",
                "The results in Table 3 shows that GFC achieves 59.5% for Hits@1 and performs much better than EmbedKGQA (53.2%), which aims to handle the multi-hop KBQA on incomplete KG specially.",
                "As for the 1-hop questions of MetaQA, GFC achives 97.7% which surpasses TransferNet and EmbedKGQA.",
                "competitive methods on the incomplete WebQSP with half KG preprocessed by EmbedKGQA (Saxena et al., 2020).",
                "For embedding-based methods, they will get worse embeddings of entities and relations because the number of triplets for training KG embeddings becomes much less. we compare GFC with other\ncompetitive methods on the incomplete WebQSP with half KG preprocessed by EmbedKGQA (Saxena et al., 2020).",
                "The second is embedding-based methods which score the embeddings of question objectives and candidate answers (Dong et al., 2015; Miller et al., 2016; Hao et al., 2017; Saxena et al., 2020).",
                "EmbedKGQA (Saxena et al., 2020) utilizes KG embeddings to score question and condidate answers.",
                "\u2022 EmbedKGQA (Saxena et al., 2020) utilizes the link predict ability of KG embeddings (Bordes et al., 2013; Trouillon et al., 2016) to handle multi-hop reasoning questions, especially on incomplete knowledge graph."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "d7e2e65a423beb73815ed8361307a9dc5a75dacf",
                "externalIds": {
                    "ACL": "2022.emnlp-main.578",
                    "DBLP": "conf/emnlp/XieHZ22",
                    "DOI": "10.18653/v1/2022.emnlp-main.578",
                    "CorpusId": 256460981
                },
                "corpusId": 256460981,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/d7e2e65a423beb73815ed8361307a9dc5a75dacf",
                "title": "A Sequential Flow Control Framework for Multi-hop Knowledge Base Question Answering",
                "abstract": "One of the key challenges of knowledge base question answering (KBQA) is the multi-hop reasoning. Since in different hops, one attends to different parts of question, it is important to dynamically represent the question semantics for each hop. Existing methods, however, (i) infer the dynamic question representation only through coarse-grained attention mechanisms, which may bring information loss, (ii) and have not effectively modeled the sequential logic, which is crucial for the multi-hop reasoning process in KBQA.To address these issues, we propose a sequential reasoning self-attention mechanism to capture the crucial reasoning information of each single hop in a more fine-grained way. Based on Gated Recurrent Unit (GRU) which is good at modeling sequential process, we propose a simple but effective GRU-inspired Flow Control (GFC) framework to model sequential logic in the whole multi-hop process.Extensive experiments on three popular benchmark datasets have demonstrated the superior effectiveness of our model. In particular, GFC achieves new state-of-the-art Hits@1 of 76.8% on WebQSP and is also effective when KB is incomplete. Our code and data are available at https://github.com/Xie-Minghui/GFC.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "49227949",
                        "name": "Minghui Xie"
                    },
                    {
                        "authorId": "2203793813",
                        "name": "Chuzhan Hao"
                    },
                    {
                        "authorId": "2151333468",
                        "name": "Peng Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018) to an information retrieval paradigm (Wang et al., 2020b; Saxena et al., 2020; Yasunaga et al., 2021; Sun et al., 2019; Xiong et al., 2019) that can tackle multi-hop relations or complex questions.",
                "How does our proposed approaches fare against the baselines for different KGQA prop-\nhttps://github.com/malllabiisc/EmbedKGQA\nerties?",
                "B.1 Baselines\nEmbedKGQA: The EmbedKGQA model (Saxena et al., 2020) performs Knowledge Graph Completion (KGC) on an existing knowledge graph, to learn node representations.",
                "Each question is associated with a source entity, as noted in the dataset of Saxena et al. (2020).",
                "We use the pruned version of the dataset provided by Saxena et al. (2020).",
                "We experiment with three relevant KGQA retrieval techniques, namely, EmbedKGQA (Saxena et al., 2020), Rel-GCN (Wang et al.",
                "The task of KGQA has evolved from a simpleclassification setting (Mohammed et al., 2018) to an information retrieval paradigm (Wang et al., 2020b; Saxena et al., 2020; Yasunaga et al., 2021; Sun et al., 2019; Xiong et al., 2019) that can tackle multi-hop relations or complex questions.",
                "To ensure EmbedKGQA can be applied in our setting, we carried out KGC on the KG associated with the question instead of the entire Freebase KG.",
                "We experiment with three relevant KGQA retrieval techniques, namely, EmbedKGQA (Saxena et al., 2020), Rel-GCN (Wang et al., 2020a), and GlobalGraph (Wang et al., 2020b).",
                "This would facilitate prior KGQA techniques, like EmbedKGQA, that perform KGC on the individual KGs to share embeddings and perform better.",
                "We limit ourselves to k=2, similar to Saxena et al. (2020).",
                "For EmbedKGQA, we use the publicly\navailable code of Saxena et al. (2020) along with the default hyper-parameters for training.",
                "EmbedKGQA can perform arbitrary multihop reasoning, is not restricted to a specific neighbourhood, and can effectively handle incomplete links/edges."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "dd340a0e407fbff6dafe7d30fc182e6dc1766257",
                "externalIds": {
                    "DBLP": "conf/naacl/DuttBGRR22",
                    "DOI": "10.18653/v1/2022.findings-naacl.19",
                    "CorpusId": 249455355
                },
                "corpusId": 249455355,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/dd340a0e407fbff6dafe7d30fc182e6dc1766257",
                "title": "PerKGQA: Question Answering over Personalized Knowledge Graphs",
                "abstract": "Previous studies on question answering over knowledge graphs have typically operated over a single knowledge graph (KG). This KG is assumed to be known a priori and is lever-aged similarly for all users\u2019 queries during inference. However, such an assumption is not applicable to real-world settings, such as healthcare, where one needs to handle queries of new users over unseen KGs during inference. Furthermore, privacy concerns and high computational costs render it infeasible to query the single KG that has information about all users while answering a specific user\u2019s query. The above concerns motivate our question answering setting over personalized knowledge graphs (P ER KGQA) where each user has restricted access to their KG. We observe that current state-of-the-art KGQA methods that require learning prior node representations fare poorly. We propose two complementary approaches, P ATH CBR and P ATH RGCN for P ER KGQA. The former is a simple non-parametric technique that employs case-based reasoning, while the latter is a parametric approach using graph neural networks. Our proposed methods circumvent learning prior representations, can generalize to unseen KGs, and outperform strong baselines on an academic and an internal dataset by 6.5% and 10.5%.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "36662010",
                        "name": "Ritam Dutt"
                    },
                    {
                        "authorId": "47812409",
                        "name": "Kasturi Bhattacharjee"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Questions are often divided by their answer type being a single graph relation (Mohammed et al., 2018), a path with multiple hops (Saxena et al., 2020), or complex answers requiring reasoning (e.g., combining information from multiple paths; Lu et al. (2019); Mitra and Baral (2016); Asai et al.\u2026",
                ", 2021), use knowledge graph embeddings (Sharp et al., 2016; Huang et al., 2019; Saxena et al., 2020), or train neural networks on knowledge graphs (Chakraborty et al.",
                ", 2018), a path with multiple hops (Saxena et al., 2020), or complex answers requiring reasoning (e.",
                "Typically, they use manually designed templates of graph patterns to detect answers (Zheng et al., 2018; Vollmers et al., 2021), use knowledge graph embeddings (Sharp et al., 2016; Huang et al., 2019; Saxena et al., 2020), or train neural networks on knowledge graphs (Chakraborty et al., 2021)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "5f705919c2f25303d5df0178dd8bc3d1ceabe75f",
                "externalIds": {
                    "ACL": "2022.coling-1.291",
                    "DBLP": "conf/coling/0001WHBN0BHP22",
                    "CorpusId": 252819460
                },
                "corpusId": 252819460,
                "publicationVenue": {
                    "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
                    "name": "International Conference on Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Linguistics",
                        "COLING"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/coling/"
                },
                "url": "https://www.semanticscholar.org/paper/5f705919c2f25303d5df0178dd8bc3d1ceabe75f",
                "title": "CausalQA: A Benchmark for Causal Question Answering",
                "abstract": "At least 5% of questions submitted to search engines ask about cause-effect relationships in some way. To support the development of tailored approaches that can answer such questions, we construct Webis-CausalQA-22, a benchmark corpus of 1.1 million causal questions with answers. We distinguish different types of causal questions using a novel typology derived from a data-driven, manual analysis of questions from ten large question answering (QA) datasets. Using high-precision lexical rules, we extract causal questions of each type from these datasets to create our corpus. As an initial baseline, the state-of-the-art QA model UnifiedQA achieves a ROUGE-L F1 score of 0.48 on our new benchmark.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "143856141",
                        "name": "Alexander Bondarenko"
                    },
                    {
                        "authorId": "1441933299",
                        "name": "Magdalena Wolska"
                    },
                    {
                        "authorId": "2343083",
                        "name": "Stefan Heindorf"
                    },
                    {
                        "authorId": "2187455383",
                        "name": "Lukas Bl\u00fcbaum"
                    },
                    {
                        "authorId": "7327160",
                        "name": "Axel-Cyrille Ngonga Ngomo"
                    },
                    {
                        "authorId": "1405867539",
                        "name": "Benno Stein"
                    },
                    {
                        "authorId": "2319913",
                        "name": "Pavel Braslavski"
                    },
                    {
                        "authorId": "145072133",
                        "name": "Matthias Hagen"
                    },
                    {
                        "authorId": "3046200",
                        "name": "Martin Potthast"
                    }
                ]
            }
        },
        {
            "contexts": [
                "[30] directly computes knowledge embeddings on the whole retrieved KSG, which is computationally intensive.",
                "EmbedKGQA [30] directly matched pretrained entity KG embeddings with question embedding, which is computationally intensive."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "39a4419f574c6bce94e943c3da533974270ef313",
                "externalIds": {
                    "CorpusId": 248218817
                },
                "corpusId": 248218817,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/39a4419f574c6bce94e943c3da533974270ef313",
                "title": "Retrieve-then-extract Based Knowledge Graph Querying Using Graph Neural Networks",
                "abstract": "The abstract of Retrieve-then-extract Based Knowledge Graph Querying Using Graph Neural Networks will be updated here.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "116267999",
                        "name": "Hanning Gao"
                    },
                    {
                        "authorId": "2162831819",
                        "name": "Zhihua Wei"
                    }
                ]
            }
        },
        {
            "contexts": [
                "It is also039 important when adopting KG embedding methods040\nto the real-world applications (Bordes et al., 2014; 041 Zhang et al., 2016; Saxena et al., 2020).",
                "In addition, it is also important to search for an optimal HP configuration when adopting KG embedding methods to the real-world applications (Bordes et al., 2014; Zhang et al., 2016; Saxena et al., 2020)."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "836d1d1c94f0fd0713c77b86ce136fffd059dbc0",
                "externalIds": {
                    "DBLP": "conf/acl/ZhangZYL22",
                    "ACL": "2022.acl-long.194",
                    "DOI": "10.18653/v1/2022.acl-long.194",
                    "CorpusId": 248779862
                },
                "corpusId": 248779862,
                "publicationVenue": {
                    "id": "1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                    "name": "Annual Meeting of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Annu Meet Assoc Comput Linguistics",
                        "Meeting of the Association for Computational Linguistics",
                        "ACL",
                        "Meet Assoc Comput Linguistics"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/acl/"
                },
                "url": "https://www.semanticscholar.org/paper/836d1d1c94f0fd0713c77b86ce136fffd059dbc0",
                "title": "Efficient Hyper-parameter Search for Knowledge Graph Embedding",
                "abstract": "While hyper-parameters (HPs) are important for knowledge graph (KG) learning, existing methods fail to search them efficiently. To solve this problem, we first analyze the properties of different HPs and measure the transfer ability from small subgraph to the full graph. Based on the analysis, we propose an efficient two-stage search algorithm KGTuner, which efficiently explores HP configurations on small subgraph at the first stage and transfers the top-performed configurations for fine-tuning on the large full graph at the second stage. Experiments show that our method can consistently find better HPs than the baseline algorithms within the same time budget, which achieves 9.1% average relative improvement for four embedding models on the large-scale KGs in open graph benchmark. Our code is released in https://github. com/AutoML-Research/KGTuner.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48379419",
                        "name": "Yongqi Zhang"
                    },
                    {
                        "authorId": "1768392566",
                        "name": "Zhanke Zhou"
                    },
                    {
                        "authorId": "3259992",
                        "name": "Quanming Yao"
                    },
                    {
                        "authorId": "2154403289",
                        "name": "Yong Li"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "81c7cf412be996953eefd04fd29da6558bf0be75",
                "externalIds": {
                    "DBLP": "conf/acl/ZhaoH0CQHR22",
                    "ACL": "2022.findings-acl.312",
                    "DOI": "10.18653/v1/2022.findings-acl.312",
                    "CorpusId": 248780030
                },
                "corpusId": 248780030,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/81c7cf412be996953eefd04fd29da6558bf0be75",
                "title": "Implicit Relation Linking for Question Answering over Knowledge Graph",
                "abstract": "Relation linking (RL) is a vital module in knowledge-based question answering (KBQA) systems. It aims to link the relations expressed in natural language (NL) to the corresponding ones in knowledge graph (KG). Existing methods mainly rely on the textual similarities between NL and KG to build relation links. Due to the ambiguity of NL and the incompleteness of KG, many relations in NL are implicitly expressed, and may not link to a single relation in KG, which challenges the current methods. In this paper, we propose an implicit RL method called ImRL, which links relation phrases in NL to relation paths in KG. To find proper relation paths, we propose a novel path ranking model that aligns not only textual information in the word embedding space but also structural information in the KG embedding space between relation phrases in NL and relation paths in KG. Besides, we leverage a gated mechanism with attention to inject prior knowledge from external paraphrase dictionaries to address the relation phrases with vague meaning. Our experiments on two benchmark and a newly-created datasets show that ImRL significantly outperforms several state-of-the-art methods, especially for implicit RL.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109881428",
                        "name": "Yao Zhao"
                    },
                    {
                        "authorId": "50535588",
                        "name": "Jiacheng Huang"
                    },
                    {
                        "authorId": "145066190",
                        "name": "Wei Hu"
                    },
                    {
                        "authorId": "2109402489",
                        "name": "Qijin Chen"
                    },
                    {
                        "authorId": "2152242778",
                        "name": "Xiaoxia Qiu"
                    },
                    {
                        "authorId": "2064461546",
                        "name": "Chengfu Huo"
                    },
                    {
                        "authorId": "5381159",
                        "name": "Weijun Ren"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "31cf084b90272bd1f7afe21c393551a0eab0548a",
                "externalIds": {
                    "DBLP": "journals/kbs/ChenZLLK22",
                    "DOI": "10.1016/j.knosys.2022.109134",
                    "CorpusId": 249212967
                },
                "corpusId": 249212967,
                "publicationVenue": {
                    "id": "12fff95b-d469-49a0-84a5-4fd4696c3f28",
                    "name": "Knowledge-Based Systems",
                    "type": "journal",
                    "alternate_names": [
                        "Knowl Based Syst",
                        "Knowledge Based Systems",
                        "Knowledge-based Syst"
                    ],
                    "issn": "0950-7051",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/525448/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/knowledge-based-systems",
                        "http://www.sciencedirect.com/science/journal/09507051"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/31cf084b90272bd1f7afe21c393551a0eab0548a",
                "title": "Temporal knowledge graph question answering via subgraph reasoning",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2117098298",
                        "name": "Ziyang Chen"
                    },
                    {
                        "authorId": "2116710953",
                        "name": "Xiang Zhao"
                    },
                    {
                        "authorId": "9387602",
                        "name": "Jinzhi Liao"
                    },
                    {
                        "authorId": "2108484173",
                        "name": "Xinyi Li"
                    },
                    {
                        "authorId": "1713134",
                        "name": "E. Kanoulas"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "cdd3ddd1eedcd705f209852a502035d7a928a794",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-06501",
                    "DOI": "10.48550/arXiv.2208.06501",
                    "CorpusId": 251564832
                },
                "corpusId": 251564832,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/cdd3ddd1eedcd705f209852a502035d7a928a794",
                "title": "Forecasting Question Answering over Temporal Knowledge Graphs",
                "abstract": "Question answering over temporal knowledge graphs (TKGQA) has recently found increasing interest. TKGQA requires temporal reasoning techniques to extract the rele- vant information from temporal knowledge bases. The only existing TKGQA dataset, i.e., C RON Q UESTIONS , consists of temporal questions based on the facts from a \ufb01xed time period, where a temporal knowledge graph (TKG) span- ning the same period can be fully used for answer inference, allowing the TKGQA models to use even the fu- ture knowledge to answer the questions based on the past facts. In real-world scenarios, however, it is also common that given the knowledge until now, we wish the TKGQA systems to answer the questions asking about the future. As humans constantly seek plans for the future, building TKGQA systems for answering such forecasting questions is important. Nevertheless, this has still been unexplored in previous research. In this paper, we propose a novel task: forecasting question answering over temporal knowledge graphs. We also propose a large-scale TKGQA bench- mark dataset, i.e., F ORECAST TKGQ UESTIONS , for this task. It includes three types of questions, i.e., entity prediction, yes-no, and fact reasoning questions. For every forecasting question in our dataset, QA models can only have ac- cess to the TKG information before the timestamp annotated in the given question for answer inference. We \ufb01nd that the state-of-the-art TKGQA methods perform poorly on forecasting questions, and they are unable to answer yes- no questions and fact reasoning questions. To this end, we propose F ORECAST TKGQA, a TKGQA model that employs a TKG forecasting module for future inference, to answer all three types of questions. Experimental results show that F ORECAST TKGQA outperforms recent TKGQA methods on the entity prediction questions, and it also shows great effec- tiveness in answering the other two types of questions.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2046761003",
                        "name": "Zifeng Ding"
                    },
                    {
                        "authorId": "2036741466",
                        "name": "Ruoxia Qi"
                    },
                    {
                        "authorId": "46947979",
                        "name": "Zongyue Li"
                    },
                    {
                        "authorId": "2147293727",
                        "name": "Bailan He"
                    },
                    {
                        "authorId": "2181618414",
                        "name": "Jingpei Wu"
                    },
                    {
                        "authorId": "10684484",
                        "name": "Yunpu Ma"
                    },
                    {
                        "authorId": "144861196",
                        "name": "Zhao Meng"
                    },
                    {
                        "authorId": "2176472488",
                        "name": "Zhen Han"
                    },
                    {
                        "authorId": "1742501819",
                        "name": "Volker Tresp"
                    }
                ]
            }
        },
        {
            "contexts": [
                "(Saxena, Tripathi, and Talukdar 2020) infuses knowledge representation to facilitate reasoning over the knowledge base."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "04db926687fa9af8f7e9be04901f440bea135da0",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2210-01425",
                    "DOI": "10.48550/arXiv.2210.01425",
                    "CorpusId": 252693020
                },
                "corpusId": 252693020,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/04db926687fa9af8f7e9be04901f440bea135da0",
                "title": "Guiding the PLMs with Semantic Anchors as Intermediate Supervision: Towards Interpretable Semantic Parsing",
                "abstract": "The recent prevalence of pretrained language models (PLMs) has dramatically shifted the paradigm of semantic parsing, where the mapping from natural language utterances to struc- tured logical forms is now formulated as a Seq2Seq task. Despite the promising performance, previous PLM-based ap- proaches often suffer from hallucination problems due to their negligence of the structural information contained in the sentence, which essentially constitutes the key semantics of the logical forms. Furthermore, most works treat PLM as a black box in which the generation process of the target logical form is hidden beneath the decoder modules, which greatly hinders the model\u2019s intrinsic interpretability. To address these two issues, we propose to incorporate the cur- rent PLMs with a hierarchical decoder network. By taking the \ufb01rst-principle structures as the semantic anchors, we propose two novel intermediate supervision tasks, namely Semantic Anchor Extraction and Semantic Anchor Alignment , for training the hierarchical decoders and probing the model inter- mediate representations in a self-adaptive manner alongside the \ufb01ne-tuning process. We conduct intensive experiments on several semantic parsing benchmarks and demonstrate that our approach can consistently outperform the baselines. More importantly, by analyzing the intermediate representations of the hierarchical decoders, our approach also makes a huge step toward the intrinsic interpretability of PLMs in the do-main of semantic parsing.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "115361209",
                        "name": "L. Nie"
                    },
                    {
                        "authorId": "2175443520",
                        "name": "Jiu Sun"
                    },
                    {
                        "authorId": "2108975906",
                        "name": "Yanlin Wang"
                    },
                    {
                        "authorId": "12723949",
                        "name": "Lun Du"
                    },
                    {
                        "authorId": "2109750123",
                        "name": "Shi Han"
                    },
                    {
                        "authorId": "2140415600",
                        "name": "Dongmei Zhang"
                    },
                    {
                        "authorId": "2055765060",
                        "name": "Lei Hou"
                    },
                    {
                        "authorId": "2133353675",
                        "name": "Juanzi Li"
                    },
                    {
                        "authorId": "2467444",
                        "name": "Jidong Zhai"
                    }
                ]
            }
        },
        {
            "contexts": [
                "For complex questions, recent IRbased methods turn their attention to graph retrieval (Sun et al., 2019; Saxena et al., 2020) and multihop reasoning over graphs (Zhou et al., 2018; He et al., 2021; Shi et al., 2021).",
                "For complex questions, recent IRbased methods turn their attention to graph retrieval (Sun et al., 2019; Saxena et al., 2020) and multihop reasoning over graphs (Zhou et al."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9ec3166f4a9f00c37ce080c4dc1fff04b719dccf",
                "externalIds": {
                    "DBLP": "conf/coling/HuWSQ22",
                    "ACL": "2022.coling-1.145",
                    "CorpusId": 252819147
                },
                "corpusId": 252819147,
                "publicationVenue": {
                    "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
                    "name": "International Conference on Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Linguistics",
                        "COLING"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/coling/"
                },
                "url": "https://www.semanticscholar.org/paper/9ec3166f4a9f00c37ce080c4dc1fff04b719dccf",
                "title": "Logical Form Generation via Multi-task Learning for Complex Question Answering over Knowledge Bases",
                "abstract": "Question answering over knowledge bases (KBQA) for complex questions is a challenging task in natural language processing. Recently, generation-based methods that translate natural language questions to executable logical forms have achieved promising performance. These methods use auxiliary information to augment the logical form generation of questions with unseen KB items or novel combinations, but the noise introduced can also leads to more incorrect results. In this work, we propose GMT-KBQA, a Generation-based KBQA method via Multi-Task learning, to better retrieve and utilize auxiliary information. GMT-KBQA first obtains candidate entities and relations through dense retrieval, and then introduces a multi-task model which jointly learns entity disambiguation, relation classification, and logical form generation. Experimental results show that GMT-KBQA achieves state-of-the-art results on both ComplexWebQuestions and WebQuestionsSP datasets. Furthermore, the detailed evaluation demonstrates that GMT-KBQA benefits from the auxiliary tasks and has a strong generalization capability.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1576053308",
                        "name": "Xixin Hu"
                    },
                    {
                        "authorId": "48661434",
                        "name": "X. Wu"
                    },
                    {
                        "authorId": "1406331721",
                        "name": "Yiheng Shu"
                    },
                    {
                        "authorId": "1887019",
                        "name": "Yuzhong Qu"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2017), knowledge-based question answering (Saxena et al., 2020) and aspect-level sentiment classification (Chen et al.",
                "Nowadays, GNNs are extensively applied in the area of natural language processing, ranging from syntax-based machine translation (Bastings et al., 2017), knowledge-based question answering (Saxena et al., 2020) and aspect-level sentiment classification (Chen et al., 2020b)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "90ea5b98c4f766cfa89e771a359b72b6d326eea2",
                "externalIds": {
                    "DBLP": "conf/coling/ChenLW0ZZH22",
                    "ACL": "2022.coling-1.242",
                    "CorpusId": 252819108
                },
                "corpusId": 252819108,
                "publicationVenue": {
                    "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
                    "name": "International Conference on Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Linguistics",
                        "COLING"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/coling/"
                },
                "url": "https://www.semanticscholar.org/paper/90ea5b98c4f766cfa89e771a359b72b6d326eea2",
                "title": "A Progressive Framework for Role-Aware Rumor Resolution",
                "abstract": "Existing works on rumor resolution have shown great potential in recognizing word appearance and user participation. However, they ignore the intrinsic propagation mechanisms of rumors and present poor adaptive ability when unprecedented news emerges. To exploit the fine-grained rumor diffusion patterns and generalize rumor resolution methods, we formulate a predecessor task to identify triggering posts, and then exploit their characteristics to facilitate rumor verification. We design a tree-structured annotation interface and extend PHEME dataset with labels on the message level. Data analysis shows that triggers play a critical role in verifying rumors and present similar lingual patterns across irrelevant events. We propose a graph-based model considering the direction and interaction of information flow to implement role-aware rumor resolution. Experimental results demonstrate the effectiveness of our proposed model and progressive scheme.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1845611487",
                        "name": "L. Chen"
                    },
                    {
                        "authorId": "2108255930",
                        "name": "Guanying Li"
                    },
                    {
                        "authorId": "2118602528",
                        "name": "Zhongyu Wei"
                    },
                    {
                        "authorId": "2152917872",
                        "name": "Yang Yang"
                    },
                    {
                        "authorId": "2187571166",
                        "name": "Baohua Zhou"
                    },
                    {
                        "authorId": "49346854",
                        "name": "Qi Zhang"
                    },
                    {
                        "authorId": "1790227",
                        "name": "Xuanjing Huang"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2019b), have been successfully used in multi-hop reasoning problems including multi-hop question-answering (QA) tasks (Weber et al., 2019; Richardson and Sabharwal, 2020; Saxena et al., 2020; Saha et al., 2021) and multihop reading comprehension (RC) (Min et al.",
                "\u2026et al., 2019b), have been successfully used in multi-hop reasoning problems including multi-hop question-answering (QA) tasks (Weber et al., 2019; Richardson and Sabharwal, 2020; Saxena et al., 2020; Saha et al., 2021) and multihop reading comprehension (RC) (Min et al., 2019; Ding et al., 2019)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fed460648303afa32247e493847e4dc73dc1a5b3",
                "externalIds": {
                    "ACL": "2022.coling-1.125",
                    "DBLP": "conf/coling/Lovon-Melgarejo22",
                    "CorpusId": 252819340
                },
                "corpusId": 252819340,
                "publicationVenue": {
                    "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
                    "name": "International Conference on Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Linguistics",
                        "COLING"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/coling/"
                },
                "url": "https://www.semanticscholar.org/paper/fed460648303afa32247e493847e4dc73dc1a5b3",
                "title": "Can We Guide a Multi-Hop Reasoning Language Model to Incrementally Learn at Each Single-Hop?",
                "abstract": "Despite the success of state-of-the-art pre-trained language models (PLMs) on a series of multi-hop reasoning tasks, they still suffer from their limited abilities to transfer learning from simple to complex tasks and vice-versa. We argue that one step forward to overcome this limitation is to better understand the behavioral trend of PLMs at each hop over the inference chain. Our critical underlying idea is to mimic human-style reasoning: we envision the multi-hop reasoning process as a sequence of explicit single-hop reasoning steps. To endow PLMs with incremental reasoning skills, we propose a set of inference strategies on relevant facts and distractors allowing us to build automatically generated training datasets. Using the SHINRA and ConceptNet resources jointly, we empirically show the effectiveness of our proposal on multiple-choice question answering and reading comprehension, with a relative improvement in terms of accuracy of 68.4% and 16.0% w.r.t. classic PLMs, respectively.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1403825975",
                        "name": "Jes\u00fas Lov\u00f3n-Melgarejo"
                    },
                    {
                        "authorId": "153230523",
                        "name": "Jos\u00e9 G. Moreno"
                    },
                    {
                        "authorId": "2171692548",
                        "name": "Romaric Besan\u00e7on"
                    },
                    {
                        "authorId": "1679133",
                        "name": "Olivier Ferret"
                    },
                    {
                        "authorId": "1737705",
                        "name": "L. Tamine"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Answer selection has received remarkable attention in various tasks, such as dialogue systems (Yuan et al., 2019; He et al., 2022b,a), knowledge base question answering (Niu et al., 2021; Saxena et al., 2020), and information retrieval (Li et al., 2021).",
                ", 2022b,a), knowledge base question answering (Niu et al., 2021; Saxena et al., 2020), and information retrieval (Li et al."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "42b7d395e42eb65ff1b517f6ebc6452b676ebe8b",
                "externalIds": {
                    "DBLP": "conf/coling/ZhongYX22",
                    "ACL": "2022.coling-1.151",
                    "CorpusId": 252819478
                },
                "corpusId": 252819478,
                "publicationVenue": {
                    "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
                    "name": "International Conference on Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Linguistics",
                        "COLING"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/coling/"
                },
                "url": "https://www.semanticscholar.org/paper/42b7d395e42eb65ff1b517f6ebc6452b676ebe8b",
                "title": "Reducing Spurious Correlations for Answer Selection by Feature Decorrelation and Language Debiasing",
                "abstract": "Deep neural models have become the mainstream in answer selection, yielding state-of-the-art performance. However, these models tend to rely on spurious correlations between prediction labels and input features, which in general suffer from robustness and generalization. In this paper, we propose a novel Spurious Correlation reduction method to improve the robustness of the neural ANswer selection models (SCAN) from the sample and feature perspectives by removing the feature dependencies and language biases in answer selection. First, from the sample perspective, we propose a feature decorrelation module by learning a weight for each instance at the training phase to remove the feature dependencies and reduce the spurious correlations without prior knowledge of such correlations. Second, from the feature perspective, we propose a feature debiasing module with contrastive learning to alleviate the negative language biases (spurious correlations) and further improve the robustness of the AS models. Experimental results on three benchmark datasets show that SCAN achieves substantial improvements over strong baselines. For reproducibility, we will release our code and data upon the publication of this paper.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2187456415",
                        "name": "Zeyi Zhong"
                    },
                    {
                        "authorId": "2144399430",
                        "name": "Min Yang"
                    },
                    {
                        "authorId": "1753529",
                        "name": "Ruifeng Xu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "KGs have been introduced into various downstream tasks of NLP, such as question answering (Saxena et al., 2020), dialogue systems (He et al.",
                "KGs have been introduced into various downstream tasks of NLP, such as question answering (Saxena et al., 2020), dialogue systems (He et al., 2017) and information extraction (Hoffmann et al., 2011), etc."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "70e57c5c6ad4e7814c6944f95d2aabf6833beeb2",
                "externalIds": {
                    "DBLP": "conf/coling/Yao0X022",
                    "ACL": "2022.coling-1.220",
                    "CorpusId": 252819516
                },
                "corpusId": 252819516,
                "publicationVenue": {
                    "id": "f51ff783-cdff-4e22-94fb-28e6336d17b3",
                    "name": "International Conference on Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Comput Linguistics",
                        "COLING"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/coling/"
                },
                "url": "https://www.semanticscholar.org/paper/70e57c5c6ad4e7814c6944f95d2aabf6833beeb2",
                "title": "Data Augmentation for Few-Shot Knowledge Graph Completion from Hierarchical Perspective",
                "abstract": "Few-shot knowledge graph completion (FKGC) has become a new research focus in the field of knowledge graphs in recent years, which aims to predict the missing links for relations that only have a few associative triples. Existing models attempt to solve the problem via learning entity and relation representations. However, the limited training data severely hinders the performance of existing models. To this end, we propose to solve the FKGC problem with the data augmentation technique. Specifically, we perform data augmentation from two perspectives, i.e., inter-task view and intra-task view. The former generates new tasks for FKGC, while the latter enriches the support or query set for an individual task. It is worth noting that the proposed framework can be applied to a number of existing FKGC models. Experimental evaluation on two public datasets indicates our model is capable of achieving substantial improvements over baselines.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144779803",
                        "name": "Y. Yao"
                    },
                    {
                        "authorId": "2005643069",
                        "name": "Zhao Zhang"
                    },
                    {
                        "authorId": "66607874",
                        "name": "Yongjun Xu"
                    },
                    {
                        "authorId": "2150357919",
                        "name": "Chao Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Knowledge Graphs (KGs) [1,3] organize and manage knowledge as structured information in the form of fact triples, which are crucial in various downstream tasks [14,33]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0ace399ede3cc37e26b708678ba5b757f3a8e49e",
                "externalIds": {
                    "DBLP": "conf/semweb/LiZZR22",
                    "DOI": "10.1007/978-3-031-19433-7_15",
                    "CorpusId": 253021913
                },
                "corpusId": 253021913,
                "publicationVenue": {
                    "id": "efa3ff7a-4d96-44a1-a022-a683408919b6",
                    "name": "International Workshop on the Semantic Web",
                    "type": "conference",
                    "alternate_names": [
                        "SemWeb",
                        "Int Workshop Semantic Web"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0ace399ede3cc37e26b708678ba5b757f3a8e49e",
                "title": "Each Snapshot to Each Space: Space Adaptation for Temporal Knowledge Graph Completion",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2154568544",
                        "name": "Yancong Li"
                    },
                    {
                        "authorId": "2115308107",
                        "name": "Xiaoming Zhang"
                    },
                    {
                        "authorId": "2156622226",
                        "name": "Bo Zhang"
                    },
                    {
                        "authorId": "2114292858",
                        "name": "Haiying Ren"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Recent work has implemented PLMs with multiple input types, e.g., audio (Nagrani et al., 2020), video (Sun et al., 2019), table (Saxena et al., 2020) and knowledge graph (Marino et al., 2021).",
                ", 2019), table (Saxena et al., 2020) and knowledge graph (Marino et al."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ad4426401f7d86575edba734260d11e7dc4198e5",
                "externalIds": {
                    "DBLP": "conf/ijcnlp/QiDZLWL22",
                    "CorpusId": 253840067
                },
                "corpusId": 253840067,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ad4426401f7d86575edba734260d11e7dc4198e5",
                "title": "TaKG: A New Dataset for Paragraph-level Table-to-Text Generation Enhanced with Knowledge Graphs",
                "abstract": "Table-to-text generation refers to a task that generates text using information provided by a given fact table. We introduce TaKG, a new table-to-text generation dataset with the following highlights: (1) TaKG defines a long-text (paragraph-level) generation task as opposed to well-established short-text (sentence-level) generation datasets. (2) TaKG is the first large-scale dataset for this task, containing three application domains and \u223c 750,000 samples. (3) To address the divergence phenomenon, TaKG enhances table input using external knowledge graphs, extracted by a new Wikidata-based method. We then propose a new Transformer-based multimodal sequence-to-sequence architecture for TaKG that integrates two pretrained language models RoBERTa and GPT-2. Our model shows reliable performance on long-text generation across a variety of metrics, and outperforms existing models for short-text generation tasks.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2089733563",
                        "name": "Qianqian Qi"
                    },
                    {
                        "authorId": "2244794",
                        "name": "Zhenyun Deng"
                    },
                    {
                        "authorId": "2116513959",
                        "name": "Yonghua Zhu"
                    },
                    {
                        "authorId": "2166934258",
                        "name": "Lia Jisoo Lee"
                    },
                    {
                        "authorId": "2819135",
                        "name": "M. Witbrock"
                    },
                    {
                        "authorId": "1688665",
                        "name": "J. Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2008), and Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014) have been successfully applied to various knowledgedriven applications, such as question answering (Saxena et al., 2020), semantic search (Xiong et al.",
                "\u2026al., 2007), Freebase (Bollacker et al., 2008), and Wikidata (Vrandec\u030cic\u0301 and Kr\u00f6tzsch, 2014) have been successfully applied to various knowledgedriven applications, such as question answering (Saxena et al., 2020), semantic search (Xiong et al., 2017), and information retrieval (Liu et al., 2018)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b9b2d5e384dfbbe64b0ad00469fbb89426c08ae5",
                "externalIds": {
                    "DBLP": "conf/emnlp/LiYH022",
                    "ACL": "2022.emnlp-main.524",
                    "DOI": "10.18653/v1/2022.emnlp-main.524",
                    "CorpusId": 256460959
                },
                "corpusId": 256460959,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/b9b2d5e384dfbbe64b0ad00469fbb89426c08ae5",
                "title": "Learning Inter-Entity-Interaction for Few-Shot Knowledge Graph Completion",
                "abstract": "Few-shot knowledge graph completion (FKGC) aims to infer unknown fact triples of a relation using its few-shot reference entity pairs. Recent FKGC studies focus on learning semantic representations of entity pairs by separately encoding the neighborhoods of head and tail entities. Such practice, however, ignores the inter-entity interaction, resulting in low-discrimination representations for entity pairs, especially when these entity pairs are associated with 1-to-N, N-to-1, and N-to-N relations. To address this issue, this paper proposes a novel FKGC model, named Cross-Interaction Attention Network (CIAN) to investigate the inter-entity interaction between head and tail entities. Specifically, we first explore the interactions within entities by computing the attention between the task relation and each entity neighbor, and then model the interactions between head and tail entities by letting an entity to attend to the neighborhood of its paired entity. In this way, CIAN can figure out the relevant semantics between head and tail entities, thereby generating more discriminative representations for entity pairs. Extensive experiments on two public datasets show that CIAN outperforms several state-of-the-art methods. The source code is available at https://github.com/cjlyl/FKGC-CIAN.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2111353835",
                        "name": "Yuling Li"
                    },
                    {
                        "authorId": "2088157995",
                        "name": "Kui Yu"
                    },
                    {
                        "authorId": "2124487985",
                        "name": "Xiaoling Huang"
                    },
                    {
                        "authorId": "2145061788",
                        "name": "Yuhong Zhang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Then, these methods traverse the KG according to the computation graph to identify the answer set (Lin et al., 2018; Guo et al., 2018; Saxena et al., 2020; Sun et al., 2019)."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7b5d2be2abc0962f7feaa9fdb6ddaa6206b9e6a9",
                "externalIds": {
                    "DBLP": "conf/emnlp/LongZAWL22",
                    "ACL": "2022.emnlp-main.194",
                    "DOI": "10.18653/v1/2022.emnlp-main.194",
                    "CorpusId": 256461184
                },
                "corpusId": 256461184,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/7b5d2be2abc0962f7feaa9fdb6ddaa6206b9e6a9",
                "title": "Neural-based Mixture Probabilistic Query Embedding for Answering FOL queries on Knowledge Graphs",
                "abstract": "Query embedding (QE)\u2014which aims to embed entities and first-order logical (FOL) queries in a vector space, has shown great power in answering FOL queries on knowledge graphs (KGs). Existing QE methods divide a complex query into a sequence of mini-queries according to its computation graph and perform logical operations on the answer sets of mini-queries to get answers. However, most of them assume that answer sets satisfy an individual distribution (e.g., Uniform, Beta, or Gaussian), which is often violated in real applications and limit their performance. In this paper, we propose a Neural-based Mixture Probabilistic Query Embedding Model (NMP-QEM) that encodes the answer set of each mini-query as a mixed Gaussian distribution with multiple means and covariance parameters, which can approximate any random distribution arbitrarily well in real KGs. Additionally, to overcome the difficulty in defining the closed solution of negation operation, we introduce neural-based logical operators of projection, intersection and negation for a mixed Gaussian distribution to answer all the FOL queries. Extensive experiments demonstrate that NMP-QEM significantly outperforms existing state-of-the-art methods on benchmark datasets. In NELL995, NMP-QEM achieves a 31% relative improvement over the state-of-the-art.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2203792852",
                        "name": "Xiao Long"
                    },
                    {
                        "authorId": "1734743",
                        "name": "Liansheng Zhuang"
                    },
                    {
                        "authorId": "1948159167",
                        "name": "Li Aodi"
                    },
                    {
                        "authorId": "2120491653",
                        "name": "Shafei Wang"
                    },
                    {
                        "authorId": "2144406784",
                        "name": "Houqiang Li"
                    }
                ]
            }
        },
        {
            "contexts": [
                "From design point of view, KBQA systems can be grouped into: (a) end-to-end trainable (Sorokin and Gurevych, 2018; Jia et al., 2018a; Saxena et al., 2020, 2021; Jia et al., 2021; Mavromatis et al., 2021) and (b) modular (Kapanipathi et al., 2021; Hu et al., 2021; Zou et al., 2014)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "618a87d0d43474b7f0a44b91b0023cc3ada59158",
                "externalIds": {
                    "DBLP": "conf/emnlp/NeelamSKIKAMLSP22",
                    "DOI": "10.18653/v1/2022.findings-emnlp.284",
                    "CorpusId": 256631114
                },
                "corpusId": 256631114,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/618a87d0d43474b7f0a44b91b0023cc3ada59158",
                "title": "SYGMA: A System for Generalizable and Modular Question Answering Over Knowledge Bases",
                "abstract": "Knowledge Base Question Answering (KBQA) involving complex reasoning is emerging as an important research direction. However, most KBQA systems struggle with generalizability, particularly on two dimensions: (a) across multiple knowledge bases, where existing KBQA approaches are typically tuned to a single knowledge base, and (b) across multiple reasoning types, where majority of datasets and systems have primarily focused on multi-hop reasoning. In this paper, we present SYGMA, a modular KBQA approach developed with goal of generalization across multiple knowledge bases and multiple reasoning types. To facilitate this, SYGMA is designed as two high level modules: 1) KB-agnostic question understanding module that remain common across KBs, and generates logic representation of the question with high level reasoning constructs that are extensible, and 2) KB-specific question mapping and answering module to address the KB-specific aspects of the answer extraction. We evaluated SYGMA on multiple datasets belonging to distinct knowledge bases (DBpedia and Wikidata) and distinct reasoning types (multi-hop and temporal). State-of-the-art or competitive performances achieved on those datasets demonstrate its generalization capability.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2965855",
                        "name": "S. Neelam"
                    },
                    {
                        "authorId": "2027112872",
                        "name": "Udit Sharma"
                    },
                    {
                        "authorId": "9621738",
                        "name": "Hima P. Karanam"
                    },
                    {
                        "authorId": "3192316",
                        "name": "S. Ikbal"
                    },
                    {
                        "authorId": "2123450380",
                        "name": "P. Kapanipathi"
                    },
                    {
                        "authorId": "145749443",
                        "name": "I. Abdelaziz"
                    },
                    {
                        "authorId": "2689774",
                        "name": "Nandana Mihindukulasooriya"
                    },
                    {
                        "authorId": "2145430350",
                        "name": "Young-Suk Lee"
                    },
                    {
                        "authorId": "2116935757",
                        "name": "Santosh K. Srivastava"
                    },
                    {
                        "authorId": "1999108",
                        "name": "Cezar Pendus"
                    },
                    {
                        "authorId": "2066641453",
                        "name": "Saswati Dana"
                    },
                    {
                        "authorId": "50252087",
                        "name": "Dinesh Garg"
                    },
                    {
                        "authorId": "2297836",
                        "name": "Achille Fokoue"
                    },
                    {
                        "authorId": "2127473246",
                        "name": "G. P. S. Bhargav"
                    },
                    {
                        "authorId": "50564082",
                        "name": "Dinesh Khandelwal"
                    },
                    {
                        "authorId": "30090681",
                        "name": "Srinivas Ravishankar"
                    },
                    {
                        "authorId": "2121386026",
                        "name": "Sairam Gurajada"
                    },
                    {
                        "authorId": "2141904540",
                        "name": "Maria Chang"
                    },
                    {
                        "authorId": "1400349389",
                        "name": "Rosario A. Uceda-Sosa"
                    },
                    {
                        "authorId": "1781292",
                        "name": "S. Roukos"
                    },
                    {
                        "authorId": "1703070",
                        "name": "Alexander G. Gray"
                    },
                    {
                        "authorId": "2053301989",
                        "name": "Guilherme Lima"
                    },
                    {
                        "authorId": "2146658",
                        "name": "Ryan Riegel"
                    },
                    {
                        "authorId": "1767482",
                        "name": "F. Luus"
                    },
                    {
                        "authorId": "143666446",
                        "name": "L. V. Subramaniam"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Example tasks of NLP include question answering [8, 100, 75], information extraction [50, 52, 79], and text summarization [104, 1, 2]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e79d3205f51e514f2c4ec3e40d8cb888a69c2cda",
                "externalIds": {
                    "CorpusId": 259855828
                },
                "corpusId": 259855828,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e79d3205f51e514f2c4ec3e40d8cb888a69c2cda",
                "title": "Non-Autoregressive Unsupervised Summarization with Length-Control Algorithms",
                "abstract": "Text summarization aims to generate a short summary for an input text and has extensive real-world applications such as headline generation. State-of-the-art summarization models are mainly supervised; they require large labeled training corpora and thus cannot be applied to less popular areas, e.g., less spoken languages, where paired data are rare. In this thesis, I present a non-autoregressive unsupervised summarization model, which does not require parallel data for training. Our approach first performs editbased search towards a heuristically defined score and generates a summary as pseudogroundtruth. Then, we train an encoder-only non-autoregressive Transformer based on the search results. Further, we design two length-control algorithms for the model, which perform dynamic programming on the model output and are able to explicitly control the number of words and characters in the generated summary, respectively. Such length control is important for the summarization task, because the main evaluation metric for summarization systems, i.e., ROUGE score, is sensitive to the summary length, and because real-word applications generally involve length constraints. Experiments on two benchmark datasets show that our approach achieves state-ofthe-art performance for unsupervised summarization, yet largely improves inference efficiency. Further, our length-control algorithms are able to perform length-transfer generation, i.e., generating summaries of different lengths than the training target.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2165304515",
                        "name": "Puyuan Liu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "More recently, EmbedKGQA (Saxena et al., 2020) uses ideas from knoweldge graph embedding literature to improve knowledge base question answering esp. on sparser incomplete knowledge graphs.",
                "Model details Following prior work (Saxena et al., 2020), we used a long short term memory (LSTM) network to learn embeddings for words in the questions with an embedding size of 256 for MetaQA and RoBERTa (768 dimensional embeddings) (Liu et al., 2019) for WebQuestionsSP datasets.",
                "Following prior work (Saxena et al., 2020), we experimented on two different settings (for both datasets) - KG Full (in which the KG is left untouched), and the more realistic KG-50 setting in which 50% links are randomly removed.",
                "We closely follow the experimental setup of a prior work (Saxena et al., 2020) for the preprocessed versions of these datasets.",
                "More recently, EmbedKGQA (Saxena et al., 2020) uses ideas from knoweldge graph embedding literature",
                "Model details Following prior work (Saxena et al., 2020), we used a long short term memory (LSTM) network to learn embeddings for words in the questions with an embedding size of 256 for MetaQA and RoBERTa (768 dimensional embeddings) (Liu et al."
            ],
            "intents": [
                "methodology"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "04b8b0dc6f92f27617ac27fa4a432715587c7eed",
                "externalIds": {
                    "DBLP": "conf/eacl/YadatiSSMG21",
                    "ACL": "2021.eacl-main.35",
                    "DOI": "10.18653/v1/2021.eacl-main.35",
                    "CorpusId": 233189553
                },
                "corpusId": 233189553,
                "publicationVenue": {
                    "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
                    "name": "Conference of the European Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Eur Chapter Assoc Comput Linguistics",
                        "EACL"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/eacl/"
                },
                "url": "https://www.semanticscholar.org/paper/04b8b0dc6f92f27617ac27fa4a432715587c7eed",
                "title": "Knowledge Base Question Answering through Recursive Hypergraphs",
                "abstract": "Knowledge Base Question Answering (KBQA) is the problem of predicting an answer for a factoid question over a given knowledge base (KB). Answering questions typically requires reasoning over multiple links in the given KB. Humans tend to answer questions by grouping different objects to perform reasoning over acquired knowledge. Hypergraphs provide a natural tool to model group relationships. In this work, inspired by typical human intelligence, we propose a new method for KBQA based on hypergraphs. Existing methods for KBQA, though effective, do not explicitly incorporate the recursive relational group structure in the given KB. Our method, which we name RecHyperNet (Recursive Hypergraph Network), exploits a new way of modelling KBs through recursive hypergraphs to organise such group relationships in KBs. Experiments on multiple KBQA benchmarks demonstrate the effectiveness of the proposed RecHyperNet. We have released the code.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "46202814",
                        "name": "N. Yadati"
                    },
                    {
                        "authorId": "2223822343",
                        "name": "Dayanidhi R S"
                    },
                    {
                        "authorId": "2071267097",
                        "name": "V. S"
                    },
                    {
                        "authorId": "2223821359",
                        "name": "Indira K M"
                    },
                    {
                        "authorId": "2220753751",
                        "name": "S. G."
                    }
                ]
            }
        },
        {
            "contexts": [
                "Illustration of the neural reasoning method EmbedKGQA for solving the multi-hop relation questions [119].",
                "Figure 8 illustrates the basic idea of EmbedKGQA.",
                "EmbedKGQA selects the entity with the highest score as the answer.",
                "In addition to singlerelation questions, EmbedKGQA [119] is proposed to deal with the multi-hop relation questions.",
                "KEQA and EmbedKGQA cannot handle complex logical questions, because these queries involve the logical operations that will result in multiple entities at each hop.",
                "[88] train an end-to-end topic entity and query relation learning model M EmbedKGQA[119] use RoBERTa to embed the question C GQE [53] deal with conjunctive logic queries C QUERY2BOX [112] deal with disjunctive queries C EmQL [126] obtain faithful embeddings",
                "KGQA is similar to KGC except that the condition of the head entity h and the query relation r is replaced by a natural language question q [2], [5], [119], [127].",
                "Generally, most of the reasoning methods for completion can be decomposed into two key components: rule finding and answer reasoning, where rule finding targets at inferring the rules from the observed triplets in KGs, and answer reasoning aims to predict the answer for the given head entity and the query\n23\nSymbolic reasoning AMIE\nPath-based reasoning Ad-hoc retrieve paths\nPRA\nPath-based reasoning Learn paths by RL\nDeepPath MINERVA\nGraph-based reasoning CogGraph\nMatrix-based reasoning TensorLog Neural LP\nNLIL\nNeural reasoning TransE ConvE\nCompGCN\nRule finding Answer reasoning\nPath-based reasoning Learn paths by VAE\nDIVA\nSymbolic-driven Neural reasoning KALE RUGE IterE\nSymbolic-driven Probabilistic reasoning MLN Prolog\n(a) KGC\nSymbolic reasoning Berant, 2013 TemplateQA\nNeural-enhanced symbolic reasoning Yih, 2014 Bao, 2016\nGraph-based reasoning PullNet\nNeural reasoning KEQA\nEmbedKGQA QUERY2BOX\nPath-based reasoning IRN SRN\nRule finding Answer reasoning\nGraph-based reasoning Ad-hoc retrieve subgraphs\nGraft-Net VRN\n(b) KGQA\nFig."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "91075f524e74bdc618fea689416de22c1cf2a472",
                "externalIds": {
                    "MAG": "3157651462",
                    "DBLP": "journals/aiopen/ZhangCZKD21",
                    "DOI": "10.1016/J.AIOPEN.2021.03.001",
                    "CorpusId": 235845598
                },
                "corpusId": 235845598,
                "publicationVenue": {
                    "id": "6c35576a-a87d-4dc1-a576-780572d8d0e6",
                    "name": "AI Open",
                    "type": "journal",
                    "issn": "2666-6510",
                    "url": "https://www.keaipublishing.com/en/journals/ai-open/"
                },
                "url": "https://www.semanticscholar.org/paper/91075f524e74bdc618fea689416de22c1cf2a472",
                "title": "Neural, symbolic and neural-symbolic reasoning on knowledge graphs",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2158144101",
                        "name": "Jing Zhang"
                    },
                    {
                        "authorId": "2152688058",
                        "name": "Bo Chen"
                    },
                    {
                        "authorId": "2145402325",
                        "name": "Lingxi Zhang"
                    },
                    {
                        "authorId": "1993985136",
                        "name": "Xirui Ke"
                    },
                    {
                        "authorId": "2113455170",
                        "name": "Haipeng Ding"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Note that EmbedKGQA (Saxena et al., 2020) use RoBERTa (Liu et al.",
                "EmbedKGQA (Saxena et al., 2020) use ComplEx (Trouillon et al.",
                "In the \u201chalf\u201d setting, we follow previous work (Saxena et al., 2020) to randomly drop 50% of triplets in the knowledge graph.",
                "For fair comparison with previous work (Sun et al., 2018, 2019a; Saxena et al., 2020), we set the embedding size to 300.",
                "EmbedKGQA EmbedKGQA (Saxena et al., 2020) models multi-hop KBQA as a link prediction task.",
                "Note that EmbedKGQA (Saxena et al., 2020) use RoBERTa (Liu et al., 2019) for word embeddings and ComplEx (Trouillon et al., 2016) for entity embeddings.",
                "Following previous work (Sun et al., 2018, 2019a; Saxena et al., 2020), we use the \u201cvanilla\u201d version of the dataset.",
                "We further compare between the unconscious phase of DCRN and EmbedKGQA (Saxena et al., 2020).",
                "Recently, multi-hop question answering over KGs has attracted great attention from both academia and industry (Li et al., 2017; Fu et al., 2020; Saxena et al., 2020).",
                "EmbedKGQA (Saxena et al., 2020) use ComplEx (Trouillon et al., 2016) to train knowledge graph embeddings, which represents entity and relation embeddings as vectors in complex spaces."
            ],
            "intents": [
                "methodology",
                "result",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "14ee04939eae5610d5d6141ad953021967ab2de5",
                "externalIds": {
                    "DBLP": "conf/acl/CaiZWW21",
                    "ACL": "2021.findings-acl.19",
                    "DOI": "10.18653/v1/2021.findings-acl.19",
                    "CorpusId": 236477378
                },
                "corpusId": 236477378,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/14ee04939eae5610d5d6141ad953021967ab2de5",
                "title": "Deep Cognitive Reasoning Network for Multi-hop Question Answering over Knowledge Graphs",
                "abstract": "Knowledge Graphs (KGs) provide human knowledge with nodes and edges being entities and relations among them, respectively. Multihop question answering over KGs\u2014which aims to find answer entities of given questions through reasoning paths in KGs\u2014has attracted great attention from both academia and industry recently. However, this task remains challenging, as it requires to accurately identify answers in a large candidate entity set, of which the size grows exponentially with the number of reasoning hops. To tackle this problem, we propose a novel Deep Cognitive Reasoning Network (DCRN), which is inspired by the dual process theory in cognitive science. Specifically, DCRN consists of two phases\u2014the unconscious phase and the conscious phase. The unconscious phase first retrieves informative evidence from candidate entities by leveraging their semantic information. Then, the conscious phase accurately identifies answers by performing sequential reasoning according to the graph structure on the retrieved evidence. Experiments demonstrate that DCRN significantly outperforms state-of-the-art methods on benchmark datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2115669924",
                        "name": "Jianyu Cai"
                    },
                    {
                        "authorId": "122542593",
                        "name": "Zhanqiu Zhang"
                    },
                    {
                        "authorId": "144864333",
                        "name": "Feng Wu"
                    },
                    {
                        "authorId": "2146041754",
                        "name": "Jie Wang"
                    }
                ]
            }
        },
        {
            "contexts": [
                "IR-based methods (Bordes et al., 2015; Dong et al., 2015; Miller et al., 2016; Sun et al., 2018, 2019; Saxena et al., 2020; He et al., 2021) directly retrieve answer candidates from the KBs and represent them to encode the semantic relationships with the questions.",
                "We compare with three IR-based KBQA models: GRAFT-Net (Sun et al., 2018), EmbedKGQA (Saxena et al., 2020) and NSM (He et al., 2021).",
                "\u2026including the semantic parsing based (SP-based) models (Berant et al., 2013; Bao et al., 2016; Liang et al., 2017; Lan and Jiang, 2020) and the information retrieval based (IR-based) models (Sun et al., 2018, 2019; Saxena et al., 2020; He et al., 2021) are commonly studied\n\u2217 Corresponding author.",
                "EmbedKGQA directly optimizes the triplet of (topic entity, question, answer) based on their direct embeddings.",
                ", 2017; Lan and Jiang, 2020) and the information retrieval based (IR-based) models (Sun et al., 2018, 2019; Saxena et al., 2020; He et al., 2021) are commonly studied",
                "We perform any subgraph reasoning model such as GRAFT-Net (Sun et al., 2018), EmbedKGQA (Saxena et al., 2020) and NSM (He et al., 2021) on Gqr to learn the embeddings for entities in the subgraph.",
                "On both datasets, the embedding dimension is set as 200 for EmbedKGQA and 50 for NSM.",
                ", 2018), EmbedKGQA (Saxena et al., 2020) and NSM (He et al."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "06df1dc2f0d22c9a6f094f5cc50ed1c7a812cf9d",
                "externalIds": {
                    "DBLP": "conf/emnlp/FengZHZLLLC21",
                    "DOI": "10.18653/v1/2021.findings-emnlp.159",
                    "CorpusId": 244119436
                },
                "corpusId": 244119436,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/06df1dc2f0d22c9a6f094f5cc50ed1c7a812cf9d",
                "title": "A Pretraining Numerical Reasoning Model for Ordinal Constrained Question Answering on Knowledge Base",
                "abstract": "Knowledge Base Question Answering (KBQA) is to answer natural language questions posed over knowledge bases (KBs). This paper targets at empowering the IR-based KBQA models with the ability of numerical reasoning for answering ordinal constrained questions. A major challenge is the lack of explicit annotations about numerical properties. To address this challenge, we propose a pretraining numerical reasoning model consisting of NumGNN and NumTransformer, guided by explicit self-supervision signals. The two modules are pretrained to encode the magnitude and ordinal properties of numbers respectively and can serve as model-agnostic plugins for any IR-based KBQA model to enhance its numerical reasoning ability. Extensive experiments on two KBQA benchmarks verify the effectiveness of our method to enhance the numerical reasoning ability for IR-based KBQA models. Our code and datasets are available online 1 .",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2150671688",
                        "name": "Yu Feng"
                    },
                    {
                        "authorId": "2155700347",
                        "name": "Jing Zhang"
                    },
                    {
                        "authorId": "51149404",
                        "name": "Gaole He"
                    },
                    {
                        "authorId": "2542603",
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "authorId": "2978364",
                        "name": "Lemao Liu"
                    },
                    {
                        "authorId": "2115901153",
                        "name": "Quan Liu"
                    },
                    {
                        "authorId": "1625473962",
                        "name": "Cuiping Li"
                    },
                    {
                        "authorId": "92779309",
                        "name": "Hong Chen"
                    }
                ]
            }
        },
        {
            "contexts": [
                "This work intends to reproduce and perform an ablation (removing relation matching module) as well as an extended study on EmbedKGQA[1].",
                "[1] fills this gap with the proposed EmbedKGQA method.",
                "Statistics for table:{1, 2} have been taken from [1].",
                "2 Methodology We have used the code provided by [1] with some customization for reproducibility.",
                "Based on the code shared by the authors, we have reproduced the results for EmbedKGQA[1].",
                "From the results of table:5 in [1] and table:6 in this report, it is evident that relationmatching(RM) is an important component inmulti-hop KGQAwhen the given KG is considerably large, i.",
                "The original values for EmbedKGQA are taken from [1].",
                "QA data statistics for each dataset according to [1] Dataset Triples Entities Relations Experiment-Alias MetaQA-KG-Full 135k 43k 9 MetaQA_full WebQSP-KG-Full 5.",
                "According to [1], using ComplEx [8] KG embeddings significantly improves Hits@1 for multi-hop KGQA task and it has been proved with the help of the results on MetaQA [9] andWebQSP [10] datasets.",
                "6 Communication with original authors We had a couple of virtual meetings with Apoorv Saxena1, the primary author of EmbedKGQA[1]."
            ],
            "intents": [
                "methodology",
                "result",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "1015d9b7670ed046c79fe237126e8ca4b751b142",
                "externalIds": {
                    "CorpusId": 250605292
                },
                "corpusId": 250605292,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/1015d9b7670ed046c79fe237126e8ca4b751b142",
                "title": "[Re] Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings",
                "abstract": "We have used the code provided by [1] with some customization for reproducibility. In addition to making the codebase more modular and easy to navigate, we have made changes to incorporate different transformers in the question embeddingmodule. QuestionAnswering models were trained from scratch as no pre-trained models were available for our particular dataset. The code for this work is available on GitHub (See page footer for the link).",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2047711797",
                        "name": "Jishnu Jaykumar"
                    },
                    {
                        "authorId": "50847752",
                        "name": "Ashish Sardana"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Baselines: We have used, KV-mem Bordes et al. (2015), GraftNet Sun et al. (2018), PullNet Sun et al. (2019), VRN Zhang et al. (2018), and EmbedKGQA Saxena et al. (2020) as the baselines.",
                "Recently, Saxena et al. (2020) presented an approach, EmbedKGQA, for joint learning, again using KG Embeddings, in the context of multi-hop relations.",
                "Our proposed approach has outperformed PullNet and EmbedKGQA on the MetaQA dataset, as shown in Section 5.",
                "(2018), and Saxena et al. (2020), except for * marked numbers."
            ],
            "intents": [
                "methodology",
                "background"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "9c89283076579b2d70cf16dfc7cd67972c7295de",
                "externalIds": {
                    "ACL": "2021.eacl-main.300",
                    "DBLP": "conf/eacl/SrivastavaPCABS21",
                    "DOI": "10.18653/v1/2021.eacl-main.300",
                    "CorpusId": 233189575
                },
                "corpusId": 233189575,
                "publicationVenue": {
                    "id": "8de18c35-6785-4e54-99f2-21ee961302c6",
                    "name": "Conference of the European Chapter of the Association for Computational Linguistics",
                    "type": "conference",
                    "alternate_names": [
                        "Conf Eur Chapter Assoc Comput Linguistics",
                        "EACL"
                    ],
                    "url": "https://www.aclweb.org/anthology/venues/eacl/"
                },
                "url": "https://www.semanticscholar.org/paper/9c89283076579b2d70cf16dfc7cd67972c7295de",
                "title": "Complex Question Answering on knowledge graphs using machine translation and multi-task learning",
                "abstract": "Question answering (QA) over a knowledge graph (KG) is a task of answering a natural language (NL) query using the information stored in KG. In a real-world industrial setting, this involves addressing multiple challenges including entity linking, multi-hop reasoning over KG, etc. Traditional approaches handle these challenges in a modularized sequential manner where errors in one module lead to the accumulation of errors in downstream modules. Often these challenges are inter-related and the solutions to them can reinforce each other when handled simultaneously in an end-to-end learning setup. To this end, we propose a multi-task BERT based Neural Machine Translation (NMT) model to address these challenges. Through experimental analysis, we demonstrate the efficacy of our proposed approach on one publicly available and one proprietary dataset.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1390997787",
                        "name": "Saurabh Srivastava"
                    },
                    {
                        "authorId": "34823900",
                        "name": "Mayur Patidar"
                    },
                    {
                        "authorId": "143996866",
                        "name": "S. A. Chowdhury"
                    },
                    {
                        "authorId": "145968137",
                        "name": "P. Agarwal"
                    },
                    {
                        "authorId": "145963427",
                        "name": "Indrajit Bhattacharya"
                    },
                    {
                        "authorId": "143725466",
                        "name": "Gautam M. Shroff"
                    }
                ]
            }
        },
        {
            "contexts": [
                "To represent entities and relations of knowledge graphs (KGs) in the semantic vector space, researchers have proposed various knowledge graph embedding (KGE) models, which have shown great potential in knowledge graph completion and knowledgedriven applications [7, 13, 19]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "15fe39a085cd7ac5d350eb40a2d0cfc215d7240e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-14930",
                    "CorpusId": 232404137
                },
                "corpusId": 232404137,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/15fe39a085cd7ac5d350eb40a2d0cfc215d7240e",
                "title": "High-efficiency Euclidean-based Models for Low-dimensional Knowledge Graph Embeddings",
                "abstract": "Recent knowledge graph embedding (KGE) models based on hyperbolic geometry have shown great potential in a low-dimensional embedding space. However, the necessity of hyperbolic space in KGE is still questionable, because the calculation based on hyperbolic geometry is much more complicated than Euclidean operations. In this paper, based on the state-of-the-art hyperbolicbased model RotH, we develop two lightweight Euclidean-based models, called RotL and Rot2L. The RotL model simplifies the hyperbolic operations while keeping the flexible normalization effect. Utilizing a novel two-layer stacked transformation and based on RotL, the Rot2L model obtains an improved representation capability, yet costs fewer parameters and calculations than RotH. The experiments on link prediction show that Rot2L achieves the state-of-the-art performance on two widely-used datasets in low-dimensional knowledge graph embeddings. Furthermore, RotL achieves similar performance as RotH but only requires half of the training time. The source code of our work is available on GitHub. ACM Reference Format: KaiWang, Yu Liu, and Quan Z. Sheng. 2021. High-efficiency Euclidean-based Models for Low-dimensional Knowledge Graph Embeddings. In Proceedings of ACM Conference (Conference\u201917). ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2148895945",
                        "name": "Kai Wang"
                    },
                    {
                        "authorId": "2146401270",
                        "name": "Yu Liu"
                    },
                    {
                        "authorId": "120607997",
                        "name": "Quan.Z Sheng"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c2d37985ffcce85703139067357a8ba8807211eb",
                "externalIds": {
                    "CorpusId": 235430668
                },
                "corpusId": 235430668,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/c2d37985ffcce85703139067357a8ba8807211eb",
                "title": "Connecting the Dots: Explaining Human Reasoning on the Graph A Case Study on Deep Question Generation",
                "abstract": "Deep Question Generation (DQG) involves generating a complex question given an input passage. This task involves reasoning over multiple sources of information in the passage. To tackle this, Pan et al. (2020) leverages recent advances in graph neural networks (GNN) to represent the passage as a collection of phrases, where each phrase forms a node in a graph. Then, to reason over this graph, information is aggregated via a confined graph structure to combine them to generate meaningful questions. However, what is unclear at this moment is why is the graph important, which structures are important, and if this human imposed structure is even required. This paper looks deeper into answering these 3 main questions via the following approaches: (1) A linguistic heuristic approach to investigate the linguistic structures that are crucial for reasoning in DQG, which can be used as structural priors for building better reasoning models, (2) A GNN learnable approach to investigate the computational structure the model uses and if this aligns with the human one, (3) Generalizing the GNN model used to allow the model to learn its own computational graph structure for the reasoning task.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1995261298",
                        "name": "Yong Liang Goh"
                    },
                    {
                        "authorId": "1527090216",
                        "name": "Yongbin Li"
                    },
                    {
                        "authorId": "39751575",
                        "name": "Yisong Miao"
                    }
                ]
            }
        },
        {
            "contexts": [
                ", 2018], question-answering [Saxena et al., 2020] and recommendation [Chen et al.",
                "KG embeddings have a number of applications ranging\nfrom KG completion [Bordes et al., 2013] to natural language inference [Peters et al., 2019], knowledge-aware conversation generation [Zhou et al., 2018], question-answering [Saxena et al., 2020] and recommendation [Chen et al., 2019]."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3e46b3b2878e39e50bf6a0df370796cd7e2bf3a0",
                "externalIds": {
                    "DBLP": "conf/akbc/VisweswariahTKR21",
                    "DOI": "10.24432/C5988G",
                    "CorpusId": 235657639
                },
                "corpusId": 235657639,
                "publicationVenue": {
                    "id": "c2abee38-d372-4e2a-ac1e-2cd22999a564",
                    "name": "Conference on Automated Knowledge Base Construction",
                    "type": "conference",
                    "alternate_names": [
                        "AKBC",
                        "Conf Autom Knowl Base Constr",
                        "Automated Knowledge Base Construction",
                        "Autom Knowl Base Constr"
                    ],
                    "url": "https://www.akbc.ws/"
                },
                "url": "https://www.semanticscholar.org/paper/3e46b3b2878e39e50bf6a0df370796cd7e2bf3a0",
                "title": "A Joint Training Framework for Open-World Knowledge Graph Embeddings",
                "abstract": "Knowledge Graphs(KGs) represent factual information as graphs of entities connected by relations. Knowledge graph embeddings have emerged as a popular approach to encode this information for various downstream tasks like natural language inference, question answering and dialogue generation. As knowledge bases expand, we are presented with newer (open-world) entities, often with textual descriptions. We require techniques to embed new entities as they arrive using the textual information at hand. This task of open-world KG completion has received some attention in recent years. However, we \ufb01nd that existing approaches su\ufb00er from one or more of four drawbacks \u2013 1) They are not modular with respect to the choice of the KG embedding model 2) They ignore best practices for aligning two embedding spaces 3) They do not account for di\ufb00erences in training strategy needed when presented with datasets with di\ufb00erent description sizes and 4) They do not produce entity embeddings for use by downstream tasks. To address these problems, we propose FOlK ( F ramework for O pen-Wor l d K G embeddings) - a technique that jointly learns embeddings for KG entities from descriptions and KG structure for open-world knowledge graph completion. Additionally, we modify existing data sources and make available YAGO3-10-Open and WN18RR-Open two datasets that are well suited for demonstrating the e\ufb03cacy of open-world KG completion approaches. Finally, we empirically demonstrate the e\ufb00ectiveness of our model in improving upon state-of-the-art baselines on several tasks resulting in performance increases of up to 72% on mean reciprocal rank.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2484153",
                        "name": "Karthik Venkat Ramanan"
                    },
                    {
                        "authorId": "83316491",
                        "name": "Beethika Tripathi"
                    },
                    {
                        "authorId": "2361078",
                        "name": "Mitesh M. Khapra"
                    },
                    {
                        "authorId": "2059566473",
                        "name": "B. Ravindran"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "01e30a41003cc04cee01b31ac2860d20e772187b",
                "externalIds": {
                    "DBLP": "conf/icml/RenDDCYSSLZ21",
                    "CorpusId": 235826211
                },
                "corpusId": 235826211,
                "publicationVenue": {
                    "id": "fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                    "name": "International Conference on Machine Learning",
                    "type": "conference",
                    "alternate_names": [
                        "ICML",
                        "Int Conf Mach Learn"
                    ],
                    "url": "https://icml.cc/"
                },
                "url": "https://www.semanticscholar.org/paper/01e30a41003cc04cee01b31ac2860d20e772187b",
                "title": "LEGO: Latent Execution-Guided Reasoning for Multi-Hop Question Answering on Knowledge Graphs",
                "abstract": "Answering complex natural language questions on knowledge graphs (KGQA) is a challenging task. It requires reasoning with the input natu-ral language questions as well as a massive, incomplete heterogeneous KG. Prior methods obtain an abstract structured query graph/tree from the input question and traverse the KG for answers following the query tree. However, they inherently cannot deal with missing links in the KG. Here we present LEGO, a L atent E xecution-G uided reas O ning framework to handle this challenge in KGQA. LEGO works in an iterative way, which alternates between (1) a Query Synthesizer, which synthesizes a reasoning action and grows the query tree step-by-step, and (2) a Latent Space Executor that executes the reasoning action in the latent embedding space to combat against the missing information in KG. To learn the synthe-sizer without step-wise supervision, we design a generic latent execution guided bottom-up search procedure to \ufb01nd good execution traces ef\ufb01ciently in the vast query space. Experimental results on several KGQA benchmarks demonstrate the effectiveness of our framework compared with previous state of the art.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "40046694",
                        "name": "Hongyu Ren"
                    },
                    {
                        "authorId": "2791430",
                        "name": "H. Dai"
                    },
                    {
                        "authorId": "144445933",
                        "name": "Bo Dai"
                    },
                    {
                        "authorId": "1425082935",
                        "name": "Xinyun Chen"
                    },
                    {
                        "authorId": "19168196",
                        "name": "Michihiro Yasunaga"
                    },
                    {
                        "authorId": "3456820",
                        "name": "Haitian Sun"
                    },
                    {
                        "authorId": "50319359",
                        "name": "D. Schuurmans"
                    },
                    {
                        "authorId": "1702139",
                        "name": "J. Leskovec"
                    },
                    {
                        "authorId": "65855107",
                        "name": "Denny Zhou"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Pre-trained KB embeddings were shown to improve multi-hop KBQA where answers are entities and no operations are involved (Saxena et al., 2020).",
                "While (Saxena et al., 2020) evaluates 2-hop questions (Yih et al.",
                "While (Saxena et al., 2020) evaluates 2-hop questions (Yih et al., 2016) and 2 and 3-hop questions with limited relation types (Zhang et al., 2018)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a89db4fca6953c3137fb0cdf8fca36d3fd6b3a11",
                "externalIds": {
                    "DBLP": "conf/rep4nlp/ThirukovalluruS21",
                    "ACL": "2021.repl4nlp-1.24",
                    "DOI": "10.18653/v1/2021.repl4nlp-1.24",
                    "CorpusId": 236486248
                },
                "corpusId": 236486248,
                "publicationVenue": {
                    "id": "8b169440-4c13-4cf4-b3f9-1dc7c39dc888",
                    "name": "Workshop on Representation Learning for NLP",
                    "type": "conference",
                    "alternate_names": [
                        "RepL4NLP",
                        "Workshop Represent Learn NLP"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a89db4fca6953c3137fb0cdf8fca36d3fd6b3a11",
                "title": "Knowledge Informed Semantic Parsing for Conversational Question Answering",
                "abstract": "Smart assistants are tasked to answer various questions regarding world knowledge. These questions range from retrieval of simple facts to retrieval of complex, multi-hops question followed by various operators (i.e., filter, argmax). Semantic parsing has emerged as the state-of-the-art for answering these kinds of questions by forming queries to extract information from knowledge bases (KBs). Specially, neural semantic parsers (NSPs) effectively translate natural questions to logical forms, which execute on KB and give desirable answers. Yet, NSPs suffer from non-executable logical forms for some instances in the generated logical forms might be missing due to the incompleteness of KBs. Intuitively, knowing the KB structure informs NSP with changes of the global logical forms structures with respect to changes in KB instances. In this work, we propose a novel knowledge-informed decoder variant of NSP. We consider the conversational question answering settings, where a natural language query, its context and its final answers are available at training. Experimental results show that our method outperformed strong baselines by 1.8 F1 points overall across 10 types of questions of the CSQA dataset. Especially for the \u201cLogical Reasoning\u201d category, our model improves by 7 F1 points. Furthermore, our results are achieved with 90.3% fewer parameters, allowing faster training for large-scale datasets.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "6476826",
                        "name": "Raghuveer Thirukovalluru"
                    },
                    {
                        "authorId": "1734869335",
                        "name": "Mukund Sridhar"
                    },
                    {
                        "authorId": "2064894364",
                        "name": "Dung Ngoc Thai"
                    },
                    {
                        "authorId": "123840422",
                        "name": "Shruti Chanumolu"
                    },
                    {
                        "authorId": "2121348263",
                        "name": "Nicholas Monath"
                    },
                    {
                        "authorId": "143871289",
                        "name": "Sankaranarayanan Ananthakrishnan"
                    },
                    {
                        "authorId": "143753639",
                        "name": "A. McCallum"
                    }
                ]
            }
        },
        {
            "contexts": [
                "\u2026graphs (Li et al., 2012; Liu et al., 2017; Park et al., 2019), which enable a number of applications such as Web search (Brin and Page, 1998; Kleinberg, 1999), social network analysis (Weng et al., 2010), RecSys (Jing et al., 2014), query disambiguation (Makris et al., 2012; Saxena et al., 2020)."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "114cd7727c71a1eb28b491e63082405d0af8fac5",
                "externalIds": {
                    "DBLP": "conf/acl/ZhaoZXZLL21",
                    "ACL": "2021.findings-acl.115",
                    "DOI": "10.18653/v1/2021.findings-acl.115",
                    "CorpusId": 236477489
                },
                "corpusId": 236477489,
                "publicationVenue": {
                    "id": "479d5605-51be-4346-b1d6-4334084504df",
                    "name": "Findings",
                    "type": "journal",
                    "issn": "2652-8800",
                    "url": "https://findingspress.org/"
                },
                "url": "https://www.semanticscholar.org/paper/114cd7727c71a1eb28b491e63082405d0af8fac5",
                "title": "Incorporating Global Information in Local Attention for Knowledge Representation Learning",
                "abstract": "Graph Attention Networks (GATs) have proven a promising model that takes advantage of localized attention mechanism to perform knowledge representation learning (KRL) on graph-structure data, e.g., Knowledge Graphs (KGs). While such approaches model entities\u2019 local pairwise importance, they lack the capability to model global importance relative to other entities of KGs. This causes such models to miss critical information in tasks where global information is also a significant component for the task, such as in knowledge representation learning. To address the issue, we allow the proper incorporation of global information into the GAT family of models through the use of scaled entity importance, which is calculated by an attention-based global random walk algorithm. In the context of KRL, incorporating global information boosts performance significantly. Experimental results on KG entity prediction against the state-of-thearts sufficiently demonstrate the effectiveness of our proposed model.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "97522134",
                        "name": "Yu Zhao"
                    },
                    {
                        "authorId": "2024855331",
                        "name": "Hannah Zhou"
                    },
                    {
                        "authorId": "3360722",
                        "name": "Ruobing Xie"
                    },
                    {
                        "authorId": "1799525",
                        "name": "Fuzhen Zhuang"
                    },
                    {
                        "authorId": "2117895423",
                        "name": "Qing Li"
                    },
                    {
                        "authorId": "40478933",
                        "name": "Ji Liu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "45ab83b119fbae912066aa65b054521e78cb8089",
                "externalIds": {
                    "MAG": "3165935919",
                    "DOI": "10.1016/J.PROCS.2021.05.024",
                    "CorpusId": 236679486
                },
                "corpusId": 236679486,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/45ab83b119fbae912066aa65b054521e78cb8089",
                "title": "Evolution of Reading Comprehension and Question Answering Systems",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2122079081",
                        "name": "Venkatesh Krishnamoorthy"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a19920428f9ad1e0e760d46695e6bb09adde5fcd",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-14420",
                    "CorpusId": 236635315
                },
                "corpusId": 236635315,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a19920428f9ad1e0e760d46695e6bb09adde5fcd",
                "title": "Talk2Data: High-Level Question Decomposition for Data-Oriented Question and Answering",
                "abstract": "). Two high-level questions are decomposed",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2064971223",
                        "name": "Danqing Shi"
                    },
                    {
                        "authorId": "2118269946",
                        "name": "Yi Guo"
                    },
                    {
                        "authorId": "2151671022",
                        "name": "Mingjuan Guo"
                    },
                    {
                        "authorId": "2134152070",
                        "name": "Yanqiu Wu"
                    },
                    {
                        "authorId": "2152520175",
                        "name": "Qing Chen"
                    },
                    {
                        "authorId": "144313415",
                        "name": "Nan Cao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c13bc9c7174ea2973f4fab7ef164842dec14daeb",
                "externalIds": {
                    "DBLP": "conf/nlpcc/ShenCX21",
                    "DOI": "10.1007/978-3-030-88480-2_28",
                    "CorpusId": 238862810
                },
                "corpusId": 238862810,
                "publicationVenue": {
                    "id": "640a5acb-a481-4a3e-a751-1eb880600a99",
                    "name": "Natural Language Processing and Chinese Computing",
                    "type": "conference",
                    "alternate_names": [
                        "NLPCC",
                        "Nat Lang Process Chin Comput"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c13bc9c7174ea2973f4fab7ef164842dec14daeb",
                "title": "Diversified Paraphrase Generation with Commonsense Knowledge Graph",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "152499850",
                        "name": "X. Shen"
                    },
                    {
                        "authorId": "5040052",
                        "name": "Jiangjie Chen"
                    },
                    {
                        "authorId": "3011950",
                        "name": "Yanghua Xiao"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9c373510adcbcf15da94137769afce3280796999",
                "externalIds": {
                    "DBLP": "conf/iccsa/BhuiyanKKA21",
                    "DOI": "10.1007/978-3-030-86970-0_24",
                    "CorpusId": 237540210
                },
                "corpusId": 237540210,
                "publicationVenue": {
                    "id": "a210e09f-0ea1-4863-bc9c-75ddc51197ac",
                    "name": "Communication Systems and Applications",
                    "type": "conference",
                    "alternate_names": [
                        "International Conference on Computer Sciences and Applications",
                        "Commun Syst Appl",
                        "International Conference on Complex Systems and Applications",
                        "CSA",
                        "Int Conf Comput Sci Appl",
                        "Conf Comput Syst Appl",
                        "ICCSA",
                        "Comput Sci it Appl",
                        "Int Conf Comput Sci It Appl",
                        "Conference on Computing Systems and Applications",
                        "International Conference on Computer Science and its Applications",
                        "Int Conf Comput Sci it Appl",
                        "Computer Science and its Applications",
                        "International Conference on Computational Science and Its Applications",
                        "Int Conf Complex Syst Appl"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=587",
                    "alternate_urls": [
                        "http://www.iccsa.org/"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9c373510adcbcf15da94137769afce3280796999",
                "title": "Semantic Improvement of Question-Answering System with Multi-Layer LSTM",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2985748",
                        "name": "Hanif Bhuiyan"
                    },
                    {
                        "authorId": "2129467906",
                        "name": "Md. Abdul Karim"
                    },
                    {
                        "authorId": "2117622660",
                        "name": "Faria Benta Karim"
                    },
                    {
                        "authorId": "33073495",
                        "name": "Jinat Ara"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "220abb60e0be14b102184d7d17ce4601de8b7a8c",
                "externalIds": {
                    "DBLP": "conf/pricai/HongBW21",
                    "DOI": "10.1007/978-3-030-89188-6_43",
                    "CorpusId": 239832634
                },
                "corpusId": 239832634,
                "publicationVenue": {
                    "id": "a058ca2a-3dc3-485e-beab-559941c41cf1",
                    "name": "Pacific Rim International Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Pac Rim Int Conf Artif Intell",
                        "PRICAI"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/220abb60e0be14b102184d7d17ce4601de8b7a8c",
                "title": "High-Quality Noise Detection for Knowledge Graph Embedding with Rule-Based Triple Confidence",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "47547671",
                        "name": "Y. Hong"
                    },
                    {
                        "authorId": "3242963",
                        "name": "Chenyang Bu"
                    },
                    {
                        "authorId": "2145502658",
                        "name": "Xindong Wu"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Saxena et al., 2020)."
            ],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6f08716480ee5ce761267fbe649cf8c5d436f636",
                "externalIds": {
                    "DBLP": "conf/emnlp/ZhangWZTJ21",
                    "DOI": "10.18653/v1/2021.findings-emnlp.120",
                    "CorpusId": 240291185
                },
                "corpusId": 240291185,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/6f08716480ee5ce761267fbe649cf8c5d436f636",
                "title": "EventKE: Event-Enhanced Knowledge Graph Embedding",
                "abstract": "Relations in most of the traditional knowledge graphs (KGs) only re\ufb02ect static and factual connections, but fail to represent the dynamic activities and state changes about entities. In this paper, we emphasize the importance of incorporating events in KG representation learning, and propose an event-enhanced KG embedding model EventKE . Speci\ufb01cally, given the original KG, we \ufb01rst incorporate event nodes by building a heterogeneous network, where entity nodes and event nodes are distributed on the two sides of the network inter-connected by event argument links. We then use entity-entity relations from the original KG and event-event temporal links to inner-connect entity and event nodes respectively. We design a novel and effective attention-based message passing method, which is con-ducted on entity-entity, event-entity, and event-event relations to fuse the event information into KG embeddings. Experimental results on real-world datasets demonstrate that events can greatly improve the quality of the KG embeddings on multiple downstream tasks. 1",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2116461591",
                        "name": "Zixuan Zhang"
                    },
                    {
                        "authorId": "2108986414",
                        "name": "Hongwei Wang"
                    },
                    {
                        "authorId": "2146232913",
                        "name": "Han Zhao"
                    },
                    {
                        "authorId": "8163721",
                        "name": "Hanghang Tong"
                    },
                    {
                        "authorId": "2113323573",
                        "name": "Heng Ji"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e9a496f9026accb51e3c56302f3f9d6dcec46d6f",
                "externalIds": {
                    "DBLP": "journals/aiopen/ZhuYZRLHWC21",
                    "MAG": "3201622035",
                    "DOI": "10.1016/j.aiopen.2021.09.002",
                    "CorpusId": 240582004
                },
                "corpusId": 240582004,
                "publicationVenue": {
                    "id": "6c35576a-a87d-4dc1-a576-780572d8d0e6",
                    "name": "AI Open",
                    "type": "journal",
                    "issn": "2666-6510",
                    "url": "https://www.keaipublishing.com/en/journals/ai-open/"
                },
                "url": "https://www.semanticscholar.org/paper/e9a496f9026accb51e3c56302f3f9d6dcec46d6f",
                "title": "Incorporating bidirectional interactive information and regional features for relational facts extraction",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1959030360",
                        "name": "Bingshan Zhu"
                    },
                    {
                        "authorId": "2152845834",
                        "name": "Yang Yu"
                    },
                    {
                        "authorId": "2124759670",
                        "name": "Mingying Zhang"
                    },
                    {
                        "authorId": "102665013",
                        "name": "Haopeng Ren"
                    },
                    {
                        "authorId": "1962403687",
                        "name": "Canguang Li"
                    },
                    {
                        "authorId": "1751661148",
                        "name": "Wenjian Hao"
                    },
                    {
                        "authorId": "2143591705",
                        "name": "Lixi Wang"
                    },
                    {
                        "authorId": "2149184259",
                        "name": "Yi Cai"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "928fbd302d04b100f9152508dcdc49f6516847ce",
                "externalIds": {
                    "DBLP": "journals/aiopen/ZhangLFZ21",
                    "DOI": "10.1016/j.aiopen.2021.12.001",
                    "CorpusId": 245154270
                },
                "corpusId": 245154270,
                "publicationVenue": {
                    "id": "6c35576a-a87d-4dc1-a576-780572d8d0e6",
                    "name": "AI Open",
                    "type": "journal",
                    "issn": "2666-6510",
                    "url": "https://www.keaipublishing.com/en/journals/ai-open/"
                },
                "url": "https://www.semanticscholar.org/paper/928fbd302d04b100f9152508dcdc49f6516847ce",
                "title": "A review of deep learning in question answering over knowledge bases",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2111574159",
                        "name": "Chen Zhang"
                    },
                    {
                        "authorId": "7827757",
                        "name": "Yuxuan Lai"
                    },
                    {
                        "authorId": "2115387922",
                        "name": "Yansong Feng"
                    },
                    {
                        "authorId": "144060462",
                        "name": "Dongyan Zhao"
                    }
                ]
            }
        },
        {
            "contexts": [
                "Annotation of Entity Spans As discussed in Soares et al. (2019), different markers for entity spans have a great impact on the BERT-based relation extraction task."
            ],
            "intents": [
                "background"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "375b9b36ef68678185f2b6e4dbbbe7bbfad6535a",
                "externalIds": {
                    "ACL": "2021.emnlp-main.296",
                    "DBLP": "conf/emnlp/YanLWZDZWX21",
                    "DOI": "10.18653/v1/2021.emnlp-main.296",
                    "CorpusId": 243865362
                },
                "corpusId": 243865362,
                "publicationVenue": {
                    "id": "41bf9ed3-85b3-4c90-b015-150e31690253",
                    "name": "Conference on Empirical Methods in Natural Language Processing",
                    "type": "conference",
                    "alternate_names": [
                        "Empir Method Nat Lang Process",
                        "Empirical Methods in Natural Language Processing",
                        "Conf Empir Method Nat Lang Process",
                        "EMNLP"
                    ],
                    "url": "https://www.aclweb.org/portal/emnlp"
                },
                "url": "https://www.semanticscholar.org/paper/375b9b36ef68678185f2b6e4dbbbe7bbfad6535a",
                "title": "Large-Scale Relation Learning for Question Answering over Knowledge Bases with Pre-trained Language Models",
                "abstract": "The key challenge of question answering over knowledge bases (KBQA) is the inconsistency between the natural language questions and the reasoning paths in the knowledge base (KB). Recent graph-based KBQA methods are good at grasping the topological structure of the graph but often ignore the textual information carried by the nodes and edges. Meanwhile, pre-trained language models learn massive open-world knowledge from the large corpus, but it is in the natural language form and not structured. To bridge the gap between the natural language and the structured KB, we propose three relation learning tasks for BERT-based KBQA, including relation extraction, relation matching, and relation reasoning. By relation-augmented training, the model learns to align the natural language expressions to the relations in the KB as well as reason over the missing connections in the KB. Experiments on WebQSP show that our method consistently outperforms other baselines, especially when the KB is incomplete.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1500528818",
                        "name": "Yuanmeng Yan"
                    },
                    {
                        "authorId": "2127717326",
                        "name": "Rumei Li"
                    },
                    {
                        "authorId": "2592528",
                        "name": "Sirui Wang"
                    },
                    {
                        "authorId": "2108854068",
                        "name": "Hongzhi Zhang"
                    },
                    {
                        "authorId": "2139774889",
                        "name": "Daoguang Zan"
                    },
                    {
                        "authorId": "2642200",
                        "name": "Fuzheng Zhang"
                    },
                    {
                        "authorId": "2144356440",
                        "name": "Wei Wu"
                    },
                    {
                        "authorId": "1753096",
                        "name": "Weiran Xu"
                    }
                ]
            }
        },
        {
            "contexts": [],
            "intents": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "13a8fd6234d93eeefc1cc920fcc86396cbf5cefd",
                "externalIds": {
                    "DBLP": "conf/iconip/LiWFZ21",
                    "DOI": "10.1007/978-3-030-92273-3_40",
                    "CorpusId": 260497624
                },
                "corpusId": 260497624,
                "publicationVenue": {
                    "id": "bc5a5118-8f5c-49c7-806e-fb8d44c10ae7",
                    "name": "International Conference on Neural Information Processing",
                    "type": "conference",
                    "alternate_names": [
                        "ICONIP",
                        "Int Conf Neural Inf Process"
                    ],
                    "url": "https://link.springer.com/conference/iconip"
                },
                "url": "https://www.semanticscholar.org/paper/13a8fd6234d93eeefc1cc920fcc86396cbf5cefd",
                "title": "Improving Question Answering over Knowledge Graphs Using Graph Summarization",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118155464",
                        "name": "Sirui Li"
                    },
                    {
                        "authorId": "145313762",
                        "name": "K. Wong"
                    },
                    {
                        "authorId": "1866074",
                        "name": "L. Fung"
                    },
                    {
                        "authorId": "2635255",
                        "name": "Dengya Zhu"
                    }
                ]
            }
        }
    ]
}