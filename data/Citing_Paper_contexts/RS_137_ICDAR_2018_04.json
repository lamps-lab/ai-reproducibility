{
    "offset": 0,
    "data": [
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "2) The majority of them are not all-rounders, making them less effective in handling complicated scenarios such as wireless tables [25], oriented or distorted tables [31, 34\u201336, 47], tables with blank cells [44, 48], or tables without additional text annotations [21, 22, 33].",
                "Graph-based research [21, 22, 33] treat detected text bounding boxes as table elements, construct graphs based on them, and use graph neural networks (GNNs) to predict whether two elements share a row or column, or cell."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3e797270bd42586b0e46ce597a5ef6d754465e43",
                "externalIds": {
                    "ArXiv": "2309.14962",
                    "CorpusId": 262825477
                },
                "corpusId": 262825477,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/3e797270bd42586b0e46ce597a5ef6d754465e43",
                "title": "GridFormer: Towards Accurate Table Structure Recognition via Grid Prediction",
                "abstract": "All tables can be represented as grids. Based on this observation, we propose GridFormer, a novel approach for interpreting unconstrained table structures by predicting the vertex and edge of a grid. First, we propose a flexible table representation in the form of an MXN grid. In this representation, the vertexes and edges of the grid store the localization and adjacency information of the table. Then, we introduce a DETR-style table structure recognizer to efficiently predict this multi-objective information of the grid in a single shot. Specifically, given a set of learned row and column queries, the recognizer directly outputs the vertexes and edges information of the corresponding rows and columns. Extensive experiments on five challenging benchmarks which include wired, wireless, multi-merge-cell, oriented, and distorted tables demonstrate the competitive performance of our model over other methods.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "10344582",
                        "name": "Pengyuan Lyu"
                    },
                    {
                        "authorId": "47121217",
                        "name": "Weihong Ma"
                    },
                    {
                        "authorId": "2109798334",
                        "name": "Hongyi Wang"
                    },
                    {
                        "authorId": "1631405123",
                        "name": "Yu Yu"
                    },
                    {
                        "authorId": "2248958848",
                        "name": "Chengquan Zhang"
                    },
                    {
                        "authorId": "2140371004",
                        "name": "Kun Yao"
                    },
                    {
                        "authorId": "2114922667",
                        "name": "Yang Xue"
                    },
                    {
                        "authorId": "1688516",
                        "name": "Jingdong Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Category 3, according to [36], is the hardest difficulty level, including tables with cell and column merging."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "43f671656d293964bb5d0c5bdcff7223876ca09f",
                "externalIds": {
                    "DBLP": "conf/doceng/DashCA23",
                    "DOI": "10.1145/3573128.3604901",
                    "CorpusId": 260441780
                },
                "corpusId": 260441780,
                "publicationVenue": {
                    "id": "32c90593-78aa-40de-8700-dd31a625b15a",
                    "name": "ACM Symposium on Document Engineering",
                    "type": "conference",
                    "alternate_names": [
                        "Doc Eng",
                        "Document Engineering",
                        "DocEng",
                        "ACM Symp Doc Eng"
                    ],
                    "url": "http://www.documentengineering.org/"
                },
                "url": "https://www.semanticscholar.org/paper/43f671656d293964bb5d0c5bdcff7223876ca09f",
                "title": "WEATHERGOV+: A Table Recognition and Summarization Dataset to Bridge the Gap Between Document Image Analysis and Natural Language Generation",
                "abstract": "Tables, ubiquitous in data-oriented documents like scientific papers and financial statements, organize and convey relational information. Automatic table recognition from document images, which involves detection within the page, structural segmentation into rows, columns, and cells, and information extraction from cells, has been a popular research topic in document image analysis (DIA). With recent advances in natural language generation (NLG) based on deep neural networks, data-to-text generation, in particular for table summarization, offers interesting solutions to time-intensive data analysis. In this paper, we aim to bridge the gap between efforts in DIA and NLG regarding tabular data: we propose WEATHERGOV+, a dataset building upon the WEATHERGOV dataset, the standard for tabular data summarization techniques, that allows for the training and testing of end-to-end methods working from input document images to generate text summaries as output. WEATHERGOV+ contains images of tables created from the tabular data of WEATHERGOV using visual variations that cover various levels of difficulty, along with the corresponding human-generated table summaries of WEATHERGOV. We also propose an end-to-end pipeline that compares state-of-the-art table recognition methods for summarization purposes. We analyse the results of the proposed pipeline by evaluating WEATHERGOV+ at each stage of the pipeline to identify the effects of error propagation and the weaknesses of the current methods, such as OCR errors. With this research (dataset and code available here1), we hope to encourage new research for the processing and management of inter- and intra-document collections.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2051833",
                        "name": "Amanda Dash"
                    },
                    {
                        "authorId": "2587465",
                        "name": "Melissa Cote"
                    },
                    {
                        "authorId": "2639980",
                        "name": "A. Albu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Researchers have also used graph neural networks for table recognition from images [24, 26, 33, 43]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fd32bc409bcb26d5c4a370febbc01772a205db1a",
                "externalIds": {
                    "DBLP": "conf/kdd/JiaGJYDMSLZIH023",
                    "DOI": "10.1145/3580305.3599366",
                    "CorpusId": 260500102
                },
                "corpusId": 260500102,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/fd32bc409bcb26d5c4a370febbc01772a205db1a",
                "title": "GetPt: Graph-enhanced General Table Pre-training with Alternate Attention Network",
                "abstract": "Tables are widely used for data storage and presentation due to their high flexibility in layout. The importance of tables as information carriers and the complexity of tabular data understanding attract a great deal of research on large-scale pre-training for tabular data. However, most of the works design models for specific types of tables, such as relational tables and tables with well-structured headers, neglecting tables with complex layouts. In real-world scenarios, there are many such tables beyond their target scope that cannot be well supported. In this paper, we propose GetPt, a unified pre-training architecture for general table representation applicable even to tables with complex structures and layouts. First, we convert a table to a heterogeneous graph with multiple types of edges to represent the layout of the table. Based on the graph, a specially designed transformer is applied to jointly model the semantics and structure of the table. Second, we devise the Alternate Attention Network (AAN) to better model the contextual information across multiple granularities of a table including tokens, cells, and the table. To better support a wide range of downstream tasks, we further employ three pre-training objectives and pre-train the model on a large table dataset. We fine-tune and evaluate GetPt model on two representative tasks, table type classification, and table structure recognition. Experiments show that GetPt outperforms existing state-of-the-art methods on these tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2053919577",
                        "name": "Ran Jia"
                    },
                    {
                        "authorId": "2110878453",
                        "name": "Haoming Guo"
                    },
                    {
                        "authorId": "2219019849",
                        "name": "Xiaoyu Jin"
                    },
                    {
                        "authorId": "2227588659",
                        "name": "Chao Yan"
                    },
                    {
                        "authorId": "12723949",
                        "name": "Lun Du"
                    },
                    {
                        "authorId": "150344398",
                        "name": "Xiaojun Ma"
                    },
                    {
                        "authorId": "2214758877",
                        "name": "Tamara Stankovic"
                    },
                    {
                        "authorId": "2227564157",
                        "name": "Marko Lozajic"
                    },
                    {
                        "authorId": "2227571357",
                        "name": "Goran Zoranovic"
                    },
                    {
                        "authorId": "2064579585",
                        "name": "Igor Ilic"
                    },
                    {
                        "authorId": "2109750123",
                        "name": "Shi Han"
                    },
                    {
                        "authorId": "2140415600",
                        "name": "Dongmei Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[26] uses a computer-vision feature augmented graph and a GNN for segmentation in order to perform table structure recognition."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "993f5c6de329f22243db1757d3a2e0b238776558",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2308-02051",
                    "ArXiv": "2308.02051",
                    "DOI": "10.48550/arXiv.2308.02051",
                    "CorpusId": 260611558
                },
                "corpusId": 260611558,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/993f5c6de329f22243db1757d3a2e0b238776558",
                "title": "A Graphical Approach to Document Layout Analysis",
                "abstract": "Document layout analysis (DLA) is the task of detecting the distinct, semantic content within a document and correctly classifying these items into an appropriate category (e.g., text, title, figure). DLA pipelines enable users to convert documents into structured machine-readable formats that can then be used for many useful downstream tasks. Most existing state-of-the-art (SOTA) DLA models represent documents as images, discarding the rich metadata available in electronically generated PDFs. Directly leveraging this metadata, we represent each PDF page as a structured graph and frame the DLA problem as a graph segmentation and classification problem. We introduce the Graph-based Layout Analysis Model (GLAM), a lightweight graph neural network competitive with SOTA models on two challenging DLA datasets - while being an order of magnitude smaller than existing models. In particular, the 4-million parameter GLAM model outperforms the leading 140M+ parameter computer vision-based model on 5 of the 11 classes on the DocLayNet dataset. A simple ensemble of these two models achieves a new state-of-the-art on DocLayNet, increasing mAP from 76.8 to 80.8. Overall, GLAM is over 5 times more efficient than SOTA models, making GLAM a favorable engineering choice for DLA tasks.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2124914109",
                        "name": "Jilin Wang"
                    },
                    {
                        "authorId": "18171842",
                        "name": "Michael Krumdick"
                    },
                    {
                        "authorId": "2227910797",
                        "name": "Baojia Tong"
                    },
                    {
                        "authorId": "2228032542",
                        "name": "Hamima Halim"
                    },
                    {
                        "authorId": "153627072",
                        "name": "M. Sokolov"
                    },
                    {
                        "authorId": "2229595712",
                        "name": "Vadym Barda"
                    },
                    {
                        "authorId": "66881015",
                        "name": "Delphine Vendryes"
                    },
                    {
                        "authorId": "1813134",
                        "name": "Christy Tanner"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3db7eebf0e5f22d4c85e13480d641cc0db7bb661",
                "externalIds": {
                    "DBLP": "journals/prl/PrietoAGSV23",
                    "DOI": "10.1016/j.patrec.2023.06.008",
                    "CorpusId": 259170576
                },
                "corpusId": 259170576,
                "publicationVenue": {
                    "id": "f35e3e87-9df4-497b-aa0d-bb8584197290",
                    "name": "Pattern Recognition Letters",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit Lett"
                    ],
                    "issn": "0167-8655",
                    "url": "https://www.journals.elsevier.com/pattern-recognition-letters/",
                    "alternate_urls": [
                        "http://www.journals.elsevier.com/pattern-recognition-letters/",
                        "http://www.sciencedirect.com/science/journal/01678655"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3db7eebf0e5f22d4c85e13480d641cc0db7bb661",
                "title": "Information extraction in handwritten historical logbooks",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2029290748",
                        "name": "J. R. Prieto"
                    },
                    {
                        "authorId": "50143831",
                        "name": "J. Andr\u00e9s"
                    },
                    {
                        "authorId": "21736158",
                        "name": "Emilio Granell"
                    },
                    {
                        "authorId": "1928123",
                        "name": "Joan Andreu S\u00e1nchez"
                    },
                    {
                        "authorId": "2059593537",
                        "name": "Enrique Vidal"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "One approach is using graph neural networks to encode structural information, capturing dependencies and relationships between elements (Qasim et al., 2019)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2b967d82b25088566980aaaf5a7062d90b2fb14f",
                "externalIds": {
                    "ArXiv": "2305.15066",
                    "DBLP": "journals/corr/abs-2305-15066",
                    "DOI": "10.48550/arXiv.2305.15066",
                    "CorpusId": 258865990
                },
                "corpusId": 258865990,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2b967d82b25088566980aaaf5a7062d90b2fb14f",
                "title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking",
                "abstract": "Large language models~(LLM) like ChatGPT have become indispensable to artificial general intelligence~(AGI), demonstrating excellent performance in various natural language processing tasks. In the real world, graph data is ubiquitous and an essential part of AGI and prevails in domains like social network analysis, bioinformatics and recommender systems. The training corpus of large language models often includes some algorithmic components, which allows them to achieve certain effects on some graph data-related problems. However, there is still little research on their performance on a broader range of graph-structured data. In this study, we conduct an extensive investigation to assess the proficiency of LLMs in comprehending graph data, employing a diverse range of structural and semantic-related tasks. Our analysis encompasses 10 distinct tasks that evaluate the LLMs' capabilities in graph understanding. Through our study, we not only uncover the current limitations of language models in comprehending graph structures and performing associated reasoning tasks but also emphasize the necessity for further advancements and novel approaches to enhance their graph processing capabilities. Our findings contribute valuable insights towards bridging the gap between language models and graph understanding, paving the way for more effective graph mining and knowledge extraction.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2218911056",
                        "name": "Jiayan Guo"
                    },
                    {
                        "authorId": "12723949",
                        "name": "Lun Du"
                    },
                    {
                        "authorId": "2139794154",
                        "name": "Hengyu Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Graph neural networks utilize both spatial and textual features [92]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1af79802f3e9864cf1dce509a137982972a8c9de",
                "externalIds": {
                    "DOI": "10.3390/smartcities6030067",
                    "CorpusId": 258819254
                },
                "corpusId": 258819254,
                "publicationVenue": {
                    "id": "d0bfb97a-a20e-4896-9afe-1e63e459db20",
                    "name": "Smart Cities",
                    "alternate_names": [
                        "Smart City"
                    ],
                    "issn": "2624-6511",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-1343723",
                    "alternate_urls": [
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-1343723",
                        "https://www.mdpi.com/journal/smartcities"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/1af79802f3e9864cf1dce509a137982972a8c9de",
                "title": "From Traffic Congestion to Sustainable Mobility: A Case Study of Public Transport in Odesa, Ukraine",
                "abstract": "Consistent and reliable information on passenger traffic is considered crucial for the efficient operation of the public transport (PT) network. The PT network is used to improve public services and thus attract more passengers. This study evaluated the passenger traffic in Odesa, Ukraine, due to the inefficient urban transport system. The main aim of this study was to make PT better by examining passenger distribution on traffic routes and specifying characteristics of PT travel influencing individual satisfaction. The metric-tabular method was used to collect data and examine the number of incoming and outgoing passengers at each bus stop. The results of the passenger and PT analysis provide valuable recommendations for optimizing future routes. It is beneficial for transport companies to implement such recommendations so that inefficient transport on the route can be reduced by either reforming the route network or choosing the optimal number of buses. According to the findings of this study, understanding PT services is the most important determinant of PT adoption. The main implications of the findings are of particular interest to policymakers who develop policies in the field of passenger transport and also to transport scientists and students.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145142157",
                        "name": "S. Myronenko"
                    },
                    {
                        "authorId": "103419590",
                        "name": "H. Oborskyi"
                    },
                    {
                        "authorId": "2213539666",
                        "name": "Dmytro Dmytryshyn"
                    },
                    {
                        "authorId": "2218280687",
                        "name": "Vyacheslav Shobik"
                    },
                    {
                        "authorId": "144702836",
                        "name": "D. Lauwers"
                    },
                    {
                        "authorId": "1702946",
                        "name": "F. Witlox"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "They analyze the connection between cell relationships using a graph neural network [29,2,34,18]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "eca4dfc1703529bd87e1b4fea1fad1baa06e0ac9",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2305-00630",
                    "ArXiv": "2305.00630",
                    "DOI": "10.48550/arXiv.2305.00630",
                    "CorpusId": 258427145
                },
                "corpusId": 258427145,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/eca4dfc1703529bd87e1b4fea1fad1baa06e0ac9",
                "title": "TRACE: Table Reconstruction Aligned to Corner and Edges",
                "abstract": "A table is an object that captures structured and informative content within a document, and recognizing a table in an image is challenging due to the complexity and variety of table layouts. Many previous works typically adopt a two-stage approach; (1) Table detection(TD) localizes the table region in an image and (2) Table Structure Recognition(TSR) identifies row- and column-wise adjacency relations between the cells. The use of a two-stage approach often entails the consequences of error propagation between the modules and raises training and inference inefficiency. In this work, we analyze the natural characteristics of a table, where a table is composed of cells and each cell is made up of borders consisting of edges. We propose a novel method to reconstruct the table in a bottom-up manner. Through a simple process, the proposed method separates cell boundaries from low-level features, such as corners and edges, and localizes table positions by combining the cells. A simple design makes the model easier to train and requires less computation than previous two-stage methods. We achieve state-of-the-art performance on the ICDAR2013 table competition benchmark and Wired Table in the Wild(WTW) dataset.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2057310228",
                        "name": "Youngmin Baek"
                    },
                    {
                        "authorId": "2064808754",
                        "name": "Daehyun Nam"
                    },
                    {
                        "authorId": "10787779",
                        "name": "Jaeheung Surh"
                    },
                    {
                        "authorId": "2111068603",
                        "name": "Seung Shin"
                    },
                    {
                        "authorId": "2109603647",
                        "name": "Seonghyeon Kim"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[27] [28] [29] [30] share the identical GNN structure."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "df9045c28a2aa3e7fc76edb3620424ad19912265",
                "externalIds": {
                    "ArXiv": "2304.11810",
                    "DBLP": "journals/corr/abs-2304-11810",
                    "DOI": "10.48550/arXiv.2304.11810",
                    "CorpusId": 258298306
                },
                "corpusId": 258298306,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/df9045c28a2aa3e7fc76edb3620424ad19912265",
                "title": "PARAGRAPH2GRAPH: A GNN-based framework for layout paragraph analysis",
                "abstract": "Document layout analysis has a wide range of requirements across various domains, languages, and business scenarios. However, most current state-of-the-art algorithms are language-dependent, with architectures that rely on transformer encoders or language-specific text encoders, such as BERT, for feature extraction. These approaches are limited in their ability to handle very long documents due to input sequence length constraints and are closely tied to language-specific tokenizers. Additionally, training a cross-language text encoder can be challenging due to the lack of labeled multilingual document datasets that consider privacy. Furthermore, some layout tasks require a clean separation between different layout components without overlap, which can be difficult for image segmentation-based algorithms to achieve. In this paper, we present Paragraph2Graph, a language-independent graph neural network (GNN)-based model that achieves competitive results on common document layout datasets while being adaptable to business scenarios with strict separation. With only 19.95 million parameters, our model is suitable for industrial applications, particularly in multi-language scenarios.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2042743634",
                        "name": "Shuyong Wei"
                    },
                    {
                        "authorId": "73580202",
                        "name": "Nuo Xu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The source codes of 6 papers were not executable [30, 33, 34, 36, 38, 39]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "50903f5f5e714e5746152b3e369c9329655e0018",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2304-10439",
                    "ArXiv": "2304.10439",
                    "DOI": "10.48550/arXiv.2304.10439",
                    "CorpusId": 258236507
                },
                "corpusId": 258236507,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/50903f5f5e714e5746152b3e369c9329655e0018",
                "title": "A Study on Reproducibility and Replicability of Table Structure Recognition Methods",
                "abstract": "Concerns about reproducibility in artificial intelligence (AI) have emerged, as researchers have reported unsuccessful attempts to directly reproduce published findings in the field. Replicability, the ability to affirm a finding using the same procedures on new data, has not been well studied. In this paper, we examine both reproducibility and replicability of a corpus of 16 papers on table structure recognition (TSR), an AI task aimed at identifying cell locations of tables in digital documents. We attempt to reproduce published results using codes and datasets provided by the original authors. We then examine replicability using a dataset similar to the original as well as a new dataset, GenTSR, consisting of 386 annotated tables extracted from scientific papers. Out of 16 papers studied, we reproduce results consistent with the original in only four. Two of the four papers are identified as replicable using the similar dataset under certain IoU values. No paper is identified as replicable using the new dataset. We offer observations on the causes of irreproducibility and irreplicability. All code and data are available on Codeocean at https://codeocean.com/capsule/6680116/tree.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "11359490",
                        "name": "Kehinde E. Ajayi"
                    },
                    {
                        "authorId": "2214808132",
                        "name": "Muntabhir Hasan Choudhury"
                    },
                    {
                        "authorId": "2528276",
                        "name": "S. Rajtmajer"
                    },
                    {
                        "authorId": "97569165",
                        "name": "Jian Wu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "31a7fccdd3547693a0ed6721c627cbcba31018f6",
                "externalIds": {
                    "DBLP": "journals/prl/Granell0PAQS023",
                    "DOI": "10.1016/j.patrec.2023.04.007",
                    "CorpusId": 258262078
                },
                "corpusId": 258262078,
                "publicationVenue": {
                    "id": "f35e3e87-9df4-497b-aa0d-bb8584197290",
                    "name": "Pattern Recognition Letters",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit Lett"
                    ],
                    "issn": "0167-8655",
                    "url": "https://www.journals.elsevier.com/pattern-recognition-letters/",
                    "alternate_urls": [
                        "http://www.journals.elsevier.com/pattern-recognition-letters/",
                        "http://www.sciencedirect.com/science/journal/01678655"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/31a7fccdd3547693a0ed6721c627cbcba31018f6",
                "title": "Processing a large collection of historical tabular images",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "21736158",
                        "name": "Emilio Granell"
                    },
                    {
                        "authorId": "144340605",
                        "name": "Ver\u00f3nica Romero"
                    },
                    {
                        "authorId": "2029290748",
                        "name": "J. R. Prieto"
                    },
                    {
                        "authorId": "50143831",
                        "name": "J. Andr\u00e9s"
                    },
                    {
                        "authorId": "40576342",
                        "name": "Lorenzo Quir\u00f3s"
                    },
                    {
                        "authorId": "1928123",
                        "name": "Joan Andreu S\u00e1nchez"
                    },
                    {
                        "authorId": "2059593537",
                        "name": "Enrique Vidal"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3e0c97baa8e7b629a3d689e00aa21b94e2193766",
                "externalIds": {
                    "ArXiv": "2303.11615",
                    "DBLP": "journals/corr/abs-2303-11615",
                    "DOI": "10.48550/arXiv.2303.11615",
                    "CorpusId": 257636821
                },
                "corpusId": 257636821,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/3e0c97baa8e7b629a3d689e00aa21b94e2193766",
                "title": "Robust Table Structure Recognition with Dynamic Queries Enhanced Detection Transformer",
                "abstract": "We present a new table structure recognition (TSR) approach, called TSRFormer, to robustly recognizing the structures of complex tables with geometrical distortions from various table images. Unlike previous methods, we formulate table separation line prediction as a line regression problem instead of an image segmentation problem and propose a new two-stage dynamic queries enhanced DETR based separation line regression approach, named DQ-DETR, to predict separation lines from table images directly. Compared to Vallina DETR, we propose three improvements in DQ-DETR to make the two-stage DETR framework work efficiently and effectively for the separation line prediction task: 1) A new query design, named Dynamic Query, to decouple single line query into separable point queries which could intuitively improve the localization accuracy for regression tasks; 2) A dynamic queries based progressive line regression approach to progressively regressing points on the line which further enhances localization accuracy for distorted tables; 3) A prior-enhanced matching strategy to solve the slow convergence issue of DETR. After separation line prediction, a simple relation network based cell merging module is used to recover spanning cells. With these new techniques, our TSRFormer achieves state-of-the-art performance on several benchmark datasets, including SciTSR, PubTabNet, WTW and FinTabNet. Furthermore, we have validated the robustness and high localization accuracy of our approach to tables with complex structures, borderless cells, large blank spaces, empty or spanning cells as well as distorted or even curved shapes on a more challenging real-world in-house dataset.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2187553093",
                        "name": "Jiawei Wang"
                    },
                    {
                        "authorId": "2108811137",
                        "name": "Weihong Lin"
                    },
                    {
                        "authorId": "2894335",
                        "name": "Chixiang Ma"
                    },
                    {
                        "authorId": "2181111465",
                        "name": "Mingze Li"
                    },
                    {
                        "authorId": "2145130513",
                        "name": "Zhengmao Sun"
                    },
                    {
                        "authorId": "2110833051",
                        "name": "Lei Sun"
                    },
                    {
                        "authorId": "2316043",
                        "name": "Qiang Huo"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "With the fast-paced development of digital transformation, Table Structure Recognition (TSR) task, aiming at parsing table structure from a table image into machineinterpretable formats, which are often presented by both table cell physical coordinates [38, 31, 15, 41, 4, 34, 36, 45, 35, 23, 25] and their logical relationships [18, 47].",
                "The method [34] employs DGCNN [44] to model interactions between visual and geometric feature vertices for their cell/row/column relations prediction.",
                "the filed, which can be mainly categorized into boundary extraction-based methods [38, 31, 15, 41, 25] and element relationship-based methods [23, 4, 34, 36] according to the component type leveraged.",
                "The first group are the single component-based methods, which rely on either boundary extraction [15, 25, 31, 38, 41] or element relationship [4, 23, 34, 36].",
                "To date, several pioneers works [18, 47, 38, 15, 41, 4, 34, 36, 45, 35, 23, 25] have achieved significant progress in"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "2c267f43aca9bfb2f01e5400fca14545c4aa14b3",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-09174",
                    "ArXiv": "2303.09174",
                    "DOI": "10.48550/arXiv.2303.09174",
                    "CorpusId": 257557431
                },
                "corpusId": 257557431,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/2c267f43aca9bfb2f01e5400fca14545c4aa14b3",
                "title": "Grab What You Need: Rethinking Complex Table Structure Recognition with Flexible Components Deliberation",
                "abstract": "Recently, Table Structure Recognition (TSR) task, aiming at identifying table structure into machine readable formats, has received increasing interest in the community. While impressive success, most single table component-based methods can not perform well on unregularized table cases distracted by not only complicated inner structure but also exterior capture distortion. In this paper, we raise it as Complex TSR problem, where the performance degeneration of existing methods is attributable to their inefficient component usage and redundant post-processing. To mitigate it, we shift our perspective from table component extraction towards the efficient multiple components leverage, which awaits further exploration in the field. Specifically, we propose a seminal method, termed GrabTab, equipped with newly proposed Component Deliberator. Thanks to its progressive deliberation mechanism, our GrabTab can flexibly accommodate to most complex tables with reasonable components selected but without complicated post-processing involved. Quantitative experimental results on public benchmarks demonstrate that our method significantly outperforms the state-of-the-arts, especially under more challenging scenes.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2216764712",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "2153898418",
                        "name": "Xin Li"
                    },
                    {
                        "authorId": "2150155841",
                        "name": "Ming Gong"
                    },
                    {
                        "authorId": "47655556",
                        "name": "Bin Liu"
                    },
                    {
                        "authorId": "2164224877",
                        "name": "Yunfei Wu"
                    },
                    {
                        "authorId": "3336726",
                        "name": "Deqiang Jiang"
                    },
                    {
                        "authorId": "2136370566",
                        "name": "Yinsong Liu"
                    },
                    {
                        "authorId": "143900241",
                        "name": "Xing Sun"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Recently, graph neural networks are also used for table structure recognition by encoding document images as graphs (Qasim et al., 2019)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "968b0d964e0639ae27a49b696cf11929313e9ff1",
                "externalIds": {
                    "ArXiv": "2303.08648",
                    "DBLP": "conf/visapp/LyT23",
                    "DOI": "10.5220/0011685000003417",
                    "CorpusId": 257360026
                },
                "corpusId": 257360026,
                "publicationVenue": {
                    "id": "3e53351c-7355-4159-a3f2-f3b03a3aa989",
                    "name": "VISIGRAPP",
                    "type": "conference",
                    "alternate_names": [
                        "Int Jt Conf Comput Vis Imaging Comput Graph Theory Appl",
                        "International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications"
                    ],
                    "issn": "2184-4321",
                    "alternate_issns": [
                        "2184-5921"
                    ],
                    "url": "http://www.visigrapp.org/"
                },
                "url": "https://www.semanticscholar.org/paper/968b0d964e0639ae27a49b696cf11929313e9ff1",
                "title": "An End-to-End Multi-Task Learning Model for Image-based Table Recognition",
                "abstract": "Image-based table recognition is a challenging task due to the diversity of table styles and the complexity of table structures. Most of the previous methods focus on a non-end-to-end approach which divides the problem into two separate sub-problems: table structure recognition; and cell-content recognition and then attempts to solve each sub-problem independently using two separate systems. In this paper, we propose an end-to-end multi-task learning model for image-based table recognition. The proposed model consists of one shared encoder, one shared decoder, and three separate decoders which are used for learning three sub-tasks of table recognition: table structure recognition, cell detection, and cell-content recognition. The whole system can be easily trained and inferred in an end-to-end approach. In the experiments, we evaluate the performance of the proposed model on two large-scale datasets: FinTabNet and PubTabNet. The experiment results show that the proposed model outperforms the state-of-the-art methods in all benchmark datasets.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "40325274",
                        "name": "N. Ly"
                    },
                    {
                        "authorId": "144752372",
                        "name": "A. Takasu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Some other researchers aimed to classify the cell relationship to construct table structure [3], [35], [20], [52]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5966cd3ac4460a97b27b4177a89c0d1041726881",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2303-06949",
                    "ArXiv": "2303.06949",
                    "DOI": "10.1109/CVPR52729.2023.01071",
                    "CorpusId": 257495840
                },
                "corpusId": 257495840,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5966cd3ac4460a97b27b4177a89c0d1041726881",
                "title": "Improving Table Structure Recognition with Visual-Alignment Sequential Coordinate Modeling",
                "abstract": "Table structure recognition aims to extract the logical and physical structure of unstructured table images into a machine-readable format. The latest end-to-end image-to-text approaches simultaneously predict the two structures by two decoders, where the prediction of the physical structure (the bounding boxes of the cells) is based on the representation of the logical structure. However, the previous methods struggle with imprecise bounding boxes as the logical representation lacks local visual information. To address this issue, we propose an end-to-end sequential modeling framework for table structure recognition called VAST. It contains a novel coordinate sequence decoder triggered by the representation of the non-empty cell from the logical structure decoder. In the coordinate sequence decoder, we model the bounding box coordinates as a language sequence, where the left, top, right and bottom coordinates are decoded sequentially to leverage the inter-coordinate dependency. Furthermore, we propose an auxiliary visual-alignment loss to enforce the logical representation of the non-empty cells to contain more local visual details, which helps produce better cell bounding boxes. Extensive experiments demonstrate that our proposed method can achieve state-of-the-art results in both logical and physical structure recognition. The ablation study also validates that the proposed coordinate sequence decoder and the visual-alignment loss are the keys to the success of our method.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "145769099",
                        "name": "Yongshuai Huang"
                    },
                    {
                        "authorId": "2054645836",
                        "name": "Ning Lu"
                    },
                    {
                        "authorId": "1679279",
                        "name": "Dapeng Chen"
                    },
                    {
                        "authorId": "2110470261",
                        "name": "Yibo Li"
                    },
                    {
                        "authorId": "2002678",
                        "name": "Zecheng Xie"
                    },
                    {
                        "authorId": "2901597",
                        "name": "Shenggao Zhu"
                    },
                    {
                        "authorId": "1777642",
                        "name": "Liangcai Gao"
                    },
                    {
                        "authorId": "2187312795",
                        "name": "Wei Peng"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "One group of bottom-up methods [3, 23, 32] treat words or cell contents as nodes in a graph and use graph neural networks to predict whether each sampled node pair is in the same cell, row, or column."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c78daabab3666d08d945098bc462f882b78803fd",
                "externalIds": {
                    "ArXiv": "2303.04384",
                    "DBLP": "journals/corr/abs-2303-04384",
                    "DOI": "10.48550/arXiv.2303.04384",
                    "CorpusId": 257405340
                },
                "corpusId": 257405340,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/c78daabab3666d08d945098bc462f882b78803fd",
                "title": "SEMv2: Table Separation Line Detection Based on Conditional Convolution",
                "abstract": "Table structure recognition is an indispensable element for enabling machines to comprehend tables. Its primary purpose is to identify the internal structure of a table. Nevertheless, due to the complexity and diversity of their structure and style, it is highly challenging to parse the tabular data into a structured format that machines can comprehend. In this work, we adhere to the principle of the split-and-merge based methods and propose an accurate table structure recognizer, termed SEMv2 (SEM: Split, Embed and Merge). Unlike the previous works in the ``split'' stage, we aim to address the table separation line instance-level discrimination problem and introduce a table separation line detection strategy based on conditional convolution. Specifically, we design the ``split'' in a top-down manner that detects the table separation line instance first and then dynamically predicts the table separation line mask for each instance. The final table separation line shape can be accurately obtained by processing the table separation line mask in a row-wise/column-wise manner. To comprehensively evaluate the SEMv2, we also present a more challenging dataset for table structure recognition, dubbed iFLYTAB, which encompasses multiple style tables in various scenarios such as photos, scanned documents, etc. Extensive experiments on publicly available datasets (e.g. SciTSR, PubTabNet and iFLYTAB) demonstrate the efficacy of our proposed approach. The code and iFLYTAB dataset will be made publicly available upon acceptance of this paper.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2128660562",
                        "name": "Zhenrong Zhang"
                    },
                    {
                        "authorId": "2067770685",
                        "name": "Pengfei Hu"
                    },
                    {
                        "authorId": "2143520841",
                        "name": "Jie Ma"
                    },
                    {
                        "authorId": "2149030934",
                        "name": "Jun Du"
                    },
                    {
                        "authorId": "39557762",
                        "name": "Jianshu Zhang"
                    },
                    {
                        "authorId": "2211065581",
                        "name": "Huihui Zhu"
                    },
                    {
                        "authorId": "2055464704",
                        "name": "Baocai Yin"
                    },
                    {
                        "authorId": "2185098372",
                        "name": "Bing Yin"
                    },
                    {
                        "authorId": "2108152462",
                        "name": "Cong Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "\u2026mechanism (Vaswani et al. 2017) in LORE to avoid making additional assumptions about the distribution of table structure, rather than graph neural networks employed by previous methods (Qasim, Mahmood, and Shafait 2019; Xue et al. 2021), which will be further discussed in experiments.",
                "Previous methods employ heuristic rules based on spatial locations (Liu et al. 2022) or graph optimizations (Qasim, Mahmood, and Shafait 2019) to reconstruct the tables.",
                "Following this work, there are models devoted to improving the relationship classification by using elaborated neural networks and adding multi-modal features (Qasim, Mahmood, and Shafait 2019; Raja, Mondal, and Jawahar 2020, 2022; Liu et al. 2021, 2022).",
                "In experiment 2a, we replace the self-attention encoder with a graph-attention encoder similar to graph-based TSR models (Qasim, Mahmood, and Shafait 2019; Xue et al. 2021) with an equal amount of parameters with LORE."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "a90a77893708ae5070e2ed4572ca9e20837e69c6",
                "externalIds": {
                    "DBLP": "conf/aaai/XingGLBZLYY23",
                    "ArXiv": "2303.03730",
                    "DOI": "10.48550/arXiv.2303.03730",
                    "CorpusId": 257378294
                },
                "corpusId": 257378294,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/a90a77893708ae5070e2ed4572ca9e20837e69c6",
                "title": "LORE: Logical Location Regression Network for Table Structure Recognition",
                "abstract": "Table structure recognition (TSR) aims at extracting tables in images into machine-understandable formats. Recent methods solve this problem by predicting the adjacency relations of detected cell boxes, or learning to generate the corresponding markup sequences from the table images. However, they either count on additional heuristic rules to recover the table structures, or require a huge amount of training data and time-consuming sequential decoders. In this paper, we propose an alternative paradigm. We model TSR as a logical location regression problem and propose a new TSR framework called LORE, standing for LOgical location REgression network, which for the first time combines logical location regression together with spatial location regression of table cells. Our proposed LORE is conceptually simpler, easier to train and more accurate than previous TSR models of other paradigms. Experiments on standard benchmarks demonstrate that LORE consistently outperforms prior arts. Code is available at https:// github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/LORE-TSR.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "2176894025",
                        "name": "Hangdi Xing"
                    },
                    {
                        "authorId": "2112256",
                        "name": "Feiyu Gao"
                    },
                    {
                        "authorId": "143799988",
                        "name": "Rujiao Long"
                    },
                    {
                        "authorId": "2064698184",
                        "name": "Jiajun Bu"
                    },
                    {
                        "authorId": "2114172383",
                        "name": "Qi Zheng"
                    },
                    {
                        "authorId": "2145730944",
                        "name": "Liangcheng Li"
                    },
                    {
                        "authorId": "2173739231",
                        "name": "Cong Yao"
                    },
                    {
                        "authorId": "2139424603",
                        "name": "Zhi Yu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[6] used a convolutional neural network [7] and graph neural network (GNN) [8] to identify table structures, the former for extracting image features and the latter for improving the correlation between vertices."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "5c3c2ac444019a96edff6ac60329aea3ef43d359",
                "externalIds": {
                    "DOI": "10.3390/electronics12030673",
                    "CorpusId": 256454161
                },
                "corpusId": 256454161,
                "publicationVenue": {
                    "id": "ccd8e532-73c6-414f-bc91-271bbb2933e2",
                    "name": "Electronics",
                    "type": "journal",
                    "issn": "1450-5843",
                    "alternate_issns": [
                        "2079-9292",
                        "0883-4989"
                    ],
                    "url": "http://www.electronics.etfbl.net/",
                    "alternate_urls": [
                        "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-247562",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-247562",
                        "https://www.mdpi.com/journal/electronics"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/5c3c2ac444019a96edff6ac60329aea3ef43d359",
                "title": "Table Structure Recognition Method Based on Lightweight Network and Channel Attention",
                "abstract": "The table recognition model rows and columns aggregated network (RCANet) uses a semantic segmentation approach to recognize table structure, and achieves better performance in table row and column segmentation. However, this model uses ResNet18 as the backbone network, and the model has 11.35 million parameters and a volume of 45.5 M, which is inconvenient to deploy to lightweight servers or mobile terminals. Therefore, from the perspective of model compression, this paper proposes the lightweight rows and columns attention aggregated network (LRCAANet), which uses the lightweight network ShuffleNetv2 to replace the original RCANet backbone network ResNet18 to simplify the model size. Considering that the lightweight network reduces the number of feature channels, it has a certain impact on the performance of the model. In order to strengthen the learning between feature channels, the rows attention aggregated (RAA) module and the columns attention aggregated (CAA) module are proposed. The RAA module and the CAA module add the squeeze and excitation (SE) module to the original row and column aggregated modules, respectively. Adding the SE module means the model can learn the correlation between channels and improve the prediction effect of the lightweight model. The experimental results show that our method greatly reduces the model parameters and model volume while ensuring low-performance loss. In the end, the average F1 score of our model is only 1.77% lower than the original model, the parameters are only 0.17 million, and the volume is only 0.8 M. Compared with the original model, the parameter amount and volume are reduced by more than 95%.",
                "year": 2023,
                "authors": [
                    {
                        "authorId": "103245682",
                        "name": "T. Zhang"
                    },
                    {
                        "authorId": "48184603",
                        "name": "Yi Sui"
                    },
                    {
                        "authorId": "1821383",
                        "name": "Shunyao Wu"
                    },
                    {
                        "authorId": "2096258",
                        "name": "Fengjing Shao"
                    },
                    {
                        "authorId": "39447552",
                        "name": "Rencheng Sun"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ca2fabed8604b296d713262794a427e5c4b51ffa",
                "externalIds": {
                    "DBLP": "journals/apin/WanZLZS23",
                    "DOI": "10.1007/s10489-022-04420-4",
                    "CorpusId": 255362168
                },
                "corpusId": 255362168,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ca2fabed8604b296d713262794a427e5c4b51ffa",
                "title": "Contextual transformer sequence-based recognition network for medical examination reports",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "152912519",
                        "name": "Honglin Wan"
                    },
                    {
                        "authorId": "2199057485",
                        "name": "Zongfeng Zhong"
                    },
                    {
                        "authorId": "2263987",
                        "name": "Tianping Li"
                    },
                    {
                        "authorId": "2856513",
                        "name": "Huaxiang Zhang"
                    },
                    {
                        "authorId": "51299154",
                        "name": "Jiande Sun"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ae6e34d182fbc42bc99ba609369a3b7645d205cb",
                "externalIds": {
                    "DBLP": "journals/prl/WangXZJ23",
                    "DOI": "10.1016/j.patrec.2022.12.014",
                    "CorpusId": 255089449
                },
                "corpusId": 255089449,
                "publicationVenue": {
                    "id": "f35e3e87-9df4-497b-aa0d-bb8584197290",
                    "name": "Pattern Recognition Letters",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit Lett"
                    ],
                    "issn": "0167-8655",
                    "url": "https://www.journals.elsevier.com/pattern-recognition-letters/",
                    "alternate_urls": [
                        "http://www.journals.elsevier.com/pattern-recognition-letters/",
                        "http://www.sciencedirect.com/science/journal/01678655"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/ae6e34d182fbc42bc99ba609369a3b7645d205cb",
                "title": "Scene table structure recognition with segmentation collaboration and alignment",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109799388",
                        "name": "Hongyi Wang"
                    },
                    {
                        "authorId": "46364544",
                        "name": "Yang Xue"
                    },
                    {
                        "authorId": "2118131879",
                        "name": "Jiaxin Zhang"
                    },
                    {
                        "authorId": "144838978",
                        "name": "Lianwen Jin"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Recently, many studies have explored table extraction by solely leveraging neural networks (NNs) [14]\u2013[16]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e604f4ec9863e900fda0dd0a81b60362f220266c",
                "externalIds": {
                    "DBLP": "journals/tcad/FayaziCYBAD22",
                    "DOI": "10.1109/TCAD.2022.3158073",
                    "CorpusId": 247426766
                },
                "corpusId": 247426766,
                "publicationVenue": {
                    "id": "e86c30b0-c1dd-4f0e-be5e-22af711f7d5f",
                    "name": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
                    "type": "journal",
                    "alternate_names": [
                        "IEEE Trans Comput Des Integr Circuit Syst"
                    ],
                    "issn": "0278-0070",
                    "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=43",
                    "alternate_urls": [
                        "http://ieee-ceda.org/publications/tcad",
                        "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=43"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e604f4ec9863e900fda0dd0a81b60362f220266c",
                "title": "FASCINET: A Fully Automated Single-Board Computer Generator Using Neural Networks",
                "abstract": "Designing single-board computers (SBCs) is becoming more challenging given the growing number of discrete components that are made available and the rate at which this number grows. Keeping track of all available components options, revisions, and functionalities is challenging for SBC designers who are striving for faster design cycles. Moreover, the procedure of deciding peripheral components, their values, and connections of an SBC is not only difficult because of various parameters that need to be considered but also is time consuming as there exist numerous components on a typical SBC nowadays. In this article, an SBC generator tool, FASCINET, is presented that uses a neural network (NN) model to design customized peripheral circuits for SBCs. The tool creates a large commercial off-the-shelf database (COTS DB) of existing components, efficiently searches through them, and selects optimal components for both main and peripheral components based on the user\u2019s requirements. Creating such a broad COTS DB requires processing abundant datasheets. A manual approach is time consuming, even if only a fraction of all available datasheets is considered. In order to automate this process, this article describes a novel NN-based approach for automatically categorizing datasheets and proposes an extraction technique for parsing relevant functional information from tables within. Our evaluation using a test set that contains over 770 000 components shows that the category of datasheets is identified correctly over 95% of the time. Additionally, the table extractor has a precision above 96%. Our proposed fully autonomous SBC design approach reduces the time for generating the schematic of an SBC to as little as 2 min. For validating the accuracy of our model, the netlists of 400 SBCs designed by FASCINET are compared to the human-designed versions. This evaluation shows that FASCINET is able to design SBCs that are identical to the manually designed ones except for minor differences.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "33935286",
                        "name": "Morteza Fayazi"
                    },
                    {
                        "authorId": "2087607913",
                        "name": "Zachary Colter"
                    },
                    {
                        "authorId": "7936000",
                        "name": "Z. Youbi"
                    },
                    {
                        "authorId": "27730985",
                        "name": "Javad Bagherzadeh"
                    },
                    {
                        "authorId": "35554623",
                        "name": "T. Ajayi"
                    },
                    {
                        "authorId": "1793651",
                        "name": "R. Dreslinski"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "SR Qasim [115] Graph NN + CNN 1) This paper also presents a unique, memoryefficient training strategy based on Monte Carlo.",
                "SR Qasim [115] presents a graph network-based architecture for table recognition as a superior alternative to typical neural networks."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a81f504a1a101b9c6fe4a88957dd9ec450cf389d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2211-08469",
                    "ArXiv": "2211.08469",
                    "DOI": "10.48550/arXiv.2211.08469",
                    "CorpusId": 253553399
                },
                "corpusId": 253553399,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a81f504a1a101b9c6fe4a88957dd9ec450cf389d",
                "title": "Deep learning for table detection and structure recognition: A survey",
                "abstract": "Tables are everywhere, from scientific journals, papers, websites, and newspapers all the way to items we buy at the supermarket. Detecting them is thus of utmost importance to automatically understanding the content of a document. The performance of table detection has substantially increased thanks to the rapid development of deep learning networks. The goals of this survey are to provide a profound comprehension of the major developments in the field of Table Detection, offer insight into the different methodologies, and provide a systematic taxonomy of the different approaches. Furthermore, we provide an analysis of both classic and new applications in the field. Lastly, the datasets and source code of the existing models are organized to provide the reader with a compass on this vast literature. Finally, we go over the architecture of utilizing various object detection and table structure recognition methods to create an effective and efficient system, as well as a set of development trends to keep up with state-of-the-art algorithms and future research. We have also set up a public GitHub repository where we will be updating the most recent publications, open data, and source code. The GitHub repository is available at https://github.com/abdoelsayed2016/ table-detection-structure-recognition. Preprint submitted to Elsevier November 17, 2022 ar X iv :2 21 1. 08 46 9v 1 [ cs .C V ] 1 5 N ov 2 02 2",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "87034581",
                        "name": "M. Kasem"
                    },
                    {
                        "authorId": "115665360",
                        "name": "Abdelrahman Abdallah"
                    },
                    {
                        "authorId": "2115948777",
                        "name": "Alexander Berendeyev"
                    },
                    {
                        "authorId": "2190955162",
                        "name": "Ebrahem Elkady"
                    },
                    {
                        "authorId": "2190955581",
                        "name": "Mahmoud Abdalla"
                    },
                    {
                        "authorId": "2163883464",
                        "name": "Mohamed Mahmoud"
                    },
                    {
                        "authorId": "2057870328",
                        "name": "Mohamed Hamada"
                    },
                    {
                        "authorId": "3253700",
                        "name": "D. Nurseitov"
                    },
                    {
                        "authorId": "1405340824",
                        "name": "I. Taj-Eddin"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "They confirm their approach to the artificial table pictures [25] [26]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e35ef6f4cfdc2319a641735608562d3886fa9fdd",
                "externalIds": {
                    "DOI": "10.1109/ICCCIS56430.2022.10037664",
                    "CorpusId": 256743427
                },
                "corpusId": 256743427,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/e35ef6f4cfdc2319a641735608562d3886fa9fdd",
                "title": "Using CoordConv for Tabular Data Detection and Structure Recognition",
                "abstract": "This paper explores the usage of CoordConv, the novel upgrade to general convolutional layers in the problem of Tabular Data Detection and Cell-Based Structure Recognition. CoordConv has been shown to provide considerably better results in the domain of Object Detection than its counterpart. The authors integrate it within the established Anchor optimization approach which leverages guided anchors to accomplish the task of recognizing rows and columns present in tabular data. In contrast to the majority of techniques implemented for Table Structure Recognition, the authors attempt to recognize the cells present in the tabular images instead of the rows and columns. They evaluate this method on the coveted ICDAR-19 dataset (International Conference on Document Analysis and Recognition - 2019) which comprises of scanned document images containing tabular regions and achieve results surpassing those of many popular techniques. They also apply this approach for the task of Table Detection and achieve results comparable to other established techniques.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2205330992",
                        "name": "Apoorva Ambulgekar"
                    },
                    {
                        "authorId": "2205330976",
                        "name": "Naman Lad"
                    },
                    {
                        "authorId": "2183415732",
                        "name": "Krunal Doshi"
                    },
                    {
                        "authorId": "2128946657",
                        "name": "Pranit Bari"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "525c0df4767d10165f172bf5c2259b01840379b0",
                "externalIds": {
                    "DOI": "10.1016/j.nucengdes.2022.111975",
                    "CorpusId": 252634797
                },
                "corpusId": 252634797,
                "publicationVenue": {
                    "id": "26378f21-f2eb-464e-a5c1-6806942f4e7c",
                    "name": "Nuclear Engineering and Design",
                    "type": "journal",
                    "alternate_names": [
                        "Nucl Eng Des"
                    ],
                    "issn": "0029-5493",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/505661/description#description",
                    "alternate_urls": [
                        "http://www.sciencedirect.com/science/journal/00295493",
                        "https://www.journals.elsevier.com/nuclear-engineering-and-design"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/525c0df4767d10165f172bf5c2259b01840379b0",
                "title": "Automatic recognition system for document digitization in nuclear power plants",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1753041373",
                        "name": "Elisa Ou"
                    },
                    {
                        "authorId": "2110188342",
                        "name": "Minhee Kim"
                    },
                    {
                        "authorId": "31888158",
                        "name": "Po-Ling Loh"
                    },
                    {
                        "authorId": "49973083",
                        "name": "Todd R. Allen"
                    },
                    {
                        "authorId": "51940545",
                        "name": "R. Agasie"
                    },
                    {
                        "authorId": "2478267",
                        "name": "Kaibo Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[34, 37, 45, 66] treat these cells as nodes in a graph and train another Graph Neural Network (GNN) to predict the relations."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4ebf44ebb9634cb5c60f5dc7fab0c0f84aed0939",
                "externalIds": {
                    "DBLP": "conf/mm/LiL0LCNPL22",
                    "DOI": "10.1145/3503161.3547885",
                    "CorpusId": 252782335
                },
                "corpusId": 252782335,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/4ebf44ebb9634cb5c60f5dc7fab0c0f84aed0939",
                "title": "End-to-End Compound Table Understanding with Multi-Modal Modeling",
                "abstract": "Table is a widely used data form in webpages, spreadsheets, or PDFs to organize and present structural data. Although studies on table structure recognition have been successfully used to convert image-based tables into digital structural formats, solving many real problems still relies on further understanding of the table, such as cell relationship extraction. The current datasets related to table understanding are all based on the digit format. To boost research development, we release a new benchmark named ComFinTab with rich annotations that support both table recognition and understanding tasks. Unlike previous datasets containing the basic tables, ComFinTab contains a large ratio of compound tables, which is much more challenging and requires methods using multiple information sources. Based on the dataset, we also propose a uniform, concise task form with the evaluation metric to better evaluate the model's performance on the table understanding task in compound tables. Finally, a framework named CTUNet is proposed to integrate the compromised visual, semantic, and position features with a graph attention network, which can solve the table recognition task and the challenging table understanding task as a whole. Experimental results compared with some previous advanced table understanding methods demonstrate the effectiveness of our proposed model. Code and dataset are available at \\urlhttps://github.com/hikopensource/DAVAR-Lab-OCR.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109614687",
                        "name": "Zaisheng Li"
                    },
                    {
                        "authorId": "2187429408",
                        "name": "Yi Li"
                    },
                    {
                        "authorId": "2187308758",
                        "name": "Qiao Liang"
                    },
                    {
                        "authorId": "2144902872",
                        "name": "Pengfei Li"
                    },
                    {
                        "authorId": "2398015",
                        "name": "Zhanzhan Cheng"
                    },
                    {
                        "authorId": "1490934795",
                        "name": "Yi Niu"
                    },
                    {
                        "authorId": "3290437",
                        "name": "Shiliang Pu"
                    },
                    {
                        "authorId": "2116227961",
                        "name": "Xi Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "For line detection, we take advantage of recent GNN proposals such as (Qasim et al., 2019) or (Carbonell et al.",
                "To do this, a GNN based on (Qasim et al., 2019) is implemented with the aim of connecting the different words previously extracted as entity tags of interest in a same line.",
                "For line detection, we take advantage of recent GNN proposals such as (Qasim et al., 2019) or (Carbonell et al., 2021)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "122d96039147e6b68b2290391923d65fa4722b17",
                "externalIds": {
                    "ACL": "2022.pandl-1.2",
                    "DBLP": "journals/corr/abs-2210-03453",
                    "ArXiv": "2210.03453",
                    "DOI": "10.48550/arXiv.2210.03453",
                    "CorpusId": 252762533
                },
                "corpusId": 252762533,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/122d96039147e6b68b2290391923d65fa4722b17",
                "title": "Key Information Extraction in Purchase Documents using Deep Learning and Rule-based Corrections",
                "abstract": "Deep Learning (DL) is dominating the fields of Natural Language Processing (NLP) and Computer Vision (CV) in the recent times. However, DL commonly relies on the availability of large data annotations, so other alternative or complementary pattern-based techniques can help to improve results. In this paper, we build upon Key Information Extraction (KIE) in purchase documents using both DL and rule-based corrections. Our system initially trusts on Optical Character Recognition (OCR) and text understanding based on entity tagging to identify purchase facts of interest (e.g., product codes, descriptions, quantities, or prices). These facts are then linked to a same product group, which is recognized by means of line detection and some grouping heuristics. Once these DL approaches are processed, we contribute several mechanisms consisting of rule-based corrections for improving the baseline DL predictions. We prove the enhancements provided by these rule-based corrections over the baseline DL results in the presented experiments for purchase documents from public and NielsenIQ datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "144347549",
                        "name": "R. Arroyo"
                    },
                    {
                        "authorId": "4428108",
                        "name": "J. Yebes"
                    },
                    {
                        "authorId": "2072672081",
                        "name": "E. Mart\u00ednez"
                    },
                    {
                        "authorId": "2226803463",
                        "name": "Hector Corrales"
                    },
                    {
                        "authorId": "2054752097",
                        "name": "Javier Lorenzo"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a0f11b53aa27ada8144812a47727768a5fb412e9",
                "externalIds": {
                    "DBLP": "conf/ictai/LuoHGQ22",
                    "DOI": "10.1109/ICTAI56018.2022.00079",
                    "CorpusId": 258221050
                },
                "corpusId": 258221050,
                "publicationVenue": {
                    "id": "ba1e9488-a629-4abe-a0c2-ec2c79c91616",
                    "name": "IEEE International Conference on Tools with Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Tool Artif Intell",
                        "ICTAI",
                        "IEEE Int Conf Tool Artif Intell",
                        "International Conference on Tools with Artificial Intelligence"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1488"
                },
                "url": "https://www.semanticscholar.org/paper/a0f11b53aa27ada8144812a47727768a5fb412e9",
                "title": "GTRNet: a graph-based table reconstructed network",
                "abstract": "Tabular data, with an exceedingly effective data structure, can give us a more intuitive visual display. To under-stand well and make use of the spatial and logic dependencies of it, we propose an end-to-end, graph-based table reconstructed network, namely GTRNet, in this paper. Our model works differently from most existing models which treat tables as either a markup sequence problem or a graph structure of rows and columns. It can utilize a table as input and extract its features in the text, image and position coordinate to predict the dependencies of the text instances and well distinguish the spatial relationship to infer whether multiple text segments belong to the same merged cell. Optimized along with this network, we can then restore the structure of this table. Moreover, we also create a new Chinese benchmark dataset GraphTable for this task to tackle complex challenges on the table. The competitive results on ICDAR-2013, GraphTable, SciTSR and FinTab benchmarks further confirm the great effectiveness of GTRN et.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "27684556",
                        "name": "Yuchen Luo"
                    },
                    {
                        "authorId": "1390799037",
                        "name": "Zheng Huang"
                    },
                    {
                        "authorId": "2117062832",
                        "name": "Jie Guo"
                    },
                    {
                        "authorId": "40453004",
                        "name": "Weidong Qiu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In addition to object-detection-based methods, earlier methods leveraged the concepts of Fully Convolutional Networks (FCN) [43,44] and Graph Neural Networks (GNN) [37,45] to resolve table detection in document images."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c18b3479a389ae4bc955338a862e6bc4f68d6ac1",
                "externalIds": {
                    "DOI": "10.3390/app12188969",
                    "CorpusId": 252171274
                },
                "corpusId": 252171274,
                "publicationVenue": {
                    "id": "136edf8d-0f88-4c2c-830f-461c6a9b842e",
                    "name": "Applied Sciences",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Sci"
                    ],
                    "issn": "2076-3417",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-217814",
                    "alternate_urls": [
                        "http://www.mathem.pub.ro/apps/",
                        "https://www.mdpi.com/journal/applsci",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-217814"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c18b3479a389ae4bc955338a862e6bc4f68d6ac1",
                "title": "Continual Learning for Table Detection in Document Images",
                "abstract": "The growing amount of data demands methods that can gradually learn from new samples. However, it is not trivial to continually train a network. Retraining a network with new data usually results in a phenomenon called \u201ccatastrophic forgetting\u201d. In a nutshell, the performance of the model on the previous data drops by learning from the new instances. This paper explores this issue in the table detection problem. While there are multiple datasets and sophisticated methods for table detection, the utilization of continual learning techniques in this domain has not been studied. We employed an effective technique called experience replay and performed extensive experiments on several datasets to investigate the effects of catastrophic forgetting. The results show that our proposed approach mitigates the performance drop by 15 percent. To the best of our knowledge, this is the first time that continual learning techniques have been adopted for table detection, and we hope this stands as a baseline for future research.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1752929623",
                        "name": "Mohammad Minouei"
                    },
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "2757942",
                        "name": "M. Soheili"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "also use SynthTable proposed in TIES [4].",
                "We conduct experiments on several popular benchmarks including PubTabNet [3] and SynthTable [4], our method achieves new state-of-the-art results.",
                "We also evaluate the SynthTable dataset proposed in TIES [4] that mainly consists of tables in diverse categories.",
                "For example, TIES [4] combines CNN [17] and GNN [14] to construct a bottom-up model to recognize the table structure.",
                "Therefore, we\n6\nalso use SynthTable proposed in TIES [4]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "92132fb36fb3e470464551210926f256a1f37280",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-14687",
                    "ArXiv": "2208.14687",
                    "DOI": "10.48550/arXiv.2208.14687",
                    "CorpusId": 251953555
                },
                "corpusId": 251953555,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/92132fb36fb3e470464551210926f256a1f37280",
                "title": "TRUST: An Accurate and End-to-End Table structure Recognizer Using Splitting-based Transformers",
                "abstract": "Table structure recognition is a crucial part of document image analysis domain. Its difficulty lies in the need to parse the physical coordinates and logical indices of each cell at the same time. However, the existing methods are difficult to achieve both these goals, especially when the table splitting lines are blurred or tilted. In this paper, we propose an accurate and end-to-end transformer-based table structure recognition method, referred to as TRUST. Transformers are suitable for table structure recognition because of their global computations, perfect memory, and parallel computation. By introducing novel Transformer-based Query-based Splitting Module and Vertex-based Merging Module, the table structure recognition problem is decoupled into two joint optimization sub-tasks: multi-oriented table row/column splitting and table grid merging. The Query-based Splitting Module learns strong context information from long dependencies via Transformer networks, accurately predicts the multi-oriented table row/column separators, and obtains the basic grids of the table accordingly. The Vertex-based Merging Module is capable of aggregating local contextual information between adjacent basic grids, providing the ability to merge basic girds that belong to the same spanning cell accurately. We conduct experiments on several popular benchmarks including PubTabNet and SynthTable, our method achieves new state-of-the-art results. In particular, TRUST runs at 10 FPS on PubTabNet, surpassing the previous methods by a large margin.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2109309324",
                        "name": "Zengyuan Guo"
                    },
                    {
                        "authorId": "2117164666",
                        "name": "Yuecheng Yu"
                    },
                    {
                        "authorId": "25604699",
                        "name": "Pengyuan Lv"
                    },
                    {
                        "authorId": "1979323",
                        "name": "Chengquan Zhang"
                    },
                    {
                        "authorId": "2579920",
                        "name": "Haojie Li"
                    },
                    {
                        "authorId": "47196393",
                        "name": "Zhihui Wang"
                    },
                    {
                        "authorId": "2140371004",
                        "name": "Kun Yao"
                    },
                    {
                        "authorId": "2272123",
                        "name": "Jingtuo Liu"
                    },
                    {
                        "authorId": "2109534192",
                        "name": "Jingdong Wang"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "GNN for document understanding was first introduced for mainly key DLA sub-tasks that include table detection [25] and table structure recognition [23]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6357238a07f0904f8279832b59f5b387a4e827f2",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-11168",
                    "ArXiv": "2208.11168",
                    "DOI": "10.1007/978-3-031-25069-9_22",
                    "CorpusId": 251765056
                },
                "corpusId": 251765056,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6357238a07f0904f8279832b59f5b387a4e827f2",
                "title": "Doc2Graph: a Task Agnostic Document Understanding Framework based on Graph Neural Networks",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2182515911",
                        "name": "Andrea Gemelli"
                    },
                    {
                        "authorId": "2150473007",
                        "name": "Sanket Biswas"
                    },
                    {
                        "authorId": "2122367862",
                        "name": "Enrico Civitelli"
                    },
                    {
                        "authorId": "2117637297",
                        "name": "Josep Llad'os"
                    },
                    {
                        "authorId": "3285734",
                        "name": "S. Marinai"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Following ideas presented in Section II-C that have been proven successful for TD [36] and TSR [37] we enrich our graph nodes with positional and textual features.",
                "A more recent work [10] proposes another meaning for TE, providing TD, TSR, and\nTFA annotations.",
                "TSR in segmented tables is addressed in [37] by classifying edges for cells, rows, and columns.",
                "More recently, [38] reformulate the problem of TSR as an end-to-end table reconstruction through node classification.",
                "TSS is referred to as Table Structure Recognition (TSR) in [10] where the recognition of column and projected row headers is defined as Table Functional Analysis (TFA).",
                "The proposed framework can address also TSR, but implementation details are still under investigation.",
                "TSR in segmented tables is addressed in [36] by classifying edges for cells, rows, and columns.",
                "In this way it will be possible to compare this approach with both TD and TSR methods.",
                "Following ideas presented in Section II-C that have been proven successful for TD [35] and TSR [36] we enrich our graph nodes with positional and textual features."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "f3b6939ce0244e89656cf6ff93f6dff8b2bef547",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-11203",
                    "ArXiv": "2208.11203",
                    "DOI": "10.1109/ICPR56361.2022.9956590",
                    "CorpusId": 251765514
                },
                "corpusId": 251765514,
                "publicationVenue": {
                    "id": "48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                    "name": "International Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Pattern Recognit (ICPR Proc Int Conf",
                        "Int Conf Pattern Recognit",
                        "ICPR",
                        "International conference on pattern recognition",
                        "Int conf pattern recognit",
                        "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                    ],
                    "issn": "1041-3278",
                    "alternate_issns": [
                        "1051-4651"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=4740202"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/f3b6939ce0244e89656cf6ff93f6dff8b2bef547",
                "title": "Graph Neural Networks and Representation Embedding for Table Extraction in PDF Documents",
                "abstract": "Tables are widely used in several types of documents since they can bring important information in a structured way. In scientific papers, tables can sum up novel discoveries and summarize experimental results, making the research comparable and easily understandable by scholars. Several methods perform table analysis working on document images, losing useful information during the conversion from the PDF files since OCR tools can be prone to recognition errors, in particular for text inside tables. The main contribution of this work is to tackle the problem of table extraction, exploiting Graph Neural Networks. Node features are enriched with suitably designed representation embeddings. These representations help to better distinguish not only tables from the other parts of the paper, but also table cells from table headers. We experimentally evaluated the proposed approach on a new dataset obtained by merging the information provided in the PubLayNet and PubTables-1M datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2182515911",
                        "name": "Andrea Gemelli"
                    },
                    {
                        "authorId": "2182519264",
                        "name": "Emanuele Vivoli"
                    },
                    {
                        "authorId": "3285734",
                        "name": "S. Marinai"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[37], [38], [14] employ graph neural networks to integrate multi-modal information and reconstruct tables by node correlations."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "84ae5630601b634b1be90ceb6223ae687b9407c4",
                "externalIds": {
                    "DBLP": "conf/icpr/LiuLCDTFC22",
                    "DOI": "10.1109/ICPR56361.2022.9956139",
                    "CorpusId": 254100141
                },
                "corpusId": 254100141,
                "publicationVenue": {
                    "id": "48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                    "name": "International Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Pattern Recognit (ICPR Proc Int Conf",
                        "Int Conf Pattern Recognit",
                        "ICPR",
                        "International conference on pattern recognition",
                        "Int conf pattern recognit",
                        "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                    ],
                    "issn": "1041-3278",
                    "alternate_issns": [
                        "1051-4651"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=4740202"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/84ae5630601b634b1be90ceb6223ae687b9407c4",
                "title": "Table information extraction and analysis: A robust geometric approach based on GatedGCN",
                "abstract": "With the rapid development of Artificial Intelligence, Optical Character Recognition(OCR) is applied to analyze and understand the contents of various images, which has a very important effect on online office and makes business more intelligent. As downstream tasks of OCR, information extraction and table analysis are indispensable for acquisition of target information. However, when the texts information are gained from an invoice image or a table image by detection and recognition methods, how to further extract necessary information from a mass of texts or to analyze table information by reconstruction method is still difficult and challenging. In the paper, based on gated graph convolutional networks (GatedGCNs), we propose a novel model to extract key information in documents and reconstruct table information from listing images. Different from manual methods, the GatedGCN-based model considers three kinds of features for the semantic entities, including the position of an entity, the box containing the entity and texts inside the box. The proposed model also considers the relationship between semantic entities, which is a key factor to improve the classification accuracy. Since the update of gated edges in GatedGCN can be treated as a new way to implement attention mechanism, the model can integrate more critical information and discard unnecessary information. Therefore, combining with the node features and the edge features we have extracted, when applying the model on key field extraction (which can be treated as node classification problems) and table reconstruction (that can be treated as link-prediction problems), the model reaches overall excellent results in terms of precision, recall, F1 score and accuracy, evaluated on Medical Invoice, Train Tickets, SciTSR, ICDAR2013 datasets.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1679704",
                        "name": "Y. Liu"
                    },
                    {
                        "authorId": "2192834525",
                        "name": "Xiaoyun Liang"
                    },
                    {
                        "authorId": "2193211589",
                        "name": "Shaoqiong Chen"
                    },
                    {
                        "authorId": "2059827588",
                        "name": "Liang Diao"
                    },
                    {
                        "authorId": "2109887784",
                        "name": "Xin Tang"
                    },
                    {
                        "authorId": "2055079520",
                        "name": "Rui Fang"
                    },
                    {
                        "authorId": "2067611",
                        "name": "Weifu Chen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "XXXXXXX thus facilitatingmany graph-related tasks from various fields including recommendations [7, 48], natural language processing [50, 54], drug discovery [27, 34], and computer vision [9, 32]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9790154e14fa8df4fafd991196ad9a3e67f3edb7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-06743",
                    "ArXiv": "2208.06743",
                    "DOI": "10.48550/arXiv.2208.06743",
                    "CorpusId": 251564290
                },
                "corpusId": 251564290,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/9790154e14fa8df4fafd991196ad9a3e67f3edb7",
                "title": "Enhancing Graph Contrastive Learning with Node Similarity",
                "abstract": "Graph Neural Networks (GNNs) have achieved great success in learning graph representations and thus facilitating various graph-related tasks. However, most GNN methods adopt a supervised learning setting, which is not always feasible in real-world applications due to the difficulty to obtain labeled data. Hence, graph self-supervised learning has been attracting increasing attention. Graph contrastive learning (GCL) is a representative framework for self-supervised learning. In general, GCL learns node representations by contrasting semantically similar nodes (positive samples) and dissimilar nodes (negative samples) with anchor nodes. Without access to labels, positive samples are typically generated by data augmentation, and negative samples are uniformly sampled from the entire graph, which leads to a sub-optimal objective. Specifically, data augmentation naturally limits the number of positive samples that involve in the process (typically only one positive sample is adopted). On the other hand, the random sampling process would inevitably select false-negative samples (samples sharing the same semantics with the anchor). These issues limit the learning capability of GCL. In this work, we propose an enhanced objective that addresses the aforementioned issues. We first introduce an unachievable ideal objective that contains all positive samples and no false-negative samples. This ideal objective is then transformed into a probabilistic form based on the distributions for sampling positive and negative samples. We then model these distributions with node similarity and derive the enhanced objective. Comprehensive experiments on various datasets demonstrate the effectiveness of the proposed enhanced objective under different settings.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2181501783",
                        "name": "Hongliang Chi"
                    },
                    {
                        "authorId": "1409870962",
                        "name": "Yao Ma"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "9e1072fda768727e936b59a41d447a466c97eb36",
                "externalIds": {
                    "DOI": "10.3390/su141610010",
                    "CorpusId": 251639085
                },
                "corpusId": 251639085,
                "publicationVenue": {
                    "id": "8775599f-4f9a-45f0-900e-7f4de68e6843",
                    "name": "Sustainability",
                    "type": "journal",
                    "issn": "2071-1050",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-172127",
                    "alternate_urls": [
                        "http://mdpi.com/journal/sustainability",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-172127"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9e1072fda768727e936b59a41d447a466c97eb36",
                "title": "An AI-Based Automatic Risks Detection Solution for Plant Owner\u2019s Technical Requirements in Equipment Purchase Order",
                "abstract": "Maintenance activities to replace, repair, and revamp equipment in the industrial plant sector are gradually needed for sustainability during the plant\u2019s life cycle. In order to carry out these revamping activities, the plant owners exchange many purchase orders (POs) with equipment suppliers, including technical and specification documents and commercial procurement content. As POs are written in various formats with large volumes and complexities, it is often time-consuming for the owner\u2019s engineer to review them and it may lead to errors and omissions. This study proposed the purchase order recognition and analysis system (PORAS), which automatically detects and compares risk clauses between plant owners\u2019 and suppliers\u2019 POs by utilizing artificial intelligence (AI). The PORAS is a comprehensive framework consisting of two independent modules and four model components that accurately reflect on the added value of the PORAS. The table recognition and comparison (TRC) module is utilized for risk clauses in POs written in tables with its two components, the table comparison (TRC-C) and table recognition (TRC-R) models. The critical terms in general conditions (CTGC) module analyzes the patterns of risk clauses in general texts, then extracts them with a rule-based algorithm and compares them through entity matching. In the TRC-C model using machine learning (Ditto model), a few errors occurred due to insufficient training data, resulting in an accuracy of 87.8%, whereas in the TRC-R model, a rule-based algorithm, errors occurred in only some exceptional cases; thus, its F1 score was evaluated to be 96.9%. The CTGC module\u2019s F2 score for automatic extraction performance was evaluated as 79.1% due to some data\u2019s bias. Overall, the validation study shows that while a human review of the risk clauses in a PO manually took hours, it took only an average of 10 min with the PORAS. Therefore, this time saving can significantly reduce the owner engineer\u2019s PO workload. In essence, this study contributes to achieving sustainable engineering processes through the intelligence and automation of document and risk management in the plant industry.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2185872248",
                        "name": "Chae-Yeon Kim"
                    },
                    {
                        "authorId": "2181842133",
                        "name": "Jong-Gwan Jeong"
                    },
                    {
                        "authorId": "2154584029",
                        "name": "S. Choi"
                    },
                    {
                        "authorId": "72817600",
                        "name": "Eul-Bum Lee"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The first group [2, 20, 34, 49] treats words or cell contents as nodes in a graph and uses graph neural networks to predict whether each sampled node pair is in a same cell, row or column."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "fb6fcca5762b26f225313f86b1f33a1cf198bfd7",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2208-04921",
                    "ArXiv": "2208.04921",
                    "DOI": "10.1145/3503161.3548038",
                    "CorpusId": 251442367
                },
                "corpusId": 251442367,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/fb6fcca5762b26f225313f86b1f33a1cf198bfd7",
                "title": "TSRFormer: Table Structure Recognition with Transformers",
                "abstract": "We present a new table structure recognition (TSR) approach, called TSRFormer, to robustly recognizing the structures of complex tables with geometrical distortions from various table images. Unlike previous methods, we formulate table separation line prediction as a line regression problem instead of an image segmentation problem and propose a new two-stage DETR based separator prediction approach, dubbed Sep arator RE gression TR ansformer (SepRETR), to predict separation lines from table images directly. To make the two-stage DETR framework work efficiently and effectively for the separation line prediction task, we propose two improvements: 1) A prior-enhanced matching strategy to solve the slow convergence issue of DETR; 2) A new cross attention module to sample features from a high-resolution convolutional feature map directly so that high localization accuracy is achieved with low computational cost. After separation line prediction, a simple relation network based cell merging module is used to recover spanning cells. With these new techniques, our TSRFormer achieves state-of-the-art performance on several benchmark datasets, including SciTSR, PubTabNet and WTW. Furthermore, we have validated the robustness of our approach to tables with complex structures, borderless cells, large blank spaces, empty or spanning cells as well as distorted or even curved shapes on a more challenging real-world in-house dataset.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2108811137",
                        "name": "Weihong Lin"
                    },
                    {
                        "authorId": "2145130513",
                        "name": "Zhengmao Sun"
                    },
                    {
                        "authorId": "2894335",
                        "name": "Chixiang Ma"
                    },
                    {
                        "authorId": "2181111465",
                        "name": "Mingze Li"
                    },
                    {
                        "authorId": "2110132415",
                        "name": "Jiawei Wang"
                    },
                    {
                        "authorId": "2110833051",
                        "name": "Lei Sun"
                    },
                    {
                        "authorId": "2316043",
                        "name": "Qiang Huo"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "644b991959bf5fb036d28d67e9918d419b212e75",
                "externalIds": {
                    "DBLP": "journals/pr/LiYDL22",
                    "DOI": "10.1016/j.patcog.2022.108946",
                    "CorpusId": 251140530
                },
                "corpusId": 251140530,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/644b991959bf5fb036d28d67e9918d419b212e75",
                "title": "Table Structure Recognition and Form Parsing by End-to-End Object Detection and Relation Parsing",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2118890077",
                        "name": "Xiaohui Li"
                    },
                    {
                        "authorId": "145820427",
                        "name": "Fei Yin"
                    },
                    {
                        "authorId": "2179477671",
                        "name": "He-Sen Dai"
                    },
                    {
                        "authorId": "1689269",
                        "name": "Cheng-Lin Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "While there are many datasets for Table Detection and LIR [15,18,22,24,26, 57,63,70,71,87,90,98,99,100], some of them are not accessible anymore [18,24, 26,70].",
                "We argue that the problems of KILE and LIR, as defined in Sec.",
                "We define Line Item (LI) and the task of Line Item Recognition (LIR) as follows:\nDefinition 2 (LI).",
                "Note that this definition of LIR allows: (i) detection of several tables with different item types, as well as different item types within a single table; (ii) a single field (e.g., a date) to belong to several line items.",
                "Definition 3 (LIR)."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "1aa8da3bbd2bca7071fae029d6fa8ea851f805b7",
                "externalIds": {
                    "DBLP": "conf/clef/SkalickySUS22",
                    "ArXiv": "2206.11229",
                    "DOI": "10.48550/arXiv.2206.11229",
                    "CorpusId": 249926391
                },
                "corpusId": 249926391,
                "publicationVenue": {
                    "id": "ab453bce-d4ec-48ec-ad78-ef19dc9333ab",
                    "name": "Conference and Labs of the Evaluation Forum",
                    "type": "conference",
                    "alternate_names": [
                        "CLEF",
                        "Conf Lab Evaluation Forum",
                        "Cross-language Evaluation Forum",
                        "Cross-Language Evaluation Forum"
                    ],
                    "url": "http://www.clef-initiative.eu/"
                },
                "url": "https://www.semanticscholar.org/paper/1aa8da3bbd2bca7071fae029d6fa8ea851f805b7",
                "title": "Business Document Information Extraction: Towards Practical Benchmarks",
                "abstract": "Information extraction from semi-structured documents is crucial for frictionless business-to-business (B2B) communication. While machine learning problems related to Document Information Extraction (IE) have been studied for decades, many common problem definitions and benchmarks do not reflect domain-specific aspects and practical needs for automating B2B document communication. We review the landscape of Document IE problems, datasets and benchmarks. We highlight the practical aspects missing in the common definitions and define the Key Information Localization and Extraction (KILE) and Line Item Recognition (LIR) problems. There is a lack of relevant datasets and benchmarks for Document IE on semi-structured business documents as their content is typically legally protected or sensitive. We discuss potential sources of available documents including synthetic data.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2071515342",
                        "name": "Maty\u00e1s Skalick\u00fd"
                    },
                    {
                        "authorId": "46369981",
                        "name": "Step\u00e1n Simsa"
                    },
                    {
                        "authorId": "3406363",
                        "name": "Michal U\u0159i\u010d\u00e1\u0159"
                    },
                    {
                        "authorId": "2052167",
                        "name": "Milan \u0160ulc"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Certain relational understanding tasks already play the important role in building electronic archives and developing office automation, which include table structure recognition [24,25,27,31,33],",
                "Now mainstream relational understanding tasks like table structure recognition [24, 25, 27, 31, 33], key information extraction [16, 17, 46] and reading order detection [37] are born with entitylevel."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "db938b8742d3d91d8e3b11a161930f3b56a4bddd",
                "externalIds": {
                    "DBLP": "conf/mm/LiZHCWJLR22",
                    "ArXiv": "2205.02411",
                    "DOI": "10.1145/3503161.3547751",
                    "CorpusId": 248524962
                },
                "corpusId": 248524962,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/db938b8742d3d91d8e3b11a161930f3b56a4bddd",
                "title": "Relational Representation Learning in Visually-Rich Documents",
                "abstract": "Relational understanding is critical for a number of visually-rich documents (VRDs) understanding tasks. Through multi-modal pre-training, recent studies provide comprehensive contextual representations and exploit them as prior knowledge for downstream tasks. In spite of their impressive results, we observe that the widespread relational hints (e.g., relation of key/value fields on receipts) built upon contextual knowledge are not excavated yet. To mitigate this gap, we propose DocReL, a Document Relational Representation Learning framework. The major challenge of DocReL roots in the variety of relations. From the simplest pairwise relation to the complex global structure, it is infeasible to conduct supervised training due to the definition of relation varies and even conflicts in different tasks. To deal with the unpredictable definition of relations, we propose a novel contrastive learning task named Relational Consistency Modeling (RCM), which harnesses the fact that existing relations should be consistent in differently augmented positive views. RCM provides relational representations which are more compatible to the urgent need of downstream tasks, even without any knowledge about the exact definition of relation. DocReL achieves better performance on a wide variety of VRD relational understanding tasks, including table structure recognition, key information extraction and reading order detection.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2153901204",
                        "name": "Xin Li"
                    },
                    {
                        "authorId": "2111094840",
                        "name": "Yan Zheng"
                    },
                    {
                        "authorId": "2149298493",
                        "name": "Yiqing Hu"
                    },
                    {
                        "authorId": "50732729",
                        "name": "H. Cao"
                    },
                    {
                        "authorId": "2164224877",
                        "name": "Yunfei Wu"
                    },
                    {
                        "authorId": "3336726",
                        "name": "Deqiang Jiang"
                    },
                    {
                        "authorId": "2136370566",
                        "name": "Yinsong Liu"
                    },
                    {
                        "authorId": "2064646914",
                        "name": "Bo Ren"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "564acb3b027a439b1a44e5b2d73ff5814312ad58",
                "externalIds": {
                    "DBLP": "journals/asc/KashinathJAAS22",
                    "DOI": "10.1016/j.asoc.2022.108942",
                    "CorpusId": 248716737
                },
                "corpusId": 248716737,
                "publicationVenue": {
                    "id": "b1994124-f1e8-4f96-a165-b6f19a04fe7e",
                    "name": "Applied Soft Computing",
                    "type": "journal",
                    "alternate_names": [
                        "Appl Soft Comput"
                    ],
                    "issn": "1568-4946",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/621920/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/applied-soft-computing",
                        "http://www.sciencedirect.com/science/journal/15684946"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/564acb3b027a439b1a44e5b2d73ff5814312ad58",
                "title": "End-to-end table structure recognition and extraction in heterogeneous documents",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2164965360",
                        "name": "Tejas Kashinath"
                    },
                    {
                        "authorId": "2164964715",
                        "name": "Twisha Jain"
                    },
                    {
                        "authorId": "2066307014",
                        "name": "Yash Agrawal"
                    },
                    {
                        "authorId": "1455135470",
                        "name": "Tanvi Anand"
                    },
                    {
                        "authorId": "2118414009",
                        "name": "S. Singh"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "One group of bottom-up methods [21, 25, 75, 76] treat words or cell contents as nodes in a graph and use graph neural networks to predict whether each sampled node pair is in a same cell, row, or column."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "29942b5588d9978af5da046ebfc642e4779b8597",
                "externalIds": {
                    "ArXiv": "2203.09056",
                    "DBLP": "journals/pr/MaLSH23",
                    "DOI": "10.1016/j.patcog.2022.109006",
                    "CorpusId": 247518554
                },
                "corpusId": 247518554,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/29942b5588d9978af5da046ebfc642e4779b8597",
                "title": "Robust Table Detection and Structure Recognition from Heterogeneous Document Images",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2894335",
                        "name": "Chixiang Ma"
                    },
                    {
                        "authorId": "2108811137",
                        "name": "Weihong Lin"
                    },
                    {
                        "authorId": "2110833051",
                        "name": "Lei Sun"
                    },
                    {
                        "authorId": "2316043",
                        "name": "Qiang Huo"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "In the case of IETD, this OCR engine is implicit in the decoder similar to [24]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "081312b993ef1df7e64e1a5f8715852a2e7ed527",
                "externalIds": {
                    "ArXiv": "2203.01017",
                    "DBLP": "journals/corr/abs-2203-01017",
                    "DOI": "10.1109/CVPR52688.2022.00457",
                    "CorpusId": 247218660
                },
                "corpusId": 247218660,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/081312b993ef1df7e64e1a5f8715852a2e7ed527",
                "title": "TableFormer: Table Structure Understanding with Transformers",
                "abstract": "Tables organize valuable content in a concise and compact representation. This content is extremely valuable for systems such as search engines, Knowledge Graph's, etc, since they enhance their predictive capabilities. Unfortu-nately, tables come in a large variety of shapes and sizes. Furthermore, they can have complex column/row-header configurations, multiline rows, different variety of separation lines, missing entries, etc. As such, the correct iden-tification of the table-structure from an image is a nontrivial task. In this paper, we present a new table-structure identification model. The latter improves the latest end-to-end deep learning model (i.e. encoder-dual-decoder from PubTabNet) in two significant ways. First, we introduce a new object detection decoder for table-cells. In this way, we can obtain the content of the table-cells from program-matic PDF's directly from the PDF source and avoid the training of the custom OCR decoders. This architectural change leads to more accurate table-content extraction and allows us to tackle non-english tables. Second, we replace the LSTM decoders with transformer based decoders. This upgrade improves significantly the previous state-of-the-art tree-editing-distance-score (TEDS) from 91% to 98.5% on simple tables and from 88.7% to 95% on complex tables.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "37525891",
                        "name": "A. Nassar"
                    },
                    {
                        "authorId": "2051177400",
                        "name": "Nikolaos Livathinos"
                    },
                    {
                        "authorId": "1578242957",
                        "name": "Maksym Lysak"
                    },
                    {
                        "authorId": "1831851",
                        "name": "P. Staar"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a9e00a1c3de05e3d06a4287b8ef07096f0cfe609",
                "externalIds": {
                    "DBLP": "journals/pr/RibaGTRFL22",
                    "DOI": "10.1016/j.patcog.2022.108641",
                    "CorpusId": 247427729
                },
                "corpusId": 247427729,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a9e00a1c3de05e3d06a4287b8ef07096f0cfe609",
                "title": "Table detection in business document images by message passing networks",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "40420775",
                        "name": "Pau Riba"
                    },
                    {
                        "authorId": "145029669",
                        "name": "Lutz Goldmann"
                    },
                    {
                        "authorId": "3045937",
                        "name": "O. R. Terrades"
                    },
                    {
                        "authorId": "1491424368",
                        "name": "Diede Rusticus"
                    },
                    {
                        "authorId": "1686569",
                        "name": "A. Forn\u00e9s"
                    },
                    {
                        "authorId": "143826881",
                        "name": "J. Llad\u00f3s"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[10], a graph model is used for the structural analysis problem of documents."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "3a562b74e3dbc9cefd2245a2015346558b63df81",
                "externalIds": {
                    "DBLP": "journals/sncs/AjijPRH22",
                    "DOI": "10.1007/s42979-022-01041-z",
                    "CorpusId": 246809635
                },
                "corpusId": 246809635,
                "publicationVenue": {
                    "id": "7a7dc89b-e1a6-44df-a496-46c330a87840",
                    "name": "SN Computer Science",
                    "type": "journal",
                    "alternate_names": [
                        "SN Comput Sci"
                    ],
                    "issn": "2661-8907",
                    "alternate_issns": [
                        "2662-995X"
                    ],
                    "url": "https://link.springer.com/journal/42979"
                },
                "url": "https://www.semanticscholar.org/paper/3a562b74e3dbc9cefd2245a2015346558b63df81",
                "title": "Robust Detection of Tables in Documents Using Scores from Table Cell Cores",
                "abstract": null,
                "year": 2022,
                "authors": [
                    {
                        "authorId": "7536937",
                        "name": "Md. Ajij"
                    },
                    {
                        "authorId": "3267968",
                        "name": "Sanjoy Pratihar"
                    },
                    {
                        "authorId": "36929516",
                        "name": "D. S. Roy"
                    },
                    {
                        "authorId": "1797464",
                        "name": "T. Hanne"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "The DL-based methods in [7, 11] are among the first to apply neural networks designed for object detection to table parsing."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "10c3efc40e72674b615864c94a231e1f11913619",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2201-01654",
                    "ArXiv": "2201.01654",
                    "CorpusId": 245704311
                },
                "corpusId": 245704311,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/10c3efc40e72674b615864c94a231e1f11913619",
                "title": "TableParser: Automatic Table Parsing with Weak Supervision from Spreadsheets",
                "abstract": "Tables have been an ever-existing structure to store data. There exist now different approaches to store tabular data physically. PDFs, images, spreadsheets, and CSVs are leading examples. Being able to parse table structures and extract content bounded by these structures is of high importance in many applications. In this paper, we devise TableParser, a system capable of parsing tables in both native PDFs and scanned images with high precision. We have conducted extensive experiments to show the efficacy of domain adaptation in developing such a tool. Moreover, we create TableAnnotator and ExcelAnnotator, which constitute a spreadsheet-based weak supervision mechanism and a pipeline to enable table parsing. We share these resources with the research community to facilitate further research in this interesting direction.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "1576005199",
                        "name": "Susie Xi Rao"
                    },
                    {
                        "authorId": "5185738",
                        "name": "Johannes Rausch"
                    },
                    {
                        "authorId": "2074406990",
                        "name": "P. Egger"
                    },
                    {
                        "authorId": "1776014",
                        "name": "Ce Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "[24] regarded a table as a graph of text contents.",
                "Some authors attempted to replace the heuristics with machinelearning methods [24, 25]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d9c676eccdff80a7d1154660a09c4d2ea5164610",
                "externalIds": {
                    "DOI": "10.1007/s11042-021-11819-7",
                    "CorpusId": 254860167
                },
                "corpusId": 254860167,
                "publicationVenue": {
                    "id": "477368e9-7a8e-475a-8c93-6d623797fd06",
                    "name": "Multimedia tools and applications",
                    "type": "journal",
                    "alternate_names": [
                        "Multimedia Tools and Applications",
                        "Multimedia Tool Appl",
                        "Multimedia tool appl"
                    ],
                    "issn": "1380-7501",
                    "url": "https://www.springer.com/computer/information+systems+and+applications/journal/11042",
                    "alternate_urls": [
                        "https://link.springer.com/journal/11042"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d9c676eccdff80a7d1154660a09c4d2ea5164610",
                "title": "Deep-learning and graph-based approach to table structure recognition",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2115242015",
                        "name": "Eunji Lee"
                    },
                    {
                        "authorId": "2370571",
                        "name": "Jaewoo Park"
                    },
                    {
                        "authorId": "2463454",
                        "name": "H. Koo"
                    },
                    {
                        "authorId": "1707645",
                        "name": "N. Cho"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a5adea80377a623011fbc63b8dc686da01821126",
                "externalIds": {
                    "DBLP": "conf/icncc/FanTLZ21",
                    "DOI": "10.1145/3510513.3510515",
                    "CorpusId": 248572782
                },
                "corpusId": 248572782,
                "publicationVenue": {
                    "id": "12ed15a2-5218-48ca-a29f-e39af72dc504",
                    "name": "International Conference on Network, Communication and Computing",
                    "type": "conference",
                    "alternate_names": [
                        "ICNCC",
                        "International Conference Network, Communication and Computing",
                        "Int Conf Netw Commun Comput"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a5adea80377a623011fbc63b8dc686da01821126",
                "title": "DeTable: Table data extraction model based on deep",
                "abstract": "The rapid development of the information age leads to the mass production and frequent transmission of data, which is difficult to deal with by human alone. With the rise and development of artificial intelligence, the use of data is becoming more efficient. Table, as a special data form, has attracted wide attention gradually. However, extracting data from table subimages presents a number of challenges, including accurately detecting table regions in the image, and then detecting and extracting information from detected table rows and columns. While some progress has been made in table detection, extracting table contents remains a challenge because it involves more fine-grained table structure identification. In this paper, DeTable: table data extraction model based on deep learning is proposed. The model uses the interdependence between the twin tasks of table detection and table structure recognition to divide the text region and the box-line region of the table. Then, rows based on semantic rules are extracted from the identified table regions. The proposed models and extraction methods were evaluated on publicly available ICDAR 2019 and Marmot table datasets and the most advanced results were obtained.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": null,
                        "name": "Yuxuan Fan"
                    },
                    {
                        "authorId": "30726645",
                        "name": "Huobin Tan"
                    },
                    {
                        "authorId": "6782471",
                        "name": "Yu Liu"
                    },
                    {
                        "authorId": "2164727626",
                        "name": "Jingxuan Zhang"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Therefore, recent methods [2, 30, 34] attempt to attack the problem via constructing visual cues of table elements as graphs and applying the deep graph model, such as Graph Convolutional Networks (GCN) [15] to reason their relationships.",
                "With the development of deep learning, table structure recognition methods have recently advanced substantially on performance, which can be classified into three categories: boundary extractionbased [13, 22, 27, 35, 40], generative model-based [18, 46], and graph-based [2, 20, 30, 34] methods.",
                "The method [30] introduces DGCNN to predict the relationship between words represented by the appearance and geometry features.",
                "Although many previous algorithms [2,13,18,20,22,30,31,34,35,40,45,46] have achieved impressive progress in the community, TSR is still a challenging task due to two factors of complicated tables.",
                "Table structure recognition (TSR) aims to recognize the table internal structure to the machine readable data mainly presented in two formats: logical structure [18, 46] and physical structure [2, 13, 20, 22, 27, 30, 31, 34, 35, 40, 45].",
                "In the similar spirit with works [30, 34], we adopt the following asymmetric edge function h\u0398(xi,xj) = xi\u2225(xi \u2212 xj) to combine graph edge features to each node, which can be denoted as H\u0398 \u2208 R \u00b7(N\u22121)/2)\u00d7d .",
                "For comparison, we also visualize the multi-head self-attention maps from the last blocks of \u201cTransformer-Mixed\u201d [42] and KNN (K = 5) selection heatmaps of all layers in DGCNN [30], where a lighter color indicates a closer relationship.",
                "To introduce richer table information, several methods [20, 30, 34] con-",
                "In previous works using DGCNN [30, 34], only local context of each node is selected by k-Nearest Neighbors algorithm (KNN) to be aggregated into node feature.",
                "3 compares the effectiveness of various extractors, including DGCNN [30] and Transformer [42], with ECE in our method."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "9fb2744ef2b91033de39c121be25d3f86f759458",
                "externalIds": {
                    "DBLP": "conf/cvpr/0003LLJL022",
                    "ArXiv": "2111.13359",
                    "DOI": "10.1109/CVPR52688.2022.00449",
                    "CorpusId": 244709555
                },
                "corpusId": 244709555,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/9fb2744ef2b91033de39c121be25d3f86f759458",
                "title": "Neural Collaborative Graph Machines for Table Structure Recognition",
                "abstract": "Recently, table structure recognition has achieved impressive progress with the help of deep graph models. Most of them exploit single visual cues of tabular elements or simply combine visual cues with other modalities via early fusion to reason their graph relationships. However, neither early fusion nor individually reasoning in terms of multiple modalities can be appropriate for all varieties of table structures with great diversity. Instead, different modalities are expected to collaborate with each other in different patterns for different table cases. In the community, the importance of intrainter modality interactions for table structure reasoning is still unexplored. In this paper, we define it as heterogeneous table structure recognition (HeteroTSR) problem. With the aim offilling this gap, we present a novel Neural Collaborative Graph Machines (NCGM) equipped with stacked collaborative blocks, which alternatively extracts intramodality context and models inter-modality interactions in a hierarchical way. It can represent the intrainter modality relationships of tabular elements more robustly, which significantly improves the recognition performance. We also show that the proposed NCGM can modulate collaborative pattern of different modalities conditioned on the context of intramodality cues, which is vital for diversified table cases. Experimental results on benchmarks demonstrate our proposed NCGM achieves state-of-the-art performance and beats other contemporary methods by a large margin especially under challenging scenarios.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48446712",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "2153898418",
                        "name": "Xin Li"
                    },
                    {
                        "authorId": "2149124539",
                        "name": "Bing Liu"
                    },
                    {
                        "authorId": "3336726",
                        "name": "Deqiang Jiang"
                    },
                    {
                        "authorId": "2136370566",
                        "name": "Yinsong Liu"
                    },
                    {
                        "authorId": "2064646914",
                        "name": "Bo Ren"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Similar to previous works [28, 31] using DGCNN, only \ud835\udc58 nearest neighbours of each node are selected by k-Nearest Neighbors algorithm (KNN) to construct the local context which is applied by the CNN to aggregate the edge information into node feature D\ud835\udc56 \u2208 R\ud835\udc41\u00d7 \ud835\udc51 \u210e for \ud835\udc56-th head.",
                "Furthermore, some methods [2, 19, 28, 31] attempt to construct the graphs of elements to reason the relationships.",
                "Qasim et al. [28] introduce DGCNN to build the relationships of the fused image and position features between text blocks to predict the relations between nodes.",
                "Similar to previous works [28, 31] using DGCNN, only k nearest neighbours of each node are selected by k-Nearest Neighbors algorithm (KNN) to construct the local context which is applied by the CNN to aggregate the edge information into node feature Di \u2208 RN\u00d7 d h for i-th head.",
                "As for the sparse context, we build SCA upon DGCNN [28] to softly introduce the relational inductive bias to our model and enable it to learn sparse contextual information in local pattern.",
                "[28] introduce DGCNN to build the relationships of the fused image and position features between text blocks to predict the relations between nodes.",
                "For the relational reasoning aspect, recent mainstream algorithms [2, 19, 28, 31] construct table elements as contextual graphs and apply the graph-based aggregator, such as Graph Convolutional Networks (GCN) [14], to reason their relationships.",
                "In addition, considering memory complexity of the classification between each proposal pair, we also introduce Monte Carlo sampling [28] to generate a fixed number of samples.",
                "Among existing methods, many of them [2, 19, 28] regard off-line extracted meta-data or OCR results as table elements for the subsequent reconstruction of table structure.",
                "For example, literature [28] simply exploits k-Nearest Neighbors algorithm (KNN) to build local context in a hard-coded way."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "7b99f9995a4b086c0e6913a8d2f109a5dbaa5014",
                "externalIds": {
                    "DBLP": "conf/mm/LiuLLJLRJ21",
                    "DOI": "10.1145/3474085.3481534",
                    "CorpusId": 239011898
                },
                "corpusId": 239011898,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/7b99f9995a4b086c0e6913a8d2f109a5dbaa5014",
                "title": "Show, Read and Reason: Table Structure Recognition with Flexible Context Aggregator",
                "abstract": "We investigate the challenging problem of table structure recognition in this work. Many recent methods adopt graph-based context aggregator with strong inductive bias to reason sparse contextual relationships of table elements. However, the strong constraints may be too restrictive to represent the complicated table relationships. In order to learn more appropriate inductive bias from data, we try to introduce Transformer as context aggregator in this work. Nevertheless, Transformer taking dense context as input requires larger scale data and may suffer from unstable training procedure due to the weakening of inductive bias. To overcome the above limitations, we in this paper design a FLAG (FLexible context AGgregator), which marries Transformer with graph-based context aggregator in an adaptive way. Based on FLAG, an end-to-end framework requiring no extra meta-data or OCR information, termed FLAG-Net, is proposed to flexibly modulate the aggregation of dense context and sparse one for the relational reasoning of table elements. We investigate the modulation pattern in FLAG and show what contextual information is focused, which is vital for recognizing table structure. Extensive experimental results on benchmarks demonstrate the performance of our proposed FLAG-Net surpasses other compared methods by a large margin.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "48446712",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "2153898418",
                        "name": "Xin Li"
                    },
                    {
                        "authorId": "2149124539",
                        "name": "Bing Liu"
                    },
                    {
                        "authorId": "3336726",
                        "name": "Deqiang Jiang"
                    },
                    {
                        "authorId": "2136370566",
                        "name": "Yinsong Liu"
                    },
                    {
                        "authorId": "2064646914",
                        "name": "Bo Ren"
                    },
                    {
                        "authorId": "145592290",
                        "name": "R. Ji"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "To achieve this purpose, existing computer vision methods either predict cell bounding boxes [6, 13], explore the adjacency relation between different cells [11, 15], or transform a table image into the markup sequence (e."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0ab6cfb57d3de5159c8d6b36f38d1efc199de11b",
                "externalIds": {
                    "DBLP": "conf/mm/XueCWLYZT21",
                    "DOI": "10.1145/3474085.3478558",
                    "CorpusId": 239011591
                },
                "corpusId": 239011591,
                "publicationVenue": {
                    "id": "f2c85de5-7cfa-4b92-8714-a0fbdcf0274e",
                    "name": "ACM Multimedia",
                    "type": "conference",
                    "alternate_names": [
                        "MM"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/0ab6cfb57d3de5159c8d6b36f38d1efc199de11b",
                "title": "A Question Answering System for Unstructured Table Images",
                "abstract": "Question answering over tables is a very popular semantic parsing task in natural language processing (NLP). However, few existing methods focus on table images, even though there are usually large-scale unstructured tables in practice (e.g., table images). Table parsing from images is nontrivial since it is closely related to not only NLP but also computer vision (CV) to parse the tabular structure from an image. In this demo, we present a question answering system for unstructured table images. The proposed system mainly consists of 1) a table recognizer to recognize the tabular structure from an image and 2) a table parser to generate the answer to a natural language question over the table. In addition, to train the model, we further provide table images and structure annotations for two widely used semantic parsing datasets. Specifically, the test set is used for this demo, from where the users can either choose from default questions or enter a new custom question.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51469630",
                        "name": "Wenyuan Xue"
                    },
                    {
                        "authorId": "2112619444",
                        "name": "Siqi Cai"
                    },
                    {
                        "authorId": "2119293226",
                        "name": "Wen Wang"
                    },
                    {
                        "authorId": "1612980447",
                        "name": "Qingyong Li"
                    },
                    {
                        "authorId": "2110431812",
                        "name": "Baosheng Yu"
                    },
                    {
                        "authorId": "1895813",
                        "name": "Yibing Zhan"
                    },
                    {
                        "authorId": "143719920",
                        "name": "D. Tao"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Other approaches include those based on image-to-text [8] and graph-based approaches [2,14]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a94c3e400fc5426a0b8a650b924242abcc1e46f2",
                "externalIds": {
                    "ArXiv": "2110.00061",
                    "DBLP": "conf/cvpr/SmockPA22",
                    "DOI": "10.1109/CVPR52688.2022.00459",
                    "CorpusId": 244462899
                },
                "corpusId": 244462899,
                "publicationVenue": {
                    "id": "768b87bb-8a18-4d9c-a161-4d483c776bcf",
                    "name": "Computer Vision and Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "CVPR",
                        "Comput Vis Pattern Recognit"
                    ],
                    "issn": "1063-6919",
                    "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147",
                    "alternate_urls": [
                        "https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a94c3e400fc5426a0b8a650b924242abcc1e46f2",
                "title": "PubTables-1M: Towards comprehensive table extraction from unstructured documents",
                "abstract": "Recently, significant progress has been made applying machine learning to the problem of table structure inference and extraction from unstructured documents. However, one of the greatest challenges remains the creation of datasets with complete, unambiguous ground truth at scale. To address this, we develop a new, more comprehensive dataset for table extraction, called PubTables-1M. PubTables-1M contains nearly one million tables from scientific articles, supports multiple input modalities, and contains detailed header and location information for table structures, making it useful for a wide variety of modeling approaches. It also addresses a significant source of ground truth inconsistency observed in prior datasets called oversegmentation, using a novel canonicalization procedure. We demonstrate that these improvements lead to a significant increase in training performance and a more reliable estimate of model performance at evaluation for table structure recognition. Further, we show that transformer-based object detection models trained on PubTables-1M produce excellent results for all three tasks of detection, structure recognition, and functional analysis without the need for any special customization for these tasks. Data and code will be released at https://github.com/microsoft/table-transformer.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2745998",
                        "name": "B. Smock"
                    },
                    {
                        "authorId": "2130464272",
                        "name": "Rohith Pesala"
                    },
                    {
                        "authorId": "2394344",
                        "name": "Robin Abraham"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "As this assumption does not hold on to more challenging tables, some researches tried to get rid of the wellaligned assumption and modeled the table structure parsing problem with graph convolution networks [2, 13, 24].",
                "For a more practical requirement of parsing table structures from images taken by hand-held cameras in the wild, the existing state-of-the-art approaches [13, 14, 11, 16, 9, 23] are prone to fail as the commonly-used assumption of tabular images no longer holds."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7a4c901e00fef01d66efb125bf0e2fdd773d8bc6",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2109-02199",
                    "ArXiv": "2109.02199",
                    "DOI": "10.1109/ICCV48922.2021.00098",
                    "CorpusId": 237420694
                },
                "corpusId": 237420694,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/7a4c901e00fef01d66efb125bf0e2fdd773d8bc6",
                "title": "Parsing Table Structures in the Wild",
                "abstract": "This paper tackles the problem of table structure parsing (TSP) from images in the wild. In contrast to existing studies that mainly focus on parsing well-aligned tabular images with simple layouts from scanned PDF documents, we aim to establish a practical table structure parsing system for real-world scenarios where tabular input images are taken or scanned with severe deformation, bending or occlusions. For designing such a system, we propose an approach named Cycle-CenterNet on the top of CenterNet with a novel cycle-pairing module to simultaneously detect and group tabular cells into structured tables. In the cycle-pairing module, a new pairing loss function is proposed for the network training. Alongside with our Cycle-CenterNet, we also present a large-scale dataset, named Wired Table in the Wild (WTW), which includes well-annotated structure parsing of multiple style tables in several scenes like photo, scanning files, web pages, etc.. In experiments, we demonstrate that our Cycle-CenterNet consistently achieves the best accuracy of table structure parsing on the new WTW dataset by 24.6% absolute improvement evaluated by the TEDS metric. A more comprehensive experimental analysis also validates the advantages of our proposed methods for the TSP task.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "143799988",
                        "name": "Rujiao Long"
                    },
                    {
                        "authorId": "2119293226",
                        "name": "Wen Wang"
                    },
                    {
                        "authorId": "2685089",
                        "name": "Nan Xue"
                    },
                    {
                        "authorId": "2112256",
                        "name": "Feiyu Gao"
                    },
                    {
                        "authorId": "2109432908",
                        "name": "Zhibo Yang"
                    },
                    {
                        "authorId": "153709848",
                        "name": "Yongpan Wang"
                    },
                    {
                        "authorId": "39943835",
                        "name": "Gui-Song Xia"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Although researchers have employed Fully Convolutional Network (FCN) [47,48] and Graph Neural Network (GNN) [34,49] to perform table detection in document images, object detection-based approaches [7,8,11,12,19,20,30\u201334] have delivered state-of-the-art results."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d10d8164e472864968725424d2195789fb5e67bb",
                "externalIds": {
                    "PubMedCentral": "8540682",
                    "MAG": "3197383755",
                    "DBLP": "journals/jimaging/HashmiPLSA21",
                    "DOI": "10.3390/jimaging7100214",
                    "CorpusId": 239202542,
                    "PubMed": "34677300"
                },
                "corpusId": 239202542,
                "publicationVenue": {
                    "id": "c0fc53c7-b0ed-487d-9191-1262c8322621",
                    "name": "Journal of Imaging",
                    "type": "journal",
                    "alternate_names": [
                        "J Imaging"
                    ],
                    "issn": "2313-433X",
                    "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-556372",
                    "alternate_urls": [
                        "https://www.mdpi.com/journal/jimaging",
                        "http://nbn-resolving.de/urn/resolver.pl?urn=urn:nbn:ch:bel-556372"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/d10d8164e472864968725424d2195789fb5e67bb",
                "title": "CasTabDetectoRS: Cascade Network for Table Detection in Document Images with Recursive Feature Pyramid and Switchable Atrous Convolution",
                "abstract": "Table detection is a preliminary step in extracting reliable information from tables in scanned document images. We present CasTabDetectoRS, a novel end-to-end trainable table detection framework that operates on Cascade Mask R-CNN, including Recursive Feature Pyramid network and Switchable Atrous Convolution in the existing backbone architecture. By utilizing a comparativelyightweight backbone of ResNet-50, this paper demonstrates that superior results are attainable without relying on pre- and post-processing methods, heavier backbone networks (ResNet-101, ResNeXt-152), and memory-intensive deformable convolutions. We evaluate the proposed approach on five different publicly available table detection datasets. Our CasTabDetectoRS outperforms the previous state-of-the-art results on four datasets (ICDAR-19, TableBank, UNLV, and Marmot) and accomplishes comparable results on ICDAR-17 POD. Upon comparing with previous state-of-the-art results, we obtain a significant relative error reduction of 56.36%, 20%, 4.5%, and 3.5% on the datasets of ICDAR-19, TableBank, UNLV, and Marmot, respectively. Furthermore, this paper sets a new benchmark by performing exhaustive cross-datasets evaluations to exhibit the generalization capabilities of the proposed method.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "1771057",
                        "name": "A. Pagani"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2dc7726a6e77fa993a1d58acaead8dee3a8183a5",
                "externalIds": {
                    "DOI": "10.1109/ICOIM52180.2021.9524385",
                    "CorpusId": 237403119
                },
                "corpusId": 237403119,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2dc7726a6e77fa993a1d58acaead8dee3a8183a5",
                "title": "Form Recognition Based on Lightweight U-Net and Tesseract after Multi-level Retraining",
                "abstract": "With the rapid development of Internet information technology and the advancement of enterprise digitization, the digitization of paper forms has also received extensive attention. The automatic conversion of paper form documents into electronic form documents mainly faces three problems. The first is that the format of the form file is diverse and the structure is complex. This article uses the XML file of the form to accurately analyze the structure of the file, which is more accurate than the current semantic segmentation method. The second problem is table area detection, this paper uses traditional algorithms to find the contours of the candidate table area, and screens according to the characteristics of the table area to complete the detection and extraction of the table area. The third is that the recognition of the table text is more difficult, not only the interference information such as the table frame will also affect the accuracy of text recognition, and the type of text information in the table is complex, including Chinese, English, numbers, symbols and mixed types, which bring huge challenges to text recognition. This paper uses the lightweight U-Net network model to segment the text area at pixel level, eliminating the interference information of text recognition. The neural network of Tesseract was retrained in a multiple, multi-level manner, and successfully realized the recognition of complex types of text information with an accuracy of about 96%. Based on deep learning and XML table structure analysis algorithm, this paper realizes the recognition of paper version of the form file and the reconstruction of the electronic version of the file.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2155122015",
                        "name": "Gang Li"
                    },
                    {
                        "authorId": "2815027",
                        "name": "Junhui Huang"
                    },
                    {
                        "authorId": "2144712727",
                        "name": "Zhao Wang"
                    },
                    {
                        "authorId": "8418310",
                        "name": "Jianmin Gao"
                    },
                    {
                        "authorId": "2110717926",
                        "name": "Kun Chen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "The graph neural network is also used in [27]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "10c8c9733cf7c671505010213ad397a50f5664d8",
                "externalIds": {
                    "DBLP": "conf/kdd/YangC0021",
                    "DOI": "10.1145/3447548.3467425",
                    "CorpusId": 236980243
                },
                "corpusId": 236980243,
                "publicationVenue": {
                    "id": "a0edb93b-1e95-4128-a295-6b1659149cef",
                    "name": "Knowledge Discovery and Data Mining",
                    "type": "conference",
                    "alternate_names": [
                        "KDD",
                        "Knowl Discov Data Min"
                    ],
                    "url": "http://www.acm.org/sigkdd/"
                },
                "url": "https://www.semanticscholar.org/paper/10c8c9733cf7c671505010213ad397a50f5664d8",
                "title": "Numerical Formula Recognition from Tables",
                "abstract": "Claims over the numerical relationships among some measures are commonly expressed in tabular forms, and widely exist in the published documents on the Web. This paper introduces the problem of numerical formula recognition from tables, namely recognizing all numerical formulas inside a given table. It can well support many interesting downstream applications, such as numerical error correction in tables, formula recommendation in tables. Here, we emphasize that table is a kind of language that adopts a different linguistic paradigm from natural language. It uses visual grammar like visual layout and visual settings (e.g., indentation, font style) to express the grammatical relationships among the table cells. Understanding tables and recognizing formulas require decoding the visual grammar while simultaneously understanding the textual information. Another challenge is that formulas are complicated in terms of diverse math functions and variable-length of arguments. To address these challenges, we convert this task into a uniform framework, extracting relations of table cell pairs in a table. A two-channel neural network model TaFor is proposed to embed both the textual and visual features for a table cell. Our framework achieves the formula-level F1-score = 0.90 on a real-world dataset of 190179 tables while a retrieval-based method achieves F1-score = 0.72. We also perform extensive experiments to demonstrate the effectiveness of each component in our model, and conduct a case study to discuss the limits of the proposed model. With our published data this study also aims to attract the community's interest in deep semantic understanding over tables.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2149535920",
                        "name": "Qingping Yang"
                    },
                    {
                        "authorId": "2107994062",
                        "name": "Yixuan Cao"
                    },
                    {
                        "authorId": "2142626460",
                        "name": "Hongwei Li"
                    },
                    {
                        "authorId": "144389949",
                        "name": "Ping Luo"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a0a97c129f1c8339b9fb4fd768cf9f52e64b2b0a",
                "externalIds": {
                    "ArXiv": "2107.05214",
                    "DBLP": "journals/corr/abs-2107-05214",
                    "DOI": "10.1016/j.patcog.2022.108565",
                    "CorpusId": 235795015
                },
                "corpusId": 235795015,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/a0a97c129f1c8339b9fb4fd768cf9f52e64b2b0a",
                "title": "Split, embed and merge: An accurate table structure recognizer",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2128660562",
                        "name": "Zhenrong Zhang"
                    },
                    {
                        "authorId": "39557762",
                        "name": "Jianshu Zhang"
                    },
                    {
                        "authorId": "2149030934",
                        "name": "Jun Du"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "A recent application example was the detection of tables in case of administrative document images [21,20]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "4a1a9bdb68b78ba45748b54fd2030040711a8f1d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2107-04357",
                    "ArXiv": "2107.04357",
                    "DOI": "10.1007/978-3-030-86159-9_38",
                    "CorpusId": 235790730
                },
                "corpusId": 235790730,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/4a1a9bdb68b78ba45748b54fd2030040711a8f1d",
                "title": "Graph-based Deep Generative Modelling for Document Layout Generation",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2150473007",
                        "name": "Sanket Biswas"
                    },
                    {
                        "authorId": "40420775",
                        "name": "Pau Riba"
                    },
                    {
                        "authorId": "2117637297",
                        "name": "Josep Llad'os"
                    },
                    {
                        "authorId": "144167309",
                        "name": "U. Pal"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Generally, these methods can be divided into the edge classification [1, 22, 11, 30] and the node classification [15].",
                "2, with the success of deep learning, recent deep learning-based table structure recognition approaches can be divided into three categories: 1) identify cell bounding boxes through visual detection and segmentation methods [25, 27, 29, 28, 8]; 2) transform a table image into the markup sequence, such as LaTeX and HTML [10, 2]; and 3) explore the adjacency relation between different table cells [30, 22, 11]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6e26602986e56d3524c325601674decb05cd8f2b",
                "externalIds": {
                    "DBLP": "conf/iccv/XueYWTL21",
                    "ArXiv": "2106.10598",
                    "DOI": "10.1109/ICCV48922.2021.00133",
                    "CorpusId": 235490364
                },
                "corpusId": 235490364,
                "publicationVenue": {
                    "id": "7654260e-79f9-45c5-9663-d72027cf88f3",
                    "name": "IEEE International Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ICCV",
                        "IEEE Int Conf Comput Vis",
                        "ICCV Workshops",
                        "ICCV Work"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
                },
                "url": "https://www.semanticscholar.org/paper/6e26602986e56d3524c325601674decb05cd8f2b",
                "title": "TGRNet: A Table Graph Reconstruction Network for Table Structure Recognition",
                "abstract": "A table arranging data in rows and columns is a very effective data structure, which has been widely used in business and scientific research. Considering large-scale tabular data in online and offline documents, automatic table recognition has attracted increasing attention from the document analysis community. Though human can easily understand the structure of tables, it remains a challenge for machines to understand that, especially due to a variety of different table layouts and styles. Existing methods usually model a table as either the markup sequence or the adjacency matrix between different table cells, failing to address the importance of the logical location of table cells, e.g., a cell is located in the first row and the second column of the table. In this paper, we reformulate the problem of table structure recognition as the table graph reconstruction, and propose an end-to-end trainable table graph reconstruction network (TGRNet) for table structure recognition. Specifically, the proposed method has two main branches, a cell detection branch and a cell logical location branch, to jointly predict the spatial location and the logical location of different cells. Experimental results on three popular table recognition datasets and a new dataset with table graph annotations (TableGraph-350K) demonstrate the effectiveness of the proposed TGRNet for table structure recognition. Code and annotations will be made publicly available at https://github.com/xuewenyuan/TGRNet.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "51469630",
                        "name": "Wenyuan Xue"
                    },
                    {
                        "authorId": "2110431812",
                        "name": "Baosheng Yu"
                    },
                    {
                        "authorId": "2119293226",
                        "name": "Wen Wang"
                    },
                    {
                        "authorId": "2075330732",
                        "name": "Dacheng Tao"
                    },
                    {
                        "authorId": "1612980447",
                        "name": "Qingyong Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "The techniques employed in extracting tabular data from images usually involves advanced machine techniques such as Deep learning [3], Graph Neural networks [4] etc."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "eb7fd03770b478a0de3dcb9101632e55f43c477d",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-09137",
                    "ArXiv": "2105.09137",
                    "CorpusId": 234778251
                },
                "corpusId": 234778251,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/eb7fd03770b478a0de3dcb9101632e55f43c477d",
                "title": "TableZa - A classical Computer Vision approach to Tabular Extraction",
                "abstract": "Computer aided Tabular Data Extraction has always been a very challenging and error prone task because it demands both Spectral and Spatial Sanity of data. In this paper we discuss an approach for Tabular Data Extraction in the realm of document comprehension. Given the different kinds of the Tabular formats that are often found across various documents, we discuss a novel approach using Computer Vision for extraction of tabular data from images or vector pdf(s) converted to image(s).",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1935310175",
                        "name": "Saumya Banthia"
                    },
                    {
                        "authorId": "2109671235",
                        "name": "Anantha Sharma"
                    },
                    {
                        "authorId": "89440985",
                        "name": "R. Mangipudi"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "The works by Schreiber et al. (2018); Qasim, Mahmood, and Shafait (2019) draw upon deep neural networks to identify table structures for rendered inputs."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "adcb8fe66d08e6187f51ecbf4443523870a4b799",
                "externalIds": {
                    "DBLP": "conf/aaai/RauschMB0F21",
                    "DOI": "10.1609/aaai.v35i5.16558",
                    "CorpusId": 235306298
                },
                "corpusId": 235306298,
                "publicationVenue": {
                    "id": "bdc2e585-4e48-4e36-8af1-6d859763d405",
                    "name": "AAAI Conference on Artificial Intelligence",
                    "type": "conference",
                    "alternate_names": [
                        "National Conference on Artificial Intelligence",
                        "National Conf Artif Intell",
                        "AAAI Conf Artif Intell",
                        "AAAI"
                    ],
                    "url": "http://www.aaai.org/"
                },
                "url": "https://www.semanticscholar.org/paper/adcb8fe66d08e6187f51ecbf4443523870a4b799",
                "title": "DocParser: Hierarchical Document Structure Parsing from Renderings",
                "abstract": "Translating renderings (e. g. PDFs, scans) into hierarchical document structures is extensively demanded in the daily routines of many real-world applications. However, a holistic, principled approach to inferring the complete hierarchical structure in documents is missing. As a remedy, we developed \u201cDocParser\u201d: an end-to-end system for parsing complete document structure \u2013 including all text elements, nested figures, tables, and table cell structures. Our second contribution is to provide a dataset for evaluating hierarchical document structure parsing. Our third contribution is to propose a scalable learning framework for settings where domain-specific data are scarce, which we address by a novel approach to weak supervision that significantly improves the document structure parsing performance. Our experiments confirm the effectiveness of our proposed weak supervision: Compared to the baseline without weak supervision, it improves the mean average precision for detecting document entities by 39.1% and improves the F1 score of classifying hierarchical relations by 35.8%.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "5185738",
                        "name": "Johannes Rausch"
                    },
                    {
                        "authorId": "2059207885",
                        "name": "O. Martinez"
                    },
                    {
                        "authorId": "1402912519",
                        "name": "Fabian Bissig"
                    },
                    {
                        "authorId": "1776014",
                        "name": "Ce Zhang"
                    },
                    {
                        "authorId": "3207649",
                        "name": "S. Feuerriegel"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "GCNs have been applied to other structured document tasks, such as table extraction [13,15]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7d74be3c872fff0c3b659e69d42b07161b5cf6b8",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2105-08194",
                    "ArXiv": "2105.08194",
                    "DOI": "10.1007/978-3-030-86549-8_27",
                    "CorpusId": 234763397
                },
                "corpusId": 234763397,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/7d74be3c872fff0c3b659e69d42b07161b5cf6b8",
                "title": "Visual FUDGE: Form Understanding via Dynamic Graph Editing",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "145566717",
                        "name": "Brian L. Davis"
                    },
                    {
                        "authorId": "2877830",
                        "name": "B. Morse"
                    },
                    {
                        "authorId": "31844147",
                        "name": "Brian L. Price"
                    },
                    {
                        "authorId": "67319819",
                        "name": "Chris Tensmeyer"
                    },
                    {
                        "authorId": "26360698",
                        "name": "Curtis Wigington"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Inspired by [24], we adopt the algorithm of Maximum Clique Search [1] to find all maximum cliques in the graph.",
                "With the great success of deep neural network in computer vision field, works began to focus on the image-based table with more general structures [21,30,24,9,13,36,23,14,33].",
                "[24] alleviates the problem of large graph nodes numbers by the pair sampling strategy.",
                "The above three works [14,2,24] also published new table datasets for this research area.",
                "Another type of methods [11,2,14,24,26] treat the detected boxes as nodes in a graph and attempt to predict the relations based on techniques of Graph Neural Networks [29].",
                "That is why many segmentation-based methods [24,23,22] struggle with complicated post-processing, such as fracture completion and threshold setting.",
                "Following the same naming convention with [14,2,24], the connecting relations can be divided into horizontal and vertical types.",
                "Another group of methods solves the above problems in a bottom-up way to firstly detect the text blocks\u2019 positions and then recover the bounding-boxes\u2019 relations by heuristic rules [38] or GNN(Graph Neural Networks) [29,14,2,24,26]."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "ea25282d27368d3d04db91b165b5003d63e335d6",
                "externalIds": {
                    "DBLP": "conf/icdar/QiaoLCZPNRT021",
                    "ArXiv": "2105.06224",
                    "DOI": "10.1007/978-3-030-86549-8_7",
                    "CorpusId": 234482682
                },
                "corpusId": 234482682,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/ea25282d27368d3d04db91b165b5003d63e335d6",
                "title": "LGPMA: Complicated Table Structure Recognition with Local and Global Pyramid Mask Alignment",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2065512194",
                        "name": "Liang Qiao"
                    },
                    {
                        "authorId": "2109614687",
                        "name": "Zaisheng Li"
                    },
                    {
                        "authorId": "2398015",
                        "name": "Zhanzhan Cheng"
                    },
                    {
                        "authorId": "2151333065",
                        "name": "Peng Zhang"
                    },
                    {
                        "authorId": "3290437",
                        "name": "Shiliang Pu"
                    },
                    {
                        "authorId": "1490934795",
                        "name": "Yi Niu"
                    },
                    {
                        "authorId": "144850642",
                        "name": "Wenqi Ren"
                    },
                    {
                        "authorId": "51151126",
                        "name": "Wenming Tan"
                    },
                    {
                        "authorId": "144894837",
                        "name": "Fei Wu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b308c6e2a236b8d73dd4a962cb5b8c7374ee3698",
                "externalIds": {
                    "MAG": "3159314278",
                    "DBLP": "journals/monet/ZuckerBVN21",
                    "DOI": "10.1007/S11036-021-01759-9",
                    "CorpusId": 235557952
                },
                "corpusId": 235557952,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b308c6e2a236b8d73dd4a962cb5b8c7374ee3698",
                "title": "ClusTi: Clustering Method for Table Structure Recognition in Scanned Images",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2070874816",
                        "name": "Arthur Zucker"
                    },
                    {
                        "authorId": "2037496520",
                        "name": "Younes Belkada"
                    },
                    {
                        "authorId": "1752866615",
                        "name": "Hanh Vu"
                    },
                    {
                        "authorId": "2757688",
                        "name": "N. V. Nguyen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "(2019) argued that graph networks are a more natural choice for table recognition problem, and they further explored two gradient-based graph neural networks for this issue [24].",
                "Furthermore, Qasim et al. (2019) argued that graph networks are a more natural choice for table recognition problem, and they further explored two gradient-based graph neural networks for this issue [24]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "abd3ca5ab184843ad7634b8d8c2a7dd36db2aac7",
                "externalIds": {
                    "DOI": "10.1007/s11036-021-01759-9",
                    "CorpusId": 254832742
                },
                "corpusId": 254832742,
                "publicationVenue": {
                    "id": "6fff9d04-a29e-4e37-87e8-27e06da9055c",
                    "name": "Journal on spesial topics in mobile networks and applications",
                    "type": "journal",
                    "alternate_names": [
                        "J spes top mob netw appl",
                        "Mobile Networks and Applications",
                        "Mob Netw Appl"
                    ],
                    "issn": "1383-469X",
                    "url": "https://link.springer.com/journal/11036"
                },
                "url": "https://www.semanticscholar.org/paper/abd3ca5ab184843ad7634b8d8c2a7dd36db2aac7",
                "title": "ClusTi: Clustering Method for Table Structure Recognition in Scanned Images",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2070874816",
                        "name": "Arthur Zucker"
                    },
                    {
                        "authorId": "2037496520",
                        "name": "Younes Belkada"
                    },
                    {
                        "authorId": "1752866615",
                        "name": "Hanh Vu"
                    },
                    {
                        "authorId": "2196642420",
                        "name": "Van Nam Nguyen"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "[28] published a method to synthetically create camera captured images from the UNLV dataset.",
                "It is important to mention that apart from these two approaches, other methods [28], [30], [100] have extracted the contents of cells in order to recognize either the tabular boundaries or tabular structures.",
                "[28] Graph Neural Networks with Convolutional Neural Networks (Section III-B2).",
                "FIGURE 11: Example of a synthetically created camera captured image by linear perspective transform method [28].",
                "[28] exploited the graph neural networks to perform table recognition for the first time.",
                "[28] which is explained in Section III-A3 did not use any well known dataset to evaluate their approach."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "bc8863440e26e48866a64f63438051d24880b793",
                "externalIds": {
                    "DBLP": "journals/access/HashmiLSAAA21",
                    "ArXiv": "2104.14272",
                    "DOI": "10.1109/ACCESS.2021.3087865",
                    "CorpusId": 233444247
                },
                "corpusId": 233444247,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/bc8863440e26e48866a64f63438051d24880b793",
                "title": "Current Status and Performance Analysis of Table Recognition in Document Images With Deep Neural Networks",
                "abstract": "The first phase of table recognition is to detect the tabular area in a document. Subsequently, the tabular structures are recognized in the second phase in order to extract information from the respective cells. Table detection and structural recognition are pivotal problems in the domain of table understanding. However, table analysis is a perplexing task due to the colossal amount of diversity and asymmetry in tables. Therefore, it is an active area of research in document image analysis. Recent advances in the computing capabilities of graphical processing units have enabled the deep neural networks to outperform traditional state-of-the-art machine learning methods. Table understanding has substantially benefited from the recent breakthroughs in deep neural networks. However, there has not been a consolidated description of the deep learning methods for table detection and table structure recognition. This review paper provides a thorough analysis of the modern methodologies that utilize deep neural networks. Moreover, it presents a comprehensive understanding of the current state-of-the-art and related challenges of table understanding in document images. The leading datasets and their intricacies have been elaborated along with the quantitative results. Furthermore, a brief overview is given regarding the promising directions that can further improve table analysis in document images.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    },
                    {
                        "authorId": "2206035070",
                        "name": "Muhammad Adnan Afzal"
                    },
                    {
                        "authorId": "2206035072",
                        "name": "Muhammad Ahtsham Afzal"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[17] made use of Graph Neural Networks for generating cell adjacency matrix for all existing OCR detected words and Tensmeyer et al.",
                "[17] GNN model, makes use of large proprietary dataset to train the network for an effective model to be tested on ICDAR 2013.",
                "[17] mentions that lack of large datasets has been a major hindrance for Deep Learning methods in table structure recognition and makes use of synthetic data to show the effectiveness of their network."
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "5dbb224c85d767730e880c3226c9dc5d59cb765f",
                "externalIds": {
                    "ArXiv": "2104.14237",
                    "DBLP": "journals/corr/abs-2104-14237",
                    "DOI": "10.1007/978-3-030-86331-9_38",
                    "CorpusId": 233443846
                },
                "corpusId": 233443846,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/5dbb224c85d767730e880c3226c9dc5d59cb765f",
                "title": "TabAug: Data Driven Augmentation for Enhanced Table Structure Recognition",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2054749978",
                        "name": "U. Khan"
                    },
                    {
                        "authorId": "49192264",
                        "name": "Sohaib Zahid"
                    },
                    {
                        "authorId": "2111291830",
                        "name": "Muhammad Asad Ali"
                    },
                    {
                        "authorId": "1403326950",
                        "name": "A. Ul-Hasan"
                    },
                    {
                        "authorId": "1688013",
                        "name": "F. Shafait"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[25] use a graph neural network to identify cell locations within tables."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "80c62ab972ec45f3e7e465539613f2cf779474cb",
                "externalIds": {
                    "DBLP": "journals/array/ColterFYKYD22",
                    "ArXiv": "2104.11287",
                    "DOI": "10.2139/ssrn.4044585",
                    "CorpusId": 233388132
                },
                "corpusId": 233388132,
                "publicationVenue": {
                    "id": "a7e4161c-9216-40df-a896-ed51a04aab3a",
                    "name": "Array",
                    "type": "journal",
                    "issn": "2590-0056",
                    "url": "https://www.journals.elsevier.com/array",
                    "alternate_urls": [
                        "https://www.sciencedirect.com/journal/array"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/80c62ab972ec45f3e7e465539613f2cf779474cb",
                "title": "Tablext: A Combined Neural Network And Heuristic Based Table Extractor",
                "abstract": "A significant portion of the data available today is found within tables. Therefore, it is necessary to use automated table extraction to obtain thorough results when data-mining. Today's popular state-of-the-art methods for table extraction struggle to adequately extract tables with machine-readable text and structural data. To make matters worse, many tables do not have machine-readable data, such as tables saved as images, making most extraction methods completely ineffective. In order to address these issues, a novel, general format table extractor tool, Tablext, is proposed. This tool uses a combination of computer vision techniques and machine learning methods to efficiently and effectively identify and extract data from tables. Tablext begins by using a custom Convolutional Neural Network (CNN) to identify and separate all potential tables. The identification process is optimized by combining the custom CNN with the YOLO object detection network. Then, the high-level structure of each table is identified with computer vision methods. This high-level, structural meta-data is used by another CNN to identify exact cell locations. As a final step, Optical Characters Recognition (OCR) is performed on every individual cell to extract their content without needing machine-readable text. This multi-stage algorithm allows for the neural networks to focus on completing complex tasks, while letting image processing methods efficiently complete the simpler ones. This leads to the proposed approach to be general-purpose enough to handle a large batch of tables regardless of their internal encodings or their layout complexity. Additionally, it becomes accurate enough to outperform competing state-of-the-art table extractors on the ICDAR 2013 table dataset.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2087607913",
                        "name": "Zachary Colter"
                    },
                    {
                        "authorId": "33935286",
                        "name": "Morteza Fayazi"
                    },
                    {
                        "authorId": "7936000",
                        "name": "Z. Youbi"
                    },
                    {
                        "authorId": "2083548026",
                        "name": "Serafina Kamp"
                    },
                    {
                        "authorId": "2052192972",
                        "name": "Shuyang Yu"
                    },
                    {
                        "authorId": "1793651",
                        "name": "R. Dreslinski"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2a242e4a54323eee9e0f514510b008d9b2119641",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2104-10538",
                    "ArXiv": "2104.10538",
                    "DOI": "10.1109/ACCESS.2021.3103413",
                    "CorpusId": 233324171
                },
                "corpusId": 233324171,
                "publicationVenue": {
                    "id": "2633f5b2-c15c-49fe-80f5-07523e770c26",
                    "name": "IEEE Access",
                    "type": "journal",
                    "issn": "2169-3536",
                    "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=6287639"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/2a242e4a54323eee9e0f514510b008d9b2119641",
                "title": "Guided Table Structure Recognition Through Anchor Optimization",
                "abstract": "This paper presents the novel approach towards table structure recognition by leveraging the guided anchors. The concept differs from current state-of-the-art systems for table structure recognition that naively apply object detection methods. In contrast to prior techniques, first, we estimate the viable anchors for table structure recognition. Subsequently, these anchors are exploited to locate the rows and columns in tabular images. Furthermore, the paper introduces a simple and effective method that improves the results using tabular layouts in realistic scenarios. The proposed method is exhaustively evaluated on the two publicly available datasets of table structure recognition: ICDAR-2013 and TabStructDB. Moreover, we empirically established the validity of our method by implementing it on the previous approaches. We accomplished state-of-the-art results on the ICDAR-2013 dataset with an average F1-measure of 94.19% (92.06% for rows and 96.32% for columns). Thus, a relative error reduction of more than 25% is achieved. Furthermore, our proposed post-processing improves the average F1-measure to 95.46% that results in a relative error reduction of more than 35%. Moreover, we surpassed the baseline results on the TabStructDB dataset with an average F1-measure of 94.57% (94.08% for rows and 95.06% for columns).",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "1403379889",
                        "name": "K. Hashmi"
                    },
                    {
                        "authorId": "143749919",
                        "name": "D. Stricker"
                    },
                    {
                        "authorId": "1743758",
                        "name": "M. Liwicki"
                    },
                    {
                        "authorId": "2207509298",
                        "name": "Muhammad Noman Afzal"
                    },
                    {
                        "authorId": "145181206",
                        "name": "Muhammad Zeshan Afzal"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Many previous works [22, 26, 23] and tools 16 have been developed to identify and parse table structures.",
                "Most recently, Graph Neural Networks [25] have also been used in table detection [23]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "300cbaf1644920b15a97692e6309f5d58e1abc0e",
                "externalIds": {
                    "DBLP": "journals/corr/abs-2103-15348",
                    "ArXiv": "2103.15348",
                    "DOI": "10.1007/978-3-030-86549-8_9",
                    "CorpusId": 232404723
                },
                "corpusId": 232404723,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/300cbaf1644920b15a97692e6309f5d58e1abc0e",
                "title": "LayoutParser: A Unified Toolkit for Deep Learning Based Document Image Analysis",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "101568984",
                        "name": "Zejiang Shen"
                    },
                    {
                        "authorId": "49775305",
                        "name": "Ruochen Zhang"
                    },
                    {
                        "authorId": "2065276972",
                        "name": "Melissa Dell"
                    },
                    {
                        "authorId": "145485725",
                        "name": "B. Lee"
                    },
                    {
                        "authorId": "2060946945",
                        "name": "Jacob Carlson"
                    },
                    {
                        "authorId": "2108801852",
                        "name": "Weining Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "[12] applied Graph Neural Network (GNN) to the table structure recognition task, effectively"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e9c50d3d75798ef5265a2ab3253669df6246267d",
                "externalIds": {
                    "DBLP": "conf/icmlc2/KongBWCZ21",
                    "DOI": "10.1145/3457682.3457752",
                    "CorpusId": 235495067
                },
                "corpusId": 235495067,
                "publicationVenue": {
                    "id": "d79c0d2a-37de-4287-87e7-1e57576dcae7",
                    "name": "International Conference on Machine Learning and Computing",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Mach Learn Cybern",
                        "International Conference on Machine Learning and Cybernetics",
                        "International Conference Machine Learning and Computing",
                        "Int Conf Mach Learn Comput",
                        "ICMLC"
                    ],
                    "url": "http://www.icmlc.com/"
                },
                "url": "https://www.semanticscholar.org/paper/e9c50d3d75798ef5265a2ab3253669df6246267d",
                "title": "A Gradient heatmap based Table Structure Recognition",
                "abstract": "Most methods to recognize the structure of a table are to use the object detection approach to directly locate each cell in the table or to segment the table line based on the fully convolutional network (FCN). The problem of the former is that it is laborious to recognize the distorted table, while the problem of the latter is that the sample imbalance makes it difficult to train the model. In this paper, a gradient heatmap based table structure recognition method is proposed, by exploring the gradient heatmaps of the vertical lines and horizontal lines in the table. Specifically, the pixels of the vertical lines of the table are obtained according to the gradient heatmap, then the pixels of the horizontal lines are obtained using the same method, and finally the table structure is restored by using the connected domain search method. Compared with the Single Shot MultiBox Detector (SSD) and Faster RCNN that directly detects cells, our Average Precision (AP) value reached up to 99.5%, which is much higher than the above models. Additionally, we demonstrate that the AP values of the proposed models are reduced almost negligibly when the IoU threshold increased from 0.5 to 0.75, while the AP value of the fast RCNN and SSD model decreased significantly.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2069275327",
                        "name": "Lingjun Kong"
                    },
                    {
                        "authorId": "2106679362",
                        "name": "Yunchao Bao"
                    },
                    {
                        "authorId": "1934355987",
                        "name": "Qianwen Wang"
                    },
                    {
                        "authorId": "47238733",
                        "name": "Lijun Cao"
                    },
                    {
                        "authorId": "2903770",
                        "name": "Shengmei Zhao"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Convolutional neural networks [150, 151], fully-convolutional neural networks [152, 153], region-based convolutional neural networks [154, 155, 156, 157], and graph neural networks [158, 159] have all been exploited to parse the physical layout of documents or to detect elements of interest (e."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "a7ef6b6e94090993d08f824f3254e5420a71b3ed",
                "externalIds": {
                    "ArXiv": "2101.06573",
                    "DBLP": "journals/corr/abs-2101-06573",
                    "CorpusId": 231632613
                },
                "corpusId": 231632613,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/a7ef6b6e94090993d08f824f3254e5420a71b3ed",
                "title": "Understanding in Artificial Intelligence",
                "abstract": "Current Artificial Intelligence (AI) methods, most based on deep learning, have facilitated progress in several fields, including computer vision and natural language understanding. The progress of these AI methods is measured using benchmarks designed to solve challenging tasks, such as visual question answering. A question remains of how much understanding is leveraged by these methods and how appropriate are the current benchmarks to measure understanding capabilities. To answer these questions, we have analysed existing benchmarks and their understanding capabilities, defined by a set of understanding capabilities, and current research streams. We show how progress has been made in benchmark development to measure understanding capabilities of AI methods and we review as well how current methods develop understanding capabilities.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2428309",
                        "name": "S. Maetschke"
                    },
                    {
                        "authorId": "91474933",
                        "name": "D. M. Iraola"
                    },
                    {
                        "authorId": "2055078423",
                        "name": "Pieter Barnard"
                    },
                    {
                        "authorId": "9585722",
                        "name": "Elaheh Shafieibavani"
                    },
                    {
                        "authorId": "2062708279",
                        "name": "Peter Zhong"
                    },
                    {
                        "authorId": "2118669012",
                        "name": "Ying Xu"
                    },
                    {
                        "authorId": "1399097376",
                        "name": "Antonio Jimeno-Yepes"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Some other works use applied graph convolutional neural network technique [15] and segmentation-based method for table structure decomposition [2]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "be18da882d59ee35b0b4545c90d120d6e9bc3612",
                "externalIds": {
                    "DBLP": "conf/icpr/WeiLZC20",
                    "DOI": "10.1109/ICPR48806.2021.9413122",
                    "CorpusId": 233877793
                },
                "corpusId": 233877793,
                "publicationVenue": {
                    "id": "48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                    "name": "International Conference on Pattern Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "Pattern Recognit (ICPR Proc Int Conf",
                        "Int Conf Pattern Recognit",
                        "ICPR",
                        "International conference on pattern recognition",
                        "Int conf pattern recognit",
                        "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                    ],
                    "issn": "1041-3278",
                    "alternate_issns": [
                        "1051-4651"
                    ],
                    "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings",
                    "alternate_urls": [
                        "http://ieeexplore.ieee.org/servlet/opac?punumber=4740202"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/be18da882d59ee35b0b4545c90d120d6e9bc3612",
                "title": "Image-based Table Cell Detection: a Novel Table Structure Decomposition Method with New Dataset",
                "abstract": "Recently deep learning has been applied to decompose table structure with the main ideas of detecting table lines and then forming table cells. However, the existing methods face problems in dealing with tables with rotation or no internal table lines. To tackle these problems, we propose a novel table structure decomposition method, which directly detects table cells as objects and creates table structure. Extensions to the existing object detection models including effective table projection module are proposed to adapt to the table cell detection. To support the training of the enhanced models, we create a large image-based table dataset TableCell with cell level annotations. A novel and efficient semi-supervised method is proposed to annotate this new dataset. Experiments demonstrate that our proposed table structure decomposition method is simple, effective and robust to the tables without table lines or with rotation. Our dataset and code will be made available11https://github.com/weidafeng/TableCell.",
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2088884394",
                        "name": "Dafeng Wei"
                    },
                    {
                        "authorId": "46385957",
                        "name": "Hongtao Lu"
                    },
                    {
                        "authorId": "46433090",
                        "name": "Yi Zhou"
                    },
                    {
                        "authorId": "153819461",
                        "name": "Kai Chen"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "(2019 [9]) presented a method that combined the advantages of convolutional neural network and graph neural network, so that the effect is better than the traditional neural network."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e0dd683a69a85cd9d574017c6c0993c84eda159f",
                "externalIds": {
                    "DBLP": "journals/puc/WuZXZL22",
                    "MAG": "3167970107",
                    "DOI": "10.1007/s00779-020-01485-1",
                    "CorpusId": 231202527
                },
                "corpusId": 231202527,
                "publicationVenue": {
                    "id": "68fd8242-3be0-4b1e-8b5d-ad0a8a02db12",
                    "name": "Personal and Ubiquitous Computing",
                    "type": "journal",
                    "alternate_names": [
                        "Pers Ubiquitous Comput"
                    ],
                    "issn": "1617-4909",
                    "url": "http://www.springer.com/computer/hci/journal/779",
                    "alternate_urls": [
                        "https://link.springer.com/journal/volumesAndIssues/779"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e0dd683a69a85cd9d574017c6c0993c84eda159f",
                "title": "TableRobot: an automatic annotation method for heterogeneous tables",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118065009",
                        "name": "Guibin Wu"
                    },
                    {
                        "authorId": "2151550608",
                        "name": "Junjie Zhou"
                    },
                    {
                        "authorId": "48935767",
                        "name": "Yongping Xiong"
                    },
                    {
                        "authorId": "2110840865",
                        "name": "Chao Zhou"
                    },
                    {
                        "authorId": "2109665210",
                        "name": "Chong Li"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "(2019 [9]) presented a method that com-"
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "c2e50e044b38d2df1fe073086857130509828b63",
                "externalIds": {
                    "DOI": "10.1007/s00779-020-01485-1",
                    "CorpusId": 254089122
                },
                "corpusId": 254089122,
                "publicationVenue": {
                    "id": "68fd8242-3be0-4b1e-8b5d-ad0a8a02db12",
                    "name": "Personal and Ubiquitous Computing",
                    "type": "journal",
                    "alternate_names": [
                        "Pers Ubiquitous Comput"
                    ],
                    "issn": "1617-4909",
                    "url": "http://www.springer.com/computer/hci/journal/779",
                    "alternate_urls": [
                        "https://link.springer.com/journal/volumesAndIssues/779"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/c2e50e044b38d2df1fe073086857130509828b63",
                "title": "TableRobot: an automatic annotation method for heterogeneous tables",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118065009",
                        "name": "Guibin Wu"
                    },
                    {
                        "authorId": "2151550608",
                        "name": "Junjie Zhou"
                    },
                    {
                        "authorId": "48935767",
                        "name": "Yongping Xiong"
                    },
                    {
                        "authorId": "2191394384",
                        "name": "Chaoyi Zhou"
                    },
                    {
                        "authorId": "2109665210",
                        "name": "Chong Li"
                    }
                ]
            }
        },
        {
            "intents": [
                "background",
                "methodology"
            ],
            "contexts": [
                "Therefore, we firstly generate a synthetic dataset with the aid of the open code [10].",
                "When compared with the method in [10], our proposed method can still get considerable results on tables in Category 3 and Category 4.",
                "Some recent approaches [10], [11] utilize Graph Convolutional Network to predict the relationship between word (or cell).",
                "Table images in different categories [10].",
                "[10] divide the relationships between words into three types: belonging to the"
            ],
            "isInfluential": true,
            "citingPaper": {
                "paperId": "5afded5e53199d2bee71c2067afde4b338d6d49c",
                "externalIds": {
                    "DOI": "10.1109/ICSP48669.2020.9321003",
                    "CorpusId": 231682183
                },
                "corpusId": 231682183,
                "publicationVenue": {
                    "id": "63410070-a9b9-46ac-bdec-92b016246795",
                    "name": "International Conference on the Software Process",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Signal Process",
                        "ICSP",
                        "Int Conf Softw Process",
                        "International Conference on Signal Processing"
                    ],
                    "url": "http://www.ece.utexas.edu/~perry/prof/ispa/index.html"
                },
                "url": "https://www.semanticscholar.org/paper/5afded5e53199d2bee71c2067afde4b338d6d49c",
                "title": "A Deep Semantic Segmentation Model for Image-based Table Structure Recognition",
                "abstract": "Table structure recognition is a crucial step for automatic table information extraction. It is conventional to utilize the features such as ruling lines or words for parsing the rows, columns and cells in a table. However, these conventional methods are ineffective for image-based tables when ruling lines are not visible or the words cannot be recognized through the OCR system. In order to overcome these problems, we propose a deep semantic segmentation model for image-based table structure recognition. Specifically, it is an end-to-end semantic segmentation neural network to determine a pixel-wise prediction map for an input table image where the labels are row separator, column separator, cell content and background. Moreover, by making the connected componnet analysis on the prediction map, we can obtain the bounding boxes of row separators, column separators and cell contents, more accurately. Then we number row/column separators in order by coordinate sorting. Thus, we can make full use of relative positions between row/column separators and cell contents, and further assign the row/column number to each cell. Due to the lack of training data, a large amount of synthetic data are automatically generated in our experiments. It is demonstrated by the experimental results that our proposed model is suitable for various table types, which can achieve 0.9769 and 0.9343 average F1 scores on a generative dataset when the IoU threshold is set to 0.6 and 0.8, respectively.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2047123130",
                        "name": "Y. Zou"
                    },
                    {
                        "authorId": "1685259",
                        "name": "Jinwen Ma"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "search for structure analyses (Li et al., 2020; Qasim et al., 2019; Zhong et al., 2019).",
                "The lack of large-scale labeled document datasets has been recognized as a major hindrance in deep-learning research for structure analyses (Li et al., 2020; Qasim et al., 2019; Zhong et al., 2019)."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "10682868f013a538129ad2bbc90f873c0b81cab3",
                "externalIds": {
                    "ACL": "2020.sdp-1.10",
                    "DBLP": "conf/emnlp/LingC20",
                    "MAG": "3098562592",
                    "DOI": "10.18653/v1/2020.sdp-1.10",
                    "CorpusId": 226283959
                },
                "corpusId": 226283959,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/10682868f013a538129ad2bbc90f873c0b81cab3",
                "title": "DeepPaperComposer: A Simple Solution for Training Data Preparation for Parsing Research Papers",
                "abstract": "We present DeepPaperComposer, a simple solution for preparing highly accurate (100%) training data without manual labeling to extract content from scholarly articles using convolutional neural networks (CNNs). We used our approach to generate data and trained CNNs to extract eight categories of both textual (titles, abstracts, headers, figure and table captions, and other texts) and non-textural content (figures and tables) from 30 years of IEEE VIS conference papers, of which a third were scanned bitmap PDFs. We curated this dataset and named it VISpaper-3K. We then showed our initial benchmark performance using VISpaper-3K over itself and CS-150 using YOLOv3 and Faster-RCNN. We open-source DeepPaperComposer of our training data generation and released the resulting annotation data VISpaper-3K to promote re-producible research.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2004871801",
                        "name": "Meng Ling"
                    },
                    {
                        "authorId": "49251914",
                        "name": "Jian Chen"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "30860e7c569d777db8d2b6fb0f7c374965ac62dd",
                "externalIds": {
                    "MAG": "3109870706",
                    "DBLP": "conf/eccv/RajaMJ20",
                    "ArXiv": "2010.04565",
                    "DOI": "10.1007/978-3-030-58604-1_5",
                    "CorpusId": 221990801
                },
                "corpusId": 221990801,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/30860e7c569d777db8d2b6fb0f7c374965ac62dd",
                "title": "Table Structure Recognition using Top-Down and Bottom-Up Cues",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2053910624",
                        "name": "S. Raja"
                    },
                    {
                        "authorId": "2896521",
                        "name": "Ajoy Mondal"
                    },
                    {
                        "authorId": "1694502",
                        "name": "C. V. Jawahar"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "1a3450026a628f49fc6306399b5b509c14175a29",
                "externalIds": {
                    "MAG": "3088403594",
                    "DBLP": "conf/doceng/GalAS20",
                    "DOI": "10.1145/3395027.3419584",
                    "CorpusId": 221866220
                },
                "corpusId": 221866220,
                "publicationVenue": {
                    "id": "32c90593-78aa-40de-8700-dd31a625b15a",
                    "name": "ACM Symposium on Document Engineering",
                    "type": "conference",
                    "alternate_names": [
                        "Doc Eng",
                        "Document Engineering",
                        "DocEng",
                        "ACM Symp Doc Eng"
                    ],
                    "url": "http://www.documentengineering.org/"
                },
                "url": "https://www.semanticscholar.org/paper/1a3450026a628f49fc6306399b5b509c14175a29",
                "title": "Cardinal Graph Convolution Framework for Document Information Extraction",
                "abstract": "Graph Convolutional Networks (GCN) have been recognized as successful for processing pseudo-spatial graph representations of the underlying structure of documents. We present Cardinal Graph Convolutional Networks (CGCN), an efficient and flexible extension of GCNs with cardinal-direction awareness of spatial node arrangement. The formulation of CGCNs retains the traditional GCN permutation invariance, ensuring directional neighbors are involved in learning abstract representations, even in the absence of a proper ordering of the nodes. We show that CGCNs achieve state of the art results on an invoice information extraction task, jointly learning a word-level tagging as well as document meta-level classification and regression. We also present a new multiscale Inception-like CGCN block-layer, as well as Conv-Pool-DeConv-DePool UNet-like architecture, which increase the receptive field. We demonstrate the utility of CGCNs on private and public datasets, with respect to several baseline models: sequential LSTM, transformer classifier, non-cardinal GCNs, and an image-convolutional approach.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "134639223",
                        "name": "Rinon Gal"
                    },
                    {
                        "authorId": "1962385210",
                        "name": "Shai Ardazi"
                    },
                    {
                        "authorId": "2509354",
                        "name": "Roy Shilkrot"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "98d89a56cd9ce4d18c18e9378926725952159fdb",
                "externalIds": {
                    "DBLP": "conf/das/LiYL20",
                    "MAG": "3049292294",
                    "DOI": "10.1007/978-3-030-57058-3_17",
                    "CorpusId": 221141532
                },
                "corpusId": 221141532,
                "publicationVenue": {
                    "id": "02d53b80-30d7-493c-9453-ed7406056b31",
                    "name": "International Workshop on Document Analysis Systems",
                    "type": "conference",
                    "alternate_names": [
                        "DAS",
                        "Document Analysis Systems",
                        "Int Workshop Doc Anal Syst",
                        "Doc Anal Syst"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=647"
                },
                "url": "https://www.semanticscholar.org/paper/98d89a56cd9ce4d18c18e9378926725952159fdb",
                "title": "Page Segmentation Using Convolutional Neural Network and Graphical Model",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2118890701",
                        "name": "Xiaohui Li"
                    },
                    {
                        "authorId": "145820427",
                        "name": "Fei Yin"
                    },
                    {
                        "authorId": "1689269",
                        "name": "Cheng-Lin Liu"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Different statistical models have been used, for example, probabilistic modelling [30], the Naive Bayes classifier [31], [32], decision trees [33], [34], Support Vector Machine [33], [35], Conditional Random Fields [35]\u2013[37], graph neural network [14], [38], [39], attention module [40], etc."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "ba86c20ea740c728b80fd6b1965eb3cbb637aaec",
                "externalIds": {
                    "MAG": "3010871581",
                    "DBLP": "journals/corr/abs-2003-07560",
                    "ArXiv": "2003.07560",
                    "DOI": "10.1007/978-3-030-68790-8_50",
                    "CorpusId": 212737040
                },
                "corpusId": 212737040,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/ba86c20ea740c728b80fd6b1965eb3cbb637aaec",
                "title": "GFTE: Graph-based Financial Table Extraction",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2111150050",
                        "name": "Yiren Li"
                    },
                    {
                        "authorId": "1390799037",
                        "name": "Zheng Huang"
                    },
                    {
                        "authorId": "3063894",
                        "name": "Junchi Yan"
                    },
                    {
                        "authorId": "46433090",
                        "name": "Yi Zhou"
                    },
                    {
                        "authorId": "2104156250",
                        "name": "Fan Ye"
                    },
                    {
                        "authorId": "8016263",
                        "name": "Xianhui Liu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [
                "and table recognition [39]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e0e21a8ac103b8ab32c2a4626173a795b9470209",
                "externalIds": {
                    "ArXiv": "2003.06999",
                    "DBLP": "journals/corr/abs-2003-06999",
                    "MAG": "3088488048",
                    "DOI": "10.1016/j.patcog.2020.107684",
                    "CorpusId": 212725818
                },
                "corpusId": 212725818,
                "publicationVenue": {
                    "id": "266f640f-003e-453b-ab76-57e4053252f8",
                    "name": "Pattern Recognition",
                    "type": "journal",
                    "alternate_names": [
                        "Pattern Recognit"
                    ],
                    "issn": "0031-3203",
                    "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/328/description#description",
                    "alternate_urls": [
                        "https://www.journals.elsevier.com/pattern-recognition",
                        "http://www.sciencedirect.com/science/journal/00313203"
                    ]
                },
                "url": "https://www.semanticscholar.org/paper/e0e21a8ac103b8ab32c2a4626173a795b9470209",
                "title": "ReLaText: Exploiting Visual Relationships for Arbitrary-Shaped Scene Text Detection with Graph Convolutional Networks",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2894335",
                        "name": "Chixiang Ma"
                    },
                    {
                        "authorId": "2110833051",
                        "name": "Lei Sun"
                    },
                    {
                        "authorId": "2665108",
                        "name": "Zhuoyao Zhong"
                    },
                    {
                        "authorId": "2316043",
                        "name": "Qiang Huo"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6d738a42d3048171882c5c0a7b959060043ab657",
                "externalIds": {
                    "ArXiv": "1911.12170",
                    "DBLP": "journals/corr/abs-1911-12170",
                    "MAG": "2991098654",
                    "CorpusId": 208310264
                },
                "corpusId": 208310264,
                "publicationVenue": {
                    "id": "1901e811-ee72-4b20-8f7e-de08cd395a10",
                    "name": "arXiv.org",
                    "alternate_names": [
                        "ArXiv"
                    ],
                    "issn": "2331-8422",
                    "url": "https://arxiv.org"
                },
                "url": "https://www.semanticscholar.org/paper/6d738a42d3048171882c5c0a7b959060043ab657",
                "title": "Document Structure Extraction for Forms using Very High Resolution Semantic Segmentation",
                "abstract": "In this work, we look at the problem of structure extraction from document images with a specific focus on forms. Forms as a document class have not received much attention, even though they comprise a significant fraction of documents and enable several applications. Forms possess a rich, complex, hierarchical, and high-density semantic structure that poses several challenges to semantic segmentation methods. We propose a prior based deep CNN-RNN hierarchical network architecture that enables document structure extraction using very high resolution(1800 x 1000) images. We divide the document image into overlapping horizontal strips such that the network segments a strip and uses its prediction mask as prior while predicting the segmentation for the subsequent strip. We perform experiments establishing the effectiveness of our strip based network architecture through ablation methods and comparison with low-resolution variations. We introduce our new rich human-annotated forms dataset, and we show that our method significantly outperforms other segmentation baselines in extracting several hierarchical structures on this dataset. We also outperform other baselines in table detection task on the Marmot dataset. Our method is currently being used in a world-leading customer experience management software suite for automated conversion of paper and PDF forms to modern HTML based forms.",
                "year": 2019,
                "authors": [
                    {
                        "authorId": "39230373",
                        "name": "Mausoom Sarkar"
                    },
                    {
                        "authorId": "6657914",
                        "name": "Milan Aggarwal"
                    },
                    {
                        "authorId": "1432235684",
                        "name": "Arneh Jain"
                    },
                    {
                        "authorId": "1381295186",
                        "name": "Hiresh Gupta"
                    },
                    {
                        "authorId": "145846953",
                        "name": "Balaji Krishnamurthy"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "99b2a05177c17dd86627863608ac45dc4cfe0446",
                "externalIds": {
                    "DBLP": "journals/corr/abs-1911-10683",
                    "MAG": "2990963180",
                    "ArXiv": "1911.10683",
                    "DOI": "10.1007/978-3-030-58589-1_34",
                    "CorpusId": 208267858
                },
                "corpusId": 208267858,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/99b2a05177c17dd86627863608ac45dc4cfe0446",
                "title": "Image-based table recognition: data, model, and evaluation",
                "abstract": null,
                "year": 2019,
                "authors": [
                    {
                        "authorId": "2113309578",
                        "name": "Xu Zhong"
                    },
                    {
                        "authorId": "9585722",
                        "name": "Elaheh Shafieibavani"
                    },
                    {
                        "authorId": "1399097376",
                        "name": "Antonio Jimeno-Yepes"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "6c411f52c151fe537ee76c310f45f0dd3da88e92",
                "externalIds": {
                    "DBLP": "series/sbcs/Bhowmik23",
                    "DOI": "10.1007/978-981-99-4277-0",
                    "CorpusId": 260337124
                },
                "corpusId": 260337124,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/6c411f52c151fe537ee76c310f45f0dd3da88e92",
                "title": "Document Layout Analysis",
                "abstract": null,
                "year": 2023,
                "authors": [
                    {
                        "authorId": "3272848",
                        "name": "Showmik Bhowmik"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "7b849a6e017c3082248b45585b1692e62bb4c337",
                "externalIds": {
                    "CorpusId": 249826116
                },
                "corpusId": 249826116,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/7b849a6e017c3082248b45585b1692e62bb4c337",
                "title": "Structure Recognition Method in Various Table Types for Document Processing Automation",
                "abstract": "In this paper, we propose the method of a table structure recognition in various table types for document processing automation. A table with items surrounded by ruled lines are analyzed by detecting horizontal and vertical lines for recognizing the table structure. In case of a table with items separated by spaces, the table structure are recognized by analyzing the arrangement of row items. After recognizing the table structure, the areas of the table items are input into OCR engine and the character recognition result output to a text file in a structured format such as CSV or JSON. In simulation results, the average accuracy of table item recognition is about 94%.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "48893141",
                        "name": "Dong-seok Lee"
                    },
                    {
                        "authorId": "34032055",
                        "name": "Soon-kak Kwon"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Considering memory efficiency, we also introduce Monte Carlo sampling for constructing collaborative graph embedding pairs in the training phase, which is similar to [12]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "20156b441f90f8238181315b37dbccf5f5493882",
                "externalIds": {
                    "CorpusId": 250563732
                },
                "corpusId": 250563732,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/20156b441f90f8238181315b37dbccf5f5493882",
                "title": "Neural Collaborative Graph Machines for Table Structure Recognition - Supplementary Materials",
                "abstract": "Geometry embedding. We derive the geometry feature of each text segment bounding box as ( x W , y H , w W , h H )\u22a4 , where W and H are the width and height of the table image. (x, y) represents the center point of the box while height h and width w correspond to its short side and long side respectively. Then a d-dimension Fully-Connected (FC) layer is applied on the above vectors to obtain the geometry embeddings FG = {g1,g2, ...,gN} \u2208 RN\u00d7d.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2143856572",
                        "name": "Hao Liu"
                    },
                    {
                        "authorId": "2153898418",
                        "name": "Xin Li"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "44220fb755ed8259a6d1183e47e5e957ab5b0874",
                "externalIds": {
                    "CorpusId": 253857301
                },
                "corpusId": 253857301,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/44220fb755ed8259a6d1183e47e5e957ab5b0874",
                "title": "Relative Layout Matching for Document Data Extraction",
                "abstract": "This thesis explores the field of business document information extraction, emphasizing one-shot learning systems that improve their performance by utilizing a database of previously processed documents. A benchmark to evaluate one-shot information extraction systems was defined and used with a newly created dataset. A novel representation-learning approach to one-shot document information extraction was proposed. For a newly received document, the proposed approach uses learned document representation to first retrieve field representations from similar documents. Retrieved representations are then used to localize information on the newly received document. The proposed method was evaluated and compared against several proposed baselines showing an improvement on fields with high positional variance. The baseline method still achieves better results on fields that remain fixed within the layout.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "2071515342",
                        "name": "Maty\u00e1s Skalick\u00fd"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "They presented a new large-scale synthetic dataset for the problem of table recognition [17]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "2ce9974fe0fffffb6f852e56e23672cfb41b6567",
                "externalIds": {
                    "CorpusId": 255227048
                },
                "corpusId": 255227048,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/2ce9974fe0fffffb6f852e56e23672cfb41b6567",
                "title": "OVERVIEW OF OCR TOOLS FOR THE TASK OF RECOGNIZING TABLES AND GRAPHS IN DOCUMENTS",
                "abstract": "This study describes OCR tools for recognizing tables and graphs. There is a great demand for solutions that can effectively automate the processing of an extensive array of documents. Existing OCR solutions can efficiently recognize text, but recognizing graphical elements, such as charts and tables, is still in the making. Solutions that can increase the accuracy of visual data recognition can be valuable for technical document processing, such as scientific, financial, and analytical documents.",
                "year": 2022,
                "authors": [
                    {
                        "authorId": "143737606",
                        "name": "O. Yaroshenko"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "bc0569bf0bc49a9ce92b25fd013eddcc8029b94d",
                "externalIds": {
                    "MAG": "3156157843",
                    "DOI": "10.1007/978-3-030-67892-0_16",
                    "CorpusId": 234917709
                },
                "corpusId": 234917709,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/bc0569bf0bc49a9ce92b25fd013eddcc8029b94d",
                "title": "Search for Falsifications in Copies of Business Documents",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "7621741",
                        "name": "O. Slavin"
                    },
                    {
                        "authorId": "144236189",
                        "name": "E. Andreeva"
                    },
                    {
                        "authorId": "35343060",
                        "name": "V. Arlazarov"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "b6cf21484943bdd6525fb9c4c0be4c012c3ea7c6",
                "externalIds": {
                    "MAG": "3156634721",
                    "DOI": "10.1007/978-3-030-67892-0_15",
                    "CorpusId": 234950094
                },
                "corpusId": 234950094,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/b6cf21484943bdd6525fb9c4c0be4c012c3ea7c6",
                "title": "Models and Methods Flexible Documents Matching Based on the Recognized Words",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "7621741",
                        "name": "O. Slavin"
                    },
                    {
                        "authorId": "35343060",
                        "name": "V. Arlazarov"
                    },
                    {
                        "authorId": "9207994",
                        "name": "I. Tarkhanov"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "B Categories of table styles in the synthetic dataset The synthetic dataset introduced in [4] contains four categories of table styles.",
                "S3: Sample table image of the four categories of table styles defined in [4]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d45eafdc47815b03dead36776c455153593332be",
                "externalIds": {
                    "DOI": "10.1016/b978-0-323-69618-0.15001-2",
                    "CorpusId": 231774893
                },
                "corpusId": 231774893,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/d45eafdc47815b03dead36776c455153593332be",
                "title": "Supplemental Material",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2113309578",
                        "name": "Xu Zhong"
                    },
                    {
                        "authorId": "9585722",
                        "name": "Elaheh Shafieibavani"
                    },
                    {
                        "authorId": "35207071",
                        "name": "A. Yepes"
                    }
                ]
            }
        },
        {
            "intents": [
                "methodology"
            ],
            "contexts": [
                "Bottom-up methods [5,26,27,40] first detect cells or text segments using detection models or OCR engines, then analyze the relations between neighbouring cells using GNN or LSTM."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "24dd39035349c38d0bff17bbe5203cfcfd0ad79a",
                "externalIds": {
                    "DBLP": "conf/icdar/LiYZL21",
                    "DOI": "10.1007/978-3-030-86549-8_6",
                    "CorpusId": 237458405
                },
                "corpusId": 237458405,
                "publicationVenue": {
                    "id": "991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                    "name": "IEEE International Conference on Document Analysis and Recognition",
                    "type": "conference",
                    "alternate_names": [
                        "IEEE Int Conf Doc Anal Recognit",
                        "International Conference on Document Analysis and Recognition",
                        "Int Conf Doc Anal Recognit",
                        "ICDAR"
                    ],
                    "url": "http://www.wikicfp.com/cfp/program?id=1327"
                },
                "url": "https://www.semanticscholar.org/paper/24dd39035349c38d0bff17bbe5203cfcfd0ad79a",
                "title": "Adaptive Scaling for Archival Table Structure Recognition",
                "abstract": null,
                "year": 2021,
                "authors": [
                    {
                        "authorId": "2118890804",
                        "name": "Xiaohe Li"
                    },
                    {
                        "authorId": "145820427",
                        "name": "Fei Yin"
                    },
                    {
                        "authorId": "2870877",
                        "name": "Xu-Yao Zhang"
                    },
                    {
                        "authorId": "1689269",
                        "name": "Cheng-Lin Liu"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "0d7a65f01299aa6105e8d8aeccd9ee04cbb19d6c",
                "externalIds": {
                    "DBLP": "conf/iniscom/2020",
                    "DOI": "10.1007/978-3-030-63083-6",
                    "CorpusId": 227076558
                },
                "corpusId": 227076558,
                "publicationVenue": {
                    "id": "96b61289-172a-48b7-b9c0-93b525b45575",
                    "name": "International Conference on Industrial Networks and Intelligent Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Ind Netw Intell Syst",
                        "INISCOM"
                    ],
                    "issn": "2411-7129",
                    "url": "https://eudl.eu/proceedings/INISCom/2015"
                },
                "url": "https://www.semanticscholar.org/paper/0d7a65f01299aa6105e8d8aeccd9ee04cbb19d6c",
                "title": "Industrial Networks and Intelligent Systems: 6th EAI International Conference, INISCOM 2020, Hanoi, Vietnam, August 27\u201328, 2020, Proceedings",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "1834516",
                        "name": "Nguyen-Son Vo"
                    },
                    {
                        "authorId": "2190756",
                        "name": "Van\u2010Phuc Hoang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "d760645d5363cf8d1309d0cb71d5cd1d47f34d1d",
                "externalIds": {
                    "DBLP": "conf/iniscom/NguyenVZBDNLH20",
                    "MAG": "3110339029",
                    "DOI": "10.1007/978-3-030-63083-6_12",
                    "CorpusId": 229183023
                },
                "corpusId": 229183023,
                "publicationVenue": {
                    "id": "96b61289-172a-48b7-b9c0-93b525b45575",
                    "name": "International Conference on Industrial Networks and Intelligent Systems",
                    "type": "conference",
                    "alternate_names": [
                        "Int Conf Ind Netw Intell Syst",
                        "INISCOM"
                    ],
                    "issn": "2411-7129",
                    "url": "https://eudl.eu/proceedings/INISCom/2015"
                },
                "url": "https://www.semanticscholar.org/paper/d760645d5363cf8d1309d0cb71d5cd1d47f34d1d",
                "title": "Table Structure Recognition in Scanned Images Using a Clustering Method",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "2757688",
                        "name": "N. V. Nguyen"
                    },
                    {
                        "authorId": "1752866615",
                        "name": "Hanh Vu"
                    },
                    {
                        "authorId": "2070874816",
                        "name": "Arthur Zucker"
                    },
                    {
                        "authorId": "2037496520",
                        "name": "Younes Belkada"
                    },
                    {
                        "authorId": "49343821",
                        "name": "H. Do"
                    },
                    {
                        "authorId": "2037502069",
                        "name": "Doanh Ngoc-Nguyen"
                    },
                    {
                        "authorId": "47267436",
                        "name": "T. T. Le"
                    },
                    {
                        "authorId": "31854609",
                        "name": "D. V. Hoang"
                    }
                ]
            }
        },
        {
            "intents": [],
            "contexts": [],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "e4506cdc954ad4eb3d752dd75b342aea0701df54",
                "externalIds": {
                    "MAG": "3109115753",
                    "DBLP": "conf/eccv/SarkarAJGK20",
                    "DOI": "10.1007/978-3-030-58604-1_39",
                    "CorpusId": 221771142
                },
                "corpusId": 221771142,
                "publicationVenue": {
                    "id": "167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                    "name": "European Conference on Computer Vision",
                    "type": "conference",
                    "alternate_names": [
                        "ECCV",
                        "Eur Conf Comput Vis"
                    ],
                    "url": "https://link.springer.com/conference/eccv"
                },
                "url": "https://www.semanticscholar.org/paper/e4506cdc954ad4eb3d752dd75b342aea0701df54",
                "title": "Document Structure Extraction Using Prior Based High Resolution Hierarchical Semantic Segmentation",
                "abstract": null,
                "year": 2020,
                "authors": [
                    {
                        "authorId": "39230373",
                        "name": "Mausoom Sarkar"
                    },
                    {
                        "authorId": "6657914",
                        "name": "Milan Aggarwal"
                    },
                    {
                        "authorId": "1432235684",
                        "name": "Arneh Jain"
                    },
                    {
                        "authorId": "1381295186",
                        "name": "Hiresh Gupta"
                    },
                    {
                        "authorId": "145846953",
                        "name": "Balaji Krishnamurthy"
                    }
                ]
            }
        },
        {
            "intents": [
                "background"
            ],
            "contexts": [
                "Experimental results indicate that our model achieves robustness across different transformations (all transformations, including scaling, rotation, adding noise, blurring, random cropping, and more) [46]."
            ],
            "isInfluential": false,
            "citingPaper": {
                "paperId": "120b87ae7e16613c3f13f351cefea12879595fbe",
                "externalIds": {
                    "DOI": "10.25046/aj0506187",
                    "CorpusId": 231690992
                },
                "corpusId": 231690992,
                "publicationVenue": null,
                "url": "https://www.semanticscholar.org/paper/120b87ae7e16613c3f13f351cefea12879595fbe",
                "title": "A Computational Modelling and Algorithmic Design Approach of Digital Watermarking in Deep Neural Networks",
                "abstract": "Article history: Received: 21 October, 2020 Accepted: 12 December, 2020 Online: 25 December, 2020 In this paper we propose an algorithmic approach for Convolutional Neural Network (CNN) for digital watermarking which outperforms the existing frequency domain techniques in all aspects including security along with the criteria in the neural networks such as conditions embedded, and types of watermarking attack. This research addresses digital watermarking in deep neural networks and with comprehensive experiments through computational modeling and algorithm design, we examine the performance of the built system to demonstrate the potential of watermarking neural networks. The inability of intruder towards the retrieval of data without the knowledge of architecture and keys is also discussed and results of the proposed method are compared with the state of the art methods at different noises and attacks.",
                "year": 2020,
                "authors": [
                    {
                        "authorId": "145396829",
                        "name": "R. Kavitha"
                    },
                    {
                        "authorId": "9353536",
                        "name": "U. Eranna"
                    },
                    {
                        "authorId": "145297449",
                        "name": "M. N. Giriprasad"
                    }
                ]
            }
        }
    ]
}