text,target_M6_predict,target_predict_M6_label
"Further, [23] studied transfer learning with winning tickets, [24] explored the generalization of lottery ticket initializations for efficient downstream adaptation, and [25] proposed inducing adversarial robustness within the transfer learning pipeline as a means of improving transferability.",1,neutral
[5] [33] pooled all layers together and determined pruning thresholds for different layers in an integrated fashion.,1,neutral
"Given non-standardized adoption of CNN models for experimentations in post-training pruning works, we examined as most models as appeared in various literature and add those as baselines in our comparison, including Global [33], Uniform [45], Uniform+ [13], LAMP [25], ER ker.",2,positive
[109] find OneTicket that can generalize across a variety of datasets and optimizers within the natural image domain.,1,neutral
"[109] observe that for image classification, winning tickets generated on larger datasets (such as bigger training set size and/or more number of classes) consistently transferred better than those generated with smaller datasets.",1,neutral
"However, this kind of method is not friendly to those significant weights whose importance is not immediately apparent at the beginning [109].",1,neutral
"(2) Some literature ([67, 109, 110, 111]) studies the transferability of a winning ticket found in a source dataset to another dataset, which provides insights into the transferability of LTH.",1,neutral
"Work by Morcos et al.[18] has already shown that for certain tasks such as computer vision, winning tickets can be transferred across different tasks/models.",1,neutral
"In fact, transferability of winning lottery tickets has been shown in several fields [16],[17],[18] including across networks with different architectures [19].",1,neutral
"Inspired by [47], we conduct experiments to study the transferability of FSTs.",2,positive
"This is because FSTs drawn from large datasets are less likely to suffer from overfitting, which is consitent with [47].",1,neutral
"Given the fact that CNN models are generally overparameterized, many works [11, 37, 39, 50] have demonstrated a sparse sub-network can still reach the accuracy comparable to the original dense network and many channels in each layer can be taken away without harming the performance.",1,neutral
We take inspiration by the work of Morcos et al. (2019) and re-train sparse initializations generated for one task-ES configuration on a different setting with a shared network architecture.,2,positive
"Specifically, it has been shown that the winning ticket can be transferred between datasets and tasks [19, 47, 13, 57].",1,neutral
"Other works demonstrated the generality of the winning ticket and showed that a single pruned network can be transferred across datasets and achieve good performance after fine-tuning [19, 47, 13] and even that LT can be used for non-natural datasets [57].",1,neutral
Another study [13] conducted experiments to demonstrate,1,neutral
"In the natural image domain, Morcos et al. (2019) finds that the winning ticket initializations generalize across a variety of vision benchmark datasets.",2,positive
"1) Experiment Setup: To answer the RQ3, we select 4 state-of-the-art model compression techniques proposed in the last two years (i.e., LAMP [30], Global [31], Uniform+ [32] and ERK [33]) to see whether our DeepArc framework can speed up the model compression efficiency.",2,positive
", LAMP [30], Global [31], Uniform+ [32] and ERK [33]) to see whether our DeepArc framework can speed up the model compression efficiency.",2,positive
"In the field of transferring sparse pretrained models, several works have explored the effect of different unstructured pruning techniques during pretraining on transfer performance (Mehta, 2019; Morcos et al., 2019; Paganini & Forde, 2020b; Sabatelli et al., 2020; Sun et al., 2022; Liu et al., 2022).",1,neutral
"…the field of transferring sparse pretrained models, several works have explored the effect of different unstructured pruning techniques during pretraining on transfer performance (Mehta, 2019; Morcos et al., 2019; Paganini & Forde, 2020b; Sabatelli et al., 2020; Sun et al., 2022; Liu et al., 2022).",1,neutral
"For example, there have been many attempts at pruning convolutional neural networks (CNNs) which include the original paper on lottery tickets itself [6] (80–90% prune rate), a generalized approach [7] to prune across many vision datasets (such as MNIST, SVHN, CIFAR-10/100, ImageNet, and Places365), and an object detection variant [8] pruned up to 80%.",1,neutral
"Second, in the generalized work on pruning CNNs [7] they mention larger datasets tend to produce better lottery tickets.",1,neutral
"Presented at the Sparsity Workshop at the 11 th International Conference on Learning Representations , Kigali, Rwanda, 2023.
cost (Morcos et al., 2019) including memory consumption and inference time, and additionally enable wide-spread democratization of DNN’s with a low carbon footprint.",2,positive
"cost (Morcos et al., 2019) including memory consumption and inference time, and additionally enable wide-spread democratization of DNN’s with a low carbon footprint.",2,positive
"This includes the transferability of lottery tickets between different datasets [37, 38, 39, 40, 41, 42] and even between different models [43].",1,neutral
[110] illustrated the computational expense of determining winning tickets in the lottery ticket hypothesis.,1,neutral
"Finally, the resulting ⊙ is the winning ticket of the LTH by using the surrogate dataset.",1,neutral
transferable to the target task on the basis of the observations in a previous study [7].,0,negative
"The LTH [3, 7, 16] claims that randomly initialized overparametrized DNNs can contain certain sub-networks called winning tickets that can achieve comparable test accuracy of the original DNNs, even with smaller parameter size of the networks.",1,neutral
The pre-training stage finds the winning ticket of the LTH through pruning the original model after training using the surrogate dataset.,0,negative
The server has a surrogate dataset to pre-train a model for finding the winning ticket of the LTH.,2,positive
"We propose an FL method that initializes a model at the server by using the winning ticket of the LTH found with a surrogate dataset, as shown in Fig.",2,positive
"Although realizations of the winning ticket depend on both the model and dataset used for the training, research showed that the winning tickets obtained from large datasets could be transferable to tasks on other datasets [7] in several cases.",1,neutral
We proposed and evaluated a communication-efficient FL method by imposing the LTH-based initialization with a surrogate dataset before the FL process.,2,positive
"The LTH claims that randomly initialized DNNs contain smaller “good” sub-networks, called “winning tickets” that can attain similar performance as the original DNNs even with their smaller model sizes.",2,positive
Li et al. proposed LotteryFL that integrates FL and the LTH [4].,2,positive
"At the pre-training stage, a surrogate dataset is used to extract the sub-network of the original model, i.e., the winning ticket of the LTH, at the server.",2,positive
"To address the issue, using sub-networks extracted from the original DNNs on the basis of the lottery ticket hypothesis (LTH) [3] has been proposed and investigated [4–6].",2,positive
"from the winning ticket for the target task in general, findings on the winning tickets [7] suggest that the resulting ticket by a different dataset can be transferable to other tasks.",1,neutral
[22] elaborately discussed the generalizing and transferring capability of lottery tickets.,1,neutral
"Later [22], [39] improved the LTH idea.",1,neutral
"However, the transfer of weight initializations across datasets was first analyzed by [22].",1,neutral
[22] observed an intriguing fact about LTH regarding the transferability of lottery tickets.,0,negative
[22]: Winning tickets are computationally expensive to generate because of the repetitive train-prune-rewind cycle.,0,negative
[21] and [22] have shown that random tickets with and without masking preserved results in lower performance than winning tickets with equal parameters.,1,neutral
Later works like [22] demonstrate that these tickets generated from one dataset can even be transferred to another and achieve accuracies comparable to the original network.,2,positive
"Pruning heuristics of our proposed method is motivated by previous work [21], [22].",2,positive
"Such division is different from previous work done by [22], because they performed experiments by dividing Cifar-10 into parts where each part consisted of all ten classes.",1,neutral
"According to [22], both tickets from two separate partitions are transferable to the entire Cifar-100 dataset.",0,negative
"Recent work has shown that LTP pruned models can generalize across a variety of datasets within an application domain as well as with different optimizers [12], [30].",1,neutral
ity property [30]; 2) The transferred tickets act as a regularizer and prevents overfitting while training [30]; and 3) winning tickets learn generic inductive biases which improve training.,1,neutral
"Empirical studies based on this statement show that NNs could potentially be significantly smaller without sacrificing accuracy (Chen et al., 2020; Frankle et al., 2019; Gale et al., 2019; Liu et al., 2018; Morcos et al., 2019; Zhou et al., 2019; Zhu and Gupta, 2017).",2,positive
"Such winning tickets have demonstrated their abilities to transfer across tasks and datasets (Morcos et al., 2019; Yu et al., 2020; Desai et al., 2019).",2,positive
"They have also not included a comparison of Global MP to SOTA algorithms [12], [13], [14], [15], [16], [17].",1,neutral
"prior works have used Global MP as a baseline [12], [13],",1,neutral
"Since the lottery hypothesis was put forward, much work has been done on expansion[3,6,28] and theoretical research[25,29].",1,neutral
"This outstanding work sparks an interest in the deep learning community, and numerous related PaI researches have emerged [4, 24, 26, 36, 39, 41].",1,neutral
Morcos et al. (2019) unveiled that the winning tickets discovered using larger datasets consistently transferred better than those generated using smaller datasets.,1,neutral
"Such cost poses several challenges for the research community: the training of a network model is associated to large carbon footprints and the commercialization of of AI research (especially for edge devices) is hindered by the resource requirements of the models Strubell et al. [2019]. For several years now, many works in literature have shown that is possible to shrink both the size and resource requirements, mainly via quantization Yang et al.",2,positive
"Such cost poses several challenges for the research community: the training of a network model is associated to large carbon footprints and the commercialization of of AI research (especially for edge devices) is hindered by the resource requirements of the models Strubell et al. [2019]. For several years now, many works in literature have shown that is possible to shrink both the size and resource requirements, mainly via quantization Yang et al. [2020], Jin et al.",2,positive
"Pruning can be performed either before training a model Lee et al. (2018); Wang et al. (2020), early in training Rachwan et al. (2022); You et al. (2020), or after the model has been fully trained Frankle et al. (2020a,b); LeCun et al. (1990); Morcos et al. (2019).",2,positive
"A line of work [51,52,16,7] discover the existence of transferable winning tickets from source dataset and successfully transfer it to target dataset.",1,neutral
"To address this, a line of work [51,52,16,7] discover the existence of transferable winning tickets from source dataset and successfully transfer it to target dataset, thus eliminating search cost.",1,neutral
"Unfortunately, such techniques do not show comparable performance with the original IMP methods, thus mainstream LTH leverages IMP as a pruning scheme [25,83,52].",1,neutral
Most of the work investigating the use of sparse sparse training in RL are in the context of the lottery ticket hypothesis; Morcos et al. (2019) studied the existence of lucky sparse initializations using pruning and late-rewinding; Hasani et al. (2020) proposed an interesting approach by…,1,neutral
"(Chen et al., 2020b; Desai et al., 2019; Morcos et al., 2019; Mehta, 2019) investigate the transferability across different datasets (i.",2,positive
"(Chen et al., 2020b; Desai et al., 2019; Morcos et al., 2019; Mehta, 2019) investigate the transferability across different datasets (i.e., dataset transfer), while other pioneers study the transferability of pre-trained tickets from supervised and self-supervised vision pre-training (Chen et al.,…",1,neutral
"In LTH, first a large network is trained and pruned to be a small subnetwork S, then retraining S using its original initialization yields comparable or even better performance, while retraining S with a different initialization performs much worse.",2,positive
"Other empirical observations like lottery ticket hypothesis (LTH) (Frankle & Carbin, 2019; Morcos et al., 2019; Tian et al., 2019; Yu et al., 2020), recently also verified in CL (Chen et al.",1,neutral
"Other empirical observations like lottery ticket hypothesis (LTH) (Frankle & Carbin, 2019; Morcos et al., 2019; Tian et al., 2019; Yu et al., 2020), recently also verified in CL (Chen et al., 2021), may also be explained similarly.",1,neutral
"For LTH, our explanation is that S contains weights that are initialized luckily, i.e., close to useful local optima and converge to them during training.",2,positive
"LTH [17] inspired many to study the transferability of the ""winning ticket"", or sparse subnetwork architecture in general, across domains [45, 47, 51].",1,neutral
"[45] showed that winning ticket initializations generalized across a variety of natural image datasets, suggesting that different tasks seem to enjoy the same sparse subnetwork structure.",1,neutral
"This phenomenon has been observed in multiple applications, including computer vision (Morcos et al., 2019; Frankle et al., 2020) and natural language processing (Gale et al.",1,neutral
"This phenomenon has been observed in multiple applications, including computer vision (Morcos et al., 2019; Frankle et al., 2020) and natural language processing (Gale et al., 2019; Yu et al., 2020).",1,neutral
[144] |Θ i | ✓ ✗ ✗ Datasets/Optimizers ✗ Zhang et al.,1,neutral
"resetting the weights to values reached early in the first, dense training [120, 144, 149].",1,neutral
"Costs for finding LTs via iterative magnitude pruning can be reduced by using early stopping and low precision training for each pre-training iteration [146], sharing LTs for different datasets and optimizers [144] or iteratively reducing the dataset together with the number of non-zero parameters [145].",1,neutral
"To overcome 143 this problem, Morcos et al. (2019) proposed to 144 transfer the WT structure from source tasks to re- 145 lated tasks in the computer vision (CV) field.",2,positive
[75] proposed to use one generalized lottery tickets for all vision benchmarks and got comparable results with the specialized lottery tickets; Frankle et al.,1,neutral
"Other ways of reducing the computational cost include finding a good subnetwork and then fine-tuning (Sreenivasan et al., 2022b), and transferring lottery tickets (Morcos et al., 2019; Chen et al., 2021c).",2,positive
", 2022b), and transferring lottery tickets (Morcos et al., 2019; Chen et al., 2021c).",0,negative
The transferability of winning tickets has been investigated in [23].,0,negative
"[23] Ari Morcos, Haonan Yu, Michela Paganini, and Yuandong Tian.",0,negative
"In [8, 30] a ”lottery ticket hypothesis” was proposed that with an optimal substructure of the neural network acquired by weights pruning directly train a pruned model could reach similar results as pruning a pre-trained network.",1,neutral
"[30] Ari S Morcos, Haonan Yu, Michela Paganini, and Yuandong Tian.",0,negative
"To evaluate our experiments, we perform three popular image classification tasks including MNIST handwritten digit recognition [6], CIFAR-10 image classification [26], and Fashion-MNIST fashion image classification [42].",2,positive
"This view also provides an explanation of the empirically observed phenomenon that universal tickets achieve good performance across a number of tasks only at moderate sparsity levels and become more universal when trained on larger datasets (Morcos et al., 2019; Chen et al., 2020).",1,neutral
"The significant computational cost associated with finding good lottery tickets has motivated the quest for universal tickets that can be transferred to different tasks (Morcos et al., 2019; Chen et al., 2020).",1,neutral
"They propose an initialization scheme that extends standard approaches like He (He et al., 2015) or Glorot (Glorot & Bengio, 2010) initialization to non-zero biases and supports the existence of LTs while keeping the large mother network f0 trainable.",2,positive
"The independence of the functions is not required and could be replaced by dictionaries, but the independence aids the compression of the bottom layers and thus our objective to find sparse LTs.",2,positive
"Even though the function families above can be represented efficiently as LTs, we should mention that a big advantage of neural networks is that the actual function family can be learned.",1,neutral
"Before we can prove the existence of strong universal LTs, we have to formalize our notion of what makes a strong LT universal.",1,neutral
"To make meaningful progress in deriving a notion of universal LTs, we therefore need a stronger simplification.",1,neutral
"Independently from LTs, we discuss conditions when this is a promising approach, i.e., when the bottom layers of the deep neural network represent multivariate (basis) functions, whose linear combination can represent a large class of multivariate functions.",1,neutral
Theorem 3 (Multivariate LTs (single layer)).,1,neutral
"In the following, we discuss the existence of polynomial LTs in more detail.",1,neutral
"Next, we can utilize our improved LT pruning results to prove the existence of universal LTs.",2,positive
"However, Morcos et al. (2019) posited the existence of so-called universal lottery tickets that, once identified, can be effectively reused across a variety of settings.",1,neutral
"It is therefore reasonable to transfer it to LTs (Morcos et al., 2019).",0,negative
"[28], which shows that the winning ticket discovered from a dataset can be transferred to a highly relevant dataset.",1,neutral
"Ensuing work confirmed this, showing that winning tickets are able to perform well on a variety of tasks beyond the one that they were originally discovered on [8, 17, 29, 30, 37, 39].",0,negative
"Given that the development of the RG led to a first principled understanding of universal behavior near phase transitions, as well as a way in which to characterize materials by such behavior, we reasoned that viewing IMP from an RG perspective may prove useful when studying the universality of winning tickets [4, 5, 8, 17, 29, 30, 37, 39] and the general success IMP has found as a tool for interrogating DNNs [15].",1,neutral
"In recent years, researchers have found an intriguing corollary: winning tickets found in the context of one task can be transferred to related tasks [4, 5, 8, 17, 29, 30, 37, 39], possibly even across architectures [7].",1,neutral
"…performance for different winning ticket sub-networks within the same dataset. on the other hand, the work of Ibrahim et al. (Alabdulmohsin et al. 2021; Morcos et al. 2019) shown the existence of winning ticket sub-networks with the ability to generalize across datasets and training policies.",2,positive
(Alabdulmohsin et al. 2021; Morcos et al. 2019) shown the existence of winning ticket sub-networks with the ability to generalize across datasets and training policies.,2,positive
"…2017) • A simple MLP model, created by removing layers from
the TabTransformer model (see (Huang et al. 2020), §3.1, paragraph 1)
• A sparse MLP, based on (Morcos et al. 2019) • TabTransformer (Huang et al. 2020) • TabNet (Arik and Pfister 2020) • Variational Information Bottleneck (VIB) (Alemi…",2,positive
"The above three-step procedure mostly originates from the prominent line of work of the Lottery Ticket Hypothesis [16, 8, 17, 19, 38, 44, 66, 67] (or LTH): i.",1,neutral
"The above three-step procedure mostly originates from the prominent line of work of the Lottery Ticket Hypothesis [16, 8, 17, 19, 38, 44, 66, 67] (or LTH): i.e., the idea that a pre-trained model contains “lottery tickets” (i.e., smaller subnetworks) such that if we select those “tickets” cleverly, those submodels do not lose much in accuracy while reducing significantly the size of the model.",1,neutral
"In addition, the existing pruning approaches usually perform the iterations of parameter removal and accuracy recovery at each layer, making it hard to achieve a global optimum [14], [17] with few retraining iterations.",1,neutral
"Consequently, the pruned network can not achieve the optimal accuracy [17].",1,neutral
…the performance of TabTransformer against following four categories of methods: (a) Logistic regression and GBDT; (b) MLP and a sparse MLP following Morcos et al. (2019); (c) TabNet model of Arik & Pfister (2019); and (d) the Variational Information Bottleneck model (VIB) of Alemi et al. (2017).,2,positive
"[34, 35] have studied the lottery ticket hypothesis in unsupervised learning to reveal how well the tickets are transformed between different datasets.",1,neutral
"Recent studies of deep learning have demonstrated that redundancy of parameters (deep layers of hidden variables) reduces generalization error, contrary to the implications of Occam’s razor (He et al. 2019; Morcos et al. 2019; Nagarajan and Kolter 2019).",1,neutral
"If the hypothesis holds for a given network, then we can reduce the computational cost by using the sparse subnetwork instead of the entire network while maintaining the accuracy [14, 19].",1,neutral
"Many properties and applications have been studied in subsequent works, such as transferability of winning subnetworks [19], sparsification before training [14, 27, 26] and further ablation studies on the hypothesis [29].",1,neutral
"…there are still many other promising avenues that could lead to efficient sparse training, for example, transferring existing lottery tickets Morcos et al. (2019); Mehta (2019), pruning weights during training (You et al., 2019), or dynamically changing the mask during training (Evci et…",2,positive
"For example, Morcos et al. (2019) hint at ""the possibility of parameterizing the distribution of such tickets, allowing us to sample generic, dataset-independent winning tickets.""",1,neutral
"Another intriguing property of lottery tickets, the transferability, has also been thoroughly examined (Mehta, 2019; Morcos et al., 2019; Desai et al., 2019; Chen et al., 2020b;a).",0,negative
"Although it has been well studied in discriminative models (Mehta, 2019; Morcos et al., 2019; Chen et al., 2020b), an in-depth understanding of transfer learning in GAN tickets is still missing.",1,neutral
Mehta (2019); Morcos et al. (2019); Desai et al. (2019) are the pioneers to study the transferability of found subnetworks.,2,positive
"Aside from training from scratch, such winning tickets have demonstrated their abilities to transfer across tasks and datasets (Morcos et al., 2019; Yu et al., 2019; Desai et al., 2019; Chen et al., 2020a).",2,positive
"Besides training from scratch, researchers also explore the existence of winning tickets under transfer learning regimes for over-parametrized pre-trained models across various tasks and datasets (Morcos et al., 2019; Yu et al., 2019; Desai et al., 2019; Chen et al., 2020a).",2,positive
Similar ticket transfer results were reported by Morcos et al. (2019) and Mehta (2019) in the context of optimizers and vision datasets.,0,negative
"Other studies concerned hyperparameters modifications [24, 25], and concentrated on the transferability [16, 26] of the pruned networks.",1,neutral
It has been actually shown that the same winning lottery ticket generalizes across training conditions and similar datasets [16].,0,negative
"This directly relates to the good transfer properties of the subnetworks corresponding to the winning tickets [16, 26].",1,neutral
"formance as the original network, when trained in isolation [16, 32].",1,neutral
"neural network parameters space properties, gives us insights on the possible existence of low-dimensional manifolds in the network parameter space [14, 16, 20, 30, 32, 37].",1,neutral
"The inaccessibility of W∗ has also be noticed in [56]–[58], proving the importance of our NTAA in synchronously learning the network weights and the architecture in the same way as our NTAA.",2,positive
The inaccessibility of W∗ has also been discussed in the theory of lottery ticket hypothesis [56]–[58].,1,neutral
"Prior work [33] found that a winning ticket of one dense network can generalize across datasets and optimizers, beyond the original training setting where it was identified.",1,neutral
"[33, 3] studied the transferability of winning tickets between datasets; the former focuses on showing one winning ticket to generalize across datasets and optimizers; and the latter investigates LTH in large pre-trained NLP models, and demonstrates the winning ticket transferability across downstream tasks.",1,neutral
"The lottery ticket literature [8, 62, 34, 42, 10] regards a dense network as a set of hypotheses (subnetworks).",1,neutral
"As baselines, we consider Global (Morcos et al., 2019), Uniform (Zhu and Gupta, 2017), and Adaptive (Gale et al., 2019) pruning techniques and LAMP (Lee et al., 2021).",2,positive
"Layerwise pruning ratio has also been investigated for NNs pruned at initialization since the explosion of the Lottery Ticket Hypothesis (Frankle and Carbin, 2019; Morcos et al., 2019).",1,neutral
"As baselines, we consider Global (Morcos et al., 2019), Uniform (Zhu and Gupta, 2017), and Adaptive (Gale et al.",1,neutral
(Morcos et al. 2019) showed successful results of model compression by generalized lottery ticket hypothesis across different benchmark datasets and popular DNN architectures.,2,positive
Morcos et al. (Morcos et al. 2019) showed successful results of model compression by generalized lottery ticket hypothesis across different benchmark datasets and popular DNN architectures.,2,positive
Morcos et al. (2019) studies the transferability of winning tickets between datasets and optimizers.,1,neutral
"[51, 50] showed those matching subnetworks to transfer between related classification tasks.",1,neutral
"…the performance of TabTransformer against following four categories of methods: (a) Logistic
regression and GBDT (b) MLP and a sparse MLP following (Morcos et al. 2019) (c) TabNet model of Arik and Pfister (2019) (d) and the Variational Information Bottleneck model (VIB) of Alemi et al. (2017).",2,positive
regression and GBDT (b) MLP and a sparse MLP following (Morcos et al. 2019) (c) TabNet model of Arik and Pfister (2019) (d) and the Variational Information Bottleneck model (VIB) of Alemi et al.,1,neutral
"Although some of these recent works have tried to answer the question – how well do the tickets transfer across domains [38, 37], when it comes to vision tasks – the buck stops at image classification.",1,neutral
"[37, 38] show that winning tickets transfer well across datasets.",1,neutral
"However, the study in [37] was limited to smaller datasets, like CIFAR-10 and FashionMNIST, and both [37, 38] are limited to classification tasks.",0,negative
"We also investigate the efficacy of methods introduced by [12, 38, 13, 42] such as iterative magnitude pruning, late resetting, early bird training, and layerwise pruning in the context of object recognition.",1,neutral
"This is in contrast with previous works related to ticket transfer in vision models [37, 38].",1,neutral
"[38] scrutinize the generalization properties of winning tickets and offer empirical evidence that winning tickets can be transferred across datasets and optimizers, in the realm of image classification.",1,neutral
[28] observed that an existing set of winning tickets from a dataset can be applied to a different dataset and with good performance.,1,neutral
(2019) explore a number of other interesting questions [69]:,1,neutral
"Morcos et al.(2019) carry out experiments in which lottery tickets are identified using one optimizer, ADAM (adaptive moment estimation), and then utilise a different optimizer, SGD (Stochastic Gradient Descent) with momentum, and vice versa on the CIFAR10 data.",2,positive
"in general, winning tickets are optimizer independent [69].",1,neutral
"Morcos et al.(2019) explore a number of other interesting questions [69]: • Are the lotteries found for one image classification task
transferable to other tasks?",1,neutral
"Furthermore, other work has shown that such subnetworks generalize well across datasets and tasks (Morcos et al., 2019).",1,neutral
"As an additional experiment, we test whether LAMP can provide a general-purpose layerwise sparsity for pruning schemes other than MP.",2,positive
"With the effectiveness of LAMP confirmed, we take a closer look at the layerwise sparsity discovered by LAMP.",2,positive
"One common method is the global MP criteria (see, e.g., Morcos et al. (2019)), ∗Work done at KAIST 1i.e., simultaneously training and pruning
ar X
iv :2
01 0.",2,positive
"Lastly, we observe that the heuristic of Gale et al. (2019) seems to provide an improvement over the Uniform MP.",2,positive
"5, we plot the layerwise survival rates and the number of nonzero weights discovered by iteratively pruning VGG-16 (trained on CIFAR-10), by Global MP, Erdős-Rényi kernel, and LAMP.",2,positive
"In Section 4, we empirically validate the effectiveness and versatility of LAMP.",2,positive
"We note that the gap between one-shot pruning and iterative pruning is quite small for LAMP; when 1.15% of all prunable weights survive, iterative LAMP brings only 1.09% accuracy gain over one-shot LAMP.",2,positive
"First, the right-hand side is free of any activation function, and is equivalent to the layerwise MP.",1,neutral
"Frankle & Carbin (2019) combine MP with weight rewinding to discover efficiently trainable subnetworks: for small nets, the authors employ uniform layerwise sparsity, but use different rates for convolutional layers and fully-connected layers (with an added heuristic on the last fully-connected layer); for larger nets, authors use global MP. Morcos et al. (2019) consider transferring the “winning ticket” initializations, using the global MP. Evci et al. (2020) proposes a training scheme for sparsely initialized neural networks, where the layerwise sparsity is given by the Erdős-Rényi kernel method; the method generalizes the scheme initially proposed by Mocanu et al. (2018) to convolutional neural networks.",2,positive
"A global threshold on the weight magnitudes is imposed on every layer to meet the global sparsity constraint, and the layerwise sparsity is automatically determined according to this threshold; see, e.g., Morcos et al. (2019).",1,neutral
"In search of a “go-to” layerwise sparsity for MP, we take a model-level distortion minimization perspective towards MP.",1,neutral
"We then observe that the problem can be relaxed to the minimization of Frobenius distortion in the weight tensor, whose solution coincides with the layerwise MP. Formally, let ξ ∈ Rn be an input vector to a fully-connected layer with the weight tensor W ∈ Rm×n.",1,neutral
"We sort and prune as in global MP, taking O(n log n) steps.7
We observe that step 4 is the dominant term, which is shared by the global MP.",1,neutral
"…convolutional layers and fully-connected layers (with an added heuristic on the last fully-connected layer); for larger nets, authors use global MP. Morcos et al. (2019) consider transferring the “winning ticket” initializations, using the global MP. Evci et al. (2020) proposes a training scheme…",2,positive
"Morcos et al. (2019) consider transferring the “winning ticket” initializations, and observe that global MP tend to work better with weight rewinding.",1,neutral
"A global threshold on the weight magnitudes is imposed on every layers to meet the global sparsity constraint, and the layerwise sparsity is automatically determined according to this threshold; see, e.g., Han et al. (2015); Morcos et al. (2019).",1,neutral
"Morcos et al. (2019); Sabatelli, Kestemont, and Geurts (2020) showed that LTs trained on large datasets transfer to smaller ones, but not vice versa.",1,neutral
"Given the close relationship between LTs and pruning solutions, the observation that LTs trained on large datasets transfer to smaller ones, but not vice versa (Morcos et al. 2019; Sabatelli, Kestemont, and Geurts 2020) can be explained by a common observation in transfer learning: networks trained in large datasets transfer to smaller ones.",1,neutral
"…relationship between LTs and pruning solutions, the observation that LTs trained on large datasets transfer to smaller ones, but not vice versa (Morcos et al. 2019; Sabatelli, Kestemont, and Geurts 2020) can be explained by a common observation in transfer learning: networks trained in large…",1,neutral
"CNN-based architectures such as VGG and ResNet) (Morcos et al. 2019), Reinforcement Learning and Natural Language Processing tasks (Yu et al. 2020).",1,neutral
"CNN-based architectures such as VGG and ResNet) (Morcos et al. 2019), Reinforcement Learning and Natural Language Processing tasks (Yu et al.",1,neutral
"In deeper networks, it
has been shown that IMP with late-rewinding (Frankle et al. 2019; Morcos et al. 2019) is more beneficial than rewinding to initialization.",1,neutral
"In practice however, pruning a large fraction of weights through one-shot pruning might null the weights that are actually important to the model leading to a significant drop in the performance (Morcos et al. 2019).",1,neutral
has been shown that IMP with late-rewinding (Frankle et al. 2019; Morcos et al. 2019) is more beneficial than rewinding to initialization.,2,positive
"Furthermore, other work has shown that such subnetworks generalize well across datasets and tasks (Morcos et al. (2019); Tanaka et al. (2020)).",2,positive
So it is not surprising to see that Figure A1 in [32] show evidence that the method may pass the layerwise rearrange check.,1,neutral
"(9)In the paper [32]’s Appendix A2 part, the authors report applying layerwise rearrange can hurt the performance when the pruning ratio is high.",0,negative
"Similar trends are also reported in Figure 3 of [41], Figure 1b of [32] and Figure 11 of [11].",0,negative
"In practice, sparse networks are mainly produced by the network pruning technique [13, 11, 21, 24, 9, 45, 16, 35, 34, 28], to name a few.",1,neutral
"Also, the relation of robustness, generalization, and pruning is a popular topic in many studies (Xu and Mannor 2012; Morcos et al. 2019; Arora et al. 2018).",1,neutral
"2020), and imposing weights sparsity (Arora et al. 2018; Morcos et al. 2019; Bartoldson et al. 2020).",1,neutral
"Note that, in addition to the polynomial terms, the relationship between accuracy and model sparsity is further modeled through an additional exponential term – a reasonable modeling assumption supported by prior knowledge of accuracy-sparsity curves in the pruning literature [9, 12, 25, 38, 31, 1, 10].",1,neutral
"Thus, for example, neural networks (NN) [33], which require many training samples, are not used in our solution, because the cost of collecting so many samples is prohibitive for HPC workflows.",1,neutral
"However, the intriguing prospect of ticket transfer [14] could provide such initializations right at the onset of training.",0,negative
"In that case, winning tickets could be transferred and trained on new tasks, even directly from their extremely lightweight versions [14].",0,negative
[20] evaluated the possibility to transfer the found tickets across optimizers or datasets.,0,negative
"Question 2: Are there patterns in the transferability of winning tickets? For example, do winning tickets transfer better from tasks with larger training sets [21] or tasks that are similar?",1,neutral
"However, the resulting subnetworks transfer between related tasks [21, 22].",1,neutral
"However, these subnetworks must be initialized to the state of the network θi after i steps of training [16, 21, 17] rather than to initialization θ0.",2,positive
[21] and Mehta [22] show that matching subnetworks transfer between vision tasks.,1,neutral
"In a similar setting, as demonstrated in the model pruning literature [7, 10, 26, 33, 38], having different pruning ratios for different layers of the network can further improve results over a single ratio across layers.",1,neutral
"…algorithm to find sparse subnetworks (winning tickets) within a dense, randomly initialized feedforward neural network, which can achieve comparable (and sometimes greater) performance to the original network, when trained separately (Frankle & Carbin, 2018; Zhou et al., 2019; Morcos et al., 2019).",1,neutral
"In this regard, the lottery ticket hypothesis (Frankle & Carbin, 2018), suggested an algorithm to find sparse subnetworks (winning tickets) within a dense, randomly initialized feedforward neural network, which can achieve comparable (and sometimes greater) performance to the original network, when trained separately (Frankle & Carbin, 2018; Zhou et al., 2019; Morcos et al., 2019).",1,neutral
"It is potentially transferable across vision tasks (Morcos et al., 2019).",2,positive
"These include challenging the need for unstructured, magnitude-based pruning [59], using learning rate warmup [16], selecting unimportant weights globally instead of layer by layer [16], and late resetting unpruned parameters to values achieved early in training, as opposed to rewinding the weights all the way to their initial values [17, 38].",1,neutral
"Additionally, preliminary evidence for LT transferability has been provided in both natural and non-natural image domains [38, 45].",0,negative
"On the other hand, lottery tickets have been shown to possess generalization properties that allow for their reuse across similar tasks, thus reducing the computational cost of finding dataset-dependent sparse sub-networks [38, 45].",1,neutral
Morcos et al. (2019) even showed that to some extent winning tickets can be transferred across different tasks and optimizers.,1,neutral
"In part, foresight pruning is motivated by evidence that specific sparse networks can be trained to yield comparable performance to the corresponding dense model (Liu et al., 2019; Frankle & Carbin, 2019; Morcos et al., 2019).",1,neutral
"A recent trend proposed to prune during training time (Frankle and Carbin, 2019; Gomez et al.; Desai et al., 2019; Morcos et al., 2019), yielding an iterative process.",2,positive
"Generally, research sped up the process of finding winning tickets (Morcos et al., 2019; Yin et al., 2020).",1,neutral
"Orthogonally, Morcos et al. (2019) or Desai et al. (2019) study whether winning tickets transfer across tasks or datasets.",1,neutral
"Iterative Magnitude Pruning (IMP) is a recently proposed pruning algorithm that has proven to be successful in finding extremely sparse trainable neural networks at initialization (winning lottery tickets) [10, 11, 12, 44, 45, 46, 47].",1,neutral
"Moreover, its been shown that some of these winning ticket subnetworks can generalize across datasets and optimizers [12].",2,positive
"This theory sparked a body of research examining the behaviour and obtainment of these winning tickets [18, 76, 54, 13, 15, 79, 17, 50], as well as some criticism [47, 19].",1,neutral
"[15] focused on the natural image domain, we investigate the possibility of transferring winning tickets obtained from the natural image domain to datasets in non natural image domains.",2,positive
"However, it also appears that there are stronger limitations to the transferability of winning initializations which were not observed by [15].",1,neutral
"The results obtained on the artistic datasets seem to suggest that winning initializations contain inductive biases that are strong enough to get at least successfully transferred to the artistic domain, therefore confirming some of the claims that were made in [15].",0,negative
"Besides the work presented in [15], there have been other attempts that aimed to better understand the LTH after studying it from a transfer-learning perspective.",1,neutral
"However, just as the study presented in [15], all this research limited its analysis to natural images.",1,neutral
"5, one research direction in particular has looked into how well winning ticket initializations can be transferred among different training settings (datasets and optimizers), an approach which aims at characterizing the winners of the LTH by studying to what extent their inductive biases are generic [15].",2,positive
We follow an experimental set-up which is similar to the one that was introduced in [15] (and that has been validated by [5]).,2,positive
"Following [6, 15], 31 winning tickets f(x;m θk) of increasing sparsity are obtained from each of these three datasets by repeating 31 iterations of network",1,neutral
"The closest approach to what has been presented in this work is certainly [15], which shows that winning models can generalize across datasets of natural images and across different optimizers.",1,neutral
"Constructing a winning ticket with parameters θk, instead of θ0, is a procedure which is known as late-resetting [4], and is a simple but effective trick that makes it possible to stably find winning-initializations in deep convolutional neural networks [4, 15].",1,neutral
"The “winning” initializations were shown to generalize across computer vision datasets (Morcos et al., 2019), and to exist both in LSTM and Transformer models for NLP tasks (Yu et al., 2020).",2,positive
"The “winning” initializations were shown to generalize across computer vision datasets (Morcos et al., 2019), and to exist both in LSTM and Transformer models for NLP tasks (Yu et al.",1,neutral
"Recently, several works have suggested the use of rewound weights from the original network as initializations [9,34,55].",1,neutral
[37] proposed a generalization method which allows reusing the same wining tickets across various datasets.,1,neutral
Morcos et al. (2019) show that these subnetworks transfer across tasks and datasets.,1,neutral
"The lottery ticket hypothesis was latter analyzed in several contexts [36], [37], [38].",1,neutral
"(14)We might consider finding a lottery ticket for BERT, which we would expect to fit the GLUE training data just as well as pre-trained BERT (Morcos et al., 2019; Yu et al., 2019).",0,negative
"14We might consider finding a lottery ticket for BERT, which we would expect to fit the GLUE training data just as well as pre-trained BERT (Morcos et al., 2019; Yu et al., 2019).",0,negative
[33] proposed a technique for sparsifying n over-parameterized trained neural model based on the lottery hypothesis.,1,neutral
"Lottery ticket mechanism since has been further explored, it has been shown that the winning tickets can be used across datasets [11], and that the tickets occur in other domains as well, such as in NLP [12].",2,positive
"Mainly, we answer the question: As is recently discovered for generic lottery ticket mechanism [11], that a winning ticket can generalize across datasets, can we extend this notion to DPLTM? Where we can use a publicly available dataset to get a winning ticket in a non-private setting, and then use that winning ticket to train a differentially private model on our sensitive dataset.",2,positive
"Similar to the experimental setup in [5], we divide the Cifar-10 dataset into two equal training splits namely Cifar-10a and Cifar-10b with 25k training samples in each,",2,positive
"It was found in [5] that global pruning outperforms local pruning when the larger networks are considered, and hence we adopt global pruning for this case as well.",1,neutral
"Similarly, in [5], the authors reported that the winning tickets generalized reasonably across changes in the training con guration.",0,negative
"In a recent study [5], it was found through extensive experimentation that the winning ticket initializations could be reused and are generalizable across models trained on di erent datasets and optimizers.",2,positive
"Following this pivotal work, several studies have been carried to understand the role of initialization, the e ect of the pruning criterion used and the importance of retraining the sub-networks [3, 4, 5, 6, 7, 8] for the success of lottery tickets.",1,neutral
"To this end, we adopt a setup similar to [5]: we investigate the transferability of tickets to di erent datasets from the same distribution.",2,positive
"For experiments with VGG-19 [33], we use its modi ed variant as in [5, 2], i.",1,neutral
"Note, this observation is consistent with results from earlier works on LTH such as [2, 5].",1,neutral
[14] (multilayer perceptron (MLP) is replaced by a fully connected layer).,1,neutral
"[14] show that winning tickets initializations transfer across different image classification datasets, thus suggesting that winning tickets do not entirely overfit to the particular data distribution on which they are found.",1,neutral
[14] have shown that winning tickets initializations can be re-used across different datasets with a common domain (natural images) trained on the same task (labels classification).,1,neutral
1 of [151].,1,neutral
"Morcos et al. (2019) show that subnetworks found by IMP with rewinding transfer between vision tasks, meaning the effort of finding a subnetworks can be amortized by reusing it many times.",1,neutral
In our work we use global pruning as used in the paper we are reproducing [4].,2,positive
The authors [4] empirically demonstrate that the structure of the subnetwork contains significant information.,2,positive
The authors of the original paper [4] did not release their code.,0,negative
We use hyperparameters provided by authors to maintain consistency with the paper we are reproducing [4].,2,positive
We employ late resetting of 1 epoch in all the experiments as used by the authors [4].,2,positive
"As a part of the NeurIPS Reproducibility Challenge’s Replication Track, we replicate the work done by [4] and investigate if the winning ticket initializations are generalizable across datasets and optimizers.",2,positive
"The paper we reproduce, ""One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers"" [4] provides empirical evidence that these winning ticket initializations generalize across multiple datasets as well as optimizers1.",2,positive
"For implementing the random ticket baseline, we generate random masks by globally permuting the winning masks as mentioned in [4].",1,neutral
"The initializations, number of epochs, learning rate annealing schedules are in accordance to [4] to maintain consistency of experiments.",1,neutral
1Authors used anywhere in this paper refers to the authors of the paper that we reproduce [4] 2The code base can be found at github.,2,positive
"2a), which helps to decide which ‘tickets’ (components) generalize and transfer model-knowledge [25] or specialize it.",1,neutral
Lottery tickets (LTs) have been show to generalize well to new tasks [44].,1,neutral
"Morcos et al. (2019) investigate the transferability of lottery tickets across multiple optimizers and datasets for supervised image classification, showing that tickets can indeed generalize (Morcos et al., 2019).",1,neutral
"…of the lottery ticket hypothesis—has shown that sparse networks can be, under certain conditions, easier to optimize (Frankle and Carbin, 2019; Morcos et al., 2019; Gale et al., 2019); and (2) sparser subnetworks have significantly less capacity than their large, over-parameterized…",2,positive
"In contrast, Morcos et al. (2019) transfers the entire ticket (sparse masks and initial values) to the target domain.",2,positive
"(2019) investigate the transferability of lottery tickets across multiple optimizers and datasets for supervised image classification, showing that tickets can indeed generalize (Morcos et al., 2019).",2,positive
"In Morcos et al. (2019), the authors refer to the transfer of initialization as both the transfer of the sparse topologies and the transfer of the initial values of the subnetworks.",1,neutral
", 2018b); and (2) if tickets can generalize across multiple datasets (Morcos et al., 2019).",2,positive
"In particular, (1) whether keeping the same initialization (e.g., θ0) is crucial for acquiring tickets (Liu et al., 2018b); and (2) if tickets can generalize across multiple datasets (Morcos et al., 2019).",1,neutral
"…are a lot of parameter redundancy in trained deep model and recent lottery ticket hypothesis (Frankle & Carbin, 2019; Frankle et al., 2019; Zhou et al., 2019; Morcos et al., 2019) shows that a subnetwork in the trained model, if trained alone, could achieve comparable or even better generalization.",2,positive
", 2015) shows there are a lot of parameter redundancy in trained deep model and recent lottery ticket hypothesis (Frankle & Carbin, 2019; Frankle et al., 2019; Zhou et al., 2019; Morcos et al., 2019) shows that a subnetwork in the trained model, if trained alone, could achieve comparable or even better generalization.",2,positive
" 2016; Han et al., 2015) shows there are a lot of parameter redundancy in trained deep model and recent lottery ticket hypothesis (Frankle &amp; Carbin, 2019; Frankle et al., 2019; Zhou et al., 2019; Morcos et al., 2019) shows that a subnetwork in the trained model, if trained alone, could achieve comparable or even better generalization. Based on our analysis, the intuition to explain both sides of the empirical evi",2,positive
"A finding that suggests a network that is pruned by magnitude consists of an optimal substructure of the original network, known as “lottery ticket hypothesis”, was presented in [11], [14].",1,neutral
The ability that we can find lottery tickets is useful when we need to retrain a pruned model on slightly different but similar datasets [14].,2,positive
"To verify whether the final model from adaptive pruning is a lottery ticket [11], [14], we reinitialize this converged model using the original random seed, and compare its accuracy vs.",2,positive
The lottery ticket is useful for retraining a pruned model on a different yet similar dataset [14].,1,neutral
"However, finding winning tickets hinged on costly (iterative) pruning and retraining (Morcos et al., 2019) studies the reuse of winning tickets, transferable across different datasets.",1,neutral
"(Morcos et al., 2019) studies the reuse of winning tickets, transferable across different datasets.",1,neutral
"Moreover, they can be transferred between datasets [16, 17] and training methods [15].",1,neutral
"These have already found applications in, for example, transfer learning [15, 16, 17], making ticket search a problem of independent interest.",1,neutral
Morcos et al. (2019) confirm the findings on the efficacy of global pruning and late resetting.,2,positive
"Although hard to find in larger regimes, when found, lottery tickets have been shown to possess generalization properties that allow for their reuse in similar tasks, thus reducing the computational cost of finding task- and dataset-dependent sparse sub-networks (Morcos et al., 2019).",1,neutral
"The Lottery Ticket Hypothesis (LTH) proposed in (Frankle & Carbin, 2019) also shows that a sparse subnetwork with comparable performance to the dense model can be found via a combination of IMP and weight resetting (Frankle & Carbin, 2019; Morcos et al., 2019).",2,positive
"In LTH, first a large network is trained and pruned to be a small subnetwork S, then retraining S using its original initialization yields comparable or even better performance, while retraining S with a different initialization performs much worse.",2,positive
"Other empirical observations like lottery ticket hypothesis (LTH) [11; 24; 30; 35], recently also verified in CL [5], may also be explained similarly.",1,neutral
"For LTH, our explanation is that S contains weights that are initialized luckily, i.e., close to useful local optima and converge to them during training.",2,positive
"[24] Ari Morcos, Haonan Yu, Michela Paganini, and Yuandong Tian.",0,negative
"Then, each layer density is automatically generated based on this ratio through the algorithm described in the research (Morcos et al., 2019).",1,neutral
"(Morcos et al., 2019; Chen et al., 2020; 2021a;b) studied the transferability of winning tickets between datasets, tasks and architectures.",1,neutral
"We also noticed is that the winning tickets identified from a larger dataset usually have better transferability, which is in accordance with (Morcos et al., 2019).",2,positive
"Several pioneer works (Mehta, 2019; Morcos et al., 2019; Desai et al., 2019; Chen et al., 2020b;a) also investigate LTH transferability across datasets and downstream tasks.",2,positive
"However, at extremely high sparsity, the performance of transferring tickets degrades faster than the winning tickets identified on the target datasets, corresponding to the observations from previous studies (Chen et al., 2020c; Morcos et al., 2019; Chen et al., 2021e).",0,negative
"The kind of tolerance, combined with the proven generalizability of winning tickets across different tasks, models, and datasets (Morcos et al., 2019), with more efficient methods of finding the winning tickets become available (Tanaka et al., 2020; You et al., 2020).",1,neutral
"The kind of tolerance, combined with the proven generalizability of winning tickets across different tasks, models, and datasets (Morcos et al., 2019), with more efficient methods of finding the winning tickets become available (Tanaka et al.",1,neutral
"331 [35] Ari S Morcos, Haonan Yu, Michela Paganini, and Yuandong Tian.",0,negative
"In [4, 35] 61 a ""lottery ticket hypothesis"" was proposed that with an optimal substructure of the neural network 62 acquired by weights pruning, directly training a pruned model could reach similar results as pruning 63 a pre-trained network.",1,neutral
"Jump-Start is motivated by the work of [22], which found that an LTN trained on one image classification dataset can be finetuned to other datasets without much loss in accuracy.",2,positive
"In addition to supervised image classification, LTH has been explored widely in numerous contexts, such as natural language processing [35, 36], reinforcement learning [37, 38], pre-training/transfer learning [39, 40, 41], and efficient training [42, 43].",1,neutral
"a small subnetwork, by studying stabilization [9] and generalization [23].",1,neutral
"Meanwhile, [7, 8, 12, 49, 54] thoroughly investigate the transferability of such intriguing matching subnetworks.",2,positive
"LTH has been explored extensively in practice and was shown to be applicable in even large-scale settings [8, 21, 22, 42, 48, 70, 71].",1,neutral
"[48] Ari S Morcos, Haonan Yu, Michela Paganini, and Yuandong Tian.",0,negative
"In recent years, researchers have found an intriguing corollary: winning tickets found in the context of one task can be transferred to related tasks [3, 4, 8, 18, 28, 29, 37, 39], possibly even across different architectures [7].",1,neutral
"understanding of the universality in behavior near phase transitions, as well as a way in which to characterize materials by such behavior, we reasoned that viewing the IMP from an RG perspective may lead to new insight on the universality of winning tickets [3, 4, 8, 18, 28, 29, 37, 39] and the general success it has found in the study of DNNs [16].",1,neutral
"It requires several tricks to find lottery tickets for complicated architectures (Morcos et al., 2019).",1,neutral
"Following that, a few works (Morcos et al., 2019; Mehta, 2019) have explored LTH in transfer learning.",2,positive
"Inspired by the recent observation that training with more classes helps consolidate a more robust sparse model (Morcos et al., 2019), we propose a curriculum pruning schedule, in which IMP is conducted more aggressively for new tasks arriving later, with n(i) ≥ n(i−1), until reaching the desired…",2,positive
"(Mehta, 2019; Morcos et al., 2019; Desai et al., 2019) pioneer to study the transferability of the ticket identified on one source task to another target task, which delivers insights on one-shot transferability of LTH.",2,positive
"Inspired by the recent observation that training with more classes helps consolidate a more robust sparse model (Morcos et al., 2019), we propose a curriculum pruning schedule, in which IMP is conducted more aggressively for new tasks arriving later, with n ≥ n(i−1), until reaching the desired sparsity.",2,positive
"Since their introduction, a number of works have attempted to understand various aspects of this phenomenon, including their transferability (Morcos et al., 2019), the factors which make particular winning tickets good (Zhou et al.",1,neutral
"Should all layers be pruned equally? Or rather, should some layers be pruned more than others? Previous studies have shown that global pruning results in better compression and performance than layerwise (or uniform) pruning (Frankle & Carbin, 2018; Morcos et al., 2019).",1,neutral
"We used global iterative magnitude pruning (IMP) for LTs because it has been shown to perform better than one shot pruning, where the pruning procedure is only done once rather than iteratively, and local (or uniform layerwise) pruning, where each layer has the same pruning ratio (Morcos et al., 2019; Frankle & Carbin, 2018).",2,positive
"Since their introduction, a number of works have attempted to understand various aspects of this phenomenon, including their transferability (Morcos et al., 2019), the factors which make particular winning tickets good (Zhou et al., 2019), and linear mode connectivity (Frankle et al., 2019b).",1,neutral
"…pruning (IMP) for LTs because it has been shown to perform better than one shot pruning, where the pruning procedure is only done once rather than iteratively, and local (or uniform layerwise) pruning, where each layer has the same pruning ratio (Morcos et al., 2019; Frankle & Carbin, 2018).",1,neutral
"Epoch 2 was chosen to be roughly similar to experiments in Morcos et al. (2019), which showed strong winning ticket performance across a number of datasets.",2,positive
"Previous studies have shown that global pruning results in better compression and performance than layerwise (or uniform) pruning (Frankle & Carbin, 2018; Morcos et al., 2019).",1,neutral
"이 런 문제를 극복하고자 프루닝을 학습이 다 끝나지 않은 상태에서 약간의 에폭(Epoch) 이후 프루닝을 일정부분 해주고, 다시 몇 번의 에폭을 반복하는 방법[23]과 수식 을 통한 중요 뉴런을 근사 계산하여 해당중요 뉴런만 남기고 프루닝 하는 방법[16]이 있다.",0,negative
"A great body of work [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16] has been dedicated to this task.",1,neutral
"Furthermore, other work has shown that such subnetworks generalize well across datasets and tasks (Morcos et al. (2019); Tanaka et al. (2020)).",2,positive
"In a similar setting, as demonstrated in the model pruning literature (Gordon et al., 2018; Liu et al., 2019b; Morcos et al., 2019; Renda et al., 2020), having different pruning ratios for different layers of",1,neutral
"In a similar setting, as demonstrated in the model pruning literature (Gordon et al., 2018; Liu et al., 2019b; Morcos et al., 2019; Renda et al., 2020), having different pruning ratios for different layers of
the network can further improve results over a single ratio across layers.",1,neutral
The success of the LT algorithm also spawned research in the area of better initialization methods based on determining winning tickets across di erent data sets [36].,1,neutral
"…lottery ticket hypothesis (LTH) (Frankle & Carbin, 2019) advocates that dense models contain highly sparse subnetworks, i.e., winning tickets, with the same good trainability, expressiveness, and transferability (Morcos et al., 2019a; Chen et al., 2020b;a) compared to their dense counterpart.",1,neutral
Similar observations also presented in Morcos et al. (2019a).,1,neutral
"…advocated the existence of sparse subnetworks (winning tickets) with comparable transferability to the full dense models (Chen et al., 2020b;a; Koohpayegani et al., 2020; Morcos et al., 2019b; Chen et al., 2022a) and adversarial robustness (Chen et al., 2022b; Gui et al., 2019; Ye et al., 2019).",2,positive
"(4) Compression w.r.t. transferability: extensive investigations (Chen et al., 2020b;a; Koohpayegani et al., 2020; Morcos et al., 2019b; Iofinova et al., 2022) indicate that there exist high quality subnetworks with competitive or even enhanced transferability across diverse datasets.",2,positive
"…et al., 2019; 2020b; Gui et al., 2019; Ye et al., 2019; Wang et al., 2018; Zhou et al., 2009; Venkatesh et al., 2020; Chen et al., 2020b;a; Koohpayegani et al., 2020; Morcos et al., 2019b; Zhang et al., 2021a; Sakamoto & Sato, 2022; Chen et al., 2022c) trying to address some part of this question.",0,negative
