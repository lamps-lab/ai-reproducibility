text,target_M6_predict,target_predict_M6_label
"…and Grosse-Wentrup, 2020; Kivva et al., 2021; Hyvarinen and Morioka, 2016; Hälvä et al., 2021; Khemakhem et al., 2020b; Lachapelle et al., 2021; Li et al., 2019; Mita et al., 2021; Roeder et al., 2021; Yang et al., 2021; Sorrenson et al., 2019; Zimmermann et al., 2021; Wang et al., 2021;…",2,positive
"Given the proliferation of recent work of identifying latent representations in the observational setting (Anandkumar et al., 2013; Choi et al., 2011; Xie et al., 2020; Yang et al., 2020; Khemakhem et al., 2020a; Markham and Grosse-Wentrup, 2020; Kivva et al., 2021; Hyvarinen and Morioka, 2016; Hälvä et al., 2021; Khemakhem et al., 2020b; Lachapelle et al., 2021; Li et al., 2019; Mita et al., 2021; Roeder et al., 2021; Yang et al., 2021; Sorrenson et al., 2019; Zimmermann et al., 2021; Wang et al., 2021; Moran et al., 2021), in this paper we consider the case of interventions.",2,positive
"Moreover, a long line of works have proposed practical methods for CRL (which includes causal disentanglement as a special case), [18, 89, 15, 91, 43, 13] to name a few.",1,neutral
iFlow [53] Normalizing Flows Conditional Factorial Prior Distribution Explicit Likelihood Computation Segment Label,1,neutral
"With deep learning’s rise, non-linear ICA has been harnessed for disentangled representation learning, with some works highlighting identifiability guarantees [52, 53, 54, 55, 56].",1,neutral
"iFlow [53], rather than optimizing a log-likelihood lower bound like iVAE, employs normalizing flow for direct loglikelihood maximization.",1,neutral
"While this result has been generalized and relaxed in several directions (Hälvä and Hyvarinen, 2020; Hälvä et al., 2021; Khemakhem et al., 2020b; Li et al., 2019; Mita et al., 2021; Sorrenson et al., 2019; Yang et al., 2021; Klindt et al., 2020; Brehmer et al., 2022), fundamentally these…",2,positive
"Since this pathbreaking work, many generalizations have been proposed (Hälvä and Hyvarinen, 2020; Hälvä et al., 2021; Khemakhem et al., 2020b; Li et al., 2019; Mita et al., 2021; Sorrenson et al., 2019; Yang et al., 2021; Klindt et al., 2020; Brehmer et al., 2022), all of which require some…",2,positive
"Since this pathbreaking work, many generalizations have been proposed (Hälvä and Hyvarinen, 2020; Hälvä et al., 2021; Khemakhem et al., 2020b; Li et al., 2019; Mita et al., 2021; Sorrenson et al., 2019; Yang et al., 2021; Klindt et al., 2020; Brehmer et al., 2022), all of which require some form of auxiliary information.",2,positive
"While this result has been generalized and relaxed in several directions (Hälvä and Hyvarinen, 2020; Hälvä et al., 2021; Khemakhem et al., 2020b; Li et al., 2019; Mita et al., 2021; Sorrenson et al., 2019; Yang et al., 2021; Klindt et al., 2020; Brehmer et al., 2022), fundamentally these results still crucially rely on the side information u.",2,positive
"This work in no way questions the result that non-linear ICA is impossible to perform in general when using infinite-capacity models (Hyvärinen & Pajunen, 1999).",1,neutral
"(2)
Thus in this approach the theory of non-linear ICA is linked to the theory of deep generative models.",1,neutral
"We create synthetic data to evaluate model performance using the same generating mechanism and mode of analysis as in previous in works on nonlinear ICA (Hyvärinen & Morioka, 2016; Hyvarinen et al., 2019; Khemakhem et al., 2020a; Sorrenson et al., 2020; Li et al., 2020; Khemakhem et al., 2020b).",2,positive
"Our discussion of the range of potential u-tasks in the context of non-linear ICA is mirrored in the deep clustering literature, that for a given dataset there are multiple ways that one could plausibly cluster the data (Li et al., 2019; Willetts et al., 2019; Falck et al., 2021).",2,positive
"4.1 Synthetic Data
We create synthetic data to evaluate model performance using the same generating mechanism and mode of analysis as in previous in works on nonlinear ICA (Hyvärinen & Morioka, 2016; Hyvarinen et al., 2019; Khemakhem et al., 2020a; Sorrenson et al., 2020; Li et al., 2020; Khemakhem et al., 2020b).",2,positive
"Further, the connection between contrastive learning and classifier-based methods to non-linear ICA has been formalised in the ‘Incomplete Rosetta stone problem’ (Gresele et al., 2020).",1,neutral
"The u-tasks used in the context of non-linear ICA are exactly the tasks one can use in contrastive self-supervised learning, and vice versa.",1,neutral
"Many of these recent results have been in the space of non-linear ICA, where data has been made by a non-linear mixing of statistically-independent latent sources.",1,neutral
"In the last few years there has been a resurgence in identifiability results in machine learning models within certain problem-settings (Hyvärinen & Morioka, 2016; Hyvarinen et al., 2019; Khemakhem et al., 2020a; Li et al., 2020; Sorrenson et al., 2020; Khemakhem et al., 2020b; Roeder et al., 2021).",1,neutral
"…the parameters are not identifiable), while still having a latent representation which is the same across these different parameters (Hyvärinen & Morioka, 2016; Hyvarinen et al., 2019; Khemakhem et al., 2020a; Li et al., 2020; Sorrenson et al., 2020; Khemakhem et al., 2020b; Roeder et al., 2021).",1,neutral
"Recently JL methods have been used in non-Linear ICA, but in a different context: acting on the output of a flow to provide the means of a variational posterior (Camuto et al., 2021).",1,neutral
"As mentioned above, classifier-based approaches to non-linear ICA (Hyvärinen & Morioka, 2016; Hyvarinen et al., 2019) are the cousins of contrastive methods for self-supervised learning (Gutmann & Hyvärinen, 2012; Oord et al., 2018; Henaff et al., 2020; He et al., 2020).",1,neutral
"Here we bring together these ideas to demonstrate the empirical identifiability of latent representations in deep clustering models, which implicitly perform non-linear ICA.",1,neutral
"We wish to underline that we in no way dispute previous results on the impossibility of generic, unique non-linear ICA.",2,positive
"We think that the two lenses we explore, clustering DGMs and non-linear ICA, might provide a way for the two self-supervised approaches of clustering and contrastive methods to be unified.",1,neutral
", the parameters are not identifiable), while still having a latent representation which is the same across these different parameters (Hyvärinen & Morioka, 2016; Hyvarinen et al., 2019; Khemakhem et al., 2020a; Li et al., 2020; Sorrenson et al., 2020; Khemakhem et al., 2020b; Roeder et al., 2021).",0,negative
"…create synthetic data to evaluate model performance using the same generating mechanism and mode of analysis as in previous in works on nonlinear ICA (Hyvärinen & Morioka, 2016; Hyvarinen et al., 2019; Khemakhem et al., 2020a; Sorrenson et al., 2020; Li et al., 2020; Khemakhem et al., 2020b).",1,neutral
In this paper we work within the strand of research into non-linear ICA that aims to learn an appropriate deep generative model for the data.,2,positive
There have been two main paradigms in identifiability results in non-linear ICA.,1,neutral
"In this paper, we will focus on a specific implementation that is directly inspired by the recent work on identifiable VAE [35, 41, 64].",2,positive
"pi-VAE generalizes recent work on identifiable VAE [35, 41, 64] to deal with spike train data.",2,positive
"We might expect that some subspace in Z could contain mere noise, as is observed in iFlows (Li et al., 2020) where dz = dx. Recall that in the case of weak identifiability one learns an affine transform using CCA to maximally-align two sets of learnt representations.",1,neutral
"…the same probability distribution (so the model’s parameters are not identifiable) while having identifiable latent representations (Hyvärinen & Morioka, 2016; Hyvarinen et al., 2019; Khemakhem et al., 2020a; Li et al., 2020; Sorrenson et al., 2020; Khemakhem et al., 2020b; Roeder et al., 2021).",1,neutral
"We might expect that some subspace in Z could contain mere noise, as is observed in iFlows (Li et al., 2020) where dz = dx.",1,neutral
"…learning the same latent representations, when tested under the same conditions as in the literature on identifiable models (Hyvärinen & Morioka, 2016; Hyvarinen et al., 2019; Khemakhem et al., 2020a; Li et al., 2020; Sorrenson et al., 2020; Khemakhem et al., 2020b; Roeder et al., 2021).",1,neutral
"In the last few years there has been a resurgence in identifiability results in machine learning models within certain problem-settings (Hyvärinen & Morioka, 2016; Hyvarinen et al., 2019; Khemakhem et al., 2020a; Li et al., 2020; Sorrenson et al., 2020; Khemakhem et al., 2020b; Roeder et al., 2021).",1,neutral
"We create synthetic data to evaluate model performance using the same generating mechanism and mode of analysis as in previous in works on non-linear ICA (Hyvärinen & Morioka, 2016; Hyvarinen et al., 2019; Khemakhem et al., 2020a; Sorrenson et al., 2020; Li et al., 2020; Khemakhem et al., 2020b).",2,positive
"In this paper we investigate how various purely-unsupervised deep generative models compare to deep generative models with side information at consistently learning the same latent representations, when tested under the same conditions as in the literature on identifiable models (Hyvärinen & Morioka, 2016; Hyvarinen et al., 2019; Khemakhem et al., 2020a; Li et al., 2020; Sorrenson et al., 2020; Khemakhem et al., 2020b; Roeder et al., 2021).",1,neutral
"In highly flexible models, multiple settings of parameters could define the same probability distribution (so the model’s parameters are not identifiable) while having identifiable latent representations (Hyvärinen & Morioka, 2016; Hyvarinen et al., 2019; Khemakhem et al., 2020a; Li et al., 2020; Sorrenson et al., 2020; Khemakhem et al., 2020b; Roeder et al., 2021).",1,neutral
"Since this pathbreaking work, many generalizations have been proposed [Hälvä and Hyvarinen, 2020, Hälvä et al., 2021, Khemakhem et al., 2020b, Li et al., 2019, Mita et al., 2021, Sorrenson et al., 2019, Yang et al., 2021, Klindt et al., 2020, Brehmer et al., 2022], all of which require some form of…",2,positive
"Since this pathbreaking work, many generalizations have been proposed (Hälvä and Hyvarinen, 2020; Hälvä et al., 2021; Khemakhem et al., 2020b; Li et al., 2019; Mita et al., 2021; Sorrenson et al., 2019; Yang et al., 2021; Klindt et al., 2020; Brehmer et al., 2022), all of which require some form of auxiliary information.",2,positive
"While this result has been generalized and relaxed in several directions [Hälvä and Hyvarinen, 2020, Hälvä et al., 2021, Khemakhem et al., 2020b, Li et al., 2019, Mita et al., 2021, Sorrenson et al., 2019, Yang et al., 2021, Klindt et al., 2020, Brehmer et al., 2022], fundamentally these results…",2,positive
"While this result has been generalized and relaxed in several directions (Hälvä and Hyvarinen, 2020; Hälvä et al., 2021; Khemakhem et al., 2020b; Li et al., 2019; Mita et al., 2021; Sorrenson et al., 2019; Yang et al., 2021; Klindt et al., 2020; Brehmer et al., 2022), fundamentally these results still crucially rely on the side information u.",2,positive
"By aligning normalizing flows with this identifiability theory it is desirable to learn a latent-variable model with identifiability guarantees as done by Li, Hooi, and Lee, 2020.",1,neutral
"The identifiability of the latent-variable family is then given by Theorem 4.1 in Li, Hooi, and Lee, 2020.",1,neutral
"The challenge lies in designing a normalizing flow which has an inverse that is efficient to calculate without sacrificing its capabilities of mapping simple distributions to complex ones (Li, Hooi, and Lee, 2020).",1,neutral
"The third component is derived from the change of volume of the normalizing flow, where hφ is a normalizing flow of any kind (Li, Hooi, and Lee, 2020).",1,neutral
"We have create synthetic data using the same configurations as in the original paper (Li, Hooi, and Lee, 2020), following the provided code.",2,positive
"Following this theory, Li, Hooi, and Lee, 2020 presented a novel identifiable framework using a flow-based model for estimating latent representations, which is able to maximize the likelihood of the data directly, resulting in higher identifiability over iVAE.",1,neutral
"This ensures enough flexibility and expressiveness (Li, Hooi, and Lee, 2020).",1,neutral
"In this work, the main results from the ""Identifying Through Flows for Recovering Latent Representations"" paper are reproduced (Li, Hooi, and Lee, 2020).",1,neutral
