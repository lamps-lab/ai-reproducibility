text,target_M6_predict,target_predict_M6_label
"Similarly, given a mini-batch of BU unlabeled data, taking popular consistency regularization frameworks (Sohn et al., 2020) as an example, the unsupervised loss is",1,neutral
"FixMatch (Sohn et al., 2020) relies on a fixed threshold but limits usage of more unlabeled data and leads to imbalanced pseudo-labels.",1,neutral
"The third line explores consistency regulaizations (Xie et al., 2020a; Sohn et al., 2020; Li et al., 2021) to prevent confirmation bias of inaccurate pseudo labels, e.",2,positive
"Firstly, mainstream methods utilize threshold-based pseudo labeling (Sohn et al., 2020; Zhang et al., 2021; Kim et al., 2022; Wang et al., 2022b) with ad-hoc or complex hand-crafted strategies to select high-quality pseudo labels.",1,neutral
"To extend SL with massive unlabeled data, semi-supervised learning (SSL) exploits the information of unlabeled data with limited labeled data (Tarvainen & Valpola, 2017; Sohn et al., 2020) in the self-training paradigm of pseudo-labeling (Lee et al.",1,neutral
"Another line of SSL is based on recent breakthroughs in consistency learning [5, 49], which encourages the network to make consistent predictions when it comes to noise perturbation on unlabeled samples.",1,neutral
"For example, FixMatch [49] combines pseudo-labeling and consistency regularization to address the problems of label scarcity for image classiication.",1,neutral
"Confidence thresholding is a simple yet effective method in ST, where the similarity with ground truth or consistency is used to measure the confidence, ensuring that flawed samples are excluded from the enlarged dataset [9, 20].",1,neutral
"In response to this constraint, many methods have been proposed with limited labeled samples, including transfer learning, small sample learning, and semi-supervised learning (SSL) [7], [8], [9], [10], [11], [12].",1,neutral
"Although this weak-strong data augmentation paradigm works well on image classification [38, 49] and object detection [24, 56, 52], weakly augmented views does not always give reliable learning targets for human pose estimation.",1,neutral
"For example, FixMatch [38] uses weakly augmented views to generate pseudo-labels which are used as targets on the strongly augmented views to ensure consistently regularized output.",1,neutral
"This scheme is similar to the confidence-based thresholding scheme used to generate pseudo labels in existing semi-supervised image classification methods such as Pseudo-Label [22] and FixMatch [38], but we specially design it for generating pseudo-heatmaps in human pose estimation.",2,positive
"Recently, DualPose [51] establishes a benchmark for semi-supervised human pose estimation and presents the dual student framework [18, 5] based on the weak-strong data augmentation paradigm [38, 49].",2,positive
"effective for semi-supervised learning in image classification [38, 49, 24, 56].",1,neutral
"Specifically, an input image I is processed with weak-strong augmentation [38, 49] through affine transformations to obtain its weakly and strongly augmented counterparts Iw, Is.",1,neutral
"State-of-the-art SSL methods usually combine these two prominent techniques [38, 55].",1,neutral
"techniques make it possible to build high-performing models by training them simultaneously on small labeled and large unlabeled datasets for image classification [22, 41, 20, 36, 50, 38] and object detection [16, 39, 56, 24, 25, 16].",1,neutral
"Recent advances in semisupervised learning have been achieved through various deep learning methods [9], with self-training [38, 22, 1, 2, 36, 15] and consistency regularization [41, 20, 4, 3, 37, 49] being the most commonly used approaches.",1,neutral
"For example, FixMatch [38] uses weakly augmented views to generate
pseudo-labels which are used as targets on the strongly augmented views to ensure consistently regularized output.",1,neutral
", 2020) and FixMatch (Sohn et al., 2020), which combine ideas of pseudo-label and consistency regularization, have achieved remarkable performance.",2,positive
", 2020), FixMatch (Sohn et al., 2020), and FreeMatch (Wang et al.",2,positive
"Specifically, SCR penalizes the feature extractors by maximizing the similarity between variations of a synthetic sample, which is inspired by consistency regularization (Bachman et al., 2014; Xie et al., 2020; Sohn et al., 2020).",1,neutral
"Recent studies on SSL have shown that the labeling cost to achieve high-performance models can be significantly reduced by using the unlabeled dataset to train the models with pseudolabeling and consistency regularization (Bachman et al., 2014; Xie et al., 2020; Sohn et al., 2020).",2,positive
", FixMatch (Sohn et al., 2020)) might confuse fθ due to the label space mismatch.",2,positive
"To regularize gψ, we design SCR based on consistency regularization (Bachman et al., 2014; Xie et al., 2020; Sohn et al., 2020), which minimizes the gap between the outputs of two variants of samples that are transformed by different data augmentations.",2,positive
"More recent methods such as FreeMatch (Wang et al., 2023) improve UDA and FixMatch to adaptively control the confidence threshold of acceptance of the pseudo labels for preventing error accumulation and overfitting.",1,neutral
"As the oracle SSL methods, We used three representative SSL methods: UDA (Xie et al., 2020), FixMatch (Sohn et al., 2020), and FreeMatch (Wang et al., 2023).",2,positive
", Therefore training with the synthetic samples via unsupervised losses using pseudo training labels in Y (e.g., FixMatch (Sohn et al., 2020)) might confuse f θ due to the label space mismatch.",1,neutral
"For the hyperparameters of oracle SSL methods, we followed the default settings of the original papers (Xie et al., 2020; Sohn et al., 2020; Wang et al., 2023).",2,positive
"UDA (Xie et al., 2020) and FixMatch (Sohn et al., 2020), which combine ideas of pseudo-label and consistency regularization, have achieved remarkable performance.",2,positive
"However, incorrect predictions can introduce noise and heavily degrade model training [81, 89].",1,neutral
"Semi-Supervision for Computer Vision Tasks: Semisupervised learning approaches have been extensively studied within the computer vision and machine learning communities [18, 47, 58, 89, 94, 118].",1,neutral
We apply weak and strong augmentations to every incoming data sample inspired by FixMatch [50].,2,positive
"Data augmentation coupled with consistency regularization is commonly employed to make a model invariant to the domain-specific attributes [68, 8, 27, 59, 4, 10, 11, 21].",1,neutral
"Dynamicthreshold approaches [70, 64] have certain improvements on known classes, but they tend to ignore samples with high uncertainty, causing low performances on novel classes, and fix-threshold approach [57] also achieves limited improvement.",1,neutral
"And assigning pseudo-labels to the high-confidence samples is another common technique [57, 70, 64, 51, 52].",1,neutral
"Following the methodology proposed in [34], a fixed threshold of 0.",1,neutral
"Drawing inspiration from previous works [3,42,50], our strong data augmentation encompasses a combination of random augmentation techniques [9], cutout [11], and the utilization of common data augmentation.",2,positive
"Additionally, advances in semi-supervised learning have been associated with effective data augmentation development [17, 39, 48].",1,neutral
1 because there should be more and more pseudo labels with confidence scores exceeding τ as training progresses [29].,0,negative
"Note that, FixMatchRA [29] uses a constant threshold (0.",1,neutral
"On these two datasets, we compare our method SPF-RA with several state-of-the-art methods closely related to ours, including FixMatch-RA [29], Dash-RA [33], RYS2 [27].",2,positive
"The most related to ours is pseudo-label filtering with confidence thresholding [11, 29, 32,33].",1,neutral
"Unless stated otherwise, we implement our method SPF-RA by incorporating SPF to FixMatch-RA [29].",2,positive
"We perform our experiments by varying the amount of labeled data, following standard SSL evaluation protocols [2, 3, 22, 29].",2,positive
"Alternatively, some recent state-of-the-art methods [29, 32] adopt a fixed high confidence threshold τ to filter unreliable pseudo labels.",1,neutral
", self-training) [9, 17, 29, 30] or co-trained with the student model (i.",0,negative
"Several works [29,32] have shown that simply keeping the pseudo labels with higher confidence values than a constant threshold (e.",1,neutral
"We compare our SPF-RA method with several latest methods, including MeanTeacher [30], LP [10], DAG [17], and FixMatch-RA [29], under 4k and 10k labels.",2,positive
We adopt the implementation of FixMatch [35] and integrate NetAug-,2,positive
FixMatch [35] is a recent SemiSL algorithm in computer vision that combines consistency regularization and pseudo-labeling and generates separate weak and strong augmentations when performing consistency regularization.,1,neutral
"This paper offers the first attempt to integrate modelefficient and label-efficient algorithms in the GAN tasks, leveraging network architecture search[4, 43], semisupervised learning[2, 40, 45] and distillation algorithm[10, 35, 51] to learn efficient GAN models in a cooperative setting.",1,neutral
"One of the popular SSL algorithms, FixMatch [1], trains identical weight-sharing teacher and student networks simultaneously using a siamese neural network (SNN).",1,neutral
"Besides the aforementioned sequential methods, the simultaneous training method is also an alternative option [1, 9, 10].",1,neutral
"For the labeled training data in all of our experiments, we follow the sampling strategy mentioned in [1] to choose an equal number of images from each class in order to avoid model bias.",2,positive
"Instead, we follow [1] to ignore the labels in the original training data and deem it as unlabeled data.",2,positive
"Following [1], we set τ in (3) to be 0.",1,neutral
"In [1], ξl in (2) is the standard cross-entropy (CE) function to measure the difference between fθ(xw) and its label vector y.",1,neutral
"By incorporating large-scale unlabeled data, SSL has shown to enhance the performance of deep neural networks, as demonstrated in various studies [1, 4, 5, 6, 7].",1,neutral
"For inference, we follow [1] to use the maintained exponential moving average of the trained parameters.",1,neutral
"Recently, [1] came up with a widely used SSL algorithm called FixMatch, which trains an SNN with limited labeled and extra unlabeled data, and achieved significant improvement comparing to traditional supervised learning methods.",1,neutral
", a different augmentation of the original input image [26, 7], mixed image [3], or a network with different parameters [28].",1,neutral
"Inspired by FixMatch [26], we also expect the above property holds for strongly-augmented data.",1,neutral
"A common framework is to employ two processes: one process generates a prediction target, usually in the form of pseudo-labeling [26, 20], but could also be logits [28] or other supervision forms [3, 38, 36].",1,neutral
"With the rapid development of deep neural networks (DNNs), extensive research on deep SSL methods [18, 28, 23, 30, 3, 35, 26] have been studied.",1,neutral
"However, existing SSL approaches are primarily optimized from randomly initialized weights, and recent works [45, 32] show that the impressive performance improvement of these standard SSL methods (includes the state-of-the-art method FixMatch [26]) will disappear when models are training from a pretrained model.",2,positive
"Although there are many existing SSL methods in the literature, this work mainly takes one of the state-of-theart approaches FixMatch [26] as an example.",1,neutral
"Worse, as most of the state-of-the-art SSL methods [26, 3, 20, 2, 44] are built upon self-training, a.",1,neutral
"The current state-of-the-art SSL approaches [18, 28, 23, 3, 26, 20, 37] usually depend on the consistency regularization [3] and pseudo-labeling [20].",1,neutral
"95 as in FixMatch [26] or a dynamic generated scalar as in FlexMatch [44], then ỹ will be used as a pseudo-label for the corresponding unlabeled sample.",1,neutral
"Semi-supervised learning (SSL) has experienced rapid progress with the development of deep neural networks (DNNs) [3, 2, 26, 44, 33].",1,neutral
"For example, FixMatch [39] encourages consistency in predictions between the weakly and strongly augmented versions, and it achieves an accuracy of 92.",1,neutral
"Strong augmentation is a good means of applying consistency regularization, and FixMatch [39] is representative of this idea.",1,neutral
The test model is updated by EMA with a decay rate of 0.999.,0,negative
"We apply SAA on the top of FixMatch [39] and FlexMatch [52], respectively.",2,positive
"As discussed in FixMatch [39], τc is commonly set as a large value to alleviate the confirmation bias [1] in SSL.",1,neutral
"Then we updateHti with exponential moving average (EMA), which can be expressed as:
Hti = (1− a)Ht−1i + αl t i .",1,neutral
"Note that the parameter α introduced is not an additional model parameter, as the model parameters are also updated with EMA [39, 52].",1,neutral
"Specifically, in the sample selection module, we first update the historical loss of the samples with exponential moving average (EMA) in each epoch, then these samples will be divided into two parts.",1,neutral
"(4)
Note that the parameter α introduced is not an additional model parameter, as the model parameters are also updated with EMA [39, 52].",1,neutral
"Soft-labels facilitate learning on interclass relationships with more label-noise for higher uncertainty [24, 27], whereas wrong estimation of hard-labels leads to confirmation bias [28].",1,neutral
Pseudo-labeling based on confidence thresholding has been used very effectively in several applications [20].,1,neutral
"Promising free lunch derived from large-scale unlabeled samples, Semi-supervised Learning (SSL) has achieved record-breaking data-efficiency in different computer vision tasks (Sohn et al. 2020; Xu et al. 2021) and attracted broad attention.",2,positive
"Being aware of the existence of OOD samples, we further extend FixMatch to also assign less-confident (controlled by threshold thr) pseudo-labeled samples as OOD instead of ignoring them in regular SSLs.",2,positive
"To verify the effectiveness of our method, we conducted two experiments that combined Prototype Fission with two popular SSL frameworks: ORCA for open-world SSL that
formulates novel classes from unlabeled data, and FixMatch for evaluating our contribution to standard SSL. CIFAR-10 and CIFAR-100 (Krizhevsky, Hinton et al. 2009) are used to evaluate the performance on both a limited number and a larger number of classes.",2,positive
Note that the original form of FixMatch and PF+FixMatch are not compared to in this experiment due to their incapability of recognizing novel classes.,1,neutral
"Specifically, we form FixMatch and PF+FixMatch as follows:
(1) FixMatch-Sigmoid: This manipulation of FixMatch ditches the Softmax activation function and uses Sigmoid as σ instead as a more proper form of FixMatch in open-set settings.",2,positive
"In the CIFAR-10 experiment, PF+FixMatch significantly outperformed FixMatch, yet the combination with ORCA showed limited difference; In the CIFAR-100 experiment which is more challenging, PF significantly outperformed ORCA in terms of discriminating OOD samples.",1,neutral
The probability threshold thr is set fixed as 0.95 as in the original FixMatch across four aforementioned variants.,2,positive
Prototype Fission + FixMatch: We combine Prototype Fission with FixMatch (Sohn et al. 2020) to validate its effectiveness on typical SSL methods without non-trivial open-set-specific efforts.,2,positive
"(2) PF+FixMatch-Sigmoid: By injecting PF to FixMatch, we firstly replaced its N -class fully connected layer with with our V N prototypes.",2,positive
"The problem of semi-supervised learning is to learn from labeled and unlabeled data [31, 26], in the context of binary classification, the labeled data contains positive and negative data.",1,neutral
", consistency regularization), data augmentation, and the assignment of pseudo-labels to unlabeled data points, in order to utilize unlabeled data (e.g., Chen et al., 2020; Sohn et al., 2020; Zhang et al., 2021; Wang et al., 2023).",1,neutral
"7M) of FixMatch [28], which results in very little additional overhead.",1,neutral
"Among existing works, FixMatch [28] is one of the most influential SSL methods, which is popular for its sim-",1,neutral
"Compared to the standard SSL methods, like FixMatch [28], IOMatch can properly utilize the outliers to mitigate their negative affects on pseudo-labeling and even be able to achieve additional performance gains from them.",2,positive
"advanced deep SSL methods can achieve the performance of fully supervised methods in some cases, such as image classification [28] and semantic segmentation [36].",1,neutral
"Taking the classical method, FixMatch [28], as an example, we can observe that adding extra outliers does hurt the classification accuracy compared to the standard SSL setting with no outlier, because it is impossible to obtain correct seen-class pseudo-labels for these outliers.",1,neutral
"For standard SSL methods, we focus on the latest state-of-the-arts, including MixMatch [3], ReMixMatch [2], FixMatch [28], CoMatch [20], FlexMatch [41], SimMatch [43] and FreeMatch [34].",2,positive
"For Semi-Supervised baselines, we choose MeanTeacher [73], MixMatch [11], ReMixMatch [10], FixMatch [68], and DASO [53] due to their previous powerful performances.",2,positive
"SOTA semi-supervised learning (SSL) approaches in computer vision are generally based on consistency learning [11, 68, 10, 73] or pseudo-labeling [40, 61].",1,neutral
"As suggested by conventional semi-supervised learning methods [20, 2, 40, 68, 11], pseudo labels ỹ are predicted by a supervisor model π : R ω −→ Y with the unlabeled data x̃, i.",1,neutral
"Even though consistency-based SSL methods achieve SOTA results in many computer vision benchmarks [73, 68, 11, 10], these methods significantly rely on the orderly design of augmentation functions that requires domain knowledge to design proper image augmentation strategies, which are even more challenging for multimodal modeling.",1,neutral
"The batch sizes used in the supervised model, semi-supervised models, and the proposed method vary due to the different semi-supervised learning contexts [73, 11, 10, 68, 53].",1,neutral
"Please note that most methods in Table 3 are based on weak/strong augmentations [68, 11, 10] requiring RGB channels, which are not adaptable for multi-modal learning.",1,neutral
"The current SSL methods can be divided into pseudo-labeling-based methods [61, 63, 13, 40] and/or consistency-based methods [73, 68, 11, 10, 41, 43].",1,neutral
"To ensure consistency with previous work, we apply data augmentation techniques to the samples in the semi-supervised learning models, including MeanTeacher [73], MixMatch [11], ReMixMatch [10], FixMatch [68], and DASO [53].",2,positive
"MixMatch [11], ReMixMatch [10], and FixMatch [68] are consistency-based SSL methods that are based on the data augmentations (i.",2,positive
"Different from prior-work semi-supervised learning methods [68, 11, 10, 53], the proposed method explicitly leverages the empirical generalizability of the classifier that is quantified with unseen RNFLT maps (e.",1,neutral
The 1st row reports the result of a bare baseline model - DeepLabV3+ with plain consistency regularization [22].,2,positive
"Previous works [22, 24, 25] have attempted to mitigate this issue by rejecting pseudo labels with classification scores below a heuristic threshold, known as confidence thresholding [22].",1,neutral
"They rely on the consistency regularization paradigm [22, 111], that comprises four key components, as shown in Fig.",1,neutral
"Pseudo labeling [21, 22] constitutes one such technique, where the unlabeled data is assigned pseudo labels based on model predictions.",1,neutral
"Some others introduce perturbations in data [22,38,39], feature [45], and/or network [36, 46].",1,neutral
"Semi-supervised learning [73, 4, 50] is another low-data solution, using both labeled and unlabeled data to enhance performance.",1,neutral
"CR-based approached methods, like Mean-Teacher [15] and FixMatch [16], leverage the label-preserving data or model perturbations to encourage prediction consistency on differently perturbed views from the same input.",1,neutral
"Following the FixMatch [16], we simply adopt a pre-defined threshold,
denoted as τ , to filter out the unlabeled data with less confident pseudo-labels.",1,neutral
"The effect of different μ is extensively discussed in semi-supervised classification [16], but rarely explored in SSMIS.",1,neutral
"Second, utilizing the student model itself generates pseudo-labels, like the FixMatch [16].",1,neutral
"Following the FixMatch [16], we simply adopt a pre-defined threshold,",1,neutral
"Pseudo-Labeled based approaches [3, 4, 26, 33] annotate some unlabeled instances with pseudo labels to expand the labeled data.",1,neutral
"Furthermore, FixMatch [33] uses the weakly augmented unlabeled instances to create a pseudo label and enforce consistent prediction against its strong augmented version.",2,positive
These guiding augmentations can also be regarded as strong [62] or having high variance [67].,1,neutral
"While this has achieved success on some tasks [2, 36], strong augmentation is insufficient to bridge large domain gaps and in most cases, degrades performance.",1,neutral
FixMatch [36] combines consistency regularization and pseudo-labeling on weak and strong augmentations of the same input.,1,neutral
"In this study, we evaluate the effectiveness of our proposed verification model in the LAVe framework by comparing it with the confidence score-based threshold method [11, 12, 13].",2,positive
"Typically, the teacher model’s confidence value is used for selecting subset [11, 12, 13, 14], but this method needs to find fixed threshold value, which requires hyper-parmeter searching.",1,neutral
A useful measure for selecting pseudo-labels is uncertainty [47].,1,neutral
"Pseudo-labeling has been used in many other vision tasks, such as classification [47], segmentation [4], and object detection [58].",1,neutral
"We follow the common practice in semi-supervised learning [47, 50] and update the teacher model based on the student model through the exponential mean average (EMA) strategy.",2,positive
"Despite some low confident samples that could be filtered out by applying a threshold as adopted in semi-supervised learning [39], it is still not guaranteed to prune out all strong OOD samples.",1,neutral
A common method is employing two different formations of data augmentation on the same image [44].,1,neutral
FixMatch [44] computes an artificial label for each unlabeled sample by computing the model’s predicted class distribution given a weakly-augmented version.,1,neutral
FixMatchDistill uses a trained Segformer-b2 to distill knowledge to ResNet50 model as described in Appendix F. FixMatchEnsemble is an ensemble of two ResNet50 model is uses 20M parameters more than ours.,2,positive
"Our Diverse Co-training, compared with cotraining baseline, can better segments the small objects that FixMatch and co-training baseline tends to ignore (e.g. the forth and fifth row).",2,positive
"The semisupervised nature of co-training brings noise into pseudo labels for unlabeled data [67, 59, 90], thus we also provide confidence thresholding following FixMatch to filter out noisy pseudo labels in which the model has low confidence.",1,neutral
"First, we can observe the better results obtained by co-training methods (i.e. (d) and (e)) as shown in the third and last row, where FixMatch is prone to under-segmentation (classifies many foreground pixels as background).",1,neutral
"As shown in the first section of Table 11, our model outperforms both FixMatchDistill and FixMatch-Ensemble consistently by a large margin.",2,positive
"Instead of labeling the unlabeled data before training, consistency regularization typically enforce invariance to perturbations on the unlabeled data in an online manner [65, 44, 85, 4, 55, 3, 67, 81].",1,neutral
"We demonstrate the superiority of co-training over FixMatch in Figure 2, from which co-training outperforms FixMatch consistently on all partitions and thresholds.",1,neutral
"The FixMatch and co-training baseline tends to ignore some foreground while our Diverse Cotraining does not, such as the visualization of the second row.",1,neutral
"From Table 8, we show knowledge transfer do take effect improving the original FixMatch baseline by 3% 1%, which can be attributed to the diverse inductive bias and the high-quality pseudo label introduced by the transformer model.",2,positive
"Illustrating the architectures for (a) FixMatch [67, 90], (b) CPS [12], (c) cross heads with shared backbone [19, 60] (d) n-CPS [20], (e) Diverse Co-training (2-cps) and (f) Diverse Co-training (3-cps).",2,positive
Then we compare FixMatch-Distill and FixMatchEnsemble which uses exactly the same or more parameters than ours but a different learning paradigm.,2,positive
We also display the paradigms used in FixMatch in (a) of Figure 1.,1,neutral
Figure 4: Visualization of C obtained in training process of FixMatch [27] on CIFAR-10 with the same setting as in Figs.,1,neutral
"For example, pseudo-labeling based methods [19, 27, 20, 34, 38] set ηi = 1/p i , i ∈ { i | i = argmax(p) ∧ p i ≥ τ } and ηj = 0, j ∈ {j | j ∈ (1, · · · , k) ∧ j ̸= i} and, i.",1,neutral
Take the current most popular SSL method FixMatch [27] as an example.,1,neutral
"Current prevailing SSL methods [2, 27, 38, 7, 11] utilize the model trained on the labeled data to impute pseudo-labels for the unlabeled data, thereby boosting the model performance.",1,neutral
"Following [27], our models are trained for 2(20) iterations, respectively using the backbone of WideResNet-28-2 (WRN) [37] for CIFAR-10, WRN-28-8 for CIFAR-100 and ResNet-18 [12] for mini-Imagenet.",2,positive
"2b, compared with FixMatch [27] trained in the conventional setting (Fig.",1,neutral
"In this section, PRG is mainly implemented as a plugin to FixMatch [27] and SimMatch [41].",2,positive
Figure 2: Results of FixMatch [27] in MNAR and the conventional SSL setting (i.,1,neutral
"Multiple baseline methods are compared, including conventional SSL algorithms: Π Model [24], MixMatch [3], ReMixMatch [2], FixMatch [27] and SimMatch [41].",2,positive
", for simplicity, [27] fixes τ to determine η for all samples and the value of τ is usually set based on experience) to guide pseudo-rectifying, especially in the MNAR settings.",1,neutral
Figure 5c illustrates the fraction of wrong pseudo-labels surpassing the threshold when training Fixmatch on MNIST (orange) and BCS-MNIST (blue).,1,neutral
"Figure 4b depicts the average accuracy of supervised learning (SPV, blue), pseudo-labeling (PL, green), Fixmatch (orange), and Flexmatch (red) for different labeling budgets on MNIST (solid) and BCI-MNIST (dashed) with random labeling.",1,neutral
Representative sampling is beneficial for Fixmatch.,1,neutral
Perturbations of the data can be obtained by introducing random noise to the input data or utilizing data augmentations [58].,1,neutral
Figure 4c visualizes the entropy over the number of pseudo-labeled instances per class that Fixmatch would choose for training for BCI-MNIST (blue) and MNIST (orange).,2,positive
"Even though the correctness ratio of the pseudo-labels surpassing the threshold using Fixmatch is larger for WCI-MNIST than for MNIST, the achieved mean test accuracy stops at roughly 82% (see Figure 6c).",2,positive
"Figures 5d to 5f denote the learning curves of Flexmatch, Fixmatch, and PL when increasing the labeled pool actively.",1,neutral
"the most informative and valuable data intelligently, semi-supervised learning (SSL) [9, 14, 58] aims to exploit the information in the unlabeled pool without asking for new labels.",1,neutral
"For instance, according to [13], Fixmatch exacerbates confusion when instances across classes are similar.",1,neutral
"Consistency Regularization [58, 9] exploits X u by encouraging invariant predictions when the input is perturbated, thereby making the model robust to different perturbed versions of unlabeled data.",1,neutral
"Fixmatch largely benefits from coverage-based sampling, representative
sampling, and uncertainty sampling for later iterations.",2,positive
"All methods suffer, but Fixmatch clearly suffers the most and is no longer better than plain supervision.",0,negative
We further include Fixmatch [58] as it is a well-established consistency regularization technique and Flexmatch [74] as a strong method tackling confirmation bias [67].,2,positive
"However, Fixmatch is affected most and even performs worse than SPV.",1,neutral
"Table 1 shows the average test accuracies of SPV, Fixmatch, PL, and Flexmatch on BCI-MNIST, BCS-MNIST, and WCI-MNIST for all AL heuristics compared to random sampling, where bold and red numbers indicate best- and worst-
performing methods per column respectively for 50 and 250 labeled instances.",0,negative
"Semi-supervised learning [5,24,46] trains the model on a mixed dataset with labeled and unlabeled data, and the use of the unlabeled data can improve the model’s performance on shifted distribution data.",1,neutral
"According to the hypothesis that the output distribution of the sample remains unchanged after the consistency regularization [12] adds a certain disturbance or enhancement to the input sample, we apply data weakaugment ( ) w weak x Augment x  = and strong-augment ( ) s strong x Augment x  = on the samples respectively, realize regularization training to make the model output stably even when the input is disturbed, greatly enrich the pixel content of the image without changing the sample label, deeply explore more sample information learning, improve the accuracy of decision boundary and the generalization ability of the model.",2,positive
"Following the suggestion from [45], we also report the median error rates of the last 20 checkpoints in Table 9.",0,negative
"These configurations follow the original papers [45, 49, 54].",1,neutral
This list of transformations is similar to the original list used in FixMatch [45] and FlexMatch [54].,2,positive
"To alleviate the confirmation bias problem, pseudo-labeling is used together with confidence-based thresholding, which keeps unlabeled samples only when predictions are sufficiently confident [40,45,49,54].",1,neutral
"We mainly compare our proposed method with recent state-of-the-art methods such as UDA [49], FixMatch [45], FlexMatch [54], CoMatch [27], SimMatch [56], and AdaMatch [7].",2,positive
"Later, FixMatch [45] simplifies them by using only ”hard” pseudo-labels from the high-confidence predictions.",1,neutral
"It is a well-established technique for semi-supervised learning [28, 45], domain adaptation [20, 33], and transfer learning [3].",1,neutral
FixMatch [45] presents a hybrid approach for SSL that combines pseudo-labeling and consistency regularization.,1,neutral
Recent work from [45] suggests using a high threshold to filter out only reliable pseudo-labels for training and masking out the rest.,1,neutral
"For 10% experiments, we follow the settings in [27, 45, 56].",1,neutral
FixMatch generate the pseudo labels and only keep the pseudo labels with high confidence [11].,1,neutral
Most of the hyper-parameters are inherited from [43].,1,neutral
"For example, MixMatch [3], ReMixMatch [2], and FixMatch [43] ar X iv :2 30 8.",1,neutral
"When the MoCo pre-trained model is adopted, our SimMatchV2 achieves a 53.13% Top1 accuracy, which is +10.08% better than the from-scratch performance, and +11.85% better than FixMatch + CCSSL.",2,positive
"For example, MixMatch [3], ReMixMatch [2], and FixMatch [43]
ar X
iv :2
30 8.",1,neutral
"There are also many works [6, 5] exploring the two-stage framework (i.e. pre-training followed by fine-tuning with SSL algorithms like FixMatch), which shows the importance of the self-supervised pretrain to the SSL tasks.",1,neutral
"Although many different types of consistency regularizations have been proposed, including class-level [3, 2, 43], instance-level [9, 24, 22], and those that consider both [66, 43], how to establish a complete and effective consistency regularizations remains unresolved.",1,neutral
"Basically, if only node-node consistency is adopted, the setting is equivalent to the FixMatch [43] with distribution alignment [2], which is our baseline in the first row of the table; Next, we can see that the node-edge, edge-node, and edge-node consistencies give +7.1%, +0.7%, +1.5% improvements, respectively.",1,neutral
"Basically, if only node-node consistency is adopted, the setting is equivalent to the FixMatch [43] with distribution alignment [2], which is our baseline in the first row of the table; Next, we can see that the node-edge, edge-node, and edge-node consistencies give +7.",1,neutral
FixMatch [43] abandons these complicated strategies and proposes a confidencebased threshold method to filter out the low-confidence pseudo-labels during the training.,2,positive
"Our SimMatchV2 achieves 43.05% Top-1 accuracy when training from scratch, which is +11.84% better than the state-of-the-art method (FixMatch + CCSSL).",2,positive
"Such consistency has been proposed in many previous works [3, 2, 43].",1,neutral
"Since then, weak-to-strong consistency regularization has become a standard practice in SSL. Eventually, the milestone
work FixMatch [28] presents a simplified framework using a fixed confidence threshold to discard uncertain samples.",2,positive
"To address this dilemma, recent works [28] simply set a fixed confidence threshold to discard potentially unreliable samples.",1,neutral
We primarily provide some notations and review a common practice in semi-supervised learning (SSL) (Sec.,1,neutral
"The primary concern in SSL [19, 28, 36, 20, 25, 16, 37, 35, 41, 43, 14, 24, 29, 23, 4, 30, 11, 33, 2, 42] is to design effective constraints for unlabeled samples.",0,negative
"The latest trend is to directly use the entropy of predictions [39], cross entropy [36], or softmax confidence [28] as a measurement for uncertainty.",1,neutral
work FixMatch [28] presents a simplified framework using a fixed confidence threshold to discard uncertain samples.,2,positive
"Inspired by this, semisupervised learning (SSL) was proposed to utilize the unlabeled data under the assistance of limited labeled data.",2,positive
Semi-supervised learning (SSL).,1,neutral
The frameworks in SSL are typically based on the strategy of pseudo labeling.,1,neutral
"Several methods have been developed to deal with label noise in automatically annotated datasets, such as semisupervised learning [4, 32], self-supervised learning [5], and robust training [20].",1,neutral
"Most other methods handle the noisy samples as unlabeled data and continue training using common semi-supervised learning algorithms [4, 32, 20].",1,neutral
"Specifically, they explored three state-ofthe-art methods (MixMatch [4], FixMatch [5], and AdaMatch [6]) as well as five classical semi-supervised methods.",1,neutral
FixMatch [41] combines the consistencyregularization-based techniques with a pseudo-label-based framework by applying a strong-weak data augmentation pipeline to input images and enforcing consistency between the augmented images.,2,positive
"The EMA teacher effectively an ensemble of student models at different training steps, which is a most widely used learning strategy in semisupervised setting [10, 14, 36, 38] and UDA [1, 15, 16, 39].",1,neutral
"Yet, there is no guarantee that the fixed, handcrafted set of strong augmentation types suggested in [43] is optimal.",1,neutral
"ployed strong augmentations techniques [43, 4, 3].",1,neutral
"In this work, we propose a new semi-supervised surgical instrument segmentation framework, termed as SegMatch, building upon the state-of-the-art semi-supervised image classification pipeline, FixMatch [43].",2,positive
[43] introduced FixMatch to combine consistency regularization and pseudo-labelling and achieve state-of-the-art performance on various semi-supervised learning benchmarks.,2,positive
"Our proposed SegMatch algorithm adapts the stateof-the-art semi-supervised image classification framework, FixMatch [43], to semi-supervised semantic segmentation.",2,positive
The architecture of the teacher network is same as that of the student and the training algorithm for teacher networks is FixMatch [27].,2,positive
"The teacher models
B.3.1 Hyperparameter setting
We train the model using FixMatch.",2,positive
FixMatch [27] is a semi-supervised learning algorithm minimizing,1,neutral
"Semi-supervised learning is a research area that focuses on effectively utilizing unlabeled data [20, 2, 32, 27, 35].",1,neutral
"FixMatch [27] is a semi-supervised learning algorithm minimizing
1
nl nl∑ i=1 ℓce(fθ(xi), yi)
+ 1
nτ nul∑ j=1 ℓce(fθ(x s j), Fθ(x w j ))1{max c pθ(c|xwj ) > τ},
where τ ∈ (0, 1) is a constant, nτ = nul∑ j=1 1{max c pθ(c|xwj ) > τ} and xsj and xwj are strongly and weakly augmented samples [8], respectively.",1,neutral
"FixMatch (Sohn et al., 2020) thereafter inspired a series of promising methods (Li et al.",1,neutral
"The most prevalent method is FixMatch (Sohn et al., 2020).",1,neutral
"FixMatch (Sohn et al., 2020) thereafter inspired a series of promising methods (Li et al., 2021; Rizve et al., 2021; Zhang et al., 2021; Nassar et al., 2021; Pham et al., 2021; Hu et al., 2021b).",2,positive
FixMatch [32] combines pseudo-label learning and consistency regularization training to constrain the model training process.,2,positive
"Hybrid methods [1, 2, 32, 32] combine different techniques to achieve SSL.",1,neutral
The classification model is trained such that the predictions of the weak and strong augmentations are similar to those of FixMatch [14].,1,neutral
"baselines, models were trained with only the given labeled images, and for the semi-supervised baselines, the model was trained with FixMatch [14].",1,neutral
The loss function in this study is similar to that of FixMatch [14].,1,neutral
"[14] proposed FixMatch, which trains a model with a small amount of labeled data, applies weak augmentation to unlabeled data, and adds it to the model.",2,positive
by adding feature masking to FixMatch [14].,2,positive
We trained semi-supervised baselines using FixMatch [14].,2,positive
"A hard-threshold [34] is applied to the model predictions and the resultant one-hot pseudo-labels are used to supervise the student model by training with a cross-entropy loss Lce,u .",1,neutral
"In FixMatch [34], consistency regularization is maintained by a pair of weak-strong data augmentation and sample confidence is realized by a simple threshold.",1,neutral
"2) Unsupervised Branch: As proposed in [34], consistency is kept by the combination of teacher-student model and strong-weak data augmentation on unlabeled data, and confidence is forced by introducing a hard-threshold on model predictions.",1,neutral
[34] leveraged consistency and confidence to,2,positive
"Table 4: Accuracy of cross-domain SSL in comparison with the peer method Fixmatch [Sohn et al., 2020].",2,positive
"In addition, we use CIFAR10, STL10 [Coates et al., 2011], MNIST [Lecun and Bottou, 1998] and SVHN [Netzer et al., 2011] to construct cross-domain SSL settings, which has few source labeled data and much unlabeled target data and show DRSSL’s advantages in cross-domain SSL over Fixmatch [Sohn et al., 2020].",2,positive
"Practically, we plug DRL into Fixmatch.",2,positive
"Here ‘weakly’ means simple flip-and-shift data augmentation while ‘strongly’ follows the same strategy as FixMatch [Sohn et al., 2020].",1,neutral
"Under the single domain setting, FixMatch trains the model using source labeled data and source unlabeled data.",2,positive
", 2019] and semi-supervised learning [Sohn et al., 2020], where they are used to solicit confident pseudolabels for re-training.",1,neutral
", 2011] to construct cross-domain SSL settings, which has few source labeled data and much unlabeled target data and show DRSSL’s advantages in cross-domain SSL over Fixmatch [Sohn et al., 2020].",2,positive
Incorporating Fixmatch with DRL improves the original Fixmatch by a relative 17% increase in accuracy under the cross-domain setting.,2,positive
Table 4 shows that DRL-powered SSL method improves the Fixmatch baseline significantly on CDSSL tasks.,2,positive
"2 is the ‘consistency loss’ in FixMatch: Lu = 1 M ∑M m=1 I(max(P̂ (ywt |xwt )) > η)H(ŷwt , P̂ (yst |xst )), where xwt and y w t represent the weakly-augmented target data, x s t and yst represent the strongly-augmented version of the same image data, and η is a threshold for generating pseudo-labels ŷwt .",1,neutral
",2020], FixMatch [Sohn et al., 2020], and Ada-CM [Li et al.",2,positive
", 2020] and [Sohn et al., 2020] set fixed thresholds to obtain pseudo labels for weakly augmented unlabeled images, and used them to supervise the prediction of the strongly augmented counterpart.",1,neutral
"To test the performance of LION, we compare it with several state-of-the-art methods, including Pseudo-Labeling [Lee et al., 2013], MixMatch [Berthelot et al., 2019], UDA [Xie et al., 2020], Margin-Mix [Florea et al., 2020], ReMixMatch [Berthelot et al.,2020], FixMatch [Sohn et al., 2020], and Ada-CM [Li et al., 2022], on all the three datasets with different ratios of labeled data.",2,positive
"Second, all the reliable samples will be used to train the model like FixMatch [Sohn et al., 2020].",2,positive
"Self-labeling module follows the fine-tuning roles of FixMatch [41], where the samples with highly confident predictions tend to be assigned to the right cluster.",1,neutral
"Self-labeling module follows the fine-tuning roles of FixMatch [41], where the samples with highly confident",1,neutral
"In this experiment, we use FixMatch [33] as the baseline for semi supervised learning and follow the same configuration for training.",2,positive
"Based on the results in Table 4, the use of the Gated Self-Supervised Learning method cannot improve from the previous baseline method, namely FixMatch [33].",2,positive
"The Gated SelfSupervised Learning method is applied in one of the semi-self-supervised learning methods, namely FixMatch [33] to improve the performance of the model.",1,neutral
"” We intentionally include such unrealistic attributes, motivated by other contexts in self- and semi-supervised learning [9, 58, 7], where they showed the strong benefit of unnatural strong augmentations to train neural networks.",1,neutral
"Existing semi-supervised learning methods typically use unlabeled samples in two ways: pseudo supervision [1, 27] and consistency regularization [20, 28, 31].",1,neutral
"The combination of pseudo-label and consistency regularization has been widely applied in SSL with many stateof-the-art methods, such as UDA [30], FixMatch [31], and MixMatch [32].",1,neutral
"This third property has made the model EMA a common choice for the teacher in many distillation setups, from semi-supervised learning (Tarvainen & Valpola, 2017; Sohn et al., 2020; Manohar et al., 2021; Higuchi et al., 2022), to Self-Supervised Learning (SSL) methods like Bootstrap Your Own Latent (BYOL) (Grill et al.",2,positive
"…has made the model EMA a common choice for the teacher in many distillation setups, from semi-supervised learning (Tarvainen & Valpola, 2017; Sohn et al., 2020; Manohar et al., 2021; Higuchi et al., 2022), to Self-Supervised Learning (SSL) methods like Bootstrap Your Own Latent (BYOL)…",2,positive
"More recently, semisupervised learning [6, 7, 23] (SSL) has been proposed to generate pseudo-labels from more easily available unlabeled data and use them to regularize the models.",1,neutral
"For the additional FixMatch and DASO hyperparameters we followed prior works on imbalanced SSL [8, 9].",2,positive
DASO extends FixMatch and provides state-of-the-art results for balanced and imbalanced SSL [8].,2,positive
We use FixMatch [7] and DASO [8] for our investigations.,2,positive
"DASO with pretrained weights and logit adjustment leads to the best recall and overall accuracy, while FixMatch from scratch provides the best precision.",2,positive
"For example, FixMatch has a particularly low recall on 3 fault ntc.",1,neutral
State of the art SSL approaches promise comparable performance to fully supervised counterparts while only using a small fraction of the labels [7].,2,positive
Transfer leaning improves recall for all methods but leads to a slight drop in precision when using FixMatch.,1,neutral
"We use the RandAugment algorithm [22] which offers automated random data augmentation and is used in other related work [7, 8].",2,positive
"In addition, FixMatch requires a weak and strong augmentation setting to perform consistency regularization.",1,neutral
"Recent practices explicitly add consistency regularization to enforce prediction or representation similarities [Bachman et al., 2014, Laine and Aila, 2016, Sohn et al., 2020].",1,neutral
"8In Appendix E.2, we further bridge the conceptual notion of DAC regularization in Equation (9) with common DAC regularization algorithms in practice like FixMatch [Sohn et al., 2020].",2,positive
"To bridge Equation (9) with common DAC regularization algorithms in practice, in Example E.1, we instantiate FixMatch [Sohn et al., 2020] – a state-of-the-art semi-supervised learning algorithm that leverages DAC regularization by encouraging similar predictions of proper weak and strong data…",1,neutral
"2, we further bridge the conceptual notion of DAC regularization in Equation (9) with common DAC regularization algorithms in practice like FixMatch [Sohn et al., 2020].",1,neutral
"Example E.1 (FixMatch [Sohn et al., 2020]).",1,neutral
"For instance, data augmentation consistency regularization [Berthelot et al., 2019, Sohn et al., 2020] generates pseudo-labels from the current model predictions on carefully designed data augmentations.",1,neutral
"Furthermore, we unify the existing analysis [Yang et al., 2023] tailored for data augmentation consistency (DAC) regularization [Sohn et al., 2020] into the cluster-aware SSL framework (Section 6).",2,positive
", 2023] tailored for data augmentation consistency (DAC) regularization [Sohn et al., 2020] into the cluster-aware SSL framework (Section 6).",2,positive
"We remark that with DAC regularizations like FixMatch [Sohn et al., 2020] and MixMatch [Berthelot et al., 2019], model predictions are matched against pseudo-labels that are gauged based on data augmentations, usually via the cross-entropy loss.",1,neutral
"We take FixMatch [Sohn et al., 2020]–a state-of-the-art algorithm in the lowlabel-rate regime–as the semi-supervised learning baseline.",2,positive
"We remark that with DAC regularizations like FixMatch [Sohn et al., 2020] and MixMatch [Berthelot et al.",1,neutral
"1, we instantiate FixMatch [Sohn et al., 2020] – a state-of-the-art semi-supervised learning algorithm that leverages DAC regularization by encouraging similar predictions of proper weak and strong data augmentations xw,xs ∈ A(x) of the same sample x.",2,positive
"They investigated techniques that labeled the unlabeled data (PseudoLabels (D.-H. Lee et al., 2013), Noisy Student (Q. Xie et al., 2020), FixMatch (Sohn et al., 2020)), that used sub-group aware training on the unlabeled data
(Domain-Adversarial Training of Neural Networks (DANN ) (Ganin et al., 2016), Adaptive Feature Norm (AFN ) (R. Xu et al., 2019), and Correlation alignment (CORAL) (Sun & Saenko, 2016; Sun et al., 2016)), and a contrastive learning technique using the unlabeled dataset (Swapping Assignments between multiple Views (SwAV ) (Caron et al., 2020)).",2,positive
"They investigated techniques that labeled the unlabeled data (PseudoLabels (D.-H. Lee et al., 2013), Noisy Student (Q. Xie et al., 2020), FixMatch (Sohn et al., 2020)), that used sub-group aware training on the unlabeled data
(Domain-Adversarial Training of Neural Networks (DANN ) (Ganin et al.,…",1,neutral
"Approaches that used self-training were also explored such as PseudoLabels (D.-H. Lee et al., 2013), Noisy Student (Q. Xie et al., 2020), and FixMatch (Sohn et al., 2020).",1,neutral
"FixMatch (Sohn et al., 2020) selectively uses the pseudo-labels that are predicted with high confidence and introduces a high level of noise during student training, showcasing remarkable effectiveness in low-labeled-data scenarios.",2,positive
"In this section, the proposed method is applied to another prevalent semisupervised self-training framework, FixMatch [33], to validate its scalability.",2,positive
"Typically, FixMatch [33] leverages pseudo labels generated from weakly augmented unlabeled images to supervise their strongly augmented versions for robust learning.",1,neutral
"Furthermore, the compatibility with FixMatch [33] confirms that our method is a general plug-and-play approach.",2,positive
"Our method builds upon the training frameworks of Mean Teacher [31] and FixMatch [33], illustrating their continued effectiveness.",2,positive
"The successful integration with the classical framework FixMatch [33] serves as a compelling example, resulting in a significant enhancement over the baseline model.",2,positive
"And the baseline methods that utilize consistency regularization, vanilla Consistency Regularization (red), self-training with Noisy Student (purple) and FixMatch (brown), outperform or are competitive with Pseudo Labeling (green).",1,neutral
"After that, FixMatch [35] first combines consistency regularization and pseudolabeling in one simple method.",1,neutral
3) FixMatch(FM) [35]: generates pseudo-labels with weak augmented inputs and then updates the student model with strong augmented input.,1,neutral
"Confidence will help efficiently improve the quality of the model, and help annotate the vast majority of unlabeled data in a scalable manner (Wang et al., 2022; Sohn et al., 2020; Xu et al., 2021).",2,positive
"Some methods [27, 31, 12, 40, 50] try to polish pseudo-labels and provide reliable guidance.",1,neutral
"It
performs competitively on all 3 datasets across all time budgets, and was not outperformed by more recent methods like FixMatch or FlexMatch.",2,positive
"FlexMatch seed0 seed1 seed2 seed3 seed4
Learning rate 0.00036 0.00016 0.00016 0.00068 0.00006 Weight decay 0.00259 0.00001 0.00371 0.00023 0.002103 Unlabeled loss coefficient 2.22 0.82 5.00 1.94 6.09
FixMatch seed0 seed1 seed2 seed3 seed4
Learning rate 0.00074 0.00034 0.00392 0.00102 0.00037 Weight decay 0.00045 0.00315 0.00001 0.00005 0.00058 Unlabeled loss coefficient 3.08 6.70 1.85 1.46 0.47
CoMatch seed0 seed1 seed2 seed3 seed4
Learning rate 0.00124 0.00145 0.00061 0.00026 0.00113 Weight decay 0.00042 0.00009 0.00005 0.00009 0.00017 Unlabeled loss coefficient 0.30 1.71 1.26 2.74 0.46 Contrastive loss coefficient 1.26 2.21 3.71 0.56 1.37
MixMatch seed0 seed1 seed2 seed3 seed4
Learning rate 0.00028 0.00003 0.00018 0.00009 0.00005 Weight decay 0.000005 0.00195 0.00005 0.00085 0.00082 Beta shape α 0.2 0.9 0.9 0.8 0.7 Unlabeled loss coefficient 9.13 37.96 8.06 25.16 11.17
Mean Teacher seed0 seed1 seed2 seed3 seed4
Learning rate 0.00062 0.00022 0.00005 0.00128 0.00125 Weight decay 0.00189 0.00001 0.00008 0.00001 0.00001 Unlabeled loss coefficient 67.67 0.87 1.25 7.60 13.56
Pseudo-label seed0 seed1 seed2 seed3 seed4
Learning rate 0.00007 0.00021 0.00005 0.00063 0.00060 Weight decay 0.00033 0.00093 0.00383 0.00005 0.00087 Unlabeled loss coefficient 0.19 0.16 8.73 0.82 0.25
SwAV seed0 seed1 seed2 seed3 seed4
Learning rate 0.00065 0.00325 0.00012 0.00086 0.00196 Weight decay 0.0001497 0.0000056 0.0000006 0.0000021 0.0000003 Number of prototypes 845 131 36 201 59
MoCo seed0 seed1 seed2 seed3 seed4
Learning rate 0.00288 0.00023 0.00043 0.00005 0.02629 Weight decay 0.000002 0.0000008 0.0000003 0.0000005 0.0000004 temperature 0.09331 0.07097 0.10987 0.07414 0.07080 Momentum 0.99242 0.99672 0.99267 0.99950 0.99538
SimCLR seed0 seed1 seed2 seed3 seed4
Learning rate 0.00217 0.00131 0.000640 0.00380 0.00136 Weight decay 0.00002 0.00001 0.00001 0.00001 0.00001 temperature 0.11719 0.10426 0.08652 0.07784 0.11478
SimSiam seed0 seed1 seed2 seed3 seed4
Learning rate 0.0002 0.00056 0.00013 0.00338 0.00098 Weight decay 0.000066 0.000046 0.000023 0.000001 0.000001
BYOL seed0 seed1 seed2 seed3 seed4
Learning rate 0.000245 0.001308 0.000371 0.001653 0.001959 Weight decay 0.0000007 0.0000057 0.0000004 0.000003 0.000001 Momentum 0.9928618 0.996167 0.9988484 0.9940063 0.9934791",0,negative
"As representative of the state-of-the-art in semi-supervised learning for image classification tasks, our selected methods are: Pseudo Labeling (Lee, 2013), Mean Teacher (Tarvainen and Valpola, 2017), MixMatch (Berthelot et al., 2019), FixMatch (Sohn et al., 2020) and FlexMatch (Zhang et al., 2021).",2,positive
"MixMatch (and other new semi-supervised methods) consistently perform better than any pre-2019 methods, while MixMatch itself is not really ever surpassed by newer methods FixMatch, FlexMatch, or CoMatch.",2,positive
"• Transfer of best hyperparameters from another medical dataset (“from Tissue”) seems to be viable, especially for strong semi-supervised methods like MixMatch, FixMatch, or FlexMatch.",1,neutral
"MixMatch further utilizes MixUp (Zhang et al., 2017), while RandAugment (Cubuk et al., 2020) is used in FixMatch and FlexMatch.",2,positive
"First, in semi-supervised learning (Zhu, 2005; Van Engelen and Hoos, 2020), recent efforts train deep classifiers jointly (Sohn et al., 2020; Berthelot et al., 2019) using an objective with two loss terms, one favoring labeled-set accuracy and the other favoring label-consistency or…",1,neutral
FlexMatch builds directly upon FixMatch by incorporating a class-specific threshold on the unlabeled samples during training.,2,positive
"On PathMNIST, semi-supervised methods dominate: FixMatch and CoMatch are best on the pretraining case, with MixMatch and Flexmatch only a few points of balanced accuracy lower.",1,neutral
"Shared By All
Optimizer Adam Learning rate schedule Cosine
Labeled only Batch size 64 Learning rate 3× 10x, X ∼ Uniform(−5,−2) Weight decay 4× 10x, X ∼ Uniform(−6,−3)
MixUp Batch size 64 Learning rate 3× 10x, X ∼ Uniform(−5,−2) Weight decay 4× 10x, X ∼ Uniform(−6,−3) Beta shape α x,X ∼ Uniform(0.1, 10)
Sup Contrast Batch size 256 Learning rate 3× 10x, X ∼ Uniform(−5.5,−1.5) Weight decay 4× 10x, X ∼ Uniform(−7.5,−3.5) Temperature x,X ∼ Uniform(0.05, 0.15)
FlexMatch Labeled batch size 64 Unlabeled batch size 448 Learning rate 3× 10x, X ∼ Uniform(−5,−2) Weight decay 4× 10x, X ∼ Uniform(−6,−3) Unlabeled loss coefficient 10x, X ∼ Uniform(−1, 1) Unlabeled loss warmup schedule No warmup Pseudo-label threshold 0.95 Sharpening temperature 1.0
FixMatch Labeled batch size 64 Unlabeled batch size 448 Learning rate 3× 10x, X ∼ Uniform(−5,−2) Weight decay 4× 10x, X ∼ Uniform(−6,−3) Unlabeled loss coefficient 10x, X ∼ Uniform(−1, 1) Unlabeled loss warmup schedule No warmup Pseudo-label threshold 0.95 Sharpening temperature 1.0
CoMatch Labeled batch size 64 Unlabeled batch size 448 Learning rate 3× 10x, X ∼ Uniform(−5,−2) Weight decay 4× 10x, X ∼ Uniform(−6,−3) Unlabeled loss coefficient 10x, X ∼ Uniform(−1, 1) Unlabeled loss warmup schedule No warmup Contrastive loss coefficient 5× 10x, X ∼ Uniform(−1, 1) Pseudo-label threshold 0.95 Sharpening temperature 0.2
For TMED2, unlabeled batch size is set to 320 to reduce GPU memory usage
MixMatch Labeled batch size 64 Unlabeled batch size 64 Learning rate 3× 10x, X ∼ Uniform(−5,−2) Weight decay 4× 10x, X ∼ Uniform(−6,−3) Beta shape α x,X ∼ Uniform(0.1, 1) Unlabeled loss coefficient 7.5× 10x, X ∼ Uniform(0, 2) Unlabeled loss warmup schedule linear Sharpening temperature 0.5
Mean Teacher Labeled batch size 64 Unlabeled batch size 64 Learning rate 3× 10x, X ∼ Uniform(−5,−2) Weight decay 4× 10x, X ∼ Uniform(−6,−3) Unlabeled loss coefficient 8× 10x, X ∼ Uniform(−1, 1) Unlabeled loss warmup schedule linear
Pseudo-label Labeled batch size 64 Unlabeled batch size 64 Learning rate 3× 10x, X ∼ Uniform(−5,−2) Weight decay 4× 10x, X ∼ Uniform(−6,−3) Unlabeled loss coefficient 10x, X ∼ Uniform(−1, 1) Unlabeled loss warmup schedule Linear Pseudo-label threshold 0.95
SwAV Batch size 256 Learning rate 1× 10x, X ∼ Uniform(−4.5,−1.5) Weight decay 1× 10x, X ∼ Uniform(−6.5,−3.5) Temperature x,X ∼ Uniform(0.07, 0.12) number of prototypes 1× 10x, X ∼ Uniform(1, 3)
MoCo Batch size 256 Learning rate 1× 10x, X ∼ Uniform(−4.5,−1.5) Weight decay 1× 10x, X ∼ Uniform(−6.5,−3.5) Temperature x,X ∼ Uniform(0.07, 0.12) Momentum x,X ∼ Uniform(0.99, 0.9999)
SimCLR Batch size 256 Learning rate 1× 10x, X ∼ Uniform(−4.5,−1.5) Weight decay 1× 10x, X ∼ Uniform(−6.5,−3.5) Temperature x,X ∼ Uniform(0.07, 0.12)
SimSiam Batch size 256 Learning rate 1× 10x, X ∼ Uniform(−4.5,−1.5) Weight decay 1× 10x, X ∼ Uniform(−6.5,−3.5)
BYOL Batch size 256 Learning rate 1× 10x, X ∼ Uniform(−4.5,−1.5) Weight decay 1× 10x, X ∼ Uniform(−6.5,−3.5) Temperature x,X ∼ Uniform(0.07, 0.12) Momentum x,X ∼ Uniform(0.99, 0.9999)
In practice, we round each sampled α value to the nearest tenth decimal place
C.2 Chosen Hyperparameters on TissueMNIST
Below we report the chosen hyperparameters on TissueMNIST that are used in the hyperparameter transfer experiments.",0,negative
"…models that enforce consistent model outputs (Laine and Aila, 2016; Tarvainen and Valpola, 2017; Berthelot et al., 2019); pseudo label-based models that impute labels for unlabeled data (Lee, 2013; Cascante-Bonilla et al., 2021); and hybrid models that combines several methods (Sohn et al., 2020).",1,neutral
"FixMatch generates two augmentation of an unlabeled sample, one with weak augmentation and the other using strong augmentations (e.g., RandAug (Cubuk et al., 2020)).",1,neutral
"Inspired by [19], we adopt two augmentations - strong augmentation and weak augmentation.",1,neutral
"For example, consistency regularization methods [13] or pseudo-labeling methods [14] can be directly used with each client’s local data with its local model.",1,neutral
"To tackle such label scarcity for general ML problems, there has been a wide range of works including methods such as consistency regularization [13, 32, 33], pseudo-labeling [14, 34, 35], virtual adversarial training [36], and per-sample weighting of unlabeled data [37].",1,neutral
"We compare FedLabel with 3 classes of baselines: i) Supervised FL baselines with Fully Labeled Data denoted as 100% (FedAvg (100%) [1], FedProx (100%) [17]), ii) Supervised FL baselines with Partially Labeled Data (FedAvg, FedProx), and iii) SSFL baselines [13, 14] with Partially Labeled Data ((FedAvg+UDA, FedAvg+FixMatch, FedProx+UDA, FedProx+FixMatch, FedTriNet [8], FedMatch [23]).",2,positive
"Another commonly used method is pseudo-labeling, a simple approach to apply thresholding to the max probability of the prediction and provide a pseudo label for a data sample [14].",1,neutral
"We observe that methods that consider localization uncertainty (3DIoUMatch, MonoLiG) perform better than methods that only use classification uncertainty (FixMatch).",1,neutral
"We compare with FixMatch [41], which uses the confidence score to filter out uncertain pseudolabel, and 3DIoUMatch [46], which, in addition to the confidence score, uses an estimated IoU for filtering.",2,positive
FixMatch [41] enhances the quality of pseudo-labels by filtering predictions from the teacher with low classification confidence.,2,positive
"Similar to previous works [40, 65, 66], we incorporate consistency regularization during training to further boost the performance of the classifier on non-abstained samples at various abstaining rates.",0,negative
"Previous works [40,65,66,87], have also demonstrated the effectiveness of enforcing consistency among the predictions of the classifier to be helpful in various setups such as semi-supervised learning and randomized smoothing.",1,neutral
"In fact, data augmentation that is prevailing in vision [46, 21, 48] and language [47, 45] also leverages randomness to remove augmentation-related context bias in training (e.",1,neutral
"Consistency Regularization is first explored in Semi-supervised Learning (SSL) [2, 35] and is recently adapted to Unsupervised Domain Adaptation (UDA) [47, 25, 7, 28].",1,neutral
"3) [29], and directly utilizing pseudo-labeled real target samples [32, 66].",1,neutral
"Many works [36, 45, 19] apply data augmentation as a perturbed strategy for pursuing outcome consistency.",1,neutral
"Previous SSL methods usually utilize a fixed threshold to filter noisy pseudo labels [36], but they are substantially hindered by corrupted labels or class imbalance on unlabeled data.",1,neutral
(1) baseline: the naive combination of FedAvg [33] and Fixmatch [36].,1,neutral
"However, FedMatch and baseline (FedAvg-Fixmatch) suffer from rapid performance degradation in the higher data heterogeneous (small Dirichlet Coefficient).",2,positive
"FedAvg+UDA, FedProx+UDA, FedAvg+Fixmatch, and FedProx+Fixmatch: a naive combination between semi-supervised methods
(UDA [41] and Fixmatch [36]) and FL algorithms.",2,positive
"FedAvg+UDA, FedProx+UDA, FedAvg+Fixmatch, and FedProx+Fixmatch: a naive combination between semi-supervised methods (UDA [41] and Fixmatch [36]) and FL algorithms.",2,positive
"Due to the computation resource limitations, we reduce the unlabeled data amounts and the number of layers of the model structure compared to the original setting in [41].",2,positive
"We follow the settings given in [41] to train each target model, with details shown in Table 1.",0,negative
"Many studies have been conducted to gradually improve the learning performance of semi-supervised paradigms [6, 7, 22, 23, 34, 41, 45, 48], resulting in promising results.",1,neutral
", MixMatch [7], Re-MixMatch [6], and FixMatch [41], proposed the holistic strategy by combining the idea of both consistency regularization and pseudo-labeling on both labeled and unlabeled data.",2,positive
", Pseudo-Labeling [23], Mean Teacher [45], UDA [48], MixMatch [7], Re-MixMatch [6], and FixMatch [41].",1,neutral
"We adopt the ResNet-16 and WRN-16 as the model structure to train the target model, by following the settings as in [41].",2,positive
"Different solutions have been proposed to create the perturbed versions of a data sample, such as adopting adversarial transformation [28], leveraging data augmentations [6, 7, 41, 48], or using model predictions in the early training epochs [22, 45].",1,neutral
"In addition, we compare our method with some pseudo-labeling based SSL methods, namely – FixMatch (Sohn et al., 2020), FlexMatch (Zhang et al., 2021), and FullMatch (Peng et al., 2023).",2,positive
"On the blood cell classification dataset, we compare our method with MT (Tarvainen and Valpola, 2017), SRC-MT (Liu et al., 2020), FixMatch (Sohn et al., 2020), DARP (Kim et al., 2020), FlexMatch (Zhang et al., 2021) and FullMatch (Peng et al., 2023).",2,positive
"For instance, FixMatch (Sohn et al., 2020) combines consistency regularization and pseudo-labeling to obtain optimal performance and selects highly confident predictions as pseudo-labels using a predefined threshold.",1,neutral
", 2020), FixMatch (Sohn et al., 2020), DARP (Kim et al.",2,positive
"Second, various approaches (Sohn et al., 2020; CascanteBonilla et al., 2021; Zhang et al., 2021; Kim et al., 2020) provide sample selection strategies to generate pseudo-labels.",1,neutral
"In addition, we compare our method with some pseudo-labeling based SSL methods, namely – FixMatch (Sohn et al., 2020), FlexMatch (Zhang et al.",2,positive
"Semi-Supervised Learning (SSL) (Rosenberg et al., 2005; Grandvalet and Bengio, 2004; Berthelot et al., 2019; Sohn et al., 2020) offers a means to utilize unlabeled data for training, thus minimizing the need for a large labeled dataset.",2,positive
"As Table VI shows, our method (CCT) outperforms FixMatch [15] for all choices of τ .",2,positive
"We compared our method with i) baseline methods: only train the labeled target samples with cross entropy loss (CE), and CE with entropy minimization for unlabeled target samples (ENT) [58], ii) semi-supervised domain adaptation (SSDA) methods: minimax entropy (MME) [6], CDAC [7], iii) semi-supervised learning (SSL) methods: MixMatch [59], FixMatch [15], iv) semi-supervised model adaptation (SSMA) methods: SHOT++ [32], SSHT [12], and v) a universal model adaptation method: UMA [29].",2,positive
"4c shows, our method (CCT) consistently outperforms MixMatch [59] and FixMatch [15] in all cases.",2,positive
"We follow the vanilla consistency regularization in SSL [14], [15] and",1,neutral
"To answer the above question, we propose a collaborative consistency training (CCT) framework which extends the vanilla consistency regularization applicable to a single model [14], [15] to our double-model case.",2,positive
"Furthermore, compared with MixMatch [59] and FixMatch [15], the performance gap increases as the size of |C̄t| increases, which implies that our method (CCT) is more suitable for the scenarios with large target-private categories.",2,positive
"Consistency regularization based methods [1, 2, 6, 3] is based on the assumption that the model should predict a similar distribution after applying different perturbations to the same input.",1,neutral
"Recent studies on SSL can be roughly categorized into two dominant mechanisms: consistency regularization [1, 2, 3] and pseudo-labeling [4, 5].",1,neutral
"For FixMatch [3], the best result can reach 94.54% acc. but the worst is only up to 86.68% on a single 40-label split of CIFAR-10 with different random seeds.",0,negative
"For FixMatch [3], the best result can reach 94.",1,neutral
"Specifically, the error rate of FixMatch is decreased by 7.67% on CIFAR-10 and 7.4% on ImageNet at most after imposing our method.",0,negative
"For a fair SSL comparison, we use the same experimental setup and common hyperparameters following [3, 27, 7].",2,positive
"As shown in Table 5, training a
model will cost 113.7 GPU-hours with FixMatch (more time with other SSL methods) on CIFAR-10.",0,negative
"However, employing self-training inevitably introduces noisy pseudo labels [14], [15], which may cause the problem of gradual drift [16].",1,neutral
"In an effort to verify the efficacy of our proposed method, we select four competing state-of-the-art semi-supervised image classification algorithms, including two for dermoscopic image classification (GLM [15] and NM [51]), one for generic image classification (Self-train [13]), and a popular and recent consistency-based algorithm (FixMatch [40]).",2,positive
"73% improvements on MCA over the baseline, and compared favorably to competing methods [40, 13, 15, 51] under most settings.",0,negative
"In addition, consistency regularization methods [47, 27, 40] enforce the model",1,neutral
"Note that the results of SupOne, FixMatch, FeatMatch, DSDGN and BPL are reported by [8].",0,negative
"We compare our method with the following state-of-the-art methods, including semi-supervised learning (SSL) methods (like FixMatch [24] and FeatMatch [25]) and recent SSDG works (like DSDGN [7] and BPL [8]).",2,positive
"We decided to use the FixMatch semi-supervised learning algorithm [19] as a proof-of-concept since it is relatively intuitive and more straightforward to implement compared to more complex algorithms such as self-training with noisy student [20], meta pseudo-labels [21], AdaMatch [22], or contrastive learning for visual representations [23].",2,positive
"This pretrained model was used to retrieve logits of the 4.5 million unlabelled images from ExoNet, which were then thresholded in a FixMatch manner.",0,negative
"B. Semi-Supervised Learning We decided to use the FixMatch semi-supervised learning algorithm [19] as a proof-of-concept since it is relatively intuitive and more straightforward to implement compared to more complex algorithms such as self-training with noisy student [20], meta pseudo-labels [21], AdaMatch [22], or contrastive learning for visual representations [23].",1,neutral
"Our best model consisted of an ImageNet-pretrained model of MobileViT XS scaled to match our input image resolution of 224×224; stochastic gradient descent (SGD) with 0.9 momentum and Nesterov acceleration; randomly initialized weights; pseudo-label cutoff τ of 0.98, a batch difference ratio of 4; unsupervised loss weight λ of 1; initial learning rate = 0.05; labelled batch size of 64; and a FixMatch variation of cosine weight decay learning policy.",2,positive
"Also, other algorithms besides FixMatch [19] could have been used to reduce the number of required labelled training images for semi-supervised learning.",1,neutral
We experimented with cosine weight decay as in FixMatch [19] and cosine decay with restarts [28].,1,neutral
"Learning Method Algorithms FLOPs(G) Params(M) Inference Time (ms)
Supervised WRN 194.63 1.4673 5.98
WRN+ CP 194.63 1.4673 7.98
Semi-supervised
Pseudo-label 778.51 1.4673 5.98
FixMatch 778.51 1.4673 5.99
FlexMatch 778.51 1.4673 5.98
ours 778.56 1.4695 7.00
To solve the problem that the labeled tongue samples are limited in tongue image database, SSL method is used in the training to allow the unlabeled samples participate in training, which maximize the use of the information of the dataset.",1,neutral
"Learning Method Algorithms Macro Precision Macro Recall Macro F1 Accuracy
Supervised WRN 0.579 0.485 0.528 0.703
WRN+ CP 0.585 0.476 0.525 0.698
Semi-supervised
Pseudo-label 0.564 0.522 0.542 0.712
FixMatch 0.595 0.517 0.553 0.711
FlexMatch 0.572 0.535 0.553 0.723
ours 0.606 0.541 0.572 0.729
Table 5 shows the ablation results on the basis of FlexMatch.",0,negative
"In 2020, Sohn et al. proposed the FixMatch [14] algorithm, which simplified the RemixMatch algorithm by setting a high confidence threshold for the predicted results of unlabeled samples to filter out pseudo-labeled samples.",2,positive
"proposed the FixMatch [14] algorithm, which simplified the RemixMatch algorithm by setting a high confidence threshold for the predicted results of unlabeled samples to filter out pseudo-labeled samples.",1,neutral
"In 2021, Zhang et al. of Tokyo Institute of Technology and Microsoft Research proposed the FlexMatch[15] algorithm, which replaced the fixed threshold in FixMatch with dynamic thresholds according to the learning complexity of different categories and the learning effect of different training stages.",2,positive
"Inspired by advanced semi-supervised methods such as FixMatch  (Sohn & Berthelot, 2020), we use the cross-entropy loss between the pseudo-label yt
i of target instance xi with
topology-based selection and its output h (
xt i
)
on the classifier to optimize target classification loss, iff. the…",2,positive
"When = 0 , NaCL without contrastive loss can be roughly viewed as semi-supervised method FixMatch.",1,neutral
"Inspired by advanced semi-supervised methods such as FixMatch  (Sohn & Berthelot, 2020), we use the cross-entropy loss between the pseudo-label yt
i of target instance xi with
topology-based selection and its output h (
xt i
)
on the classifier to optimize target classification loss, iff. the output confidence exceeds a threshold ( =0.95), formalized as
where h (
xt i
)
yt i
denotes the yt i -th component of the vector-valued function h
(
xt i
)
to obtain the confidence of pseudo-label yt
i for target instance xi.",2,positive
"Inspired by the consistence regularization in FixMatch, NaCL further improves performance, compared to NaCL (w/o LT ), by training the model with target classification loss, although the improvement is slight.",2,positive
"Recently, data augmentations have proven effective for boosting SSL on image classification, such as Mixmatch (Berthelot et al. 2019) and UDA (Xie et al. 2020a), which prompted the model to generate consistent predictions on multiple views, and Fixmatch (Sohn et al. 2020a), which trains the model by using one-hot high-confidence pseudo-labels generated from weakly augmented images to supervise strongly augmented ones.",2,positive
"tasks (Sohn et al. 2020a), and there are still many unsolved problems in generalizing them to object detection.",1,neutral
"However, a simple generalization of this scheme in the field of object detection fails to achieve stunning performance comparable to classification tasks (Sohn et al. 2020a).",1,neutral
2020b) follows Fixmatch (Sohn et al. 2020a) to generate pseudo labels at the image level using weakly augmented images and then train the model on strongly augmented ones.,1,neutral
"Recently, STAC (Sohn et al. 2020b) follows Fixmatch (Sohn et al. 2020a) to generate pseudo labels at the image level using weakly augmented images and then train the model on strongly augmented ones.",1,neutral
"2020a), which prompted the model to generate consistent predictions on multiple views, and Fixmatch (Sohn et al. 2020a), which trains the model by using one-hot high-confidence pseudo-labels generated from weakly augmented images to supervise strongly augmented ones.",2,positive
"Prediction Consistency: The prediction consistency regularization technique has been shown to be beneficial in many scenarios such as unsupervised domain adaptation (Ma et al. 2021), semi-supervised learning (Berthelot et al. 2019; Sohn et al. 2020) and self-supervised learning (Chen et al. 2020b).",1,neutral
"2021), semi-supervised learning (Berthelot et al. 2019; Sohn et al. 2020) and self-supervised learning (Chen et al.",1,neutral
"Motivated by recent works on self-training with consistency (Berthelot et al. 2020; Yang et al. 2020) that utilize augmentation and consistency regularization to enhance the stability of the self-training process, we propose a robust single-view self-training approach.",2,positive
"Following FixMatch (Yang et al. 2020), we use filp-and-shift as weak augmentation and the four approaches as strong augmentation.",1,neutral
"Thus, we leverage a new effective training approach named FixMatch (Berthelot et al. 2019; Yang et al. 2020), which improves the robustness of self-training via entropy minimization.",2,positive
"Recently, consistency regularization (Berthelot et al. 2019; Sohn et al. 2020; Xie et al. 2020), a variant of mutual learning, aligns the peer outputs with different data augmentations of the The Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)",2,positive
"STAC generates only one-time pseudo labels, however, using the initial prediction of pseudo labels will limit the model accuracy improvement.",1,neutral
An overview of STAC method is shown in Fig.,1,neutral
STAC [21] proposes a SSOD algorithm based on hard pseudo label.,1,neutral
STAC [26] explores different variants of transformation operations and determinates a group of effective combinations: 1) global color transformation; 2) global geometric transformation; 3) box-level transformation.,1,neutral
These semi-supervised learning methods for image classification can be classified into the following categories: generative methods [9] [10]; graph-based methods [11] [12]; consistency regularization methods [13] [14]; pseudo-label methods [15]-[18] and hybrid methods [19][21].,1,neutral
"Different from the STACwhich adopts hard labels, soft labels are applied to Humble teacher [37] which gets the soft label targets from the predicted distribution of the class probabilities and offsets of all possible classes when the head is performing class-dependent bounding box regression.",2,positive
"The research we focus on builds on common supervised or semi-supervised deep learning [51,53,29].",1,neutral
"Therefore, we recommend using DC3 proposals [46], which have been shown to perform better on real-world ambiguous data than common network predictions and work with many semi-supervised methods [51,30,53].",2,positive
"This naive pseudo-labeling baseline differs from ConstraintMatch in the handling of unconstrained samples, similar to the processing of unlabeled data in FixMatch [33]: weakly augmented, un-",1,neutral
"is a key component of recent semi-supervised classification models [21], [33].",1,neutral
"This naive pseudo-labeling baseline differs from ConstraintMatch in the handling of unconstrained samples, similar to the processing of unlabeled data in FixMatch [33]: weakly augmented, unconstrained samples are selected via a confidence threshold over their predicted cluster assignment and the major predicted cluster is chosen as pseudo-label.",1,neutral
"[16] with the semi-supervised method Fixmatch [33], refer to Fig.",1,neutral
we build upon the intuition of consistency regularization via weak and strong data augmentation strategies a() and A() [33] as follows.,1,neutral
1) Pseudo-Label Selection Semi-supervised approaches use model confidence as measured via the maximal prediction probability [33] or alternative uncertainty metrics [1] as se-,1,neutral
"tion, the rationale of consistency regularization lead to substantial improvements over supervised baselines [2], [21], [33], [46].",1,neutral
"It uses unsupervised clustering in a pretraining step and combines training strategies from constrained clustering [14], [16] with the semi-supervised method Fixmatch [33], refer to Fig.",1,neutral
"For strong augmentations, A(), we used the RandAugment strategy with the data augmentation procedures used in FixMatch and described in Appendix D of [33].",2,positive
"2) Informativeness criterion to carry information of whether two samples xi and xj are predicted to be in the same or a different cluster, which cannot be done via maximal prediction probability [33] or alternative uncertainty metrics [1], and 3) Unification of losses utilizing a constraint-based loss for the unlabeled set.",1,neutral
"16T ) with η being the initial learning rate, t the current training step and T = 20000 the total amount of training steps following [33].",1,neutral
"Among these, FixMatch [33] yields state-of-theart model performance even in settings with very low supervision.",1,neutral
This baseline follows the use of unlabeled samples in FixMatch [33] and similarly leverages the weak-strong augmentation scheme for consistency regularization.,1,neutral
"Following this, ReMixMatch [3] and FixMatch [28] encourage correct predictions between strongly and weakly augmented unlabeled data.",2,positive
"In this way, ST benefits from a vast number of unlabeled instances and extends the generalization bound [Wei et al., 2021b; Zhang et al., 2022], boosting a wide spectrum of tasks like Image Classification [Han et al., 2019; Xie et al., 2020], Speech Recognition [Park et al., 2020], and Natural Language Understanding (NLU) [Mukherjee and Hassan Awadallah, 2020; Vu et al., 2021; Li et al., 2021].",2,positive
"…including NLU [Vu et al., 2021; Du et al., 2021; Bhat et al., 2021; Chen et al., 2021], Image Classification [Han et al., 2019; Xie et al., 2020; Sohn et al., 2020], Speech Recognition [Park et al., 2020; Kahn et al., 2020], and Neural Machine Translation (NMT) [Zhang and Zong, 2016; He et al.,…",2,positive
", sample selection, selects only a part of unlabeled instances in terms of (1) model confidence to avoid over-noisy pseudo labels [Sohn et al., 2020; Bhat et al., 2021], (2) prediction uncertainty to obtain informative instances and enhance performance on the hard ones [Mukherjee and Hassan Awadallah, 2020; Jiao et al.",1,neutral
", 2021], Image Classification [Han et al., 2019; Xie et al., 2020; Sohn et al., 2020], Speech Recognition [Park et al.",1,neutral
"This paradigm iteratively produces pseudo labels for
2Code available at https://github.com/peterfengyx/KEST.
massive unlabeled data and reduces labeling bottleneck, facilitating varied downstream tasks where massive unlabeled in-domain text exists, including NLU [Vu et al., 2021; Du et al., 2021; Bhat et al., 2021; Chen et al., 2021], Image Classification [Han et al., 2019; Xie et al., 2020; Sohn et al., 2020], Speech Recognition [Park et al., 2020; Kahn et al., 2020], and Neural Machine Translation (NMT) [Zhang and Zong, 2016; He et al., 2020; Jiao et al., 2021].",2,positive
"…first line, i.e., sample selection, selects only a part of unlabeled instances in terms of (1) model confidence to avoid over-noisy pseudo labels [Sohn et al., 2020; Bhat et al., 2021], (2) prediction uncertainty to obtain informative instances and enhance performance on the hard ones [Mukherjee…",1,neutral
"In our preliminary experiments, we observed that the model performance was robust to the choice of ∆ ∈ [0.01, 0.1] and use ∆ = 0.1 in all our experiments; we set τ = 0.01 and set confidence threshold η = 0.95 following FixMatch.",2,positive
"We use SSL-VAE and FlowGMM as the baselines for generative semi-supervised methods and Π Model [Rasmus et al., 2015], Pseudo-Labelling [Lee et al., 2013], Mean Teacher [Lee et al., 2013], MixMatch [Berthelot et al., 2019] and FixMatch [Sohn et al., 2020] as baselines for discriminative semi-supervised methods.",2,positive
"…and FlowGMM as the baselines for generative semi-supervised methods and Π Model [Rasmus et al., 2015], Pseudo-Labelling [Lee et al., 2013], Mean Teacher [Lee et al., 2013], MixMatch [Berthelot et al., 2019] and FixMatch [Sohn et al., 2020] as baselines for discriminative semi-supervised methods.",2,positive
"…gaussian noise is a typical choice for implementing consistency, state-of-the-art semi-supervised algorithms such as UDA[Xie et al., 2020], FixMatch[Sohn et al., 2020] and MixMatch [Berthelot et al., 2019] use strong augmentations (i.e., heavily distorted images) or mix-up as the noised images.",1,neutral
"While gaussian noise is a typical choice for implementing consistency, state-of-the-art semi-supervised algorithms such as UDA[Xie et al., 2020], FixMatch[Sohn et al., 2020] and MixMatch [Berthelot et al., 2019] use strong augmentations (i.e., heavily distorted images) or mix-up as the noised images.",1,neutral
", 2019] and FixMatch [Sohn et al., 2020] as baselines for discriminative semi-supervised methods.",1,neutral
", 2020], FixMatch[Sohn et al., 2020] and MixMatch [Berthelot et al.",2,positive
"For the labeled samples, we compute the cross-entropy loss using ground truth labels on both samples diffused to time τ and a uniformly sampled time t.
LLCE = E t∼U(0,T ) (x0,y)∼xL x∼pt(x|x0) [− log pφ(y|x, t)] + E (x0,y)∼xL x∼pτ (x|x0) [− log pφ(y|x, τ)] (7)
For the unlabeled examples, we use samples diffused to time τ for obtaining pseudo-labels; following FixMatch, predictions having confidence greater than some confidence threshold η are used in computing classification loss on the unlabeled examples diffused to uniformly sampled time t:
LUCE = E t∼U(0,T ) x0∼xU
x∼pt(x|x0)
[−1 {pφ(yτ |xτ , τ) ≥ η} log pφ(yτ |x, t)] (8)
where, yτ = maxy pφ(y|xτ , τ) and 1 is a binary indicator function that is 1 if the network predicts yτ with a confidence greater than η.
Finally, we introduce a temporal consistency loss (Figure 6) to allow the classifier to also learn from unlabeled examples without confident predictions.",2,positive
"Typical weakly supervised learning settings include semisupervised learning (Chapelle, Scholkopf, and Zien 2006; Sohn et al. 2020), noisy-label learning (Liu and Tao 2015; Malach and Shalev-Shwartz 2017; Patrini et al.",1,neutral
"Typical weakly supervised learning settings include semisupervised learning (Chapelle, Scholkopf, and Zien 2006; Sohn et al. 2020), noisy-label learning (Liu and Tao 2015; Malach and Shalev-Shwartz 2017; Patrini et al. 2017), and positive-unlabeled learning (Elkan and Noto 2008; Niu et al. 2016;…",1,neutral
"Indeed, consistency regularization has been proven to be an effective learning signal for SSL as it forces outputs to be consistent under different perturbations [1,21,25].",1,neutral
"Successful methods for semi-supervised object classification [21, 25] consist of using a self-consistency loss to ensure that a network outputs similar predictions for different input image augmentations.",1,neutral
Firstly we show that using model augmentation instead of image augmentation [21] as a SSL framework yields higher performance for both tasks.,2,positive
"For the image-based transformation augmentation, the most representative approach is FixMatch [28], which operated by adopting weak and strong auto-augmentation for SSL.",1,neutral
"For the image-based transformation augmentation, the most representative approach is FixMatch [28], which operated by adopting weak and strong autoaugmentation for SSL.",1,neutral
"To conduct an empirical comparison between our proposed method and the SOTAmethods on the wearable-based stress detection task, we reproduced the following methods that have been proven in computer vision tasks: two consistency regularization-based SOTA methods such asΠ-model [26] and virtual adversarial training (VAT) [32] and two hybrid semi-supervised learning methods including interpolation consistency training (ICT) [50], MixMatch [5], and FixMatch [41].",2,positive
"On the SMILE dataset, FixMatch achieved the best performance in both accuracy and F1 score; whereas VAT performed the best on TILES.",0,negative
"In alleviating the sparse label issue, semi-supervised algorithms have been widely studied, especially for the computer vision applications [5, 12, 26, 32, 41, 50, 51].",1,neutral
"To overcome the difficulty in learning models with a small number of labels, numerous semi-supervised learning methods have been developed to leverage massive unlabeled samples [5, 12, 26, 32, 41, 50, 51].",1,neutral
FixMatch [41]: The FixMatch approach relied on the consistency of the outputs from the weakly-augmented samples and strongly-augmented samples.,1,neutral
"Motivated by the success of self-training in domain adaptation [2, 28] and semi-supervised [7, 40], we use the segmentation model to pseudo-label the images in the rest of the dataset, i.",2,positive
"We implement FixMatch (Sohn et al., 2020), FlexMatch (Zhang et al., 2021a), ABC (Lee et al., 2021), and DARP (Kim et al., 2020) on long-tailed CIFAR100, the first two are general semi-supervised methods, and the last two are elaborately designed for long-tail learning.",2,positive
"We implement FixMatch (Sohn et al., 2020), FlexMatch (Zhang et al.",2,positive
"proposed to use a simple interpolation method to introduce disturbances, and conduct unsupervised training through consistent regularization and pseudo-label technology, which greatly improved the prediction performance of the model [18].",2,positive
"Furthermore, thresholding techniques like [7] are shown to fail in the case of very few labels by [23], which remedy this using a similar masking strategy as us to distill training signal when there is a lack of quality pseudo labels.",1,neutral
"For unlabeled data, we are interested in threshold-based consistency regularization approaches [19, 48, 7, 49].",1,neutral
"Encouraging consistency between weakly perturbed data views has shown improvements [17, 18], and further progress has been made with the combination of weak and strong data augmentation [7, 19].",1,neutral
"where τ is a threshold value, μ is the ratio of unlabeled-to-labeled data, and p̂m(y|ω(x)) represents the pseudo-label, which can either be a “hard” one-hot label [7] or a sharpened “soft” label [19].",1,neutral
"Differently from sharp consistency regularization [7, 14, 19], our soft objective encourages consistent predictions at (the same) high temperatures without sharpening the target, in order to avoid confirmation bias produced by noisy pseudo-labels.",2,positive
"FixMatch has shown impressive results in balanced settings [7] but requires high-confidence pseudo-labels, which are often scarce in practice.",1,neutral
"Threshold-based methods have been proposed to minimize confirmation-bias [11, 20] by exploiting highly-confident pseudo-labels above a threshold [7, 19].",1,neutral
"Following the protocol from [41], we use a Wide-ResNet-28-2 [53] as the backbone and FixMatch [7] as the base semi-supervised learning algorithm.",2,positive
"Semi-supervised learning (SSL) is a mature field, with significant advancements made in recent decades [1, 7, 8, 9, 10].",1,neutral
"This approach was shown to be quite effective, and thus popularized by FixMatch [7].",1,neutral
"Since FixMatch [48] combines self-training and consistency regularization into a single stage, it has become the baseline for many semi-supervised semantic segmentation approaches.",1,neutral
"Recently, FixMatch [48] provides a simple yet effective weak-to-strong consistency regularization framework and serves as many other relevant methods’ baseline [49, 15, 51, 62].",1,neutral
"As semi-supervised learning has achieved surprising results in the image classification task [34, 50, 4, 48, 33], there are some works considering the same setting for semantic segmentation [20, 6, 42].",1,neutral
"Inspired by self-labeling approaches from the SSL literature [27, 48, 59], we follow a teacher-student approach where a teacher version of our models generates pseudo-labeled temporal segments for training the student.",2,positive
"Combined with FixMatch [1] for semi-supervised learning, FedRPO outperformed FedAvg on precision by 50.",2,positive
We implement naive extensions of FixMatch [1] with both FedAvg and FedRPO.,2,positive
[1] and Remixmatch [33] regulate the prediction consistency from more advanced data augmentation functions [34–36].,1,neutral
"Pseudo-labeling is central in many recent state-of-the-art semi-supervised learning algorithms [1,33].",1,neutral
"These improvements are observed both in the FL framework, as well as the SSFL framework, when our method is combined with an extended version of FixMatch [1], a novel algorithm for semi-supervised learning.",2,positive
"In label-efficient learning, discarding unreliable pseudolabels is widely used to prevent performance degradation [11], [13], [14], [61].",1,neutral
"However, in the typical weak-to-strong selftraining paradigm [61], unreliable pixels are usually simply discarded.",1,neutral
"The typical paradigm of these two settings is the selftraining framework [61], [101], which consists of two models with the same architecture, named teacher and student, respectively.",1,neutral
"FixMatch [61], on the basis of this idea, further proposes a weak-to-strong training pipeline to produce meaningful supervision signals.",1,neutral
"While our model effectively leverages the partially labeled data, its performance can be potentially improved by addressing missing labels from self-supervised [19] or semi-supervised learning [47, 46].",2,positive
CoMatch [15] improves upon FixMatch by introducing a graph-based contrastive loss that learns both class representations and low-dimensional embeddings.,1,neutral
"Next, we examine another aspect of our regularization approach by replacing two strong augmentations with one weak and one strong augmentation (similar to FixMatch).",1,neutral
"Thus, unlike FixMatch [23], MixMatch [12], and ReMixMatch [13], which use weak augmentations for their supervised modules, we apply hard augmentations on the labelled samples in our supervised module.",2,positive
"This is due to the fact that the quality of labelled data greatly influences the performance in such low data settings [23, 33].",1,neutral
FixMatch is another popular hybrid method known for its simplicity and performance.,2,positive
"For our main experiments, we follow the standard semi-supervised evaluation protocol from prior works [23], and present the results for four datasets: CIFAR-10 [29], CIFAR-100 [29], SVHN [30], and STL-10 [31].",2,positive
"Similarly, SimMatch [17] improves FixMatch by introducing an instance similarity loss in addition to the semantic similarity loss imposed by pseudo-labels.",1,neutral
"Subsequently, several works have attempted to improve different aspects of FixMatch.",1,neutral
"To address the first, conventional methods assign labels to instances with high-confidence predictions [36].",1,neutral
"Unlike conventional pseudolabeling approaches that bootstrap off labeled data and are used as semi-supervised learning techniques [36, 4, 40], VLMs can generate pseudolabels in any learning setting.",1,neutral
"Some approaches use confidence thresholds [36, 4, 40] and others average predictions from multiple augmentations [3].",1,neutral
FixMatch [31] is another hybrid semi-supervised learning method that shows impressive performance in many applications.,1,neutral
"Another hybrid method is FixMatch [31], which has gained tremendous success because of its simplicity while achieving state-of-the-art results in various domains.",2,positive
"Another related paradigm is the concept of automatically labeling or pseudo-labeling unlabeled data instances, something that is followed heavily in the semi-supervised learning (SSL) literature [5].",1,neutral
"SSL has been shown successful for datasets of everyday objects [4, 5, 34, 55], such as CIFAR [20], STL10 [9], SVHN [27], or ImageNet [11] but frequently collapses in expert domains, even under-performing supervised baselines trained on the small labeled dataset [36, 44, 50].",1,neutral
"Semi-supervised learning (SSL): SSL can be broadly divided into representation learning [7, 16] and pseudolabelling [21, 34].",1,neutral
"Two popular approaches are self-training [6, 21] and consistency-based learning [5, 34].",1,neutral
"In this work, we consider an alternative formulation, inspired by semi-supervised learning (SSL) methods [4,5,28, 34, 35] where a classifier trained on labelled data produces pseudo-labels for unlabeled examples.",1,neutral
"Center: SSL [21, 34] methods produce a pseudo-label that is accepted or rejected by thresholding a confidence score.",1,neutral
"Self-training is a popular semi-supervised learning paradigm in which machine-generated pseudo-labels are used for training with unlabeled data (Lee, 2013; Berthelot et al., 2019b,a; Sohn et al., 2020a).",1,neutral
"STAC (Sohn et al., 2020b) enforces consistency between strongly augmented versions of confidence-filtered pseudo-labels.",1,neutral
"Self-training has been applied in both 2D computer vision problems (Liu et al., 2021a; Jeong et al., 2019; Tang et al., 2021; Sohn et al., 2020b; Zhou et al., 2022) and 3D problems (Park et al., 2022; Wang et al., 2021; Li et al., 2023; Liu et al., 2023) object detection.",1,neutral
"Self-training is one widely adopted and popular approach in computer vision and autonomous driving for leveraging information from all three components (Lee, 2013; Berthelot et al., 2019b,a; Sohn et al., 2020a; Xie et al., 2020; Jiang et al., 2022; Qi et al., 2021).",1,neutral
"For example, Xie et al. (2020) introduce an extra weight (m+ n)/n on the labeled samples, and add noise to the student model; Sohn et al. (2020a) use confidence thresholding to filter unreliable pseudo-labels.",1,neutral
"FixMatch (Sohn et al., 2020a) uses confidence thresholding to select only high-quality pseudo-labels for student training.",2,positive
"Inspired by previous efforts in SSL [67, 85] and UDA [41, 93], we consider
constructing positive pairs from strongly and weakly augmented views of the same image.",1,neutral
"Inspired by previous efforts in SSL [67, 85] and UDA [41, 93], we consider constructing positive pairs from strongly and weakly augmented views of the same image.",1,neutral
"Inspired by recent success in semi-supervised learning [67, 85, 91] and unsupervised learning [9, 18], we propose two types of contrastive losses.",1,neutral
"Regardless of pseudo label generation, using only sintra and replacing ps with features and p∗,p− with classifier weights, the above Equation (4) is actually the FixMatch loss [67].",1,neutral
"Many of these methods employ the semi-supervised learning (SSL) techniques to pseudo-label the noisy data, and most of them use global (say MixMatch [3], FixMatch [39]) or class-wise threshold (say FlexMatch [54]) to recalibrate labels.",1,neutral
"Fixmatch [39] reached great success in SSL, which utilizes two diverse augmentations, i.",2,positive
"Inspired by previous methods of RRL [30], AugDisc [35] and FixMatch [39], DISC also adopts weak and strong
augmentations to produce two different views for image classification via a shared-weight model.",2,positive
"Inspired by previous methods of RRL [30], AugDisc [35] and FixMatch [39], DISC also adopts weak and strong augmentations to produce two different views for image classification via a shared-weight model.",2,positive
It is important to note that the threshold value determines the balance between the quality and quantity of pseudolabels as explained in [23].,1,neutral
"95 with not much effect on the overall system performance [23], [54], [68].",0,negative
"that makes use of datasets with a greater proportion of unlabeled samples and a smaller number of labeled ones [19], [20], [21], [22], [23].",1,neutral
This suggests that achieving high accuracy depends more on the quality than the number of pseudo-labels [23].,1,neutral
"Along with source-only training with ERM, we experiment with Unsupervised Domain Adaptation (UDA) methods that aim to improve target performance with unlabeled target data (FixMatch (Sohn et al., 2020), DANN (Ganin et al.",2,positive
"Along with source-only training with ERM, we experiment with Unsupervised Domain Adaptation (UDA) methods that aim to improve target performance with unlabeled target data (FixMatch (Sohn et al., 2020), DANN (Ganin et al., 2016), CDAN (Long et al., 2018), and BN-adapt (Li et al., 2016)).",2,positive
", 2019), semi-supervised learning (Berthelot et al., 2019; Sohn et al., 2020), contrastive learning (Kalantidis et al.",1,neutral
"…are known to be helpful in other aspects including model calibration (Thulasidasan et al., 2019), semi-supervised learning (Berthelot et al., 2019; Sohn et al., 2020), contrastive learning (Kalantidis et al., 2020; Verma et al., 2021), and natural language processing (Guo et al., 2019; Sun et…",1,neutral
"Furthermore, we follow the data augmentation guidelines given in STAC [23] and FixMatch [22] for training and pseudo-label generation.",2,positive
"There are two typical frameworks for this task: consistency regularization [10, 26, 31] and self-training [3, 65, 77, 94].",1,neutral
[3] developed a framework for multi-label classification of abdominal organs in ultrasound images by adapting deep clustering with PICA [4] and semi-supervised learning with FixMatch [10].,1,neutral
Dadoun et al. [3] developed a framework for multi-label classification of abdominal organs in ultrasound images by adapting deep clustering with PICA [4] and semi-supervised learning with FixMatch [10].,1,neutral
"Consistency regularization (CR) is an approach that is actively used in semi-observed learning [1, 10, 15, 19].",1,neutral
"It has been successfully used in several tasks including semi-supervised learning [23], unsupervised representation learning, contrastive learning [1,2,4,28].",1,neutral
"Semi-supervised learning (SSL) is a well-studied topic and recent researches can be summarized in two branches which are entropy minimization [14, 41, 50] and consistency regularization [2, 21, 26, 35,37,38,45].",1,neutral
FixMatch [37] proposes to generate pseudo labels from confident predictions on weakly augmented images and use them to supervise the prediction of its strongly augmented version.,2,positive
"Selftraining using pseudo-labeling is the most commonly used method and has shown effectiveness in both object detection [9, 12, 19, 21] and classification [18, 29].",1,neutral
"Inspired by FixMatch [18], they introduce a joint confidence-based pseudo-label filtering mechanism using predicted objectness and class probabilities.",2,positive
"To learn more compact representations, we also use consistency regularization [1,39,45] which is a powerful solution in semi-supervised learning.",1,neutral
"Inspired by the success of consistency regularization in previous works [1, 39, 45], we integrate it into our framework to learn a more robust model.",2,positive
"There are methods like assigning per-sample loss weights using entropy-measured probability uncertainty [15], selecting samples with a strictly high confidence threshold [28], and designing functions to adaptive assign loss weights [4].",1,neutral
"As widely used in semi-supervised learning [11, 28, 34], there are two types of ŷ: when ŷ = argmax(pθ(y|x)), we refer it as pseudo-label, whereas when ŷ = pθ(y|x), it is noted as soft-label.",1,neutral
"To effectively hallucinate the fine supervision signal with the weak localization guidance, we come up with a self-training framework [36, 37, 47].",2,positive
"Previous self-training-based methods [36, 37, 47] often employ region-level augmentation (e.",1,neutral
"Traditionally, a fixed threshold is pre-defined [25], but setting a fixed threshold usually makes the model fail to consider different learning status and learning difficulties of different classes.",1,neutral
"Many recent works use the combinations of consistency regularization and pseudo labeling and achieves state-of-the-art performance [1, 2, 25, 30].",1,neutral
"In semi-supervised learning, for unlabeled data, traditional approaches [2, 25, 34], such as Fixmatch [25] and Flexmatch [34], use original data or their weak augmented version in one batch to generate pseudo labels.",1,neutral
The main approach of methods [12]–[14] is to make the model have the same classification result for weakaugment and strong-augment for the same image.,1,neutral
"[18] introduced the FixMatch method, which uses the latest data Augmentation method, using weak Augmentation and strong Augmentation for unlabeled data, and using pseudolabel technology to generate pseudo-labels for weakly augmented data.",1,neutral
"Three methods, MixMatch [29], FixMatch [30], and RSABL, were used for comparison in this experiment.",1,neutral
"FixMatch [39] is a simple method which combines semi-supervised learning and consistency learning, achieving superior performance in this field.",1,neutral
"Our framework must take into account the errors present in both few-shot NeRF and estimated monocular depths, which will propagate [45] and intensify during the distillation process if left unchecked.",0,negative
"To prevent this, we adopt confidence modeling [23, 45] inspired by self-training approaches [45, 1], to verify the accuracy and reliability of each ray before the distillation process.",2,positive
"The pseudo-labeling pipeline of our method is inspired by FixMatch [10] that combined consistency regularization with confidence-based filtering, surpassing SOTA semi-supervised techniques at the time.",2,positive
"Furthermore, we take advantage of this text-only pre-trained classifier by employing it in a pseudo-labeling pipeline (inspired by FixMatch [10]), to further finetune the CLIP vision encoder on an unlabeled image collection.",2,positive
"Contrary to Fixmatch, in our unsupervised finetuning we set αw as an identity transformation.",2,positive
"Inspired by Fixmatch [10], for each training image x, we generate two views: the weakly-augmented view αw(x) and the strongly-augmented view αh(x), where α denotes a stochastic augmentation function.",1,neutral
"The idea of consistent label predictions under input augmentations stems from FixMatch [33], which considers a classification setting; we instead promote consistency at the level of intermediate features in a dense fashion, lifting the need for explicit pseudolabels.",1,neutral
"processes like FixMatch (Sohn et al., 2020), additional augmentation models are usually needed for text augmentation.",1,neutral
"As a result, unlike straightforward and effective image augmentation processes like FixMatch (Sohn et al., 2020), additional augmentation models are usually needed for text augmentation.",1,neutral
"In computer vision, a straightforward solution is obtaining confident pseudo-labels by augmenting input images (Shin et al., 2020; Mandal et al., 2020; Sohn et al., 2020), including shifting, rotating, or adding noise to pixels.",1,neutral
methods 92 183 366 732 1464 FixMatch [61] 63.,1,neutral
"Results on SensatUrban test.
methods 92 183 366 732 1464 FixMatch [61] 63.9 73.0 75.5 77.8 79.2
+ ERDA 73.5 74.9 78.0 78.5 80.1
Table 6.",0,negative
"Pseudo-labeling [43], a versatile method for entropy minimization [23], has been extensively studied in various tasks, including semi-supervised 2D classification [78, 56], segmentation [61, 85], and domain adaptation [92, 69].",1,neutral
"Specifically, we study an important task of semi-supervised segmentation on image [68, 85, 8] and implement our method to the popular baseline, FixMatch [61], which combines the pseudo-labels with weak-to-strong consistency and is shown to benefit from stronger augmentation [85].",2,positive
"Compared to weakly-supervised 2D image segmentation [93, 46, 71, 1, 61], weakly-supervised 3D point cloud segmentation is less explored.",1,neutral
"For instance, common image augmentations [61] like cropping and resizing may translate to point cloud upsampling [90], which remains an open question in the related research area.",1,neutral
"For FixMatch [61], we use the publicly available implementation here.",2,positive
"To generate high-quality supervision, various label selection strategies have been proposed based on learning status [68, 86, 19], label uncertainty [56, 92, 69, 47], class balancing [94], and data augmentations [61, 85, 94].",1,neutral
"To further increase diversity across heterogeneous views for the contrastive loss [5, 32, 46, 50], we add channel-wise and sequence-wise dropout during training.",2,positive
"Here, we are interested in how about the accuracy of LEMWEC if using a fixed threshold for pseudo-labels generation (i.e., follow the idea of FixMatch) as well as if changing the parameter r in the percentile scoring.",2,positive
"Second, LEMWEC achieves significant improvements over its supervised version LEMWEC-, and it outperforms two weakly-supervised algorithms FixMatch and FlexMatch in 0.1%, 0.5% and 1% cases.",2,positive
"For example, FixMatch [20] uses such strategy.",1,neutral
"Then, we consider the weakly-supervised learning algorithms denoted by (c) FixMatch [20]: TextCNN is used as the base classifier and the fixed thresholding strategy is used to generate pseudolabels where the confidence threshold is set to 0.",1,neutral
"Restrictions apply.
data, the Macro-F1 of LEMWEC is slightly smaller but is competitive to FixMatch.",2,positive
"Then, we consider the weakly-supervised learning algorithms denoted by (c) FixMatch [20]: TextCNN is used as the base classifier and the fixed thresholding strategy is used to generate pseudolabels where the confidence threshold is set to 0.95; (d) FlexMatch [7]: TextCNN is also used as the base classifier and the dynamic thresholding strategy is used.",1,neutral
"Often, such methodology is also combined with consistency regularization [Bachman et al., 2014], as prominently used in classical semi-supervised learning [Sohn et al., 2020, Liu et al., 2020, Yao et al., 2021, Liu et al., 2022].",1,neutral
Semi-supervised learning (SSL) significantly improves the performance of various image recognition tasks by utilizing a large amount of available unlabeled data (Sohn et al. 2020a; Berthelot et al. 2019; Sohn et al. 2020b; Tarvainen and Valpola 2017; Yu et al. 2020).,1,neutral
"Although extensive studies have shown that deep neural networks can achieve high accuracy even with limited samples when trained in the semi-supervised manner [1, 27, 29, 33, 34], the majority of existing approaches assume that the distribution of labeled and unlabeled data are class-balanced.",1,neutral
"FixMatch [27] builds upon both consistency regularization and pseudo labeling, and presents state-of-the-art performance on classbalanced datasets, but produces limited results when the data distribution is imbalanced.",2,positive
"This results from the fact that the mainstream of SSL relies on pseudo labels produced by teacher networks [27, 29, 34], which is trained with a handful of labeled samples that are drawn from a skewed class distribution.",1,neutral
FixMatch [27] is one of the most popular SSL algorithms that enables deep neural networks to effectively learn from unlabeled data.,1,neutral
"As demonstrated in Figure 1(b), with the help of BMB, the accuracy ofminority classes exhibits significant boost compared to the baseline model [27].",1,neutral
"ADAMATCH builds on FIXMATCH and introduces a relative confidence threshold and a modified distribution alignment from (Berthelot et al., 2019a).",2,positive
FIXMATCH only retains an artificial label if the model assigns a high probability to one of the possible classes.,1,neutral
"A wide range of approaches has been proposed including Pseudo Labeling (Lee et al., 2013), Temporal Ensemble (Laine and Aila, 2017), Mean Teacher (Tarvainen and Valpola, 2017), Virtual Adversarial Training
(Miyato et al., 2018), FixMatch (Sohn et al., 2020).",2,positive
"In this section, we conduct additional experiments on ST approaches, including VAT, DASH, and FIXMATCH to demonstrate that our findings are applicable to other ST approaches as well.",2,positive
"This approach outperforms these state-of-the-art ST approaches (Sohn et al., 2020; Xu et al., 2021b; Zhang et al., 2021; Berthelot et al., 2022) as well as the conventional CLS-based fine-tuning with TAPT.",2,positive
"FLEXMATCH (Zhang et al., 2021) also extends FIXMATCH by introducing the
concept of curriculum learning (Bengio et al., 2009) to flexibly adjust thresholds for different classes at each time step and select unlabelled data and their pseudo labels that are more likely to be informative.",2,positive
"We implement five state-of-the-art ST approaches, including VAT (Miyato et al., 2018), FixMatch (Sohn et al., 2020), Dash (Xu et al., 2021b), FlexMatch (Zhang et al., 2021), and AdaMatch (Berthelot et al., 2022) (see descriptions of these approaches in Appendix §B).",2,positive
", 2018), FixMatch (Sohn et al., 2020), Dash (Xu et al.",2,positive
", 2020b,a), confidence threshold (Sohn et al., 2020; Zhang et al., 2021; Berthelot et al., 2022) usually leads to substantial improvements in model performance.",2,positive
"FIXMATCH (Sohn et al., 2020) generates artificial labels using both consistency regularization and pseudo-labelling, where the artificial labels are produced based on weaklyaugmented unlabelled data.",1,neutral
"DASH (Xu et al., 2021b) extends FIXMATCH by introducing a mechanism with a dynamically adjusted threshold of loss to select a subset of training examples from the unlabelled data for performing SSL.
FLEXMATCH.",2,positive
FIXMATCH.,2,positive
"…ST with techniques such as consistency regularization (Miyato et al., 2018; Clark et al., 2018; Berthelot et al., 2019b), strong data augmentation (Sohn et al., 2020; Xie et al., 2020b,a), confidence threshold (Sohn et al., 2020; Zhang et al., 2021; Berthelot et al., 2022) usually leads to…",1,neutral
"…regularization (Miyato et al., 2018; Clark et al., 2018; Berthelot et al., 2019b), strong data augmentation (Sohn et al., 2020; Xie et al., 2020b,a), confidence threshold (Sohn et al., 2020; Zhang et al., 2021; Berthelot et al., 2022) usually leads to substantial improvements in model performance.",2,positive
"The reported FixMatch baseline results are from (Sohn et al., 2020), supervised baseline results are from (Cubuk et al.",2,positive
"To back up our claim that we offer a framework that can be used for a wide-range of problems, we apply Tied-Augment to a semi-supervised learning algorithm: FixMatch (Sohn et al., 2020).",2,positive
"In this section, we apply the Tied-Augment framework to FixMatch (Sohn et al., 2020) as a case study to demonstrate the easy adaptability of our framework.",2,positive
"Therefore, current research efforts try to mitigate this issue by using unlabeled data [54,4,55] or forms of self-supervision [33,19,22,18].",1,neutral
"…Labeling (Lee et al., 2013), Mean Teacher (Tarvainen and Valpola, 2017), VAT (Miyato et al., 2019), MixMatch (Berthelot et al., 2019), FixMatch (Sohn et al., 2020), CRMatch (Fan et al., 2022), FlexMatch (Zhang et al., 2021), AdaMatch (Berthelot et al., 2022), and SimMatch (Zheng et al., 2022),…",2,positive
"Semi-supervised learning (SSL) emerges as a practical paradigm to improve model generalization by leveraging both limited labeled data which is hard to obtain and extensive unlabeled data which is inexpensive to collect (Rasmus et al., 2015; Lee et al., 2013; Tarvainen and Valpola, 2017; Miyato et al., 2019; Berthelot et al., 2019; Sohn et al., 2020; Fan et al., 2022; Zhang et al., 2021; Berthelot et al., 2022; Zheng et al., 2022).",2,positive
"Other SSL algorithms integrated with BERT are implemented in a unified semi-supervised learning benchmark (USB) (Wang et al., 2022a) for classi-
6https://github.com/google-research/bert 7https://github.com/andersjo/pyrouge
fication, including Mean Teacher (Tarvainen and Valpola, 2017), VAT (Miyato et al., 2019), FixMatch (Sohn et al., 2020), CRMatch (Fan et al., 2022), AdaMatch (Berthelot et al., 2022), and SimMatch (Zheng et al., 2022), all utilizing unlabeled data for invariant predictions.",2,positive
"…7https://github.com/andersjo/pyrouge
fication, including Mean Teacher (Tarvainen and Valpola, 2017), VAT (Miyato et al., 2019), FixMatch (Sohn et al., 2020), CRMatch (Fan et al., 2022), AdaMatch (Berthelot et al., 2022), and SimMatch (Zheng et al., 2022), all utilizing unlabeled data…",2,positive
"…is hard to obtain and extensive unlabeled data which is inexpensive to collect (Rasmus et al., 2015; Lee et al., 2013; Tarvainen and Valpola, 2017; Miyato et al., 2019; Berthelot et al., 2019; Sohn et al., 2020; Fan et al., 2022; Zhang et al., 2021; Berthelot et al., 2022; Zheng et al., 2022).",2,positive
"The majority of SSL algorithms are primarily concentrated in the field of computer vision, including Pseudo Labeling (Lee et al., 2013), Mean Teacher (Tarvainen and Valpola, 2017), VAT (Miyato et al., 2019), MixMatch (Berthelot et al., 2019), FixMatch (Sohn et al., 2020), CRMatch (Fan et al., 2022), FlexMatch (Zhang et al., 2021), AdaMatch (Berthelot et al., 2022), and SimMatch (Zheng et al., 2022), all of which exploit unlabeled data by encouraging invariant predictions to input perturbations (Sajjadi et al., 2016).",2,positive
"We consider a general scheme (Lee et al., 2013; Gong et al., 2021) that unifies many prior semisupervised algorithms (including FixMatch and FlexMatch):
θn+1 ← argmin θ {Ls (θ) + µuLu (θ; θn)} , (3)
where θn denotes the model parameters at the n-th iteration, Ls(θ) is a supervised loss.",1,neutral
"Consistency regularization generally involves generating pseudo-labels and applying suitable data augmentation (Tschannen et al., 2019; Berthelot et al., 2019b; Xie et al., 2020; Sohn et al., 2020; Gong et al., 2021).",1,neutral
"With this idea, semi-supervised learning has achieved outstanding performance with very few labeled data, compared with the supervised learning counterparts (Sohn et al., 2020; Zhang et al., 2021; Wang et al., 2022c).",2,positive
", 2021), FixMatch (Sohn et al., 2020), and FlexMatch (Zhang et al.",2,positive
"Consistency regularization has proven to be a simple and effective approach, serving as a foundational component in many state-of-the-art semi-supervised learning algorithms(Sohn et al., 2020; Zhang et al., 2021).",1,neutral
"The state-of-the-art semi-supervised learning algorithms are mostly based on a notion called pseudo label (Lee et al., 2013; Tschannen et al., 2019; Berthelot et al., 2019b; Xie et al., 2020; Sohn et al., 2020; Gong et al., 2021), generated on the fly for the unlabeled data by the neural network f during training.",2,positive
"Baselines We consider prior semi-supervised learning methods similar to FixMatch, including Π-Model, Pseudo-Label (Lee et al., 2013), VAT (Miyato et al., 2018), MeanTeacher (Tarvainen & Valpola, 2017), MixMatch (Berthelot et al., 2019b), ReMixMatch (Berthelot et al., 2019a), UDA (Xie et al., 2020), Dash (Xu et al., 2021), MPL (Pham et al., 2021), FixMatch (Sohn et al., 2020), and FlexMatch (Zhang et al., 2021).",2,positive
"…et al., 2013), VAT (Miyato et al., 2018), MeanTeacher (Tarvainen & Valpola, 2017), MixMatch (Berthelot et al., 2019b), ReMixMatch (Berthelot et al., 2019a), UDA (Xie et al., 2020), Dash (Xu et al., 2021), MPL (Pham et al., 2021), FixMatch (Sohn et al., 2020), and FlexMatch (Zhang et al., 2021).",2,positive
"…semi-supervised learning algorithms are mostly based on a notion called pseudo label (Lee et al., 2013; Tschannen et al., 2019; Berthelot et al., 2019b; Xie et al., 2020; Sohn et al., 2020; Gong et al., 2021), generated on the fly for the unlabeled data by the neural network f during training.",1,neutral
"Hyperparameters For a fair comparison, we use the same hyperparameters as FixMatch (Sohn et al., 2020).",2,positive
"CNN methods have significantly advanced the field [1, 3, 15, 27, 32] while ViT architectures have only recently demonstrated promising results [2, 33] with SSL.",1,neutral
"This setting is related to semi-supervised learning but does not assume we know all of the classes in the data [5, 41].",1,neutral
"These breakthroughs include semi-supervised pseudo-labelling methods [Sohn et al., 2020] and self-supervised contrastive learning approaches [Chen et al., 2020b], which continue to be surpassed by more recent techniques [Wang et al., 2022b, Assran et al., 2021].",1,neutral
"These breakthroughs include semi-supervised pseudo-labelling methods [Sohn et al., 2020] and self-supervised contrastive learning approaches [Chen et al.",1,neutral
"This technique has since been refined in various ways and can be highly effective — Fixmatch [Sohn et al., 2020] is one such method that achieved state of the art on a number semi-supervised benchmarks when it was published, through clever application of image augmentations.",2,positive
FixMatch [11] is another consistency-based method for semi-supervised classification.,1,neutral
"During pseudo-label generation, the NMS and FixMatch [11] strategy is used to remove duplicate bounding box candidates.",1,neutral
"image reconstruction), consistency regularization and pseudo-labeling [73,170], and generative modeling [35,152].",1,neutral
"Note that DAC-MR is with an evident difference from the consistency regularization used in machine learning [51], [52], which solely enforces model predictions invariant to input perturbations.",1,neutral
"Specifically, in our implementation, we adopt data augmentation consistency [51], [52] to integrate such invariance knowledge into the meta-regularization term in Eq.",2,positive
"Such regularization methods have been successfully applied to semi-supervised learning [51], [52], self-supervised learning [72], [73], unsupervised domain adaptation [74], adversarial robustness [75], [76], fewshot learning [77], image generation [78], [79], and transfer learning [80].",1,neutral
"In practice, we often instantiate A with strong data augmentations as in [52], which can ensure that ddac = d − c holds.",1,neutral
"The DAC stems from recent advances in semi-supervised learning [51], [52], and to the best of our knowledge, we exploit it to meta-regularize the complexity of meta-model function class for the first time, which enforces the model facilitated by meta-model to output similar predictions under input data augmentations.",2,positive
"FixMatch (Sohn et al., 2020) introduces a strong augmentation and a weak augmentation to an unlabeled sample, and predicts the labels for the two types of augmentations.",1,neutral
", 2020)), and two semi-supervised learning methods (FixMatch (Sohn et al., 2020) and VAT (Miyato et al.",1,neutral
"To address the challenge of learning with limited labeled data, many recent efforts (Caron et al., 2018; Noroozi & Favaro, 2016; Gidaris et al., 2018; He et al., 2020; Chen et al., 2020; Sohn et al., 2020; Miyato et al., 2018) have been devoted to extracting feature representations in a selfsupervised or semi-supervised learning fashion.",1,neutral
"…et al., 2018), JigsawPuzzle (Noroozi & Favaro, 2016), Rot (Gidaris et al., 2018), MoCo (He et al., 2020), and SimCLR (Chen et al., 2020)), and two semi-supervised learning methods (FixMatch (Sohn et al., 2020) and VAT (Miyato et al., 2018)), on the CelebA, LFWA, and MAAD datasets, respectively.",2,positive
"…limited labeled data, many recent efforts (Caron et al., 2018; Noroozi & Favaro, 2016; Gidaris et al., 2018; He et al., 2020; Chen et al., 2020; Sohn et al., 2020; Miyato et al., 2018) have been devoted to extracting feature representations in a selfsupervised or semi-supervised learning…",1,neutral
"Semi-supervised learning (SSL) has been a long standing research topic which many effective method proposed [36, 41, 1, 28, 43].",1,neutral
"Consistency-based methods are among the most effective methods for SSL, such as Mean-teacher [43], MixMatch [1], and FixMatch [41].",1,neutral
"Semi-supervised learning (SSL) [34] is proposed as a solution to learn a model on both labelled data and unlabelled data, with many works achieving promising performance [1, 43, 41].",1,neutral
"computer vision tasks to achieve state-of-the-art performance [6, 7].",1,neutral
"Here,Aw denotes a function of weak data augmentation [6], fθ is a parameterized Keyword Spotting (KWS) model, and ` is the cross-entropy function.",1,neutral
"Here, As denotes a function of strong data augmentation [6], and τ is a threshold value.",1,neutral
"Following advances in consistency regularization and entropy minimization, some SSL methods use mixed data argumentation to achieve SOTA performance [23], [24].",1,neutral
"By simplifying the above methods, FixMatch (Sohn et al., 2020) simply combined the consistency regularization and pseudo labeling.",1,neutral
"Compared with semi-supervised learning (SSL) that has achieved great advances (Berthelot et al., 2019b; Sohn et al., 2020), relatively less attention has been paid to SSMLL in the context of deep learning.",1,neutral
"The most commonly used strategy adopted by the SSL method called FixMatch (Sohn et al., 2020) is to select the label with the largest probability as the ground-truth one:",1,neutral
"To exploit the information of unlabeled data, inspired by recent SSL works (Berthelot et al., 2019b; Sohn et al., 2020), an intuitive strategy is to provide the unlabeled examples with pseudo labels based on model outputs.",1,neutral
"(1)), which is similar to FixMatch (Sohn et al., 2020) that selects the most probable label as the groundtruth one; Top-k (Eq.",2,positive
"The most commonly used strategy adopted by the SSL method called FixMatch (Sohn et al., 2020) is to select the label with the largest probability as the ground-truth one:
ŷk = 1 if k = arg maxc∈[q] fc(x),0 otherwise.",1,neutral
"[6] used the consistent regularization loss [36], which was widely used in semi-supervised learning, and adopted the group-based model average method.",1,neutral
"To adapt FixMatch for a segmentation task, we added Gaussian noise as weak augmentation and “RandomAug” [46] for strong augmentation; 4) “self-loop [47]”, which solves a self-supervised jigsaw problem as pre-training and combines with pseudo-labelling.",2,positive
"Although FixMatch and its variants have achieved great successes in image classification, it was noticed that it was not straightforward to directly apply FixMatch-style methods in image segmentation, because the cluster assumption does not hold at pixel-level in dense prediction tasks [8].",1,neutral
"In FixMatch [5], the model intakes two forward passes, one pass with weakly augmented (e.",1,neutral
"In FixMatch [5], the model intakes two forward passes, one pass with weakly augmented (e.g. flipping) input and the other one pass with strongly augmented (e.g. shearing, random intensity) input.",1,neutral
"Data Supervised Semi-Supervised Labelled 3D U-net FixMatch CCT CPS SegPL SegPL+VI Volumes [45](2015) [5](2020) [12](2020) [13](2021) (Ours, 2022) (Ours, 2022) 2 56.",2,positive
"Then the output of the weakly augmented input will be used to generate a pseudo label as the ground truth for training the output of the strongly augmented input, the workflow of FixMatch is illustrated in (d) in Fig.1.",1,neutral
"Both types of the consistency regularisation [5], [6], [9]–[11] enforce the deep learning models to make predictions which are invariant to the perturbations at the input level or the feature level [8], [9], [12], [13].",1,neutral
"Data Supervised Semi-Supervised Labelled 2D U-net Self-Loop FixMatch CPS SegPL SegPL+VI Slices [45](2015) [47](2020) [5](2020) [13](2021) (Ours, 2022) (Ours, 2022) 50 54.",2,positive
"Among different implicit forms of entropy minimisation, consistency regularisation is the most common one and it is behind most of the recent state-of-the-art methods in semi-supervised classification and segmentation [5]–[9].",1,neutral
Another more advanced teacher-student model called FixMatch achieved state-of-the-art performance in semisupervised classification [5].,1,neutral
"We compared SegPL with state-of-the-art consistency based methods: 1) “cross pseudo supervision” or CPS [13], which is considered the current state-of-the-art for semisupervised segmentation; 2) another recent state-of-the-art model “cross consistency training” [12], denoted as “CCT”, due to hardware restriction, our implementation shares most of the decoders apart from the last convolutional block; 3) a classic model called “FixMatch” (FM) [5].",2,positive
"Furthermore, our evaluations show that PCP outperforms state-of-the-art semi-supervised approaches [80, 94, 96, 99, 12] with greater simplicity, eliminating the need for an iterative process and extra data augmentation (§4.",2,positive
"Various efforts have been made to mitigate confirmation bias, such as using only high-confidence pseudo labels [80, 99, 12] or relying heavily on data augmentation [94, 21, 11].",1,neutral
"Self-training [98, 58] is another powerful semi-supervised approach, which typically uses student-teacher models to assign pseudo-labels to the unlabelled data [46, 44, 83, 62, 4, 15, 27, 94, 80, 29].",1,neutral
"Additionally, we compare the proposed PCP with four state-of-the-art semi-supervised approaches, including FixMatch [80], Dash [96], FlexMatch [99], and AdaMatch [12] (see descriptions of these approaches in Appendix §C), where back-translation [64] is used for data augmentation as previous works [94, 77] and prompt-based FT (hard) is used as the backbone.",2,positive
"For our proposed DLP, we take FixMatch (Sohn et al.",2,positive
"In particular, we use FixMatch (Sohn et  al.",1,neutral
"For our proposed DLP, we take FixMatch (Sohn et al. 2020) as the semi-supervised method.",2,positive
"Since FedAvg and FedProx are not semi-supervised learningmethods, we use the stateof-the-art semi-supervised strategy FixMatch [38] to train labeled and unlabeled data jointly.",2,positive
"Since FedAvg and FedProx are not semi-supervised learning methods, we use the state-of-the-art semi-supervised strategy FixMatch [38] to train labeled and unlabeled data jointly.",2,positive
"In terms of unsupervised loss Lui , there are two typical forms: pseudo-labels (Sohn et al., 2020) based on labeled data and consistency regularization (Xie et al., 2020) based on data augmentation.",1,neutral
We use consistency learning [27] and view gvk as a slightly perturbed version of gv.,2,positive
1Weak augmentation refers to standard flip-and-shift augmentation strategies [48].,1,neutral
"Inspired by FixMatch [37], a newly recent SOTA algorithm for semi-supervised image classification, we incorporate pseudo-labeling into consistencybased regularization and then can measure this similarity directly with cross-entropy.",2,positive
"Since ours bears many resemblances to FixMatch [37], we also compare with it but replace those image data augmentations originally used by FixMatch with our signal data augmentations.",2,positive
It should be pointed out that our proposed method shares many similarities with FixMatch [37].,2,positive
"According to the degree of supervision, machine learning can be divided into supervised learning [5], semi-supervised learning [6] and unsupervised learning [7].",1,neutral
"For improving accuracy, the consistency in prediction is equally important across the distributions Pi for 1 ≤ i ≤ K [35, 40].",1,neutral
"Despite the popularity of self-training methods, most of the works [42, 1, 35] have focused on the objective of improving prediction accuracy.",1,neutral
"In recent years, semi-supervised learning algorithms are increasingly being used for training deep neural networks [4, 13, 35, 43].",1,neutral
"In the case of accuracy or 0-1-error, a self-training based SSL algorithm using a consistency regularizer achieves the state-of-the-art performance across a variety of datasets [35] and its effectiveness has been proved theoretically [40].",1,neutral
"More formally, a classifier F is trained so that the consistent regularizer R(F ) is small while a supervised loss or a loss between pseudo labeler are minimized [40, 35].",1,neutral
"Modern self-training methods not only leverage pseudo labels, but also forces consistent predictions of a classifier on augmented examples or neighbor examples [40, 22, 42, 35].",1,neutral
"For example, it’s possible to obtain a model with minimal accuracy degradation (≤ 1%) using 5% of labeled data with semi-supervised algorithms compared to supervised models trained using 100% labeled data [35].",1,neutral
"A lot of recent works [22, 35, 42] in semi-supervised learning use DKL to enforce consistency between the model’s prediction on unlabeled data and its augmentations, pm(x) and pm(A(x)).",1,neutral
", adversarial perturbations [22], augmentations [42, 35], etc.",1,neutral
"In the case of the balanced error, the validity of this assumption is justified by the fact that the existing work [35] using consistency regularizer on data augmentation obtains classifier F , that achieves high accuracy (i.",1,neutral
"Alongside self-training, we adopt a consistency training [55, 59, 57], in which the student network Fθ is trained on augmented target images, whereas, the mean teacher network Fφ predicts pseudo-labels for actual target images.",2,positive
"The pseudo-labels can be stabilized with consistency regularization [59, 57] based on pseudo-label prototypes [75], different data augmentation schemes [1, 9, 42, 22], cross-domain mixup strategies [60, 78], and multiple resolutions [21].",1,neutral
"A common technique for semisupervised learning is also related to the label, specifically, using (pseudo-)labels [6, 36, 20].",1,neutral
The default setting of SEAL for semi-supervised learning is adopted from the same configuration and hyper-parameters used in FixMatch[36].,2,positive
"The baselines we consider in our experiments are those prior works similar to FixMatch, such as Π-Model [26], Pseudo Label [28], Mean Teacher [39], MixMatch [6], ReMixMatch [5], VAT [32], UDA [45], FlexMatch [47]and DebiasPL [42].",2,positive
"(13) Many existing approaches [36, 47, 42] fit into this paradigm.",1,neutral
"Empirical evaluation demonstrates that adding the SEAL framework consistently and significantly improves the classification performance on supervised learning and various semi-supervised learning methods [36, 47, 42].",2,positive
"Recent studies have introduced a confidence threshold, which is a trade-off between the quality and the quantity of pseudo-labels [25, 47].",1,neutral
"Our model also did not show a significant difference compared to the previous result not using the threshold, consistent with the study [25], where the authors proposed to introduce the confidence threshold in the SSL approach, however, the approach did not show performance improvement.",0,negative
"Furthermore, while an overlap between a source and target task might unfairly benefit the performance of our methods compared to IN+TRANSFER and IN+FIXMATCH, such an overlap would likely benefit the fine-tuned source models even more making this baseline even harder to outperform (see e.g. Figure 2 and Table 1).",2,positive
"Along this vein, methods such as pseudo-labeling/self-training [25, 43] or consistency regularization [7, 36, 39] have shown remarkable results in reducing deep networks dependencies on large labeled datasets via unlabeled data.",1,neutral
"In principle, we could add additional semi-supervised losses, such as the FixMatch loss [36] to propagate label information from the labeled set to the unlabeled set for better performance, but this would add additional hyperparameters and entangle the effect of our methods.",2,positive
"We consider a set of different baselines: based on ImageNet initializations we consider IN+TRANSFER (fine-tunes ImageNet representations using only the labeled data), and IN+FIXMATCH [36] (fine-tunes the ImageNet representation using labeled and unlabeled data), and based on source model initializations we fine-tune the highest-ranked source model of each source architecture.",2,positive
"More surprisingly, even with the aid of unlabeled data, models fine-tuned from ImageNet representations using a label propagation style approach (IN+FIXMATCH) still underperform distillation-based methods by at least 3.9% on average.",1,neutral
The Distribution alignment module uses two techniques to account for differences between source and target distributions - random logit interpolation and a modified distribution alignment inspired by FixMatch [10].,2,positive
"Inspired by FixMatch [10], the distribution alignment module modifies the consistency of the features passed through the classifier.",2,positive
“Single Stage2 SSL” applies only FixMatch[22].,2,positive
"As shown in Table 4, “+Anchors” indicates that the pretrained UDA model is then fine-tuned only on 1-shot or 3-shot labeled target samples using Lt. “+MT” and “+FixMatch” indicates Ld Mean Teacher and Lu FixMatch.",0,negative
“+Anchors+MT” and “+Anchors+FixMatch” are combinations of fine-tuning and semi-supervised methods.,1,neutral
"In contrast to semisupervised methods such as Fixmatch [19] which stress consistent output of intra-class samples across tasks, our method stresses the difference of intra-class samples.",1,neutral
"2020), Tri-training (Blum and Mitchell 1998), and Fix-match (Sohn et al.",1,neutral
"2020), Tri-training (Blum and Mitchell 1998), and Fix-match (Sohn et al. 2020)) as hyperparameters of the inspection model, and adopted the greedy algorithm to select the optimal semi-supervised learning method.",1,neutral
"In general, FixMatch learns information about unlabeled images using the consistency loss [11] between strong and weak augmentation images produced by RandAugment [12].",1,neutral
"1https://dfu-2021.grand-challenge.org
Our prior work [9] combined the unlabeled data with FixMatch [10] to provide some useful results.",2,positive
Our prior work [9] combined the unlabeled data with FixMatch [10] to provide some useful results.,2,positive
"We compared SPFSeg’s segmentation performance with other state-of-the-art semi-supervised frameworks (including Mean Teacher [12] (MT), FixMatch [15] (FM), and Cross Pseudo Supervision [26] (CPS)) in terms of Dice score and Jaccard Index.",2,positive
Many studies generated one-hot pseudo-labels using a fixed confidencebased threshold (similar to the operation in FixMatch [15]) or an adaptive threshold based on the learning ability or performance [16–18] to generate one-hot pseudo-labels.,1,neutral
"Consistency regularization-based approaches (Laine and Aila, 2016; Miyato et al., 2018; Sohn et al., 2020) ensure that predictions remain stable under perturbations in input data and model parameters.",1,neutral
"After processing the unlabeled samples, the FixMatch [6] is frequently utilized to finish the semi-supervised learning, in which the strong augmentation scheme is easy to damage the useful information in retinal images.",1,neutral
"The existing methods such as MeanTeacher [4], FixMatch [6], PseudoLabel [14] are used for comparison.",1,neutral
"Additionally, the confirmation bias caused by pseudo labels and the cross-entropy loss on unlabeled dataset in [6] also bring much difficulty to model training.",1,neutral
"The model optimization is constrained by the loss of consistency of prediction after perturbing the network in time domain (Π model [3], Mean-teacher [4]) or space domain (VAT [5], FixMatch [6]).",1,neutral
"Many SSL methods have been proposed for biomedical image analysis; pseudo labeling [9, 10, 11], which makes pseudo labels based on the estimated confidence using a network trained by small labeled data; consistency loss [10, 12], which forces a network to produce similar outputs for augmented images; mean teacher algorithm [13], which trains both teacher and student networks with consistency.",1,neutral
"Performance evaluation: We evaluated the segmentation performance of the following five methods: 1) Supervised, which use only a small amount of training data by a standard cross-entropy; 2) Pseudo [9], which gives pseudo labels based on the confidence of each patch; 3) Fix match [10], which uses consistency with augmented data; 4) Uncertainty [20], which selects effective pseudo labels on the basis of the model uncertainty; and 5) the proposed method (Ours).",2,positive
"Recently, there have been intensive researches in unsupervised and semi-supervised learning [20, 21, 22].",1,neutral
"However, the model often suffers from incorrect pseudo labels [11, 12], degrading the whole semi-supervised learning.",1,neutral
"…semi-supervised learning (Embedding-SSL) approaches Embedding-FixMatch and Embedding-CoMatch against the following baselines: First, we use the algorithms FixMatch (Sohn et al. 2020) and CoMatch (Li, Xiong, and Hoi 2021) without an embedding model as semi-supervised learning baselines (SSL).",2,positive
"We benchmark our proposed embedding semi-supervised learning (Embedding-SSL) approaches Embedding-FixMatch and Embedding-CoMatch against the following baselines: First, we use the algorithms FixMatch (Sohn et al. 2020) and CoMatch (Li, Xiong, and Hoi 2021) without an embedding model as semi-supervised learning baselines (SSL).",2,positive
We denote these two instantiations of the proposed approach as Embedding-FixMatch and Embedding-CoMatch respectively.,1,neutral
FixMatch calculates q̂j = argmax(qj) to obtain a hard pseudo-label.,1,neutral
"For further technical details, we refer to Sohn et al. (2020) for FixMatch and Li, Xiong, and Hoi (2021) for CoMatch.",2,positive
"In detail, we use FixMatch and CoMatch as both have demonstrated that they can outperform other semi-supervised approaches (Sohn et al. 2020;
Li, Xiong, and Hoi 2021).",2,positive
"Whereas FixMatch’s loss function consists of a supervised and an unsupervised loss term, CoMatch includes an additional contrastive loss term:
L = 1 l l∑ i=1
H(hbini ,Φex(Φemb(Augw(xi))))︸ ︷︷ ︸ Ls +
λu ∗ 1
u u∑ j=1
1(max(qj) ≥ τ)H(q̂j ,Φex(Φemb(Augs(xj))))︸ ︷︷ ︸ Lu +
λc ∗ 1
u u∑ j=1
H(Ŵ qj , Ŵ z j )︸ ︷︷ ︸
Lc (CoMatch only)
.",1,neutral
(4) Figure 1 provides an overview of the proposed approach with the FixMatch loss terms as an exemplary instantiation of the semi-supervised learning component.,1,neutral
"Additionally, we compared with FedSemi methods that can be easily
translated into the label set mismatch scenario including FSSL [22] and FedAvg with FixMatch [18].",2,positive
(a) FedAvg with 100% labeled data (b) FedAvg [15] (c) FedAvg* [15] (d) FedProx* [8] (e) MOON* [7] (f) FedRS [10] (g) FPSL [2] (h) FedAvg-FixMatch* [18] (i) FSSL* [22].,2,positive
translated into the label set mismatch scenario including FSSL [22] and FedAvg with FixMatch [18].,2,positive
"For example, Π-model [24] uses self-ensembling to leverage label predictions on different epochs and under different conditions; Mean Teacher [32] utilizes averaging model weights instead of label predictions; FixMatch [30] and FlexMatch [38] employ pseudo-labels generated from model predictions to guide the training.",1,neutral
"…2022d, 2021c) were also explored to enhance the discrimination capability of the model, based on techniques like autoencoder, Mean Teacher (Tarvainen and Valpola, 2017), MixMatch (Berthelot et al., 2019), Virtual Adversarial Training (VAT) (Miyato et al., 2018), and FixMatch (Sohn et al., 2020).",2,positive
"To efficiently leverage limited strongly annotated data and a large amount of unlabeled data for training, unsupervised learning (Zhang et al., 2020a) and semi-supervised learning techniques (Zhang et al., 2022b; Wang et al., 2022d, 2021c) were also explored to enhance the discrimination capability of the model, based on techniques like autoencoder, Mean Teacher (Tarvainen and Valpola, 2017), MixMatch (Berthelot et al., 2019), Virtual Adversarial Training (VAT) (Miyato et al., 2018), and FixMatch (Sohn et al., 2020).",2,positive
"Inspired by [47], [33], we assume that the prediction label owns high confidence if the model assigns a high probability to one of the possible classes.",1,neutral
"Besides, inspired by semi-supervised learning works [32], [33], [34], we propose label correction (LC) to leverage the model’s prediction as target label and use it to correct the unreliable pseudo label, instead of discarding the unreliable data directly [29].",2,positive
"In the past few years, semi-supervised learning (SSL) [2, 32] has achieved impressive performance in image classification.",1,neutral
"Hence, semi-supervised learning (SSL) [17,1,26,18,21] becomes a popular technique to exploit unlabeled data with only limited annotated data.",1,neutral
"Augmentation-based methods such as FixMatch [26] and MixMatch [1] also fail to produce good results, which may be due to the inappropriate choice of data augmentations.",1,neutral
"However, many existing semi-supervised algorithms [62, 54, 65, 66] assume that no distributional shift exists between D and D∗.",1,neutral
", predictions on unlabeled data made by a previously-learned model) to boost the model’s performance (Yarowsky, 1995; Grandvalet & Bengio, 2004; Lee et al., 2013; Wei et al., 2020; Sohn et al., 2020).",2,positive
"…leveraging unlabeled data with DNNs. Self-training methods train a model to fit pseudo-labels (i.e., predictions on unlabeled data made by a previously-learned model) to boost the model’s performance (Yarowsky, 1995; Grandvalet & Bengio, 2004; Lee et al., 2013; Wei et al., 2020; Sohn et al., 2020).",1,neutral
"The closest method to our work is FixMatch (Sohn et al., 2020) which adopted Consistency Regularization (Bachman et al.",2,positive
"The closest method to our work is FixMatch (Sohn et al., 2020) which adopted Consistency Regularization (Bachman et al., 2014) to semisupervised learning and achieved state-of-the-art results on image classification tasks.",2,positive
"Our SSL framework is inspired by semisupervised approaches [9], [10], which is composed of supervised loss and consistency loss between the contrastive features of unlabeled samples.",2,positive
"Recent years have witnessed significant progress in semisupervised classification for image domain [8], [9], [10].",1,neutral
"[14] proposed to classify the image perturbation into two types of strong and weak perturbation, and to use the prediction results of weak perturbation [15] processed as the pseudo-label of strong perturbation, because the prediction results after weak perturbation are more stable, and this novel method promotes the development of semi-supervised learning.",1,neutral
"Regarding representation learning, multiple approaches have been recently developed, consisting of supervised (Touvron et al., 2021), self-supervised (Chen et al., 2020), semi-supervised (Sohn et al., 2020) and natural language supervised (Radford et al., 2021) methods.",1,neutral
", 2020), semi-supervised (Sohn et al., 2020) and natural language supervised (Radford et al.",1,neutral
"First of all, as self-training (ST) has demonstrated great success in semisupervised learning [38, 49] and domain adaptation [29] by exploiting unlabeled data, we propose to employ selftraining for TTAOD by learning from the unlabeled testing samples.",2,positive
Self-training (ST) has demonstrated tremendous effectiveness for semi-supervised learning [38].,1,neutral
Motivated by (Tarvainen and Valpola 2017; Berthelot et al. 2019; Sohn et al. 2020) adding appropriate perturbations to the input (e.,2,positive
"the source-free domain adaptation methods [74], our framework is based on a reliable pre-trained model on the source domain, where the domain gap is not too large.",2,positive
"• The proposed method is the first depth-based hand pose estimation method to incorporate advances from recent SSL methods such as [30, 25, 26], which target general-purpose image classification.",1,neutral
"A standard practice of SSL methods in image classification is to use a fixed threshold for masking [30, 25].",1,neutral
"Image classification SSL methods such as [30, 25, 53] have traditionally taken a binary approach for masking (ma = 1 and mr = 0), which is a special case of Eq.",1,neutral
"Our work is closely related to the recent line of SSL methods based on pseudo-labeling [44, 45, 46], where they produce artificial label for unlabeled data samples and train the model to predict the artificial label when fed unlabeled samples as input, and consistency training [30, 25, 47] wherein they enforce the model predictions to be consistent across a sample and its perturbed version.",2,positive
"The reason is that UDA, FixMatch and FlexMatch only retain the samples with reliable pseudo labels based on the confidence scores.",0,negative
"The data scarcity problem is solved by semi-supervised learning (SSL) for the classification task [3,25,28,34].",1,neutral
"We compare with state-of-the-art semisupervised approaches, including UDA [30], FixMatch [25] and FlexMatch [34].",2,positive
"More recent Unsupervised Data Augmentation (UDA) [30] and FixMatch [25] combine pseudo labeling, consistency regularization with strong augmentations [6] and achieve superior performance.",2,positive
", Cutout [233], RandAugment [234]), several prominent semi-supervised learning methods [104], [235] unleash the power of consis-",1,neutral
"typical cases: self-training [100], [101], which encourages the model to produce confident predictions; consistency regularization under input variations [102], [103], [104] and model variations [105], [106], which forces networks to output similar predictions when inputs or models are perturbed; and graph-based regularization [107], which seeks local smoothness by maximizing the pairwise similarities between nearby data points.",1,neutral
"Formally, an exemplar of consistency regularization [104] is expressed as:",1,neutral
"…segmentation methods, including Entropy Minimization (Ent-Mini) (Vu et al., 2019), Cross Consistency Training (CCT) (Ouali et al., 2020), FixMatch (Sohn et al., 2020), Regularized Dropout (R-Drop) (Wu et al., 2021), Cross Pseudo Supervision (CPS) (Chen et al., 2021b), Uncertainty Rectified…",2,positive
", 2020), FixMatch (Sohn et al., 2020), Regularized Dropout (R-Drop) (Wu et al.",1,neutral
"We compare our proposed framework with four baselines and some recent SOTA semi-supervised segmentation methods, including Entropy Minimization (Ent-Mini) (Vu et al., 2019), Cross Consistency Training (CCT) (Ouali et al., 2020), FixMatch (Sohn et al., 2020), Regularized Dropout (R-Drop) (Wu et al., 2021), Cross Pseudo Supervision (CPS) (Chen et al., 2021b), Uncertainty Rectified Pyramid Consistency (URPC) (Luo et al., 2022b) and Cross Teaching between CNN
and Transformer (CTCT) (Luo et al., 2022a).",2,positive
"Here, we propose to follow the self-supervised learning principle [25, 31] and introduce a self-distillation learning strategy to benefit the MLR model from the structured semantic correspondence among labels.",1,neutral
"[60] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li.",0,negative
"Semi-supervised learning [8,9,60,74,82] studies how to leverage a training dataset with both labeled data and unlabeled data to obtain a model with high accuracy.",1,neutral
"To avoid such reliance, various learning frameworks have been proposed and investigated: Self-supervised learning aims to learn meaningful representations with heuristic proxy visual tasks, such as rotation prediction [16] and the more prevalent instance discrimination task, the latter, being widely applied in self-supervised contrastive learning framework; semisupervised learning usually considers a dataset for which only a small part is annotated – within this setting, pseudo labelling methods [24] and consistency regularization techniques [1, 29] are typically used; Moreover, learning using more accessible but noisy data, such as web-crawled data, has also received increasing attention [13, 25].",1,neutral
"In particular, the pre-trained model for re-teachers, the pre-trained model for semi-supervised learning, and the algorithm for semisupervised learning of Prom-PATE in Table 4 are Swin Transformer [25], EVA [15], and FreeMatch [41], respectively, while those of Prom-PATE in Tables 1∼3 are Swin Transformer [25], ViT [13], and FixMatch [36].",1,neutral
"Tables 18, 19, and 20 also report the accuracies when two different semi-supervised learning methods, FixMatch [36] and FreeMatch [41], are used.",1,neutral
We also utilize counterfactual reasoning and adaptive margins proposed in DebiasPL [28] to remove the bias of pseudo label in FixMatch.,2,positive
"And many pseudo labeling methods like FixMatch [24], FlexMatch [34] and AdaMatch [4] focus on how to select confidently pseudolabeled data.",1,neutral
"For the Softmax classifier, we compute the standard cross-entropy lossLCE for labeled data and the FixMatch loss LFM for the high-certainty inliers I𝑢 identified by the EDL detector.",2,positive
"To illustrate the effectiveness of our method on Open-set SSL, we compare it against several baseline methods, including FixMatch [24], MTC [32] and OpenMatch [20].",2,positive
"For hyperparameters of FixMatch, we set them the same as in OpenMatch [20].",1,neutral
"During training, with EDL excluding outliers, we adopt an SSL method (i.e., FixMatch following OpenMatch [20]) to our Softmax head to enhance representation quality and classification accuracy.",2,positive
"In FixMatch, pseudo label is generated by Softmax head and will also be used to softmax head as a supervision signal.",2,positive
"It has achieved promising results across various computer vision tasks, such as image classification [40, 42], image segmentation [14, 18], object detection [41, 50] and action recognition [52].",1,neutral
"In the last decades, semisupervised learning, which combines a small amount of labeled data with a large collection of unlabeled data during training, has achieved considerable success on various tasks such as image classification [3, 40], object detection [41, 50] and semantic segmentation [14, 18].",1,neutral
"We obtain FixMatch [17] by choosing τs to be strong augmentations, τt to be weak augmentations, t to share weights with s, f to filter out predictions with a probability below a given threshold, and p to set the predicted values to 1 if they are above that threshold.",1,neutral
Recently FixMatch [17] has gained popularity.,2,positive
"Self-training: These methods are among the most popular approaches in semisupervised learning; examples include Mean Teacher [20], ReMixMatch [1] and FixMatch [17].",1,neutral
"Semi-Supervised Learning (SSL) is attractive because of its capability to further unveil the power of machine learning with abundant cheap unlabeled data [50, 50, 39, 25, 4, 46, 40].",1,neutral
"Surprisingly, it has been rarely explored in monocular 3D detection, despite its success in numerous 2D vision tasks [48, 4, 46, 55, 56, 64, 60, 58, 31, 59].",1,neutral
"Such a paradigm has demonstrated great success in image classification [48, 4, 46, 55], semantic segmentation [58, 31, 59], and 2D object detection [56, 64, 60].",1,neutral
"Teacher-Student setup is inspired by [23, 6, 55, 31, 52].",1,neutral
"Semi-supervised learning commonly assumes that unlabeled and labeled data are from the same source (Sohn et al., 2020).",1,neutral
"As an important application of semi-supervised learning, many semisupervised learning methods [38, 22] have been applied and extended to solve the semi-supervised segmentation problem [46, 41, 20].",1,neutral
"Early studies mainly focused on image classification task [14, 34] and have gradually been generalized to various tasks [28, 35, 45].",1,neutral
"As early attempts, CSD [13] encourages consistent predictions for horizontally flipped image pairs, whereas STAC [35] transfers the weak and strong augmentations from FixMatch [34] to SSOD.",1,neutral
"Most early studies on semi-supervised object detection [13, 25, 35] can be considered as a direct extension of SSL methods designed for image classification, using a teacher-student training paradigm [2,34,37].",1,neutral
"Following extensive pioneering studies on semi-supervised image classification [2, 14, 34], several methods on semisupervised object detection have emerged.",1,neutral
"As early attempts, CSD [13] encourages consistent predictions for horizontally flipped image pairs, whereas STAC [33] transfers the weak and strong augmentations from FixMatch [32] to SSOD.",1,neutral
"In semi-supervised learning, pseudo labels are often constructed as one-hot forms while ensuring high reliability and are regarded as the training target of standard cross-entropy [10].",1,neutral
MixMatch achieved higher accuracy in classifying images compared to earlier SSL algorithms with a small number of labelled images [10].,1,neutral
"We choose three of them to combine with split-learning architecture in this work: the least demanding (MeanTeacher [19]), the most demanding (FixMatch [22]), and one between (Virtual Adversarial Training VAT [25]).",2,positive
"When combined with confidencebased pseudo-labeling (Sohn et al., 2020; Zhang et al., 2021), at each iteration, the process can be summarized as follows:
1.",1,neutral
"For results in Table 1, we follow the default training settings of FixMatch (Sohn et al., 2020), yet reduce the number of iterations to 6 × 216 following CREST (Wei et al., 2021).",2,positive
"Critically, these methods still rely on confidence-based thresholding (Lee et al., 2013; Sohn et al., 2020; Xie et al., 2020a; Zhang et al., 2021) for pseudo-labeling, in which only the unlabeled samples whose predicted class confidence surpasses a very high threshold (e.g., 0.95) are…",1,neutral
"FixMatch (Sohn et al., 2020) and evaluate it on the balanced CIFAR10 (Krizhevsky & Hinton, 2009), SVHN (Netzer et al., 2011), and STL-10 (Coates et al., 2011) benchmarks using the default FixMatch training settings.",2,positive
"To help explain our approach, Inlier Pseudo-Labeling (InPL), we first overview the consistency regularization with confidence-based pseudo-labeling framework (Sohn et al., 2020; Zhang et al., 2021; Xie et al., 2020a) that state-of-the-art SSL methods build upon, as our method simply replaces one step — the pseudo-labeling criterion.",2,positive
"To evaluate the proposed InPL, we integrate it into the classic FixMatch (Sohn et al., 2020) framework and the recent state-of-the-art imbalanced SSL framework ABC (Lee et al.",2,positive
", 2013) combined with consistency regularization (Laine & Aila, 2017; Tarvainen & Valpola, 2017; Berthelot et al., 2020; Sohn et al., 2020; Xie et al., 2020a).",1,neutral
"The recently introduced weak-strong data augmentation paradigm (Sohn et al., 2020; Xie et al., 2020a) can be viewed as the combination of these two directions.",1,neutral
"For this, we integrate InPL into the FixMatch framework (Sohn et al., 2020) (denoted as FixMatch-InPL), and compare it with the following FixMatch variants, which all use confidence-based pseudo-labeling: UDA (Xie et al., 2020a), FixMatch-Debiased (Wang et al., 2022), and UPS (Rizve et al., 2021).",2,positive
"…our approach, Inlier Pseudo-Labeling (InPL), we first overview the consistency regularization with confidence-based pseudo-labeling framework (Sohn et al., 2020; Zhang et al., 2021; Xie et al., 2020a) that state-of-the-art SSL methods build upon, as our method simply replaces one step — the…",2,positive
"…Xie et al., 2020b; Sohn et al., 2020; Xie et al., 2020a; Zhang et al., 2021; Xu et al., 2021), leveraging standard frameworks such as FixMatch (Sohn et al., 2020) to predict pseudo-labels on weakly-augmented views of unlabeled images and train the model to predict those pseudo labels on…",2,positive
"We compare InPL with the confidence-based methods UDA (Xie et al., 2020a) and FixMatch (Sohn et al., 2020).",2,positive
"…hard labels as optimization targets, while consistency regularization (Laine & Aila, 2017; Tarvainen & Valpola, 2017; Berthelot et al., 2019; 2020; Sohn et al., 2020; Xie et al., 2020a) trains a model to produce the same outputs for two different views (e.g, strong and weak augmentations) of an…",2,positive
"When combined with confidencebased pseudo-labeling (Sohn et al., 2020; Zhang et al., 2021), at each iteration, the process can be summarized as follows:",1,neutral
", 2021), leveraging standard frameworks such as FixMatch (Sohn et al., 2020) to predict pseudo-labels on weakly-augmented views of unlabeled images and train the model to predict those pseudo labels on strongly-augmented views.",1,neutral
"Due to limited computation, we are unable to use large batch sizes as in FixMatch (Sohn et al., 2020) (1024 for labeled data and 5120 for unlabeled data).",2,positive
"In this section, we first compare our energy-based pseudo-labeling approach to confidence-based pseudo-labeling approaches that build upon FixMatch (Sohn et al., 2020) and ABC (Lee et al., 2021) framework and compare to state-of-the-art imbalanced SSL approaches.",2,positive
", 2013) with consistency regularization (Laine & Aila, 2017; Tarvainen & Valpola, 2017; Berthelot et al., 2019; 2020; Xie et al., 2020b; Sohn et al., 2020; Xie et al., 2020a; Zhang et al., 2021; Xu et al., 2021), leveraging standard frameworks such as FixMatch (Sohn et al.",2,positive
"Pseudo-labeling, a type of self-training (Scudder, 1965; McLachlan, 1975) technique, converts model predictions on unlabeled samples into soft or hard labels as optimization targets, while consistency regularization (Laine & Aila, 2017; Tarvainen & Valpola, 2017; Berthelot et al., 2019; 2020; Sohn et al., 2020; Xie et al., 2020a) trains a model to produce the same outputs for two different views (e.",1,neutral
"We conduct ablation studies to better understand our approach InPL, where we integrate it into the framework of FixMatch (Sohn et al., 2020) and ABC (Lee et al., 2021).",2,positive
"To evaluate the proposed InPL, we integrate it into the classic FixMatch (Sohn et al., 2020) framework and the recent state-of-the-art imbalanced SSL framework ABC (Lee et al., 2021) by replacing their vanilla confidence-based pseudo-labeling with our energy-based pseudo-labeling.",2,positive
", 2021) are build upon the pseudo-labeling and consistency regularization frameworks (Sohn et al., 2020; Xie et al., 2020a) by augmenting them with additional modules that tackle specific imbalanced issues (e.",2,positive
"…the frontier of semi-supervised learning (SSL) has seen significant advances through pseudo-labeling (Rosenberg et al., 2005; Lee et al., 2013) combined with consistency regularization (Laine & Aila, 2017; Tarvainen & Valpola, 2017; Berthelot et al., 2020; Sohn et al., 2020; Xie et al., 2020a).",1,neutral
"Critically, these methods still rely on confidence-based thresholding (Lee et al., 2013; Sohn et al., 2020; Xie et al., 2020a; Zhang et al., 2021) for pseudo-labeling, in which only the unlabeled samples whose predicted class confidence surpasses a very high threshold (e.",1,neutral
"…(Lee et al., 2013) with consistency regularization (Laine & Aila, 2017; Tarvainen & Valpola, 2017; Berthelot et al., 2019; 2020; Xie et al., 2020b; Sohn et al., 2020; Xie et al., 2020a; Zhang et al., 2021; Xu et al., 2021), leveraging standard frameworks such as FixMatch (Sohn et al., 2020) to…",2,positive
"FixMatch (Sohn et al., 2020) InPL (ours)
Confidence threshold τ = 0.95 τ = 0.8 τ = 0.7 τ = 0.6 -
Accuracy 73.73 74.12 71.53 73.55 77.03
Table A: Comparison to FixMatch with various confidence thresholds on CIFAR10-LT.",0,negative
"…SSL methods (Kim et al., 2020; Wei et al., 2021; Lee et al., 2021) are build upon the pseudo-labeling and consistency regularization frameworks (Sohn et al., 2020; Xie et al., 2020a) by augmenting them with additional modules that tackle specific imbalanced issues (e.g., using per-class…",2,positive
"It is widely adopted in semi-supervised learning [34, 35, 36, 37, 38, 39] and weakly supervised learning [40, 41, 42].",1,neutral
"Integral to the success of Soft Teacher is a student-teacher data augmentation strategy inspired by STAC (Sohn et al., 2020b).",2,positive
"High-Quality Baselines Following existing literature (Liu et al., 2021; Sohn et al., 2020b; Tang et al., 2021; Xu et al., 2021), we evaluate our approach for semi-supervised detection on VOC and COCO 2017 datasets.",2,positive
"As is common practice [44, 48], the teacher’s parameters θ̄ are updated from the student’s via θ̄ = EMA(θ) at each training step.",1,neutral
"All hyper-parameters related to Soft Teacher remain the same, including the EMA momentum, which defaults to 0.999 following common practice in the semi-supervised classification literature (Sohn et al., 2020a; Tarvainen & Valpola, 2017).",1,neutral
"Soft Teacher vastly improves upon STAC (Sohn et al., 2020b) and Unbiased Teacher (Liu et al., 2021) by enabling end-toend pseudo-labeling on unlabeled images.",2,positive
"At test time, we resize all instances to the shorter side of 800 resolution, but otherwise do not perform any test-time augmentation, following standard supervised (Ren et al., 2015) and semi-supervised (Liu et al., 2021; Sohn et al., 2020b; Tang et al., 2021; Xu et al., 2021) protocols.",2,positive
"As is common practice (Sohn et al., 2020a; Tarvainen & Valpola, 2017), the teacher’s parameters θ̄ are updated from
the student’s via θ̄ = EMA(θ) at each training step.",2,positive
"In addition, the efficiency of the ensemble was improved by adding a second weak augmentation technique to FixMatch, which used only one weak augmentation technique.",2,positive
’s FixMatch [15] is a semi-supervised learning method using consistency regularization and pseudo-labeling.,1,neutral
"Train model with ensembled FixMatch for epoch←0 to max_epoch do:
for j←0 to batches do: compute loss on 𝑋𝑙 : 𝑙𝑠
𝑙𝑜𝑔𝑖𝑡𝑠1, 𝑙𝑜𝑔𝑖𝑡𝑠𝐸𝑀𝐴1 = model(Augment1(𝑋𝑢 )), EMA (Augment1(𝑋𝑢 )) 𝑙𝑜𝑔𝑖𝑡𝑠2, 𝑙𝑜𝑔𝑖𝑡𝑠𝐸𝑀𝐴2= model(Augment2(𝑋𝑢 )), EMA (Augment2(𝑋𝑢 )) ensembeled logit 𝑙𝑒 = (𝑙𝑜𝑔𝑖𝑡𝑠1 + 𝑙𝑜𝑔𝑖𝑡𝑠𝐸𝑀𝐴1 + 𝑙𝑜𝑔𝑖𝑡𝑠2 + 𝑙𝑜𝑔𝑖𝑡𝑠𝐸𝑀𝐴2) / 4
compute loss on (Augment3(𝑋𝑢 ), 𝑙𝑒 ) : 𝑙𝑢 𝜆 ← 𝐶𝑜𝑠𝑖𝑛𝑒_𝑑𝑒𝑐𝑎𝑦 (𝜆𝑠 , 𝜆𝑓 , 𝑒𝑝𝑜𝑐ℎ) 𝑙 = 𝑙𝑠 + 𝜆𝑙𝑢 update model weights
end end
𝑠ℎ𝑎𝑟𝑝𝑒𝑛 (𝑙,𝑇 ) = 𝑙
1 𝑇
𝑖∑𝐿 𝑗=0 𝑙 1 𝑇 𝑗
(3)
After calculating the loss between the generated pseudo label and the predicted value of the strongly-augmented image, sharpening is applied to the prediction to reduce the entropy of the label distribution, as Equation 3.",2,positive
Equation 1 is a loss term of unlabeled data in FixMatch.,1,neutral
"𝑙𝑠 = 𝐻 (𝑦, 𝑝 (𝑦 |𝑥𝑙 ))
𝑙𝑢 = 𝐻 (𝑞, 𝑝 (𝑦 |𝐴 (𝑥𝑢 ))) (1)
𝑙 = 𝑙𝑠 + 𝜆𝑙𝑢
Sohn et al.’s FixMatch [15] is a semi-supervised learning method using consistency regularization and pseudo-labeling.",1,neutral
The proposed method of this paper is a modified FixMatch method in the sense that labels for noisy label data are removed to make unlabeled data and clean data are used for supervised learning.,1,neutral
"The proposed method is based on FixMatch, and it is described in Algorithm 1.",2,positive
"For the unlabeled target data, we adopt pseudo-labels for cross-entropy calculation, following a semi-supervised learning method, FixMatch [41].",1,neutral
"For Camelyon17 WILDS, the extended SSL methods are compared to reimplemented UDG methods (DARLING, DiMAE) but also to the Semi-Supervised Learning method FixMatch (Sohn et al., 2020) and SSL method SWaV trained with additional data from the target domain.",2,positive
Extended SSL methods with BSS even surpass FixMatch and SWaV trained with additional unlabeled data from the target domain.,2,positive
"For example, [28] utilizes the predictions of weakly augmented data to guide the learning of strongly augmented versions, where they select reliable predictions as pseudo labels based on the prediction confidence and use them to enforce the consistency constraints to regularize the model training.",1,neutral
"The straightforward way is to directly extend the FixMatch [28] from images to point clouds, i.e., selecting confident predictions of the weakly augmented point clouds as pseudo labels and applying consistency constraints to guide the predictions of strongly augmented ones.",1,neutral
"The straightforward way is to directly extend the FixMatch [28] from images to point clouds, i.",1,neutral
"1 (d)), they are being discarded and not utilized during training [17, 28], resulting in a sub-optimal performance.",0,negative
"Others adopt pseudo-labeling [9], [2], [32], [6], [45], which is another popular method of SSL that can be treated as a variant of consistent regularization.",1,neutral
"Previous works are mainly divided into two categories of methods, consistency regularization [29], [28], [36], [25], [35] and pseudo-labeling [9], [2], [32].",1,neutral
FixMatch [32] sets a confidence threshold to filter the low quality pseudolabels.,1,neutral
"Some SSL methods [28], [29], [32] believe that data augmentations are very important to SSL.",1,neutral
"To avoid this phenomenon, different from the fixed threshold methods [38], [32], we propose a dynamic threshold strategy.",1,neutral
"The aim of this technique is to expand the knowledge provided by the teacher through its predictions and activations on the unlabeled dataset: [7], [8], [9], [10], [11], [12], [13], [14], [15].",2,positive
FixMatch [7] achieved ar X iv :2 30 3.,1,neutral
"Pseudo-labeling is a simple but effective technique used in semi-supervised learning [31, 54], self-supervised learning [4] and domain adaptation [5, 37, 38, 60].",1,neutral
Fix-Match [54] and Ontarget [60] are methods that used pseudo-labels but they did not perform any labels refinement.,1,neutral
"SSL has achieved significant progress in classification tasks in recent years, with performance comparable to fully supervised learning [21, 29].",1,neutral
"Pseudo-labeling methods [10, 21] employ weak data augmentation to annotate pseudo-labels for unlabeled data.",1,neutral
"In our setting, threshold like FixMatch [30] does not play a major role in selecting high-confidence unlabaled samples.",2,positive
"3) FedAvg+MixMatch [3] and 4) FedAvg+FixMatch [30] apply supervised FL on fully-labeled clients and semi-supervised learning by MixMatch/FixMatch on partially-labeled clients, then local models are aggregated on the server by FedAvg.",2,positive
"Recently, SSL methods that combine consistency of augmented data and label generation have achieved the promising performance, such as MixMatch [3], ReMixMatch [2], FixMatch [30] and SimPLE [9].",2,positive
"Inspired by consistency regularization [8, 67, 76], superset learning [27, 28, 29, 60] and distributional alignment [36], “credal self-supervised learning” aims at decreasing the reliance on a single distributional assumption.",1,neutral
"Although the SSL approach for the PCB_SS model was adopted from the FixMatch model [18], the applications of which are limited to object-specific classification, the defect detection in this study was related to feature-related recognition rather than object detection.",1,neutral
"001 for both models and scheduled during training using a cosine learning rate decay [18,23]: η cos ( 7πk 16K ) , where η, k, and K denote the initial learning rate, the current training step, and the total number of training steps, respectively.",1,neutral
"Labeled data are critical for creating the underlying feature space, and SSL takes advantage of prior knowledge of the domain and data distribution to relate the data and labels to improve the classification performance [17,18,24,31,32].",2,positive
"However, the effect of unlabeled data increases during training [18].",0,negative
A high threshold is expected to improve the performance of the pseudolabeling [18].,1,neutral
FixMatch is a multimixed SSL method proposed by Google Brain [24].,2,positive
"In particular, they explore the usage of selfsupervised and semi-supervised learning based on SimCLR (Chen et al. (2020)) and FixMatch (Sohn et al. (2020)) for riverbed segmentation, land cover land use classification, and flood inundation mapping on the Sen1Floods11 dataset (Patel et al. (2021)).",2,positive
"the input data is disturbed slightly if the model is well-trained [12], [13].",1,neutral
"We compare RoPAWS (and PAWS) with various baselines: Pseudo-label (Lee et al., 2013), FixMatch (Sohn et al., 2020), Self-training (Hinton et al.,
8RoPAWS does not suffer from representation collapse similar to PAWS, as discussed in Appendix B.
2014), and CCSSL (Yang et al., 2022), trained from…",2,positive
"We compare RoPAWS with PAWS and FixMatch (Sohn et al., 2020), together with state-of-the-art robust Semi-SL (OOD filtering) methods: UASD (Chen et al., 2020d), DS3L (Guo et al., 2020), OpenMatch (Saito et al., 2021), and OpenCos (Park et al., 2021).",2,positive
"We compare RoPAWS with PAWS and FixMatch (Sohn et al., 2020), together with state-of-the-art robust Semi-SL (OOD filtering) methods: UASD (Chen et al.",2,positive
"Thus, one can adapt the softmax classifier of previous algorithms, such as FixMatch (Sohn et al., 2020), as a GMM classifier to estimate the uncertainty of samples.",1,neutral
"Combining two objectives, prediction-based approaches has shown notable results (Berthelot et al., 2019; 2020; Sohn et al., 2020; Kuo et al., 2020; Li et al., 2021).",2,positive
", 2020a), FixMatch (Sohn et al., 2020), and MPL (Pham et al.",2,positive
", 2013), FixMatch (Sohn et al., 2020), Self-training (Hinton et al.",1,neutral
"…PAWS (state-of-the-art) with various baselines: (a) prediction-based semi-supervised learning (Semi-SL) such as UDA (Xie et al., 2020a), FixMatch (Sohn et al., 2020), and MPL (Pham et al., 2021), and (b) pre-training with self-supervised learning (Self-SL) and fine-tuning such as BYOL (Grill et…",2,positive
"2020) and FixMatch (Sohn et al. 2020), where a fixed threshold is chosen to determine confident examples.",1,neutral
"Specifically, the confidence-based thresholding, which uses unlabeled data only when the predictions are sufficiently confident, is presented in state-of-the-art methods UDA (Xie et al. 2020) and FixMatch (Sohn et al. 2020), where a fixed threshold is chosen to determine confident examples.",2,positive
"Semi-supervised Learning We select FixMatch (Sohn et al. 2020), a simple yet effective SSL example, as the base of the proposed SSL algorithm.",1,neutral
"2020) and FixMatch (Sohn et al. 2020), under the fully SLbased skyline (Li, Kiseleva, and de Rijke 2020).",2,positive
"Unlike the multi-class classification setting in (Sohn et al. 2020) that accepts or rejects a whole example, the feedbackenhanced thresholds filter a set of confidently accepted (rejected) atomic dialog actions (i.",1,neutral
"…GDPL (Takanobu, Zhu, and Huang 2019); 2) CRM methods: IPS (Swaminathan and Joachims 2015) and BanditNet (Joachims, Swaminathan, and De Rijke 2018); and 3) SSL methods: Act-VRNN (Huang et al. 2020) and FixMatch (Sohn et al. 2020), under the fully SLbased skyline (Li, Kiseleva, and de Rijke 2020).",2,positive
"Unlike prior SSL algorithms (Sohn et al. 2020; Zhang et al. 2021a), we do not pre-split the data examples but determine them in each batch adaptively during training.",2,positive
"Unlike the multi-class classification setting in (Sohn et al. 2020) that accepts or rejects a whole example, the feedbackenhanced thresholds filter a set of confidently accepted (rejected) atomic dialog actions (i.e., confident classes) for each example.",1,neutral
Recent SSL algorithms (Sohn et al. 2020; Xie et al. 2020) select high-confidence unlabeled data with a fixed confidence threshold for each class.,2,positive
"Table 7 presents the results on the Office-Home dataset when combining our method DUC with the semi-supervised learning method FixMatch Sohn et al. (2020), where the hyper-parameter τ is set to 0.8.",2,positive
"Here, we consider one representative semi-supervised learning method: FixMatch (Sohn et al., 2020).",1,neutral
"95, which is consistent with the parameters used in the Fixmatch [9].",1,neutral
FixMatch [9] selects pseudo-labels as consistency,1,neutral
"FixMatch [9], UDA [10]), while using federated learning strategies to aggregate the learned weights.",1,neutral
"In our study, we have chosen k = 7 and threshold τ equal to 0.95, which is consistent with the parameters used in the Fixmatch [9].",1,neutral
"In computer vision, the research of SSL on image classification garners an influx of interest [14], [15], [16], [33], [34], [35], [36].",1,neutral
"This paradigm can well exploit large amounts of unlabeled data based on limited label information, showing great advantages over previous semisupervised approaches [10], [11], [14], [16].",1,neutral
"Inspired by the advances in image classification [14], [15], [16], recent endeavors also resort to teacher-student learning for SSOD [10], [11], [12], [13], [17].",1,neutral
"Recently, researchers resort to teacher-student learning and combine the merits of these two methodologies for better SSL [14], [15], [16], [35].",1,neutral
"In early SSL methods [14], [15], [16], [35], one popular solution is to apply consistency regularization, which encourages the model to make consistent predictions with different perturbations, e.",1,neutral
"More recently, some methods also aim to filter the noisy pseudo-labels by the prediction probability [16], and address the classimbalanced problem [42].",1,neutral
"On the other hand, while it is natural to use images sampled from generative models for semisupervised classification [9, 10], discriminative methods [12, 13, 14] dominant the area recently.",1,neutral
"DPT outperforms competitive baselines [14, 13, 12] substantially with four labels per class, achieving an impressive state-of-the-art error rate of 4.",0,negative
"For semi-supervised classification, classical work includes generative approaches based on VAE [9, 29, 30] and GAN [31, 21, 32, 33, 34, 35], and discriminative approaches with confidence regularization [36, 37, 38, 39] and consistency regularization [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 12, 13, 50].",1,neutral
"53% on CIFAR-10, which even outperforms the performance of SOTA methods [12, 14, 13] on CIFAR-10 with 25 labels per class.",0,negative
"AEL method is based on FixMatch [163], a widely used hybrid method originally proposed for image classification.",2,positive
"Then, several variants of PPL have been proposed and widely used in SSL field [37]–[39].",1,neutral
"More recent works have shown promising results in improving deep neural network by applying SSL methods, especially in the field of image classification [Sohn et al., 2020, Sun and Ge, 2020, Han et al., 2020].",1,neutral
"pseudo-labeling: pseudo-labeling has often been used for semisupervised learning [10, 11], in which a pre-trained model is first trained using few labeled data.",1,neutral
"Note that SSL is applied to data with high shares (typically over 80% [Sohn et al., 2020, Arazo et al., 2020]) of unlabeled data, where initial overfitting is more likely than final overfitting.",1,neutral
2019) and to impose consistencies (Sohn et al. 2020; Xie et al. 2020; Berthelot et al. 2019).,0,negative
"Methods such as [34, 42, 1, 30] have explored semi supervised techniques to make use of both labelled and unlabelled data.",1,neutral
"The basic principle of [34] is that it uses pseudo labels predicted on the weak augmented image, to consistently train heavily augmented images.",1,neutral
"Another issue with stateof-the-art classification methods such as FixMatch  (Sohn et  al., 2020), UDA (Xie et  al., 2020) or ReMixMatch (Berthelot et al., 2019) is that they rely heavily on data augmentation techniques that are specific to visual data only.",2,positive
"Furthermore, we compare our debiased version of Fixmatch (Sohn et al., 2020), designed to handle informative labels, with its original counterpart (Fixmatch) and its debiased version for MCAR labels (DeFixmatch) (Schmutz et al., 2023) on the CIFAR-10 dataset (Krizhevsky, 2009).",2,positive
"In the CIFAR-10 dataset and considering Setting S2., we compare the original version of Fixmatch (Sohn et al., 2020) with its debiased versions for MCAR data (Schmutz et al., 2023) and for MNAR data3.",2,positive
"Recently, state-of-the-art methods such as (Sohn et al., 2020; Berthelot et al., 2019a) have also been developed to make the model more robust to data augmentation of the features.",2,positive
"SSL algorithms (Rizve et al., 2021; Sohn et al., 2020) that utilize pseudo-label techniques often employ a fixed threshold to select relevant imputations from unlabeled data.",1,neutral
"Copyright 2023 by the author(s).
deep learning techniques demonstrating remarkable empirical successes, particularly through the systematic use of data augmentation (Berthelot et al., 2019a; Xie et al., 2020; Sohn et al., 2020; Rizve et al., 2021).",2,positive
"FixMatch generates various data views through image augmentations followed by Cutout (DeVries and Taylor, 2017).",2,positive
"5We adapted the FixMatch implementation https://github.com/kekmodel/FixMatch-pytorch
We denote weak augmentation of input x by α(x) and the strong augmentation by A(x).",2,positive
"It shows that, on natural images CIFAR10 for which FixMatch is developped, while the original FixMatch with random labeled data is still outperformed by SOEL, FixMatch with our proposed querying strategy k-means++ has a comparable performance with SOEL.",2,positive
"In addition, the FixMatch framework is restrictive and may not be applicable for tabular data and medical data, as the augmentations are specially designed for natural images.",2,positive
"We noticed that, although FixMatch focuses on making use of the unlabeled data, its performance is highly affected by the quality of the labeled data subset.",2,positive
"On F-MNIST, SOEL outperforms all ablations clearly under all query budgets, while on CIFAR-10, SOEL outperforms all ablations except for FixMatch when query budget is low.",2,positive
FixMatch is designed for classification.,2,positive
Train the detector with the FixMatch loss Eq.,2,positive
"The training objective function we used is
LFixMatch(θ) = 1 |Q| ∑ j∈Q ( yjL θ 1(α(xj)) + (1− yj)Lθ0(α(xj)) ) + 1
|U| ∑ i∈U 1(S(α(xi)) < q0.",1,neutral
"However, such advantage of FixMatch diminishes for the gray image dataset F-MNIST, where both variants are beat by SOEL on all querying budgets.",2,positive
"FixMatch, as a semi-supervised learning algorithm, regularizes the image classifier on a large amount of unlabeled data.",1,neutral
"To this end, we adapted an existing semi-supervised learning framework FixMatch (Sohn et al., 2020a) to our setup and compared with our method in Fig.",2,positive
"As follows, we will first describe the experiment results and then state the adaptation of FixMatch to anomaly detection we made.",2,positive
"…results of ten shallow and deep anomaly detection methods (Qiu et al., 2022a; Ruff et al., 2018; Deecke et al., 2018; Golan and El-Yaniv, 2018; Hendrycks et al., 2019; Tax and Duin, 2004; Liu et al., 2008; Diederik P. Kingma, 2014; Makhzani and Frey, 2015; Sohn et al., 2020b) on the CIFAR10 one-vs.",2,positive
"To mitigate this problem, many semisupervised techniques [1, 26] are proposed to exploit large amounts of unlabeled data by automatically generating pseudo labels without introducing manual annotation.",1,neutral
"Semi-supervised object detection, inherited from the semi-supervised image classification methods [1, 25, 26, 30, 34], is divided into consistency-based schemes [13, 28] and pseudo-labeling schemes [21, 27, 35, 40].",1,neutral
"As a general model optimization strategy, consistency training has been widely spread in the semi-supervised learning for classification [69], [70], [71], [72], with the assumption that the model should output similar predictions when fed different perturbed versions of the same image.",1,neutral
"Based on the consistency regularization (Bachman, Alsharif, and Precup 2014; Sohn et al. 2020), which holds that the model should output similar predictions when fed augmented versions of the same image, we regard the memory feature mi as an augmentation of v A i , proposing to achieve in-domain self-matching via the loss function:",2,positive
"Based on the consistency regularization (Bachman, Alsharif, and Precup 2014; Sohn et al. 2020), which holds that the model should output similar predictions when fed augmented versions of the same image, we regard the memory feature mAi as an augmentation of v A i , proposing to achieve in-domain…",2,positive
"UDA, ReMixMatch, and FixMatch adopt cross-entropy loss and robust augmentations [45]–[47].",1,neutral
"Semi-supervised Learning Our approach is also closely related to the semi-supervised approaches like FixMatch (Sohn et al., 2020) and PAWS (Assran et al., 2021) that learn encoders by matching student-teacher distributions.",2,positive
"Our self-supervised approach is based on the success of the semi-supervised learning algorithm FixMatch (Sohn et al., 2020).",2,positive
"L G
] 2
2 Fe
b 20
23
propose a novel method for self-supervised learning called Q-Match which is closely related to the semi-supervised learning framework called FixMatch (Sohn et al., 2020).",1,neutral
"Semi-supervised Learning Our approach is also closely related to the semi-supervised approaches like FixMatch (Sohn et al., 2020) and PAWS (Assran et al.",1,neutral
"propose a novel method for self-supervised learning called Q-Match which is closely related to the semi-supervised learning framework called FixMatch (Sohn et al., 2020).",1,neutral
"(3) PEACE w C utilizes the confidence to measure the uncertainty following existing works [55], [80].",1,neutral
"Following the recent semi-supervised techniques [55], we first calculate the confidence of each sample, which is the largest probability among the distribution:",1,neutral
"For the exact implementation of RandAugment, we directly use the implementation of Sohn et al. (2020).",2,positive
"We include the following three algorithms: FixMatch (Sohn et al., 2020), Noisy Student (Xie et al.",2,positive
"Consequently, benchmark-driven research has produced a variety of heuristic methods (Ganin et al., 2016; Sohn et al., 2020; Wang et al., 2021; Li et al., 2016) that despite yielding gains in benchmark performance tend to break when ppyq shifts.",2,positive
"L G
] 5
J un
2 02
3
driven research has produced a variety of heuristic methods (Ganin et al., 2016; Sohn et al., 2020; Wang et al., 2021; Li et al., 2016) that despite yielding gains in benchmark performance tend to break when ppyq shifts.",2,positive
"We include the following three algorithms: FixMatch (Sohn et al., 2020), Noisy Student (Xie et al., 2020a), Selective Entropy Optimization via Committee Consistency (SENTRY (Prabhu et al., 2021)).",2,positive
"While Sohn et al. (2020) augments labeled examples with weak augmentation, Sagawa et al. (2021) proposed to strongly augment the labeled source examples.",1,neutral
"FixMatch Sohn et al. (2020) proposed FixMatch as a variant of the simpler Pseudo-label method (Lee et al., 2013).",2,positive
"We fix the step at which λt reaches its maximum value λ be 40% of the total number of training steps, matching the implementation to (Sohn et al., 2020; Sagawa et al., 2021).",2,positive
"More recently, a massive literature has emerged exploring a benchmark-driven heuristic approach (Long et al., 2015; 2017; Sun & Saenko, 2016; Sun et al., 2017; Zhang et al., 2019; 2018; Ganin et al., 2016; Sohn et al., 2020).",2,positive
We adapted our implementation from Sagawa et al. (2021) which matches the implementation of Sohn et al. (2020) except for one detail.,2,positive
"However, this pseudo-label training approach naturally works with incremental learning algorithms like neural networks, where the model is gradually learned through optimizing an objective function (Lee et al., 2013; Berthelot et al., 2019; Zoph et al., 2020; Xie et al., 2020; Sohn et al., 2020).",1,neutral
"The consistency regularization can be enforced via pseudo-labels [2, 37] or prediction distribution alignment [20, 42, 44, 46].",1,neutral
"The different views can be created by strong and weak data augmentation techniques on the input image [2, 37, 42, 46], or the Mixup operation [2, 44].",1,neutral
"Here, approaches have explored incorporating a hierarchical loss on the coarse labels [2] or using the coarse labels to filter pseudo-labels [13] in addition to the standard semi-supervised learning techniques of consistency regularization [33], pseudo-labeling [34, 35], and contrastive learning [14].",1,neutral
"Researchers train deep learners on a small subset of labeled data, and then produce pseudo labels on unlabeled data [Sohn et al., 2020].",1,neutral
"The success of FixMatch inspired several subsequent methods [8], [9], [10], [15], [16], [17].",1,neutral
Deterministic Methods FixMatch [7] 43.,1,neutral
"Current SOTA methods for the semi-supervised image classification task are deterministic, including FixMatch [7], CoMatch [15], and FlexMatch [8], which have achieved promising results on public benchmarks.",2,positive
"FixMatch [7] combines the merits of these two approaches: given an unlabeled image, weak data augmentation and strong data augmentation are performed on the image, leading to two versions of the image, and then FixMatch produces a pseudo-label based on its weaklyaugmented version and a preset confidence threshold, which is used as the true label for its strongly augmented version to train the whole framework.",2,positive
"During training, we followed previous work [7], [8], [9], [15] to utilize the exponential moving average (EMA) technique.",2,positive
"They can be further classified into two categories, namely, deterministic [7], [8], [10], [15], [16], [17] and probabilistic ones [9].",1,neutral
"Most recent approaches to SSL for image classification are based on the combination of consistency regularization and pseudo-labeling [7], [8], [9], [10], [15], [16], [17].",1,neutral
"We use a batch of 16 samples and two network architectures that are widely used in previous works [7], [8], [15], [95], namely, WRN-28-2 on CIFAR-10 (Figure 3 (a)) and WRN-28-8 on CIFAR-100 (Figure 3 (b)).",2,positive
"We evaluated NP-match on these two datasets following the evaluation settings used in previous works [7], [8], [15].",2,positive
"Based on these two approaches, FixMatch [7] is proposed, which achieves new state-of-the-art (SOTA) results on the most commonlystudied SSL benchmarks.",2,positive
"SSL aims to learn from few labeled data and a large amount of unlabeled data, and it has been a long-standing problem in computer vision and machine learning [7], [8], [9], [10], [11], [12], [13], [14].",1,neutral
FixMatch [28] learns target unlabeled data ui by generating pseudo-labels from models’ predictions on weakly-augmented data A(ui )) and filtering only high confidence data by selecting the prediction that has confidence more than the threshold.,1,neutral
"UDA [37], FixMatch [28], MixMatch [3], and RemixMatch [2] utilize consistency training without the actual labeled data.",2,positive
"5, ADDA [32], CDAN [21], and FixMatch [28] along with ""Supervised Learning"" which train only on source label data without domain adaptation and lastly, our proposed method, GaitSADA.",2,positive
"However, with the confidence threshold, FixMatch [28] suggests that the pseudo label produces a natural curriculum learning.",2,positive
"We repropose FixMatch [28], a semi-supervised learning method, for our unsupervised domain adaptation problem.",2,positive
"5 Evaluation We evaluate themodel without any domain adaptation (supervised learning) as a baseline comparing with the five representative domain adaptation methods including GAN [16], GRL [10], ADDA [32], CDAN [21], and FixMatch [28] along with our method, GaitSADA, on the four different locations (laboratory, conference room, server room, and office) to explore the spatial domain adaptation using 1-3 days of data for training and the rest of data for testing.",2,positive
"hybrid methods on benchmarks [21], [25], we include the three SOTA methods in the comparison.",2,positive
"Considering
that Mean Teacher outperforms other consistency regularization methods [19], and that FixMatch and SimPLE outperform other hybrid methods on benchmarks [21], [25], we include the three SOTA methods in the comparison.",2,positive
"Semi-supervised deep learning is a fast-growing field, mainly including deep generative methods [15], [16], pseudolabeling methods[17], [18], consistency regularization methods [19], [20], hybrid methods [21], [22], etc.",1,neutral
"Third, both the supervised TBN and SS-TBN outperform the Mean Teacher, FixMatch, and SimPLE, suggesting that the performance improvement benefits from both the tri-branch architecture and novel hybrid semi-supervised learning strategy specifically tailored to special characteristics of the application problem.",2,positive
"from the aforementioned methods [14], such as MixMatch and its extension [22], [43], FixMatch [21], and SimPLE [25].",1,neutral
"[21] K. Sohn et al., “FixMatch: Simplifying semi-supervised learning with consistency and confidence,” in Proc.",1,neutral
"Second, to test the requirement of labeled sample for efficient training of the proposed SS-TBN, we just use 100 manually annotated image slices for training, and the results show that our method achieves better performance compared to Mean Teacher [19], FixMatch [21], and ResNet-50 with single semi-supervised learning strategies, maintaining high classification accuracies (>92",2,positive
"FixMatch combines pseudo labeling and consistency regularization, outperforming the aforementioned consistency regularization and hybrid methods across a variety of standard benchmarks, but consistency regularization in FixMatch is just used to improve confidence of pseudo labels [21].",2,positive
"outperforming the aforementioned consistency regularization and hybrid methods across a variety of standard benchmarks, but consistency regularization in FixMatch is just used to improve confidence of pseudo labels [21].",1,neutral
"Hybrid methods can boost performance by combining ideas from the aforementioned methods [14], such as MixMatch and its extension [22], [43], FixMatch [21], and SimPLE [25].",2,positive
"Second, to test the requirement of labeled sample for efficient training of the proposed SS-TBN, we just use 100 manually annotated image slices for training, and the results show that our method achieves better performance compared to Mean Teacher [19], FixMatch [21], and ResNet-50 with single semi-supervised learning strategies, maintaining high classification accuracies (>92%) with an extremely small training sample.",2,positive
"For example, in the case of images, recent methods like MixMatch, FixMatch and Mean Teacher [1] [12] [13] have proposed data augmentation techniques which rely on simple pre-defined transformations such as cropping, resizing, etc.",1,neutral
"Self-supervised tasks on unlabeled data can serve as auxiliary tasks to improve the performance of the target task in computer vision and natural language processing, without requiring additional labeled data [31, 66, 10, 3].",1,neutral
"Among the efforts, pseudo-labeling (Lee et al., 2013; Arazo et al., 2020) with confidence thresholding (Xie et al., 2020; Sohn et al., 2020; Xu et al., 2021b; Zhang et al., 2021) is highly-successful and widely-adopted.",2,positive
"4 QUALITATIVE ANALYSIS In this section, we provide a qualitative comparison on CIFAR-10 with 250 labels of FixMatch (Sohn et al., 2020), FlexMatch (Zhang et al.",2,positive
"In confidence thresholding (Arazo et al., 2020; Sohn et al., 2020; Xie et al., 2020), we can view the sample weights as being computed from a step function with confidence max(p) as the input and a pre-defined threshold τ as the breakpoint.",1,neutral
"We compare SoftMatch with two strong baselines: FixMatch (Sohn et al., 2020) and FlexMatch (Zhang et al.",2,positive
"FixMatch (Sohn et al., 2020) uses a fixed threshold to select pseudo-labels with high quality, which limits the data utilization ratio and leads to imbalanced pseudo-label distribution.",2,positive
"On the one hand, a high confidence threshold as exploited in FixMatch (Sohn et al., 2020) ensures the quality of the pseudo-labels.",2,positive
", 2020) with confidence thresholding (Xie et al., 2020; Sohn et al., 2020; Xu et al., 2021b; Zhang et al., 2021) is highly-successful and widely-adopted.",2,positive
"The core idea of threshold-based pseudo-labeling (Xie et al., 2020; Sohn et al., 2020; Xu et al., 2021b; Zhang et al., 2021) is to train the model with pseudo-label whose prediction confidence is above a hard threshold, with the others being simply ignored.",2,positive
"In most of the previous methods (Tarvainen & Valpola, 2017; Berthelot et al., 2019b;a; Sohn et al., 2020; Zhang et al., 2021; Xu et al., 2021b), although they do not explicitly set λ(p), the introduction of loss weight schemes implicitly relates to the PMF of p.",1,neutral
"…amount of unlabeled data, has shown great potential in practical applications for significantly reduced requirements on laborious annotations (Fan et al., 2021; Xie et al., 2020; Sohn et al., 2020; Pham et al., 2021; Zhang et al., 2021; Xu et al., 2021b;a; Chen et al., 2021; Oliver et al., 2018).",2,positive
"…al., 2018; Berthelot et al., 2019b;a; Xie et al., 2020; Cubuk et al., 2020; Sajjadi et al., 2016), label allocation (Tai et al., 2021), feature consistency (Li et al., 2021; Zheng et al., 2022; Fan et al., 2021), and confidence thresholding (Sohn et al., 2020; Zhang et al., 2021; Xu et al., 2021b).",0,negative
"Confidence thresholding introduces a filtering mechanism, where the unlabeled data whose prediction confidence max(p) is above the pre-defined threshold τ is fully enrolled during training, and others being ignored (Xie et al., 2020; Sohn et al., 2020).",1,neutral
"1 INTRODUCTION Semi-Supervised Learning (SSL), concerned with learning from a few labeled data and a large amount of unlabeled data, has shown great potential in practical applications for significantly reduced requirements on laborious annotations (Fan et al., 2021; Xie et al., 2020; Sohn et al., 2020; Pham et al., 2021; Zhang et al., 2021; Xu et al., 2021b;a; Chen et al., 2021; Oliver et al., 2018).",2,positive
"We compare SoftMatch with two strong baselines: FixMatch (Sohn et al., 2020) and FlexMatch (Zhang et al., 2021).",2,positive
"Confidence thresholding methods (Sohn et al., 2020; Xie et al., 2020; Zhang et al., 2021; Xu et al., 2021b) adopt a threshold to enroll the unlabeled samples with high confidence into training.",0,negative
"We fine-tune the pre-trained BERT-Base (Devlin et al., 2018) model for all datasets using UDA (Xie et al., 2020), FixMatch (Sohn et al., 2020), FlexMatch (Zhang et al., 2021), and SoftMatch.",2,positive
"Although we can set the parameters to fixed values as in FixMatch (Sohn et al., 2020) or linearly interpolate them within some pre-defined range as in Ramp-up (Tarvainen & Valpola, 2017), this might again oversimplify the PMF assumption as discussed before.",2,positive
"(1)
For the unsupervised loss, most existing methods with pseudo-labeling (Lee et al., 2013; Arazo et al., 2020; Xie et al., 2020; Sohn et al., 2020; Xu et al., 2021b; Zhang et al., 2021) exploit a confidence thresholding mechanism to mask out the unconfident and possibly incorrect pseudo-labels…",1,neutral
", 2020), FixMatch (Sohn et al., 2020), FlexMatch (Zhang et al.",2,positive
"For the unsupervised loss, most existing methods with pseudo-labeling (Lee et al., 2013; Arazo et al., 2020; Xie et al., 2020; Sohn et al., 2020; Xu et al., 2021b; Zhang et al., 2021) exploit a confidence thresholding mechanism to mask out the unconfident and possibly incorrect pseudo-labels from training.",0,negative
"In this section, we provide a qualitative comparison on CIFAR-10 with 250 labels of FixMatch (Sohn et al., 2020), FlexMatch (Zhang et al., 2021), and SoftMatch from different aspects, as shown in Fig.",2,positive
"Recent deep learning based SSL methods can be divided into four main categories: (1) pseudo-label generation [41], (2) consistency regularization [50], [62], (3) combination of pseudo-labeling with consistency regularization [7], [6], [61], [38], and (4) generative model based methods [14], [18].",1,neutral
"FixMatch [46] first generated a pseudo label using the model’s predictions on weakly-augmented unlabeled images, and then putted a strongly-augmented version of the same image into the model for self-training, aiming to achieve consistency regularization.",2,positive
"001); data transformation: (RandAugmentMC weak, Standard) Semi-SL For our Semi-SL models we follow [47] with regard to HP selection as closely as possible.",2,positive
For Semi-SL methods we fix all HPs with the exception of learning rate and weight decay noted as crucial in [47].,1,neutral
"For a more thorough overview over Semi-SL we refer interested readers to [4, 40] In our experiments we use FixMatch [47] as Semi-SL method which combines both aforementioned principles of consistency and uncertainty reduction in a simple manner.",2,positive
"Semi-SL Semi-SL training is identical to the one proposed with the FixMatch method [47], except that we do not use exponentially moving average models and restrict the training step from 1e6 to 2e5.",2,positive
"Specifically, alternative training paradigms SelfSL [8, 18] and Semi-SL [32, 47] have shown strong potential to make efficient use of an unlabeled data pool in a classification task thus alleviating the labeling burden.",1,neutral
A detailed list of image transformations alongside the corresponding values can be seen in [47] (Table 12).,1,neutral
On imbalanced datasets we change the supervised term to the weighted CE-Loss and use distribution alignment on every dataset except for CIFAR-10 (where it does not improve performance [47]).,2,positive
"[47] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li.",0,negative
"And as Semi-SL method, we use FixMatch [47] which combines the principles of consistency and uncertainty reduction in a simple manner.",1,neutral
"Recently, frameworks such as FixMatch [30] and UDA [35] introduced a new paradigm for semi-supervised learning combining pseudo-labeling with consistency regularization using data augmentations.",1,neutral
"On the contrary, the seminal SSL method FixMatch [30] outperforms all evaluated OSSL methods in terms of closed-set accuracy.",1,neutral
"For comparison, we include the widely used closedset SSL method FixMatch [30] and a fully supervised baseline trained using only the labeled subset.",2,positive
Worth noting is that lp is equivalent to the pseudolabeling loss in FixMatch [30] with the exception that FixMatch selects pseudo-labels based on thresholding the maximum value of the predicted softmax distributions.,1,neutral
"Using a combination of labeled and unlabeled data for training a model, so-called semi-supervised learning (SSL), has been a well-studied field of machine learning for a long time [29, 39, 19, 32, 30].",1,neutral
"For samples confidently predicted as ID, we apply a pseudo-labeling loss similar to those of FixMatch [30] and UDA [35].",2,positive
"The student model at each client is updated by optimizing labeled and unlabeled losses through FixMatch (Sohn et al., 2020) and an additional FedProx (Li et al.",2,positive
"We demonstrate that FedSwitch achieves state-ofthe-art generalization performance on the CIFAR-10 dataset and improves on existing approaches such as FedMatch (Jeong et al., 2021), FedRGD (Zhang et al., 2021) and FedProx-FixMatch (Sohn et al., 2020).",2,positive
"The student model at each client is updated by optimizing labeled and unlabeled losses through FixMatch (Sohn et al., 2020) and an additional FedProx (Li et al., 2020) loss term.",2,positive
"Unlike the FixMatch method (Sohn et al., 2020), where a single model both generates pseudo-labels and trains on them, in the teacher-student framework with EMA, we maintain two separate models, where θt and θs are the learnable parameters of the teacher and the student model respectively.",2,positive
"Unlike the FixMatch method (Sohn et al., 2020), where a single model both generates pseudo-labels and trains on them, in the teacher-student framework with EMA, we maintain two separate models, where θt and θs are the learnable parameters of the teacher
and the student model respectively.",2,positive
"Semi-Supervised Learning [37], [38], [39], [40], [41] is to promote the supervised learning performance with unlabeled samples.",1,neutral
"Diverse methods focus on semi-supervised learning, which can be divided into consistency-regularization methods [38], [40], [22] and pseudo-labeling [39], [42].",1,neutral
"FixMatch [40] generates the pseudo-label for a weakly-augmented unlabeled image first, then the model is trained to predict the pseudolabel of a strongly-augmented version of the same image.",1,neutral
"images is helpful for improving the generalization and robustness of the model [46], [47].",1,neutral
"Pseudo Labeling [27], which converts model predictions on unlabeled data to one-hot labels, is a widely-used technique [3, 39, 50] in semi-supervised learning.",1,neutral
"On the basis of MixMatch, Fixmatch [39] further simplify the learning process while FlexMatch [50] introduces a class-wise confidence threshold to boost model performance.",2,positive
"Previous research work alleviates this problem by filtering out predictions that are lower than a threshold of classification scores [3, 39, 50].",1,neutral
FixMatch [2] expresses consistency through the strongly augmented sample and weakly augmented sample between the same image samples.,1,neutral
"Of the above work, our approach is similar to [2, 18, 19, 42].",1,neutral
FixMatch [2] simplified SSL and obtained better classification performance by combining consistency regularization with pseudo-labeling.,1,neutral
"It can be seen that FixMatch [2] performs poorly with only 5% of labeled samples, while our method has a large performance gain with fewer labeled samples, especially the most significant gain in the F1 metric.",2,positive
"1 3 as a part of their pipeline to produce better results [2, 18, 19].",0,negative
"[36] SOHN K, BERTHELOT D, CARLINI N, et al.",1,neutral
"95[36]: L 1 max p τ ⋅ CE fΘ u , ?̂? (5) 通过公式 4 所示的学习目标优化,本文所提方法 EdgeHML 能 够综合利用在外界环境变化时遇到的新数据以及层级化数据记",0,negative
"数据集。根据半监督方法以及持续学习方法的相关研究工 作 [10,32,36,40],我们采用不同规模的 CIFAR-10 数据集 [37]、 CIFAR-100 数据集[37]以及 TinyImageNet 数据集[38]构造半监督 持续学习任务,持续学习的设定为任务增量。具体包括:(1) CIFAR-10:完整的 CIFAR-10 数据集包括飞机、汽车、鸟、猫 等 10 个物体类别,我们将 CIFAR-10 按照类别划分为 5 个任务, 即每个任务包括 2 个类别,不同任务所包含的类别互不重叠。 在每个任务的每个类别上,我们仅使用 5 个样本及对应的标注 作为有标注数据,而包括这 5 个样本在内的所有其余样本将作 为无标注数据,构建半监督持续学习任务 Split Semi CIFAR-105;(2)CIFAR-100:CIFAR-100 数据集包含 100 个物体类别, 我们按照类别将其划分为 20 个任务,每个任务包含 5 个不同 的类别。类似地,我们在每个类别上使用 5 个样本作为有标注 样本,其余所有样本(包含这 5 个)作为无标注样本,构建半 监督持续学习任务 Split Semi CIFAR-100-5 ;( 3 ) TinyImageNet:TinyImageNet 数据集是 ImageNet 数据集[39]的变 体,包含 200 个物体类别,根据前人研究工作 [40],本文将 TinyImageNet 数据集中的图像缩放到 64*64 像素,按照类别划 分为 10 个任务,每个任务包含 20 个类别。每个类别的样本中 包含 5 个有标注样本作为有标注数据,包含这 5 个样本的所有 其余样本作为无标注数据,构建半监督持续学习任务 Split Semi TinyImageNet-5。 基准方法。我们从朴素训练方法、持续学习方法以及半监",0,negative
"3 不同标注数量与池容量条件下结果及分析 在上述实验的基础上,为了更进一步地验证 EdgeHML 在 不同环境设定下的性能表现,我们额外设置了两组实验:(1) 不同的标注数量:参考前人工作[11,36,41]中的实验设置,我们分 别对每个类别提供 5 个、25 个以及 100 个样本作为有标注样本, 其余样本作为无标注样本,构造 Split Semi CIFAR-100-5、Split Semi CIFAR-100-25 以及 Split Semi CIFAR-100-100 三个半监督",0,negative
"We display the results of FixMatch (Sohn et al., 2020) and MixMatch (Berthelot et al., 2019).",2,positive
", 2019), holistic methods (Berthelot et al., 2019; Sohn et al., 2020), and generative methods (Kingma et al.",1,neutral
"Other representative SSL methods include consistency regularization (Laine & Aila, 2017; Tarvainen & Valpola, 2017; Miyato et al., 2019), holistic methods (Berthelot et al., 2019; Sohn et al., 2020), and generative methods (Kingma et al., 2014).",1,neutral
"It is noticeable that DS3L underperforms supervised baseline when all the unlabeled data are drawn
3We note that there are also other methods like FixMatch (Sohn et al., 2020) and MixMatch (Berthelot et al., 2019) focus on augmentation or other tricks.",2,positive
"It is noticeable that DS3L underperforms supervised baseline when all the unlabeled data are drawn 3We note that there are also other methods like FixMatch (Sohn et al., 2020) and MixMatch (Berthelot et al.",2,positive
supervision strategy in line with previous works [3] [7] [8] in semi-supervised learning.,1,neutral
"[7] proposed to threshold the model predictions and only preserve those with high confidence, which reduces the false label ratio.",1,neutral
"Pseudo supervision is vulnerable to label noise, thus thresholding the pseudo label with high confidence is a common method [7] to reduce label noise.",1,neutral
"These methods may benefit from pseudo-label confidence measures [Sohn et al., 2020; Rizve et al., 2021; Zhang et al., 2021] as well as thresholding [Xu et al.",2,positive
", 2013] or jointly in an online fashion [Berthelot et al., 2019; 2020; Sohn et al., 2020].",2,positive
"Pseudo-labels can either be generated prior to a subsequent supervised learning phase [Yarowsky, 1995; Riloff, 1996; Lee et al., 2013] or jointly in an online fashion [Berthelot et al., 2019; 2020; Sohn et al., 2020].",1,neutral
"These methods may benefit from pseudo-label confidence measures [Sohn et al., 2020; Rizve et al., 2021; Zhang et al., 2021] as well as thresholding [Xu et al., 2021], temporal ensembling [Laine & Aila, 2017], or stronger regularization to mitigate bias in early model training [Sajjadi et al., 2016;…",2,positive
"UDA (Xie et al., 2020a) and FixMatch (Sohn et al., 2020) are two examples of recent brilliant works in semi-supervised learning.",1,neutral
", 2021) mainly changed GD setting through defining some policies over unlabeled data by using superclasses of the CIFAR100 and using the FixMatch method (Sohn et al., 2020).",2,positive
"DM (Smith et al., 2021) mainly changed GD setting through defining some policies over unlabeled data by using superclasses of the CIFAR100 and using the FixMatch method (Sohn et al., 2020).",2,positive
"In the FixMatch method (Sohn et al., 2020), consistency regularization and pseudo-labeling are combined, and cross-entropy loss is used to calculate both supervised and unsupervised losses.",1,neutral
"In [Sohn et al., 2020], new methods were proposed that utilize high-confidence pseudolabels generated with weakly-augmented images.",1,neutral
"Given an unlabeled set U , a common practice in semisupervised learning literature [110, 104, 77] is to adopt the Pseudo Labeling technique [45], which assumes that the decision boundary usually lies in low-density areas and data samples in a high-density area have the same label.",1,neutral
"Normal Benign in situ carcinoma Invasive carcinoma
Model Accuracy P R P R P R P R
Supervised VGG (9) 56.3 ( ± 10.9) 60.0 60.0 61.1 55.0 60.0 60.0 62.5 45.5
Inception (26) 71.3 ( ± 9.9) 68.4 65.0 71.4 75.0 68.1 75.0 66.7 77.8
ResNet101 (44) 86.3 ( ± 7.5) 89.4 85.0 81.8 90.0 80.9 85.0 94.4 85.0
Semi-supervised Pseudo-Labeling (30) 61.3 ( ± 10.7) 57.9 55.0 61.9 65.0 52.4 55.0 73.7 70.0
Mean Teacher (48) 70.0 ( ± 10.0) 66.7 70.0 68.4 65.0 70.0 70.0 75.0 75.0
MixMatch (49) 87.5 ( ± 7.2) 90.0 90.0 80.9 85.0 85.0 85.0 94.7 90.0
FixMatch (14) 87.5 ( ± 7.2) 94.4 85.0 85.0 85.0 81.8 90.0 90.0 90.0
Semi-His-Net 90.0 (± 6.6) 90.0 90.0 89.4 85.0 85.7 90.0 95.0 95.0
We show the average Accuracy with a 95% confidence interval in parentheses, Precision (P) and recall (R) (",0,negative
Our model draws on the strategy of generating artificial labels via consistency regularization and pseudo-labeling in FixMatch (14).,1,neutral
"Taking the fully supervised ResNet101 as the baseline, we could find that the performance of the model with the semi-supervised strategy, MixMatch, FixMatch and the proposed Semi-His-Net has been significantly improved, with the accuracy increased by 1.2%, 1.2% and 3.7% respectively.",2,positive
"Inspired by this novel SSL mechanism, we propose extending FixMatch (14) to WSIs analysis.",2,positive
"The latest semi-supervised methods (14, 34) incorporate previous research: consistency regulation, pseudo-label or label sharpening, entropy minimization, and other DA strategies, and the performance has also been significantly improved.",2,positive
(xiii) FixMatch: Fixmatch [130] is a method for improving the performance of semi-supervised learning (SSL).,1,neutral
"This image shows the procedure of FixMatch, image is taken from [130].",1,neutral
"Recent state-of-the-art consistency learning methods like MixMatch [3], UDA [79] and FixMatch [59] introduce strong data augmentations [79] to the learning paradigm—they use predictions on weakly augmented images as the target to train the model to produce similar outputs given the strongly augmented views of the same images.",2,positive
"Recent state-of-the-art consistency learning methods like MixMatch [3], UDA [79] and FixMatch [59] in-",2,positive
"Semi-Supervised Learning (SSL) Numerous SSL methods are based on consistency learning [56, 3, 4, 59, 90, 81], which forces a model’s predictions on two",1,neutral
"The teacher model can be either a pretrained model [59] or an exponential moving average of the student model [49, 29, 66, 43].",1,neutral
"Some studies [1] have also demonstrated that using the student model being trained to produce the target can reach decent performance—the trick is to inject strong noise to the student model, such as applying strong data augmentations to the input [59].",1,neutral
"Such a combination of pseudo-labeling and confidence thresholdsbased filtering has been largely inspired by research on semi-supervised image classification [3, 79, 59, 53]).",1,neutral
"Low-Confidence Consistency Regularization In order to prevent the network from unreliable and noisy pseudo labels for unlabeled data, some efforts [2], [16] have been put to boost the stability of training and reduce negative impacts of incorrectly classified samples.",2,positive
"Specifically, we build on FixMatch[14] framework except replace all CNNs with ViTs.",2,positive
"Following [14], we sample 10% labeled images from the ImageNet training set and leave the rest as unlabeled data.",2,positive
Base architecture FixMatch[14] is one of the most popular SSL frameworks in recent times.,2,positive
"FixMatch[14] integrated these two strategies: on unlabeled data, hard pseudo labels are generated with weak augmentation as the target, and the model is fed a strongly-augmented version of the same image.",2,positive
"As discussed in [13], using FixMatch[14], one of the most popular SSL approaches, to train a ViT presents an inferior performance than CNN architectures.",2,positive
"MAE branch As figured out in [13], the performance of ViT in SSL building on FixMatch[14] is inferior to CNN counterparts.",2,positive
"In this section, we will first review FixMatch[14] algorithm and then elaborate proposed MAE branch.",2,positive
"We demonstrate that pure ViTs can outperform CNN-based[14, 16, 17] and joint[13] SSL frameworks.",2,positive
"Building on FixMatch[14], Semi-MAE introduces a masked autoencoder (MAE) branch to assist the encoder’s visual representation learning.",2,positive
"We can divide the SSL in two approaches: consistency-based [4, 7, 33, 34] and contrastive energy-based [8, 10, 16, 19, 25, 41].",1,neutral
", BadNets and DeNeB, pseudo-labeling based SSL algorithms [1, 2, 15, 19, 29, 35] strive to assign correct labels to unlabeled examples, while the poisoned unlabeled examples coming from different classes expect themselves to be misclassified into the target class.",1,neutral
"On SVHN, baseline poisonings fail on all SSL algorithms except FixMatch which has the ASR of 75.47%.",2,positive
"This is because F s can be trained by semi-supervised learning [29, 35] or unsupervised learning [5, 9], which is beyond our research scope.",2,positive
"5.1.2 SSL algorithms: We select some representative SSL algorithms from three types introduced in Section 2.3: consistency regularization is PI-Model [30], MeanTeacher [38], VAT [25], and ICT [41], pseudo-labeling is PseudoLabel [19], pseudo-labeling with
consistency regularization is FixMatch [35].",2,positive
"Existing SSL algorithms can be categorized into three types: consistency regularization [17, 25, 30, 38, 41, 44], pseudo-labeling [15, 19, 29], and pseudo-labeling with consistency regularization [1, 2, 35].",1,neutral
"On FixMatch, our poisoning performs the best, while on VAT, the ASR is the lowest.",2,positive
"FixMatch [35] performs weak augmentation and strong augmentation for each unlabeled example, and the predicted label of weak augmentation is used as the pseudo label of strong augmentation.",1,neutral
"3: consistency regularization is PI-Model [30], MeanTeacher [38], VAT [25], and ICT [41], pseudo-labeling is PseudoLabel [19], pseudo-labeling with consistency regularization is FixMatch [35].",1,neutral
"Likewise, when consistency regularization based SSL algorithms [1, 2, 17, 25, 30, 35, 38, 41, 44] are employed, the noises or augmentations the SSL algorithms add to the examples or models will make the models to unlearn backdoor patterns but to focus on the semantic information of the poisoned unlabeled examples.",1,neutral
"To focus on the impact of varying hyperparameters or situations on poisoning performance, the evaluations in this section are performed on CIFAR10 trained with FixMatch.",0,negative
It is simple yet effective and can be easily incorporated into the existing SSL methods.,2,positive
"Recent attempts have exploited a holistic method [42, 54, 6] that combines consistency regularization and self-training.",1,neutral
"Therefore, by applying the PGCL to previous SSL approaches, we aim to demonstrate that it can effectively improve the segmentation performance.",2,positive
"To address this problem, semi-supervised learning (SSL) [27, 44, 42, 46, 47] has attracted attention for semantic segmentation, in which it is assumed that only a fraction of the entire dataset is labeled.",1,neutral
"Image Classification The problem of SSL in computer vision was historically tackled first for the image classification task, with significant progress made using deep neural networks [28, 32, 24, 2, 29].",1,neutral
"Following recent trends [29, 7], our work takes inspiration from both groups of methods adapted to OD, by training a student model to match the predicted probability distributions of proposals made by a teacher model.",2,positive
"A popular type of approach in this field uses pseudo-labeling [17, 2, 1, 29], by generating pseudo-labels from class predictions for unlabeled data, either offline [17] or online [2, 29], and then training on a mix of ground truth and pseudo-labels.",1,neutral
"Future works could push the data scarcity in OD even further to consider very few labeled examples for each class, and better understand how to match the performance of SSL methods for image classification in this setting [29].",2,positive
"As shown in Figure 2, our approach is composed of a student-teacher architecture, which is common for semisupervised learning [32, 29].",1,neutral
"This type of SSL-based classification methods are generally divided into several main classes in terms of the use of unlabelled data: consistency regularization (Verma et al. 2022), pseudo labelling (Arazo et al. 2020), generic regularization and their combinations (Sohn et al. 2020).",1,neutral
"2020), generic regularization and their combinations (Sohn et al. 2020).",1,neutral
"It is trained with depth image marked with desired grasping point-pair using a consistency based self-training method [19], [20].",1,neutral
"It has achieved remarkable results in computer vision by introducing various techniques, including entropy minimization [11, 23], Mean Teacher [41, 50], MixMatch [4, 3, 40], consistency regularization [1, 38, 22, 49], and label propagation [16].",1,neutral
"We use strong augmentations based on RandAugment [20], but adjust some parameters and delete color augmentation to fit the ultrasound data.",2,positive
"The weak and strong data augmentations are important for the semi-supervised learning, and have been widely used in SSL method ([20][41]).",1,neutral
The semi-supervised leaning (SSL) shows potential for improving network performance when labeled data is scarce by using pseudo-labeling and consistency regularization to utilize unlabeled data ([19];[20]).,1,neutral
"FixMatch ([20]) generated pseudo label on weakly-augmented unlabeled images, and the model was trained to predict the pseudo label when being fed a strongly-augmented version of the same image.",1,neutral
"Most previous work simply set a fixed threshold to filter samples with low confidence (Sohn et al., 2020; Xie et al., 2020).",1,neutral
"Consistency regularization[1], [10], [14] obtains an artificial label using the model’s predicted distribution after randomly modifying the input or model function[15].",1,neutral
"For example, [15], [13], [19] have proved to be powerful SSL algorithms with superior performance than supervised learning.",1,neutral
"CTS model and some other main-stream SSL methods,including Π Model[11], Mean Teacher[18], Mixmatch[13], Fixmatch[15], Flexmatch[21], are evaluated and test.",2,positive
"Inspired by previous works in self-training (Xie et al., 2020; Sohn et al., 2020; Xu et al., 2021), we use strong data augmentation during Phase-II to counteract the noise in pseudo boxes and further boost the performance of GOOD.",2,positive
"To avoid collapsing, we balance the ratio of labeled and unlabeled data in one batch to 1:7 similarly to [55].",1,neutral
"These are notoriously challenging to obtain [20, 54] compared to other types of annotation, such as image class labels [55] and segmentation labels [75].",1,neutral
"Several SSL approaches [9, 14, 22, 23, 27] for 2D object detection are developed based on teacher-student mutual learning, where pseudo-labels of unlabeled data are estimated and used as supervisory signals for detector training.",1,neutral
"2) Teacher-student framework: It often employs two identical networks, one for a teacher model and the other for a student model [22, 27].",1,neutral
Fixmatch [22] combines the teacher-student framework and pseudo-labeling.,2,positive
"To evaluate the effect of different SSL methods in the proposed pipeline, we implement two other SSL methods, Mean Teacher (Tarvainen and Valpola 2017) and FixMatch (Sohn et al. 2020), as a substitute for MixMatch algorithm and compare their performance in the proposed pipeline.",2,positive
"Hence, the performance of the FixMatch is not as good as the MixMatch.",2,positive
"As part of the unsupervised loss function, FixMatch algorithm needs weak and strong augmentations applied to the input data.",1,neutral
The FixMatch algorithm (Sohn et al. 2020) generates weak and strong augmentations for each unlabelled input; the prediction of weak augmented input is then used as a target when predicting strongly augmented inputs along with a cross-entropy loss function.,1,neutral
Most recent SSL methods use pseudo-labelling (Lee et al. 2013; Sohn et al. 2020) and consistency regularisation (Tarvainen and Valpola 2017; Berthelot et al. 2019; Sohn et al. 2020) to hypothesise labels for unlabelled samples.,1,neutral
"For weak and strong augmentation in the FixMatch and Mean Teacher algorithms, we used the same augmentation sets but with a higher rate of change for strong ones.",1,neutral
This paper introduces a semi-supervised learning approach based on [4] and a distribution alignment strategy [22] to address the problem of labeling land use and land cover images and to address the problem of class imbalance.,1,neutral
There have been significant advances in semi-supervised learning (SSL) in recent years [4]–[6].,1,neutral
We show that our suggested technique outperforms FixMatch [4] under the custom augmentation,2,positive
FixMatch [4] is an algorithm that combines consistency regularization and pseudo-labeling.,1,neutral
Fixmatch [4] is an advanced semi-supervised learning (SSL) algorithm that combines consistency regularization and pseudo-labeling to achieve greater performance on a few available data.,1,neutral
"We compare our results with two semi-supervised learning techniques, MSMatch [23] and FixMatch [4] (with tweaked augmentation) on three datasets, EuroSAT [9] [10], UC Merced Land Use (UCM) dataset [12] and WHU-RS19 [13] [14].",2,positive
"In this paper, we introduce a semi-supervised learning approach based on [4] and a distribution alignment strategy [22], to address the problem of labeling land use and land cover",2,positive
We show that our suggested technique outperforms FixMatch [4] under the custom augmentation by 2.,2,positive
"Proposed Method: (a) Model trained using baseline SSL algorithm, Fixmatch [4] (b) Pseudo labeling is done on unlabeled data and classrebalancing is applied with multiple generations",2,positive
VAT [25] and FixMatch [37] are two popular semi-supervised methods.,1,neutral
"The accuracy of Model trained by trained by standard supervised learning, VAT [25] and FixMatch [37] remain almost zero at all chosen perturbation radii.",1,neutral
FixMatch [37] and our approach involve both the labelled data and unlabelled data for model training.,2,positive
"against the standard supervised learning method (Natural), virtual adversarial training (VAT) [25] and FixMatch [37].",1,neutral
results also show that VAT [25] and FixMatch [37] couldn’t provide adversarial robustness for vision transformers.,2,positive
"Our first guidance method is inspired by FixMatch [51], which leverages both pseudolabeling [29] and consistency regularization methods [2, 45].",2,positive
"One naive approach for providing depth guidance to the images in the absence of ground truth depth information is pseudo-labeling [29, 51], but it is challenging to generate a confident prediction that can be pseudo-label.",1,neutral
"We apply the stopgradient operation to the strong features, preventing the strong prediction from learning the weak prediction [6, 51].",1,neutral
The FixMatch approach [19] was introduced in 2020 with the intention of simplifying the increasing complexity of SSL algorithms and consequently established itself as the state-of-the-art.,2,positive
We adopt horizontal flip and random crop as weak augmentation and augmentation used in FixMatch [29] as strong augmentation.,2,positive
"We use MLP as the trunk network and use the same
strategy from FixMatch [8] to train the network.",2,positive
We use MLP as the trunk network and use the same strategy from FixMatch [8] to train the network.,2,positive
"Compared with MixMatch [33]
and FixMatch [34], it does not destroy the image details and does not strictly require the quality of unlabeled images, so it is more suitable for the retinal disease classification task.",2,positive
"Compared with MixMatch [33] and FixMatch [34], it does not destroy the image details and does not strictly require the quality of unlabeled images, so it is more suitable for the retinal disease classification task.",2,positive
FixMatch simplifies some existing semisupervision and enhances the prediction results [34].,2,positive
"Consistency regularization is an important component of many recent state-of-the-art self-supervised learning algorithms, which utilizes unlabeled data by relying on the assumption that the model should output similar predictions when fed perturbed versions of the same image [39], [40].",1,neutral
"We compare our method with some advanced SSL methods, including π - Model [47], Pseudo-Label [35], Mean Teacher [51], VAT [40], MixMatch [4], ReMixMatch [3], UDA [52], MutexMatch [14], CCSSL [55] and FixMatch [49].",2,positive
"Following [49], we use the weak augmentation α(·) to transform all the training data and additionally use a strong augmentation A(·) to produce an extra view of the unlabeled data.",2,positive
"FixMatch [49] and UDA [52] rely on a fixed threshold during training, and only retain the unlabeled data whose prediction probability is above the threshold.",1,neutral
"Besides Lc, we follow [49, 59] and consider two additional loss terms to build the overall training",0,negative
"Recently, semi-supervised learning (SSL) has achieved great success in training deep models on large-scale datasets without expensive labeling costs [14, 49, 55, 59].",1,neutral
"In Table 3, the results demonstrate that CCL can significantly improve the performance of FixMatch [49] and FlexMatch [59] under the label-scarce setting.",2,positive
"Following [36, 49, 55, 59], we conduct extensive experiments on several benchmarks, including CIFAR-10 [32], STL-10 [12], SVHN [42], and CIFAR-100 [32], each with various amounts of labeled data.",2,positive
The recently proposed FixMatch [12] achieves a significant performance boost by combining weak as well as strong DA and applying the CR criterion.,2,positive
"Such CR-based methods, in either classification [14, 45, 48, 54] or segmentation tasks [17,25,36,58], rely on various perturbation techniques to generate disagreement on the same inputs, so that models can be trained by enforcing prediction consistency on unlabeled data without knowing labeled information.",1,neutral
"More sophisticated methods for incorporating the confidence of the predictions and filtering out spurious samples were also developed, such as FixMatch [46], which employs a student-teacher architecture.",1,neutral
"On the other hand, semisupervised learning approaches [15,29,46,58] balance well the labeled and unlabeled sets but cannot handle the continual scenario, and suffer from forgetting even when paired with well-known CL methods (see Fig.",1,neutral
"Semi-Supervised Learning Methods We perform our analysis on FixMatch (Sohn et al., 2020) which achieves a classification accuracy of 94.93% on CIFAR-10 with only 250 labeled samples.",2,positive
"Like Sohn et al. (2020), we used a cosine learning rate decay and quoting from them, we set the “learning rate to ηcos ( 7πk 16K ) , where η is the initial learning rate, k is the current training step, and K is the total number of training steps.”",2,positive
"This limited labeled data is not enough to achieve the user’s desired classification accuracy, so they collect a large amount of unlabeled data U from less trusted sources and train their model using the FixMatch semi-supervised learning method (Sohn et al., 2020) to improve accuracy.",2,positive
"Recent semi-supervised learning methods achieve high accuracy with very few labeled samples (Xie et al., 2020; Berthelot et al., 2020; Sohn et al., 2020) using the strategies of pseudolabeling and consistency regularization which introduce new considerations when assessing the risk posed by backdoor attacks.",1,neutral
"We largely follow the experimental details from (Sohn et al., 2020), using a WideResNet-28-2 (Zagoruyko and Komodakis, 2016) architecture, RandAugment (Cubuk et al., 2020) for strong augmentation, and horizontal flipping and cropping for weak augmentation.",2,positive
"Semi-Supervised Learning Methods We perform our analysis on FixMatch (Sohn et al., 2020) which achieves a classification accuracy of 94.",2,positive
"For the FixMatch implementation, we closely followed the training set up from Sohn et al. (2020).",2,positive
"We largely follow the experimental details from (Sohn et al., 2020), using a WideResNet-28-2 (Zagoruyko and Komodakis, 2016) architecture, RandAugment (Cubuk et al.",2,positive
"Recent semi-supervised learning methods achieve high accuracy with very few labeled samples (Xie et al., 2020; Berthelot et al., 2020; Sohn et al., 2020) using the strategies of pseudolabeling and consistency regularization which introduce new considerations when assessing the risk posed by…",1,neutral
"We focus on a subset of recent semi-supervised learning techniques that have significantly improved classification performance (Xie et al., 2020; Berthelot et al., 2020; Sohn et al., 2020).",2,positive
"We compare our method with FairGen, and use the same attribute classifier for both methods, which is trained by a semi-supervised learning technique called FixMatch [41].",2,positive
"Regarding the specific techniques used, we reproduce the main ingredients of existing methods [12, 16], by following FixMatch [32] and applying MixUp [39] augmentation.",2,positive
2: Perform SSL training [32] using (soft)-labeled D1 as labeled data and D2 as unlabeled data (see Sec.,1,neutral
"[3] Sohn K, Berthelot D, Carlini N, et al.",1,neutral
can achieve better generalization performance [1][4][3].,1,neutral
"To increase the robustness of the self-training, consistency regularization [63, 67, 71] is often applied to ensure consistency over different data augmentations [1, 9, 18, 54], different crops [31, 38], multiple models [88, 93, 94], or domain-mixup [30–32, 72, 95].",1,neutral
"Furthermore, holistic approaches incorporate the dominant methods of SSL in a framework to achieve better performance [64]–[67].",1,neutral
"While supervised methods have achieved very impressive results [35], the extensive need for supervision inspired many works aiming to learn with fewer labels [6, 33].",1,neutral
[9] proposed an integrated classifier to give unlabeled images with pseudo-labels and used them to improve the performance of a model on image classification tasks.,1,neutral
"Their proposed algorithm is a federated version of the FixMatch technique [4] and, therefore, is dependent on data augmentations to perturb the images.",2,positive
"For the semi-supervised research problem, Pseudo labeling [21] is a popular approach that converts the model predictions into pseudo labels by applying a threshold [4].",1,neutral
"Due to their satisfactory performance, consistency-based approaches have gained popularity recently, such as Mean-Teacher [34], MixMatch [3], UDA [39] or FixMatch [32].",1,neutral
", 2022) Semi-supervised learning Train a predictive model via pseudo-labeling and representation learning L+U FixMatch (Sohn et al., 2020), VIME (Yoon et al.",1,neutral
"5), standard semi-supervised learning methods (Sohn et al., 2020; Chen et al., 2020a; Grill et al., 2020) are sub-optimal for anomaly detection under distribution mismatch, because they are developed with the assumption that labeled and unlabeled data come from the same distribution.",1,neutral
"Most semi-supervised learning methods assume that the labeled and unlabeled data come from the same distributions (Sohn et al., 2020; Chen et al., 2020a; Grill et al., 2020).",1,neutral
"(even when they aren’t developed for anomaly detection) can be adapted to the semi-supervised anomaly detection setting (Sohn et al., 2020; Chen et al., 2020a; Grill et al., 2020).",2,positive
"While it is common to use a trained binary classifier for pseudo-labeling (Lee et al., 2013; Sohn et al., 2020), we argue that it may be sub-optimal for anomaly detection with distribution shift as the decision boundaries of binary classifiers could be highly biased by the small labeled data.",1,neutral
"As a way of employing OCCs, SPADE differentiates from typical pseudo-labeling methods used in semisupervised learning (Lee et al., 2013; Sohn et al., 2020) that require building binary classifiers to assign pseudo-labels.",1,neutral
"The core idea of our framework, Semi-supervised Pseudo-labeler Anomaly Detection with Ensembling (SPADE), is based on self-training, following recent advances in semi-supervised learning (Sohn et al., 2020; Chen et al., 2020a).",2,positive
"State-of-the-art methods (Sohn et al., 2020; Chen et al., 2020a; Grill et al., 2020) are developed under the assumption that both labeled and unlabeled samples come from the same distribution.",2,positive
"L G
] 3
0 N
ov 2
02 2
(even when they aren’t developed for anomaly detection) can be adapted to the semi-supervised anomaly detection setting (Sohn et al., 2020; Chen et al., 2020a; Grill et al., 2020).",1,neutral
FixMatch [37] and PseudoSeg [38] encourage the consistency of the predictions under stronglyaugmented and the weakly-augmented.,1,neutral
"learning [5], [6], [7], [8], [9], [10] adversarial learning [11], [12], [13], self-training [14], [15], [16], contrastive learning [17], [18], [19], and collaborative training [20], [21].",1,neutral
"The subsequent improvements [6], [7], [8], [9], [10], [30] use different consistency regularization strategies to improve the prediction quality of unlabeled data and avoid network overfitting.",1,neutral
FixMatch [44] uses photometric transformation based strong-weak augmentation strategy on student-teacher,2,positive
"In order to ensure that the training process is regularized, pseudolabel prototypes [43], or consistency regularization [26,28], or domain mix-up are implemented [29, 47].",2,positive
"Here, we evaluate how our approaches to TTA that take advantage of this information perform compared to the baseline VAS
without TTA, as well as state-of-the-art TTA baselines discussed in Section 3.2 (where FixMatch is adapted to also take advantage of observed labels).",2,positive
"• We propose two new variants of test-time adaptation (TTA) variants of VAS: (a) Online TTA and (b) Stepwise TTA, as well as an improvement of the FixMatch stateof-the-art TTA method [24].",2,positive
"Even as we adapted them, TTT and FixMatch do not fully take advantage of the rich information obtained at decision time in the VAS context as we proceed through each input task: we not only observe the input image x, but also observe query results over time during the search.",2,positive
"Nevertheless, two common techniques can be either directly applied, or adapted, to our setting: 1) Test-Time Training (TTT) [25] and 2) FixMatch [24].",2,positive
"The TTT approach performs the worst, followed by (out adaptation of) FixMatch, which is only slightly better than TTT. Stepwise TTA (our approach) outperforms both TTT and FixMatch, albeit slightly, and Online TTA is, somewhat surprisingly much better than all others (this is surprising since it has a lower frequency of model update compared to Stepwise TTA).",2,positive
"Note that we can readily compose both of these TTA approaches with conventional TTA methods, such as TTT and FixMatch.",1,neutral
"In this case, we would expect the conventional TTT and FixMatch methods to be more competitive, as they have been specifically designed to account for distribution shift.",2,positive
"The original variant of FixMatch uses pseudo-labels at decision time, which are predictions on weakly augmented variants of the input image x (keeping only those which are highly confident), to update model parameters.",1,neutral
"[16] devised FixMatch, which first generates pseudo-labels using the model predictions on weakly augmented unlabeled images.",1,neutral
"Later, FixMatch [3] proposed a simple idea that showed very good results across different datasets and training setups.",2,positive
": Given the unlabelled dataset XU = (xi)Ni=1, we aim to sample a subset Xl = (xi) n i=1, 1 ≤ n   N , to form the labelled set XL = (xi, yi) n i=1 for downstream use by the SSL method S along with the unlabelled set XU .",2,positive
"Recently, some of the most successful methods combined consistency regularization with entropy minimization in a hybrid framework [3], [4], [15], which showed improved performances.",1,neutral
"We experiment with the labelled set selection and supervision policy modules on 8 well-known semi-supervised methods as backbones: PiModel [8], PseudoLabel [12], MeanTeacher [10], VAT [9], MixMatch [4], ReMixMatch [15], UDA [14], and FixMatch [3].",2,positive
": Next, we utilize the best policy from Table VIII on different SSL methods, and summarize the results in Table IX.",2,positive
"As one of the potential solutions to this problem, semi-supervised learning (SSL) [3], [4] has gained increased popularity in recent years.",1,neutral
The batch size of the unlabelled data is the same as the original SSL implementations.,2,positive
": To train an SSL backbone S, the labelled subset X∗L can be used according to a supervision policy P , where P dictates: (a) how many labelled samples XPL = (xi, yi) p i=1 ∈ X∗L are used at epoch e, and (b) the order in which XPL are used in the training process.",1,neutral
"Generally, semi-supervised methods randomly select a few samples from a large set of unlabelled data and label/annotate those samples to create a small labelled set [3]–[5].",1,neutral
"state-of-the-art semi-supervised learning methods [2, 23], consistency regularization is also introduced into the field of SSL to further boost the performance of unsupervised models, recently.",1,neutral
", FixMatch [35], to ViT [13] leads to an inferior performance [44].",1,neutral
"The research and application of SSL mainly focus on image recognition [35, 38, 44] with a two-step process: data augmentation and consistency regularization.",1,neutral
FixMatch [35] may lead to model collapse with limited labels as shown in [6].,1,neutral
FixMatch (NeurIPS 2020) [35] SlowFast-R50 V X 200 16.,2,positive
SVFormer follows the popular semi-supervised learning framework FixMatch [35] that use a consistency loss ⊛ λ,2,positive
"in Simultaneously, we also compute the pseudo-labels of all the unlabeled data using the current model, and perform strong augmentations of those target examples, for which we are confident about their pseudo-labels [12], [13], [13].",1,neutral
"Semi-Supervised Learning (SSL): SSL approaches utilize a small amount of labeled data and a huge amount of unlabeled data for the required task [12], [13], [28], [29],",1,neutral
"versions of a data-point [12], [13], [13], [33].",1,neutral
"Other approaches such as CT-Augment have been proposed in [12], [28] which uses a similar strategy as AutoAugment [26], but uses a fixed algorithm for assigning augmentation magnitudes.",1,neutral
"Inspired by the recent works in consistency regularization and self-training/pseudo-labeling [12], [13], [28], while training UM, we incorporate an augmentation-consistency based self-training to further aid the unsupervised learning, which is briefly described below for completion.",2,positive
"Recent approaches such as Mixmatch [13], ReMixmatch [28] and Fixmatch [12] provide an elegant framework for SSL, leveraging consistency regularization and MixUp [34] to obtain state-of-the-art results for SSL.",2,positive
"SSL has been actively studied in the context of image classification [18,26,35,37].",1,neutral
"Accurate pseudo-labeling is another crucial element for SSL to provide high-quality supervision for unlabeled data [20, 35].",1,neutral
"How to dynamically update? Previous attempts [44] to evaluate the reliability of pseudo-labels mainly focus on pixel-level filtering methods, with the common strategy being to filter out low-confidence pixel information via manual or adaptive thresholding.",1,neutral
"With the rapid improvement of semisupervised learning (SSL) methods [44, 16, 59, 37, 2], solutions based on different paradigms have made progress in semi-supervised semantic segmentation tasks.",1,neutral
"By K-fold cross-filtering, we can find noisy data, discard their labels, and warm up the main network using the SSL method presented in [46].",1,neutral
"(4)
For the unsupervised loss function, we exploit consistency regularization loss, a function used by FixMatch [46], one of the most prevalent modern SSL frameworks.",1,neutral
"Additionally, by utilizing consistency regulation as previously presented in [46], we train the model on all data included in the training dataset X .",2,positive
"In recent times, a holistic approach that makes use of all of the aforementioned methodologies shows an improved performance [5, 6, 46].",1,neutral
"For the unsupervised loss function, we exploit consistency regularization loss, a function used by FixMatch [46], one of the most prevalent modern SSL frameworks.",2,positive
"When SplitNet is removed, it becomes a consistency regularization [46] in general.",1,neutral
"As for the base SSL algorithms, apart from the evaluation on the widely used FixMatch [31] and ReMixMatch [2] in Sec.",1,neutral
"tings [31, 35], which highlights the importance of reducing imbalance ratio.",1,neutral
"1 (top), the test accuracy of FixMatch [31] (and other imbalanced algorithms) increases almost linearly as the imbalance ratio of the labeled set decreases.",1,neutral
"Consistency-based semisupervised learning (SSL) has demonstrated promising results in recent years [2,3,13,23,24,27,31,33,35,36,39,42].",1,neutral
"Meanwhile, many efforts have been put on the study of how to exploit the unlabeled data more efficiently, such as using confidence thresholding to mask out possibly incorrect pseudo-labels [31,36,40,42], and re-weighting of the pseudo-labels [20, 30].",1,neutral
"While AdaMatch [4] and FlexMatch [42] perform better than FixMatch [31] on balanced settings, they show inferior performance on imbalanced settings of CIFAR10-LT and CIFAR100-LT.",2,positive
"(Kingma et al., 2014), (Laine & Aila, 2016), (Sohn et al., 2020), (Xie et al., 2020), (Shu et al., 2018), (Zhang et al., 2019) and (Laine & Aila, 2016) have put a lot of effort into using unlabeled data.",2,positive
", 2014), (Laine & Aila, 2016), (Sohn et al., 2020), (Xie et al.",1,neutral
"B is batch size, μ = 7 as in [34], and H(y, p) is the cross-entropy.",1,neutral
"The current SSL techniques [23, 34, 41] use predicted classes from a discriminative classifier as pseudo-labels, for the unlabeled data, with a threshold to filter out low-confidence predictions.",1,neutral
"We use the exponential moving average of model parameters to report the final performance, as most SSL methods [3, 23, 34] do.",1,neutral
It is also clearly better than the FixMatch baseline [34] by 8.,1,neutral
"We remark that we downplay low-confidence pseudo-labels rather than simply ignore them as current methods do [23, 34].",1,neutral
We follow [34] to adopt Wide ResNet-28-2 for CIFAR-10 and Wide ResNet-28-8 [40] for CIFAR-100.,2,positive
"Following [34], we evaluate our methods in the settings of training with 4, 25, and 400 labels per class, respectively.",0,negative
"We train our model on 1000 labels, with 100 for each class, following [34].",0,negative
"We also follow [34] to report results of our models trained on 4, 25, and 100 labels per class, respectively.",0,negative
FixMatch [34] also employs weak augmentation for pseudo-label generation but it obtains the one-hot ‘hard’ pseudo-labels.,1,neutral
"High confidence predictions are usually used to filter noisy unlabeled data [3, 28].",1,neutral
"FedAvg-FixMatch [23, 28]: The naive combination of FedAvg with FixMatch is used to tackle federated semisupervised learning.",1,neutral
"(2) The naive combination of FL with semi-supervised learning, for example, FedAvg-FixMatch [23, 28] and Fedprox-
FixMatch [16, 28].",1,neutral
"(2) The naive combination of FL with semi-supervised learning, for example, FedAvg-FixMatch [23, 28] and FedproxFixMatch [16, 28].",1,neutral
"Following these methods, pseudo-labeling and consistency regularization training [3,28,34] has gained excellent performance.",1,neutral
"For simplicity, we use Fixmatch [28] for pseudo-label-based consistency learning in our method.",2,positive
"Fedprox is a generalization and re-parametrization of FedAvg, which adopts a proximal term to tackle the heterogeneity inherent in FL. FedproxFixMatch considers the systems and statistical heterogeneity while solving the problems of labeled clients and unlabeled clients.",2,positive
"Weak and Strong augmentations are adopted to simulate image perturbations for producing different views of an image [3,28].",1,neutral
"• FedAvg-FixMatch and Fedprox-FixMatch achieve some performance improvements compared to the FedAvg-SL-Lower method on the CIFAR-10 and CIFAR-100 datasets, but it fails to improve the performance on SVHN dataset with the NonIID setting.",2,positive
"Weak-strong augmentation schema To generate highquality pseudo-labels and improve generalization, we employ the recently weak-strong data augmentation schema [33].",2,positive
It utilizes a simpler method as the guessed labels are kept only if the model assigns a high probability to one of the possible classes [7].,1,neutral
"In recent years, different SSLM approaches have been proposed yielding interesting results, like in [7] and [8].",1,neutral
"After MixMatch was released, another approach known as FixMatch was published [7].",2,positive
"For the pseudo-label based method, setting a classification confidence threshold to filter pseudo labels with low quality can bring a big improvement to the performance, which is shown in FixMatch [23].",1,neutral
"Pseudo labelling is also explored in conjunction with augmentation techniques [45, 66].",1,neutral
"Among the methods represented are Mixup [36], MixMatch [37], ReMixMatch [38], FlxMatch [39], FlexMatch [40], etc.",2,positive
"1 Introduction and Related Works The Deep Learning community has recently made significant progress in semi-supervised learning via self-training or pseudo-labeling (PL) [1] in vision, speech and other domains [2, 3, 4, 5, 6, 7, 8, 9, 10, 11].",1,neutral
"Another line of research leverages pseudolabeling and data augmentation [18,19] approaches.",1,neutral
"Compared with other consistency-based methods such as FixMatch [67], Okapi has the advantage of being agnostic to both the task and the modality, in addition to being distributionally robust.",2,positive
"FixMatch [67] is one example of a consistency-based method which has proven effective for semi-supervised classification, despite its simplicity, and various works [28, 46] have since built on the its framework prescribing the use of weakly- and strongly-augmented inputs to generate the targets and predictions, respectively.",1,neutral
"We compare Okapi against two baselines, ERM and FixMatch [67], both according to our reimplementation and according to the original implementation given in [63].",2,positive
"To highlight the efficiency of Okapi, we provide estimates in 6 of the carbon footprint associated with the running of it and of the ERM and FixMatch baselines on the iWildCam dataset, using the same hyperparameter configuration used to generate the results in the main text.",2,positive
"Self-training applies to any framework predicated on using a model’s own predictions to produce pseudo-labels for the unlabelled data which can either be used as targets for self-distillation [75] or enforcing consistency between predictions that themselves have been perturbed [5, 75] or that have been generated from perturbed/multi-view inputs [67].",1,neutral
"Namely, on three datasets from the WILDS 2.0 benchmark, representing two different tasks (classification and regression) and modalities (image and text), we show that Okapi outperforms both the ERM and FixMatch baselines according to the relevant OOD metrics.",2,positive
"Our method belongs to the broad family of consistency-based methods, characterised by methods such as FixMatch [67], where the idea is to enforce similarity between a model’s outputs for two views of an unlabelled image.",2,positive
"In particular, networks are required to make consistent predictions over previous training iterations (Laine & Aila, 2017; Tarvainen & Valpola, 2017; Izmailov et al., 2018), noisy inputs (Miyato et al., 2018), or augmented inputs (Berthelot et al., 2019b;a; Sohn et al., 2020).",1,neutral
"It can be combined with knowledge distillation [58], data augmentation [59], and other techniques to achieve extremely competitive results.",2,positive
The authors reported accuracy gains ranging from 1 to 3 percent with respect to FixMatch.,0,negative
MixMatch has been extended to include more sophisticated approaches of data augmentation in FixMatch [27] and ReMixMatch [3].,2,positive
"To this end, we experiment with an extremely simple semi-supervised learning technique (similar to [41]) to aid active learning.",1,neutral
"Their results are reported in Table 1, where LP-A3 consistently improves FixMatch and the improvement becomes more significant if reducing the labeled data.",0,negative
"We apply LP-A3 in FixMatch [36] and compare it with the original FixMatch and InfoMin [40], a learnable augmentation method for semi-supervised learning.",2,positive
"It’s worth noting that the original FixMatch already employs a carefully designed set of pre-defined augmentations [13] that have been tuned to achieve the best performance, indicating that LP-A3 is complementary to existing data augmentations.",2,positive
"For instance, the FixMatch [48] SSL algorithm uses only 40 labeled data along with about 50,000 unlabeled data and achieves 90% accuracy on CIFAR10.",2,positive
"We detail our intuition for the FixMatch [48] algorithm, but it applies to any semi-supervised algorithms [7], [6], [60], [54] that use pseudo-labeling and consistency regularization (Section II-A).",1,neutral
"As proposed in original works [48], [7], we use the same number of the labeled samples for each of the 10 classes, i.",1,neutral
"For instance, with less than 10% of training data labeled, FixMatch [48] and FlexMatch [60] SSL algorithms outperform the fully-supervised algorithm.",1,neutral
"SSL is being explored extensively by both academia [60], [54], [55] and industry [48], [49], [7], [6], as recent SSL algorithms offer state-of-the-art performances comparable or even superior to those achieved by conventional supervised techniques—but with no need of large well-inspected labeled data.",2,positive
We briefly describe these two techniques as in [48] followed by the semi-supervised algorithms we consider in this work.,1,neutral
(4) FixMatch [48] simplifies the complex ReMixMatch algorithm by proposing to use a combination of Pseudo-labeling and consistency regularization based on augmentation anchoring (discussed above).,1,neutral
"We compare our method with seven semisupervised learning methods, including PL [22], mean teacher (MT) [12], VAT [41], MixMatch [24], UDA [31], ReMixMatch [42], FixMatch [26], and AdaMatch [43].",2,positive
"The FixMatch algorithm [26] combines consistency regularization with pseudo-labeling (PL), simplifying the model while introducing more appropriate data augmentation methods to obtain a superior classification result.",1,neutral
"1) Double-Threshold Filtering: In the semisupervised training, low-confidence pseudo-labels will result in the noisy training and the confirmation bias [35], and it is proven that training without the samples with low-confidence pseudo-labels will effectively improve the performance of the classifier [26].",1,neutral
"The shapes of the accuracy curves of UDA [31] and FixMatch [26] are similar to each other; meanwhile, the FixMatch’s accuracy is higher because it fully exploits the benefits of the consistency regularization strategy based on both strongly and weakly data augmentation.",2,positive
"[26] K. Sohn et al., “FixMatch: Simplifying semi-supervised learning with consistency and confidence,” in Proc.",1,neutral
"The distribution alignment used the distribution of a model's aggregated class predictions to match the marginal distribution of ground‐truth class labels, while the augmentation anchoring was an upgraded version of the consistency regularization method.20 Sohn et al.21 proposed FixMatch, a combination of ReMixMatch and pseudolabeling, to capture the relationships in labeled data.",2,positive
"As Berthelot et al.,19 Sohn et al.,21 and Lee59 suggest, τ > 0.50 (i.e., high‐confidence) is helpful to reduce the entropy of a model with the pseudolabeling technique on unlabeled data.",1,neutral
"Sohn et al.(21) proposed FixMatch, a combination of ReMixMatch and pseudolabeling, to capture the relationships in labeled data.",1,neutral
"Pseudo-labeling can be successfully used in combination with consistency regulation, which aims at training models that are invariant to different data augmentations [6], [7], [34].",1,neutral
"Semi-supervised methods (Berthelot et al., 2019; Sohn et al., 2020) learn representations when ground-truth labels are available for only a small fraction of the data (0.",1,neutral
"We compare the representations of semi-supervised (Fixmatch (Sohn et al., 2020)), contrastive (SimCLR (Chen et al.",1,neutral
"We compare the representations of semi-supervised (Fixmatch (Sohn et al., 2020)), contrastive (SimCLR (Chen et al., 2020a), Barlow-twins (Zbontar et al., 2021)) and supervised learning in Fig.",1,neutral
"Semi-supervised methods (Berthelot et al., 2019; Sohn et al., 2020) learn representations when ground-truth labels are available for only a small fraction of the data (0.1–1%).",1,neutral
"[11] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D.",0,negative
"Other related methods include mixmatch [1] that rely on mixing labeled and unlabeled samples, and methods relying on pseudo-labels [11] and data augmentation [18].",1,neutral
"Data augmentation is effective in visual representation learning [58], [59], an adequate face augmentation technique will help the encoder to learn the individual facial characteristics.",1,neutral
"In addition, the proposed method could be easily used in the semisupervised learning framework [73], [74], [75].",1,neutral
"To investigate the performance of ASD with limited training data, ASD is compared with other methods including DeepCluster [37], JigsawPuzzle [38], Rotation [39], FixMatch [40], VAT [41], and SSPL [15].",2,positive
"The idea is to force the output of the model to be invariant to different augmentations of the same input [51, 66, 7, 36], or variations in the internal representations [4, 50], or the model parameters at different training epochs [33].",1,neutral
"Consistency is widely used for the problem of semi-supervised learning [50, 51, 66, 7, 33].",1,neutral
"Self-training has been applied to a wide variety of tasks, including classification [63,56,1], semantic segmentation [40,6,62,22], object detection [47,57,71,4], speech recognition [41,27].",1,neutral
"Due to its simplicity, many attempts have been made for the field of semi-supervised learning [18, 2, 4, 28] and unsupervised domain adaptation [49, 50].",1,neutral
"Previous methods attempt to reduce the noise by ensembling multiple predictions for an image under different augmentations [4, 22] or by selecting only the pseudo-labels with high confidence [28].",1,neutral
"Given a specified sampling budget, we compare the test accuracy of semi-supervised learning (FixMatch) on sampled data in Table 2.",1,neutral
"We adopt FixMatch for semi-supervised learning with the coefficient of 0.1 on the pseudo-labeled loss, the moving average factor of 0.9, and the batch size of 64 for DomainNet and 128 for Digits.",2,positive
"Then, we adopt the popular semi-supervised learning method, FixMatch [46], to train the classifier.",2,positive
"To solve the learning problems including FixMatch and distillation-based compression, we use stochastic gradient descent with the momentum of 0.9 and the weight decay of 5× 10−4.",1,neutral
"To simplify the consistency regularization process, FixMatch (Sohn et al., 2020) classified two unlabeled augmented views into a weak view and a strong view, and then minimized the divergence between the probability dis-",1,neutral
"Also, a couple of consistency regularization methods are introduced to simplify the semi-supervised learning process (Berthelot et al., 2019a; Sohn et al., 2020) as well as to boost performance in domain adaptation scenarios (Berthelot et al.",1,neutral
"To simplify the consistency regularization process, FixMatch (Sohn et al., 2020) classified two unlabeled augmented views into a weak view and a strong view, and then minimized the divergence between the probability distribution of the strong view and the pseudo label of the weak view.",1,neutral
"Semi-supervised learning in NLP has received increasing attention in improving performance in few-shot scenarios, where both labeled data and unlabeled data are utilized (Berthelot et al., 2019b; Sohn et al., 2020; Li et al., 2021).",1,neutral
"Also, a couple of consistency regularization methods are introduced to simplify the semi-supervised learning process (Berthelot et al., 2019a; Sohn et al., 2020) as well as to boost performance in domain adaptation scenarios (Berthelot et al., 2021) to improve semi-supervised learning.",1,neutral
"To test the effectiveness of our approach, we compared it with several popular self-training methods: UDA (Xie et al., 2020), MixText (Chen et al., 2020a), FixMatch (Sohn et al., 2020).",2,positive
"is to utilize data consistency [51], [52].",1,neutral
"FixMatch(Kurakin et al., 2020) is another SSL algorithm that we considered post-hoc.",2,positive
Holistic methods [28-30] comprehensively combine the two methods mentioned above.,1,neutral
"This paper selects fully supervised fedavg [26] learning as the comparison upper bound, fine-tune [27] and Fedprox [28] method as the comparison federal learning methods, and UDA [29] and fixmatch [23] methods as the semi supervisesemi-supervised methods.",2,positive
[15] formulates the consistency loss as Equation (1):,1,neutral
"Equation (3) is simply the combination of Equations (1) and (2) provided in [15], where α refers to the weak data augmentation and A refers to the strong data augmentation.",1,neutral
FixMatch [15] is a state-of-the-art semi-supervised learning method that produces pseudo (one-hot) labels from weakly augmented samples and utilizes the cross-entropy loss to ensure the consistencies between pseudo labels and the predictions of the same samples (strongly augmented).,1,neutral
"During the experiment, we compare FocalMatch with several sate-of-the-art semisupervised learning models on the aforementioned three datasets: Π model [38], Mean Teacher [39], MixMatch [40], ReMixMatch [24], UDA [25], and FixMatch [15].",2,positive
The authors in [15] give the definition of the loss function of pseudolabeling as Equation (2):,1,neutral
The supervised loss Ls is the same as FixMatch [15]:,1,neutral
FixMatch [15] is a recently proposed state-of-the-art semi-supervised learning algorithm.,1,neutral
FixMatch [15] generates one-hot pseudo labels from predictions on weakly-augmented data with a predefined high threshold and ensures consistency against strongly-augmented data.,1,neutral
"In FixMatch [15], the cross-entropy loss is used between the pseudo labels and the predictions of strongly-augmented images.",1,neutral
"Comparing FocalMatch to other state-of-theart models (including Π model [38], Mean Teacher [39], MixMatch [40], ReMixMatch [24], UDA [25], and FixMatch [15]), our experiments reveal that FocalMatch significantly reduces the difference in the total number of pseudo labels generated for each class and has a more gradual learning curve.",2,positive
"Semi-supervised learning, on the other hand, is a widely used technique for leveraging a large unlabeled dataset alongside a small fully-labeled subset [2, 25, 30, 33, 35, 40].",1,neutral
"This subsection compares the performance of G2NetPL with the state-of-the-art semi-supervised models that exploit the pseudo labels to train the network, such as FixMatch [35] and",2,positive
"(i) Our approach can ensure smoothness and convergence of pseudo labels during training, which are important quality indices, while most existing methods (e.g., FixMatch [35] and UPS [33]) use threshold-based strategies that lack the continuity and guarantee of convergence in pseudo label updating.",2,positive
"This subsection compares the performance of G2NetPL with the state-of-the-art semi-supervised models that exploit the pseudo labels to train the network, such as FixMatch [35] and
Figure 4: Comparison between our proposed model and semi-supervised models on SSPL.
UPS [33], on two datasets VoC and COCO under different SSPL settings.",2,positive
This is because both FixMatch and UPS models suffer from the same issue of using threshold-based strategies as [12] (more details can be found in Section 2.,1,neutral
"As mentioned above, with necessary modifications, some semi-supervised learning algorithms, such as Fixmatch [35] and UPS [33], can work under this setting.",1,neutral
", FixMatch [35] and UPS [33]) use threshold-based strategies that lack the continuity and guarantee of convergence in pseudo label updating.",1,neutral
"4, G2NetPL outperforms FixMatch and UPS in both datasets.",2,positive
"Witnessing the impressive results achieved by a series of pseudolabeling based semi-supervised methods [2, 25, 36], we divide the",1,neutral
"FixMatch [46] uses consistency regularization with confidence-based pseudolabelling, SemCo [32] builds on FixMatch but leverages label semantics (via a knowledge graph) to account for preknown similarities among classes, and MeanTeacher [49] uses momentum teachers for SSL.",2,positive
"Another approach to SSL is using pseudolabels which are generated by either training the model on the labelled samples and pruning the confident predictions on unlabelled data [5, 6, 25, 32, 41, 46], or by us-",1,neutral
"Current SSL approaches utilise the prediction of a model as supervision for the training on unlabelled data and can be weakly separated into consistency regularization [1, 2, 11, 21, 30] and self-training approaches [26, 28].",1,neutral
"Both approaches utilise strong data augmentation such as CutOut [9], CutMix [11], ClassMix [19] and colour space augmentations [21, 28, 34] when training with pseudo-labels.",2,positive
"Some methods further apply confidence-based filtering of pseudo labels [20, 21, 27, 30, 31, 33].",1,neutral
"Previous work based on consistency regularization has shown that adding perturbations at the image space when training with pseudo-labels helps to improve the performance [11, 19, 21].",1,neutral
FixMatch [21] uses a fixed confidence threshold for all classes to remove uncertain labels for image classification.,1,neutral
"FixMatch [26] and Noisy Student selftraining [31] are two methods combining such building blocks: the former follows an online approach where pseudo-labels are generated during training, and the latter has subsequent pseudo-labeling and re-training phases.",1,neutral
"Modern methods achieve competitive results at a fraction of the amount of annotated samples required for standard supervised learning [1, 2, 26].",1,neutral
"However, SSL methods have been mainly developed and studied with image-level classification in mind [28, 21, 31, 1, 26].",1,neutral
"To this end, we perform simple but effective modifications to FixMatch [26] to adapt it for a larger class of dense or structured task, staying as close as possible to the original formulation.",2,positive
"In addition, it is common to use only high-confidence predictions as pseudo-labels [31, 26].",1,neutral
We adapt FixMatch [26] for its use in structured and dense prediction tasks in the semi-supervised setting.,2,positive
"FixMatch [26] applied to image classification uses a standard flip-and-shift augmentation strategy as α and either RandAugment [7] or CTAugment [2] as A, always applying Cutout [8] last.",1,neutral
Lsel f is borrowed from Fixmatch [37] that imposes prediction consistency of unseen points on two augmented scenes.,1,neutral
L is borrowed from Fixmatch [37] that imposes prediction consistency of unseen points on two augmented scenes.,1,neutral
Our SSL algorithm is based on FixMatch (see Fig.,2,positive
"3), which achieves competitive performance by focusing on consistency regularization and entropy minimization [45].",1,neutral
"For fair comparison with existing noisy-label learning methods, we shorten the training schedules typically used by FixMatch implementations (we use 100,000 training iterations with µ = 4 rather than 1,000,000 iterations with µ = 8).",2,positive
"Following existing implementations of FixMatch (such as by TorchSSL [59]), we use Exponential Moving Average (EMA) models to perform temporal ensembling [24].",2,positive
"In this paper, we utilize the SSL method FixMatch [45], which uses pseudo-labelling thresholds and consistency between strong and weak augmentations to regularize training through consistency regularization [3, 24, 48] and entropy minimization [15, 26].",2,positive
"In our implementation of FixMatch, we drop the noisy label of supervised samples and strong augmentations of unsupervised samples 50% of the time.",2,positive
"Relation to consistency regularization Popular consistency regularization methods (Sajjadi et al., 2016; Laine & Aila, 2016; Sohn et al., 2020; Berthelot et al., 2019) leverage the idea that a model should output the same distribution for an unlabeled example even after it has been augmented.",1,neutral
"Semi-supervised learning (SSL) [2, 3, 5, 17, 35, 48], a paradigm to use abundant unlabeled data to improve
ar X
iv :2
21 0.",1,neutral
"In SSL, the class distribution of the labeled data is usually adopted as a prior to calibrating the pseudolabel distribution of the unlabeled data [2, 17].",1,neutral
"Semi-supervised learning (SSL) [2, 3, 5, 17, 35, 48], a paradigm to use abundant unlabeled data to improve",1,neutral
"This problem has been investigated in domain adaption [11, 30, 32, 38, 52] but is usually neglected in semi-supervised learning [2, 3, 5, 17, 35, 37, 43, 48], which would result in noisy pseudo-labels and hurt the model performance.",1,neutral
"Inspired by FixMatch [8], we use two types of augmentations, strong and weak, denoted asA(·) and α (·) respectively, as the perturbed versions for unlabeled instances.",1,neutral
"Inspired by FixMatch [8], we use two types of augmentations, strong and weak, denoted asA(·) and 𝛼 (·) respectively, as the perturbed versions for unlabeled instances.",1,neutral
"For better representation learning of unlabeled target data, we take the self-training technique widely applied in semi-supervised learning [26, 32].",1,neutral
[26] has proposed that the data augmentation is crucial for semisupervised learning by assigning the pseudo labels as regularization.,1,neutral
"Variants of their combination have achieved great success [36,40,46,49,52], whose core inspiration is computing consistency regularization via pseudo labeling.",1,neutral
"On the other hand, consistency regularization aims to obtain prediction invariance under various perturbations, including input perturbation [32, 40, 48], feature perturbation [35], network perturbation [11,19,36,42], etc.",1,neutral
"filtering low-confidence pseudo labels out [15,22,40,53,55] and generating pseudo labels more accurately [9,21,27,50].",1,neutral
"Entropy minimization enforces the predicted probability distribution to be sharp by training upon pseudo labels [3, 4, 24, 28, 40, 48].",1,neutral
Sohn et al. (2020) showed that augmenting the confidently pseudo-labeled images can help to improve the accuracy of their model.,1,neutral
"To further eliminate the local-optimization effect that the overfitting brings, we add the entropy of the UA outputs to avoid the outcome overconfidence [38], [39], which is Lentropy = −∑j∈Ks ∑ i∈N eij ln eij, where enk = exp(znk)/ ∑ i∈N exp(zjk).",1,neutral
"At last, we will further improve the robustness of our framework with some famous semisupervised deep learning methods such as FixMatch [57] and FMixCutMatch [58].",2,positive
"Pseudo labels are widely used in the semisupervised learning setting (Iscen et al., 2019; Xie et al., 2020b; Sohn et al., 2020; Pham et al., 2021).",1,neutral
"T1, T2 U(10,20) U(5,10) Number of phase state: 2 Number of segments: [18,20,22]",1,neutral
"T3 ,T4 U(10,30) U(5,10) Number of phase state: 2 Number of segments: [18,20,22] Bandwidth U(5,10) MHZ",1,neutral
"Next, the model is trained to minimize the semi-supervised loss `semi[22].",1,neutral
"Inspired by recent success in semi-supervised learning, we make use of unlabeled data and very few labeled data for model training which reduces heavy expert labeling labor [16].",2,positive
"Following the common practice state-of-the-art SSL frameworks [16], [26], [27], the proposed framework contains two main components: consistency regularization and pseudo labeling.",2,positive
Another classical method combining consistency regularizations and pseudo labels is FixMatch [7] as shown in Figure 1.,1,neutral
"For unlabeled images, a weakly augmented image is fed into the model to obtain a one-shot pseudo-label, and then FixMatch uses the same image but with strong augmentation for training.",1,neutral
"For example, Mean Teacher [50] first showed that the momentum network improves the performance of semi-supervised image classification, and recent advanced approaches [2, 46] adopted this idea for achieving state-of-the-art performances.",1,neutral
"Regarding algorithms performing explicit in- and out-of-distribution noise detection, EvidentialMix [24] (EDM) fits a three component GMM to the evidential-loss [26]; JoSRC [38] (JoSRC) uses the Jensen–Shannon divergence; Dynamic Softening for Out-of-distribution Samples [2] (DSOS) uses the collision entropy and Spectral Noise clustering from Contrastive Features [1] (SNCF) clusters unsupervised features using OPTICS; Progressive Label Correction [44] (PLC) iteratively refine their noise detection under Bayesian guaranties,",1,neutral
"By doing so, the dataset creation time is greatly reduced but label noise becomes an issue [2] and can greatly degrade the classification accuracy [42].",1,neutral
Label noise in web crawled datasets has been evidenced to be a mixture between in-distribution (ID) noise and out-ofdistribution (OOD) noise [2].,1,neutral
Corruption rout rin CE M DB JoSRC ELR EDM DSOS RRL SNCF PLS INet32 0.2 0.2 63.68 66.,0,negative
"Regarding algorithms performing explicit in- and out-of-distribution noise detection, EvidentialMix [24] (EDM) fits a three component GMM to the evidential-loss [26]; JoSRC [38] (JoSRC) uses the Jensen–Shannon divergence; Dynamic Softening for Out-of-distribution Samples [2] (DSOS) uses the collision entropy and Spectral Noise clustering from Contrastive Features [1] (SNCF) clusters unsupervised features using OPTICS; Progressive Label Correction [44] (PLC) itera-
tively refine their noise detection under Bayesian guaranties, Peer-Learning [32], Co-teaching [9], PENCIL [39] co-train two networks and identify clean samples by voting agreement; SELFIE [29] select low entropy noisy samples to be relabeled while discarding the rest.",2,positive
Real world noisy data is often out-of-distribution [2].,1,neutral
"In the case where a separate out-of-distribution detection is performed, the samples can either be removed from the dataset [24], assigned a uniform label distribution over the classes to promote rejection by the network [38, 2], or used in an unsupervised objective [1].",1,neutral
"EvidentialMix [24] uses the evidential loss [26], JoSRC evaluates the Jensen-Shannon divergence between predictions [38], and DSOS [2] computes the collision entropy.",1,neutral
"Several recently published semi-supervised [21], [40], [41] and contrastive learning approaches [25], [26], [28] are chosen for comparison.",1,neutral
FixMatch [41] is a pseudo-label based approach in which the pseudo label is only retained when the model produces a high-confidence prediction.,1,neutral
"Following standard evaluation protocols [4, 29], we discard the labels of samples as the unlabeled set but leave a small portion with class-balanced labels as the labeled set to construct SSL scenarios.",2,positive
"Latest follow-up studies have achieved more significant performance by combining these two mechanisms [1, 3, 16, 29, 34] or incorporating self-supervised losses [5, 41].",1,neutral
"Methods STL-10
1000 labels
Supervised (w. RandAugment [9]) 20.66 ± 0.83 Π-Model [17] 26.23 ± 0.82 (-5.57) Mean Teacher [31] 21.43 ± 2.39 (-0.77) MixMatch [4] 10.18 ± 1.46 (+10.48) UDA [34] 7.66 ± 0.56 (+13.00) FixMatch [29] 7.98 ± 1.50 (+12.68) Ours 6.46 ± 0.62 (+14.20)
advantages of semantic modal knowledge enriching supervision signals and the more efficient utilization of unlabeled samples.",1,neutral
"1, taking FixMatch [29] as an example, the proportions of pseudo-labels and correct pseudo-labels are relatively low in early epochs.",1,neutral
"Our method is built on top of the general SSL framework, especially FixMatch [29], but we introduce semantic modal knowledge in order to alleviate the training dilemma which is frequently caused by lack of supervision.",2,positive
", the mean and standard deviation of test error over 3 or 5 different folds except ACR [1]) are from [4, 29] or reported by the original works.",0,negative
"Then, we apply a sharpening function to reduce the entropy of the label distribution instead of a argmax function used in [29] since some potential associations between different class embeddings are covered.",1,neutral
"Following others [4, 29], we use the WRN-28-2 backbone (1.",2,positive
"Note that the results with ‡ are reported by our re-implementation and others are from [4, 29] or reported by the original works.",0,negative
"as functions of the epoch, measured with FixMatch [29] on",1,neutral
"Combined with data augmentation strategies [3, 9, 10, 43], preexisting state-of-the-art SSL approaches which rely on consistent regularization [3, 4, 17, 22, 29, 31, 34] and pseudo-labeling [6, 14, 18, 26, 27] have made significant breakthroughs.",1,neutral
The recently proposed semisupervised method FixMatch [34] successfully combine these two techniques together to produce the state-of-the-art classification performance.,2,positive
Those two schemes are often integrated into a teacher-student learning paradigm: the teacher model generates pseudo labels to train a student model that takes a perturbed input [34].,1,neutral
The recently proposed semisupervised method FixMatch [36] successfully combine these two techniques together to produce the state-of-the-art classification performance.,2,positive
"Overview: the overall structure of the proposed method is shown in Figure1, our approach is built on top of the popular student-teacher framework for semi-supervised learning [35, 34, 47, 28, 43].",2,positive
"Following the common practice [34], we also adopt the weak-strong augmentation paradigm by feeding the teacher model weakly-augmented images and the student strongly-augmented images.",1,neutral
"Inspired by Tarvainen & Valpola (2017); Sohn et al. (2020); Chen et al. (2022), we use a student-teacher model and an online memory bank refinement to generate pseudo labels.",2,positive
"To verify the effectiveness of our approach, we compare the proposed HiCo with supervised ResNet18 backbones (i.e., “ImageNet” pretrained on ImageNet dataset, “Supervised” pretrained on US-4 dataset), and other backbones pretrained on US-4 dataset with semi-supervised methods (i.e., Temporal Ensembling (TE) [59], Π Model [59], FixMatch [60], USCL [3]) and self-supervised methods (i.e., MoCo v2 [13], SimCLR [10]).",2,positive
", Temporal Ensembling (TE) [59], Π Model [59], FixMatch [60], USCL [3]) and self-supervised methods (i.",1,neutral
Supervised contrastive learning (SCL) is extended from the standard contrastive loss by incorporating the label information to construct positive and negative pairs [37].,1,neutral
"Self-training [3, 29, 34, 47, 48] leverage unlabeled data by utilizing a segmentation network pretrained on labeled data to produce pseudo segmentation maps with which the network is retrained.",1,neutral
"These include consistency regularization (Bachman et al., 2014; Sajjadi et al., 2016; Samuli & Timo, 2017; Sohn et al., 2020) and co-training (Blum & Mitchell, 1998; Balcan et al.",1,neutral
"These include consistency regularization (Bachman et al., 2014; Sajjadi et al., 2016; Samuli & Timo, 2017; Sohn et al., 2020) and co-training (Blum & Mitchell, 1998; Balcan et al., 2004; Han et al., 2018).",1,neutral
"Indeed, such a consistency-based training strategy has successfully been applied in the semisupervised learning literature [28].",1,neutral
"DENG, LI, SONG, XIANG: ROBUST TARGET TRAINING 13 [33] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D.",0,negative
"For training the final model in BORT2, the pseudo-labels are generated in the same way as [33].",1,neutral
"(2) Inspired by FixMatch [33], we also put a threshold τ to select the most confident “hard” labels.",2,positive
"FixMatchCM adapts the vanilla FixMatch [33], originally proposed for semi-supervised learning, to MSDA.",2,positive
54% on two FixMatch [33] variants (or see #3 vs.,1,neutral
[23] to the task of semantic segmentation.,1,neutral
"-ese assumptions consider that data pointing close to each other are likely from the same class, which are often used in classification tasks [29, 30].",1,neutral
"This idea has been used in many discriminative networks (Yun et al., 2019; Zhang et al., 2018a; Sohn et al., 2020) and, recently, Beckham et al.",1,neutral
"This idea has been used in many discriminative networks (Yun et al., 2019; Zhang et al., 2018a; Sohn et al., 2020) and, recently, Beckham et al. (2019) proposed adversarial Mixup to regularize the latent space of an unsupervised auto-encoder.",1,neutral
"In order to strongly and fairly quantify the contribution of ActorCLR, we first extend the SOTA SSL methods in the image domain [9]–[11] into the video domain as a baseline.",2,positive
"Though our formulation is based on FixMatch [9] for simplicity, we have also integrated our method with recent strong SSL methods [10], [11].",2,positive
"The SOTA consistency-based methods [9], [10] utilizes unlabeled data by assuming that the model should predict similar output when fed strongly-augmented versions of the same data.",1,neutral
"Subsequently, we integrate this procedure with the powerful SSL teacher-student models [9]–[11] to optimize the contrastive loss and consistency regularization objectives jointly.",2,positive
"Generally, existing video SSL methods are the extensions of imagebased SSL methods [9].",1,neutral
"While we adopt image-based SSL methods [9]–[11] for video, we define two types of data augmentations T : weak and strong augmentations.",2,positive
"More recently, SSL representation learning models [9] have shown",1,neutral
"The results show that, the straightforward extension of image-based SSL methods [9]–[11] can offer acceptable performance improvement on all datasets.",2,positive
The general pipeline of SSL follows consistency regularization [9]–[11] and pseudo-labeling [12] schemes.,1,neutral
The two important aspects of SSL are pseudo-labeling [12] and consistency regularization [9]–[11].,1,neutral
"Moreover, in the eth (e ∈ E) round of the local iterations, the unlabeled data set Dm is first expanded by using two different types of data augmentations (DA) in [33], i.",1,neutral
"FixMatch [36] first generates high-confidence 240 pseudo-labels of weakly-augmented unlabeled images, then 241 the pseudo-labels can be used to train the model on the 242 strongly-augmented images.",1,neutral
"[23] K. Sohn et al., “FixMatch: Simplifying semi-supervised learning with consistency and confidence,” in Proc.",1,neutral
"We compare our framework with state-of-the-art semisupervised methods, including mean teacher (MT inshort) [4], uncertainty-based mean teacher (UA-MT in short) [18], crossconsistency training (CCT in short) [22], FixMatch [23], cross pseudo supervision (CPS in short) [24], dual-task consistency (DTC in short) [26], conservative-radical network (CoraNet in short) [21], cross teaching between CNN and transformer (CTBCT in short) [25].",2,positive
"We compare our framework with state-of-the-art semisupervised methods, including mean teacher (MT inshort) [4], uncertainty-based mean teacher (UA-MT in short) [18], crossconsistency training (CCT in short) [22], FixMatch [23], cross pseudo supervision (CPS in short) [24], dual-task consistency",2,positive
"The use of unlabeled data in the surrogate model allows us to use any task-specific semi-supervised learning algorithm such as FixMatch (Sohn et al., 2020), MixMatch (Berthelot et al.",1,neutral
"The use of unlabeled data in the surrogate model allows us to use any task-specific semi-supervised learning algorithm such as FixMatch (Sohn et al., 2020), MixMatch (Berthelot et al., 2019), or Noisy Student (Xie et al., 2020).",2,positive
"In our active sampling and robustness experiments, we use FixMatch (Sohn et al., 2020) as our semi-supervised algorithm (see Appendix A for more details).",2,positive
"We describe FixMatch (Sohn et al., 2020), the semi-superivsed learning algorithm used in the experiments in Section 5.",2,positive
"In our work we use FixMatch (Sohn et al., 2020), a general-purpose semi-supervised learning method, and pseudo-labeling and show they are key to the success of our framework.",2,positive
"Semi-supervised Learning Semi-supervised learning trains models using limited labeled data alongside large amounts of unlabeled data (Berthelot et al., 2019; Xie et al., 2020; Sohn et al., 2020).",1,neutral
"Student Learning To utilize the readily available unlabeled images Du, we use the pseudo-labeling method to generate labels for Du to train the student model, which has been shown effective for semi-supervised image classification [29,27] and object detection [19,18].",2,positive
"Notably, our model outperforms the latest SOTA method FixMatch [27] by 4.",2,positive
"Inspired by the recent advances in SSL for image classification [29,27], we use the teacher-student pseudo labeling method as the training paradigm of our framework.",2,positive
"For effective and efficient distillation, we initialize the student model with the weights of the teacher model and apply strong data augmentations on input images to the student following the common SSL paradigm [27].",2,positive
"Considering that the 3D voxel resolution of Pix3D is 128(3), which increases the model complexity, we compare it with the supervised method and the two state-of-the-art methods of MeanTeacher [29] and FixMatch [27]",2,positive
"Generalizing best practices [29,27] that work well in the 2D domain to 3D reconstruction, while appealing, is challenging.",1,neutral
"Secondly, we extend stateof-the-art SSL methods for image classification such as MeanTeacher [29], MixMatch [1] and FixMatch [27], to the task of 3D reconstruction, which serve as strong semi-supervised baselines.",2,positive
"While appealing, directly extending existing SSL methods like MeanTeacher [29] and FixMatch [27] for single-view 3D reconstruction is challenging since the pseudo labels from the teacher model can be quite noisy for two main",2,positive
Existing semi-supervised classification methods often use the confidence scores predicted by the network as a proxy and only keep the confident predictions as pseudo labels through applying predefined thresholds [41] or using Top-k selection [27].,1,neutral
The addition of FixMatch can only improve plain FedAvgM without BottleGAN and even leads to slightly worse performance in terms of NLL and ECE if combined with BottleGAN.,2,positive
"Further, as BottleGAN should be considered a federated semi-supervised learning (FSSL) algorithm due to its capability to include unlabeled clients, we also compare against the naive combination of FedAvgM and the state-of-theart semi-supervised learning algorithm FixMatch [28].",2,positive
"Presumably, the consistency learning of FixMatch also results in some sort of client homogenization but not to the extent BottleGAN can achieve.",1,neutral
"Method IOU ECE NLL
FedAvgM 0.470 0.061 0.339 + FixMatch 0.613 0.016 0.193
BottleGAN 0.671 0.011 0.177 + FixMatch 0.671 0.013 0.180
Table 1: The IOU (↑), ECE (↓), and NLL (↓) results on the test set for all evaluated methods.",0,negative
"Consistency regularization [68, 73] or label prototypes [86] formulated on CDMS [74, 89] or data augmentation [3, 14, 48] are used to address training instabilities.",1,neutral
"More importantly, the learnable parameters of our proposed OpenPrompt are far less than the existing methods, e.g., 23.44M → 0.16 M compared with OpenMatch and 23.8M → 0.16 M compared with FixMatch Sohn et al. (2020), which are no more than 1%.",2,positive
"Therefore, we set the prediction results from student network as pseudo-labels Y̌ = {y̌i}ni=1 for supervision signals and use a threshold Sohn et al. (2020) η = 0.7 to make sure all used pseudo-labels are reliable.",2,positive
"Therefore, we set the prediction results from student network as pseudo-labels Y̌ = {y̌i}i=1 for supervision signals and use a threshold Sohn et al. (2020) η = 0.",1,neutral
The goal of semi-supervised learning (SSL) is to improve a model’s performance by leveraging unlabeled data Sohn et al. (2020).,2,positive
"Following Saito et al. (2021), we train two models, one using only labeled samples (Labeled Only) and the other one employing the FixMatch Sohn et al. (2020) method.",2,positive
"Confidence scores are used in many research fields, including semi-supervised learning [37, 32] and OOD detection [18].",1,neutral
"For augmentation strategies, previous works [80, 105, 78] show that composing the weak augmentation strategy for the “pivot-to-target” model (i.",1,neutral
"It includes UDA, Fix Match [4], UDA Flex and Flex Match [5].",2,positive
"This paper introduces a rebalancing method based on generated data to solve the problem of sample imbalance in industrial scenes, which is compared with solutions to imbalance problems including traditional Focal Loss [4] and Mixup [5], and new methods of decoupling in recent years [6]–[8].",2,positive
FixMatch [2] retains the pseudo labels only when the model produces a high-confidence prediction.,1,neutral
"Similar to [2], we set a confidence threshold to filter out the detected bounding boxes that have a confidence level below the threshold.",1,neutral
"In the past few years a significant progress has been made in semi-supervised image classification [1], [2], [4]–[10].",1,neutral
"Similarly, we can see the benefit of learning invariant representations in the teacher network using FixMatch and the consistency regularization as opposed to simply doing self-distillation (No FixMatch).",1,neutral
"Unlike [51, 49], we minimize the l2 distance between the strong and the weak embedding on few samples from the target distribution",1,neutral
We implement this by consistency regularization and FixMatch [51] as shown in Figure 2a.,1,neutral
The main differences between TeST and [57] are that (1) they propose to use an InfoMax loss [13] to train the teacher as opposed to consistency regularization and FixMatch [51]; (2) they propose a contrastive learning loss to initialize the student that is retrained from scratch on the target data as opposed to directly updating the available pre-trained student network.,2,positive
"adaptation method that utilizes consistency regularization to learn invariant representations [51, 49], self-distillation using a student-teacher framework to learn robust representations from the test-data for adaptation [59], and entropy minimization to produce confident predictions on the novel test data [58].",1,neutral
"Following the standard SSL evaluation protocols [3], [10], [11], we evaluate our method on four popular benchmark image classification datasets: CIFAR-10, CIFAR-100, SVHN and STL-10.",2,positive
"Accordingly, there has been a growing trend in the applications of SSL ranging from image classification [3], object detection [4], semantic segmentation [5], image translation [6] to gait recognition [7].",1,neutral
"Our evaluation is carried out with 4/25/400 labeled samples per class on CIFAR-10, 4/25/100 labeled samples per class on CIFAR-100 and SVHN, 100 labeled samples per class on STL-10 following the setting of [3].",0,negative
"Recently, FixMatch [3] is proposed to combine pseudo-labeling and consistency regularization to boost performance.",1,neutral
", aggressive data augmentation) which induces extra inductive bias could further improve the performance, resulting in much more competitive models like Unsupervised Data Augmentation (UDA) [11], ReMixMatch [12] and FixMatch [3].",2,positive
"Among existing SSL frameworks, consistency regularization-based models have attracted much attention due to their surprising performance [3], [8], [9], [10], [11], [12].",1,neutral
"This formulation coincides with the consistency regularization adopted in the state-of-the-art methods [3], [10], [11], [12], which forces the prediction to be invariant toward noise/augmentation over input examples.",1,neutral
"Specifically, the mean and variance of error rates for 5 different “folds” of labeled data are provided as [3] reported.",1,neutral
"Following previous work along the direction of consistency regularization [3], [11], each augmented data xi is often obtained by performing an operation τi upon x, which is a combination of multiple atomic class-invariant data transformations such as solarization, rotation, and translation.",1,neutral
Here we choose the state-ofthe-art method FixMatch [3] as a base model to instantiate our framework.,2,positive
(ii) A parallel mechanism is inspired by the application of pseudo-labeling schemes in semisupervised learning [19].,1,neutral
"To improve the representation ability of SoLar, we further involve consistency regularization [19] and Mixup [25] techniques on reliable examples; see Appendix B.",1,neutral
"Similar to [25], we infer pseudo labels from weak augmentation of data and simultaneously enforce consistency with the corresponding strong augmentation.",1,neutral
Our motivation comes from success of applying similar strategy in computer vision tasks [25].,2,positive
"In parallel with KD, SSL techniques based on consistency regularization (CR) have become popular in image domain [24, 25, 26].",1,neutral
"FixMatch (FMM) was recently proposed for image recognition tasks, and showed superior performance while significantly simplifying existing SSL methods [15].",2,positive
"Recent SSL approaches which employ consistency regularization on unlabelled data show improved results [12, 13, 14, 15, 17, 18].",1,neutral
"An appealing way to this weakly supervised problem is the self-training method depending either on contrastive learning with data augmentation [57,44,41] or on additional data pretraining [47,52].",1,neutral
"FixMatch is a combination of two approaches to semi-supervised learning: consistency regularization and pseudo-labeling (Sohn et al., 2020).",1,neutral
"A contrastive loss can also be applied to OpenMatch to improve the accuracy and speed of the FixMatch training process (Sohn et al., 2020).",1,neutral
"The learning related to these pseudo inliers is called FixMatch (Sohn et al., 2020), and there is another corresponding",1,neutral
"The success of SSL in image classification [56, 57] has been recently extended to video classification [16, 17, 18, 19, 20, 58, 59].",1,neutral
"In this way, we set up three groups of experiments with 50, 200, and 500 labeled examples, which is a commonly used settings (Huang et al. 2020; Wei and Zou 2019; Sohn et al. 2020).",2,positive
"The teacher model is updated from the student model using exponential moving average, and box proposals are generated using FixMatch [11].",2,positive
"Conventionally, the partial images (PI) setting is well studied in image classification [27] and object detection [28,19].",1,neutral
", (Sohn et al. 2020), supervised training also is applied to both (X; θV iT ) and f(X; θCNN ) for the limited labeled data.",1,neutral
Consistency regularization is widely applied for semi-supervised segmentation (Sohn et al. 2020).,1,neutral
"Lastly, similar to other SSL approaches, e.g., (Sohn et al. 2020), supervised training also is applied to both (X; θV iT ) and f(X; θCNN ) for the limited labeled data.",1,neutral
"3.3 presents our modification of the multi-view similarity loss [1], used in SSL, to the SSDA setting.",2,positive
"Semi-Supervised learning (SSL) [1,36,3,45] has already proven to be highly efficient in terms of performance per annotation and thus provides a more economical way to train deep learning models.",1,neutral
"CDAC [40] uses confident predictions of the weakly augmented unlabeled samples as pseudo-labels for the strongly augmented views of the same sample, similar to FixMatch [36].",1,neutral
"Therefore, recent SSL algorithms [23], [32] use pseudo-labeling as a component.",1,neutral
"Following [23], these pseudo labels whose values are larger than the threshold τ are selected as the hard labels.",1,neutral
"Whereas, FixMatch [23] achieves EntMin with threshold pseudo-labeling.",1,neutral
8 Semi-Supervised FixMatch [39] – – 71.,1,neutral
"Table 7 Evaluation of different learning techniques on CIFAR-10
Learning type Method 1% labels 5% labels
Top-1 Top-5 Top-1 Top-5
Supervised SimCLR [16] 48.3 75.5 65.6 87.8 Semi-Supervised FixMatch [39] – – 71.5 89.1 Self-supervised PIRL [15] 30.7 57.5 60.4 83.7 PCL [21] – 75.6 – 86.2 Proposed SWIN-TCSSL 73.5 89.9 88.1 96.9
Note: The values are as reported in the respective works and (−) indicate values not reported in the manuscript",0,negative
"Also from Tables 6, 7 and 8, it is clear that SWIN-TCSSL takes advantage of the Swin-T based feature extraction, MMI based clustering, and back propagation of features and outperforms when compared to the SOTA systems [10, 39].",2,positive
"Inspired from [17], [21], [22], the matching between predictions on weakly and strongly augmented images is utilized to screen out noisy samples considering it can well separate noisy samples from clean samples of tail classes.",1,neutral
"For the final annotations, we chose to keep the soft-labels and not round them, as utilizing soft-labels for model training was shown to be a more robust approach when dealing with noisy data [70].",2,positive
"For each crop, each augmented version is then processed by a PAI network and the results are averaged such that the output is robust to noise [70].",1,neutral
"approach that is actively used in semi-supervised learning [1, 10, 15, 19].",1,neutral
Applying semi-supervised learning algorithms [11] to scenarios j and k in Fig.,1,neutral
"posed [36, 37] to achieve state-of-the-art recognition performance [38, 39].",0,negative
"Compared to FixMatch, PercentMatch has three main modifications addressing the special challenges in multi-label classification problems.",2,positive
"Some modern SSL methods have shown that the combinations of these two paradigms produce outstanding results for singlelabel classification tasks [30, 22, 35].",1,neutral
"We notice that PercentMatch gains bigger improvement when the labeled ratio is big comparing to FixMatch, partially due to the fact that higher labeled ratio leads to earlier involvement of unlabeled loss by exceeding the uniform Υ0 we use for all experiments.",2,positive
"As we cannot find any reported MS-COCO benchmark on multi-label semi-supervised classification, we re-implement FixMatch using the same hyperparameters, except percentile thresholds κ± are replaced with fixed confidence thresholds τ+ = 0.95 and τ− = 0 following the original paper.",2,positive
"It is worth mentioning that UPS is much slower than FixMatch and our method, as it performs a big number of inference on each sample for uncertainty calculation and the training process need to be repeated for multiple generations of teacher-student cycles.",2,positive
"In single label classification tasks, a widely adopted solution is selecting unlabeled samples whose maximum class score exceeds a high score threshold, which could be fixed [13, 22] or varying during training [35].",1,neutral
"To address these challenges, we propose PercentMatch, an percentile-based
dynamic thresholding framework that maintains the simplicity of FixMatch and naturally introduces dynamical score thresholds for positive and negative pseudo-labels.",2,positive
"The proposed PercentMatch can consistently outperform FixMatch on the mAP score, confirming that the dynamic thresholding is important to the overall performance.",2,positive
"UDA [30] and FixMatch [22] perform a weak stochastic augmentation and a strong augmentation on unlabeled data, and introduce a fixed high score threshold to select only pseudo-labels whose confidence scores exceed the threshold.",2,positive
"At each training step, PercentMatch adds a negligibly small computational cost compared to FixMatch, mainly due to updating the score histogram vectors for unlabeled mini-batch and calculating dynamic unlabeled loss weights.",2,positive
"Inspired by SSL methods for image classification [1,23,39,45,54,52,11,14,40], recent works on semi-supervised object detection (SSOD) [41,29,55,44] have applied the self-training method, which trains the model with the pseudo-labels of the unlabeled data.",1,neutral
"(b) While most of the self-training methods [40,29] rely on thresholding for filtering unreliable pseudo-labels, some OOD objects in unlabeled images are mispredicted as inlier objects with high confidence.",1,neutral
"Recent successful methods on semisupervised learning for image classification have applied various data augmentation and consistency regularization on unlabeled data [1,23,39,45,54,52,11,14,40].",1,neutral
(13) accuracy = Number of correct predictions,0,negative
"For CIFAR-10 and CIFAR-100, we perform a strong augmentation (containing Cutout [46] and random horizontal flip) for each training image as done in [47] 3.",2,positive
"Following choices in original implementations, each method is trained using either Adam with fixed learning rate or SGD with a cosine-annealing schedule for learning rate [Sohn et al., 2020].",2,positive
"…2014, Kumar et al., 2017, Nalisnick et al., 2019], the dominant approaches for semisupervised training of deep image classifiers today continue to modify standard objectives for discriminative neural nets by adding a regularization term using unlabeled data [Miyato et al., 2019, Sohn et al., 2020].",1,neutral
"Using a large unlabeled set, the FixMatch SSL method [Sohn et al., 2020] delivers error below 2.5%, while even more recent work has pushed below 2% [Xu et al., 2021, Han et al., 2020].",2,positive
"…2017], Pseudo-Label [Lee, 2013], Mean-Teacher [Tarvainen and Valpola, 2017], Virtual Adversarial training (VAT) [Miyato et al., 2019], and FixMatch [Sohn et al., 2020] all fit this objective, with variations in (1) the choice of function for `U ; (2) how data augmentation may alter images x; (3)…",2,positive
"We found that several baselines were notably improved using the cosine-annealing schedule of learning rate suggested by [Sohn et al., 2020].",2,positive
"…performance of perfect filtering, such as class imbalance even among known classes in the unlabeled set [Kim et al., 2020, Lai et al., 2022], sensitivity to hyperparameters [Su et al., 2021, Sohn et al., 2020], or perhaps how unlabeled data may affect the training via batchnorm [Zhao et al., 2020].",1,neutral
"We also integrate a semi-supervised method, FixMatch [26], into our framework to explore the useful information in the abandoned noisy samples, which has significantly advanced the model performance for noise-robust learning.",2,positive
We set the trade-off coefficient λ in loss function as 1 and the threshold c in FixMatch as 0.95 following [26].,1,neutral
"Our framework is flexible that can easily integrate the semi-supervised technology (SSL) to further boost the generalization performance, denoting SFT+ (with FixMatch) or SFT+∗ (with MixMatch).",2,positive
"To further exploit the useful knowledge in the discarded examples, we adopt the idea of semi-supervised learning and incorporate FixMatch into our SFT.",2,positive
"Hence, to further explore the knowledge in the discarded noise set, we introduce FixMatch [26] to the main learning stage.",2,positive
"Since FixMatch is play-and-plug for SFT, we denote Self-Filtering with FixMatch as SFT+ in the following section.",2,positive
"Typical transformation strategies can range from simple data augmentation [37], [39], [46], to more complex transformations such as adversarial perturbations [48], [49], [74], [81], rotations [123] and patch reordering [121], autoencoding transformations [200], [201] and automated augmentation [27], [38], [51].",1,neutral
"Later on, FixMatch [38] unifies multiple augmentation strategies including Cutout [78], CTAugment [27], and RandAugment [77] and produces even more strongly augmented samples as input.",2,positive
"While our review mainly covers generic semi-supervised learners for image classification [26], [27], [37], [38], the ideas behind thembe generalized to solve other vision recognition tasks.",2,positive
"Although entropy minimization is originally proposed for logistic regression to impute the labels of samples classified with high confidence [92], it is later extended to train deep neural networks in SSL setting by minimizing the entropy of the class assignments either derived in the prediction space [26], [27], [38], [49], [56], [93] or the feature space [57], as detailed next.",1,neutral
"Similarly, FixMatch [38] assigns the one-hot labels only when the confidence scores of the model predictions are higher than a certain threshold.",1,neutral
"Automated augmentation ReMixMatch [27], UDA [51], FixMatch [38]",2,positive
We compare our PLMCL with the existing semi-supervised models such as FixMatch [32] and UPS [30] on COCO dataset.,2,positive
", data augmentation and stochastic regularization, to produce two versions for the same image, and (iii) use the prediction of one image-version after applying the threshold as the pseudo label for the other image-version [3,32,36].",1,neutral
"Most of semi-supervised learning (SSL) methods on multi-class classification, while a few of them study multi-label classification [1,3,6,25,23,28,30,32,36,38,37].",1,neutral
This is because FixMatch and UPS are trained based on the supervised loss over the set of observed labels while ignoring the unobserved labels of the labeled set as explained in subsection 2.2.,1,neutral
"We also adapt some semi-supervised models to be applicable for multi-label classification problems, by replacing Softmax layer with Sigmoid, that are designed for multi-class classification, such as [32].",1,neutral
FixMatch [39] takes advantage of the weak augmentation to generate the pseudo labels and supervise the images with strong augmentation.,1,neutral
"Since our method is based on FixMatch [55], we first briefly review its core idea (§3.",2,positive
"Moreover, our framework, along with its precedents, such as the FixMatch serials [55, 6, 5, 72, 61] and UDA [64], heavily relies on the pseudo labeling quality on unlabeled images.",2,positive
"To pursue elegance and effectiveness meantime, we adopt the weak-to-strong consistency regularization framework from FixMatch [55], which can be end-to-end trained in a single stage with a single model.",2,positive
"Here, we further examine the superiority of our UniMatch compared with its FixMatch baseline [55].",2,positive
"Among them, FixMatch [55] proposes to inject strong perturbations to unlabeled images and supervise training process with predictions from weakly perturbed ones to subsume the merits of both methodologies.",1,neutral
"In this work, we focus on the weak-to-strong consistency regularization framework, that is popularized by FixMatch [55] from the field of semisupervised classification, and then impacts many other relevant tasks [41, 57, 66, 45, 63, 67].",1,neutral
"We tackle the pseudo-label noise problem which may cause severe performance degradation (Sohn et al. 2020) by
confidence thresholding (τ).",2,positive
"Recently, SSL has shown promising outcomes in improving model performance and is receiving growing attention of the computer vision research community (Van Engelen and Hoos 2020; Sohn et al. 2020).",2,positive
"Traditionally, SSL can be approached with adapting state-of-the-art (SOTA) image classification methods such as method proposed by Sohn et al. (2020) for object detection.",1,neutral
"This simple idea, however, shows surprising performance enhancements in various applications, including image object recognition [16, 25, 39, 63, 70], semi-supervised learning [4, 58] self-supervised learning [35, 40, 42], noisy label training [43], meta-learning [69], semantic segmentation [10, 20], natural language understanding [24, 34], and audio processing [37, 48, 49].",1,neutral
"In the second category, there are semi-supervised approaches, such as MixMatch [1], FixMatch[56] and FlexMatch[77], and others [1, 41, 48, 52, 69] that use pseudo-labeling or selflabeling, where unlabelled data is assigned pseudo-labels.",1,neutral
"After sample selection is conducted for each class, we adopt semi-supervised learning [1, 42] to train with all clean samples D as labeled data and all noisy samples D as unlabeled data.",2,positive
"These observations, while not intuitive at first, are not surprising given the wellestablished results in the pseudo-labeling and unsupervised domain adaptation literature [27], [28], [29], [30], [31], [32] that has already shown the effective use of labeled data from another source domain to evaluate neural nets intended for a different target domain.",1,neutral
"Typical setting of semi-supervised learning is few-shot classification, where unlabeled data are utilized because labeled data are insufficient to learn a meaningful classifier [15,19].",1,neutral
"The semisupervised methods include our baseline based on WideResNet28-2 model with limited data, MixMatch [33], MarginMix [34] and Ada-CM [35], all of these methods are for scenarios where only part of the label data is available.",2,positive
"In this experiment, we mainly compare our recognition performance with MixMatch and MarginMix methods, which selected the WideResNet-28-2 as the backbone.",2,positive
"In Table III, compared with MixMatch and MarginMix in Table III, our method achieved an accuracy improvement of about 10%.",2,positive
"In this supplementary material, we additionally provide the benchmark results of FixMatch [5]-based ConMatch (ConMatch with FixMatch) on SVHN and STL-10 datasets.",2,positive
"Class-wise Results with Other SSL Techniques In Table 2 and Table 3 of the main paper, we demonstrated that semi-supervised learners [5, 6] combined with ConMatch outperform their baselines by a significant margin in most SSL benchmark settings.",2,positive
", FixMatch [5] and FlexMatch [6] as demonstrated in the Sec 4.",1,neutral
"Per-class quantitative evaluation on ConMatch and base semisupervised learners [5, 6] on CIFAR-10 with 40 labels.",1,neutral
ConMatch w/ [5] achieves performance improvement over FixMatch in all the classes except for the dog class.,2,positive
"SVHN STL-10 Method 40 250 1,000 FixMatch [5] 3.",1,neutral
"19% in CIFAR-10 40 labels for a fair comparison between FixMatch [5], FlexMatch [6], and ConMatch",2,positive
This verifies that ConMatch w/ [5] effectively alleviate the confirmation bias by making use of confidence estimator.,1,neutral
"ConMatch w/ [5] on the other hand, records high accuracy score for all classes including cat class.",0,negative
"Specifically, we analyze the time taken to achieve the best accuracy of FixMatch [5], 86.",1,neutral
"Consistency [3, 37, 39] and contrastive learning [16, 6] are two popular strategies.",1,neutral
"Pseudo label is a popular choice for entropy minimization in semi-supervised learning, which can be either hard (one-hot) labels [28, 14, 37, 21, 8, 18] or soft labels [3, 44, 7, 47].",1,neutral
"It may be obtained from the same network used in training [3, 37], a temporal ensemble network using exponential moving",1,neutral
"Aspect-based Sentiment Analysis (ABSA) contains a set of various subtasks (Pontiki et al., 2014; Liu et al., 2015; Tang et al., 2016; Wang et al., 2016; Ning et al., 2018; Zhao et al., 2020; Wu et al., 2020a).",1,neutral
"TC-BiLSTM (Fan et al., 2019) follows the design of the work for target-oriented sentiment classification (Tang et al., 2016) and concatenate an opinion target embedding for each word position to perform sequence labeling.",2,positive
"Semi-Supervised learning algorithms [7] are often proclaimed to work efficiently in that setting [30, 31, 37, 48], some methods showing impressive performance with a quantity of labels as low as one per class for CIFAR10 [44], or 1% of labeled data for ImageNet [54].",1,neutral
We refer the reader to [44] for details.,2,positive
This semi-supervised model [44] feeds an unlabeled image to a weak augmentation and a strong augmentation: the weakly augmented input is used for pseudo-labeling and the strongly augmented input is used for computing a cross-entropy loss against the pseudo-label.,1,neutral
"MeanTeacher [11] 0.9275 0.9243 0.9260 0.9246 0.9138
MixMatch [16] 0.9317 0.9271 0.9306 0.9271 0.9167
FixMatch [13] 0.9239 0.9181 0.9240 0.9185 0.9069
VAT [12] 0.9214 0.9090 0.9114 0.9096 0.8967
VATNM [14] 0.9315 0.9268 0.9300 0.9177 0.9053
GLM [45] 0.9450 0.9420 0.9490 0.9465 0.9410
TSC [46] 0.9500 0.9499 0.9501 0.9483 0.9427
Ours 0.9528 0.9506 0.9518 0.9507 0.9436
The bold text indicate its performance outperforms other methods
1 3
regularization is integrated into VAT.",0,negative
[13] employed consistent regularization and pseudo-label to re-explore SSL.,1,neutral
"Our proposed model is compared with several state-ofthe-art semi-supervised methods, including PL [7], VAT [12], VATNM [14], MeanTeacher [11], MixMatch [16], FixMatch [13], GLM [45], and TSC [46].",2,positive
"In addition, we compared the performance of our semi-supervised method with other consistent-learning methods (MeanTeacher, MixMatch, and FixMatch) in different confidence (τ ) to investigate whether the high confidence improves performance.",2,positive
FixMatch [13] proposed the weak and strong data augmentation on unlabeled data sharing the same pseudo-label.,1,neutral
"Although PL achieved good results in our tasks, the results were not as good as MeanTeacher, MixMatch, and FixMatch, indicating that consistent-learning is important in semi-supervised learning.",2,positive
"Teacher-student framework has been widely used in semi-supervised learning (SSL) [46,51,48,32,12], where the predictions of the teacher model on unlabeled samples serve as pseudo-labels to guide the student model.",1,neutral
"…2017] to obtain pseudo labels for unlabelled data where a model is expected to make consistent predictions on an unlabeled instance and its augmented versions, cf. [Miyato et al., 2019, Xie et al., 2020a, Verma et al., 2019, Berthelot et al., 2019, Sohn et al., 2020, Zhu et al., 2021, inter alia].",2,positive
"Therefore, Mean Teacher using mean square error (MSE) could be more suitable than FixMatch for the proposed spatiotemporal consistent learning framework.",1,neutral
"However, MixUp provides a smoother estimate of
uncertainty, limiting the training of FixMatch.",1,neutral
"We also experiment two deep learning-based semi-supervised frameworks (DCGAN [18] and FixMatch [29]), which have been investigated in previous semi-supervised traffic classification works.",2,positive
"FixMatch [29] is another advanced method for SSL, which has achieved state-of-the-art performance across a variety of standard SSL benchmarks by combining consistency regularization and pseudo-labeling.",2,positive
"For fair comparison of different semi-supervised frameworks, we reimplement Original Mean Teacher and FixMatch with the proposed FlowFormer model.",2,positive
FixMatch treats highly confident predictions of teacher model as pseudo labels to guide the training of student model.,2,positive
Pseudo-labeling by a teacher model is proven to be a successful technique in semi-supervised learning [38].,1,neutral
"Subsequent methods can be classified as the variants of Π model, where the difference lies in enforcing the consistency between model perturbation [36], data perturbation [37, 29], and exploiting unlabeled data [20, 21].",1,neutral
"SSL has made remarkable progress in recent years [16, 17, 18, 19, 20, 21], yet there are still several limitations with the popular evaluation protocol in the literature [22, 20, 21].",2,positive
"Specifically, as shown in Table 1a, it takes about 335 GPU days (279 GPU days without ImageNet) to evaluate FixMatch [20] with TorchSSL [21].",2,positive
"We implement 14 SSL algorithms in the codebase for USB, including Π model [35], Pseudo Labeling [59], Mean Teacher [36], VAT [37], MixMatch [28], ReMixMatch [23], UDA [29], FixMatch [20], Dash [24], CoMatch [60], CRMatch [61], FlexMatch [21], AdaMatch [62], and SimMatch [47], all of which exploit unlabeled data by encouraging invariant predictions to input perturbations [13, 14, 63, 64, 65, 66, 67].",2,positive
"Training cost is estimated by using FixMatch [20] on a single NVIDIA V100 GPU from Microsoft Azure Machine Learning platform, except for ImageNet where 4 V100s are used.",2,positive
", CIFAR-10/100, SVHN, STL-10, and ImageNet classification [22, 23, 20, 24, 21], as summarized in TorchSSL [21]), precluding consistent and diverse evaluation over tasks in natural language processing (NLP), audio processing (Audio), etc.",1,neutral
"We highly recommend reporting ImageNet [8] results since it is a reasonable dataset for hill-climbing [20, 47, 21].",0,negative
"CIFAR-10 [39] and SVHN [46] in TorchSSL are not included in USB because the state-of-the-art SSL algorithms [29, 20, 24] have achieved similar performance on these datasets to fully-supervised training with abundant fully labeled training data 5.",2,positive
"Following [20, 21], validation data are not provided for CV datasets.",0,negative
"In FixMatch [15], the method first generates pseudolabels using the model’s predictions on weakly-augmented unlabeled images, and is then trained to predict the pseudolabel when fed a strongly-augmented version of the same image.",1,neutral
"To tackle this task, StyleMatch [14] is proposed via extending the FixMatch [15] with a couple of new ingredients, which resorts to enhancing the diversity from the image level and the classifier level.",2,positive
FixMatch is a significant simplification of existing SSL methods [15].,2,positive
"In our method, we merely use the cross-entropy loss to train our model as in FixMatch [15].",2,positive
", MeanTeacher [18], EntMin [16], DebiasPL [21], FlexMatch [22], FixMatch [15]) and the semi-supervised DG method (i.",1,neutral
"The semi-supervise learning has achieved the remarkable performance in the recent years [16], [17], [18], [24], [25], [15], [26], [27], [19], [20], [21], [22], [28].",1,neutral
"Inspired by the theory of the multi-domain learning, we extend the FixMatch [15] 1 to a multi-task learning method, named MultiMatch, for semi-supervised domain generalization.",1,neutral
"The recent works can be roughly clustered into two groups, consistency-based [37, 58, 46, 66, 62] and pseudo-labeling based [38, 51, 49, 10].",1,neutral
2 [51] of EMAN-ResNet50 [10] when trained from scratch for 100 epochs.,0,negative
"Under the pseudo-labeling based SSL framework [38, 51, 49, 10], given a unlabeled sample and its pseudo label (x, ŷ), only when its confidence o is not smaller than the confidence threshold τ , it will contribute to loss Lu, as seen in (2).",1,neutral
"example, as discussed in [64], the direct application of FixMatch [51], one of the most popular SSL methods, to ViT leads to an inferior performance (about 10 points worse) than when used with a CNN architecture.",1,neutral
FixMatch [51] emerged as a popular SSL method in the past few years.,1,neutral
"This idea becomes popular in SSL recently [38, 51, 49, 10, 67].",1,neutral
", FixMatch [51]; 2) The model is un/selfsupervised pretrained first and finetuned on the labeled data later [25, 14, 22]; 3) The model is self-supervised pretrained first and then semi-supervised finetuned on both labeled and unlabeled data [10].",2,positive
"Although several recent methods in SSL have significantly advanced the field [37, 58, 7, 51, 66, 10, 49], the transfer of these methods from Convolutional Neural Networks (CNN) to ViT architectures has yet to show much promise.",1,neutral
"At the final stage of semi-supervised fine-tuning, we adopt the EMA-Teacher framework [58, 10], an improved version over the popular FixMatch [51].",2,positive
", at the end of every training epoch, but for online pseudo-labeling [51, 10] the teacher model is updated continuously along with the student.",1,neutral
"In this work, we attempt to complement the existing regularization techniques with an idea from the fields of self-supervised learning and consistency regularization [6, 15, 41, 48].",1,neutral
"Lately, confidence-based pseudo-labeling [23,21] and distribution alignment [4,2,21,10] have been introduced to SSL, boosting the performance to a new height.",1,neutral
"Under this assumption, we can use Cx to guide the prediction for u by distribution alignment [2,21], which can improve the performance of consistentency-based or pseudo-labeling based methods [2,23,21,10].",1,neutral
", convert model’s predictions to hard labels to reduce entropy) [20,23,21,28].",1,neutral
"Following [23], RandAugment [7] is used for strong augmentation.",1,neutral
"RDA exploits all unlabeled data for training, whereas previous consistency-based methods waste low-confidence data [23,21,27].",1,neutral
"Recent SSL algorithms like [23,21,27,31] set a confidencebased threshold to refine the pseudo-labels and obtain outstanding performance.",1,neutral
"Following [23], we integrate consistency regularization into RDA.",1,neutral
"Previous entropy minimization based methods like [23,21,27] achieve superior performance in SSL by pseudo-labeling.",1,neutral
"We compare RDA mainly with three recent state-of-the-art SSL methods: (1) FixMatch [23], combining consistency regularization and entropy minimization; (2) FixMatch with distribution alignment [2]; (3) CoMatch [21], combining graphbased contrastive learning and consistency regularization.",2,positive
"Recently, FixMatch [23] utilizes the confidence-based threshold to select more accurate pseudo-labels and proves the superiority of this technique.",1,neutral
"Consistency regularisation methods [24,23,2,3,27] make the networks invariant to perturbations at the input level, the feature level [18,8] or the network architectural level [6].",1,neutral
"Labelled 2D U-net Self-Loop FixMatch CPS SegPL SegPL+VI Slices [22](2015) [15](2020) [23](2020) [6](2021) (Ours, 2022) (Ours, 2022) 50 54.",2,positive
One of the predictions is then used as a pseudo-label to supervise the other prediction [23].,1,neutral
"We compared SegPL with state-of-the-art consistency based methods: 1) “cross pseudo supervision” or CPS [6], which is considered the current state-of-the-art for semi-supervised segmentation; 2) another recent state-of-the-art model “cross consistency training” [18], denoted as “CCT”, due to hardware restriction, our implementation shares most of the decoders apart from the last convolutional block; 3) a classic model called “FixMatch” (FM) [23].",2,positive
"Labelled 3D U-net FixMatch CCT CPS SegPL SegPL+VI Volumes [22](2015) [23](2020) [18](2020) [6](2021) (Ours, 2022) (Ours, 2022) 2 56.",2,positive
"The categories of semi-supervised learning methods include unsupervised pretraining followed by fine-tuning [7,19,28,56], consistency regularization [26,48,53], pseudo labeling [18, 29, 55], and combination of these methods [2, 51, 60].",1,neutral
"It is probably because the AL and SSL methods rely on the i.i.d. assumption, and the SSDG method does not label and exploit the important source data.",2,positive
"But most of the SSL methods rely on the i.i.d. assumption, which can impair their generalization performance under domain shift.",1,neutral
"SSL [3, 4, 23, 50, 52] is a practicable way to use both labeled and unlabeled data.",1,neutral
Semi-Supervised Learning (SSL).,1,neutral
"Active learning (AL) [1, 19, 24, 25, 45, 57] and semi-supervised learning (SSL) [3, 4, 50, 52] provide possible solutions to the introduced LEDG task.",1,neutral
"CEG vs AL, SSL, SSDG methods.",1,neutral
"Since the DG methods may not be good at tackling the label-limited task as they can only use the labeled data, we further compare our CEGmethod with AL, SSL, and SSDGmethods.",2,positive
"We further utilize the unknown and known regions by adopting augmentation consistency constraint [50] for the unlabeled data and prediction supervision for the labeled data, respectively.",2,positive
"Semi-supervised domain generalization (SSDG) [36, 47, 59, 65, 70] tackles domain shift under the SSL setting.",1,neutral
95 as in [50]) selects high dependable data.,1,neutral
"However, the existing AL and SSL methods mostly depend on the i.i.d. assumption, hence may not be favorably extended to the generalization scenarios under distinct domain shifts.",1,neutral
"(3) Semi-supervised learning (SSL): MeanTeacher [52], MixMatch [4], and FixMatch [50].",1,neutral
AL aims to query the labels of high-quality samples and SSL leverages the unlabeled data to improve performance with limited labeled data.,2,positive
We set the weights of Lce and Lac to 1 as in [50].,1,neutral
", 2014) and random data augmentations (Sohn et al., 2020; Sajjadi et al., 2016) are nameda few.",1,neutral
"Although Semi-supervised deep learning approaches were not well explored for defect inspection in automated manufacturing, they are heavily explored in computer vision and machine learning literature for various problems such as image recognition (Berthelot et al., 2019; Sohn et al., 2020), Natural LanguageProcessing (Liang, 2005), etc.",1,neutral
"…they are heavily explored in computer vision and machine learning literature for various problems such as image recognition (Berthelot et al., 2019; Sohn et al., 2020), Natural LanguageProcessing (Liang, 2005), etc.Consistency regularization and Pseudo-labeling are two prominent categories of…",1,neutral
"Hence, only the pseudo-labels with high confidence predictions (confidence above a threshold) were widely considered for better training (Rizve et al., 2021; Sohn et al., 2020).",2,positive
"Several methods were used to produce perturbed inputs, Dropout (Srivastava et al., 2014) and random data augmentations (Sohn et al., 2020; Sajjadi et al., 2016) are nameda few.",1,neutral
"Usually the quality of the pseudo-labels are more important than their quantity (Sohn et al., 2020).",1,neutral
"Once we have identified clean samples and relabeled noisy ones, we leverage off-the-shelf and well-established techniques, such as mixup regularization [50] and consistency regularization [36], to perform further SSL-based model training.",2,positive
Label consistency is a good choice to achieve this goal because it encourages the fine-tuned model to produce the same output when there are minor perturbations in the input [36].,1,neutral
"To better understand the performance of the NCLC step in label correction, we replace NCLC with an existing label correction scheme, called Confidence Thresholding (CT) [36], which relabels such samples whose pseudo-labels have a confidence value exceeding a predefined threshold, e.",1,neutral
"[0, 1) ShearX [1, 2) ShearX ShearY [2, 3) ShearX TranslateX [3, 4) ShearX TranslateY [4, 5) ShearX Rotate [5, 6) ShearX Solarize [6, 7) ShearX Posterize [7, 8) ShearX Contrast [8, 9) ShearX Color [9, 10) ShearX Brightness [10, 11) ShearX Sharpness [11, 12) ShearX AutoContrast [12, 13) ShearX Invert [13, 14) ShearX Equalize .",1,neutral
"2699, Qianjin Street, Chaoyang District, Changchun City, 130012, Jilin Province, China applied in the various paradigms of supervised learning [8, 9], semi-supervised learning [10, 11], and unsupervised learning [12–14].",1,neutral
"Such training can lead to huge improvements [1,3,15,19,21,22,2], and unlabeled data can often ar X iv :2 20 8.",1,neutral
"Still, in most cases the evaluation of those methods [1,3,11,15,19,21,22,2] is done using well established datasets like ImageNet [6] which mainly include natural images in a common setting, in which the accumulation of data is comparatively easy in contrast to more domain-specific tasks which include more restrictive data.",1,neutral
FixMatch [19] combines pseudo-labeling and consistency regularization in a quite simple yet effective approach.,1,neutral
"Most modern Semi-Supervised Learning methods [1,3,11,15,19,21,22,2] are mostly evaluated on the ImageNet dataset[6] or similar natural image datasets like CIFAR-10 [10] and CIFAR-100 [10], where the classification task is to recognize full objects in real life images.",1,neutral
"One possible solution to avoid an expensive annotation process is utilizing Semi-Supervised Learning [1,3,11,15,19,21,22,2] that requires labeling for only a small fraction of the data and combines training from labeled data with learning from large-scale unlabeled data.",1,neutral
"Recently, many semi-supervised learning methods [1,3,11,15,19,21,22,2] emerged implementing different algorithms to set new state-of-the-art results on a vari-",1,neutral
One of the state-of-the-art Semi-Supervised Learning methods for image classification that has recently been published is FixMatch [19].,1,neutral
"Recently many Semi-Supervised Learning methods [1,3,11,15,19,21,22,2] have been published and evaluated.",1,neutral
"Integrating DADA augmentation learning into FixMatch: For the augmentation training, the FixMatch architecture [19] was adapted to perform the additional augmentation learning step using bi-level optimization after each regular update step.",2,positive
"We combine FixMatch [19], a recently published method for Semi-Supervised Learning , with the augmentation learning approach of DADA [13].",2,positive
"Inspired by [20, 26], we assume that a prediction label owns high confidence if the model assigns a high probability to one of the possible classes.",1,neutral
The slowest algorithm (FixMatch) demands a factor of approximately 2.55 higher time duration in comparison with the fastest algorithm (Pseudo-label).,2,positive
"8) FixMatch [17]: This algorithm first checks whether the model’s prediction on weakly augmented unlabeled samples is above a fixed threshold for any class and if so, the prediction is converted to hard pseudo-labels.",1,neutral
"The algorithms FlexMatch, UDA, RemixMatch, and FixMatch perform the best compared to other algorithms for the same number of batch iterations.",2,positive
"However, given the existing resource constraints on the edge devices, other simpler SSL algorithms will gain preferable advantages, like FixMatch, Pseudo-label MeanTeacher, or πmodel.",2,positive
"9) FlexMatch [18]: Instead of employing a pre-defined constant threshold for all classes, the FlexMatch method improves FixMatch by using Curriculum Pseudo Labeling (CPL), which leverages unlabeled input according to the model’s learning status.",2,positive
"On the other hand, UDA, FixMatch, and FlexMatch algorithms use almost double the memory than other algorithms need.",1,neutral
"In general, advanced SSL algorithms like FlexMatch, FixMatch, or UDA may have slightly better accuracy metrics.",1,neutral
"Similar to the existing approaches, such as FixMatch [35] and DS3L [14] , we consider three widely applied datasets, CIFAR-10, CIFAR100 [23], and compare our CAPT with such existing state-of-the-art SSL approaches.",2,positive
"Currently, most of the existing SSL approaches are achieved by consistency regularization [34, 4, 35, 26], which utilizes unlabeled data and assumes that the model gets similar class distribution for an unlabeled example even if the example is fed with different augmentations; or Entropy Minimization [25], which proposes that unlabeled samples should be used to ensure the classifier’s decision boundary.",1,neutral
"Similar to the existing approaches, such as FixMatch [35] and DS(3)L [14] , we consider three widely applied datasets, CIFAR-10, CIFAR100 [23], and compare our CAPT with such existing state-of-the-art SSL approaches.",2,positive
"The success of such models heavily relies on the huge amount of data with human annotations, to reduce the expensive labeling costs for DNNs training, researchers develop the semi-supervised learning (SSL) [13, 25, 31, 4, 35], which further incorpoFigure 1: Illustration of an open-set unlabeled data may contain novel classes that have never been existing in the labeled set, the semi-supervised learning should be able to classify samples from seen classes, but also distinguish and cluster for the unseen classes.",2,positive
"Specifically, for the SSL baseline, we choose the Pseudo-Labeling [25] and recently FixMatch [35], DS3L [14]; for the discovery of novel classes, we compare the CAPT with two current SOTA approaches, DTC [16], Auto-Novel [15].",2,positive
"Specifically, for the SSL baseline, we choose the Pseudo-Labeling [25] and recently FixMatch [35], DS(3)L [14]; for the discovery of novel classes, we compare the CAPT with two current SOTA approaches, DTC [16], Auto-Novel [15].",2,positive
"There are also some mixed ideas between consistency regularization and self-training, such as FixMatch [33] and ISMT [34].",1,neutral
"Using the notations of FixMatch [1] for consistency, let X = {(xb, pb) : b ∈ (1, .",1,neutral
It is not clear if the more complex architectures provide better performance across different types of medical imaging datasets or not compared to simpler approaches such as FixMatch [1].,2,positive
It builds upon the recent progress in semi-supervised learning for classification tasks and adapts state-of-the-art FixMatch [1] to semantic segmentation tasks.,2,positive
FixMatch combines consistency regularization and pseudo-labeling to provide a simple yet powerful semi-supervised method for classification task [1].,1,neutral
"Motivated by semi-supervised methods [33], we utilize the confident predictions of each detector on easy views, to enforce consistency of the other detector on hard views.",1,neutral
"Specifically, the former aims to train the models with pseudo-labels whose prediction confidence goes beyond a hard threshold [150], [151], and the latter, on the other hand, attempts to maintain the output consistency under perturbations on data [152], [153] or model [154], [155].",1,neutral
"Compared to the baseline FixMatch (Sohn et al., 2020), our proposed L2AC results in about 4% performance gain over all evaluation metrics, and outperforms all the SOTA methods.",2,positive
"Baselines: We evaluate our L2AC based on two recent popular SSL methods, i.e., MixMatch (Berthelot et al., 2019b) and FixMatch (Sohn et al., 2020).",2,positive
"For unlabeled sample xm, its pseudo-label ŷm can be a ‘hard’ one-hot label (Lee et al., 2013; Sohn et al., 2020; Zhang et al., 2021a) or a sharpened ‘soft’ label (Xie et al.",2,positive
"The comparison methods includes: Vanilla, classifier retraining (cRT) (Kang et al., 2020), FixMatch (Sohn et al., 2020), DARP (Kim et al., 2020a) and ABC (Lee et al., 2021).",2,positive
"9, FixMatch (Sohn et al., 2020) remains to achieve high recall on the majority classes and low recall on the minority classes.",0,negative
"With pseudo-labeling techniques, current state-of-the-art SSL methods (Xie et al., 2020b; Sohn et al., 2020; Zhang et al., 2021a) generate pseudo-labels for unlabeled data to augment the training dataset.",2,positive
", 2019b) and FixMatch (Sohn et al., 2020), and compare it with the following methods: 1) The Vanilla model merely trained with labeled data; 2) Recent re-balancing methods that are trained with labeled data by considering class imbalance, including: Re-sampling (Japkowicz, 2000), LDAM-DRW (Cao et al.",2,positive
"For fair comparison, all the experiments are based on FixMatch (Sohn et al., 2020).",2,positive
"Pseudo-labeling (Lee et al., 2013; Xie et al., 2020a;b; Sohn et al., 2020; Zhang et al., 2021a) is evolved from entropy minimization (Grandvalet & Bengio, 2004) and commonly trains the model using labeled data together with unlabeled data whose labels are generated by the model itself.",1,neutral
"We evaluate the proposed L2AC upon FixMatch (Sohn et al., 2020) and compare it with the following methods: DARP (Kim et al., 2020a), CReST+ (Wei et al., 2021), ABC (Lee et al., 2021).",2,positive
"8 visualizes the confusion matrices of pseudo-labels of Fixmatch (Sohn et al., 2020) and our L2AC under the imbalance ratio γl = γu = 100.",2,positive
", 2020), FixMatch (Sohn et al., 2020), DARP (Kim et al.",2,positive
"3, our L2AC provides a relatively balanced per-class recall compared with the baseline FixMatch (Sohn et al., 2020) and other imbalanced SSL methods, e.",2,positive
"9 visualizes the confusion matrices of pseudo-labels for Fixmatch (Sohn et al., 2020) and our L2AC under the imbalance ratio γl = 100 while γu = 100 (Reversed).",2,positive
"We evaluate the proposed L2AC based on two widely-used SSL methods: MixMatch (Berthelot et al., 2019b) and FixMatch (Sohn et al., 2020), and compare it with the following methods: 1) The Vanilla model merely trained with labeled data; 2) Recent re-balancing methods that are trained with labeled…",2,positive
"We compare our proposed algorithm with the baseline model FixMatch (Sohn et al., 2020) and two stateof-the-art methods (Kim et al., 2020a) and (Lee et al., 2021), as L2AC uses the same code base with
these two methods.",2,positive
"2) pseudo-labeling based SSL methods where both labeled and unlabeled data is used (without considering class-imbalance), including: Pseudo-labels (Lee et al., 2013), MixMatch (Berthelot et al., 2019b) and FixMatch (Sohn et al., 2020).",1,neutral
"3, our L2AC provides a relatively balanced per-class recall compared with the baseline FixMatch (Sohn et al., 2020) and other imbalanced SSL methods, e.g., DARP (Kim et al., 2020a) and ABC (Lee et al., 2021).",2,positive
"Figure 3: Confusion matrices of FixMatch (Sohn et al., 2020), DARP (Kim et al.",1,neutral
"13, compared with FixMatch (Sohn et al., 2020), our L2AC certainly improves the separability of the tail classes from the head classes.",2,positive
"In contrast, our L2AC tends to a relatively equal per-class recall, especially on minority classes the performance is significantly improved compared to FixMatch (Sohn et al., 2020).",2,positive
"For unlabeled sample xm, its pseudo-label ŷm can be a ‘hard’ one-hot label (Lee et al., 2013; Sohn et al., 2020; Zhang et al., 2021a) or a sharpened ‘soft’ label (Xie et al., 2020a; Wang et al., 2021).",2,positive
"In such a general scenario, the current state-of-the-art FixMatch (Sohn et al., 2020) may suffer from performance degradation.",2,positive
"The comparison methods includes: Vanilla, cRT (Kang et al., 2020), FixMatch (Sohn et al., 2020), DARP (Kim et al., 2020a) and ABC (Lee et al., 2021).",2,positive
"Therefore, Entropy Minimization doesn’t work in some cases [102].",1,neutral
"However, when combined with other semi-supervised learning strategies, Entropy Minimization may improve the performance [107].",1,neutral
"Therefore, the strategy of using hard label in the pseudo labeling is closely related with Entropy Minimization [105].",1,neutral
Another application of Entropy Minimization is the use of hard label in the pseudo labeling.,1,neutral
"As arg max operation applied to a probability distribution can produce a valid “one-hot” low-entropy (i.e., high-confidence) distribution, both the Entropy Minimization and pseudo labeling encourages the decision boundary passing low-density regions.",1,neutral
"Moreover, four semisupervised learning methods, Pseudo Label[12], Noisy Student[33], FixMatch[29] and Meta Pseudo Label[21] are adopted to demonstrate the competitive performance of FedSA with limited labeled data.",2,positive
"Nine methods Scratch Training, Transfer, Pseudo Label[12], Noisy Student[33], FixMatch[29], Meta Pseudo Labels[21], FedAvg+[19], FedMD+[14] and FedProx+[16] are adopted as baselines in experiments to validate the effectiveness of the proposed method.",2,positive
"In particular, Fixmatch surpasses the baseline by 1.90 to 2.95 mAPH.",2,positive
We compare our STE and CBV modules with the threshold-based method FixMatch [24].,2,positive
"Visualization of pseudo labels produced by our STE and CBV modules, or by threshold-based FixMatch [24].",2,positive
"For instance, FixMatch [24] directly uses the classification score to filter the pseudo boxes.",1,neutral
13 100% (∼ 80k Labels) Fixmatch [24] +2.,1,neutral
74 50% (∼ 40k Labels) Fixmatch [24] +2.,1,neutral
62 20% (∼ 16k Labels) Fixmatch [24] +2.,1,neutral
"Then, we compare our ProficientTeachers model with a strong competitor, Fixmatch [24].",2,positive
"Most of the SSL approaches are proposed for classification tasks [1,24] and a few SSL approaches have been proposed to leverage the object detection task [10, 25], especially 3D object detection.",1,neutral
"Since this filtering strategy derives from Fixmatch [24], we name it Fixmatch for simplicity.",1,neutral
"By contrast, our method obtains much better results than Fixmatch, i.e., improving the baseline by 4.34 to 5.35 mAPH.",2,positive
"Both Fixmatch and our ProficientTeachers achieve better results than the full-supervised baseline, which proves the advantage of semi-supervised learning.",2,positive
"…recent empirically work in DA (Long et al., 2015; 2017; Sun and Saenko, 2016; Sun et al., 2017; Zhang et al., 2019; 2018; Ganin et al., 2016; Sohn et al., 2020) focuses on settings motivated by benchmark datasets (e.g., WILDS (Sagawa et al., 2021; Koh et al., 2021), Office-31 (Saenko et…",2,positive
"Fixmatch [7] obtains a pseudo label for unlabeled data using a weak augmentation, and then uses the pseudo label to monitor the strongly augmented output values.",1,neutral
"step (instead of using a fixed global value) was better for semisupervised training [7], similar to what used in UDA [10].",1,neutral
"Recall that state-of-theart SSL methods [35,29,39] leverage both weak and strong data augmentations to the unlabeled samples during the training.",1,neutral
"Recently, FixMatch [29] achieves state-of-the-art classification accuracy via assigning the strongly augmented unlabeled samples with the pseudo labels produced from",2,positive
"Generally speaking, state-of-the-art SSL techniques [29,36,39] produce “pseudo labels” for the unlabeled samples when the model’s predictions are confident enough based on pre-defined threshold strategies.",1,neutral
"Semi-supervised learning (SSL) [16,21,3,29,36,39] aims to train accurate models via exploiting a large amount of unlabeled data when the labeled data is scarce.",1,neutral
"We follow previous work [29,36,39,28,10] and adopt testing accuracy as the evaluation metric for target model performance.",2,positive
"Instead of directly using the posteriors as a “soft” label, those SSL methods switch the posteriors into a “sharpen” [36] or “hard” label [29,39].",1,neutral
"Concretely, the state-of-the-art SSL methods [29,36,39] leverage weak augmentation to the labeled samples and trains them in a supervised manner.",1,neutral
FixMatch improves performance by up to 0.13% compared to using SWR alone (up to 2.11% increase when NSP is applied).,2,positive
"To further validate the superiority of NSP, we conduct experiments while keeping SWR but replacing NSP with FixMatch [14] using this Pytorch implementation(2) on settings (a) and (b) in Table 1.",2,positive
"B.1 Impact of Number of Source Samples in SWR
As described in Section 3.1, the penalty vector w is calculated by employing the average cosine similarity between two gradient vectors g and g′ from N source
2 https://github.com/kekmodel/FixMatch-pytorch/blob/master/train.py
samples.",0,negative
"We can find that learning only input consistency, such as applying FixMatch, is not sufficient to handle the distribution shift between source and target.",1,neutral
"To further validate the superiority of NSP, we conduct experiments while keeping SWR but replacing NSP with FixMatch [51] using this Pytorch implementation2 on settings (a) and (b) in Table 1.",2,positive
"For example, FixMatch [55] uses the pseudo label generated from weakly augmented data to guide the prediction on strongly augmented data and achieves SOTA performance; VPU [6] learns from positive and unlabeled data.",1,neutral
"Several approaches have been proposed for semi-supervised classification in recent years, such as MixMatch [4], EnAET [67], FixMatch [55], SelfMatch [18], VPU [6] and ActiveMatch [75].",1,neutral
"We compare one of the SOTA methods, FixMatch [55], under the anomaly detection setting and report the results on CIFAR-10, CIFAR-100, and LSUN datasets in Table 3.",2,positive
"Due to their outstanding capacity to learn task-relevant image features such methods - in particular convolutional neural networks (CNN) – have outperformed “classical” machine learning algorithms in many image analysis tasks[31,25,24,27,10,26].",1,neutral
To compute the pseudo labels we follow Sohn et al. [2020] and we first compute the prediction output using weakly augmented versions of the input sample and then we use the output as a pseudo label for the strongly augmented version of the same sample.,1,neutral
"Recent trends concerning UDA leverage the idea of producing pseudo-labels [26] for self-training over the target domain, inspired by the recent success in semi-supervised tasks [55,36].",1,neutral
"by [65], a strong augmentation operator A and a weak augmentation operator a are applied to transform xi into A(x t i) and a(x t i), respectively.",1,neutral
"This phenomenon can be avoided in [65], since the supervised training on a few labeled samples can regularize the model via L.",1,neutral
(8) is similar in spirit to the semi-supervised learning method [65].,1,neutral
"In [69,76,32,63,78,39,31,82,60], semi-supervised learning methods, such as entropy minimization, adding perturbation, contrastive learning and randomly dropout, further boost performance.",1,neutral
FixMatch [34] further explicitly generated the pseudo labels from the data with weak augmentations and used them to guide the prediction from the strongly augmented samples.,1,neutral
"For example, [34,2,8] encouraged the model to produce invariant results under various strong data augmentations.",1,neutral
"Specifically, first of all, state-of-the-art SSL algorithms [1, 2] guess pseudo labels for those unlabeled examples, and then fit unlabeled examples on these pseudo labels.",1,neutral
"Thus, some semi-supervised learning algorithms [1, 2] have been proposed.",1,neutral
"In the SSL algorithm FixMatch [1], all training parameters are the same as the original paper, except that the initial learning rate on CIFAR10 is 0.003, and the initial learning rate on CIFAR100 is 0.01.",2,positive
"As with these attacks, our experiments also employ SSL algorithm FixMatch [1] with RandAugment.",2,positive
"In the SSL algorithm FixMatch [1], all training parameters are the same as the original paper, except that the initial learning rate on CIFAR10 is 0.",2,positive
"On SSL algorithm FixMatch [1], our attack reaches a favorable attack success rate, and can bypass backdoor defenses Neural Cleanse [8] and Activation Clustering[9] for supervised learning, as well as the recently proposed unlabeled backdoor detection DePuD [3] for SSL.",2,positive
"On SSL algorithm FixMatch [1], our attack reaches a favorable attack success rate, and can bypass backdoor defenses Neural Cleanse [8] and Activation Clustering[9] for supervised learning, as well as the recently proposed unlabeled backdoor detection DePuD [3] for SSL.
2.",2,positive
"For instance, Fixmatch retains the unlabeled data with pseudo-labels only when the prediction is confident enough [33].",1,neutral
"This phenomenon is coherent with the observation of [27, 33, 42, 45].",1,neutral
"On the one hand, pseudo-labeling refers to works that assign a hard label to each unlabeled sample with the given model [1, 33].",1,neutral
"To make better utilization of the unlabeled samples, [33] and [45] propose to select unlabeled samples whose maximum prediction probability is higher than a predefined threshold.",1,neutral
"Please note that, unlike previous methods where the threshold τ is set by manual tuning [33, 45], we derive the threshold automatically.",1,neutral
"The effectiveness of using unlabeled data to improve the model accuracy is evidenced by many previous semi-supervised learning works [33, 34, 42, 45].",1,neutral
"For example, inspired by the seminal SSL work (Sohn et al. 2020b) that introduces a weak-strong data augmentation scheme for SSL in classification, some works (Zhou et al. 2021; Tang et al. 2021; Liu et al. 2021; Xu et al. 2021) integrate such a scheme with a mean teacher strategy (Tarvainen and…",1,neutral
"Among them, STAC (Sohn et al. 2020b) provides a seminal framework for SSOD.",2,positive
"MS-COCO: Similar as (Sohn et al. 2020b; Zhou et al. 2021), we separately randomly select 1%, 5% and 10% images from the COCO training set train2017 as the labeled data while the remaining are used as unlabeled data to evaluate the SSOD performance under different amounts of labeled data.",0,negative
"Method Remark 1% 5% 10% Supervised 10.0±0.26 20.92±0.15 26.94±0.111
STAC (Sohn et al. 2020b) arxiv 2020 13.97±0.35 24.38±0.12 28.64±0.21 Unbiased Teacher (Liu et al. 2021) ICLR 2021 20.75±0.12 28.27±0.11 31.50±0.10 Instant-Teaching (Zhou et al. 2021) CVPR 2021 16.00±0.20 25.50±0.05 29.45±0.15…",0,negative
al (Sohn et al. 2020a) propose to integrate both ideas mentioned above and consequently lead to obvious performance improvement.,2,positive
"In the following experiments, we compare the proposed method with a supervised baseline and five other state-ofthe-art SSOD methods, including STAC (Sohn et al. 2020b), Unbiased Teacher (Liu et al. 2021), Instant-Teaching (Zhou et al. 2021), Humble Teacher (Tang et al. 2021) and Soft Teacher (Xu et al. 2021).",2,positive
"In the following experiments, we compare the proposed method with a supervised baseline and five other state-ofthe-art SSOD methods, including STAC (Sohn et al. 2020b), Unbiased Teacher (Liu et al. 2021), Instant-Teaching (Zhou et al. 2021), Humble Teacher (Tang et al. 2021) and Soft Teacher (Xu et…",2,positive
"Till now, increasing effort, especially these pseudo labeling based methods (Sohn et al. 2020b; Zhou et al. 2021; Tang et al. 2021; Liu et al. 2021; Xu et al. 2021), have been made in SSOD.",2,positive
"Till now, many pseudo labeling based SSOD methods (Sohn et al. 2020b; Zhou et al. 2021; Tang et al. 2021; Liu et al. 2021; Xu et al. 2021) have been proposed, which mainly focus on constructing various semi-supervised learning framework to exploit the pseudo ar X
iv :2
20 7.",2,positive
"Method Remark 1% 5% 10% Supervised 10.0±0.26 20.92±0.15 26.94±0.111
STAC (Sohn et al. 2020b) arxiv 2020 13.97±0.35 24.38±0.12 28.64±0.21 Unbiased Teacher (Liu et al. 2021) ICLR 2021 20.75±0.12 28.27±0.11 31.50±0.10 Instant-Teaching (Zhou et al. 2021) CVPR 2021 16.00±0.20 25.50±0.05 29.45±0.15 Instant-Teaching∗ (Zhou et al. 2021) CVPR 2021 18.05±0.15 26.75±0.05 30.40±0.05 Humble Teacher (Tang et al. 2021) CVPR 2021 16.96±0.38 27.70±0.15 31.61±0.28
Soft Teacher (Xu et al. 2021) ICCV 2021 20.46±0.39 30.74±0.08 34.04±0.14 Ours 23.55±0.25 32.10±0.15 35.30±0.15
actly follow the same experimental settings on both datasets as these methods and directly report the results released in their provenance in the following tables.",0,negative
"Recently, Sohn et. al (Sohn et al. 2020a) propose to integrate both ideas mentioned above and consequently lead to obvious performance improvement.",2,positive
"Following previous works (Sohn et al. 2020b; Zhou et al. 2021), we evaluate the proposed method on two commonly utilized SSOD benchmark s including PASCAL VOC (Everingham et al. 2010) and MS-COCO (Lin et al. 2014).",2,positive
"In our implementation, the magnitude for each transformation is also randomly selected, which is similar to [33].",1,neutral
"For the target-to-source alignment, since the pseudo labels become more confident along with the training process, a natural curriculum is achieved by setting a confidence threshold to filter out the target samples with low confidence pseudo labels [33].",1,neutral
"Methods in the first group [32,34,19,37,46,39,43,23,6] do not use the unlabeled auxiliary data during meta-training, while methods in the second group [11,35,36,33,26,18] utilize the unlabeled target images to facilitate crossing the domain gap.",0,negative
"However, with only 10% samples remained, our approach still outperforms FixMatch which uses 100% auxiliary data.",2,positive
FixMatch [33] 10% 40% 70% 100% 0% 10% 40% 70% 100%,0,negative
98% over the previous best method FixMatch [33] on DomainNet.,2,positive
"When comparing our approach with methods in the second group, we find that the stabPA outperforms them in all situations, improving 5-shot accuracy by 5.98% over the previous best method FixMatch [33] on DomainNet.",2,positive
"For instance, UDA [42] and ReMixMatch [1] apply confidence-based strategies and sharpened soft pseudo labels to select sufficiently confident data for training, whereas FixMatch [36] uses one-hot hard labels.",1,neutral
"For instance, UDA [44] and ReMixMatch [1] apply confidence-based strategies and sharpened soft pseudo labels to select sufficiently confident data for training, whereas FixMatch [39] uses one-hot hard labels.",1,neutral
"More recently, SemCo [23] successfully complements FixMatch by introducing semantic prior knowledge from language models [25].",1,neutral
"Recently proposed models [44,2,1,39,24,47] combine these two techniques by using unsupervised data augmentation [44].",1,neutral
"Two dominant learning paradigms in SSL are pseudo-labeling [47,67,26,24] and consistency regularization [51,25,60,36].",1,neutral
"Semi-supervised learning (SSL) offers a solution to the annotation cost problem by exploiting a large amount of unlabeled data along with limited labeled data [51,47].",1,neutral
", Mean Teacher [51] and FixMatch [47]) is indeed effective in improving model generalization.",1,neutral
"For FixMatch [47], we use temporal flip (i.",1,neutral
Stage II: Semi-supervised fine-tuning We implement temporal mask semisupervised learning following the pseudo label paradigm [47].,1,neutral
"Semi-supervised learning (SSL) [71,10] has been widely adopted in computer vision for image classification [5,47,13], object detection [50,69], semantic segmentation [41,21], and pose estimation [15,35].",1,neutral
"We select two top SSL methods (Mean Teacher [51] and FixMatch [47]), and a state-of-the-art TAD model based on a popular proposal generation method BMN [27] (using TSN features) and GTAD [63] (using I3D features).",2,positive
"Examples of the similarity metrics are Jensen-Shannon divergence (JS) [51, 38] and `2 distance [52, 53] and Kullback–Leibler divergence [54, 55].",1,neutral
"STAC [27] borrows the separate weak and strong augmentation from FixMatch [25] and applies it to SSOD, obtaining promising performance.",2,positive
"to various input perturbations, leading to robust representation learning [22, 25].",1,neutral
"As another strong method, FixMatch [25] introduces a separate weak and strong augmentation and converts model predictions into hard labels, since the labels with low entropy are believed to be beneficial in their pipeline.",1,neutral
", setting to one-hot encoding based on the probabilities of different class labels (Sohn et al., 2020).",1,neutral
"In recent years, plentiful SSL methods for classification [1, 41, 51, 55], 2D object detection [9, 12, 13, 17, 42, 44, 50, 52, 53, 56] and LiDAR-based 3D object detection [46, 59] have been proposed and applied.",1,neutral
"Semi-supervised Learning focuses on training models with both labeled and unlabeled data, which has achieved state-of-the-art performance on classification [1, 41, 51, 55], 2D object detection [9, 12, 13, 17,42,44,50,52,53,56] and LiDAR-based 3D object detection [46, 59].",2,positive
"FixMatch [41] first generates pseudo labels on weakly augmented unlabeled images, and then the student model is trained to predict the same classifications on strong augmented data.",1,neutral
"Especially, we use the one-hot class distributions for pseudo label generation as in [28, 45].",1,neutral
Note that here we only calculate the augmented consistency loss for those pseudo-labelled pixels for which the uncertainty estimate Sk is less than a threshold μ to avoid error accumulation due to incorrect pseudo labels [14].,1,neutral
"For instance, the performance of SSL [3, 56] has been greatly improved thanks to the augmented benign OOD data1.",1,neutral
"For instance, the performance of SSL (Berthelot et al., 2019; Sohn et al., 2020) has been greatly improved thanks to the augmented benign OOD data.",2,positive
"Recently proposed semi-supervised methods mainly focus on image classification [34,4].",1,neutral
"Semi-supervised learning, which makes use of limited labelled data combined with large amounts of unlabelled data for training, has shown great potential to reduce the reliance on large amounts of labels [25,4,34,22].",1,neutral
"As in [60,48], we incorporate multiple augmentations in our experiments, including gaussian blur, color jitter and random scaling.",1,neutral
"Derived from FixMatch [60] which performs consistency training under the scenario of image classification, PixMatch [48] explores on various image augmentation strategies for domain adaptive image segmentation task.",2,positive
"Different from adversarial learning [23,64,45,21,67,18], consistency training [68,60,48,1] is widely explored in semi-supervised learning and domain adaptation recently with the benefits of its higher training stability and lower empirical risk.",1,neutral
"unlabelled data by enforcing model outputs to be invariant to data augmentation [68,60,53].",1,neutral
"More recently, it has been shown that stronger image augmentation [68,3,60] can better improve the consistency training.",1,neutral
"As for the augmentation set A, there have been studies [68,3,60] presenting that stronger augmentation can benefit the consistency training more.",1,neutral
"Consistency training is a prevalent semi-supervised learning scheme that regularizes network predictions to be invariant to input perturbations [68,60,53,20,10].",1,neutral
"As in FixMatch [60], the use of a hard label for consistency training in PixMatch encourage the model to obtain predictions with not only augmentation robustness but also high certainty on unlabeled data.",2,positive
"To mitigate this issue, inspired of recent studies in semi-supervised learning [25, 26], we perform disambiguation for candidate labels by introducing the consistency regularization between original images and their augmented versions to prevent the model from over-fitting to the noisy labels in the candidate set.",1,neutral
"Pseudo-labeling [2, 5] and consistency regularization [3, 5] have shown success on semantic segmentation tasks, utilizing unlabeled data by first training a network on the small labeled dataset, and then retraining on both real labels and highly confident pseudolabels predicted on the unlabeled data.",1,neutral
"On image classification tasks, the dominated method of mining information from unlabeled data is “Consistency-based Pseudo-Labeling” [5,24,4,3].",1,neutral
"This approach does not require annotation and can be used in combination with other methods; therefore, it is widely adopted in many SSL frameworks [2,24,4].",1,neutral
"Another type of semi-supervised methods uses data augmentation to generate better feedback signals for unlabeled data, or combines pseudo-label generation and consistency regularization [3, 4, 39].",1,neutral
"The former can be introduced into the training process to enhance the performance of the model, but they need Real samples (weakly augmented samples) for guidance, which is reflected in self-supervised learning [3, 5] or semi-supervised learning [4, 43].",1,neutral
"OpenLDN also outperforms other baseline methods: FixMatch [53], DS(3)L [21], CGDL [54], and SimCLR [12].",2,positive
"OpenLDN also outperforms other baseline methods: FixMatch [53], DS3L [21], CGDL [54], and SimCLR [12].",2,positive
"Finally, the hybrid approaches [4,3,53] combine both consistency regularization and pseudo-labeling.",1,neutral
"Even though recent SSL methods [4,3,53,63] have achieved promising results, one of their primary assumptions is that both labeled and unlabeled data come from the same underlying distribution.",1,neutral
"At this point, we are able to apply any standard closed-world SSL method [4,63,53,57].",2,positive
"We further improve our ST-CoNAL method by adopting the principle of entropy minimization used for SSL [3,4,11,22,35,40].",2,positive
"Data augmentation can provide additional artificial data [10,12,23,25].",1,neutral
"Usually, consistency regularization enforces models’ predictions to be invariant under different data augmentations [25].",1,neutral
"Consistency regularization underlies many successful works, such as semi-supervised learning [6,5,25].",1,neutral
"dominant approaches to reduce the cost of annotation is semi-supervised learning (SSL) [63,6,43,49,62], where the objective is to leverage a set of unlabeled data in conjunction with a limited labeled set to improve performance.",1,neutral
The same trend is observed for FixMatch [62] (a state-of-the-art closed-world SSL method).,1,neutral
"Semi-Supervised Learning Extensive research has been conducted on closedworld SSL [19,28,46,36,53,13,10,58,41,48,63,43,60,6,5,62].",1,neutral
"We use a batch of 16 samples and two network architectures that are widely used in previous works (Zagoruyko & Komodakis, 2016; Zhang et al., 2021; Sohn et al., 2020; Li et al., 2021), namely, WRN-28-2 on CIFAR-10 (Figure 3 (a)) and WRN-28-8 on CIFAR-100 (Figure 3 (b)).",2,positive
"FixMatch (Sohn et al., 2020) combines the merits of these two approaches: given an unlabeled image, weak data augmentation and strong data augmentation are performed on the image, leading to two versions of the image, and then FixMatch produces a pseudo-label based on its weakly-augmented version…",1,neutral
"Most recent approaches to SSL for image classification are based on the combination of consistency regularization and pseudo-labeling (Sohn et al., 2020; Li et al., 2021; Rizve et al., 2021; Zhang et al., 2021; Nassar et al., 2021; Pham et al., 2021; Hu et al., 2021).",1,neutral
"…aims to learn from few labeled data and a large amount of unlabeled data, and it has been a long-standing problem in computer vision and machine learning (Sohn et al., 2020; Zhang et al., 2021; Rizve et al., 2021; Pham et al., 2021; Li & Zhou, 2014; Liu et al., 2010; Berthelot et al., 2019; 2020).",1,neutral
"During training, we followed previous work (Sohn et al., 2020; Zhang et al., 2021; Rizve et al., 2021; Li et al., 2021) to utilize the exponential moving average (EMA) technique.",2,positive
"Based on these two approaches, FixMatch
(Sohn et al., 2020) is proposed, which achieves new stateof-the-art (SOTA) results on the most commonly-studied SSL benchmarks.",2,positive
"They can be further classified into two categories, namely, deterministic (Sohn et al., 2020; Li et al., 2021; Zhang et al., 2021; Nassar et al., 2021; Pham et al., 2021; Hu et al., 2021) and probabilistic ones (Rizve et al.",1,neutral
"We evaluated NP-match on these two datasets following the evaluation settings used in previous works (Sohn et al., 2020; Zhang et al., 2021; Li et al., 2021).",2,positive
"SSL aims to learn from few labeled data and a large amount of unlabeled data, and it has been a long-standing problem in computer vision and machine learning (Sohn et al., 2020; Zhang et al., 2021; Rizve et al., 2021; Pham et al., 2021; Li & Zhou, 2014; Liu et al., 2010; Berthelot et al., 2019; 2020).",2,positive
"Current SOTA methods for the semi-supervised image classification task are deterministic, including FixMatch (Sohn et al., 2020), CoMatch (Li et al.",1,neutral
"The success of FixMatch inspired several subsequent methods (Li et al., 2021; Rizve et al., 2021; Zhang et al., 2021; Nassar et al., 2021; Pham et al., 2021; Hu et al., 2021).",2,positive
"FixMatch (Sohn et al., 2020) combines the merits of these two approaches: given an unlabeled image, weak data augmentation and strong data augmentation are performed on the image, leading to two versions of the image, and then FixMatch produces a pseudo-label based on its weakly-augmented version and a preset confidence threshold, which is used as the true label for its strongly augmented version to train the whole framework.",2,positive
"Current SOTA methods for the semi-supervised image classification task are deterministic, including FixMatch (Sohn et al., 2020), CoMatch (Li et al., 2021), and FlexMatch (Zhang et al., 2021), which have achieved promising results on public benchmarks.",2,positive
"(Sohn et al., 2020) is proposed, which achieves new stateof-the-art (SOTA) results on the most commonly-studied SSL benchmarks.",2,positive
"Deterministic Methods FixMatch (Sohn et al., 2020) 43.",1,neutral
"They can be further classified into two categories, namely, deterministic (Sohn et al., 2020; Li et al., 2021; Zhang et al., 2021; Nassar et al., 2021; Pham et al., 2021; Hu et al., 2021) and probabilistic ones (Rizve et al., 2021).",1,neutral
"Data augmentation approaches, such as MixUp [Zhang et al., 2018], MixMatch [Berthelot et al., 2019], and FixMatch [Sohn et al., 2020], have also been developed and integrated into DL models.",2,positive
"FixMatch [Sohn et al., 2020] is the augmentation of MixMatch.",2,positive
", 2019], and FixMatch [Sohn et al., 2020], have also been developed and integrated into DL models.",2,positive
"In the context of either semi-supervised [Grandvalet and Bengio, 2005; Saito et al., 2019; Sohn et al., 2020] or unsupervised learning [Melacci and Gori, 2012; Rutquist, 2019], minimizing the entropy value of the predictions performs as a regularization term to shape a model and to obtain appealing predictions.",1,neutral
"In the context of either semi-supervised [Grandvalet and Bengio, 2005; Saito et al., 2019; Sohn et al., 2020] or unsupervised learning [Melacci and Gori, 2012; Rutquist, 2019], minimizing the entropy value of the predictions performs as a regularization term to shape a model and to obtain appealing…",1,neutral
"Motivated by advances in semi-supervised classification [Berthelot et al., 2019; Sohn et al., 2020; Zhang et al., 2021], recent
∗Corresponding author
semi-supervised segmentation favors pseudo labeling [Chen et al., 2020] and consistency regularization [Mittal et al., 2019].",2,positive
"Motivated by advances in semi-supervised classification [Berthelot et al., 2019; Sohn et al., 2020; Zhang et al., 2021], recent",2,positive
"In this work, to set the criterion, we follow [Sohn et al., 2020] to simply adopt prediction confidence as reliability indicator.",2,positive
", FixMatch (Sohn et al., 2020), fail in the MNAR scenario.",2,positive
"Since our methods are implemented as a plug-in module to FixMatch, common network hyper-parameters, e.g., learning rates, batch-sizes, are the same as their original settings (Sohn et al., 2020).",2,positive
"Following previous works (Berthelot et al., 2019b; Sohn et al., 2020; Hu et al., 2021), we used Wide ResNet (WRN)-28-2 for CIFAR-10, WRN-28-8 for CIFAR-100, WRN-37-2 for STL-10 and ResNet-18 for mini-Imagenet.",2,positive
"Recent state-of-the-art SSL methods (Sohn et al., 2020; Berthelot et al., 2019a) combine the two existing techniques and predict improved pseudolabels.",2,positive
"Prevailing SSL methods (Sohn et al., 2020; Berthelot et al., 2019b;a) share a similar strategy: training a model with the labeled data and generating pseudo-labels for unlabeled data based on the model predictions.",2,positive
"…methods first train a model using the labeled data, then uses the model to impute the missing labels with the predicted pseudolabels for the unlabeled data (Van Buuren, 2018), and finally combine the true- and pseudo-labels to further improve the model (Sohn et al., 2020; Berthelot et al., 2019a).",1,neutral
"Prevailing SSL methods first train a model using the labeled data, then uses the model to impute the missing labels with the predicted pseudolabels for the unlabeled data (Van Buuren, 2018), and finally combine the true- and pseudo-labels to further improve the model (Sohn et al., 2020; Berthelot et al., 2019a).",1,neutral
", learning rates, batch-sizes, are the same as their original settings (Sohn et al., 2020).",1,neutral
"Performances of Fixmatch are the reported results in their paper (Sohn et al., 2020).",2,positive
"Ideally, the “improve” is theoretically guaranteed if the missing label imputation is perfect (Grandvalet & Bengio, 2005); otherwise, imperfect imputation causes the well-known confirmation bias (Arazo et al., 2019; Sohn et al., 2020).",0,negative
"For example, Ls is normally the standard cross-entropy
loss, and Lu can be implemented as squared L2 loss (Berthelot et al., 2019b) or cross-entropy loss (Berthelot et al., 2019a; Sohn et al., 2020).",1,neutral
"We design an experiment on CIFAR-10 to further illustrate how existing state-of-the-art SSL methods, e.g., FixMatch (Sohn et al., 2020), fail in the MNAR scenario.",2,positive
"As for the ID samples in U , although T < 1 was used in previous works (Xie et al. 2019; Sohn et al. 2020) to encourage the high-confidence output, we still set T > 1 in Eq (2) since exploiting the OOD samples in U plays an important role in OOD detection.",2,positive
"data, consistency regularization (Xie et al. 2019; Sohn et al. 2020) over U = U in ∪ U can be formulated as:",1,neutral
"We compare our method with various related methods, including: Baseline (Hendrycks and Gimpel 2017); OE
(Hendrycks, Mazeika, and Dietterich 2019); MCD (Yu and Aizawa 2019); SSD (Sehwag, Chiang, and Mittal 2021); FixMatch (Sohn et al. 2020); UASD (Chen et al. 2020b).",2,positive
"Our method performs slightly worse than the FixMatch method, perhaps because we use temperature T > 1 for the samples of U in in Eq (2).",2,positive
"The SSD method and the FixMatch method are developed for the pure unlabeled ID data, while the OE method is developed for the pure unlabeled OOD data.",2,positive
"We compare our method with various related methods, including: Baseline (Hendrycks and Gimpel 2017); OE (Hendrycks, Mazeika, and Dietterich 2019); MCD (Yu and Aizawa 2019); SSD (Sehwag, Chiang, and Mittal 2021); FixMatch (Sohn et al. 2020); UASD (Chen et al.",2,positive
"Following that in Sohn et al. (2020), the augmentation A(·) is implemented with the standard data augmentations (random flip and crop), and the augmentation A′(·) is implemented with RandAugment.",2,positive
"As for the ID samples in U in, although T < 1 was used in previous works (Xie et al. 2019; Sohn et al. 2020) to encourage the high-confidence output, we still set T > 1 in Eq (2) since exploiting the OOD samples in U plays an important role in OOD detection.",2,positive
"But our method is developed for detecting OOD samples and performs much better than the FixMatch method in OOD detection, which is shown in Table 1.",2,positive
Other hyperparameters are the same as that of Sohn et al. (2020) for a fair comparison.,1,neutral
"As for the unlabeled
data, consistency regularization (Xie et al. 2019; Sohn et al. 2020) over U = U in ∪ Uout can be formulated as:
LCR = 1 |U | ∑ x∈U CE ( qθ ( A(x), T ) ∥ qθ ( A′(x) )) , (2)
where A(·) and A′(·) are different data augmentations.",1,neutral
"When |Uout| = 0, our method is slightly worse than the FixMatch method, which is developed for the pure unlabeled ID data (|Uout| = 0).",2,positive
Semi-Supervised Learning (SSL) methods (Grandvalet and Bengio 2004; Lee 2013; Berthelot et al. 2019; Xie et al. 2019; Sohn et al. 2020) were also developed for utilizing limited labeled data and abundant unlabeled data.,1,neutral
"Similar to Sohn et al. (2020), we normalize these losses with |U | to take the capacity of the selected sets into consideration.",1,neutral
"Recent work in the use of Consistency regularization for semi-supervised learning has yielded a number of augmentation functions that perform well for image and digit classification datasets as described by the following papers: (Sohn et al., 2020; Berthelot et al., 2019b,a).",1,neutral
"The first branch being the weak augmentation of the unlabeled sample and the second branch being the strong augmentation of the same unlabeled sample (Sohn et al., 2020).",1,neutral
"Consistency Regularization is another smoothness based strategy that has led to MixMatch and its derivatives for image classification (Berthelot et al., 2019b; Sohn et al., 2020; Berthelot et al., 2019a; Mustafa and Mantiuk, 2020).",2,positive
"Consistency regularization as a form for contrastive learning is used as a part of the proposed approach (Sohn et al., 2020; Berthelot et al., 2019b,a).",1,neutral
"Hybrid methods (Berthelot et al. 2019, 2020; Sohn et al. 2020) simultaneously combine consistency regularization, pseudolabeling, and data augmentation (Xie et al.",1,neutral
"2020b; Cascante-Bonilla et al. 2021) and some hybrid methods (Berthelot et al. 2019, 2020; Sohn et al. 2020).",2,positive
"Hybrid methods (Berthelot et al. 2019, 2020; Sohn et al. 2020) simultaneously combine consistency regularization, pseudolabeling, and data augmentation (Xie et al. 2020a; Cubuk et al. 2019; Devries and Taylor 2017).",1,neutral
The use of CR in SSL is first adopted in the field of computer vision (Berthelot et al. 2019; Sohn et al. 2020; Xie et al. 2020) and then draws attentions in graph data.,1,neutral
"Existing semi-supervised methods typically apply the consistency regularization [8,14,23,24] or use the pseudo-labeling [1,2,29,31] on unlabeled samples.",1,neutral
"Moreover, we incorporate the widely-used semi-supervised method (FixMatch [24], abbreviated as FM) into the baseline FedAvg [16] and existing FL algorithms for the class imbalance problem, including the FedProx [12] (MLSys’20) and the FedAdam (ICLR’21) [18].",2,positive
"We draw inspirations from recent progress on self-training in transferring accuracy under domain shifts [61, 5, 70, 3, 49, 55].",2,positive
"Following FixMatch [55], we use the pseudo-labels generated by the teacher model as supervision for consistency training where the model should have consistent predictions under transformations.",1,neutral
"However, the consistency regularization in FixMatch [55] does not distinguish groups which might amplify the bias as observed in [76] and our experiments.",1,neutral
"[55] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel.",0,negative
"Guided by the theoretical algorithm, we propose a practical self-training algorithm which builds upon Laftr [42], an adversarial learning method for fairness, and FixMatch [55], a self-training framework.",2,positive
"Based on the findings that model’s consistency to input transformations is important to generalization [75] and is a core component of self-training [52, 55, 26], we improve the consistency regularization in [55] to achieve fair transferring.",2,positive
"In line with results from computer vision, FixMatch even surpasses the baseline model trained with the entire labeled dataset.",2,positive
"However, our proposed method outperformed all other SSL methods we compared against including FixMatch for both datasets.",2,positive
"We compared our proposed co-training with H and E views to a baseline ResNet18 model that uses RGB H&E images as input, as well as other state-of-the-art SSL methods, such as MixMatch and FixMatch, considering they are already widely used in histopathology image analysis [13].",2,positive
Weak/strong data transformation consistency (FixMatch) [19] has been applied to detection of dysplasia of the esophagus [14].,1,neutral
"For comparison with other state-of-the-art SSL methods, we used consistency regularization [17,9], MixMatch [3] and FixMatch [19].",2,positive
"And then, Kihyuk et al.[24] simplify semi-supervised learning with consistency and confidence.",1,neutral
"Pseudo-labeling (Lee, 2013; Sohn et al., 2020) as a special variant is one of the basic SSL techniques, which exploits the pseudo-labeled target predicted by the model itself to train a classifier.",1,neutral
"Semi-supervised learning has gradually matured in recent years, with a lot of outstanding advancements (Lee, 2013; Miyato et al., 2018; Berthelot et al., 2019; Sohn et al., 2020; Xie et al., 2020, 2019; Laine & Aila, 2016).",2,positive
"Concretely, to figure out how SSL works, we select two similar species in the Semi-iNat dataset and analyze the predictions produced by FixMatch (Sohn et al., 2020), a representative SSL algorithm with state-of-the-art performance on balanced SSL benchmarks.",2,positive
", 2019), and FixMatch (Sohn et al., 2020)] are tested using the same settings Method CIFAR10 CIFAR100",2,positive
"Therefore, a large number of SSL methods (Lee, 2013; Tarvainen & Valpola, 2017; Berthelot et al., 2019; Sohn et al., 2020) with high performance gains at low cost have been proposed.",2,positive
"We also show in experiments that BiSTF improves over FixMatch  (Sohn et  al., 2020) by a large margin on imbalanced semi-supervised benchmarks.",2,positive
", 2019; Laine and Aila 2016) and hard (Sohn et al., 2020) pseudo-labels to learn the classifier.",2,positive
"Besides, these algorithms are focused on researching standard SSL image recognition benchmarks (Tarvainen & Valpola, 2017; Berthelot et al., 2019; Sohn et al., 2020; Xie et al., 2020) instead of fine-grained image recognition benchmarks.",2,positive
"Note that fixing most or all hyper-parameters is widely adopted in existing approaches [51], [65], [82].",1,neutral
"Pseudo-labeling (PL) [51]–[53], also known as self-training, is widely used for semi-supervised learning due to its effectiveness and simplicity.",1,neutral
"We also tried consistency training by letting strongly-augmented samples learn the PLs generated by their weakly-augmented counterparts as suggested in [16, 20], however, it did not bring benefits in our experiments compared to directly learning PLs generated without augmentations.",2,positive
"There are also a number of SSL algorithms that are firstly proposed in the image recognition field [13, 14, 15, 16, 17] and then adopted to speech recognition [11, 18, 19, 20, 21].",1,neutral
"In terms of the entropy loss for the target domain, we adopt a variant of the loss, FixMatch [43], in order to utilize the confident predictions of the target instances.",1,neutral
"Self-training methods are also proposed to mitigate the DA problems by pseudo-labeling the target instances [22, 19, 30, 35], originally tailored to semi-supervised learning [14, 43].",1,neutral
"The proposed CSA is applicable to any data domain, and could be used in concert with consistency-based approaches (Sohn et al., 2020), but is particularly useful for data domain where pretext tasks and data augmentation are not applicable, such a tabular data.",2,positive
"Domain specific: Semi-supervised learning for image and language data has made rapid progress (Oymak & Gulcu, 2021; Zhou, 2021; Sohn et al., 2020) largely by exploiting the inherent spatial and semantic structure of images (Komodakis & Gidaris, 2018) and language (Kenton & Toutanova, 2019).",2,positive
"We can make use of CSA as the main label assignment method for SSL and integrate it into training deep learning model to build CSAMatch, analogous to FixMatch [42], MixMatch [3] and FlexMatch [58].",2,positive
"Domain specific: Semi-supervised learning for image and language data has made rapid progress [36, 59, 3, 42, 58] largely by exploiting the inherent spatial and semantic structure of images [24] and language [22].",1,neutral
"Another extension is to use CSA as the main label assignment method for SSL and integrate it into training deep learning model to build CSAMatch, analogous to FixMatch (Sohn et al., 2020) and MixMatch (Berthelot et al., 2019).",2,positive
"The proposed CSA is applicable to any data domain, and could be used in concert with consistency-based approaches [42], but is particularly useful for tabular data where pretext tasks and data augmentation are not applicable.",2,positive
"Algorithms Not domain specific Uncertainty consideration Non-greedy PL [26], FlexMatch [58] 3 7 7 Vime [56] 7 7 NA MixMatch[3], FixMatch[42] 7 7 NA UPS [40] 3 3 7 LSA [43] 3 7 3 CSA 3 3 3",2,positive
"Pseudo-labeling, a type of self-training [31, 24] technique, converts model predictions on unlabeled samples into soft or hard labels as optimization targets, while consistency regularization [18, 35, 1, 2, 32, 37] trains the model to produce the same pseudo-label for two different views (strong and weak augmentations) of an unlabeled sample.",1,neutral
"When integrated into FixMatch [32], our method performs significantly better than standard confidence-based pseudo-labeling methods when the training data is imbalanced across categories, which we believe better reflects real-world data distributions.",2,positive
"The recently introduced weak-strong data augmentation paradigm [32, 37] can be viewed as the combination of these two directions.",1,neutral
"In recent years, the frontier of SSL has seen significant advances through pseudo-labeling [30, 20] combined with consistency regularization [18, 35, 1, 2, 32, 37].",1,neutral
FixMatch [32] and UDA [37] predict pseudo-labels on weakly-augmented views of unlabeled images and train the model to predict those pseudo labels on strongly-augmented views.,1,neutral
"Most of these state-of-the-art methods use confidence thresholding to retain high-quality pseudo-labels [32, 37].",1,neutral
"We compare to the latest methods developed for long-tailed SSL (DARP [15], CReST [36], and ABC [21]) and for balanced SSL (UDA [37], UPS [29], FixMatch [32], and FlexMatch [40]).",2,positive
"State-of-the-art methods rely on confidencebased thresholding [20, 32, 37, 40] for pseudo-labeling, in which only the unlabeled samples whose predicted class confidence surpasses a very high threshold (e.",1,neutral
"When combined with confidence-based pseudo-labeling [32, 40], at each iteration, the process can be summarized as follows:",1,neutral
"We first overview the framework for state-of-the-art SSL methods that combine consistency regularization with confidence-based pseudo-labeling [32, 40, 37], as our proposed approach simply replaces one step — the pseudo-labeling criterion.",2,positive
[50]–[53] introduce teacher model in doing Consistency Regularization.,1,neutral
", 2021], Local Fixmatch [Sohn et al., 2020], and Local Mixup [Zhang et al.",2,positive
"Pseudo Labeling To generate confident pseudo labels, following [Sohn et al., 2020], only the class with an extremely high prediction probability is regarded as the pseudo label.",1,neutral
"We compare the SDA-FL framework with SemiFL [Diao et al., 2021], Local Fixmatch [Sohn et al., 2020], and Local Mixup [Zhang et al., 2018] to show its effectiveness.",2,positive
"FixMatch [51] is a strong baseline method in semi-supervised classification, which proposes to create two augmented versions of one image.",1,neutral
We employ the FixMatch technique [49] of using data augmentation for training two multiple branches.,2,positive
"…loss against the model’s prediction pb for a strongly-augmented version of the same image:
Lcls = 1
B B∑ b=1 1(max qb ≥ τ)H(q̂b, pb) (2)
Following FixMatch (Sohn et al., 2020), we only use pseudo-labels with maximum scores above a threshold τ , and convert the soft labels qb into…",1,neutral
"We construct our self-training objective by following three principles: (1) consistency regularization (Sohn et al., 2020; Laine & Aila, 2017) which enforces the model to output the same prediction when the input is perturbed; (2) entropy minimization (Grandvalet & Bengio, 2004) which encourages the model to give “sharp” predictions with low entropy; (3) prediction fairness (Berthelot et al.",2,positive
"Our method is more closely related to the self-training approaches proposed for semi-supervised learning (Tarvainen & Valpola, 2017; Sohn et al., 2020; Berthelot et al., 2020), where pseudo-labels on unlabeled data are used as training targets.",2,positive
"We construct our self-training objective by following three principles: (1) consistency regularization (Sohn et al., 2020; Laine & Aila, 2017) which enforces the model to output the same prediction when the input is perturbed; (2) entropy minimization (Grandvalet & Bengio, 2004) which encourages…",2,positive
"Following FixMatch (Sohn et al., 2020), we only use pseudo-labels with maximum scores above a threshold τ , and convert the soft labels qb into “one-hot” hard labels by q̂b = arg max(qb).",2,positive
"During each update of θ, ∆ are updated with
∆ = µ∆ + (1− µ)θ (1)
Let qb denote the EMA teacher’s softmax prediction for the weakly-augmented image, we enforce a cross-entropy loss against the model’s prediction pb for a strongly-augmented version of the same image:
Lcls = 1
B B∑ b=1 1(max qb ≥ τ)H(q̂b, pb) (2)
Following FixMatch (Sohn et al., 2020), we only use pseudo-labels with maximum scores above a threshold τ , and convert the soft labels qb into “one-hot” hard labels by q̂b = arg max(qb).",2,positive
"Self-training, which is also called pseudo-labeling, has been heavily investigated in this regard [22]–[26].",1,neutral
This work follows the ideas of FixMatch [26] and applies a form of consistency regularization by introducing an additional augmentation strategy.,1,neutral
This is consistent with the results reported for the evolution from MixMatch to FixMatch as investigated in [24]–[26].,1,neutral
"Additionally, works such as [24]–[26] combine self-training with consistency regularization.",1,neutral
"[68] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han Zhang, and Colin Ra el.",0,negative
"• For FixMatch [68], we adopt it on ne-tuning from noisy labels.",2,positive
"Independent label noise Sketch, ResNet-18 Sketch, ResNet-101 MRPC, RoBERTa-Base
40% 60% 40% 60% 20% 40%
Early Stopping 72.41±3.53 53.84±3.09 77.14±3.09 61.39±1.28 81.39±0.73 66.05±0.63 Label Smooth [MKH19] 74.69±1.97 55.35±1.60 81.47±1.36 64.90±2.93 80.00±0.62 65.87±0.95 Mixup [ZCD+18] 70.65±1.85 58.49±3.25 76.04±2.29 60.12±2.37 80.62±0.14 68.37±1.33 FixMatch [SBL+20] 73.35±3.15 61.51±2.17 76.19±1.39 62.19±1.57 81.10±1.76 68.48±1.43 ELR [LNR+20] 74.29±2.52 63.14±2.05 80.00±1.45 64.35±3.98 82.78±0.86 67.86±1.44 APL [MHW+20] 75.63±1.81 64.69±2.72 78.69±2.45 64.82±2.41 80.49±0.24 66.49±0.93 SAT [HZZ20] 75.18±1.54 62.33±2.24 80.00±2.96 65.58±2.91 82.80±0.42 67.50±1.00 GJS [EA21] 73.22±2.34 59.63±5.15 77.14±2.46 63.27±2.37 81.42±1.03 67.57±1.53 DualT [YLH+20] 72.49±3.17 59.59±3.44 77.96±0.33 62.31±3.98 82.49±0.53 66.49±0.93 SupCon [GDC+20] 75.14±1.73 61.06±3.20 78.86±1.80 63.92±2.15 82.30±1.80 68.32±1.16 SAM [FKM+21] 77.63±2.16 64.53±2.84 80.57±2.70 67.27±2.39 82.61±0.91 69.06±1.41
Ours 81.96±0.98 70.00±1.71 85.44±1.26 71.84±2.72 83.55±0.63 72.64±1.77
Correlated label noise DomainNet, ResNet-18
Clipart Infograph Painting Quickdraw Real Sketch 41.47% 63.29% 44.50% 60.54% 34.64% 47.68%
Early Stopping 73.88±2.04 38.82±2.59 69.69±1.35 44.16±1.92 78.52±1.03 61.84±3.67 Label Smooth [MKH19] 74.56±2.30 38.40±2.67 70.76±1.74 46.50±3.03 81.39±0.93 62.29±2.48 Mixup [ZCD+18] 72.88±0.94 39.27±3.10 69.28±3.18 47.66±3.20 80.17±2.05 62.08±3.06 FixMatch [SBL+20] 77.04±2.52 41.95±1.52 73.31±2.10 48.74±2.08 86.33±2.54 64.61±3.28 ELR [LNR+20] 76.08±2.03 40.14±2.74 72.06±1.73 47.40±3.09 83.64±2.09 65.76±3.19 APL [MHW+20] 77.40±2.33 41.22±2.58 73.61±3.12 49.88±3.24 85.79±1.59 64.69±2.30 SAT [HZZ20] 75.24±2.79 39.58±1.47 70.69±2.69 48.18±2.95 81.90±1.07 65.39±2.77 GJS [EA21] 77.20±2.59 40.94±2.19 72.51±2.87 48.14±3.40 85.05±1.94 65.43±3.35 DualT [YLH+20] 75.24±2.02 38.75±2.12 70.27±2.24 46.62±3.16 83.33±3.01 65.47±1.91 SupCon [GDC+20] 76.56±3.53 40.38±1.94 72.51±2.45 49.20±2.63 81.87±0.84 65.67±2.90 SAM [FKM+21] 79.04±1.57 41.50±1.94 73.23±2.29 50.10±1.66 84.61±2.04 66.73±2.88
Ours 83.28±1.64 43.38±2.45 76.32±1.08 50.32±2.74 92.36±0.78 66.86±3.29
• First, we fine-tune ResNet-18 on the Sketch domain from the DomainNet dataset with 40% and 60% synthetic noise.",0,negative
"We consider the following three kinds of baselines: (i) Regularization: Finetuning with early stop (Early stopping), label smoothing [MKH19], Mixup [ZCD+18; WZV+20], and EarlyLearning Regularization (ELR) [LNR+20]; (iii) Selftraining: FixMatch [SBL+20] and Self-Adaptive Training (SAT) [HZZ20]; (ii) Robust loss function: Active Passive Loss (APL) [MHW+20], Generalized Jensen-Shannon Divergence (GJS) [EA21], dual transition estimator (DualT) [YLH+20], Supervised Contrastive learning (SupCon) [GDC+20], and Sharpness-Aware Minimization (SAM) [FKM+21].",1,neutral
"We consider the following three kinds of baselines: (i) Regularization: Fine-tuning with early stop (Early stopping), label smoothing [52], Mixup [88, 78], and Early-Learning Regularization (ELR) [40]; (iii) Self-training: FixMatch [68] and Self-Adaptive Training (SAT) [29]; (ii) Robust loss function: Active Passive Loss (APL) [47], Generalized Jensen-Shannon Divergence (GJS) [19], dual transition estimator (DualT) [84], Supervised Contrastive learning (SupCon) [25], and Sharpness-Aware Minimization (SAM) [21].",1,neutral
"To tackle this issue, we are inspired by the correlation between network’s stability and confidence and pseudo label accuracy [19, 26], and propose to filter out potentially incorrect pseudo labels.",2,positive
UDA further sharpens the predicted distribution whereas FixMatch uses pseudo-labels instead.,2,positive
"In particular, we implement the strategy used in methods such [8, 37, 45], which consider a weak and a strong augmentation of an input sample denoted by α(.",1,neutral
We compare our results for each of these datasets with the recent SOTA SSL methods: FixMatch [37] and MPL [33].,2,positive
"In phase two, the positive set D a is used in semi-supervised learning based on consistency regularization across augmentations [37].",1,neutral
"7% top-1 classification accuracy when using 4K CIFAR10 labeled and 46K CIFAR10 unlabeled examples [37], drops down to only 58.",0,negative
"To mitigate these issues, there are a number of semi-supervised learning (SSL) algorithms proposed in the literature which aim to supplement a small set of available labeled data with a larger set of unlabeled examples during training [5,9,37,42,47].",1,neutral
"For example, FixMatch- with 95.7% top-1 classification accuracy when using 4K CIFAR10 labeled and 46K CIFAR10 unlabeled examples [37], drops down to only 58.48% accuracy when using 4K CIFAR10 labels but 100K Tiny-Imagenet [22] unlabeled examples (more evidence and results in Section 5).",0,negative
"Enforcing consistency in predictions across different augmentations of the same input image is a commonly used method in semi-supervised learning [8, 9, 37].",1,neutral
"Following [37] during training, for a batch B of labeled data, μB batches of unlabeled data are sampled and used.",1,neutral
AuxMix is close to 10% better than FixMatch and close to 19% better than MPL when TinyImagenet is used as the auxiliary dataset.,2,positive
Our work is also closely related to UDA [45] and FixMatch [37].,2,positive
We use RandAugment [13] with parameters as defined in [37] as the strong augmentation and random horizontal flips and random crops as the weak augmentation for AuxMix.,2,positive
"To address it, existing methods adopt confidence filtering [10, 18, 37], which abandons the samples whose prediction confidences (ranged in [0, 1]) are lower than a predefined high threshold (e.",1,neutral
"Recall that existing SSL methods [37, 44] compute the loss by using the labeled and highly confident unlabeled samples only, which are commonly believed to be the most reliable ones.",1,neutral
The state-of-the-art method of FixMatch [37] combines self-training and consistency regularization as follows.,1,neutral
"Recently, a simplified version [37] trains the model with the strongly-augmented version of any unlabeled sample and uses as supervision the class of maximum prediction probability of its weakly-augmented version, where only the high-confidence samples are selected.",1,neutral
"Recent works [4, 24, 37, 44] also use crossentropy to measure the prediction divergence.",1,neutral
", combining self-training and consistency regularization [4, 5, 18, 37, 44, 48, 48].",1,neutral
"Following [37], we apply to unlabeled training samples the two types of strategies, i.",2,positive
"To challenge the current state of the art, we choose FixMatch [37] as the baseline.",2,positive
"We note that the gradients are closely related to the optimization dynamic [1, 14, 50] and the features characterize a certain level of semantics of the specific class [27,36,37], i.",1,neutral
"The barely supervised study from FixMatch [37] has shown that the samples in one class differ in their prototypicality, i.",1,neutral
"Currently, the research of deep SSL methods can be categorized into three main branches: consistency regularization [18,31,33], pseudo-labeling [3, 30, 36], and hybrid methods [1, 2, 32].",1,neutral
"Hybrid methods [1, 2, 32] combine both consistency regularization and pseudo-labeling, and use data augmentation [5,7,35].",1,neutral
"We compare SAFE-STUDENT on test data that only contain seen-class instances with SSL baselines: Pseudo-Labeling [21], Pi-Model [31], Temporal Ensembling [18], Mean Teacher [33], Virtual Adversarial Training (VAT) [26], FixMatch [32], UASD [4], DS3L [9], Multi-Task Curriculum (MTC) [37], and Curriculum Labeling (CL) [3].",2,positive
"We compare SAFE-STUDENT on test data that only contain seen-class instances with SSL baselines: Pseudo-Labeling [21], Pi-Model [31], Temporal Ensembling [18], Mean Teacher [33], Virtual Adversarial Training (VAT) [26], FixMatch [32], UASD [4], DS(3)L [9], Multi-Task Curriculum (MTC) [37], and Curriculum Labeling (CL) [3].",2,positive
"NCD is in concept related to Zero-Shot Learning (ZSL), Semi-Supervised Learning (SSL), and unsupervised clustering, but is also significantly different from them.",1,neutral
"We consider a new task setting that naturally addresses the limitations of both SSL and ZSL, namely Novel Class Discovery (NCD) [15, 16].",2,positive
"To compensate the lack of supervised information, the two aforementioned SSL techniques are often employed to strengthen clustering performance — using pseudo labels estimated in the novel set as clustering targets, and enforcing consistency between different transformations of a same input.",1,neutral
"In particular, ZSL [3, 9, 28, 30, 37, 40, 43] aims to recognize novel classes never seen in training, relying on auxiliary semantic attributes to infer class relations, which are absent in NCD. SSL [2,4,5,29,33,39] also follows a labeledand-unlabeled training paradigm, with the assumption that all unlabeled samples are from the classes of labeled samples, while NCD assumes no class-overlap between labeled and unlabeled samples.",1,neutral
"Requiring only a small amount of annotations, SSL addresses unannotated data using pseudo-labeling or consistency regularization, yet limited to known classes from existing annotations.",1,neutral
"In this regard, SemiSupervised Learning (SSL) [4,5,39] sheds some light on the dilemma.",1,neutral
"SSL [2,4,5,29,33,39] also follows a labeledand-unlabeled training paradigm, with the assumption that all unlabeled samples are from the classes of labeled samples, while NCD assumes no class-overlap between labeled and unlabeled samples.",1,neutral
"Recently, FixMatch [27] and various studies based on it are being conducted [12, 17, 35].",2,positive
"Many recent SSL studies [2,3,12,17,27,32] did not propose a model selection method.",1,neutral
"To verify the proposed propagation regularizer and model selection methods, we combine the proposed methods to each of UDA [32] and FixMatch [27], and we perform SSL image classification benchmarks.",2,positive
"However, previous SSL approaches [2, 3, 12, 17, 27, 32] disregard about the stopping condition or the model selection.",0,negative
"Recent approaches such as FixMatch [27], SelfMatch [12], FlexMatch [35], and CoMatch [17] considered label-scarce situations.",1,neutral
"Those hyperparameters are set based on the original works [17, 27].",1,neutral
"Semisupervised learning techniques have shown remarkable performances in various fields such as image segmentation [5, 8, 31], object detection [1, 11, 19], text classification [4, 10, 20], and graph embedding [27, 29] as well as image classification [17, 27].",1,neutral
"Most SSL methods [2,3,12,17,27,32] are based on consistency regularization [14,26] and pseudo labeling [15,21].",1,neutral
"We conduct experiments with FixMatch [27], a representative pseudo-labeling method in SSL, and three datasets: moon dataset, star dataset and CIFAR-10 [13].",2,positive
"Furthermore, FixMatch [26] inherited previous findings and significantly simplified the hybrid framework, but achieved the state-of-the-art performance.",2,positive
"This technique has then been widely utilized in the latest studies for either balanced SSL [11, 18, 26] or imbalanced SSL settings [28].",1,neutral
"Differently, recent consistency-based approaches [16, 18, 20, 26, 27, 29] can si-",1,neutral
"For fair comparison [18,26], we use Wide ResNet-28-2 for CIFAR-10, Wide ResNet-288 for CIFAR-100, ResNet-18 for Mini-Imagenet and STL10, respectively.",2,positive
"In recent SSL studies [4, 13, 18, 26], the EMA model is only used for testing rather than proposing pseudo-labels.",1,neutral
"As the most simplified but effective consistency-based SSL method, FixMatch [26] adopted a fixed high-confidence threshold to alleviate the confirmation bias [2] of pseudolabels.",2,positive
"To this end, we revisit the exponentially moving averaged (EMA) model in SSL and carefully study i) why the EMA model is employed merely for the testing instead of the training process in recent SOTA SSL methods [1, 13, 14, 18, 26], and ii) how the EMA model can benefit the distribution estimation on unlabeled samples.",2,positive
"Combining the entropy minimization [12], it is claimed in [26] and [2] that retaining only the pseudo-labels with highconfidence predictions can effectively alleviate the bias.",1,neutral
"As shown in Figure 1c, two stateof-the-art (SOTA) SSL methods, FixMatch [26] and CoMatch [18], can achieve promising results on CIFAR-10 with only 40 labeled samples when the labeled and unlabeled class distributions are matched, e.",2,positive
"Following the same experimental setting as [81], we compare Self-Tuning with HCR against three classical semi-supervised learning methods: Pseudo-Labeling [41], Π-model [40], and Mean Teacher [73], as well as three recent methods UDA [85], FixMatch [69], SimCLRv2 [10], and Self-Tuning itself [81].",2,positive
"While FixMatch obtains a higher error rate with EfficientNet-B2 than WRN28-8, Self-Tuning outperforms those methods on WRN-288.",2,positive
"For this reason, semi-supervised learning (SSL) that learning from few labeled data and a large number of unlabeled data has received broad attention [4, 5, 42, 62, 63, 69, 71, 73, 84, 85].",1,neutral
"While confidence thresholding has been demonstrated to work well in classification (image-level [25] or box-level [20, 26, 38]), we observed solely relying on the box confidence cannot effectively remove the misleading instances in the box regression, and there are several reasons why it does not perform favorably: (1) First, the confidence thresholding in existing works selects pseudo-boxes based on the box scores, which only reflect the confidence of object classification in Faster-RCNN [20], and there is no explicit module estimating the confidence (or uncertainty) of regression prediction, i.",1,neutral
"Existing SSL image classification works [1,6,8,12,22,25,28,35, 36] apply input augmentations/perturbations and consistency regularization on unlabeled images to improve the model trained with the limited amount of labeled data.",1,neutral
"Consistencybased approaches [2, 3, 12, 17, 34] constrain the model to make it robust to noise via producing consistent prediction results.",1,neutral
"Inspired by recent success in image classification [2, 3, 19, 34, 39], some practitioners resort to teacher-student learning for semi-supervised object detection (SSOD) [24, 35, 37].",1,neutral
"Pseudo labeling [1,24,34,35,37,50] methods firstly train the classifiers with ground-truth annotations and generate pseudo-labels for unlabeled data, and finally retrain models with all data.",1,neutral
"Unlabeled data has been effectively explored by semi-supervised learning methods, including a combination of weak data augmentation and strong augmentation [37], consistent regularization [16],",1,neutral
"Inspired by FixMatch [34] and its applications in segmentation [2, 40], we introduce a simplified consistency regularization on the segmentation predictions to overcome this shortcoming.",2,positive
"Inspired by FixMatch [20], we further propose an effective IoUguided suppression strategy.",2,positive
While PseudoSeg [32] and CPS [4] follow the FixMatch [24] scheme and design an online pseudo labeling mechanism.,1,neutral
"Second, weakly-supervised models (e.g., FixMatch [42], CCT [34],
and UPS [38]) are mostly superior to traditional methods (including VGG [41], GoogLeNet [43], and ResNet [17]) on AUC.",1,neutral
"Based on MixMatch [3], UDA [49], and ReMixMatch [2], FixMatch [42] produces artificial labels using both consistency regularization and pseudo-labeling.",2,positive
"Moreover, the proposed method also achieves higher accuracy than the semi-supervised methods FixMatch [28] and VAT [21].",2,positive
Fixmatch [27] simplifies the learning process by training the model with high-confidence pseudo labels.,1,neutral
"Inspired by recent 3D PSD [38] and 2D FixMatch [27], we combine the pseudo label and consistency regularization strategy in an end-to-end training scheme for large-scale point clouds.",2,positive
"Pseudo labeling [14] uses the model’s class prediction as supervision to train again, and benefits from the popular 2D Fixmatch [27].",2,positive
"Common methods for selftraining can include converting the highly confident predictions into hard-labels [65], [76], the opposite [53], or applying a model ensemble [43].",1,neutral
"Most self-training approaches focus on the task of image classification [53], [65], [76] whereby each training image is considered a particular class.",1,neutral
"In (Sohn et al., 2020; Tang et al., 2021), the pseudo-labeled data is ranked prior to student model training.",2,positive
"Results on FERPlus show the same [11] 88.41% 86.15% FixMatch [49] 87.74% 86.45% PT(ours) 88.69% 86.60% ResNet-18 is Used as Backbone. y AffectNet is used as auxiliary dataset. z For RAF-DB, FERPlus is used as auxiliary dataset and vice versa.",2,positive
"State-of-art-art methods in semi-supervised learning, Mean Teacher [11] and FixMatch [49], are also provided for comparison.",1,neutral
"Regarding the easy-to-adapt subdomain as a labeled set and the hard-to-adapt subdomain as an unlabeled set, we can leverage prevailing semi-supervised learning methods [7, 61] to solve the DABP problem.",1,neutral
"In terms of random image augmentation methods, unlike [8], which applies weak augmentation to labeled and unlabeled data simultaneously, we configure a medium augmentation for labeled data, which may introduce some noise to the model judgment Ta bl e 1 D et ai ls of tr an sf or m at io n m et ho ds in w ea k au gm en ta tio n an d m ed iu m au gm en ta tio n",2,positive
"1 ] of im ag e he ig ht /w id th ,t ra ns la te by [10 ,1 0] re la tiv e to he ig ht /w id th ,r ot at e by [25 ,2 5] de gr ee s, sh ea r by [8, 8] de gr ee s",1,neutral
"FixMatch [8] performs weak augmentation and strong augmentation [21] on unlabeled data respectively for consistency regularization, where the prediction results of high-confidence weakly augmented data are used as pseudo-labels for strong augmented data.",1,neutral
"Through the interactive collaboration among edge models, MM surpasses multiple top semi-supervised learning algorithms such as MixMatch [7] and FixMatch [8] in terms of accuracy and efficiency in the online-evolutive experiment setup.",2,positive
"All methods (Pseudo-Labeling [12], mutual learning [38], MixMatch [7], UDA [44], FixMatch [8] and Mutual Match) are tested using the same experimental environment with 20% non-i.",2,positive
"UDA sharpens predictions for computing the targets, while FixMatch generates pseudo-labels for that.",1,neutral
"Note that on CIFAR-100, we compare our method to other methods that use similar architectures, and thus, we do not include FixMatch [22] since it reports results using WRN-28-8 which is multiple times larger than the above-mentioned architectures.",2,positive
"UDA [26] and FixMatch [22] employ RandAugment [7] for SSL, while FixMatch employs additionally Cutout [9] on top.",2,positive
"Besides the competitive performance, one further advantage of our method is that it requires only one-third of the number of training steps required by its closest competitor FixMatch [22].",2,positive
"The recent literature in SSL is diverse [8, 13, 17, 23] and consistency regularization [22, 24, 26] has proven to be a promising approach.",1,neutral
We trained one randomly-initialized Wide ResNet-28 for each of the five subsets of the training data using the FixMatch algorithm [33].,2,positive
"• FixMatch-FedAvg and FixMatch-FedProx: Naïve combination of the state-of-the-art SSL method, FixMatch [43], with the two FL frameworks.",2,positive
"Pseudo-labeling produces either soft [25, 45, 5] or hard [43] labels for unlabeled data using a model prediction.",1,neutral
"While a number of methods for semi-supervised learning (SSL), such as FixMatch [43] and UDA [51], have been proposed, a naïve application to FSSL that simply replaces the local model training with SSL methods is shown less",1,neutral
FixMatch [43] pseudo-labels an unlabeled sample only when the model predicts with high confidence and applies consistency regularization.,1,neutral
"FixMatch [31] is a popular variant of self-training, which generates high-quality pseudo-labels by confidence thresholding and enforcing consistency between augmented copies of data.",1,neutral
"[31] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li.",0,negative
"According to many studies [23, 84, 92, 98] in self-training, selectively utilizing the samples in the pseudolabeled dataset is a common approach since the confidence of the teacher model’s predictions varies from sample to sample.",1,neutral
"Furthermore, inspired by the consistency regularization [84, 91] widely utilized in recent SSL algorithms, we also propose the multimodal consistency regularization (MCR) to improve the generalization capability of the student.",2,positive
"In this paper, we study the above challenges in the context of SSL, especially self-training [6, 16, 23, 31, 35, 45, 50, 58, 71, 77, 84, 85, 91, 92, 98], where a teacher model trained on labeled data predicts the pseudo labels for unlabeled data.",1,neutral
"Unlike existing studies in self-training that have mainly studied uni-modal, discriminative tasks such as image classification [84, 92, 98] or text classification [16, 35, 58], we extend the idea of self-training to the task of multimodal conditional text generation.",1,neutral
"Many variants have been studied on this setup: (1) selecting the subset of the pseudo-labeled dataset [23,84,92], (2) adding noise to inputs [23,84,91,92,98], and (3) iterating the above setup multiple times [23, 92].",1,neutral
Some works used a threshold of the loss to distinguish atypical data [35] or not well-learned data [36] in voice-face,1,neutral
"Consistency regularization has been widely used in the field of semi-supervised learning, which can force the model to become more confident in predicting labels on unlabeled data [26,43,2,4,3,50,38].",1,neutral
"Consistency loss has been widely used in semi-supervised learning, to improve the model’s confidence in predicting unlabeled data [26,43,2,4,3,50,38].",1,neutral
Recent works have shown that data augmentation plays a critical role in semisupervised learning [21].,1,neutral
The current SSL methods set the same fixed threshold for all the classes to select the unlabeled data for the model training [3; 11].,2,positive
"To this end, leading SSL methods first perform data augmentation, and then engage the consistency regularization to control the model to have consistent output for different augmented forms of the same data [1; 2; 3; 4; 5; 6; 7; 8; 9; 10].",1,neutral
"Like FixMatch [3], MixMatch [4], and ReMixMatch [5], the weak augmentation on CIFAR-10 and CIFAR-100 is to randomly flip an image with a probability of 50% and then randomly translate the image horizontally or vertically by 12.5%.",2,positive
"Like FixMatch [3], MixMatch [4], and ReMixMatch [5], the weak augmentation on CIFAR-10 and CIFAR-100 is to randomly flip an image with a probability of 50% and then randomly translate the image horizontally or vertically by 12.",2,positive
Consistency Regularization: Consistency regularization was first proposed in [1; 2] and then developed in [3; 5].,1,neutral
"Baseline Methods: Nine baseline methods including SimPLE [11], FixMatch [3], Π-Model [22], Pseudo-Labeling [16], Mean Teacher [7], MixMatch [4], UDA [8], ReMixMatch [5], and VAT[6],",2,positive
"But it is interesting that some SSL methods, such as UDA [8], ReMixMatch [5], FixMatch [3], SimPLE [11], and our ADT-SSL, outperform the fully-supervised method.",2,positive
FixMatch [3] generates the one-hot labels for the unlabeled data whose predictions exceed the fixed threshold and applies distribution alignment to them.,2,positive
"To this end, the authors in [3; 11] set a very high fixed confidence threshold, e.",1,neutral
"Baseline Methods: Nine baseline methods including SimPLE [11], FixMatch [3], Π-Model [22], Pseudo-Labeling [16], Mean Teacher [7], MixMatch [4], UDA [8], ReMixMatch [5], and VAT[6],
are added to the performance comparison.",2,positive
FixMatch [17] ensures the low-entropy predictions with a threshold.,1,neutral
"Almost all methods [16, 17, 23] add the same perturbation type at each iteration where the parameters are updated.",1,neutral
FixMatch [17] proposes multiple networks with shared weights to ensure the output consistency when a variety of perturbations are applied to the unlabeled inputs.,1,neutral
"overall loss is Lmix = Ls + {ymb ≥ }1Lcons , where is a balancing coefficient, and is a threshold to ensure low-entropy predictions [17].",1,neutral
"Inspired by FixMatch [17], SCSeg achieves entropy minimization with a threshold, and consistency regularization by adding multiple perturbations to the encoder output.",2,positive
"Recently, SSL received increasing attention in both image classification (Tarvainen and Valpola, 2017; Berthelot et al., 2019b; Sohn et al., 2020) and text classification (Xie et al.",2,positive
"Specifically, 109 Π-Model (Laine and Aila, 2017) and UDA (Xie 110 et al., 2019b) and FixMatch (Sohn et al., 2020) di- 111 rectly add various perturbations to the input data, 112 Mean-teacher (Tarvainen and Valpola, 2017) uses a 113 teacher model to simulate sample perturbation, and 114 Virtual Adversarial Training (Miyato et al., 2018) 115 skillfully constructs an adversarial sample.",2,positive
"Specifically, 109 Π-Model (Laine and Aila, 2017) and UDA (Xie 110 et al., 2019b) and FixMatch (Sohn et al., 2020) di- 111 rectly add various perturbations to the input data, 112 Mean-teacher (Tarvainen and Valpola, 2017) uses a 113 teacher model to simulate sample perturbation, and 114 Virtual…",2,positive
", 2019b) and FixMatch (Sohn et al., 2020) directly add various perturbations to the input data, Mean-teacher (Tarvainen and Valpola, 2017) uses a teacher model to simulate sample perturbation, and Virtual Adversarial Training (Miyato et al.",2,positive
"Another line of works [20, 25] applies semi-supervised learning [4, 32] that treats wrongly-labeled samples as unlabeled and assigns them with pseudo labels.",1,neutral
"In particular, we adopt FixMatch [32] that selects the high-confidence samples in Duk and relabel them by the model predictions (i.e., pseudo labels) estimated over multiple weak
augmentationsAw(·).",2,positive
"In particular, we adopt FixMatch [32] that selects the high-confidence samples in D k and relabel them by the model predictions (i.",2,positive
"Additionally, FixMatch [26] shows its effectiveness by enforcing consistency constraints on predictions generated by weak and strong augmentation.",1,neutral
FixMatch [26] leverages consistency regularization between weakly and strongly augmented views of the same unlabeled image in a single-stage training pipeline.,2,positive
Reference [5] used consistent regularization and PL to improve the performance of the model.,2,positive
"Some prior work only trains the student on a small number of unlabeled examples having the highest confidence (Rosenberg et al., 2005; McClosky et al., 2006; Sohn et al., 2020).",1,neutral
"This resembles the idea of using the consistency, confidence, and disagreement to improve the model performance in semi-supervised learning [Zhou and Li, 2010, Lee et al., 2013, Sohn et al., 2020].",1,neutral
"…could be presumably regarded as a filter for the labels on heritage values Y HV, to only keep the samples with high inter-annotator (model) agreement [Nowak and Rüger, 2010] as the “ground-truth"" [pseudo-] labels, while treating the others as unlabeled [Lee et al., 2013, Sohn et al., 2020].",0,negative
"Similar operations could be found in semi-supervised learning [Zhou and Li, 2010, Lee et al., 2013, Sohn et al., 2020].",1,neutral
"Despite its simplicity, it has been revealed that ST can find a model with better prediction performance than the model trained by supervised learning on the labeled data only, both empirically (Lee et al., 2013; Yalniz et al., 2019; Sohn et al., 2020; Xie et al., 2020) and theoretically (Zhong et al.",2,positive
"…performance than the model trained by supervised learning on the labeled data only, both empirically (Lee et al., 2013; Yalniz et al., 2019; Sohn et al., 2020; Xie et al., 2020) and theoretically (Zhong et al., 2017; Oymak and Gulcu, 2020, 2021; Wei et al., 2020; Frei et al., 2021; Zhang…",1,neutral
"These methods, such as AugMix [10] and FixMatch [11], include auxiliary loss terms that encourage the model to produce similar distributions over y for different transformations of the same input image.",1,neutral
VAE [9] Reconstruction 7 7 7 Pre AugMix [10] Consistency 3 7 7 Joint FixMatch [11] Consistency 3 7 7 Joint CPC [12] Contrastive 7 7 3 Pre MoCo [13] Contrastive 3 3 7 Pre SimCLR [2] Contrastive 3 7 3 Pre SimCLRv2 [14] Contrastive 3 7 3 Pre BYOL [15] Bootstrap 3 3 3 Pre,0,negative
"However, a drawback of these methods is that they either need a fixed threshold (Xie et al., 2020a; Sohn et al., 2020; Zhang et al., 2021) or an ad-hoc threshold adjusting scheme (Xu et al.",2,positive
", 2020a), FixMatch (Sohn et al., 2020), and FlexMatch (Zhang et al.",2,positive
"In addition to the commonly-chosen labeled amounts, following (Sohn et al., 2020), we further include the most challenging case of CIFAR-10: each class has only one labeled sample.",1,neutral
"Following previous work (Sohn et al., 2020; Xu et al., 2021; Zhang et al., 2021), we conduct experiments with varying amounts of labeled data.",2,positive
", 2020a) and FixMatch (Sohn et al., 2020) retain the fixed high threshold to ensure the quality of useful pseudo labels.",2,positive
"To reduce confirmation bias, confidence-based thresholding techniques are proposed to ensure the quality of pseudo labels (Xie et al., 2020a; Sohn et al., 2020; Zhang et al., 2021; Xu et al., 2021), where only the unlabeled data whose confidences are higher than the threshold are retained.",2,positive
"Recently, the combinations of these two paradigms have shown promising results (Xie et al., 2020a; Sohn et al., 2020; Pham et al., 2021; Xu et al., 2021; Zhang et al., 2021).",2,positive
"Pseudo labeling is a variant of self-training that converts probabilities to hard “one-hot” labels (Lee et al., 2013; Sohn et al., 2020).",1,neutral
"To handle barely supervised setting (Sohn et al., 2020) more effectively, we further propose a class fairness objective to encourage the model to produce diverse (i.",2,positive
"It helps the model produce diverse predictions especially for barely supervised settings (Sohn et al., 2020), and therefore converges faster and generalizes better.",1,neutral
"In this work, we adopt the speech chain reconstruction as a data augmentation method and focus on the FixMatch algorithm [15] which has recently been applied on S2S ASR [5].",2,positive
"Also, these perturbations should change the distribution of input speech without altering the corresponding transcripts [15].",1,neutral
FixMatch algorithm [15] is a semi-supervised algorithm for image classification that combines consistency regularization with pseudo-labeling.,1,neutral
"Our design shares the general consistency regularization spirit whilst differentiating from typical data augmentation based consistency in formulation [39], [36].",2,positive
"(3) Combining consistency regularization and pseudo-labeling, the recent closed-set SSL method FixMatch [36] reasonably performs at a level between [38] and [37].",1,neutral
"Most existing semi-supervised learning (SSL) works [36], [37], [38], [39], [40], [41], [42], [43], [44] make a closed-set assumption that unlabeled training data share the same label space as the labeled data.",1,neutral
"Competitors We evaluate four representative closed-set (PseudoLabel [37], MeanTeacher [38], MixMatch [39] and FixMatch [36]) and three state-of-the-art open-set (MTCR [46], T2T [48] and OpenMatch [49]) SSL methods.",2,positive
"Popular ideas revolve around consistency regularization [5, 6, 38], and self-training [10, 20, 34].",1,neutral
"1) Semi Supervision: Research on semi-supervised learning has mostly been conducted on the image classification setting [12], [13], [14], [15], [16], [17], [13], [18], [19], [20], [21], with interest in segmentation and object detection only rising recently [22], [23], [24], [25],",1,neutral
"1) Self Labeling: The self labeling approach is adapted from FixMatch [21] for image classification, in which we generate the pseudo labels using the model itself.",1,neutral
We follow [21] and only include pseudo labels whose confidence scores are above a threshold.,1,neutral
"A recent line of work in SSL utilizes data aug-
mentations, such as TF-IDF and back-translation, to enforce local consistency of the model (Sajjadi et al., 2016; Miyato et al., 2018).",2,positive
"For example, FixMatch (Sohn et al., 2020), a SOTA semi-supervised method, applies an indicator function to select high confident examples at each iteration.",1,neutral
", 2004) and recent deep learning settings (e.g. Wang et al., 2018b; Sohn et al., 2020; Xu et al., 2021).",1,neutral
"Here SSL can further distill information from unlabeled data and gradually propagate label information from labeled examples to unlabeled one during the training stage (Xie et al., 2020; Zhang et al., 2021c).",2,positive
"This procedure can select examples that are insensitive to transformation g(·) and hence smoother with respect to the changes in the input space (Berthelot et al., 2019b,a; Sohn et al., 2020).",1,neutral
"For example, FixMatch (Sohn et al., 2020), a SOTA semi-supervised",1,neutral
"Few-shot learners suffer from the quality of labeled data (Sohn et al., 2020), and previous acquisition functions usually fail to boost the performance from labeling random sampled data.",1,neutral
"…from Chapelle et al. (2009), which is widely used in both classic machine learning problems (e.g. Blum and Chawla, 2001; Chapelle et al., 2002; Seeger, 2000; Zhu et al., 2003; Zhou et al., 2004) and recent deep learning settings (e.g. Wang et al., 2018b; Sohn et al., 2020; Xu et al., 2021).",1,neutral
"Curriculum Learning (CL) We further combine our acquisition function with advances in semi-supervised learning (SSL) (Berthelot et al., 2019a; Sohn et al., 2020), which also integrates abundant unlabeled data into learning.",2,positive
"Attempts to remedy this have focused either on regularization (He et al., 2020) or on variations of consistency (Xie et al., 2019; Sohn et al., 2020) for a given task–such as round-trip consistency of question generation and answer prediction (Alberti et al., 2019; Puri et al., 2020) for QA or…",1,neutral
", 2020) or on variations of consistency (Xie et al., 2019; Sohn et al., 2020) for a given task–such as round-trip consistency of question generation and answer prediction (Alberti",1,neutral
"positive confidence score would increase by adjusting temperature values of softmax, minimizing entropy.(65) In medical imaging, there are various datasets with class imbalance as follows.",1,neutral
A more closely-related work to our proposed method is FixMatch [48].,2,positive
"More specifically, FixMatch trains the predictions from the strongly-augmented version to match the pseudo-label produced from the weakly-augmented view of the same image.",2,positive
"In this section, the proposed method is compared with unsupervised methods: SCAN [Gansbeke et al., 2020] and SimCLR [Chen et al., 2020], few-shot methods: Prototype Net [Jake et al., 2017] and Simple CNAPS [Bateni et al., 2020], semi-supervised methods: MixMatch [David et al., 2019] and FixMatch [Sohn et al., 2020], transfer learning methods: Transfer(10) and Transfer(100), and fully supervised methods: SiameseNet [Koch et al., 2015], VGG-16 [Simonyan et al., 2014], ResNet-50 [He et al., 2016], MobileNetV2 [Sandler et al., 2018], DenseNet-121 [Huang et al., 2017] and ViT-B/16 [Dosovitskiy et al., 2021].",2,positive
", 2019] and FixMatch [Sohn et al., 2020], transfer learning methods: Transfer(10) and Transfer(100), and fully supervised methods: SiameseNet [Koch et al.",2,positive
80% of the target datasets are used as annotated samples for semi-supervised methods (MixMatch and FixMatch).,2,positive
"…Prototype Net [Jake et al., 2017] and Simple CNAPS [Bateni et al., 2020], semi-supervised methods: MixMatch [David et al., 2019] and FixMatch [Sohn et al., 2020], transfer learning methods: Transfer(10) and Transfer(100), and fully supervised methods: SiameseNet [Koch et al., 2015], VGG-16…",2,positive
"Semi-Supervised Finetuning: In classification task we use FixMatch [3] as the semi-supervised learner, we stick to the original hyperparameters for CIFAR-10, CIFAR-100 and SVHN reported in [3].",2,positive
"Semi-supervised learning (SSL) aims to address learning with small labeled data and large unlabeled data [1]–[3], [20], [22], [23].",1,neutral
"Recent consistency-based SSL methods obtain pseudo-labels for unlabeled data through ensembles of predictions [1], network parameters [2] or weaker augmentations [3].",1,neutral
"We further evaluate two state-of-the-art SSL approaches, Self-Tuning [6] and FixMatch [3] with different initial weights.",2,positive
FixMatch [3] achieved significantly higher performance by,2,positive
"The reported large performance improvement of finetuning + SSL [6], [17] over task-specific SSL [2], [3], [19], [20], inspired us to explore the following questions: 1) What is the key ingredient that enables the success of finetuning + ar X iv :2 20 5.",2,positive
"For CIFAR-10, CIFAR-100 and SVHN, we follow the standard data splits for semi-supervised evaluation as in [3].",1,neutral
"More recently, pseudo-labeling method [31] proposed to integrate the idea of consistency regularization to entropy minimization.",1,neutral
"Besides the Non-IID methods, we utilize personalized federated learning methods combined with FixMatch for comparison: FedPer [9], LG-FedAvg [22] and pFedMe [11].",2,positive
"Further, we take the FedAvg-SL and FixAvg (FedAvg with FixMatch) as the baseline methods.",2,positive
"Compared methods: To validate our method, we compare the performance of UM-pFSSL with several key related methods for Non-IID federated learning: 1) FedProx-SL: FedProx [7] with fully labeled data, 2) FedBN-SL: FedBN [17] with fully labeled data, 3) FixProx: FedProx with semisupervised pseudo-labeling method FixMatch [31] and 4) FixBN: FedBN with FixMatch.",2,positive
"Compared Methods: To validate our method, we compare the performance of UM-pFSSL with several key related methods for Non-IID federated learning: 1) FedProx-SL: FedProx [7] with fully labeled data, 2) FedBN-SL: FedBN [17] with fully labeled data, 3) FixProx: FedProx with semi-supervised pseudo-labeling method FixMatch [31] and 4) FixBN: FedBN with FixMatch.",2,positive
"On the other hand, it can be used as a pseudo-label strategy under the semi-supervised learning framework to achieve end-to-end learning, such as FixMatch [36].",1,neutral
"In addition, the unlabeled target images are strongly augmented for the student model (denoted as Itgt), and weakly augmented unlabeled one I ′ tgt serve as the input of teacher model [25].",1,neutral
"During backbone training, we adopt cross entropy loss as labeled loss and FixMatch [26] consistency loss as unlabeled loss.",2,positive
"Consistency-based SSL has demonstrated superior performance partially due to explicitly exploiting pseudo labels on unlabeled data [22], [27], [26].",1,neutral
Our baseline is conducted with DGCNN backbone trained by FixMatch semi-supervised loss with all unlabeled weights fixed to 1.,2,positive
"The state-ofthe-art SSL method [26] generates pseudo-labels for weakly augmented samples with confident predictions, and then match them with the prediction of the strongly augmented ones.",1,neutral
We migrate them to our point cloud setting and combine them with the same FixMatch SSL.,2,positive
"[22] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D.",0,negative
"There are about four approaches for image classification based on deep semi-supervised learning: CoTraining [15-18], Consistency Regularization [19-22], Mixup [23-25] and Semi-supervised GAN [26-28].",1,neutral
[22] directly takes the high-confidence predictions of weakly augmented images as supervisory signal for the corresponding strongly augmented ones to conduct supervised training.,1,neutral
"The state-of-the-art semi-supervised algorithms are selected for comparison, including Co-Training based algorithms DCT [16], Tri-net [17], PLW-ML [18], and Consistency Regularization based algorithms, including π-Model [19], Mean Teacher [20], UDA [21], FixMatch [22], ACL [49], and algorithms based on Mixup, scuh as MixMatch [24], ReMixMatch [25], SelfMatch [45], RML-CNN [48], and methods based on adversarial training, such as VAT [46] and CCS-GAN [47].",2,positive
"In our BAPS model, the two sub-networks are the modified version of FixMatch [22].",2,positive
"After that, several variants [Berthelot et al., 2019; Sohn et al., 2020] are further proposed to extend its applications.",2,positive
Most prior researches of semi-supervised learning concentrate on achieving the stateof-the-art model accuracy but ignore training efficiency [23].,1,neutral
"Specifically, we regard the probability of label ŷj in the prediction as its confidence [23]:",1,neutral
"see [37], [38], [39], [40]) where the key idea is to ensure that similar samples should yield similar labels.",1,neutral
"…& Bengio, 2005) 72.56±3.33 Pseudo Label (Lee et al., 2013) 74.49±2.26 Soft Pseudo Label 78.44±2.41 Consistency Regularization 79.17±1.79 FixMatch (Sohn et al., 2020) 74.31±3.27
UDA (Xie et al., 2020) 80.01±0.14 10 50 100 500 Pseudo Dataset Size (K images)
72
74
76
78
80
T op
-1 A
cc ur
ac…",0,negative
"Scratch 76.21±1.40 Pseudo Supervised 53.03±1.79 EntMin (Grandvalet & Bengio, 2005) 72.56±3.33 Pseudo Label (Lee et al., 2013) 74.49±2.26 Soft Pseudo Label 78.44±2.41 Consistency Regularization 79.17±1.79 FixMatch (Sohn et al., 2020) 74.31±3.27
UDA (Xie et al., 2020) 80.01±0.14 10 50 100 500 Pseudo Dataset Size (K images)
72
74
76
78
80
T op
-1 A
cc ur
ac y
(% )
BigGAN (ImageNet Pre-trained)
Figure 4: Top-1 accuracy of P-SSL when scaling pseudo dataset size
Label: attaching uniformly sampled source labels, Softmax: using softmax outputs of CAss (default), Temperature Softmax: applying temperature scaling to output logits of CAss and using the softmax output, Argmax: using one-hot labels generated by selecting the class with the maximum probability in the softmax output of CAss , Sparsemax (Martins & Astudillo, 2016): computing the Euclidean projections of the logit of Ct representing sparse distributions in the source label space, and Classwise Mean: computing the mean of softmax outputs of CAss for each target class and using it as representative pseudo source labels of the target class to generate pseudo samples.",2,positive
"We used six SSL algorithms: EntMin (Grandvalet & Bengio, 2005), Pseudo Label (Lee et al., 2013), Soft Pseudo Label, Consistency Regularization, FixMatch (Sohn et al., 2020), and UDA (Xie et al., 2020).",2,positive
"UDA (Xie et al., 2020) and FixMatch (Sohn et al., 2020), which combine ideas of pseudo-label and consistency regularization, have achieved remarkable performance.",2,positive
"More importantly, the methods using hard labels (Pseudo Supervised, Pseudo Label, and FixMatch) failed to outperform the scratch models, whereas the soft label based methods improved the performance.",1,neutral
"Further, DAFormer [28] uses consistency training [50,52,57], i.",1,neutral
"After each training step t, the teacher model gφ is updated with the exponentially moving average of the weights of fθ, implementing a temporal ensemble to stabilize pseudo-labels, which is a common strategy in semi-supervised learning [17,52,57] and UDA [1,39,58]",1,neutral
"In order to regularize the training and to avoid pseudo-label drift, approaches such as confidence thresholding [43, 92], pseudo-label prototypes [39, 80, 81], and consistency regularization [50, 52, 57] based on data augmentation [1, 10, 44, 46], different context [35, 89], domain-mixup [20, 29, 39, 58, 89], or multiple models [79, 87, 88] have been used.",1,neutral
"However, in the early phase of training, consistency regularization regularizes the model towards high entropy predictions, and prevents it from achieving good accuracy[13].",1,neutral
"The MAE learns these unknown regions to reconstruct the masked patches, and achieves excellent results.",2,positive
"1) Batch Random Mask Block: In order to obtain more robust features and make the prediction of the network adapt to complex and changeable environments, we propose a batch random mask (BRM) block inspired by the Masked AutoEncoders (MAE) [50], in which the random patches of the image are masked to reconstruct these missing regions by the model.",2,positive
"Existing pretrain pretext tasks can be divided into discriminative tasks [4, 9, 14, 48] and generative tasks [3, 11, 19, 25, 26, 32, 51, 55].",1,neutral
"Inspired by the works of SimMIM[14], MAE[13], BEiT[21], etc.",1,neutral
"Self-supervised learning has made remarkable progress in addressing the issue of small sample sizes in the domain of natural language processing (NLP) in recent times, the application of this learning algorithm has progressively extended to the domain of computer vision(CV) [12][13][14][15].",1,neutral
"Inspired by the works of SimMIM[14], MAE[13], BEiT[21], etc., our framework is based on self-supervised representation learning of recovered pixels, learns representations by mask
modeling, masks part of the input tongue image signal, masking a portion of the input tongue image signal and predict the raw signal in the masked region.",2,positive
"Our results in Section 5.2 shows that while off-the-shelf
CLIP representations can be poor (especially for RGB-stacking see Figure 6 Right), adapting them through our proposed adapters results in similar performance as other adapted representations (such as MAE ones).",2,positive
", 2020) and masked auto-encoder (MAE) for ViT (He et al., 2022).",2,positive
"Moreover, the adapted representations match the performance of more performant models (e.g. MAE).",1,neutral
"This advantage of MAE features for control is also observed in (Xiao et al., 2022).",1,neutral
", 2020) and for ViT we use masked auto-encoder (MAE) (He et al., 2022).",2,positive
"• Comprehensive evaluation of our approach across 3 different manipulation suites (35 individual tasks), 3 major model architectures (ViTs, NFNets, and ResNets) with supervised (imagenet classification) and self-supervised pretraining (CLIP, BYOL, Visual MAE).",2,positive
"Existing self-supervised pretrained visual models, such as SimCLR (Chen et al., 2020), BYOL (Grill et al., 2020) or Visual MAE (He et al., 2022), have also been shown to provide strong initializations for a wide range of visual downstream tasks.",2,positive
"For NFNet we use CLIP pretraining (Radford et al., 2021), for ResNet we use BYOL (Grill et al., 2020) and for ViT we use masked auto-encoder (MAE) (He et al., 2022).",2,positive
"In addition to imagenet pretraining across all three architectures, we also evaluate using ALIGN (Jia et al., 2021) for NFNet, BYOL for ResNet (Grill et al., 2020) and masked auto-encoder (MAE) for ViT (He et al., 2022).",2,positive
", 2020) or Visual MAE (He et al., 2022), have also been shown to provide strong initializations for a wide range of visual downstream tasks.",1,neutral
"image classification [71, 89, 82, 23], object detection [45, 77, 78, 6], semantic segmentation [87, 80, 69] and so on [70, 41, 86, 47, 24, 40].",1,neutral
"Recently, the prevailing trend has been disrupted by self-supervised learning based on contrastive learning [26, 8, 70] and masked image modeling [2, 25, 74], which learns better transferable representations with only unlabeled images.",1,neutral
"Recently, self-supervised learning [26, 8, 25, 2] has",1,neutral
"Inspired by Masked Auto Encoders (MAE) [11], Point-MAE [22] further explores pre-training methods for point cloud Transformers, implementing a pre-training pipeline for point cloud tasks with a completely standard Transformer structure.",2,positive
"By contrast, self-supervised pre-training methods (He et al., 2020; 2022; Radford et al., 2021; Jia et al., 2021) can be easily scaled to billions of unlabeled examples by designing an appropriate pretext task, such as solving jigsaw puzzles (Noroozi & Favaro, 2016), invariant mapping (Chen & He,…",1,neutral
MAE [10] applies the concept of reconstruction from BERT to the domain of computer vision.,1,neutral
"Because of its large model capacity and generalizing capability, this transformer-based architecture is widely used in LVMs. MAE [8] applies the concept of reconstruction from BERT to the domain of computer vision.",1,neutral
"To demonstrate that the performance of the foundation model improves with the increase in the number of model parameters when pretrained using the same number of datasets in the remote sensing field, we pretrain models with different
KEUMGANG CHA et al.: A BILLION-SCALE FOUNDATION MODEL FOR REMOTE SENSING IMAGES 5
numbers of parameters using MAE [5] and the large-scale remote sensing imagery dataset, MillionAID [44].",2,positive
This is because the amount of pretraining data (MillionAID) and the methodology used in pretraining (MAE) should be the same for fair comparison.,0,negative
"AID) [44], and pretraining method (MAE) [5].",1,neutral
"Clearly, models pretrained with MAE outperform those pretrained with IMP, with mAP differences ranging from 3.85 to 5.05.",0,negative
"However, since models with a large number of parameters can experience overfitting to the pretext task (MAE in this paper), the models are pretrained with 400 epochs using the AdamW optimizer [95] and a batch size of 2048.",2,positive
CV BYOL [1] ResNet200 2x 375 Million SimCLR v2 [2] ResNet152 3x w sk 795 Million DINO [3] ViT Base 84 Million iBOT [4] ViT Large 307 Million MAE [5] ViT Huge 632 Million ALIGN [6] EfficientNet-L2 800 Million CLIP [7] ViT Large 307 Million SEER [8] RegNety-256gf 1.,0,negative
The one is to predict the pixel values of the masked area by MSE loss [5].,1,neutral
"As expected, the performance of models pretrained with MAE is higher than those pretrained with IMP.",0,negative
numbers of parameters using MAE [5] and the large-scale,1,neutral
"In the original MAE, pretraining is applied with 1600 epochs [5].",1,neutral
The MAE means MAE on the MillionAID.,0,negative
"masked image modeling randomly masks parts of an image and learns to reconstruct the masked part [5], [53].",1,neutral
"learning papers using vision transformers in computer vision, only classification and semantic segmentation performance are introduced, and it is challenging to find object detection performance [3], [5], [55], [92].",1,neutral
"Specifically, the dataset, pretraining method, and foundation model structure are the same as in previous research, namely MillionAID [44], MAE [5], and vision transformer [43].",2,positive
"In this section, we discuss the details of the model architecture (vision transformer) [43], pretraining dataset (MillionAID) [44], and pretraining method (MAE) [5].",2,positive
2) MAE: MAE [5] learns representations by reconstructing randomly masked images using the encoder-decoder structure of the vision transformer.,1,neutral
"a Transformer[70], while features are learned through self supervision (such as masked input reconstruction) on large datasets [20, 49, 5, 28].",1,neutral
"For our default model, we re-use weights from the publicly available MAE model.",2,positive
"Following MAE [28], the encoder processes only the visible part of the inputs for all stages.",1,neutral
Stage 1: We follow settings from MAE [28].,0,negative
"Following [28], we adopt an encoder-decoder architecture during training and ask the model to reconstruct patches that are randomly masked out, at the pixel level.",2,positive
"We use masked input prediction[20, 28, 5, 75] objective for unimodal stages.",1,neutral
"The difference between our approach and [20, 49] is that we follow the encoder-decoder structure in [28], where masked tokens are removed for the encoder and are reconstructed through a separate decoder.",2,positive
[28] shows that ViT better generalizes under pixel-level supervision with aggressive masking.,1,neutral
"Self-supervised learning has proven to be a very effective training mechanism for learning representations that transfer well to various downstream tasks[20, 28, 57, 4].",1,neutral
"In Computer Vision, Masked Image Modeling (MIM) [11, 36, 3, 41, 2] also gains significant popularity for self-supervised representation learning.",1,neutral
"The success of MLM [8] and MIM [3] demonstrate that mask-based pretraining helps learn global and generalizable
features, which is beneficial to various downstream tasks.",1,neutral
"• RGB mask Following MIM [41, 11, 36, 14, 3], we randomly mask blocks of 2D input images and take an additional UNet-like decoder after the feature extractor to reconstruct the masked regions.",2,positive
"Note that the masked language modeling (MLM) [8] in natural language processing and the masked image modeling (MIM) [3, 41, 11, 14] in computer vision both mask the inputs to the model and predict the masked components in the output space, enforcing the model to learn global features from the neighbour words or pixels.",1,neutral
"Motivated by the fact that masked modeling [3, 8, 41, 14] , i.",1,neutral
"Pre-training techniques, as one of the self-supervised learning approaches, can leverage a big model to learn the general representations with amounts of unlabeled dataset [12, 19, 33, 43].",1,neutral
"Large pre-trained models have achieved substantial results in many areas including natural language processing [12, 33], computer vision [7, 19] and software engineering [2, 15, 17, 18, 64].",1,neutral
", 2018), denoising or recovering partiallymasked input (Vincent et al., 2008; He et al., 2022), or encouraging learned representations to be invariant to a set of handcrafted representations (Chen et al.",1,neutral
"…(Noroozi & Favaro, 2016), predicting rotations (Gidaris et al., 2018), denoising or recovering partiallymasked input (Vincent et al., 2008; He et al., 2022), or encouraging learned representations to be invariant to a set of handcrafted representations (Chen et al., 2020; Caron et al.,…",1,neutral
Understanding the inherent spatial redundancy in local representations proves beneficial for learning visual representations for segmentation tasks [4].,1,neutral
"Following (He et al., 2022), during linear probing we include BatchNorm without the affine transformation before the final classifier.",1,neutral
", 2021) or masked reconstruction (He et al., 2022) objectives.",2,positive
"This can be operationalized using contrastive (Radford et al., 2021; Jia et al., 2021) or masked reconstruction (He et al., 2022) objectives.",2,positive
", 2018), images (He et al., 2022), videos (Feichtenhofer et al.",2,positive
"For audio, we use a fixed 2D sinusoidal positional encoding as described in (Huang et al., 2022) and (He et al., 2022).",2,positive
"Most previous approaches have focused on single modalities, such as text (Devlin et al., 2018), images (He et al., 2022), videos (Feichtenhofer et al., 2022), and audio (Baade et al., 2022; Chong et al., 2022).",2,positive
"MAE (He et al., 2022) is pre-trained by the masked image modeling (MIM) task (Bao et al., 2021) inspired by the success of masked language modeling in NLP (Devlin et al., 2018).",2,positive
"Masked Image Modeling (MIM) (Bao et al., 2021) pursues a different direction by learning to predict removed pixels (He et al., 2022), discrete visual tokens (Peng et al., 2022), or pre-computed features (Wei et al., 2022).",1,neutral
"MAE (He et al., 2022) is pre-trained by the masked image modeling (MIM) task (Bao et al.",2,positive
", 2021) pursues a different direction by learning to predict removed pixels (He et al., 2022), discrete visual tokens (Peng et al.",1,neutral
"ViT-B/16: MoCo v3 (Chen et al., 2021b), DINO (Caron et al., 2021), MAE (He et al., 2022), iBOT (Zhou et al., 2021) and CLIP (Radford et al., 2021).",0,negative
"Because most largescale vision models are based on masked image modeling (MIM) [12, 3, 6, 2], different prompt fusion methods activate knowledge at different locations in the large-scale model, affecting the downstream task performance.",1,neutral
"While these models can achieve impressive results on many tasks [12], they often require massive amounts of data and computation to train, making them impractical for many real-world applications.",1,neutral
"Previous studies [35, 3] have focused on combining images and labels into a new image and using MIM for pre-training, resulting in models with in-context learning capabilities.",1,neutral
The efects of model capacity have attracted signifcant attention in other felds like CV [15] and NLP [4] as it is demonstrated that SSL can largely beneft from increasing model parameters.,1,neutral
"SAM has already shown remarkable potential in accurately segmenting objects in realworld scenarios; its extensive training and zero-shot learning allow it to respond appropriately to any prompt at inference time [17, 18].",2,positive
"Recently, masked autoencoding has become a very popular approach for self-supervised Visual Transformer (VT) pre-training [2,7,16,20,35,45,60,61], together with other alternatives including contrastive learning [8,9,23,47,68,72,74] and selfdistillation [6, 18, 28].",1,neutral
"Among the various self-supervised pre-training strategies, masked autoencoding [2, 16, 20] is a prominent approach that has been widely explored.",1,neutral
"Due to its natural compatibility with the tokenwise representation in VTs, masked autoencoding has been explored for pre-training VTs on data across many fields, such as RGB images [2, 20, 61], pose data [11, 33] and 3D data [71].",1,neutral
"These works [2, 6, 20, 60] generally pre-train VTs on a large dataset in a self-supervised manner, allowing them to extract semantically meaningful and generalizable features without the need for annotated",1,neutral
"In many cases, we have access to large-scale unlabeled data containing such corruptions and noise, and would like to exploit them to pre-train a VT that can be effectively adapted to a wide range of downstream tasks [2, 11,20, 61].",2,positive
"Having rapidly risen in popularity in recent years, Vision Transformer (ViT) [16] and its variants [19] have shown impressive performance across various computer vision tasks, such as image classification, video recognition and 3D action analysis [2, 20, 31, 51, 60].",1,neutral
"Amongst this wave of research on Visual Transformers (VTs), there has emerged a popular paradigm – self-supervised VT pretraining [2, 6, 20, 60] – which has attracted a lot of attention in the research community.",1,neutral
"Previous works [2,16,20] have explored masked autoencoding to train Transformers, which we follow.",1,neutral
"Masked autoencoding [2,11,16,20,33,60,61,71] works by randomly masking a portion of input tokens or patches, and letting the VT reconstruct them.",1,neutral
"Due to the popularity of pre-training methods and their potential widespread applications in realworld scenarios [2, 11, 20, 33, 35, 71], it becomes important to improve their robustness to corrupted data, but this is often overlooked by previous methods.",1,neutral
"There’s another line of self-supervised learning work [2, 25] based on vision transformers, which naturally uses fixed-size patch level representation due to the structure of the vision transformers.",1,neutral
"One popular solution to this problem is the pretraining-finetuning approach, which has gained widespread adoption in natural language processing [7, 8] and computer vision [10, 11].",1,neutral
A missing component in most previous SSL studies (except MAE [15]) is input normalization although it is a basic and indispensable preprocessing step for effective training.,1,neutral
"We conducted experiments on six SSL methods: DINO [4], MoCo v3 [7], iBoT [35], Mugs [36], MAE [15], and MSN [1], using the Vision Transformer architecture (ViT) [10].",2,positive
MAE [15] is one of the representative methods of the masked image modeling (MIM) approach.,1,neutral
"Many studies have proposed effective learning strategies: contrastive learning that performs instance discrimination based on randomly augmented views [30, 5, 7, 2], a teacher-student framework that trains representations by using outputs of a momentum encoder as supervision [13, 11, 4, 21], and masked image modeling [15, 3, 23, 35] that aims to reconstruct randomly masked patches.",1,neutral
"At the same time, the pre-training & finetune paradigm has broadly applied to various visual recognition tasks because loading a pre-trained model usually can boost training convergence and performance [1, 8, 33, 14].",1,neutral
"Self-supervised learning methods [9, 14, 15] aim to pretrain a visual backbone with rich semantic representation, while the other parts of detectors designed for downstream tasks are ignored and usually initialized randomly.",1,neutral
"[83] also use a similar training strategy to pretrain a CV model, which makes a great success on the downstream tasks in the CV community.",2,positive
"Masked autoencoder (MAE) [83] develops an asymmetric encoder-decoder architecture to couple the self-supervised reconstruction and backend training, yielding a promising transfer performance for the downstream tasks.",2,positive
"Compared to MAE, MB1 outperforms MAE by 2% in both UF1 and UAR, approximately.",2,positive
"We utilize the encoder and decoder parts of μ-BERT (without DMA and POI) to train previous SSL methods (MoCo V3 [4], BEIT [1], MAE [9]) and then continue learning the MER task on the large-scale database CASME3 [14].",2,positive
"Three self-supervised methods (MoCo V3, BEIT, and MAE) got better results when they were pretrained on CASME before fine-tuning to the recognition task.",1,neutral
"We utilize the encoder and decoder parts of µ-BERT (without DMA and POI) to train previous SSL methods (MoCo V3 [4], BEIT [1], MAE [9]) and then continue learning the MER task on the large-scale database CASME3 [14].",2,positive
"Especially, MAE [9] achieves 3.5% and 2% up on UF1 and UAR compared to ViT-S, respectively.",2,positive
"He et al., [9] presented a Masked Auto Encoder (MAE) that utilizes the BERT framework.",2,positive
"Transformers and deep learning have significantly improved results for many tasks in computer vision [1,7,9,22, 23, 26, 27, 30, 40, 41].",1,neutral
", [9] presented a Masked Auto Encoder (MAE) that utilizes the BERT framework.",2,positive
"Regarding the pixel prediction, the per-patch normalization [34] consistently helps the fine-tuning accuracy.",1,neutral
MAE [34] predicts pixel colors with an efficient asymmetric architecture.,1,neutral
"3), and is outperformed by concurrent self-supervised pre-training algorithms such as Masked Autoencoders (MAE) [34].",1,neutral
scratch pre-trained MAE [34] ViT-L 304M 82.,0,negative
"6% top-1 [34], which aligns with our observations on fine-tuning the pre-trained ADM.",0,negative
"Inspired by the work of SimpleClick [35], we employ large models for feature encoding, such as the widely used MAE-pretrained Vision Transformer (ViT) [21].",2,positive
"For instance, ViT-Base (ViTB) [21] patchifies the input image of size H ×W into a sequence of 16×16 patches, which are then projected intoC0dimensional vectors.",1,neutral
"As our ViT backbones are pre-trained on 224 × 224 pixel images by MAE [21], we resize the pre-trained absolute positional embeddings to match the size of our images.",2,positive
"More recently, FocalClick [10] employed SegFormer for interactive segmentation, and SimpleClick [35] introduced the MAEpretrained ViT [21] into interactive segmentation.",1,neutral
"In the global contrast branch, we use the SGD optimizer to pre-train a ResNet-50 [19] (for MAE [16], we use ViT-base [11].) encoder Eg for 200 epochs.",2,positive
"In the global contrast branch, we use the SGD optimizer to pre-train a ResNet-50 [19] (for MAE [16], we use ViT-base [11].",2,positive
"Recently, He et al. propose MAE [33] and yield a nontrivial and meaningful generative self-supervisory task, by masking a high proportion of the input image.",1,neutral
Global Pre- ImageNet Relative Rotation MAE BYOL SimSiam training Method [28] Loc [10] Pred [21] [16] [14] [7] w/ MoCo [17] 78.,1,neutral
"Inspired by the masked autoencoding module, et al. [30] generalized the concept of MAE [13] to 3D point cloud and achieved some improvements.",2,positive
[30] generalized the concept of MAE [13] to 3D point cloud and achieved some improvements.,1,neutral
"The masked attention is widely used [4, 9, 25] for invalid-token masking, self-supervised training [19, 44], image inpainting [25], etc.",1,neutral
"For self-supervised pretraining, we take inspiration from recent contrastive learning and masked image modeling methods [3, 8, 11, 13, 34, 118] as they can learn both objectlevel global representations and part-level local features.",2,positive
"• MAE: Masked Autoencoders [15]: ViT-B16, ViTL16.",2,positive
"Many examples exist in the image domain for the training of representation models via solving explicit proxy tasks [16,17,55,59,83], discriminating instances through contrastive learning [10,27,32,72], optimizing clustering and representation [2,7,8], bootstrapping knowledge with self-distillation [9,11,23] or image reconstruction with masked autoencoders [3, 26].",1,neutral
"Originally inspired by the way human learning works, pre-training is now a fundamental element of high performance DL [194, 195].",1,neutral
"Motivated by scalability and powerful pretraining methods, we use an MAE [47] pre-trained Vision Transformer (ViT) [33] minimally adapted to process high resolution inputs [62].",2,positive
"Unless otherwise specified: (1) SAM uses an MAE [47] pre-trained ViT-H [33] image encoder and (2) SAM was trained on SA-1B, noting that this dataset includes only automatically generated masks from the final stage of our data engine.",0,negative
"Motivated by scalability and access to strong pre-training, we use an MAE [47] pre-trained Vision Transformer (ViT) [33] with minimal adaptations to process high resolution inputs, specifically a ViT-H/16 with 14×14 windowed attention and four equally-spaced global attention blocks, following [62].",2,positive
We initialize SAM from an MAE [47] pre-trained ViT-H.,2,positive
"While our model is initialized with a selfsupervised technique (MAE [47]), the vast majority of its capabilities come from large-scale supervised training.",2,positive
"For a comprehensive comparison, we also compare TDMR with MRKD which means a simple combination of model reprogramming (MR) and knowledge distillation (KD), and transfer learning method linear probing [7], [80].",2,positive
"For the foundation model application to downstream tasks, a common transfer method is the linear probing [7], [80] which just modifies the output dimension of the teacher classifier to the total number of categories of the target data.",1,neutral
"Subsequent work such as ViT [13] and MAE [21] have adapted this approach to the computer vision domain with great success, and transformer models trained on large scale data have become the de facto computer vision backbone.",2,positive
"Subsequent work such as ViT [13] and MAE [21] have adapted this ap-1 Work mainly done while author was at Allen Institute for AI proach to the computer vision domain with great success, and transformer models trained on large scale data have become the de facto computer vision backbone.",2,positive
Backdoored MAE vs. defended MoCo-v3.,0,negative
We compare finetuned ViT-B models trained with MoCo-v3 and MAE in Table 5.,2,positive
"We find that MoCo-v3 defended with PatchSearch and i-CutMix is better than MAE both in terms of Acc and FP for 1% labeled
finetuning data, but MAE quickly catches up in the 10% regime.",2,positive
"Restrictions apply.
from [44] indicate that MAE is robust against backdoor attacks.",0,negative
"In addition to ResNet-18 [27], we also conduct experiments on ViT-B [17] with MoCo-v3 [13] and MAE [25].",2,positive
NGswin Dense connection [20] Merged multi-scale encoder features Asymmetric [17],2,positive
"As this recovery process requires the information in the surrounding areas of each pixel [7, 17, 82] and CNN is conventionally good at extracting local features, proper use of CNN is essential.",2,positive
", computer vision [42, 63, 100], which suggests that Transformer has become a unified backbone architecture for both NLP and computer vision.",2,positive
"In contrast, self-supervised vision models [4, 14, 13] learn to encode pixels by keeping the representation of different augmented views being consistent.",1,neutral
"Compared to the above text-supervised models, selfsupervised vision models show some emerging properties on grouping pixels into spatially-consistent regions [4, 13, 14, 6].",1,neutral
"Self-supervised learning, also known as unsupervised learning, aims to learn good visual representations of images without any human-defined labels [4, 13, 14, 6].",1,neutral
"Self-supervised vision methods [4, 13, 14] learn to encode pixels into semantic features by keeping different augmented views consistent, and the consistency can be further enhanced with the self-distilling process [12].",1,neutral
"Considering that the pixels in images have heavy spatial redundancy [13], they are encoded inconsistently, resulting in coarse and spurious groupings of regions in Fig.",1,neutral
"Lots of methods [14, 15, 41] are built to enhance the representation learning of models and achieve superior results in downstream tasks.",1,neutral
"Following previous pre-training approaches [14, 25], we use the default image input size of 224×224.",2,positive
Masked autoencoder (MAE) [14] randomly masked patches and reconstructed the missing region.,1,neutral
"Motivated by the success of unsupervised learning in NLP, some self-supervised learning methods [2, 6, 14, 15, 28, 38, 40, 41] are introduced for vision tasks.",1,neutral
Comparison with Hiera [52]: We show class-level performance (average precision and relative gain) of Hiera [52] (pre-trained on using MAE [24]) and ours.,2,positive
"Overall our method has a gain of +2.8 mAP compared to Video MAE [15, 49].",0,negative
"To further explicitly supervise motion feature generation, inspired by MAE [18], we design a decoder (Figure 3(b)) to reconstruct pixel motion dynamics.",2,positive
"MaskDP, on the other hand, leverages masked auto-encoding (He et al., 2022), a bi-directional sequence modeling technique to improve the generalization of BC.",1,neutral
"While the most straightforward way to augment diverse visual features is to employ discrete hard masks as recent trends [13, 45], which aim to reconstruct images, our approach suppresses information in images using the soft masks with real-valued ar X iv :2 30 4.",2,positive
"As the core of VLM, various vision-language pre-training objectives [14], [18], [20], [26], [81], [82], [83], [84] have been designed for learning rich vision-language correlation.",2,positive
9: Illustration of masked image modelling [82].,1,neutral
"The detailed pre-training hyper-parameters are in Table 7, which mainly follows the training settings used in the original MAE [37].",0,negative
"Following the autoencoding pipeline in the original MAE [37], the encoder only takes visible embedded patches as input, and the decoder is input with all the embedded patches for masked patch reconstruction.",2,positive
"Inspired by masked language modeling [6,23], masked image modeling (MIM) approaches are proposed for learning unsupervised image [37, 92] or video representations [31, 74], which have been shown to be effective for many downstream tasks including image classification, object detection and video action recognition.",1,neutral
The masked autoencoder (MAE) model [37] consists of an encoder and a decoder.,2,positive
"As can be seen, the implementation is simple and neat, which could be flexibility incorporated into existing approaches like MAE [37].",2,positive
", 75%) and training hyper-parameters of MAE [37] to pretrain the TwinMAE and DropMAE models.",2,positive
"Inspired by the great success of self-supervised learning in NLP, recent advances [37, 92] in computer vision suggest that training large-scale vision transformers may undertake a similar trajectory with NLP.",1,neutral
The seminal work MAE [37] reconstructs the input image from a small portion of patches.,1,neutral
"MAE [12] exploited an autoencoder architecture to reconstruct the raw normalized RGB pixels of the masked patches, without the need of passing masked tokens into the encoder.",2,positive
"Take MAE[12] for example, it masks patches of the source images, feeds the
ar X
iv :2
30 4.",1,neutral
"Recently, Masked Image Modeling(MIM)[12, 2, 29, 31, 1] methods have aroused great interest in the community.",1,neutral
"Take MAE[12] for example, it masks patches of the source images, feeds the ar X iv :2 30 4.",1,neutral
"In recent years, Autoencoder (AE)-based methods predominantly rule the DR space [20, 21, 22] where multiple variations of AE have been developed to address the problem of ‘Curse of Dimensionality’ in several application domains including computer vision and computational biology.",1,neutral
"SpectralMAE requires performing masking operations in the spectral dimension, in contrast to imageMAE [36], which applies random masking operations in the spatial dimension.",1,neutral
"Additionally, some other self-prediction methods are designed to drop a part of the input signal and recover the missing input component for the predicted output in training (Devlin et al. 2018; Bao et al. 2022; He et al. 2022).",1,neutral
"The recently proposed MAE [10] follows the high-level idea of masked auto-encoding meanwhile carefully designing the masking strategy, the encoder and the decoder according to the properties of images.",2,positive
"For example, BERT [9] in NLP and MAE [10] in CV adopt self-supervised learners through pre-training to leverage the inherent co-occurrence dependencies of data so that they can capture the underlying general and universal patterns of sentences and images, respectively.",1,neutral
"As proven in [10], a narrow decoder is enough for the MAE task, so we set L′′ to 1.",1,neutral
The computational challenges introduced by global attention mechanisms were later addressed by Masked Autoencoders (MAE) through high image masking strategies [3].,1,neutral
", the LaCViT-trained MAE [3], achieves an increase of 10.",1,neutral
"Transformers have significantly advanced the field of computer vision, particularly in tasks such as image classification [1, 2, 3].",1,neutral
"While several works have attempted to mitigate these issues by either incorporating convolutional neural networks or modifying the transformer architecture [8, 9, 10], these approaches often negate the native advantages of transformers such as training efficiency and scalability [2, 3].",1,neutral
"• Extensive experiments on eight image classification datasets, shows that LaCViT significantly outperforms baseline models (e.g., the LaCViT-trained MAE [3], achieves an increase of 10.78% on Top-1 Accuracy compared with the original MAE on CUB-200-2011).",2,positive
"Since VC-1 was trained with MAE (He et al., 2021), it captures features that are generally useful for reconstructing images.",2,positive
"In this work, we use one such pre-training algorithm (MAE (He et al., 2021)) to explore scaling and adapting pre-trained visual representations (PVRs).",2,positive
"We now ask: how much does the relevance and diversity of the pre-training dataset and the model size matter? To study this, we fix the pre-training objective – MAE (He et al., 2021) – and then vary the composition of the pre-training dataset and the size of the visual backbone (ViTB with 86M parameters and ViT-L with 307M parameters).",2,positive
"Recently, a flurry of works have proposed using the vision transformers (ViTs) (Dosovitskiy et al., 2021) with masked image modeling (He et al., 2021; Baevski et al., 2022a; Yao et al., 2022), which among other benefits reduces the computation time required for pre-training.",2,positive
"The last few years have seen increasing interest in the self-supervised learning (SSL) of visual representations (He et al., 2021; Caron et al., 2020; Baevski et al., 2022b; Chen et al., 2020; 2021).",2,positive
"Experiment Details of Training PVRs
To train the MAE models, we use the official codebase released by the authors on GitHub (He et al., 2021) and use the default hyperparameters provided by the repo to train the ViT-B and ViT-L models.",2,positive
", 2022b), or reconstructive (Bao et al., 2021; He et al., 2021) objectives for training.",2,positive
"Specifically, in MAE adaptation we continue training the backbone network with the MAE (He et al., 2021) pre-training objective on task-specific data.",2,positive
"These algorithms use contrastive (Chen et al., 2020; 2021), distillation-based (Caron et al., 2020; Baevski et al., 2022b), or reconstructive (Bao et al., 2021; He et al., 2021) objectives for training.",2,positive
"We train vision transformers (ViT-B and ViT-L) (Dosovitskiy et al., 2020) on these 4 datasets using Masked AutoEncoding (MAE) (He et al., 2021), and systematically analyze their performance on CORTEXBENCH.",2,positive
", 2020) on these 4 datasets using Masked AutoEncoding (MAE) (He et al., 2021), and systematically analyze their performance on CORTEXBENCH.",2,positive
"End-to-end (E2E) fine-tuning with a task-specific loss function can in-principle capture both of the aforementioned benefits of adaptation, and is widely used in computer vision literature (He et al., 2020; Caron et al., 2021; He et al., 2021; Baevski et al., 2022b).",1,neutral
", 2021) with masked image modeling (He et al., 2021; Baevski et al., 2022a; Yao et al., 2022), which among other benefits reduces the computation time required for pre-training.",2,positive
"To study this, we fix the pre-training objective – MAE (He et al., 2021) – and then vary the composition of the pre-training dataset and the size of the visual backbone (ViTB with 86M parameters and ViT-L with 307M parameters).",2,positive
"As the final step in this paper, but as a first step towards this open problem, we study adapting VC-1 with either task-specific training losses or datasets (via MAE (He et al., 2021)) to specialize VC-1 for each domain.",2,positive
"In this work, we refer to a certain attribute of an element as a field and formulate the various design tasks as a unified masked field prediction, which is inspired by the recent masked autoencoders [9,15] and multi-task models [19,36].",1,neutral
CV community [66] and has been successfully applied to,2,positive
"Results for the Masked Autoencoder (He et al., 2022) (Appendix B.",0,negative
MAE [23] proposed a simple yet effective asymmetric framework for masked image modeling.,1,neutral
Self-supervised learning has achieved remarkable results on large-scale image datasets [23].,1,neutral
"MAE [21] encoder, with reduced embedding dimension to 480, depth to 8, and heads to 8 for efficient computation.",1,neutral
"The structural configuration of the dual-attention Transformer follows the design of the MAE [21] encoder, with reduced embedding dimension to 480, depth to 8, and heads to 8 for efficient computation.",2,positive
"Masked prediction has been successful in unimodal areas such as language (BERT [81], GPT [82]), and vision (MAE [19]), and its popularity has been increasing in multimodal areas due to its ability to unify different modalities.",1,neutral
", as famously realized by instance discrimination [18] or masked prediction objectives [19].",1,neutral
"As shown in Table 8, the CSformerT pre-trained for 30 epochs significantly performs better than without pre-training, but worse than pre-trained for 60 epochs, which shows that the performance improves steadily with longer pre-training [29].",0,negative
"Specifically, we apply a linear layer to project the latent features Fl to patch pixels [86], and compute the mean squared error (MSE) between the reconstructed and original images on the masked pixels [29].",1,neutral
"MAE [29] finds that masking a high proportion of the input image can contribute to meaningful self-supervised learning, and proposes an asymmetric encoder-decoder structure to reduce pre-training time.",1,neutral
"Among them, masked autoencoders (MAE) [7, 110, 29, 86, 28], which pre-train image models by predicting masked tokens from seen tokens, have demonstrated superior learning ability and scalability on various high-level vision tasks.",1,neutral
"Similar to MAE [29] and SIMMIM [86], our MAEIP is a simple autoencoding approach, which masks a portion of image signals and learns to reconstruct them.",2,positive
Learning image representation is a more difficult task [29].,1,neutral
"Masked autoencoders such as BEiT [7], MAE [29], and SimMIM [86], first embed image patches to tokens, and then adopt a random mask on input tokens.",1,neutral
"We follow the conventions in [29, 86] and mask random patches with 16× 16 pixels, and adopt a high masking ratio i.",2,positive
"Besides the improvement of architectural design, recent self-supervised learning frameworks, such as DINO [10], MOCO-V3 [17], MAE [29], have further unleashed the potential of ViT and achieved high performance on various high-level vision tasks [32, 30].",2,positive
"Following MAE [27], ẑ is then “unmixed” to recover the input batch before mixing by inserting a special [MASK] token with M j .",1,neutral
"Specifically, adding color jittering, an essential augmentation technique of contrastive learning [9], with MAE [27] even degrades transfer results, suggesting that MIM might possess a different preference for data augmentations, and the effective data augmentation strategies for MIM are still an open question.",1,neutral
", random [3, 27], attention-guide [31] and sample-dependent [50]).",1,neutral
"In this paper, we explore the usage of image mixing, a commonly used technique in both supervised [60, 61] and contrastive learning [49, 59], with MAE [27].",1,neutral
"and the reconstruction target due to the redundancy of image signals [27], naı̈ve mixing will instead increase the MI, and thus, ease the reconstruction pretext task.",1,neutral
"In this section, we start by adopting mixing in MAE [27] with a simple baseline in Sec.",1,neutral
"Specifically, MixedAE surpasses MAE [27] consistently with only 3% extra overhead, while outperforms the strong iBOT [63] with only 53.",0,negative
MAE [27] proposes an asymmetric encoder-decoder architecture for better efficiency.,2,positive
", visual tokenizers [3, 17], pixels [27,58], graphical features [54] and instance discrimination [2, 19, 63]) and masking strategies (e.",1,neutral
"Instead of random masking [3, 27], AttMask [31] proposes a novel attention-guided masking strategy by masking according to the attention map of the final Transformer layer, while ADIOS [50] introduces an adversarial objective between masking and reconstruction to generate learnable masks for MIM pre-training.",2,positive
"Recently, breakthrough frameworks have been developed based on masked image modeling (MIM) (He et al., 2022; Bao et al., 2021; Tong et al., 2022).",2,positive
", 2021), MAE (He et al., 2022), and CAE (Chen et al.",2,positive
"By reconstructing pixels, MIM methods produce visual features that are more generalizable.",1,neutral
"Meanwhile, the feature representations via SSL are more generalizable to benefit downstream recognition scenarios (Grill et al., 2020; Chen et al., 2021; Xie et al., 2021; He et al., 2022; Wang et al., 2021).",2,positive
"Implementation Details: We follow most of the practices of [1, 8].",2,positive
"To this end, we closely follow the approach of MAE [8], and employ the Masked Image Modeling strategy.",2,positive
"With the advent of Vision Transformers (ViT) [7], similar strategies such as Masked Image Modeling have been developed for computer vision [2, 8, 24], showing equally high benefit in complex computer vision tasks.",1,neutral
We tailor the MAE approach for the endoscopic setting with three modifications: Layer Wise Learning Rate Decay: The MAE encoder and decoder consist of several layers.,2,positive
"Among the self-supervised learning tasks, masked image modeling (MIM) [3, 23, 58, 62, 64, 68] achieves SoTA finetuning performance on ImageNet [14].",1,neutral
"While iGPT [10], ViT [17], and BEiT [3] adopt sophisticated paradigm in modeling, MAE [23] and SimMIM [63] show that directly regressing the masked continuous RGB pixels can achieve competitive results.",1,neutral
"Paired Masked Image Modeling (MIM) MIM is extensively adopted in image classification task [23, 63].",1,neutral
"Recently, researchers use semantic masks to facilitate representation learning [4, 8, 37], where a mask predictor is required.",1,neutral
"Recently, masking strategy has been widely utilized in various language [9, 23, 2] and visual applications [1, 12, 41, 38, 43, 28] to learn meaningful representation.",1,neutral
"Especially, the image masking strategy is used to pre-train a large capacity backbone model to learn general representation for various downstream tasks, such as recognition [12, 41], video applications [38], 3D application [28].",1,neutral
"This unsupervised learning style normally requires numerous data and computation resources [41], [42], so we put the training of it on the resourceful cloud which can collect a lot of data from multiple edges.",2,positive
"Inspired by the progress of Masked Image Modeling (MIM) in image classification [12,39,40], P-STMO [32] applies Masked Joint Modeling to 3D HPE with self-supervised learning.",1,neutral
"Kaiming He recently proposed a patch-level occlusion and reconstruction model called MAE [18], which is based on the ViT [13] autoencoder, dif-",2,positive
"Nevertheless, since the introduction of the MAE[18] method, autoencoders based on pure Transformer[34] have gained attention and have been applied to a variety of downstream tasks[9, 15, 37].",1,neutral
"In this section, we will first revisit the original image-version Masked Autoencoders (MAE) (He et al., 2022) and the video extension of Masked Autoencoders (Feichtenhofer et al., 2022), then detail the tailored MAE for the specific tasks.",2,positive
"In detail, (a) depicts the original MAE proposed by He et al (He et al., 2022), (b) represents the customized version of MAE pre-trained on task-specific data, and (c) is tailored by replacing the transformer architecture of the original MAE with the pure convolution neural network.",2,positive
"To implement our belief in a regressive manner where the learned loss function can be treated as image prior, the first choice is the masked autoencoders (MAE) proposed by He et al (He et al., 2022).",2,positive
"In this section, we will first revisit the original image-version Masked Autoencoders (MAE) (He et al., 2022) and the video extension of Masked Autoencoders (Feichtenhofer et al.",2,positive
"To solve this issue, we adopt an approach inspired by MAE [22], where we only feed the non-masked embeddings to the student .",2,positive
"Due to the sucess of pre-trained Transformer architectures in various domains [2, 9, 16, 22], we recently see a shift towards pre-training Transformer-based approaches for point clouds [28, 31, 49, 51].",1,neutral
[22] show that moving masked embeddings to a deferred shallow decoder reduces memory requirements and training time significantly.,1,neutral
"At the same time, self-supervised training has shown impressive results in natural language processing [16, 47], speech [3, 25], and 2D vision [2, 9, 12, 21, 22], enabling learning of meaningful representations from massive unla-",1,neutral
"This is an important difference to other masked-prediction methods such as BERT [16] and MAE [22], where the targets only comprise local information, e.",1,neutral
"The success of self-supervised learning in 2D vision [2, 4, 5, 9, 12, 21, 22, 42], natural language processing [2, 16], and speech [2, 3] has inspired a number of recent works proposing self-supervised learning frameworks for point cloud understanding tasks.",1,neutral
"Only recently, we have seen self-supervised methods being successfully applied to Transformer architectures for 2D vision [2, 9, 22] and 3D point clouds [32, 49, 51].",1,neutral
"With the emergence of MAE [16], some works [53, 12, 5] combine multi-modality learning with MAE-based pre-training paradigm and achieve great representation capabilities.",1,neutral
"Following the framework of [49], I2P-MAE [53] utilizes projected multi-view 2D depth maps to guide 3D point cloud pre-training.",2,positive
"For instance, if BERT4ETH follows the original masking ratio (15%) and uses 85% unmasked addresses to predict 15% masked addresses, the masked addresses have a high likelihood of being present in the unmasked addresses, leading to an overly easy prediction task [12].",1,neutral
", 2022f) for example, supports a video masked encoder for MAE (He et al., 2022) losses in addition to a module similar to ALBEF.",1,neutral
"InternVideo [52] for example, supports a video masked encoder for MAE style losses in addition to a module similar to ALBEF.",1,neutral
"We train the tokenizers of the columns C, the encoder, and the decoder by following the approach of the masked autoencoder (He et al., 2022); that is, we randomly select the columns with a masking ratio and replace the corresponding embeddings with a special embedding called a mask token.",2,positive
"While the original masked autoencoder in He et al. (2022) uses its decoder only in the pretext task and removes it in the downstream task, our model uses the decoder trained on the pretext task in the downstream tasks.",2,positive
"Furthermore, we note that our model does not use the [cls] token, unlike the approach by He et al. (2022).",2,positive
"First, TabRet is pre-trained based on the reconstruction loss with masking augmentation (Devlin et al., 2019; He et al., 2022).",2,positive
"In He et al. (2022), whose modality of interest is images, models have to solve qualitatively different problems as the pretext and downstream tasks – for example, image inpainting for the pretext task object classification for the downstream task.",1,neutral
"Recent works on generative modeling have also learned efficient representations for both global and dense prediction tasks like classification [28, 33, 13, 8, 19] and segmentation [46, 82, 10, 3, 9].",1,neutral
"Given the limited data scale, we leverage mask modeling [32] to make good use of the video data.",2,positive
"The progress in video understanding is currently driven by the Image Foundation Models (IFMs) [23, 32, 6, 62, 37], which are trained from massive datasets and adapted for different downstream tasks [18, 90, 99, 61].",2,positive
"BeiT [7] is the first to propose a BERT-like mask-then-predict framework to recover the discrete tokens [63], while MAE [32] designs masked autoencoders to reconstruct normalized pixel values, which reduces memory consumption by processing only unmasked tokens in the encoder.",1,neutral
This asymmetric encoder–decoder structure ensures the encoder learns rich semantic features and reduces the pretraining time significantly [32].,1,neutral
Masked Autoencoders (MAEs) [32] are self-supervised pretraining models based on an encoder–decoder structure that enable the encoder to learn visual representations by reconstructing the masked image.,1,neutral
"For plain architectures, we adopt the MAE [31] pre-training strategy to provide a good initialization for QFormerp, which has been proven effective in mitigating over-fitting in plain ViTs.",2,positive
"The official MAE pre-trained weights for the backbone are utilized, and the entire model is finetuned for 100 epochs on the MS COCO dataset.",0,negative
"For large models, we use plain ViT as the reference architecture and adopt the MAE pretrained [31] weights for initialization.",2,positive
"The hyperparameter for the proposed regularization term is set to 1, and all other parameters follow the same settings as in [31].",1,neutral
"The MAE pre-trained weights are used to initialize the backbone, and the whole model is fine-tuned for 160k iterations.",2,positive
"We use the official MAE pre-trained model to initialize the ViT-B backbone and the default training settings in MMPose, i.e., an input image size of 256×192 and a learning rate of 5e-4.",2,positive
"The UPerNet [38] is adopted as the segmentation head, following the common practice [11], [31], and the default training setting in MMSegmentation [39] is adopted.",1,neutral
"For the plain models, we adopt the fine-tuning practice proposed in [31], which involves pre-training the model on the ImageNet-1K dataset for 1,600 epochs using the ImageNet-1K training set, followed by supervised fine-tuning.",2,positive
Masked Autoencoders [19] are scalable self-supervised learners.,1,neutral
We overcome the above-mentioned issues by leveraging generic Masked AutoEncoder (MAE) [19].,2,positive
We use the publicly available Masked Autoencoders [19] pretrained on ImageNet to assist blind defense.,2,positive
Masked Autoencoders (MAE) [19] are scalable self-supervised learners based on Vision Transformer [12].,1,neutral
A novel framework of Blind Defense with Masked Autoencoders (BDMAE) is devised to detect possible triggers and restore images on the fly.,2,positive
"Masked Autoencoders (He et al., 2022) are scalable self-supervised learners.",1,neutral
"We use two pretrained Masked Autoencoders (He et al., 2022) that are available from their official repository.",2,positive
"In terms of supervision, in addition to the standard supervised pre-training on ImageNet-1k, we also consider self-supervised methods MoCo-V3 [17], SwAV [11], DINO [12], MAE [32] trained on ImageNet-1k, the weakly supervised SWAG [76] trained on 3.6B uncurated instagram images and CLIP [59] trained on 400M image-language pairs [71].",2,positive
"In terms of supervision, in addition to the standard supervised pre-training on ImageNet-1k, we also consider self-supervised methods MoCo-V3 [17], SwAV [11], DINO [12], MAE [32] trained on ImageNet-1k, the weakly supervised SWAG [76] trained on 3.",2,positive
"It is common to use large scale self-supervised [11, 12, 15, 17, 32, 33] and weakly-supervised [37, 53, 76] pre-trained models as starting points in various downstream applications.",1,neutral
"IN 1k MOCO-IN 1k
SWAV-IN 1k DINO-IN 1k MAE-IN 1k
SWAG-IG 3.7B CLIP-LAION 400M
Figure 7.",0,negative
6B (ViT-B-swag-3B) shows significant improvements over self-supervised training methods like MAE (ViT-B-mae-IN1k) and DINO (ViTB-dino-IN1k).,2,positive
"Another interesting direction is the observation of stepwise behavior in masked-image modeling frameworks, which currently constitute a large fraction of the SSL literature (Baevski et al., 2022; He et al., 2022; Assran et al., 2023).",1,neutral
"Masked auto-encoders assign mask tokens to an encoded latent space [16, 44], and adopt them to masked sequences at the decoder level.",1,neutral
"Previous approaches have employed transformer decoder-level mask tokens and linear interpolation (LERP)-based tokens have been explored to work around this issue [10, 16, 31].",1,neutral
"the performance of our architecture against the BERTbased motion in-painting transformer [10], the encoderdecoder-based ∆-interpolator [31], the RNN-based approach TGcomplete [15], and the masked auto-encoder (MAE) architecture [16].",2,positive
"Among them, TTT-MAE [399] is a recent extension of TTT that utilizes the transformer backbone and replaces the self-supervision with masked autoencoders [401].",2,positive
"Moreover, inspired by MIM [2, 10], we propose a simple yet effective strategy named Masked Pose and Shape Estimation, which helps the global transformer further mine the inter-frame correlation on human motion.",2,positive
", masked Language modeling (MLM) [6] in NLP, masked image modeling (MIM) [2, 10] in CV, enhance the representation of transformers.",1,neutral
"Except for the design of the structure, some strategies for pretraining transformers, e.g., masked Language modeling (MLM) [6] in NLP, masked image modeling (MIM) [2, 10] in CV, enhance the representation of transformers.",1,neutral
"During the global decoder phase, following [10], we pad the SMPL tokens onto the masked location and send the whole sequence into the global decoder to obtain a long-term representation.",2,positive
"the multiple discriminative features and spatial information [9, 21, 29, 34, 47, 48, 59], so it is more helpful to use SSL pre-trained encoder than label-supervised encoder for robustly performing on diverse downstream tasks.",1,neutral
"Discriminative SSL methods [11, 22, 27, 79] learn the embeddings by enforcing closeness and/or distantness on the pairwise distance structure among the augmented training samples.",1,neutral
"Beyond the label supervision [31, 66], selfsupervised learning (SSL) [12,25,29,30,88,92] approaches that do not rely on human-annotated labels hit the machine learning community.",1,neutral
"Beyond the label supervision [28, 55], selfsupervised learning (SSL) [11,22,26,27,75,79] approaches that do not rely on human-annotated labels hit the machine learning community.",1,neutral
We conjecture that an SSL pre-trained encoder is desirable to capture the demanding diverse semantics instead of a supervised one learned from pre-defined labels.,1,neutral
"Given a frozen prediction model Pθ(y|x), and perturbed image x̃ with prompt corresponding to the label y, the training objective is formulated as:
argmin ϕ
− logPθ;ϕ(y|x̃)
While VP and AR optimize the input space visual prompt directly, we reparameterize the visual prompt to the prompt generation network hϕ(·) parameterized by ϕ = {ϕd, ϕt} ∈ Rd. Specifically, we build a novel autoencoderstyle network named Coordinator composed of a frozen encoder f(·) which is pre-trained on ImageNet [14] by selfsupervised learning (SSL) objective and followed by an extremely light-weight learnable decoder gϕd(·).",2,positive
"Therefore, for Coordinator, BlackVIP adopts an SSL encoder (i.e., Masked Auto-Encoder [26]).",1,neutral
"Though the encoder can also be a supervised counterpart or lightweight learnable network, we adopt the SSL pre-trained encoder for the following three reasons: 1) It has been widely substantiated that self-supervised representation contains
the multiple discriminative features and spatial information [8, 19, 26, 31, 42, 43, 50], so it is more helpful to use SSL pre-trained encoder than label-supervised encoder for robustly performing on diverse downstream tasks.",1,neutral
SSL approaches can roughly be categorized into discriminative and generative approaches.,1,neutral
"Meanwhile, the recently emerging generative SSL methods [4, 29, 88] are based on masked image modeling, which supervises the model by encoding and reconstructing the partially masked individual images.",1,neutral
"Consequently, the image equipped with a prompt (prompted image) is constructed as follows:
x̃ = clip(x+ ϵhϕ(x)) hϕ(x) = gϕd(zx, ϕt)
where zx = f(x) is the feature vector of x from the frozen SSL encoder f(·), and ϵ ∈ [0, 1] is a hyperparameter that controls the intensity of visual prompt.",1,neutral
"SSL approaches are appealing not only due to their label-free training regime but also produce a more transferable representation [9, 29, 48] getting over the pre-defined label category.",0,negative
"SSL approaches are appealing not only due to their label-free training regime but also produce a more transferable representation [8, 26, 43] getting over the pre-defined label category.",0,negative
"Meanwhile, the recently emerging generative SSL methods [4, 26, 75] are based on masked image modeling, which supervises the model by encoding and reconstructing the partially masked individual images.",1,neutral
6 confirms that the SSL encoder outperforms the supervised pre-trained or randomly initialized encoder (scratch).,2,positive
We exploit an SSL pre-trained encoder while we plug the randomly initialized extremely lightweight decoder.,2,positive
"Concretely, the computation of (2) can directly benefit from just dropping all the masked patches before forwarding them, in a similar manner to masked autoencoder [12].",1,neutral
"MaskAHand can be viewed as an extension of the “masked image modeling” paradigm [20,54,56] to “masked hand grounding”.",1,neutral
"Recent works [27, 9, 6, 21, 8, 28] attempted to decode fMRI signals based on pre-trained generative models like Instance-Conditional GAN [3], diffusion models [17], masked autoencoders [14], CLIP [31], to name a few.",1,neutral
"(4) Inspired by the principles of self-supervised training [41, 15, 14], we leverage this methodology to train our encoder-decoder architecture.",2,positive
"Inspired by the advantage of long-range receptive fields from transformer layers, we follow MinD-Vis [6] to adopt the architecture of masked autoencoder [14] as the encoder-decoder model for fMRI signals.",2,positive
The only difference from MAE is that we finetune on iNaturalist21 rather than iNaturalist17.,2,positive
"As a base model, we consider the ViT-Base model using the Masked AutoEncoder (MAE) pretraining setup [16], which leads to state-of-the-art results for this general task.",2,positive
"For iNaturalist, we consider a Masked AutoEncoder (MAE) [16] pre-trained model and finetune it following the same procedure and hyperparameters as the original paper.",2,positive
"Mask modeling has been proven to be effective in both recognition learning [9, 14, 16] and generative modeling [7, 32].",1,neutral
"To meet the high-quality image generation requirements of the diffusion model, the sideinterpolater is placed in the middle of the network instead of the end of the network in recognition models [3, 16].",1,neutral
"In comparison, mask modeling for recognition models commonly calculates loss on masked tokens [3,16].",1,neutral
"In vision recognition, pretraining schemes that utilize mask modeling enable good representation quality [54], scalability [16] and faster convergence [14].",1,neutral
"Existing literature pays significant attention to both the unsupervised pretraining [7,13,14,18] and supervised finetuning [26].",0,negative
"Some recent research [2,14,19,45] explores generative methods that predict the missing content inside input samples, also achieving promising performance over vision transformers.",1,neutral
"In the most recent work, STEP [247] proposed a pretraining model combined with the Mask Auto-Encoder (MAE) [248] architecture to efficiently learn temporal patterns from very long-term history spatio-temporal graph data.",1,neutral
"In the predictive branch, the STG decoder directly outputs the prediction results and traditional data point errors, such as mean absolute error (MAE), can be used as the loss function.",1,neutral
"design a unified and efficient architecture to process crossmodal data since Transformer [36] has shown the flexibility and superiority in vision [37], [38], [24] and language [39], [40], [41] modeling.",1,neutral
work and has shown improvements in vision [24] and language processing [25].,1,neutral
"tasks [39], [40], [41] then entered vision [46], [37], [38], [24] and 3D field [47], [48], [49], [50], [42].",1,neutral
"Self-supervised learning aims to learn indicative feature representations from unlabeled data, which are then used to assist downstream supervised learning tasks [28, 29, 30].",1,neutral
"Though the scRNA-seq data is distinct from images, our results show that the performance gains of xTrimoGene are comparable to those of MAE, with more efficient training and better downstream task performance.",2,positive
"Unlike MAE, xTrimoGene utilizes the biased masking strategy to avoid the learning process being dominated by zero tokens.",2,positive
"Similarly, the principle of asymmetric encoder-decoder design has been proven powerful in masked autoencoders (MAE) (He et al., 2021), which is tailored for CV data pre-training.",1,neutral
"To further tackle the mode collapse challenge in unsupervised learning and scale up the optimization iterations, inspired by recent masked autoencoders [23, 57], we construct a masked point modeling paradigm where both point color reconstruction objective and surfel normal reconstruction objective are proposed to recover the masked color and geometric information of the point cloud respectively.",2,positive
"Based on a denoising autoencoder [48]-style architecture, the task is to reconstruct the RGB value [23,57], discrete token [3,60], or feature [50] of masked pixels.",2,positive
"This varies from the conclusion of MAE [23], in which a higher masking ratio of 75% achieves top performance.",0,negative
"In 2D unsupervised learning, there is also a recent trend of switching the pretext task from instance discrimination [4, 6, 7, 20, 24] to masked image modeling [3,23,50,57,60].",1,neutral
"Motivated by the success of masked image modeling [23, 57] in 2D representations, we propose masked point modeling, which can be naturally integrated into our contrastive learning framework.",2,positive
"To this end, we turn to use MAE [19], a SOTA image encoder pre-trained in an unsupervised manner, similar to MLM.",2,positive
"To this end, we turn to use MAE [18], a SOTA image encoder pre-trained in an unsupervised manner, similar to MLM.",2,positive
"We find MAEbase-MLM clearly improves the standard MAEbase on HM with the TS model, but obtains marginal gains with the E2E model.",2,positive
"We extract the ViT features from the same ViT architecture training in different ways: (b) supervised ViT [17], (c) self-supervised DINO [43], and (d) MAE [44].",2,positive
"Fortunately, the difference can be significantly revealed by the ViT features, especially the MAE [44] shown in Fig.",1,neutral
"The second and third types, called DINO [43] and MAE [44], respectively, are based on selfsupervised learning frameworks.",1,neutral
using transformer feature representations extracted from pretrained masked autoencoder (MAE) [44].,1,neutral
"Note that other popular image feature extractors [3, 9] can also be applied in our framework.",2,positive
"[26] pre-trained fMRI data using a method similar to MAE [27], and fine-tuned the LDM [28] using the extracted characterizations from the 2D fMRI structure to obtain reconstructed images.",1,neutral
"Chen et al. [26] pre-trained fMRI data using a method similar to MAE [27], and fine-tuned the LDM [28] using the extracted characterizations from the 2D fMRI structure to obtain reconstructed images.",1,neutral
"MAE was initially used in images [7], dividing a picture",1,neutral
"Given the simplicity and efficiency of Masked AutoEncoding (MAE) [33], we leverage it as the self-supervised prepretraining approach.",1,neutral
We follow [33] to train MAE models on IG-3B without using any labels.,0,negative
"In particular, we show that (i) MAE not only scales with model size as shown in [33], but also with the size of the training data (Figure 2).",1,neutral
"Of particular interest to us is MAE [33] for its state of the art performance on many transfer tasks [25, 28, 33, 46, 74] and its computational efficiency.",2,positive
"With the advent of Vision Transformers [23], approaches based on reconstructions such as [5, 33, 78] got renewed interest for their simplicity and state of the art performance.",1,neutral
We first begin with the Masked Autoencoder (MAE) [33] self-supervised learning technique to pre-pretrain vision models without using any labels.,2,positive
We compare the performance of MAE pretraining on the large scale IG-3B with the original MAE [33] models trained on IN1k for 1600 epochs.,2,positive
Pre-pretraining (MAE) [33] learns visual representations from image datasets without using any labels.,1,neutral
We follow the same hyperparameters used in [33] for pretraining on IN1k.,2,positive
"We follow SimpleClick [26] to build the interactive segmentation model, which consists of two patch embedding modules for image and click map respectively, a ViT [10] backbone initialized with MAE [16], a simple feature pyramid [21], and an MLP segmentation head.",2,positive
"As shown in Table 3, the model initialized with MIS pre-training the MAE [16] counterpart especially on the relatively hard dataset (i.e., SBD and DAVIS).",2,positive
"As shown in Table 3, the model initialized with MIS pre-training the MAE [16] counterpart especially on the relatively hard dataset (i.",2,positive
"Following the principle of MAE [27], we randomly mask out some tokens, and utilize the EAT encoder to extract high-level latent features.",2,positive
Masked auto-encoding [27] masks a portion of input data and adopts an auto-encoder to reconstruct explicit features (e.,1,neutral
"Inspired by the successful applications of transformers in NLP [16], [26] and image region [27], [28], a lot of 3D vision backbones have been proposed.",1,neutral
"Finally, MAE [18] relies on a masked autoencoder pipeline and a reconstruction objective to learn dense representations.",1,neutral
"As MAE does not rely on a cross-view consistency objective, this approach is well-suited for scenecentric datasets and of particular interest to us.",2,positive
"…contrastive learning (He et al., 2020; Chen et al., 2020c;a; Oord et al., 2018), and non-contrastive learning (Grill et al., 2020; Zbontar et al., 2021; Chen & He, 2021; Bardes et al., 2022; Wei et al., 2022; Gidaris et al., 2021; Asano et al., 2019; Caron et al., 2020; He et al., 2022).",2,positive
", 2018), and non-contrastive learning (Grill et al., 2020; Zbontar et al., 2021; Chen & He, 2021; Bardes et al., 2022; Wei et al., 2022; Gidaris et al., 2021; Asano et al., 2019; Caron et al., 2020; He et al., 2022).",2,positive
"Compared with previous MIM works [2, 22, 68], using the same 300 epochs of pre-training, our CIM can achieve better performances with both ViT-S/16 and ViT-B/16 models.",2,positive
"Compared with previous MIM works [2, 26, 79], using the same 300 epochs of pre-training, our CIM can achieve better performances with both ViT-S/16 and ViT-B/16 models.",2,positive
"In terms of how to mask, most MIM approaches, such as BEiT [2], MAE [26] and SimMIM [79], extend the mask-word recipe in MLM to randomly mask image patches in the spatial domain.",1,neutral
"Beyond augment-and-compare or mask-and-predict pretext tasks in MV-SSL and MIM, in this paper, we endeavor to investigate another simple yet effective paradigm for selfsupervised visual representation learning.",1,neutral
"Among existing pretraining paradigms, Multi-View Self-Supervised Learning (MV-SSL) [9, 11–14, 25, 27] and Masked Image Modeling (MIM) [2, 26, 65, 79] are two leading methods in the selfsupervised learning racetrack, thanks to their nontrivial and meaningful self-supervisory pretext tasks.",1,neutral
"Overall, our CIM is a simple yet effective approach that can perform on par or better than existing MV-SSL and MIM methods with both ViT and ResNet models.",2,positive
"As to what to predict, beyond default raw pixels [26, 79], several other reconstruction targets are proposed, e.",1,neutral
"Due to the architectural difference between ViT and CNN models, we observe performance degeneration of some MIM and MV-SSL pretraining methods, such as SimMIM [68], MoCo v2 [10], and SimSiam [11].",2,positive
"On the contrary, following the success of Masked Language Modeling (MLM) [16], MIM conducts a mask-and-predict pretext task within a single view (Figure 1(b)) – removing a proportion of random image patches and then learning to predict the missing information.",1,neutral
"Following previous works [2, 26], we fine-tune ViT-S/16 for 200 epochs, and ViT-B/16 for 100 epochs.",2,positive
"Unlike existing MV-SSL and MIM approaches, CIM considers correlation modeling in visual tracking as a useful pre-training paradigm.",1,neutral
"Among existing pretraining paradigms, Multi-View Self-Supervised Learning (MV-SSL) [8–12, 21, 23] and Masked Image Modeling (MIM) [2, 22, 54, 68] are two leading methods in the selfsupervised learning racetrack, thanks to their nontrivial and meaningful self-supervisory pretext tasks.",1,neutral
"Our CIM is a general framework that can learn meaningful representations for both ViT and CNN architectures, unlike state-of-the-art methods such as MAE [26].",2,positive
"Our CIM is a general framework that can learn meaningful representations for both ViT and CNN architectures, unlike state-of-the-art methods such as MAE [22].",2,positive
"Two key steps can be identified in a typical MIM pipeline: i) how to mask, ii) what to predict.",1,neutral
All these initiatives are proven less effective than the state-of-the-art MIM and MV-SSL approaches in large-scale visual pre-training.,2,positive
2) We demonstrate the advantages of our CIM in learning transferable representations for both ViT and ResNet models that can perform on par or better than the current state-of-the-art MIM and MV-SSL learners while improving model robustness and training efficiency.,2,positive
"In terms of how to mask, most MIM approaches, such as BEiT [2], MAE [22] and SimMIM [68], extend the mask-word recipe in MLM to randomly mask image patches in the spatial domain.",1,neutral
"For all ablation experiments, we choose ViT-Base (ViT-B/16) as the default backbone and follow a common setting used in existing works [2, 26]: 300-epoch self-supervised pre-training without labels and 100-epoch supervised end-to-end fine-tuning, to evaluate the quality of learned representations.",2,positive
"We omit the results in other metrics (NDCG, MAE MAPE) and on other data as their trends are similar.",2,positive
"In recent years, masked autoencoding has found versatile applications thanks to several groundbreaking practices, such as BERT [13] in natural language processing (NLP) and the very recent MAE [24] in computer vision (CV).",1,neutral
"Q2: for typical prediction tasks, how to design a principled family of loss functions and training scheme for dynamic graphs? In recent years, masked autoencoding has found versatile applications thanks to several groundbreaking practices, such as BERT [13] in natural language processing (NLP) and the very recent MAE [24] in computer vision (CV).",1,neutral
"We also adopt the mean absolute error (MAE), rooted mean squared error (RMSE) as well as mean absolute percentage error (MAPE) to evaluate the model accuracies.",2,positive
"The concatenation of unmasked patches’ embedding and masked tokens are processed by the decoder to predict the unmasked patches [17, 43].",1,neutral
Prior work [17] showed that MAE is both efficient at reducing redundancy in feature representations and capturing detailed information from holistic image statistics.,1,neutral
"This paper presents DRAM, a test-time defense using masked autoencoder (MAE) [17], one of the strongest SSL tasks, to detect and repair adversarial attacks, including unforeseen ones.",2,positive
Masked autoencoder (MAE) [141] (see Figure 8) simplifies it to an end-to-end denoising framework by predicting the masked patches from the unmasked patches.,1,neutral
The second class of AI techniques mainly consists of backbone architecture (like Transformer [443]) and self-supervised pretraining (like BERT [87] or MAE [141]).,1,neutral
MAE structure (figure obtained from [141]).,1,neutral
"Outperforming contrastive learning and negative-free joint-embedding methods, MAE has become a new variant of the visual SSL framework.",2,positive
Masked Autoencoders (MAE) [30].,1,neutral
"One of the key observations of the MAE work [30] is that the decoder does not need to be very good for the encoder to achieve good performance: by using only a small decoder, MAE successfully trains a ViT in an auto-encoder fashion.",2,positive
Masked Autoencoders (MAE) [30] perform a random masking of the input token and give the task to reconstruct the original image to a decoder.,1,neutral
"Then, rapidly, a wide variety of other tasks have been conquered by Transformer-based architectures, such as object detection [27], image segmentation [28], self-supervised learning [29, 30] and image generation [31, 32].",1,neutral
"Self-supervised visual representation learning has led to great success in image benchmarks [10, 28, 8, 27].",1,neutral
"ViC-MAE pre-training follows previously used configurations [27, 21].",0,negative
"Compared to a model that uses masked image modeling, the original MAE [27] and to the MaskFeat model [55], our model underperforms by 0.",2,positive
"More recently, self-supervised learning approaches based on masked auto encoders (MAE) [27] rely on masked image modeling adapted to video data to pre-train models.",1,neutral
"These methods work by randomly masking out parts of the input and forcing a model to predict the masked parts [3, 27, 21, 55].",1,neutral
"First, inspired by [28, 97], we randomly mask some joints and require the model to reconstruct them.",2,positive
"Generative modeling trains the model to reconstruct the entire original image, or some target regions within it [36], from its corrupted [37] or masked [38] variants.",1,neutral
"Inspired by the spirits of these works, this paper provides a thorough evaluation of MIM visual representation learning [5, 148, 133, 51] that significantly bridge the gap between large-scale visual representations that achieve stateof-the-art performance and models that are affordable and accessible for the wider research community.",2,positive
"Following the evaluation procedure in [51, 45], all these models are first fine-tuned on the original IN-1K training set, and then directly evaluated on different val sets using the same fine-tuned model without further hyperparameter selection and specialized fine-tuning.",0,negative
"weight initialization JAX random initialization [14, 51] MIM teacher EVA-CLIP vision encoder [45] image data source IN-21K / IN-21K / IN-21K / Merged-38M peak learning rate 3e-3 / 3e-3 / 1.",0,negative
"Starting with the baseline ViT configurations used in the original BEiT series pre-training [5, 92, 123] (∗ in Table 2), we progressively refine the model design and make the following observations: (i) The performance of SwiGLU FFN is mediocre with the random weight initialization method used in BEiT, but works quite well with JAX weight initialization [14, 51] (+1.",2,positive
") [121], as well as ObjectNet (ObjNet) [6], following the settings in [51, 45].",1,neutral
"Still, we note that this efficient decoder retains much of the modeling capacity of standard masked decoders [5, 8, 14] that employ full self-attention on context and mask {zL, zM}.",1,neutral
"Since the standard masked modeling [5, 8, 14] of applying a bidirectional transformer on mask and context {zM , zL} leads to a complexity of O((NM + NL)(2)), we opt into an efficient decoder that reduces the cost to linear to mask sizeO(NM ) while preserving as much modeling capacity as possible.",2,positive
"Various pretext tasks have been proposed to learn these representations, such as image inpainting [22], colorization [64], or prediction of the rotation [18] or position of patches [14, 4].",1,neutral
"The current state-of-the-art SSL methods integrate contrastive learning with Siamese networks to increase the similarity between two augmented versions of images [26, 2, 12, 10, 3], or use autoencoder to reconstruct the input images [8, 1, 11], while all these solutions assume training images are centrally available in cloud servers.",2,positive
"Recent advancement of Transformers has great achievements in computer vision [8, 11].",1,neutral
"The current most popular way of pretraining Transformer is MAE [11] and MaskFeat [25], which utilize the idea of self-supervised learning to predict features of the masked area.",1,neutral
"First of all, we pre-train a Masked Autoencoder (MAE) [13, 36] on a large-scale facial dataset in a selfsupervised manner.",2,positive
Our framework starts with pre-training a masked autoencoder (MAE) [9] from a set of face databases.,2,positive
Masked autoencoder (MAE) [9] is a self-supervised learned model which reconstructs original images from a set of masked images.,1,neutral
"The concise of both network and training scheme support its generalized representation of the real dataset [45,15].",1,neutral
Li et al. [23] also use MAE pre-trained weights combined with AffectNet supervised pre-trained weights and ranked 2nd in ABAW4.,0,negative
It also used MAE pre-trained weights to enhance its performance.,0,negative
"Differently, MAE [7] try to reconstruct a masked image to learn semantic features.",1,neutral
"To address this bottleneck, graph neural network (GNN) architectures [82], [114] were proposed and applied with success to model liquids and granular materials.",1,neutral
"Masked autoencoders [23, 3, 62] introduce BERT-like masked image modeling (MIM) pre-training for Vision Transformers [17], but they seem not natural and practical for convolutional networks.",1,neutral
"More recently, masked autoencoders (MAE) [23] have further highlighted the effectiveness of denoising pre-training, which can also be inherited by networks in diffusion models — resembling MAE’s de-masking, recovering images with large and multi-scale noise is a nontrivial task and may also require a high-level understanding of visual concepts.",1,neutral
"To verify transfer learning ability on scalable Vision Transformers beyond UNets, we compare ImageNet 2562 pre-trained DiT [43] to MAE pre-trained vanilla ViTs [17] on CIFAR-10
and Tiny-ImageNet.",2,positive
"Drawing from the connections between diffusion networks and denoising autoencoders (Section 3.1), we infer that DDAEs can produce linear-separable representations at some implicit encoderdecoder interfaces, resembling DAEs and MAEs.",1,neutral
"However, diffusion pre-trained DiTs may not be as efficient as MAE pre-trained ViTs on recognition tasks, since the former is specifically designed for advanced image generation without optimizing its representation learning ability.",1,neutral
"Evaluations on CIFAR-10 [34] and Tiny-ImageNet [35] show that our diffusion-based approach is comparable to supervised Wide ResNet [65], contrastive SimCLRs [12, 13] and MAE [23] for the first time.",2,positive
"The comparison between DDAE and MAE on transfer learning further suggests that de-masking may not be a necessary, essential, and optimal choice for vision.",1,neutral
"Computer Vision MAE [23] Encoder-only, MIM SemiMasked iGPT [11] Decoder-only, AR Full –",2,positive
Table 3 and Table 4 show that the scaled DiT-XL/2 outperforms the smaller MAE ViT-B/16 under all settings by large margins except for linear probing on CIFAR-10.,2,positive
"Following [10, 26], we randomly mask the features of joints after the embedding layer.",1,neutral
"Different from MAE [10] and some existing graph data augmentation methods [12, 23], we feed the topology of the original joints into the encoder.",2,positive
"However, MAE [10] uses the location information of the image patches for the decoder to assist in reconstructing.",2,positive
The great success of MAE [10] makes us rethink data augmentation.,2,positive
"Surprisingly, when we directly reconstruct masked joints using a method similar to that in MAE [10], the performance degrades instead.",1,neutral
We will discuss the relationship between MAE [10] and graph data augmentation in detail in Sec.,2,positive
"Inspired by MAE [10], we propose an augmentation framework named MGPose.",2,positive
MAE [10] uses an autoencoder architecture.,2,positive
"MAE [10] uses the encoder to extract features of the masked image patches, and then reconstructs the original image patches by the decoder.",2,positive
"Pre-training on self-supervised tasks such as masked image modeling and autoregressive text generation is effective for large language and vision models (Brown et al., 2020; He et al., 2022).",1,neutral
"Despite the empirical success in language and vision (Brown et al., 2020; He et al., 2022), their performance on graph data applications remains unsatisfactory because of the significant gap between the graph self-supervised task and the graph label prediction task.",2,positive
"Early work in fine-tuning focused on adapting pre-trained models through layer-wise fine-tuning [49, 48, 12], where only a subset of layers in the network were fine-tuned, while the rest of the network remained frozen.",1,neutral
"Basically, these techniques [3, 33, 11, 10, 44] pre-train a deep model on large-scale data and then adapt the pretrained model to novel tasks.",1,neutral
The second type of visual feature is MAE-based model [23].,1,neutral
"To address the burdens of collecting large-scale labeled datasets for supervised learning [16, 68, 84], self-supervised learning methods [12, 13, 25,28,29,79] have been introduced to learn general-purpose visual representations from unlabeled data.",1,neutral
"To overcome the data scarcity problem, SelfSupervised Learning (SSL) techniques are nowadays being widely used for computer vision tasks [9, 10, 16, 21, 23, 24]), and more precisely for text recognition [2, 52].",1,neutral
"However, the MAE original design does not take continual learning into consideration and thus can not generalize well both in the previous and current tasks.",2,positive
"As in MAE [23], the encoded tokens zT (Eqn.",1,neutral
"Thus, this work is based on Masked AutoEncoders (MAE) [23] for pre-training.",2,positive
"Specifically, MAE discards low-level information by masking a large portion of the image patches and enables the encoder to extract semantic information by reconstructing the pixels from a very small number of neighboring patches [6] with a lightweight decoder.",2,positive
"We implemented the 3D MAE using 3D-ResNetBlocks instead of Vision Transformer, different from the previous study [13], due to the constraint of GPU memory.",2,positive
"Following the previous study [13], we implement an asymmetric design with a lightweight decoder compared to the encoder.",2,positive
"Following the previous study [13], MSE calculated on masked regions is used as loss function.",1,neutral
", w/o MAE pretraining) in downstream tasks [13].",1,neutral
"To avoid information leakage, we use self-supervised MAE for model pre-training on split ImageNet-subset.",2,positive
One exception is fine-tuning MAE on ImageNet-subset where we found the transformations used for CPP generate inferior results and thus we follow the data transformations used in the original paper [20].,1,neutral
"DualPrompt [59] is originally designed using Transformer structures, so we only change the pre-trained weights to MAE to avoid information leakage.",2,positive
"To this end, we implement CPP with four up-to-date pre-training methods including ViT [14], Deit [53], Dino [5], and MAE [20] that sweep supervised and self/un-supervised learning as well as discriminative and generative models.",2,positive
"However, these reconstruction objectives used in works such as iBOT [40], BEiT [3] and MAE [17] are computationally expensive and rely on vision transformers exclusively.",1,neutral
"works such as iBOT [40], BEiT [3] and MAE [17] are computationally expensive and rely on vision transformers exclusively.",1,neutral
"In SSL, Self-attention has been widely used on generative frameworks [17, 3, 40], where they train the transformer backbone to reconstruct the given masked image.",1,neutral
"2) MIM: To handle unpaired images, we leverage the Masked Image Modeling (MIM) paradigm [59].",1,neutral
"Therefore, we use a multiway transformer to extract multi-modal features and two linear layers to solve PLM and MIM tasks, respectively [38], [59].",1,neutral
"Our VLP model achieved the best performance with a higher masking ratio of 85%, which is in contrast to the optimal masking ratio of 75% reported by MAE [59].",2,positive
"PosterV2-Vit feature: To acquire advanced visual features, we trained a Vision Transformer (ViT) [4] model on the AffectNet7 dataset [21] using unsupervised learning techniques [5].",2,positive
"Self-supervised Vision Transformers (ViT) [13], such as DINO [6], MAE [21], and BEiT [4], have demonstrated immense potential in unsupervised dense prediction tasks.",1,neutral
"Differently from reconstruction targets in natural language processing with rich semantics, reconstruction targets in computer vision are low-level pixels [15, 54].",1,neutral
"In this paper, we revisit deep supervision for masked image modeling (MIM) [15, 11, 48, 2], a self-supervised pretraining strategy for Vision Transformer [12] (ViT).",2,positive
"Recent MIM works have studied the question: what are appropriate reconstruction targets? Proposals have included discrete tokens [2], RGB pixels [15, 54], histograms of oriented gradients [48] and CLIP features [35, 23].",1,neutral
"In the same spirit as masked language modeling, MAE [15] and SimMIM [54] employ raw pixels as the targets for reconstruction.",1,neutral
"Then, similar to MAE [15], we randomly mask a high proportion of patches, yielding a masked image x̃.",1,neutral
"For concreteness, we use MAE [15] to illustrate our underlying approach.",2,positive
", RGB pixels [15, 54], discrete tokens [2], histograms of oriented gradients [48], CLIP features [23] and DINO features [4].",1,neutral
"Then ViT-B is finetuned on ImageNet-1K [9], following common practice [15].",1,neutral
"We follow the same training recipe of MAE [15], more details can be found in supplementary materials.",0,negative
"works [39, 15, 11] have explored more semantic reconstruction targets, e.",1,neutral
"In particular, the variance of ViT-B/16 pre-trained with MAE [20] is twice as large as that of the supervised pre-trained ViT-B/16.",1,neutral
"FULL 100% 79.1 86.2 59.7 75.0
Addition-based methods
MLP-3 1.60% 73.6 75.2 35.7 61.5 PROMPT-SHALLOW 0.04% 79.9 82.5 37.8 66.7 PROMPT-DEEP 0.23% 76.8 84.5 53.4 71.6 ADAPTER-8 1.18% 81.7 87.3 61.2 76.7 SPT-ADAPTER (ours) 0.33% 83.0 87.3 62.1 77.5
Reparameterization-based methods
baseline method on VTAB-1k benchmark with only 0.26% and 0.08% trainable parameters for MAE and MoCo v3 pretrained backbones, respectively.",0,negative
"We show more parameter sensitivity patterns for ViT-B/16 with various pre-training strategies (i.e., MAE [20] and MoCo V3 [11]) and datasets sampled from FGVC benchmark [24].",2,positive
"We conduct experiments on the plain vision Transformer backbone ViT-B/16 [13] that is pre-trained on ImageNet [27] with different pre-training strategies following [24], including supervised pre-training and self-supervised pre-training with MAE [20] and MoCo v3 [11] following [24].",2,positive
"Visualizations of sampled VTAB-1k datasets with MAE and MoCo V3 pre-trained ViTB/16 are shown in Figures 5, 6, 7.",1,neutral
Table 2: Comparisons on VTAB-1k [62] benchmark using self-supervised ViT-B/16 backbone pre-trained by MAE [20] and MoCo v3 [11].,2,positive
"As shown in Table 2, existing PEFT approaches exhibit inferior results than full fine-tuning with the self-supervised pre-trained
backbones MAE and MoCo v3.",2,positive
"Nearly all SSL methods [8, 11, 16, 17] proposed on natural image recognition utilize a small portion of annotations to get promising fine-tuning accuracy compared to full supervision, which is higher than Linear Probing [17] by a large margin.",1,neutral
"in [16,17], the vanilla FT method can improve about 15 percent of accuracy in IN-1K compared to vanilla Linear Probing [17].",1,neutral
"Self-supervised Learning (SSL) has shown to be a promising paradigm both in computer vision [4,8,11,16,17] and natural language processing [12,33,44].",1,neutral
"Similar to some visual attribution methods like CAM [37, 49], LRP [3] and patch masking [4, 16, 28] in ViT, an IB-based attribution method is proposed in [36] by adding noise to intermediate feature maps, restricting the flow of information, then how much information image regions provide can be quantified.",1,neutral
"The proposed framework only relies on annotations at WSI levels, which is similar to recent SSL approaches [8,11,16,17].",2,positive
"Recently, mask-based image augmentation has been proved an efficient way to extract global context information, especially combined with transformers [14, 17, 46].",1,neutral
"Masked image modeling (MIM) [2, 19, 42] trains models to predict masked regions of input images, a training mechanism we adopt for RFFR.",1,neutral
"For our RFFR model, we adopt a base version of Masked Autoencoder (MAE) [19] and train it on real faces with a batch size of 128.",2,positive
"Prediction can happen in the input space by, for instance, reconstructing one part of an image from another, as for autoencoders [19], or by predicting the next word in a sentence, as done in language models.",1,neutral
ing (MAE) [18] – an efficient self-supervised visual representation learning algorithm designed for pretraining vision transformers [13] (ViTs) – to improve the performance of our ViT-based agent.,2,positive
"A flurry of recent work on image and video understanding has found that visual transformers [13] (ViTs) powered by self-supervised representation learning can provide general-purpose visual representations for recognition [3, 11, 18] and generation [4, 6] tasks.",1,neutral
We find that Data2Vec [3] (row 3) attains similar performance to the MAE [18] (row 4) initialization we use in Sec.,2,positive
"Specifically, we find that visual representation learning (using masked autoencoding (MAE) [18]) not only improves performance, but also enables model scaling with ViTs.",1,neutral
Table 3: Visual pretraining using MAE [18] enables positive scaling of the ViT-BASE architectures on IMAGENAV.,2,positive
"Specifically, we propose novel FaceMAE module, a masked autoencoder [25] specialized for FER-W, to achieve the goal of FFL.",2,positive
"However, we designed FaceMAE, a masked autoencoder [25] specialized for FER-W, by making two major modifications to the original masked autoencoding scheme.",2,positive
"Masked autoencoding has strengths in context learning because a defined autoencoder infers the entire image with only limited information [50, 25].",1,neutral
"In the case of random masking, a certain level of context can be considered because the masked facial image must be reconstructed with extremely limited information due to the high masking ratio [50, 25].",1,neutral
The previously proposed MAE [25] with ViT-base [15] was used for the autoencoder architecture.,2,positive
"However, we notice that since MAE’s mask tokens only pass through the decoder [20], a shareddecoder design is critical in our scheme for mask tokens to learn mutual information before performing reconstructions in separate modal-specific decoders.",2,positive
"[20], which demonstrate a straightforward yet powerful pre-training framework for Vision Transformers [10] (ViTs) and show promising results for independent modalities of both 2D and 3D vision [2, 14, 17, 63, 64].",2,positive
We follow MAE [20] and Point-M2AE [64] to generate input tokens from images and point clouds.,2,positive
"Recently, inspired by advances in masked language modeling, masked image modeling (MIM) approaches [3,20,59] have shown superior performance, proposing a self-supervised training method based on masked image prediction.",1,neutral
"Consequently, we design a MAE [20]-structured multi-modal learning framework that incorporates projection alignment for more interactive multi-modal learning.",2,positive
"MAE [20], in particular, predicts pixels from highly masked images using a ViT decoder.",1,neutral
"For the image branch, we follow [20] to divide images into regular patches with a size of 16 × 16, before the ViT backbone.",1,neutral
"Towards this end, an interesting question may be raised: is there a principled way to automatically distill the important self-supervision signals for adaptive augmentation? Inspired by the emerging success of generative self-supervised learning in vision learner [12] with the reconstruction objectives, we propose an automated framework for self-supervised augmentation in graph-based CF paradigm via a masked graph auto-encoder architecture, to explore the following questions for model design.",2,positive
"This result is highly competitive and surpasses all other existing methods, even when compared with counterparts with vision transformer backbone [95, 67] or with advanced model pretraining mechanism [31, 67].",2,positive
"Due to the gap of information density [19] between languages and images, prompting for vision models is more challenging and complex.",1,neutral
"With the increasing scale of training data and model size, the pretraining-finetuning paradigm has shown remarkable achievement in many areas, including natural language processing (NLP) [4,13] and computer vision (CV) [2,7,8,19].",1,neutral
"Recently, equipped with a more aggressive masking strategy, SimMIM [62] and MAE [26] further demonstrate that simple pixel reconstruction can achieve competitive results from previous pre-training methods.",2,positive
"To examine the effectiveness of our method, we perform DPPMask on two representative MIM methods: MAE [26], iBOT [67], which represent two different MIM frameworks: pixel reconstruction and feature contrast.",2,positive
"Another key factor of successfully applying MIM is the masking ratio of input images [26, 62].",1,neutral
"Benefiting from the new network architectures like ViT [17], Masked Image Modeling (MIM) has become highly popular, and there is a series of more aggressive masking strategies like MAE [26], simMIM [62].",2,positive
"For the self-supervised learning models, DINO [5] and MAE [16], we additionally measure the endto-end performance of benchmark models pre-trained on our UnlabelledNAIP.",2,positive
"DINO [5] based on knowledge distillation and the generative model MAE [16] based on autoencoder, on our FireRisk.",2,positive
"For the self-supervised architectures, MAE [16] and DINO [5], we use ViT-B/16 [10] as the backbone and fine-tune on FireRisk using latent representa-",2,positive
"Using transfer learning, we fine-tune ResNet-50 [17], ViT-B/16 [10], as well as DINO [5] and MAE [16] with ViT-B/16 as the backbone, all of which were pre-trained on ImageNet [8], using our",2,positive
"• To investigate the performance of supervised and self-supervised learning on our FireRisk, we employ ResNet [17], ViT [10], DINO [5], and MAE [16] as benchmark models.",2,positive
"Inspired by the outstanding performance of ViT [10] for feature extraction, MAE [16] reconstructed randomly masked patches using the",2,positive
"On FireRisk, we present benchmark performance for supervised and self-supervised representations, with Masked Autoencoders (MAE) [16] pre-trained on ImageNet1k [8] achieving the highest classification accuracy, 65.",2,positive
"learning, we select two representative self-supervised models for their performance, namely DINO [5] and MAE [16].",2,positive
"In generating the latent representations, we use ViT-B/16 as the backbone architecture for MAE [16], pre-",1,neutral
"• We gather an unlabelled dataset, UnlabelledNAIP, from the NAIP remote sensing project and utilize it to pre-train novel latent representations of DINO [5] and MAE [16].",2,positive
"Among them, ViT-B/16-DINO and ViT-B/16-MAE are trained with self-supervised loss, and ViT-B/16-CLIP is trained on 400 million image-text pairs with contrastive loss.",1,neutral
"2 [23] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
", ResNet18/50/152 [24], ViT-B/16-IN1K/21K, ViT-L/16-IN1K, ViT-B/16-DINO [7], ViT-B/16-SAM [11], ViT-B/16-MAE [23], ViT-B/16CLIP [50] (image encoder) for a holistic evaluation, and report the results in Figure 6.",0,negative
"We choose publicly available PTMs, i.e., ResNet18/50/152 [24], ViT-B/16-IN1K/21K, ViT-L/16-IN1K, ViT-B/16-DINO [7], ViT-B/16-SAM [11], ViT-B/16-MAE [23], ViT-B/16CLIP [50] (image encoder) for a holistic evaluation, and report the results in Figure 6.",0,negative
"Additionally, we report the backbones adopted in the ablation study, including ViT-L/16-IN1K, ViTB/16-DINO [7], ViT-B/16-SAM [11], ViT-B/16-MAE [23], ViT-B/16-CLIP [50] (image encoder), in the table.",2,positive
"3M MoCo-v2 [7], MAE [24] TOTAL (47) 32M",0,negative
"(3) RefCOCO, RefCOCOg, RefCOCO+ [81] 60K MLM with PEVL text encoder [78] Phrase Grounding (1) Flickr30K [79] 32K MLM with PEVL text encoder [78]
Visual Relationship Detection (1) Visual Genome [41] 101K MLM with PEVL text encoder [78] Visual Commonsense Reasoning (1) VCR [84] 100K MLM with PEVL text encoder [78]
Self-Supervised Learning (2) ImageNet-1K [10] 1.3M MoCo-v2 [7], MAE [24] TOTAL (47) 32M
them for specific downstream task.",2,positive
", classification [26], contrastive learning [6, 25, 54] and masked image modeling [1, 24, 76].",1,neutral
An example of a pretext task is to retain part of the input data to be predicted by a model that is trained on the other part of the data [85].,1,neutral
"Similar to MAE [22] and PointMAE [37], we compute the loss only on masked parts.",1,neutral
"Following MAE [22] and VideoMAE [51], we adopt the asymmetric encoder-decoder design to reduce computation.",2,positive
"One of the most popular methods is MAE [22], which randomly masks input patches and trains the model to recover masked patches in pixel space.",1,neutral
"One of the most promising self-supervised methods is the masked autoencoders (MAE) [22] which achieve success in various tasks [37, 51].",1,neutral
The random masking approach in the original MAE [16] cannot adaptively perceive the variation of information density and applies a unified masking probability over the entire image.,1,neutral
MAE [16] and SimMIM [42] show that masking a high ratio of patches and directly predicting RGB values can achieve BEiT-level performance.,2,positive
"Inspired by the masked language modeling framework, masked autoencoder (MAE) [16, 42, 23], also known as masked image modeling (MIM), has been introduced to computer vision and become a promising self-supervised learning method based on Vision Transformer (ViT) [12].",1,neutral
(a) The original MAE [16] randomly masks 70% image patches with a uniform probability.,1,neutral
"We use 3 alternative models to initialize the feature extractor in the mask generator: (1) the ViT-B network pretrained with MAE [16] (termed as MAE-800), (2) the ViT-B pretrained with iBOT [48] (termed as iBOTB), and (3) the ViT-S pretrained with DINO [3] (termed as DINO-S).",2,positive
"However, based on some recent research, the MAE method may not have strong domain generalization capability compared to the second approach.",2,positive
"The first approach is to reconstruct the input image using masked auto-encoders (MAE) [11] directly, and the second is to introduce the pairing text descriptions of images as weak supervising labels.",1,neutral
"Also, since the methods
in this section refer to MAE, a comparison test is done between the methods in this section and MAE using the same training method.",1,neutral
"The self-supervised pretraining method in this paper takes reference from MAE, but differs from it in that MAE uses two identical structures of ViT as encoder and decoder, while our method uses a symmetric convolution-deconvolution structure for the autoencoder.",2,positive
"The resulting data are shown in Table III, from which it can be seen that in each of the four datasets, our method is higher than MAE by more than 3 points in each metric.",0,negative
"Meanwhile, to better prove the reliability of the method proposed in this chapter, we conducted peer-to-peer experiments using MAE, i.e., we first performed masked self-supervised
learning pre-training, and then selected the one with the lowest training loss model for the pedestrian re-identification task.",2,positive
"In 2022, Kaiming He proposed MAE [31], which enables the network to easily cope with various computer vision downstream tasks by reconstructing important regions in images for pre-training.",2,positive
"A straightforward solution can be obtained from Masked Image Modeling (MIM) [2,9].",1,neutral
"Considering ViT’s flexibility and great potential in masked image modeling [9,14], we explore acceleration algorithms based on the standard ViT.",2,positive
"We implement a baseline inspired by MIM [2,9].",2,positive
", EViT [15] and DynamicViT [21], to sparse encoding, and masked image modeling (MIM) [9,2] to token completion.",1,neutral
"A-TA requires model-specific adapters to adapt different pre-trained models, e.g., the residual adapter TSA [25] for ResNets [15], the self-attention adapter eTT [55] for ViTs [9].",1,neutral
"Over the past years, a community-wide enthusiasm has been ignited to narrow this gap, especially in fields such as computer vision [15,26,47], machine translation [5, 31, 54] and reinforcement learning [11, 17, 39].",1,neutral
"Various attempts have been made to adapt the pre-trained models to few-shot tasks by devising model-specific adapters, e.g., the residual adapter TSA [25] for ResNets [15], the self-attention adapter eTT [55] for ViTs [9].",1,neutral
", the residual adapter TSA [25] for ResNets [15], the self-attention adapter eTT [55] for ViTs [9].",1,neutral
"Additionally, traditional image augmentation methods often make limited adjustments at the feature level due to the vast amount of redundant and irrelevant information in digital images [28, 14].",1,neutral
"Instead of a random formulation [9, 30], we sample a fixed ratio γ of the tokens [M ] (X) to be masked out according to a multinomial distribution related to the patch hand saliency m(X):",1,neutral
"Then this prior is encoded as the token form [9], and a ViT [15] sketcher is trained to disentangle the corresponding structure tokens from partial image patches [30].",1,neutral
"Inspired by this, we implement an MAE by masking the outputs of our encoder, Fo, and then passing the masked encoded features along with the masked tokens to our decoder.",2,positive
"As illustrated in Figure 1 (b), we adopt a simple Regression head composed of two deconvolution layers and one 1×1 convolution layer on the reshaped Fo following the common setting of the previous works [23].",1,neutral
"Masked Auto-Encoders (MAE) for pre-training transformer networks has shown strong results on a variety of applications such as NLP [24], pose estimation [11], and image classification [23].",1,neutral
"We initialize the Temporal ViTPose weights from the pre-trained model of MAE [23], and perform masked region reconstruction with a masking rate of 75%.",2,positive
This 3D-ViT was then embedded in the MAE approach of [13].,1,neutral
"One leading self-supervised approach is the masked autoencoder (MAE) [13], which was developed on natural imaging data.",1,neutral
"Weights of the trained model of [13], developed on ImageNet, were used to initialize the transformer layers in the encoder, while weights of the encoding layer and the decoder were randomly initialized.",1,neutral
"Regarding selfsupervised models, the masked autoencoder (MAE [30]), DINO [7], MoCov3 [10], MSN [2] were selected, because they all include the base ViT (ViT-B/16) for comparison between pretraining schemes (Fig.",0,negative
"There has been a lot of recent work in self-supervised learning where the input is masked and the model is tasked at reconstructing the missing pixels (He et al., 2022; Vincent et al., 2010; Assran et al., 2022).",1,neutral
"The video features contain much redundant information, while the text features are more semantic and have higher information density [9].",0,negative
"Partial Fintuning is a setting between head finetuning and full finetuning [23], which finetunes the last several layers while freezing the others.",1,neutral
Side length of the random 3D patches is set to 16 voxels following He et al. (2022). xsub is initialized to Gaussian noise.,2,positive
"Self-Supervised Multimodal Representation Learning via M3AE: Masked autoencoders (MAEs) have been proven successful as scalable self-supervised vision learn-
ers (He et al. 2022), where the pretext task is to reconstruct the original signal given its partial observation.",1,neutral
"Here, we make a modification to the original MAE (He et al. 2022) by replacing the masked contents with x and processing them in the same way as non-masked ones, instead of discarding them.",2,positive
"Therefore, we empirically set an even higher combined masking rate of 87.5% (compared to 75% used by He et al. (2022)) in our M3AE to make the self-supervising task nontrivial.",2,positive
"ers (He et al. 2022), where the pretext task is to reconstruct the original signal given its partial observation.",1,neutral
"A notable difference between the original MAE for natural images and our M3AE is that, masked patches of the former can only be inferred from surrounding context, whereas those of the latter can be additionally inferred from other modalities and thus expected to be easier.",2,positive
"Here, we make a modification to the original MAE (He et al. 2022) by replacing the masked contents with xsub and processing them in the same way as non-masked ones, instead of discarding them.",2,positive
"Above all, witnessing the recent success of masked autoencoders in learning rich visual representations (He et al. 2022), we propose multimodal masked autoencoder (M3AE), where a random subset of the modalities and random patches of the remaining ones are masked simultaneously.",1,neutral
"Therefore, we sample a random subset of the modalities for masking to mimic the real situation, in addition to randomly masking 3D patches of the remaining modalities as in the original MAE for natural images.",1,neutral
"Evaluation of Representation: So, why is the Slow Learner such effective, and what accounts for the remaining performance gap? We perform a linear probing experiment [12] to evaluate the performance of the representation layer.",1,neutral
"In addition to supervised pre-training, we consider representative self-supervised paradigms that provide pre-trained checkpoints on ViT-B/16, i.e., MoCo v3 [4], BEiT [2] and
MAE [12].",2,positive
"exceeds that of the more recent MAE [12], although their joint training performance is comparable.",0,negative
"Considering architectural consistency with previous works of CLPM [43, 42], we select representative self-supervised methods (i.e., MoCo v3 [4], MAE [12] and BEiT [2]) that release checkpoints on ViT-B/16 in our comparisons.",2,positive
", MoCo v3 [4], MAE [12] and BEiT [2]) that release checkpoints on ViT-B/16 in our comparisons.",2,positive
"Interestingly, the performance of Seq FT w/ SL for MoCo v3 [4] far
exceeds that of the more recent MAE [12], although their joint training performance is comparable.",2,positive
"DeiT [33] is a strong supervised method for (pre-)training vision transformer, while MoCo v3 [4], MAE [12] and BEiT [2] are representative self-supervised methods.",1,neutral
We utilize the mask ratio 25% and 8 decoder blocks following the practices in MAE [4].,2,positive
"To address the above issues, we propose to Mimic before Reconstruct for Masked Autoencoders, termed as MR-MAE, which is a simple and effective strategy to enhance MAE [4] by regularizing the intermediate representations with pre-trained off-the-shelf feature encoders.",2,positive
Compared with the original MAE base model [4] (83.,0,negative
"BeiT [18], MAE [4], CAE [36] have validated Masked Image Modeling (MIM) paradigm to be effective approaches for pre-training vision transformers.",1,neutral
"Although DINO and CLIP exhibits strong objectness cues and open-world recognition ability, the fine-tuning performance on downstream tasks are inferior to representations learned through MAE [4] manner.",1,neutral
"Secondly, MAE [4] employs an asymmetric architecture with a heavy encoder and a light decoder, where the encoder is preserved after pre-training for downstream transfer learning.",2,positive
Masked Autoencoders (MAE) [4] employ an asymmetric encoder-decoder design for computationally efficient masked image modeling.,1,neutral
"Different from the high-level supervisions in language modeling, the low-level RGB signals of MAE [4] is too primitive and redundant, which fail to unleash the full understanding capacity of masked autoencoding on downstream vision tasks.",1,neutral
"Motivated by this, Masked Autoencoders (MAE) [4] explore how to adopt MLM paradigm into vision representation learning with a vision transformer [5] of asymmetric encoder-decoder architectures.",1,neutral
"Motivated by MLM, masked image modeling (MIM) was proposed to boost the visual pretrained models [20, 63, 7].",1,neutral
"In comparison, the pre-training tasks in vision area are various, e.g., supervised pre-training [15], masked image modeling (MIM) [20, 7], and masked visual token modeling (MVTM) [2, 44].",1,neutral
"The supervised pre-trained models are not equipped with generative task, and MIM pre-training task recovers each patch in pixel space, which lacks semanticrich representations.",1,neutral
", supervised pre-training [15], masked image modeling (MIM) [20, 7], and masked visual token modeling (MVTM) [2, 44].",1,neutral
"…learning successes across many domains follow a common recipe: rst pre-training large and expressive models on general-purpose, Internetscale data, followed by ne-tuning the pre-trained initialization on a limited amount of data for the task of interest (He et al., 2022; Devlin et al., 2018).",2,positive
"Modern machine learning successes across many domains follow a common recipe: rst pre-training large and expressive models on general-purpose, Internetscale data, followed by ne-tuning the pre-trained initialization on a limited amount of data for the task of interest (He et al., 2022; Devlin et al., 2018).",1,neutral
"High-quality representation learning has been a core topic in deep learning research, which is challenging for computer vision due to the low information density [19, 18].",1,neutral
"Inspired by the popular masked image modeling [12, 13], we build a universal masked autoencoder architecture for VAD by embedding random mask tokens to simulate anomalies, which is a simple yet efficient synthetic method that avoids extra data processing in the original normal data.",2,positive
"As shown in Figure 1, fine-tuning [10, 24] and linear probing [15, 30] are two commonly used methods for this adaptation.",1,neutral
"SimpleClick [15] greatly improved performance by adopting the Plain Vision Transformer (Plain ViT), which was pretrained with MAE [9], as the backbone of the RITM approach.",2,positive
"The image encoder is initialized with the first 12 layers of MAE-base (He et al. 2022) weight, which is pre-trained on the ImageNet-1k without any labels.",2,positive
"We also provide a new pipeline achieving data augmentation efficiently for imbalanced image datasets, using cGAN or diffusion models and ResNet or Masked Autoencoder (MAE) classifiers.",2,positive
Table 6 provides goodness-of-fit (R2) and Mean Absolute Error (MAE) measurements for the function f .,1,neutral
"To justify our proposed metric and pipeline work regardless of the classifier selection, we also provide the results with Masked Autoencoder (MAE) ViT-H128 [14] which shows state-of-the-art results in dataset such as [12].",2,positive
a ResNet [15] or Masked Autoencoder (MAE) [14] classifier also trained on the original set with sub-class labels are applied on the synthesized images to select qualified ones.,1,neutral
5% 1 State-of-the-art (SOTA) classification validation accuracy with Masked Autoencoder ViT-H448 [14],0,negative
The high R2 and low MAE values show that the formulation of f is highly effective on modeling the relationship between SSIM-supSubCls and accuracy improvement with our proposed data augmentation pipeline.,2,positive
"The ResNet18 classifiers are trained for 100 epochs and the MAE for 50 epochs when their validation accuracy converges, with their hyperparameters remaining the same throughout the whole procedure in each case.",2,positive
Table 7 compares the state-of-the-art (SOTA) classification validation accuracy from [14] and ours on the whole iNaturalist-2019 dataset.,0,negative
"To certify that this conclusion can be drawn regardless of the classifier selection, we also conduct the experiments with MAE classifier and the same results are obtained.",2,positive
"Then
a ResNet [15] or Masked Autoencoder (MAE) [14] classifier also trained on the original set with sub-class labels are applied on the synthesized images to select qualified ones.",1,neutral
"From this table, our results with Masked Autoencoder (MAE) is comparable with the SOTA one, even though the SOTA is with 448× 448 while ours is with 128×128 input image resolutions which largely reduce the need of running time and computational resources.",2,positive
"In our experiments, the deep generative models, ResNet18 and MAE classifiers are first trained on the original imbalanced set with sub-class instead of super-class labels. cGAN models are trained until the Frechet Inception Distance (FID) scores converge.",2,positive
", 2021), masked image models (He et al., 2022; Dosovitskiy et al., 2020; Doersch et al., 2015; Pathak et al., 2016), masked language models (Devlin et al.",1,neutral
"…in training foundational models with enormous computational power, vast amounts of data, and gigantic neural networks (Radford et al., 2021; Chen et al., 2020; Radford et al., 2019; Brown et al., 2020; Ramesh et al., 2021, 2022; Sohl-Dickstein et al., 2015; Rombach et al., 2022; He et al., 2022).",1,neutral
"…(Chen et al., 2020; He et al., 2020; Grill et al., 2020; Chen and He, 2021; Noroozi and Favaro, 2016; Zbontar et al., 2021), masked image models (He et al., 2022; Dosovitskiy et al., 2020; Doersch et al., 2015; Pathak et al., 2016), masked language models (Devlin et al., 2018; Raffel et al.,…",1,neutral
"In recent years, remarkable progress has been made in training foundational models with enormous computational power, vast amounts of data, and gigantic neural networks (Radford et al., 2021; Chen et al., 2020; Radford et al., 2019; Brown et al., 2020; Ramesh et al., 2021, 2022; Sohl-Dickstein et al., 2015; Rombach et al., 2022; He et al., 2022).",2,positive
"MIM, which is first proposed in BEiT, has been validated its remarkable results in recent works [2, 21, 35, 43, 48] and becomes the new paradigm in visual pre-training.",1,neutral
"Contrarily, MAE [21] reconstructs raw pixels of the image explicitly",1,neutral
"Specifically, our efficient centroid-based MIM outperforms the prior tokenbased MIM [2] and pixel-based MIM [21] in equivalent ViT size and epochs.",2,positive
"Pixel-based MIM with non-parametric tokenizer such as MAE [21] and SplitMask [16], considers vanilla pixels or patches as pre-training targets instead of tokens and need a redundant decoder.",1,neutral
"Actually, in contrast to primarily language tokens as the target in NLP, various reconstruction targets have emerged in previous works in computer vision, including visual tokens [2, 14, 31, 35, 48], high-level features [11] , vanilla pixels [21] and original image features [43], due to the different information density between vision and language.",1,neutral
"…of pretext tasks are tasks to recover an input image from the image with incomplete information [Pathak et al., 2016, Zhang et al., 2016, 2017, He et al., 2022], tasks to predict spatial relationships between subregions of an image, [Doersch et al., 2015, Noroozi and Favaro, 2016, Noroozi et…",1,neutral
"…recall (Equation 8), specificity (Equation 9), and F1 Score (Equation 10) (Labatut and Cherifi, 2012; Giraudo et al., 2018; Alamprese et al., 2021; Chen et al., 2022a; Saranya et al., 2022) in this paper.
accuracy = (TP + TN)
(TP + FP + FN + TN) × 100 (5)
Kappa =
∑n i=1 xii N − ∑n i=1 ( ∑n j=1…",1,neutral
Chen et al. (2022b) developed a high-performance classification model based on a 152-layer deep ResNet to identify different types of walnuts.,2,positive
"The recent MAE method (He et al., 2022) has achieved great success without explicit learning of augmentation invariances.",1,neutral
"2020] and masked autoencoding [He et al. 2022] can be directly applied to the agent’s image observations, providing state representations that can be further finetuned by BC or RL objectives [Sermanet et al.",2,positive
"For instance, when an agent’s perception is based on images, contrastive learning [Chen et al. 2020] and masked autoencoding [He et al. 2022] can be directly applied to the agent’s image observations, providing state representations that can be further finetuned by BC or RL objectives [Sermanet et…",1,neutral
"Self-supervised learning (SSL) is a widely adopted solution in Natural Language Processing (NLP) [1, 2] and Computer Vision (CV) [3, 4].",1,neutral
"of patches, which is adopted by recent pre-training methods like MAE [13].",1,neutral
"Besides, we also consider a baseline strategy (nofill) that drops the masked patches without refilling as in MAE [13].",2,positive
"Masked image modeling [4,13,42,46] has been proved to be an effective approach to vision model pre-training, where random sampling is a common strategy for masking.",1,neutral
"After choosing the patches to mask, simply dropping them following MAE [13] is an intuitive approach to constructing the masked image, i.",1,neutral
"Inspired by the work [24], we propose to use a masked autoencoder based on vision transformer.",2,positive
"Moreover, we have picked up some parameters that have been proven successful in [24].",2,positive
"Thus, a recent development in selfsupervised learning [17, 25, 24], especially with the rise of transformers architectures [15, 18], is now appearing as a solution.",1,neutral
"Motivated by this success in computer vision, such as image classification and object detection [24, 25], image retrieval [26] and speech recognition [27] tasks, we propose in this paper an end-to-end keyword spotting approach in handwritten documents which is based on a self-supervised technique and makes use of masked autoencoders with the self-attention mechanism.",1,neutral
"Different from conventional autoencoders, the used masked encoderdecoder operates in an asymmetric fashion [24] enabling the encoder to operate on only the partial observed signal (without the masked patches) and the decoder to rebuild the full image based on the representation given by the encoder and the masked patches.",2,positive
"This study is inspired by MAE [37], an image representation method that first masks random patches of the input image and then encourages the model to reconstruct those missing pixels.",1,neutral
"Similar with BERT [46] and original MAE [37], only the reconstruction loss of masked hyperedges was calculated.",1,neutral
"2, the partially masked set of hyperedges with the positional embedding was fed into a Transformer [45] based asymmetric autoencoder [37] to reconstruct the missing hyperedges according to the semantic of available hyperedges",1,neutral
"We use the standard ViT-B [14] as the encoder network and initialize it with the MAE pretrained [18] weights following [43, 88].",2,positive
"Language models have also demonstrated their ability to model high-level, long-term sequences for different content types, as shown by the recent advances in text [40, 41, 42, 43, 44] and image [45, 46, 47, 48, 49, 50].",1,neutral
"1 Some works address this issue by tailoring frameworks for dense prediction tasks [7], [25], [38], [59], [62] and a few studies examine segmentation tasks, however, with very large datasets [10].",1,neutral
"The core of the paper is a meticulous analysis based on the milestone algorithm – MAE [20], which discloses critical but neglected bottlenecks of most pixel-based MIM methods.",2,positive
75% in MAE [20] and 60% in SimMIM [56]).,0,negative
"Early MIM methods share a simple pipeline – a portion of non-overlapped image patches are randomly masked, and the model learns to extract discriminative representations by reconstructing the pixel or feature values of the masked patches [1, 20, 56].",1,neutral
"Instead of reconstructing these highlevel features, MAE [20] reconstructs these masked pixel values.",2,positive
BEiT [1] RRC+40% mask ViT+Linear DALLE SimMIM [56] RRC+60% mask ViT+Linear RGB MaskFeat [53] RRC+40% mask ViT+Linear HOG ConvMAE [15] RRC+75% mask ConvViT+MSA RGB MAE [20] RRC+75% mask ViT+MSA RGB,1,neutral
"In the following analysis, we investigate MAE [20] by diagnosing its reconstruction target and input image patches, identifying two important but overlooked bottlenecks that could have hurt the representation quality.",2,positive
"As in supervised learning, the random resized crop (RRC) is the de facto operation for A(·) in MIM [1,20,56].",1,neutral
"We then present a careful analysis with the milestone method, MAE [20], to disclose two important but overlooked bottlenecks of most pixel-based MIM approaches (subsection 3.",2,positive
"We thoroughly evaluate it with three well-established approaches, MAE [20], ConvMAE [15], and LSMAE [28].",2,positive
Pioneering works such as BEiT [1] and MAE [20] exploit Vision Transformers (ViT) to learn discriminative visual represen-,1,neutral
This design lends itself to unsupervised learning and is particularly useful for denoising [17] and reconstruction [18] applications.,1,neutral
"Inspired by MAE [25], our approach reconstructs the holistic features from the latent occluded features.",2,positive
"However, unlike the MAE, the occlusion positions of samples generated by OIA are randomly sampled, so we need to conduct completion on each instance.",2,positive
"It is worth noting that the position of MAE’s mask token is fixed within the batch, which means that each instance can use the same set of mask tokens for feature completion.",1,neutral
"As mentioned in III-C, our FCD adapts MAE’s notion [25] of restoring entire features using implicit unoccluded features.",2,positive
We customize the original masking strategy from MAE [22] and design a facial part masking strategy to ensure that the model can learn the consistencies of all facial parts.,2,positive
Our masked autoencoder is based on an asymmetric encoder-decoder architecture [22].,2,positive
"Unlike MAE [22], which randomly masks most areas of the image,Mover introduces a masking strategy conditioned on facial part consistencies to randomly mask the Regions of interest (ROIs).",1,neutral
"To realize this restoration concept, we pretrain a simple yet powerful masked autoencoder (MAE) model [22] solely on real images.",2,positive
"The original MAE [22] masks random patches of the input image, but the same strategy is not suitable for ourMover for the following reasons.",2,positive
MAE [22] is designed for pretraining Vision Transformers (ViT) rather than for Deepfake detection.,2,positive
"In this stage, we customize MAE[22] and its masking strategy with modifications to perform self-supervised learning of real faces, so that the model can learn generic facial part consistency features and can also prevent over-fitting to specific forgery patterns.",2,positive
"inal MAE [22], cheek & nose, and the strategy without divid-",1,neutral
"Inspired by [22], we propose a novel self-supervised pretraining method that is based on masking and recovering faces, named Mover.",2,positive
The emergence of the masked autoencoder (MAE) [22] has greatly influenced our community.,2,positive
self-supervised pre-training MAE-ViT-L/16 [17] 126K - - 53.,0,negative
"Conventional visual pre-training methods aim to encode the input image as latent representations and learn the representations with pretext tasks like contrastive learning [18, 10] and masked image modeling [2, 17] or massive annotations in classification and vision-language tasks.",1,neutral
"Besides, self-supervised learning such as contrastive learning [7, 18] and masked image modelling [38, 17] have also proved to be able to learn transferrable representations.",1,neutral
The compared methods include self-supervised pre-training (MAE [17]) and superTable 2.,1,neutral
"For 8K iterations, we find VPDA32 surpass all the baseline methods, including those pre-trained on mask image modelling [17, 38], contrastive learning [7] and supervised learning [27, 29].",1,neutral
"Recently, vision transformers [12], [13] have been ported to the audio domain [14]–[18] showing excellent audio classification and general-purpose audio extraction results.",2,positive
"Despite recent advances in deep learning [15, 24, 23, 22], deep neural networks often suffer from performance degradation when the source and target domains differ significantly [8, 43, 38].",1,neutral
"Recent work has shown tremendous improvements in vision community, which are mainly built on top of convolution or attention (e.g., ConvNeXt (Liu et al., 2022), MAE (He et al., 2022), and CLIP (Radford et al., 2021)).",2,positive
"♠ SpiderCNN (Xu et al., 2018) 69.8 73.7 ♠ DGCNN (Wang et al., 2019) 73.6 78.1 ♠ PointCNN (Li et al., 2018) 75.1 78.5 ♠ GBNet (Qiu et al., 2021) 77.8 80.5 q PointBert (Yu et al., 2022d) - 83.1 q Point-MAE (Pang et al., 2022) - 85.2 q Point-TnT (Berg et al., 2022) 81.0 83.5
♣ PointNet (Qi et al., 2017a) 63.4 68.2 ♣ PointNet++ (Qi et al., 2017b) 75.4 77.9 ♣ BGA-PN++ (Uy et al., 2019) 77.5 80.2 ♣ PointMLP (Ma et al., 2022) 83.9 85.4 ♣ PointMLP-elite (Ma et al., 2022) 81.8 83.8 r PointMLP-CoC (ours) 84.4↑0.5 86.2↑0.8
Context Clusters are a natural fit for point clouds Qi et al. (2017b); Lu et al. (2022).",0,negative
", 2022), MAE (He et al., 2022), and CLIP (Radford et al.",2,positive
"on masked image modeling [117], [118] are developing rapidly and is able to even surpass fully-supervised conterparts.",1,neutral
"1 on ImageNet and do not use dropout when pre-training on the much larger JFT-300M dataset; recent languagesupervised or self-supervised vision models (Radford et al., 2021; He et al., 2021) do not use dropout.",0,negative
", 2020) and algorithms for learning with unlabeled or weakly-labeled data (Brown et al., 2020; Radford et al., 2021; He et al., 2021) have provided even more data to train on than the model can fit to.",2,positive
"are exploited to learn image representations, for instance, [3, 21] via image tokens, [31] in pretraining, [36], [36, 81] in self-supervised segmentation, and [19] in detection.",1,neutral
"In particular, a CNN-based network with the RoI-Align He et al. (2017) operation produces visual features for the masked regions.",2,positive
"In MAE He et al. (2022), BEiT Bao et al. (2022), and SimMIM Xie et al. (2022), patch-level Masked Image Modeling has shown strong potential in representation learning.",1,neutral
"MAE He et al. (2022) and SimMIM Xie et al. (2022) take a simpler behavior, predicting RGB values of raw pixels by direct regression.",1,neutral
"…modeling (MIM) that inherits the concept of vision-based self-supervised learning such as BEiT Bao et al. (2022), SimMIM Xie et al. (2022), MAE He et al. (2022), CAE Chen et al. (2022b), and DiT Li et al. (2022), etc. MIM is a powerful image-only pre-training technique to learn the visual…",2,positive
"In particular, a CNN-based network with the RoI-Align He et al. (2017) operation produces visual features for the masked regions. Inspired by ViBERTGrid Lin et al. (2021), the backbone uses FPN Lin et al.",2,positive
"In particular, a CNN-based network with the RoI-Align He et al. (2017) operation produces visual features for the masked regions. Inspired by ViBERTGrid Lin et al. (2021), the backbone uses FPN Lin et al. (2017) to integrate features of CNN.",2,positive
"Unlike MAE He et al. (2022), BEiT Bao et al. (2022), and DIT Li et al. (2022), we do not use patch-level masking strategy for pre-training.",2,positive
"…2018; Brown et al., 2020; Radford et al., 2021; Jia et al., 2021), which demonstrate strong generalization ability across multiple downstream tasks in visual (He et al., 2022b; Bao et al., 2021), language (Liu et al., 2019; Raffel et al., 2020) and
1Alibaba Group 2National University of Singapore.",2,positive
"Another way to insert adapters is to add a scaling factor and design the adapter explicitly as a parallel module (He et al., 2022a; Chen et al., 2022), which can be similarly viewed as parallel structures.",1,neutral
"…8, we compare several tuning methods with our proposed U-Tuning based on different architectures and pre-trained models, i.e., IN-21K (Deng et al., 2009), MAE (He et al., 2022b) and CLIP (Radford et al., 2021) pre-trained ViT/B16 (Dosovitskiy et al., 2020), as well as CLIP pre-trained ViT/L-14.",2,positive
"In Table 8, we compare several tuning methods with our proposed U-Tuning based on different architectures and pre-trained models, i.e., IN-21K (Deng et al., 2009), MAE (He et al., 2022b) and CLIP (Radford et al., 2021) pre-trained ViT/B16 (Dosovitskiy et al., 2020), as well as CLIP pre-trained ViT/L-14.",2,positive
"For FFNs, the adaptation is generally made by adapter (Houlsby et al., 2019) and its generalized versions (Pfeiffer et al., 2020; Karimi Mahabadi et al., 2021b;a; He et al., 2022a), which usually insert a bottleneck layer into each FFN layer.",2,positive
"Due to the lack of labeled resources, to train such large models, some self-supervised methods have achieved great success, such as MAE [66] and masked language modeling [4].",1,neutral
"Masked autoencoders [20], [21], which can well overcome these aforementioned limitations, have been proposed before, whose main philosophy is to encode the maskingstyle corrupt input into latent space followed by a recovery of the raw inputs via the encoder and decoder.",1,neutral
"Currently, self-supervised pre-training paradigms have nearly become the default configuration in the domains of natural language (NLP) [12], [13], [20] and computer vision (CV) [14], [15], [21].",1,neutral
"Instead of using a fixed masking ratio in language and visual pre-training [6, 17], the generative transformer needs to generate tokens from scratch and applies a randomly sampled ratio γ(r) ∈ (0, 1] in training.",1,neutral
"They are reported to achieve results better than supervised models trained on fewer labeled samples and have found applications in computer vision (He et al., 2022), natural language processing (Devlin et al., 2018; Vaswani et al., 2017) and audio processing (Schneider et al., 2019).",1,neutral
"vised models trained on fewer labeled samples and have found applications in computer vision (He et al., 2022), natural language processing (Devlin et al.",1,neutral
", MAE [15]), we use this finding to further reduce the computational complexity of our model.",1,neutral
"Similar to recent works in the computer vision domain (e.g., MAE [15]), we use this finding to further reduce the computational complexity of our model.",1,neutral
"Recently, some general pre-trained models [8], [9] have been widely used for better representation and then they are fine-tuned for various downstream tasks, e.",1,neutral
"Motivated by the success of BERT [13] in NLP, many recent works show a variety of MIM schemes for pre-training vision models in a self-supervised way, using reconstruction targets such as pixels [9], [40] and discrete tokens [41].",1,neutral
"Masked language modeling (MLM) (Kenton & Toutanova, 2019) and masked image modeling (MIM) (Xie et al., 2022; He et al., 2022) have been dominant self-supervised approaches in NLP and CV domains.",1,neutral
", 2021), self-supervised pretraining (He et al., 2022), to name a few.",2,positive
"…of fast-evolving and well-studied computer vision techniques in the time series domain, such as better model architecture (Liu et al., 2022), data augmentation (Shorten & Khoshgoftaar, 2019), interpretability (Chefer et al., 2021), self-supervised pretraining (He et al., 2022), to name a few.",2,positive
"problem is easy to tune by changing the used time horizon or including masking during training [12,14].",1,neutral
"For model fine-tuning with MAE, we adopt the settings in (He et al., 2021).",1,neutral
"For example, we obtain better results by integrating our method into the fine-tuning process of the Masked AutoEncoder (MAE) in (He et al., 2021), achieving improvements on ImageNet by 0.3%, ImageNet-A by 1.8%, ImageNet-R by 1.4% and ImageNet-C by 3.7% on Vision Transformer Large (Dosovitskiy et…",2,positive
"The baseline models are taken directly from (He et al., 2021).",2,positive
"15 is that we take the results directly from (He et al., 2021), which have no detailed data of different corruptions.",0,negative
"For example, we obtain better results by integrating our method into the fine-tuning process of the Masked AutoEncoder (MAE) in (He et al., 2021), achieving improvements on ImageNet by 0.",2,positive
"C.15 is that we take the results directly from (He et al., 2021), which have no detailed data of different corruptions.",0,negative
"FT follows MAE (He et al., 2022), while Ours uses MoCo v2 contrastive loss and regularization coefficient λ = 0.",1,neutral
"FT follows MAE (He et al., 2022), while Ours uses MoCo v2 contrastive loss and regularization coefficient λ = 0.1.",2,positive
"FT follows the training in MAE (He et al., 2022) mostly, using SGD for 200 epochs with a cosine learning-rate scheduler, the base learning rate of 0.",2,positive
"FT follows the training in MAE (He et al., 2022) mostly, using SGD for 200 epochs with a cosine learning-rate scheduler, the base learning rate of 0.06, weight decay 5e-4, momentum 0.9 and batch size 256.",2,positive
"We adopted the masked auto-encoder (MAE) (He et al., 2022) to process the point cloud data (denoted as MAE-dense) for pre-training, which is close with Point-MAE (Pang et al., 2022).",2,positive
"To pre-train the MAE-dense and MAE-sampled, we first follow the standard train-test split of ShapeNet (Chang et al., 2015) adopted by Pang et al. (2022); Yu et al. (2022).",2,positive
Few Shot Learning The pre-trained MAE-dense and MAE-sampled models are finally evaluated on a few shot learning task.,1,neutral
It is denoted as MAE-sampled.,1,neutral
"In addition, the representation power of the transformer has been explored by the pre-training and fine-tuning models (Bao et al., 2021; Yu et al., 2022; He et al., 2022).",2,positive
"Specifically, we build the classifier by keeping the encoder structure and weights of the pre-trained MAE-dense and MAE-sampled models, followed by max-pooling as well as a fully connected layer of dimension [256, 256, 40] to map the global token of a dimension of 256 to the 40 categories.",2,positive
"Classification The pre-trained MAE-dense and MAEsampled models are first evaluated on the classification task in ModelNet40 (Wu et al., 2015).",1,neutral
Note that MAE-dense adopts dense-attention layers in its encoder and decoder network.,1,neutral
"3, our proposed MAE-sampled outperformed all state-of-the-art methods on 3 out of 4 settings, while MAE-sampled consistently outperformed MAE-dense.",2,positive
"We adopted the masked auto-encoder (MAE) (He et al., 2022) to process the point cloud data (denoted as MAE-dense) for pre-training, which is close with Point-MAE (Pang et al.",2,positive
"To evaluate the effectiveness of our claimed contribution, we replace the dense-attention layer in MAE-dense with our sampled-attention layer (Fig.",2,positive
"To maintain a fully self-supervised pre-training paradigm, we initialize with weights obtained by self-supervised ImageNet pre-training [19].",2,positive
MAE [19] is used for pre-training with randomly shuffled ScanNet images.,1,neutral
"MAE [19] then proposed an approach inspired by BERT [13], which randomly masks words in sentences and leveraged masked image reconstruction for self-supervised pre-training that achieved state-of-the-art results in ViT.",2,positive
"Similar to MAE [19], we normalize the output patches as well as the target patches prior to computing the loss, which we found to empirically improve the performance.",2,positive
2) MAE-unsupIN→SN [19] ViT ImageNet+ScanNet 54.,1,neutral
6) MAE-unsupIN→SN [19] ViT ImageNet+ScanNet 63.,1,neutral
"MOCOv3 achieves slightly better performance than normal training as shown in Table 4, and MAE improves the natural robustness significantly based on ViT, as shown in Table 5.",0,negative
"Second, MAE (He et al, 2022) improves adversarial robustness on ViTs, while MOCOv3 (Chen et al, 2021) benefits adversarial robustness.",2,positive
"Although similar to MAE, we choose SimMIM because it adopts the backbone of Swin Transformer, which performs better than ViT adopted in MAE, as shown in the experiment.",2,positive
"It is shown that the robust curves of the models with the same
Springer Nature 2021 LATEX template
ARES-Bench 3
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8 1.0 A cc ur ac y
(a)
VGGs VGG13_Normal VGG16_Normal VGG19_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (b)
ResNets ResNet50_Normal ResNet50_MOCO ResNet101_Normal ResNet152_Normal Wide-ResNet50_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(c)
DenseNets DenseNet121_Normal DenseNet161_Normal DenseNet201_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(d)
ConvNexts ConvNextS_Normal ConvNextS_21K ConvNextB_Normal ConvNextB_21K ConvNextL_Normal ConvNextL_21K
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(e)
ViTs ViTS_Normal ViTS_21K ViTB_Normal ViTB_21K ViTB_MAE ViTL_Normal ViTL_21K ViTL_MAE
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(f)
XciTs XciTS_Normal XciTM_Normal XciTL_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (g)
T2Ts T2T14_Normal T2T19_Normal T2T24_Normal
0.000 0.001 0.002 0.003 0.004 0.005 Perturbation Budget
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(h)
Swins SwinS_Normal SwinS_21K SwinB_Normal SwinB_21K SwinL_21K
Fig.",0,negative
"Springer Nature 2021 LATEX template
ARES-Bench 13
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8 1.0 A cc ur ac y
(a)
Normal Models VGG19_Normal ResNet152_Normal DenseNet161_Normal ConvNextL_Normal ViTL_Normal XciTL_Normal T2T24_Normal SwinB_Normal
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (b)
Pre-trained Models ResNet50_Normal ResNet50_MOCO ViTB_Normal ViTB_21K ViTB_MAE ConvNextL_Normal ConvNextL_21K SwinB_Normal SwinB_21K
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(c)
AT Models ResNet152_Normal ResNet152_AT ConvNextL_Normal ConvNextL_AT ViTB_Normal ViTB_AT XciTL_Normal XciTL_RB SwinB_Normal SwinB_AT",0,negative
"For self-supervised learning methods, MAE has a negative impact on the adversarial robustness especially under large perturbation budgets, but MOCOv3 improves adversarial robustness compared with the normally trained models, as shown in Fig.",1,neutral
"IN-Val IN-V2 IN-Real ON IN-A IN-R IN-V IN-C SIN IN-Sketch
ViTS
Normal 74.4 61.6 80.0 13.1 8.8 30.4 11.2 32.0 9.1 19.9 34.0 Pre-train 81.4 70.3 86.8 22.7 27.3 45.7 16.6 47.1 15.8 32.5 44.6 AT 70.2 57.3 77.9 11.5 6.1 46.0 8.5 27.8 16.8 29.8 35.2
ViTB
Normal 75.8 61.6 80.9 13.2 11.4 32.8 13.3 34.3 10.9 23.7 35.8 Pre-train 84.6 73.9 88.8 27.4 44.5 56.8 19.4 57.5 22.6 43.0 51.9 MAE 83.6 73.1 88.1 24.9 37.7 49.8 18.2 49.4 20.2 36.4 48.1 AT 73.4 60.4 80.5 12.7 8.9 50.7 9.4 36.6 22.2 35.7 39.1
ViTL
Normal 75.2 60.7 79.8 11.2 11.3 33.3 13.4 35.4 9.3 25.0 35.4 Pre-train 85.8 76.0 89.2 30.5 56.1 64.2 25.5 65.3 30.1 51.8 57.4 MAE 85.1 75.6 89.0 27.3 50.6 60.0 21.5 56.2 24.1 46.4 53.6
XciTS Normal 82.4 71.5 86.8 23.7 31.3 45.0 17.0 50.1 19.5 32.9 46.0
RB 73.3 60.5 80.6 12.7 6.3 45.7 9.7 28.5 18.4 31.2 36.7
XciTM Normal 82.6 71.0 86.8 23.4 33.3 44.7 17.7 50.5 20.3 33.1 46.3
RB 74.1 61.7 81.3 13.6 7.0 47.1 9.5 30.2 19.7 32.6 37.7
XciTL Normal 83.0 72.0 86.9 23.7 36.2 46.2 17.9 50.2 20.4 34.4 47.1
RB 75.1 62.7 81.7 13.4 8.8 49.0 10.7 32.0 19.9 34.4 38.7
T2T14 Normal 81.6 70.9 86.8 22.3 24.1 44.7 16.7 46.8 17.7 32.2 44.4
T2T19 Normal 82.3 71.6 87.2 23.2 29.0 47.3 18.0 50.2 20.9 34.4 46.4
T2T24 Normal 82.4 71.7 87.2 22.9 29.7 47.9 18.0 52.0 20.8 35.1 46.8
SwinS
Normal 83.2 72.1 87.5 24.7 33.0 44.9 19.3 45.1 16.8 32.0 45.8 Pre-train 83.3 73.5 88.6 28.1 43.9 54.8 21.3 50.6 17.2 41.2 50.3 AT 75.8 63.3 82.6 15.3 10.6 52.5 10.8 37.1 21.1 37.1 40.6
SwinB
Normal 83.4 72.3 87.6 25.5 35.8 46.6 20.2 45.6 17.9 32.4 46.7 Pre-train 85.1 75.2 89.1 28.8 51.8 59.1 22.7 56.4 19.6 45.1 53.3 AT 76.8 64.5 83.4 15.5 13.1 53.5 11.8 39.3 22.7 39.3 42.0
SwinL Pre-train 86.3 77.0 89.6 31.6 61.0 63.6 26.4 61.3 23.4 48.8 56.9
AT 78.7 66.9 84.9 18.2 18.1 57.3 11.6 43.4 25.2 42.9 44.7
increases from 34.1% of ResNet50 to 36.8% of ResNet101, and finally to 38.0% of ResNet-152.",0,negative
"C V
] 2
8 Fe
b 20
23
2 ARES-Bench
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8 1.0 A cc ur ac y
(a)
VGGs VGG13_Normal VGG16_Normal VGG19_Normal
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (b)
ResNets ResNet50_Normal ResNet50_MOCO ResNet50_AT ResNet50_RB ResNet50_RL ResNet101_Normal ResNet101_AT ResNet152_Normal ResNet152_AT ResNet152_FD Wide-ResNet50_Normal Wide-ResNet50_AT Wide-ResNet50_RB
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(c)
DenseNets DenseNet121_Normal DenseNet161_Normal DenseNet201_Normal
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(d)
ConvNexts ConvNextS_Normal ConvNextS_21K ConvNextS_AT ConvNextB_Normal ConvNextB_21K ConvNextB_AT ConvNextL_Normal ConvNextL_21K ConvNextL_AT
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(e)
ViTs ViTS_Normal ViTS_21K ViTS_AT ViTB_Normal ViTB_21K ViTB_MAE ViTB_AT ViTL_Normal ViTL_21K ViTL_MAE
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(f)
XciTs XciTS_Normal XciTS_RB XciTM_Normal XciTM_RB XciTL_Normal XciTL_RB
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac y (g)
T2Ts T2T14_Normal T2T19_Normal T2T24_Normal
0 1 2 3 4 5 Severity
0.0
0.2
0.4
0.6
0.8
1.0
A cc
ur ac
y
(h)
Swins SwinS_Normal SwinS_21K SwinS_AT SwinB_Normal SwinB_21K SwinB_AT SwinL_21K SwinL_AT
Fig.",0,negative
"…including typical CNNs and Transformers, and different learning algorithms, including normal supervised training, pre-training on large-scale datasets (Dosovitskiy et al, 2021), selfsupervised learning (SSL) (Chen et al, 2021; He et al, 2022), and adversarial training (AT) (Madry et al, 2018).",2,positive
Different reconstruction results of MAE [19] correspond to different mask seeds.,1,neutral
"Besides, mask image modeling [1,4,14,19,30,48,53] is currently the focus of the research community.",1,neutral
MAE [19] adopts 75% mask ratio while BERT [12] uses 15% mask ratio).,1,neutral
"Is it possible to reduce the random mask ratio to increase pre-training efficiency and improve consistency? In fact, the prior work [19] already shows that reducing the mask ratio brings lower transfer ability for downstream tasks.",1,neutral
It is noted that MAE [19] proposes an asymmetric encoder-decoder architecture for the MIM task and shows excellent performance in a variety of visual downstream tasks.,2,positive
"Specifically, according to the pioneering work [19], given a natural image from an unlabeled dataset X, we divide it into N regular image patches, denoted as x ∈ RN×S where S denotes the patch size (e.",1,neutral
"Commonly, the random mask ratio of MIM is much higher than that of MLM due to the difference in the information density of image and language data [19] (e.",1,neutral
"Following the setting of MAE [19], EMAE is mainly evaluated on linear probing, finetuning classification, object detection, instance segmentation, and semantic segmentation tasks.",1,neutral
"The training settings are the same as MAE [19], we adopt the same encoder-decoder structure to perform the MIM task.",2,positive
"For a fair comparison, we fully follow the hyperparameters of MAE [19] in our image classification experiments.",2,positive
"Masked Convolution Meets Masked Autoencoders (ConvMAE) ConvMAE [21], a derivative of the popular MAE [19], is proposed to train scalable visual representation with hybrid convolution-transformer architectures and masking convolution.",2,positive
"Vision Image BEiT v1 [16], v2 [27], MAE [19], SimMIM [28], ADIOS [29], AMT [30], AttMask [31], Beyond-Masking [32], BootMAE [33], CAE [20], CAN [34], ConvMAE [21], Contrastive MAE [22], ContrastMask [35], dBOT [36], DMAE [37], Denoising MAE [38], GreenMAE [23], iBOT [39], LoMaR [40], LS-MAE [41], MaskAlign [42], MaskDistill [18], MaskFeat [43], MaskTune [44], MetaMask [45], MFM [46], MILAN [47], MixMask [48], MixMIM [24], MRA [49], MSN [50], MST [51], MultiMAE [52], MVP [53], RC-MAE [54], SDMAE [55], SemMAE [56], SdAE [57], SupMAE [58], U-MAE [59], UM-MAE [60]",2,positive
MAE’s ablation study also points out that a high masking ratio is good for fine-tuning and linear probing [19].,1,neutral
"The most famous one is Masked Autoencoder (MAE) [19], which owns a very simple learning architecture but has been proven to be a strong and scalable pre-training framework for visual representation learning.",2,positive
"With those meticulous designs, MAE is three times (or more) faster than BEiT [16] while achieving superior performance [19].",2,positive
"In detial, MAE[7] and SimMIM[23] replace a random subset of input tokens with a special MASK symbol and aim at reconstructing original image tokens from the corrupted image with Vision transformers[5, 14].",1,neutral
"Several works[7, 23, 19, 11, 12] focus on using valid information via the pre-text task to improve downstream vision tasks.",1,neutral
"Subsequently, VideoMAE[19] proves that an extremely high proportion of masking ratio still yields favorable performance on videos.",2,positive
"In detail, MAE[7] and SimMIM[23] replace a random subset of input tokens with a special MASK symbol designed to reconstruct the original image tokens from corrupted images using Vision transformers[5, 14].",1,neutral
"Several works[7, 23, 19, 11, 12] focus on improving downstream visual tasks by using effective information in pre-text tasks.",1,neutral
Dataset Images ViT[13] DeiT III[14] MAE[15] IN-LT 18.,1,neutral
"DeiT[14] proposes an effective receipt to train ViT with limited data, and MAE[15] adopts a masked autoencoder to pre-train the ViT.",0,negative
"We adopt the recipe in vanilla ViT[13], DeiT III[14], and MAE[15] to train ViTs.",2,positive
"supervised visual pre-training can be classified into three categories: contrastive learning based [15, 23, 40], distillation based [6, 20], and masked image modeling based [22, 52].",1,neutral
"But along with the progress of these efforts, the huge amount of data, in turn, becomes a barrier to both storage and training [56, 20].",2,positive
"Follow MAE (He et al., 2021), we evaluate the performance of the proposed Layer Grafted Pre-training with different number of fixing blocks.",2,positive
"…leads to different strengths: On the one hand, empirical results highlight that the
masked view can benefit the downstream fine-tuning task (Touvron et al., 2022; He et al., 2021), which may be because it helps to learn the correlation between sparse patches that cannot be built under full view.",2,positive
"…first step of Layer Grafted Pre-training, since it identically follows the original MIM pipeline, we directly adopt the pre-trained model of MAE (He et al., 2021) and conduct the second step, where we initialize the network with the MIM pre-trained model and train with Moco V3 (Chen et al.,…",2,positive
"For other settings such as data augmentation, LR scheduling or weight decay, we identically follow He et al. (2021).",1,neutral
"The performance of MIM and CL are from MAE (He et al., 2021) and MocoV3 (Chen et al.",2,positive
"…65.3 -
C-MAE (Huang et al., 2022) 73.9 65.3 77.3 MimCo (Zhou et al., 2022) 70.2 62.7 - Layer Grafted Pre-training (Ours) 77.7 65.5 77.8
ViT-L/16 MAE (He et al., 2021) 75.8 55.2 78.7
Moco V3 (Chen et al., 2021) 77.6 - - Layer Grafted Pre-training (Ours) 81.0 69.3 80.1
Our method also demonstrates…",0,negative
"MAE (He et al., 2021) and simMIM (Xie et al., 2022) further show the possibility of directly reconstructing the original pixels.",2,positive
"For fine-tuning, we train with AdamW (Loshchilov & Hutter, 2017) for 100 epochs following MAE (He et al., 2021).",2,positive
"We by default adopt MAE (He et al., 2021) and Moco V3 (Chen et al., 2021) as our MIM and CL frameworks, respectively.",2,positive
"Then, the image with minimal augmentation would be utilized for computing MIM loss following MAE He et al. (2021).",1,neutral
"…with two mainstream representative methods: Contrastive learning (CL) (Chen et al., 2020b; He et al., 2020; Chen et al., 2020d; 2021; Grill et al., 2020; Caron et al., 2021), and Mask Image Modeling (MIM) (Bao et al., 2021; He et al., 2021; Xie et al., 2022; Dong et al., 2021; 2022).",2,positive
"In contrast, we employ MAE (He et al., 2021), an image modeling framework that is free from strong augmentations.",2,positive
", 2021), and Mask Image Modeling (MIM) (Bao et al., 2021; He et al., 2021; Xie et al., 2022; Dong et al., 2021; 2022).",2,positive
"However, the masked autoencoders [23] that corrupt input and then attempt to recover it has shown great success in vision tasks, which might also be applicable to graphs.",1,neutral
The mask-and-reconstruct training paradigm has been proven to be effective in computer vision [23] and NLP [1].,1,neutral
"For example, BERT [1] adopts random dropping to generate partially observed word sequences for language modeling, and MAE [23] applies patch-aware random masking to yield masked image channels for visual representation.",1,neutral
"However, if the graph structure is corrupted, especially when the non-trivial perturbation is conducted [23], the GNN encoder would inevitably be affected, leading to noisy node representations.",1,neutral
"Similar to MAE [23], the intuitive solution is to treat nodes as pixels and then uniformly sample neighboring nodes for edge masking.",1,neutral
Masked autoencoding is a highly successful framework for pretraining in text [1] and image [23] domains.,1,neutral
A similar idea has been successfully explored in the text [1] and image [23] fields.,1,neutral
"In this paper, we propose to predict the masked edges in Emask in training, inspired by the success of masked autoencoding in computer vision [23].",2,positive
"Also, inspired by masked image modeling [He et al., 2021], a series of works for masked point modeling [Liu et al.",1,neutral
"In 2D modality, MAE [He et al., 2021] and its followup work efficiently conduct 2D masked autoencoding with multi-scale convolution stages [Gao et al.",1,neutral
"Unlike MAE [He et al., 2021] and Point-MAE [Pang et al.",1,neutral
"As shown in Figure 4, we adopt two reconstruct heads of simple linear layers to respectively recover the masked 2D pixels and 3D coordinates following MAE [He et al., 2021] and Point-MAE [Pang et al.",1,neutral
"Inspired by the success of MAE pre-training independently in 2D and 3D vision [He et al., 2021; Gao et al., 2022; Pang et al., 2022; Zhang et al., 2022a], we expect to fully incorporate MAE pre-training and multi-modality learning to unleash their potientials for 3D representation learning.",2,positive
"The information density of image is much lower than that of text, and more patches need to be masked for a better performance [11, 33].",1,neutral
"Since previous research [13] has shown that a 75% masking rate produces optimal representations in autoencoders, this paper will use a 75% masking rate by default.",1,neutral
"Additionally, some researchers [10] replaced the ViT [12] used by MAE [13] with Swin Transformer [14] to adapt to small medical datasets, while others [11] applied the masked autoencoder to medical multimodal data.",1,neutral
"Additionally, we compared our method with some recent self-supervised methods, including MoCoV2 [32], MAE [13], and ConvMAE [33].",2,positive
", 2021), masking-based (Bao et al., 2021; He et al., 2022), or multimodal (Radford et al.",2,positive
"Foundation models [57, 7, 3, 24], trained on broad data (with self-supervised learning [32, 68, 11, 51]), have shown great promise on a wide spectrum of downstream tasks [39, 49, 16, 1] with great generalisation ability.",1,neutral
"masked input, like MAE [9] and SimMIM [10].",1,neutral
"Besides, several state-of-theart self-supervised learning methods are chosen as the selfsupervised learning baselines in our experiments, including SimCLR [5], BYOL [6], SwAV [7], MAE [9], and SimMIM [10].",2,positive
"Generative approaches hypothesize that a model that can capture the image distribution will learn semantically relevant features [26, 31, 37, 70, 98, 115].",1,neutral
"Training models to predict masked out portions of the input data is an approach to self-supervised learning that has led to strong empirical results in the deep learning literature (Devlin et al., 2019; Yang et al., 2019; Brown et al., 2020; He et al., 2022).",1,neutral
"Masked Visual Pretraining [MVP; 65] proposes using masked autoencoding [29] to prioritize visual reconstruction from heavily masked video frames, encoding representations that facilitate per-pixel reconstruction.",1,neutral
Masked AutoEncoders (MAE) [29] masks random patches from the input image and learns a beneficial visual representation via reconstructing the missing patches in the pixel space.,1,neutral
"In this paper, we present VoxFormer, a strong camera-based 3D semantic scene completion (SSC) framework composed of (1) class-agnostic query proposal based on depth estimation and (2) class-specific segmentation with a sparse-to-dense MAE-like design.",2,positive
Stage-2 is based on a novel sparse-to-dense MAE-like architecture as shown in Fig.,2,positive
"Our framework is a two-stage cascade composed of class-agnostic proposals and class-specific segmentation similar to [68]: stage-1 generates class-agnostic query proposals, and stage-2 uses an MAE-like architecture to propagate information to all voxels.",2,positive
"After obtaining voxel query proposals based on depth, VoxFormer generates semantic voxels via an MAE-like architecture [3].",2,positive
"Motivated by reconstruction-before-hallucination and sparsity-in-3D-space , we build a two-stage framework: stage-1 based on CNN proposes a sparse set of voxel queries from image depth to attend to images since the image features correspond to visible and occupied voxels instead of non-visible and empty ones; stage-2 based on Transformer uses an MAE-like architecture to first strengthen the featurization of the proposed voxels by voxel-to-image cross-attention, and then process the full set of voxels with self-attention to enable the voxel interactions.",2,positive
We name such learnable parameter as mask token [3] for conciseness since unselected from Q is analogous to masked from Q.,1,neutral
• A novel Transformer similar to masked autoencoder (MAE) [3] that yields complete 3D scene representation.,1,neutral
"As a result, a modified variant of the MAE architecture named DropMAE was introduced, as depicted in Fig.",1,neutral
"Similar to DropMAE, MAT randomly masks patches of template and search image pairs, which are then jointly processed by the encoder to capture their visual representations.",1,neutral
OSTrack utilizes a self-supervised learning-based Masked Auto Encoder (MAE) [108] pre-trained model to initialize the ViT backbone.,2,positive
"Closely related to DropMAE, another masked encoderbased pre-trained model has been specifically designed and trained for the tracking task, referred to as MAT [102].",2,positive
"Track [66], have demonstrated that initializing the backbone with a self-supervised learning based Masked Autoencoder (MAE) [108] pre-trained model can achieve higher tracking accuracy compared to models based on supervised learning.",1,neutral
"This improvement can be attributed to the MAE’s ability to capture fine-grained local structures within an image, which are essential for accurate target localization.",1,neutral
The OSTrack [62] approach showed better performance while fine-tuning the DropMAE as the backbone compared to the MAE [108] backbone.,2,positive
"Due to the great success of OSTrack in the tracking community, several recent follow-up approaches [63], [65], [66], [67] have been proposed, utilizing self-supervised learning based MAE pre-trained model to initialize the backbone network.",2,positive
"encoding, in contrast to the single decoder of other models [65], [108], MAT employs two identical decoders to separately reconstruct the search image and the target region in the search image.",1,neutral
"Additionally, DropMAE follows an attention dropout mechanism that restricts the interaction between tokens within the",1,neutral
DropMAE captures spatial cues within individual images and also captures the correlated spatial cues between two frames by randomly masking the input frames and processing them through the encoder-decoder architecture.,2,positive
The experimental results demonstrated that the OSTrack [62] approach achieved superior performance when utilizing DropMAE to initialize the backbone compared to initializing with MAE backbone.,2,positive
enhanced their tracking accuracy by utilizing self-supervised learning based masked autoencoder pre-trained models [108] to initialize the tracker encoder.,2,positive
"Recently, Wu et al. [65] discovered that the MAE architecture exhibits a lack of robustness when applied to feature matching tasks between two images.",1,neutral
"[65] Q. Wu, T. Yang, Z. Liu, B. Wu, Y. Shan, and A. B. Chan, ‘‘DropMAE: Masked autoencoders with spatial-attention dropout for tracking tasks,’’ in Proc.",1,neutral
Auto Encoder (MAE) [108] pre-trained model to initialize the ViT backbone.,2,positive
"However, recent fully Transformer-based trackers, such as OSTrack [62], ProContEXT [63], GRM [67], and VideoTrack [66], have demonstrated that initializing the backbone with a self-supervised learning based Masked Autoencoder (MAE) [108] pre-trained model can achieve higher tracking accuracy compared to models based on supervised learning.",1,neutral
"Algorithmically, the Figure 2: Illustration of representative SSL methods: SimCLR [9], MoCo V3 [9], BYOL [15], and the Masked Auto-Encoder [10].",1,neutral
"We further evaluate our defense under other popular SSL training algorithms and different model structures and datasets, e.g., ResNet-18 and ViT-Small/16 trained using SimCLR, MoCO V3, BYOL, MAE over CIFAR-10 or the ImageNet (Appendix 6.3).",2,positive
"By contrast, the recently proposed SSL method, MAE [10], trains the encoder f (·|θ) by masking a portion of pixels in an image x (the masked image is denoted by x′) and then using f (x′|θ) with a decoder d(·) to restore x.",1,neutral
"With the thriving development of SSL, especially contrastive learning (e.g., SimCLR [9, 13], MoCo [14, 41, 42], BYOL [15]) and the MAE [10], backdoor attacks targeting SSL have also been explored.",1,neutral
", SimCLR [9], MoCo V3 [14], BYOL [15], and the MAE [10], for evaluation.",2,positive
", through contrastive learning [13–15] or masked autoencoder (MAE) [10]) and then fine-tunes only the last layer using labeled data from a specific downstream task.",1,neutral
"Here we use self-supervised learning methods consistent with those in Section 4.3, including the contrastive learning method SimCLR, MoCO V3, BYOL, and the masked-model training method MAE.",2,positive
", [10, 11, 24]); (2) FT-last (or linear adaptation): only the last fully-connected layer is updated (e.",1,neutral
"In SSL adaptation, one pre-trains a model on large unlabeled data (e.g., through contrastive learning [13–15] or masked autoencoder (MAE) [10]) and then fine-tunes only the last layer using labeled data from a specific downstream task.",1,neutral
", SimCLR [9, 13], MoCo [14, 41, 42], BYOL [15]) and the MAE [10], backdoor attacks targeting SSL have also been explored.",2,positive
"For Case-1, we incorporate four state-of-the-art SSL training methods, i.e., SimCLR [9], MoCo V3 [14], BYOL [15], and the MAE [10], for evaluation.",2,positive
This paper focuses primarily on two of the most recent SSL schemes: contrastive learning and masked auto-encoder (MAE).,1,neutral
"For example, the accuracies of our model pre-trained with MAE were 8.68% and 5.16% higher than those of ResNet101 pre-trained with SimSiam in the five-class and binary classification tasks.",0,negative
"By leveraging a simple SSL framework, MAE, we alleviated the problem of training classification models without sufficient high-quality labeled OCT images.",2,positive
"He et al. [35] have demonstrated that MAE has the advantages of scalability and simplicity compared with contrastive self-supervised learning approaches, such as DINO and MoCo.",1,neutral
"Third, MAE can help Transformer-based models achieve better classification performance than DINO, one of the SOTA contrastive learning frameworks.",2,positive
"The first stage is that the proposed classification model is pre-trained with MAE [35], a non-contrastive SSL framework, using label-free cervical OCT images.",2,positive
"Besides, it is worth noting that Transformer-based models pretrained with MAE outperformed those supervised ViT models pre-trained on the ImageNet-1 K dataset.",2,positive
[35] as the initial weights of our model.,2,positive
"The purpose of such operations is to reduce the image resolution as much as possible, thereby reducing the amount of graphics processing unit (GPU) memory used and speeding up model pre-training with MAE.",2,positive
Section III presents the image classification model built based on ViT and MAE for cervical OCT images.,0,negative
"The first stage is that the proposed classification model is pre-trained with MAE [35], a non-contrastive
SSL framework, using label-free cervical OCT images.",2,positive
"Due to the popularity and advantage of non-contrastive SSL frameworks (e.g., masked auto-encoders (MAE) [35]), this study aims to design a ViT-based classification model for cervical OCT images and pre-train it in a non-contrastive self-supervised manner.",1,neutral
"In the model pre-training stage, we transfer the weights of the MAE encoder and decoder pre-trained on the ImageNet-1 K dataset [35] (see the upper part of Fig.",2,positive
"For the classification task of cervical OCT images, we are
the first to propose a ViT-based image classification model pre-trained with a non-contrastive SSL framework, MAE, which can help ease the burden of insufficient labeled image data on the model’s prediction performance.",2,positive
A few essential parameters were configured in the selfsupervised model pre-training with MAE.,2,positive
", masked auto-encoders (MAE) [35]), this study aims to design a ViT-based classification model for cervical OCT images and pre-train it in a non-contrastive self-supervised manner.",1,neutral
"The decoder of MAE, more specifically, a lightweight Transformer decoder, needs to process the complete token set of the input image.",1,neutral
"For example, when selecting ViT-B as the backbone, the five-class accuracy, binary accuracy, sensitivity, specificity, and AUC of MAE were increased by 2.39%, 1.52%, -0.30%, 2.98%, and -0.08%, respectively.",1,neutral
"1(a) presents the self-supervised pre-training of our model with MAE, which includes a ViT encoder and a lightweight Transformer decoder.",2,positive
"Meanwhile, the weights of three ViT models are transferred from the weights obtained in the self-supervised model pre-training with MAE on the image resolution of 224×224 pixels.",2,positive
"For example, compared with the supervised MViT-B (the SOTA model) with the weights transferred from the ImageNet-1 K dataset, the five-class and binary classification accuracies of ViT-B (224) pre-trained with MAE were increased by 2.65% and 1.52%, respectively.",0,negative
"[35] have demonstrated that MAE has the advantages of scalability and simplicity compared with contrastive self-supervised learning approaches, such as DINO and MoCo.",1,neutral
"…2018), clustering (Van Gansbeke et al., 2020; Caron et al., 2020), contrastive learning (Chen et al., 2020; He et al., 2020), mask and reconstruct (He et al., 2022), etc. are adopted to extract transferable representations from the
Algorithm 1 Effective bias-Conflicting Scoring (ECS)
Input:…",2,positive
"This provides a strong regularization effect, and improves generalization consistently, across architectures, modalities and loss functions (Hernández-Garcı́a & König, 2018; Steiner et al., 2021; Hou et al., 2018; Shen et al., 2020; Chen et al., 2020; Caron et al., 2020; He et al., 2021).",2,positive
"However, note that our approach is general and easily extends to self-supervised settings. e.g. (Chen et al., 2020; He et al., 2021; Chen et al., 2021).",1,neutral
"This provides a strong regularization effect, and improves generalization consistently, across architectures, modalities and loss functions (Hernández-Garcı́a & König, 2018; Steiner et al., 2021; Hou et al., 2018; Shen et al., 2020; Chen et al., 2020; Caron et al., 2020; He et al., 2021).",2,positive
We only take the unmasked patches as the input of the encoder similar to MAE [9].,2,positive
"• Vanilla MAE (MAE) [9], which uses an autoencoder to reconstruct the images from masked images.",1,neutral
"(Yang et al., 2022)
Oriented RCNN ViTAE 81.24 Use MAE (He et al., 2022) to pretrain the plain ViTAE transformer.",2,positive
"24 Use MAE (He et al., 2022) to pretrain the plain ViTAE",2,positive
"Furthermore, to explore the effectiveness of unsupervised pretraining, MAE (He et al., 2022) is employed to pretrain the model on the MillionAID dataset (Long et al.",2,positive
"With the pretraining method MAE andRVSA, the detector outperformed all previousmethods, achieving 81.24% and 71.05%mAP onDOTA-V1.",0,negative
"Furthermore, to explore the effectiveness of unsupervised pretraining, MAE (He et al., 2022) is employed to pretrain the model on the MillionAID dataset (Long et al., 2021).",2,positive
"In the first stage, Semi-ViT uses the pre-training model of MAE.",0,negative
"First, it trains a ViT-based encoder fθ(·) on all images in X via self-supervised methods such as MAE [28].",2,positive
"Recent advances in self-supervised learning3 [90, 91, 92, 27, 93, 28] and diffusion probabilistic models [1, 2, 3, 4, 5, 6] achieve excellent performance in the two tasks respectively.",1,neutral
"For feature extraction, we use MAE [7].",2,positive
"In the seminal work of [39], the authors propose a simple framework where an autoencoder is fed with partially masked images, and the accompanying decoder is tasked with reconstructing the original images.",1,neutral
"While the core underlying idea of the MaskedKD can be applied to the case of selfsupervised learning, it is unclear how one can combine it with self-supervision strategies that utilize masking, e.g., masked autoencoders (He et al., 2022).",1,neutral
", 2019), the masked pre-training of ViTs gives significant performance boosts to various downstream supervised tasks (Dosovitskiy et al., 2020; He et al., 2022).",2,positive
"Similar to the masked language modeling methods (Devlin et al., 2019), the masked pre-training of ViTs gives significant performance boosts to various downstream supervised tasks (Dosovitskiy et al., 2020; He et al., 2022).",2,positive
"For instance, many advanced frameworks
firstly were developed for CV tasks, like CPC[118], momentum contrast (MoCo)[119], SimCLR[113], BYOL[114], SwAV[120], MAE[121], Siamese[122], etc. Additionally, BERT[8] still performs poorly in processing NLP-related tasks.",1,neutral
"For instance, many advanced frameworks firstly were developed for CV tasks, like CPC[118], momentum contrast (MoCo)[119], SimCLR[113], BYOL[114], SwAV[120], MAE[121], Siamese[122], etc.",1,neutral
"Z = z1, z2, · · · , zB Q = q1, q2, · · · , qB C = c1, c2, · · · , cK Swapping assignments between multiple views (SwAV)[120] is a cluster assignment-based contrastive learning paradigm.",1,neutral
The ViT models are further improved by pre-training masked auto-encoders on unlabeled images [7].,2,positive
"For pre-training Edge MAE, we tuned the learning rate as 1.5e-4, weight decay as 0.05, the mask ratio as 1/3, and batch size as 2048.",0,negative
"From the two tables, we can see that, Edge Transformer outperforms almost all baseline models, and the results can be further improved by pre-training Edge MAE on unlabeled edges.",2,positive
"Furthermore, we propose a novel Edge Transformer model and pre-train the model via Masked Auto-Encoders (MAE).",2,positive
The decoder layer of Edge MAE is tuned as 1.,1,neutral
The encoder of an Edge MAE is an Edge Transformer but only applied on unmasked tokens.,1,neutral
All four edge tokens are the input to the Edge MAE decoder.,1,neutral
"In this section, we evaluate our Edge Transformer and Edge MAE in two settings.",2,positive
"Following MAE, Edge MAE randomly masks a proportion of input tokens.",1,neutral
"We can see the proposed Edge MAE achieves the best results, and link prediction models perform better as they model the whole return process entirely.",2,positive
"4.2 Parameter Settings For Edge Transformer and Edge MAE, we set the latent vector size 𝐷 as 256, the dropout rate as 0.0, and the number of attention heads as 3.",1,neutral
The reconstruction target in MAE allows the model to learn the prior distribution of node and edge features.,1,neutral
"Inspired by MAE [7] in CV which pre-trains masked auto-encoders on unlabeled images, we propose to improve Edge Transformer with an Edge MAEmodel.",2,positive
"For the MAEViT-Base (He et al., 2022) baseline, we report additional results for λ values 1, 10, and 50.",2,positive
"When η = 1.0, it means the number of features in the baseline is the same as the number of features learned using PISCO and as a result, similar for CIFAR-10 results in §D.2, we observe the in-distribution performance of PISCO in this case being almost the same as that of baseline methods even for higher values of λ.
MAE-ViT-Base results for λ values 1, 10, and 50: Results for additional values of λ when MAE-ViT-Base is the baseline are in Table 19.",2,positive
"We also report analogous results for another popular feature extractor, MAE-ViT-Base (He et al., 2022), in Table 5.",2,positive
"We train two types of auto-encoders (AEs) to reconstruct the images in the collection D, namely denoising AEs [73] and masked AEs [28].",2,positive
We employ Adam [36] and the mean squared error (MSE) to train both AEs.,2,positive
"However, as soon as an image from a different distribution is given as input, AEs exhibit poor reconstruction capabilities.",1,neutral
"First of all, we propose four novel pre-retrieval predictors, namely (i) the magnitude of the reconstruction error of denoising [73] or masked [28] auto-encoders trained on the database, (ii) the density of the k-means cluster to which the query image embedding is assigned, (iii) the confidence distribution of a classification head attached to the embedding layer of the retrieval model, and (iv) the score predicted by a fine-tuned ViT model [20].",2,positive
"Many new SOTA performances are achieved on several downstream CV tasks, including object detection[61], semantic segmentation[62], image processing[63], video understanding[63].",1,neutral
"We compare the soups to a nominal model, the l∞-robust classifier used in the soups, their ensemble, the Masked Autoencoders of [18], AdvProp [54], PyramidAT [24], and the ensemble obtained by averaging the output (after softmax) of the four models included in the soups.",2,positive
"Finally, while the capability of selecting a model specific to each image distribution is a main point of our model soups, we also show that it is possible to jointly select a soup for average performance across several IMAGENET variants to achieve better accuracy than adversarial and self-supervised baselines [18,24].",2,positive
Other alternatives carefully tweak R per dataset and architectures e.g. to only compute the reconstruction loss on parts of the data as with BERT Devlin et al. (2018) or MAEs He et al. (2022).,2,positive
"Transformers have been used for robot control and planning [25, 26, 27], object recognition [28], and robot navigation [29].",1,neutral
"|D<t) where k ∈ [1, . . . ,K] on ImageNet for 4 different architecture and pre-training methods: ResNet50 + SimCLR, ResNet50 + BYOL, ViT/B16 + DINO and ViT/B16 + MAE.",2,positive
"…and self-supervised learning, the network is trained on a surrogate task, such as reconstruction (Hinton & Salakhutdinov, 2006; Kingma & Welling, 2013; He et al., 2021) and contrastive prediction (van den Oord et al., 2018; Chen et al., 2020), which is supposed to capture generic prior of the data.",2,positive
"MAE generalizes well with dataset shift, but the performance on natural datasets suffers compared to other methods.",2,positive
"MAE (He et al., 2021) uses masked reconstruction tasks to learn representation.",1,neutral
"In unsupervised and self-supervised learning, the network is trained on a surrogate task, such as reconstruction (Hinton & Salakhutdinov, 2006; Kingma & Welling, 2013; He et al., 2021) and contrastive prediction (van den Oord et al.",1,neutral
"For ViT/B16 backbone, DINO features outperform both supervised and MAE, while supervised and MAE have no significant difference.",2,positive
"Natural Special Structured
A rc
hi te
ct ur
e
O bj
ec tiv
e
C al
te ch
10 1
C ifa
r1 00
D T D Fl
ow er
s1 02
Pe ts SV H N Su
n3 97
E ur
oS A T Pa tc hC
am el
yo n
R es
is c4 5 R et in
op at hy C le vr C ou nt C le vr D is
ta nc e D M L ab D Sp ri te sL oc at
io n
D Sp
ri te
sO ri
en ta
tio n
K itt
iD is
ta nc e Sm al lN O R B A
zi m
ut h
Sm al
lN O
R B
E le
va tio
n
ResNet50 SUP 1 1 1 1 1 1 1 1 3 1 3 4 3 3 4 2 4 3 2 ResNet101 SUP 1 1 3 1 0 1 1 1 2 1 2 3 3 2 2 2 4 3 2 ResNet152 SUP 1 1 1 2 0 1 1 1 2 1 2 3 4 2 1 2 5 3 7 ResNet50 SimCLR 1 1 1 1 3 2 1 1 4 1 2 7 3 5 3 3 4 7 7 ResNet50 BYOL 1 1 1 1 1 2 1 1 6 1 3 4 5 5 4 3 7 6 7 ResNet101 BYOL 1 1 1 1 1 2 1 1 3 1 3 4 6 5 3 3 6 6 7 ResNet152 BYOL 1 1 1 1 0 2 1 1 3 1 3 5 4 4 2 3 4 6 3 ViT/S16 SUP 1 1 1 1 1 1 0 1 4 1 3 3 2 2 4 5 5 7 4 ViT/B16 SUP 0 1 2 0 1 1 1 1 3 1 3 7 6 2 3 3 4 3 7 ViT/L16 SUP 1 1 2 1 2 0 1 1 3 2 2 7 7 3 1 3 5 4 7 ViT/B16 DINO 0 0 1 0 1 1 0 1 3 1 4 5 5 6 3 3 3 3 7 ViT/S16 MAE 1 1 1 1 2 2 1 1 3 1 1 2 2 6 7 4 7 6 5 ViT/B16 MAE 0 1 1 1 2 1 0 1 2 1 1 3 4 4 4 6 5 7 4 ViT/L16 MAE 1 1 1 1 2 1 1 1 2 1 1 4 4 4 4 5 6 5 6
Readout Models.",0,negative
"This is consistent with previous observations (Resnick et al., 2019; He et al., 2021) and demonstrates the importance of incorporating a multitude of readout methods into the evaluation framework.",2,positive
"Interestingly, the reconstruction-based MAE objective scales well to larger architectures and has not saturated at ViT/L16.",2,positive
"For ResNet-50, we pre-train on supervised SimCLR and BYOL losses; for ViT/B16, we pre-train on supervised (Touvron et al., 2020; He et al., 2021) DINO and MAE losses.",0,negative
"Natural # Special # Structured #
ViT/B16 DINO 1.57 1 1.00 1 2.63 3 ResNet50 BYOL 2.71 2 3.00 2 2.13 1 ViT/B16 MAE 5.71 6 3.25 3 2.38 2 ViT/B16 SUP 2.86 3 3.25 4 5.75 6 ResNet50 SimCLR 4.29 5 5.75 6 3.38 4 ResNet50 SUP 3.86 4 4.75 5 4.75 5
ranks in Table 1b.",0,negative
He et al. (2021) also argue that the use of linear probes limits the development of methods that induce non-linear representations.,1,neutral
"Linear probing, using a linear layer for readout, is the most prevalent evaluation protocol for representations, the performance of which is reported and compared in almost all of the recent representation learning works (van den Oord et al., 2018; Chen et al., 2020; Grill et al., 2020; Li et al., 2021; Caron et al., 2021; He et al., 2021).",1,neutral
"…is the most prevalent evaluation protocol for representations, the performance of which is reported and compared in almost all of the recent representation learning works (van den Oord et al., 2018; Chen et al., 2020; Grill et al., 2020; Li et al., 2021; Caron et al., 2021; He et al., 2021).",2,positive
"Following the command in Masked AutoEncoder [9], we use a lightweight decoder, which has only 10% computation per token compared with the encoder.",2,positive
"After the pre-training & fine-tune method [18] was proposed, more andmore experiments demonstrated its superior performance, so here, we also used the Masked AutoEncoder pre-trained ViT-base model.",2,positive
"As for the structure of our decoder, inspired by Masked AutoEncoder [9], Our decoder is also based on the attention algorithm, which has the same structure as the encoder(ViT-base) but is only 10% of its size.",2,positive
Proposed Masked AutoEncoder.,2,positive
"(2022), Z is obtained by clustering the latent features produced by a Masked Autoencoder (He et al., 2022).",1,neutral
"For example, in Liu et al. (2022), Z is obtained by clustering the latent features produced by a Masked Autoencoder (He et al., 2022).",1,neutral
MAEDAY [68] uses the reconstruction error of a pre-trained masked autoencoder [27] to generate anomaly segmentation masks.,1,neutral
"[27] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"Inspired by [11], a Self-supervised Masking Reconstruction branch (SMR) is designed to impose implicit regularization by providing extra noise-free supervised signal.",1,neutral
"To further improve the robustness of SGLDM to different sketches, we adopt the arbitrarily masking conditional training strategy in training inspired by Masked AutoeEncoder [7], which masks random patches of the input and let the model reconstruct it automatically.",2,positive
"2021; Tomasev et al., 2022; Caron et al., 2021; Chen et al., 2021; Li et al., 2022a; Zhou et al., 2022a;b; HaoChen et al., 2021; He et al., 2022; Bardes et al., 2022), catching up to supervised baselines in tasks requiring high-level information such as classification.",0,negative
Currently two types of popular pretext tasks have been designed and applied for self-supervised learning: masked image modeling (Bao et al. 2022; He et al. 2022) and contrastive learning (Chen et al. 2020; He et al. 2020; Grill et al. 2020).,1,neutral
"2022), we aim to adapt the data-efficient masked image autoencoder (MAE) (He et al. 2022) to multi-modal and small-scale RGB-D datasets for better representation learning.",2,positive
"Driven by the good self-supervised learning performance of MAE (He et al. 2022) in images, we design a multi-modal masked autoencoder (MM-MAE) in RGBD data.",2,positive
"Unlike the original MAE (He et al. 2022), our CoMAE presents a shared encoder and decoder among RGB and depth modalities and acts as a kind of regularizer to guide pre-training.",2,positive
Currently two types of popular pretext tasks have been designed and applied for self-supervised learning: masked image modeling (Bao et al. 2022; He et al. 2022) and contrastive learning (Chen et al.,1,neutral
"competitive and even better results of the same ViT-B model pre-trained by ImageMAE (He et al. 2022), this reveals the data efficiency advantage of our CoMAE pre-training.",2,positive
"Inspired by the results in VideoMAE (Tong et al. 2022), we aim to adapt the data-efficient masked image autoencoder (MAE) (He et al. 2022) to multi-modal and small-scale RGB-D datasets for better representation learning.",2,positive
"In addition, our CoMAE instantiated with ViT-B also achieves
competitive and even better results of the same ViT-B model pre-trained by ImageMAE (He et al. 2022), this reveals the data efficiency advantage of our CoMAE pre-training.",2,positive
", 2021), MAE (He et al., 2022), and ResNet (He et al.",2,positive
"Vision models We include 14 computer vision models (VMs) in our experiments, representing three families of models: SegFormer (Xie et al., 2021), MAE (He et al., 2022), and ResNet (He et al., 2016).",2,positive
"With the rapid growth of the number of large-scale pretrained models [15, 42], we believe our work paves a new way for efficient model development and deployment, yielding a significant step towards Green AI.",2,positive
"Most recently, large-scale self-supervised pretraining has helped ViTs achieve promising results on ImageNet, including contrastive learning [6, 9] and masked image modeling [3, 15, 19, 67].",1,neutral
", BEiT [4] and MAE [3,18]) benefit the excellent representation learning, which improve the finetuning performance in downstream tasks.",1,neutral
We also compare M2A2E with the symmetric multimodal MAE [3] when finetuning on all downstream modality settings.,2,positive
Then random sampling strategy [18] is used to mask out p percentage of the visual tokens in Xi.,1,neutral
"On the other hand, self-supervised masked image modeling (MIM) methods (e.g., BEiT [4] and MAE [3,18]) benefit the excellent representation learning, which improve the finetuning performance in downstream tasks.",1,neutral
"Relation to modality-symmetric autoencoders [3, 18].",1,neutral
"Compared with the vanilla MAE [18], M2A2E adopts the same masked strategy in unimodal ViT encoder but targeting at multimodal reconstruction with multiple unshared ViT decoders.",2,positive
"Different from the conclusions from [3, 18] using very large mask ratio (e.",1,neutral
"Compared with the vanilla MAE [18], M(2)A(2)E adopts the same masked strategy in unimodal ViT encoder but targeting at multimodal reconstruction with multiple unshared ViT decoders.",2,positive
"Besides, M2A2E is similar to the multimodal MAE [3] only when partial tokens from a single modality are visible while masking all tokens from other modalities.",1,neutral
"Despite mature exploration and finds [3,18,44] of ViT on other computer vision communities (e.",1,neutral
"We find that 1) leveraging local feature descriptors benefits the ViT on IR modality; 2) partially finetuning or using adapters can achieve reasonable performance for ViT-based multimodal FAS but still far from satisfaction; and 3) mask autoencoder [3, 18] pre-training cannot provide better finetuning performance compared with ImageNet pre-trained models.",2,positive
"Compared with modality-symmetric autoencoders [3,18], the proposed M(2)A(2)E is able to learn more intrinsic task-aware representation and compatible with modality-agnostic downstream settings.",2,positive
"9 that with more challenging reconstruction target (from masked unimodal inputs to multimodal prediction), M2A2E is outperforms the best settings of multimodal MAE [3] on most modalities (‘RGB’, ‘IR’, ‘RGB+IR’, ‘RGB+Depth’, ‘RGB+IR+Depth’), indicating its excellent downstream modality-agnostic capacity.",2,positive
Comparison between multimodal MAE [3] and M2A2E.,1,neutral
"Following MAE [21], we use the normalized pixel values as targets in the reconstruction task.",2,positive
"Following MAE [21], we only feed the visible, unmasked patches to the online encoder Es.",0,negative
"Here, u and r are random masking, which is similar to the “random sampling” adopted in MAE [21].",1,neutral
"For MIM tasks, we apply augmentations and masking strategy following MAE [21] to the input of the online encoder.",2,positive
"Existing work mainly differ in their regression objectives [2, 14, 21, 48, 50, 53] or masking strategies [27, 30, 43].",1,neutral
"Nowadays, contrastive learning (CL) [3, 11, 40, 57] and masked image modeling (MIM) [2, 21, 53], together with Vision Transformers (ViTs) [16, 32], have revolutionized the field of SSL in computer vision and medical imaging, which achieve the state-of-the-art (SOTA) performance for a variety of tasks [3, 21, 45, 52, 63].",1,neutral
"However, when pretrain and downstream tasks are very different, adapting the features is important and FT outperforms HP (Chen et al., 2020b; Zhai et al., 2019; He et al., 2022).",2,positive
"Recently, self-supervised Masked Autoencoder (MAE) [13] has achieved great success in feature representation and assisted many downstream tasks.",1,neutral
"Since MAE [47] only applied masks in 2D images, while video anomalies are related to the temporal information, TMAE first located video foregrounds and constructed temporal cubes to be masked objects.",1,neutral
Liu et al. [87] proposed an Appearance-Motion united Auto-Encoder (AMAE) framework using two independent auto-encoders to perform denoising and optical flow generation tasks separately.,1,neutral
"Inspired by the Masked Auto-Encoder (MAE) [47], their proposed TMAE learned representations using a visual transformer performing a complementary task.",2,positive
"Meanwhile, several works [16, 38] have demonstrated that pretraining networks to predict masked patches from unmasked patches on a large-scale dataset can enhance the fully-supervised training on another small-scale dataset significantly.",1,neutral
"annotation-free pretext tasks and learning to predict them [25], [26].",1,neutral
"5, which is lower than the best configuration reported in (He et al., 2022).",0,negative
"Interestingly, fixed-region representations, namely CNNFeat and MAEPatch, failed for the comparison tasks while also utilizing a transformer pooling layer.",1,neutral
"MAE
The Multi-modal Auto-Encoder (MAE) encoder architecture is based on the Vision Transformer Base (ViT-Base) architecture (Dosovitskiy et al., 2020).",2,positive
", 2018) for end-to-end learned fixedregion representations and the mini-patch representations from pre-trained Masked AutoEncoder (MAE) (He et al., 2022) for pre-trained fixed-region representations.",1,neutral
"The masking ratio used during pre-training was set to 0.5, which is lower than the best configuration reported in (He et al., 2022).",0,negative
"For our experiments, we use the CNN feature map (Santoro et al., 2017; Zambaldi et al., 2018) for end-to-end learned fixedregion representations and the mini-patch representations from pre-trained Masked AutoEncoder (MAE) (He et al., 2022) for pre-trained fixed-region representations.",2,positive
"The pre-training of the MAE encoder was conducted using a batch size of 128, a learning rate of 0.001, and a weight decay of 0.05.",0,negative
"Recently, [41] and [18] show that a masked image modeling task of simply regressing directly on the image pixels is sufficient and effective.",1,neutral
"Masked image modeling (MIM) has been proposed in various forms in recent years, and has recently been found to be particularly effective in the natural image domain, surpassing many contrastive works and being shown to be friendlier to downstream optimization [41, 18, 44, 3, 40] In general, the goal is to learn from data in a self-supervised manner by asking the model to generate pixel values for intentionally-withheld regions in an image.",1,neutral
"On the other hand, MIM objectives like [41, 18] rely only on simple spatial augmentations such as flipping and cropping.",1,neutral
"In this work, we develop our pretraining objective based on a masked image modeling approach like [41, 18].",2,positive
MIM was first proposed in natural language processing [2].,1,neutral
"However, MIM on a single ViT significantly improves its AUROC to 98.30% (2.07% higher), Model Fine-tuned AUROC(",2,positive
We perform OOD detection with MIM pretext task with each metric – the results are shown in Tab.,2,positive
"For one-class OOD detection, we pre-train the MIM model and finely tune it on ImageNet-21k [49], as recommended by BEiT [2].",2,positive
"Specifically, we adopt the masked image modeling (MIM) [2, 11, 20] as our self-supervised pretext task, which has been demonstrated to have great potential in both natural language processing [11] and computer vision [2,20].",2,positive
"In our method, we pre-train the model with Masked Image Modeling (MIM) pretext [11] on a large dataset and fine-tune it on the ID dataset.",2,positive
"Specifically, we adopt the masked image modeling (MIM) [2,11,20] as our self-supervised pretext task, which has been demonstrated to have great potential in both natu-ral language processing [11] and computer vision [2,20].",2,positive
We compare the performance of MIM and contrastive learning pretext task MoCov3 [8] in Tab.,2,positive
"In the MIM task, we split images into patches and randomly mask a proportion of image patches before feeding the corrupted input to the vision transformer.",2,positive
"For multi-class OOD detection, we pre-train the MIM model, intermediately use fine-tuning on ImageNet-21k [49], and apply fine-tuning again on the ID dataset.",2,positive
"Transformer has achieved promising performance in computer vision [2, 20] and natural language processing [11].",2,positive
"Follow-up research [11, 20] transfers the similar idea from natural language processing to computer vision, masking different proportions of the image patches to recover results.",1,neutral
The self-supervised pretext task in our framework is Masked Image Modeling (MIM).,2,positive
It shows that the performance of MIM is much increased by 13.3% to 98.66%.,0,negative
"Specifically, masked autoencoders are a form of more general denoising autoencoders [32, 79], which adopt a simple concept to remove a proportion of the data and then learn to recover the removed parts.",1,neutral
"Inspired by the tremendous success of the masked autoencoding paradigm in NLP [19] and Computer Vision [32], we propose to incorporate a new masked autoencoder model into our generalized framework.",2,positive
"paradigm in NLP [19] and Computer Vision [32], we propose to incorporate a new masked autoencoder model into our generalized framework.",2,positive
"Notably, whenγ = 1, our masked autoencoder is equivalent to MAE for vision [32].",1,neutral
"Parameter-efficient finetuning techniques (Houlsby et al., 2019; Hu et al., 2022; Lester et al., 2021; Li & Liang, 2021; He et al., 2022a; Ben Zaken et al., 2022; Sung et al., 2021; Qing et al., 2022) are first proposed in NLP since full finetuning the increasingly larger language models for…",2,positive
"To alleviate the labeling cost, self-supervised learning methods (Chen et al., 2021; Bao et al., 2021; Zhou et al., 2021; He et al., 2022b; Xie et al., 2022) are introduced to learn effective representations from unlabeled data.",2,positive
"We also hope to incorporate selfsupervised methods that learn representations by training on a union of unlabeled data from source and target via proxy tasks like reconstruction (Gidaris et al., 2018; He et al., 2022) and contrastive learning (Caron et al.",2,positive
"We also hope to incorporate self-supervised methods that learn representations by training on a union of unlabeled data from source and target via proxy tasks like reconstruction (Gidaris et al., 2018; He et al., 2022) and contrastive learning (Caron et al., 2020; Chen et al., 2020).",2,positive
"The lack of usability explains why generative encoders such as MAE do not give a good linear probing performance, despite their strong fine-tuning performance (He et al., 2022).",2,positive
"SSL pipelines differ in many design choices, such as the objective (Chen et al., 2020a; He et al., 2022), architecture (Caron et al.",2,positive
"Pretrained MAE and CLIP with world model (MAE+WM, CLIP+WM)
Masked autoencoder (MAE; He et al. 2021) learns visual representation in a self-supervised manner by training a vision Transformer (ViT; Dosovitskiy et al. 2021) to reconstruct masked patches.",2,positive
", 2021) and MAE (He et al., 2021) representations as they have recently shown to be effective for robotic manipulation (Shridhar et al.",1,neutral
"Convolutional feature embedding Unlike prior work that masks random pixel patches (He et al., 2021), we embed camera observations into convolutional feature maps and mask these features following the design of Seo et al. (2022a).",2,positive
", 2022a) that masks convolutional features instead of pixel patches (He et al., 2021) and predicts rewards to learn representations capturing fine-grained details required for visual control.",2,positive
"Specifically, we consider CLIP (Radford et al., 2021) and MAE (He et al., 2021) representations as they have recently shown to be effective for robotic manipulation (Shridhar et al., 2021; Radosavovic et al., 2022).",2,positive
"We hypothesize this is because spatial information redundancy (He et al., 2021) is more significant in visual observations from
manipulation tasks than natural images.",1,neutral
"We hypothesize this is because spatial information redundancy (He et al., 2021) is more significant in visual observations from manipulation tasks than natural images.",1,neutral
"We also incorporate the idea of a prior work (Seo et al., 2022a) that masks convolutional features instead of pixel patches (He et al., 2021) and predicts rewards to learn representations capturing fine-grained details required for visual control.",2,positive
"Convolutional feature embedding Unlike prior work that masks random pixel patches (He et al., 2021), we embed camera observations into convolutional feature maps and mask these features following the design of Seo et al.",2,positive
"R O
] 3
1 M
ay 2
02 3
experiments on RLBench (James et al., 2020) show that our method outperforms single-view representation learning baselines (Radford et al., 2021; He et al., 2021; Seo et al., 2022a) and a multi-view representation learning baseline (Sermanet et al., 2018).",2,positive
", 2020) show that our method outperforms single-view representation learning baselines (Radford et al., 2021; He et al., 2021; Seo et al., 2022a) and a multi-view representation learning baseline (Sermanet et al.",2,positive
"…as computer vision (Ren et al., 2015; He et al., 2017; Chen et al., 2020a; He et al., 2020; Chen et al., 2020b; Chen & He, 2021; Grill et al., 2020; He et al., 2022), natural language processing (Devlin et al., 2019; Brown et al., 2020; Raffel et al., 2020) and cross-modal learning (Radford et…",1,neutral
"…compare logistic regression and naïve Bayes on the CIFAR10 and CIFAR100 datasets in various models, which are trained on image-label pairs (Dosovitskiy et al., 2021; He et al., 2016), image-text pairs (Radford et al., 2021), or pure images (Chen et al., 2020d;c; He et al., 2022; Xie et al., 2022).",1,neutral
"We then perform extensive experiments on CIFAR10 and CIFAR100 datasets with various representative pre-trained vision models (He et al., 2016; Dosovitskiy et al., 2021; Chen et al., 2020d;c; Radford et al., 2021; Xie et al., 2022; He et al., 2022), which are trained in supervised or self-supervised manners.",2,positive
"Deep representation learning has achieved great success in many fields such as computer vision (Ren et al., 2015; He et al., 2017; Chen et al., 2020a; He et al., 2020; Chen et al., 2020b; Chen & He, 2021; Grill et al., 2020; He et al., 2022), natural language processing (Devlin et al.",1,neutral
"1, we empirically analyze the main assumptions of our theory in various deep vision models (Dosovitskiy et al., 2021; He et al., 2016; Radford et al., 2021; Chen et al., 2020d;c; He et al., 2022; Xie et al., 2022).",2,positive
"It has made remarkable progress in various machine learning fields (Ren et al., 2015; He et al., 2017; Chen et al., 2020a; He et al., 2020; Chen et al., 2020b; Chen & He, 2021; Grill et al., 2020; He et al., 2022; Xie et al., 2022; Devlin et al., 2019; Brown et al., 2020; Raffel et al., 2020).",2,positive
"com/facebookresearch/mae (He et al., 2022) License https://github.",2,positive
"We adopt pre-trained checkpoint in (He et al., 2022).",2,positive
"…extensive experiments on CIFAR10 and CIFAR100 datasets with various representative pre-trained vision models (He et al., 2016; Dosovitskiy et al., 2021; Chen et al., 2020d;c; Radford et al., 2021; Xie et al., 2022; He et al., 2022), which are trained in supervised or self-supervised manners.",2,positive
", 2022) extends MAE (He et al., 2022) by reconstructing masked point clouds.",2,positive
"In 3D, PointMAE (Pang et al., 2022) extends MAE (He et al., 2022) by reconstructing masked point clouds.",2,positive
"(3) (4)Note that for methods like MAE (He et al., 2022; Pang et al., 2022), the teacher network is an identity mapping with no parameters, and therefore φ is an empty set.",1,neutral
"Ouyang et al., 2022), 2D machine vision (He et al., 2020; 2022), and both (vision-language, VL) (Radford et al., 2021; Rombach et al., 2022; Alayrac et al., 2022).",2,positive
"(3) 4Note that for methods like MAE (He et al., 2022; Pang et al., 2022), the teacher network is an identity mapping with no parameters, and therefore φ is an empty set.",1,neutral
"Generative Masked Representation Learning has emerged as another paradigm of self-supervised learning from NLP (Devlin et al., 2019) to Vision (He et al., 2022).",1,neutral
"It can be `2 distance (He et al., 2022), crossentropy (Devlin et al.",1,neutral
"It can be `2 distance (He et al., 2022), crossentropy (Devlin et al., 2019; Bao et al., 2022), or Chamfer-Distance (Fan et al., 2017; Pang et al., 2022).",1,neutral
He et al. (2022) propose masked autoencoder (MAE) to reconstruct RGB pixels.,2,positive
"The masked ratio can be even higher than the original configuration described in MAE (He et al., 2022) (e.",1,neutral
"Masked AutoEncoder Masked AutoEncoder (He et al., 2022) is the dominant approach in visual pre-training, surpassing the performance of contrastive learning with less computational requirements.",1,neutral
", 2022), (He et al., 2022)) has become another main paradigm for learning self-supervised vision representations.",1,neutral
"MAE (He et al., 2022) (see Figure 2) and SimMIM (Xie et al.",1,neutral
", 2021) and MAE (He et al., 2022), we adopted AdamW (Loshchilov & Hutter, 2017) for pre-training, fine-tuning.",2,positive
", 2020)(He et al., 2022) has shown impressive potential in various vision tasks and applications, owing to increasingly available data and advancing hardware.",2,positive
"According to the setup in MoCo v3 and MAE (He et al., 2022), both adopted a batch size of 4,096.",0,negative
"To address the representation deficiency issue in MLM, we propose a simple framework MAE-LM, which pretrains bidirectional Transformer encoders using the MLM objective, but based on the Masked Autoencoder (He et al., 2022) structure.",2,positive
"To address the representation deficiency issue, we propose a simple text encoder pretraining method, MAE-LM, which conducts MLM pretraining based on the Masked Autoencoder architecture (He et al., 2022).",2,positive
"(2021a); Ho et al. (2020) with ↓ (x) = √γx + √ 1− γε, with ε ∼ N (0, I) and γ uniformly sampled as γ ∼ U(0, 1). As shown in the quantitative results in Table 2 and visualizations in Figure 3, with proper degrees of corruption, restoring the original images may require the network to infer the general content given the corrupted pixels, and recover the details using the knowledge learned from the true samples and stored in the network weights. For example, in the image colorization experiments, the pretrained vision model learns the common colors of different objects from the massive unlabeled data in a self-supervised way. As visualized in Figure 3, the vision model learns common knowledge such as stop signs are usually red, and the background of a horse is usually green while manatees are marine mammals therefore the background is usually blue. Summarizing such knowledge requires the vision models to learn identifying objects first, therefore transferable network weights and feature representations can be obtained from pretraining. And as shown in the ‘Denoising’ row of Figure 3, with strong noise injected to the input, the model is able to recover objects that are almost invisible. This finding potentially connects our pretraining method with genitive models Song et al. (2021a); Ho et al. (2020); Song & Ermon (2019); Song et al. (2021b). We will further investigate the connections in future efforts.",0,negative
"AE + SR 16 denotes a baseline experiment with a auto-encoder architecture as in He
et al. (2022).",2,positive
The remarkable success of exploiting context information resides in the massive unlabeled data in natural language processing (NLP) stimulates the recent progress of self-supervised vision model through masked image modeling (MIM) He et al. (2022); Wei et al. (2021); Xie et al. (2022).,1,neutral
"In Table 3, we compare our method against DINO Caron et al. (2021), MoCo-V3 Chen et al. (2021), MaskFeat Wei et al. (2021), BEiT Bao et al. (2021), iBOT Zhou et al. (2022), and MAE He et al. (2022).",2,positive
"Note that in the Gridded (16) experiments, the patch partition in the image masking matches exactly with the patch partition in the ViT networks, therefore it is a fair comparison against MAE He et al. (2022).",2,positive
Recent self-supervised vision model pretraining methods Xie et al. (2022); He et al. (2022); Wei et al. (2021) invariably adopt masked image modeling as the pretext task.,1,neutral
The proposed method can achieve better results with fewer epochs of training compared to MAE He et al. (2022).,2,positive
"This phenomena is also observed and discussed in He et al. (2022), and may be attributed to the fact that both He et al. (2022) and our method do not explicitly encourage linear separation of features in the pretraining stage as the contrastive learning based method do.",1,neutral
"Unlike recent methods Zhou et al. (2022); He et al. (2022), we do not perform exhaustive searches for the optimal hyperparameters such as learning rates.",1,neutral
Most of the experimental settings follow He et al. (2022).,2,positive
"Different from models like MAE He et al. (2022) and SimMIM Xie et al. (2022) that are specifically tailored for particular network architectures, our framework can be seamlessly applied to any deep vision models without any customization or auxiliary network components beside the simple linear head h.",2,positive
"Different from models like MAE He et al. (2022) and SimMIM Xie et al. (2022) that are specifically tailored for particular network architectures, our framework can be seamlessly applied to any deep vision models without any customization or auxiliary network components beside the simple linear head…",2,positive
The result of MAE He et al. (2022) with 400 epochs is based on our reimplementation.,2,positive
"(2021a); Ho et al. (2020) with ↓ (x) = √γx + √ 1− γε, with ε ∼ N (0, I) and γ uniformly sampled as γ ∼ U(0, 1).",1,neutral
"We follow the details presented in MAE He et al. (2022) and implement an asymmetric
Methods GPUs × H Acc.",2,positive
Masked auto-encoder He et al. (2022) adopts an asymmetric encoder-decoder architecture and shows that scalable vision learners can be obtained simply by reconstructing the missing pixels.,2,positive
"(2021a); Ho et al. (2020) with ↓ (x) = √γx + √ 1− γε, with ε ∼ N (0, I) and γ uniformly sampled as γ ∼ U(0, 1). As shown in the quantitative results in Table 2 and visualizations in Figure 3, with proper degrees of corruption, restoring the original images may require the network to infer the general content given the corrupted pixels, and recover the details using the knowledge learned from the true samples and stored in the network weights. For example, in the image colorization experiments, the pretrained vision model learns the common colors of different objects from the massive unlabeled data in a self-supervised way. As visualized in Figure 3, the vision model learns common knowledge such as stop signs are usually red, and the background of a horse is usually green while manatees are marine mammals therefore the background is usually blue. Summarizing such knowledge requires the vision models to learn identifying objects first, therefore transferable network weights and feature representations can be obtained from pretraining. And as shown in the ‘Denoising’ row of Figure 3, with strong noise injected to the input, the model is able to recover objects that are almost invisible. This finding potentially connects our pretraining method with genitive models Song et al. (2021a); Ho et al. (2020); Song & Ermon (2019); Song et al.",2,positive
"For MIM, representative work [He et al., 2022] shows that in vision, removing the masked image patches before the encoder demonstrates stronger performance and 3× or more lower overall pre-training time and memory consumption than keeping the masked tokens.",2,positive
"For MIM, representative work [32] shows that in vision, removing the masked image patches before the encoder demonstrates stronger performance and 3× or more lower overall pre-training time and memory consumption than keeping the masked tokens.",1,neutral
"Token masking is a dominant approach in self-supervised pre-training tasks, such as masked language modeling (MLM) [43, 9] and masked image modeling (MIM) [6, 32].",1,neutral
"With the prevailing of large pretrained Transformer-based models, such as BERT [9] and MAE [32], the pretraining and fine-tuning paradigm has yielded strong empirical performance on various downstream tasks in NLP and CV.",1,neutral
"Token masking is a dominant approach in self-supervised pre-training tasks, such as masked language modeling (MLM) [Kenton and Toutanova, 2019; Brown et al., 2020] and masked image modeling (MIM) [Bao et al., 2022; He et al., 2022].",1,neutral
"Since squeezing the sequence length reduces both the computational and memory complexity quadratically, skipping processing the masked tokens brings considerable training efficiency gain for MLM and MIM.",1,neutral
"While BEiT proposes to predict discrete tokens based on a pretrained image tokenizer, MAE [10] and SimMIM [24] show that simple target like `1 or `2 loss on pixels is effective enough.",1,neutral
"supervised learning in the last few years, but recently, with the introduction of strong and scalable Transformer-based vision models [9, 14, 15], masked image modeling (MIM) has been developed rapidly and became the new dominant paradigm for visual feature pretraining [6, 10, 17, 24].",1,neutral
", contrastive learning [5] is shown to be effective for training ViTs without label, now it is a common belief that the generative method MIM is the most promising framework for ViTs’ self-supervised pretraining [10, 14, 17, 24].",1,neutral
"To show the effectiveness of noisy image modeling in visual feature learning and adversarial defense, we adopt two simple, representative MIM methods, SimMIM [24] and MAE [10] for comparison.",1,neutral
"For example, for masking strategy, [10] has shown that up to 75% of patches can be masked in order to learn rich representations, while [24] mask 60% of patches with a larger mask patch size — 32 pixels instead of 16 pixels for model’s patch size, and an earlier work BEiT [2] adopts a less mask ratio.",1,neutral
"It is also notable that TST (2021) outperforms all the contrastive-based baselines, where TST directly adopts the
vanilla masking protocol presented by He et al. (2022) into time series.",2,positive
"Elaborative manually-designed self-supervised tasks are presented, which can be roughly categorized into contrastive learning (He et al., 2020; Chen et al., 2020) and masked modeling (Devlin et al., 2018; He et al., 2022).",1,neutral
"…progress in natural language processing (NLP) (Brown et al., 2020; Devlin et al., 2018; Gao et al., 2020; Radford et al., 2019) and computer vision (CV) (He et al., 2022; Li et al., 2022; Radford et al., 2021) to alleviate the problem of overreliance on large-scale labeled data in deep learning.",1,neutral
", 2019) and computer vision (CV) (He et al., 2022; Li et al., 2022; Radford et al., 2021) to alleviate the problem of overreliance on large-scale labeled data in deep learning.",2,positive
", 2020) and masked image modeling (MIM) (He et al., 2022; Xie et al., 2022b; Li et al., 2022).",2,positive
"Especially, as a well-recognized pre-training paradigm, masked modeling has achieved great successes in many areas, such as masked language modeling (MLM) (Devlin et al., 2018; Radford et al., 2019; Raffel et al., 2020; Brown et al., 2020; Gao et al., 2020) and masked image modeling (MIM) (He et al., 2022; Xie et al., 2022b; Li et al., 2022).",1,neutral
"…paradigm, masked modeling has achieved great successes in many areas, such as masked language modeling (MLM) (Devlin et al., 2018; Radford et al., 2019; Raffel et al., 2020; Brown et al., 2020; Gao et al., 2020) and masked image modeling (MIM) (He et al., 2022; Xie et al., 2022b; Li et al., 2022).",1,neutral
", 2018) and masked patches of an image (He et al., 2022; Xie et al., 2022b) respectively.",0,negative
"This paradigm has been widely explored in computer vision and natural language processing, which is to predict the masked words of a sentence (Devlin et al., 2018) and masked patches of an image (He et al., 2022; Xie et al., 2022b) respectively.",1,neutral
"Bert-type models [1,10,21,27,29,38] and denoising auto-encoding based approaches [13, 32, 45] are good examples for this strategy.",1,neutral
"In our study, the random sampling masking ratio is 75% [37].",2,positive
The powerful global modeling capability of Transformer [37] enables it to utilize a small set of patches to repair the image.,2,positive
"Inspired by the powerful global modeling ability of Transformer [37], our network can utilize the information in a small number of image patches to repair an image.",2,positive
"Referring to the research about the decoder in pretraining in MAE [37], combined with the task of our work, we design an extremely lightweight encoder.",2,positive
Masked Auto-encoders (MAE) [31] are self-supervised learning approaches based on an asymmetric encoder-decoder architecture.,1,neutral
"Going into details, the solution proposed in [31] is based on an asymmetric encoder-decoder design where the encoder takes in input a subset of the image patches ignoring the masked ones.",2,positive
"On the other hand, two transformer-based architectures have been implemented: self-attention learners called Masked AutoEncoders (MAE) [31], which are able to automatically highlight relevant regions in brain images, and data-efficient image transformers (DeiT) [32,33], which use a renewed training procedure and require far fewer data and computational resources to build a powerful image classification model.",1,neutral
"Beyond ViT base implementation, some improvements have been recently proposed and two of the most promising approaches are the Masked Auto-Encoders (MAE) [31] and Data-efficient image Transformers (DeiT) [32].",2,positive
"The ViT architectures used the training hyperparameters and image augmentation strategies based on the cutmix and mix up approaches as described in the original paper implementation of [31,32].",2,positive
"Inspired by the success of the Transformer-based encoder presented in [31] for image restoration in terms of masked autoencoders, to improve anomaly detection performance, this article uses an inpainting subnetwork based on the Swin Transformer to restructure the masked anomaly image to an anomaly-free image.",2,positive
Recent dominant masked autoencoders for CV [12] and NLP [11] frameworks suggest that self-supervised learning should consider two tasks of the optimization process: reconstructing the masked input and forecasting whether a given sequence is the next sequence.,1,neutral
"Application-wise, masked autoencoder, a more general form, has achieved tremendous successes in both natural language processing (NLP) [11] and computer vision (CV) [12] domains.",1,neutral
"However, recent studies like MAE show promise of reconstruction-based objectives over contrastive-based counterparts, which we believe is worthy of exploration [He et al., 2022].",2,positive
"Because the reconstruction part appears only in pretraining and is not used in the sequence-based inference, we follow He et al. (2022) and use a lightweight decoder with fewer channels in Revolution.",2,positive
Mask mechanism has widely used in computer vision [26] and natural language processing [27].,1,neutral
"Later, the MAE algorithm based on the scalable self supervised learning device [19] proposed by he Kaiming and other large model designs in the CV field have been successively launched, and the architectural rudiments of the application of large models in the CV field have gradually emerged.",1,neutral
"At present, the design of self-supervised learning task is mainly image enhancement and restoration, that is, input the image processed by rotation and cutting, and then restore it [8]; Suppression or mask prediction task similar to natural language processing, randomly mask the image, and then make the model reconstruction realize self-supervised learning [9].",1,neutral
"On the other hand, visual self-supervision (Caron et al. (2021); He et al. (2022); Chen et al. (2020a); Zhou et al. (2021b)) has been widely used for visual pre-training.",1,neutral
Visual self-supervision is mainly divided into generative (He et al. (2022); Bao et al. (2021)) and contrastive (He et al. (2020a); Caron et al. (2021); Chen et al. (2020a)).,2,positive
"(2021b); Kim et al. (2021)) tasks, including textsupervised semantic segmentation (Xu et al.",1,neutral
It includes two categories: reconstructing masked images (He et al. (2022); Zhou et al. (2021b)) and multicrop image contrast (Caron et al. (2021); Chen et al. (2020a)).,2,positive
"Inspired by the work of Masked Autoencoder [20], our transformer can be divided into an encoder part and a decoder
1 3
part.",2,positive
"As mentioned in [20], a wellperformance transformer requires huge amounts of labeled training data.",1,neutral
"Inspired by the work of Masked Autoencoder [20], our transformer can be divided into an encoder part and a decoder Fig.",2,positive
"Recently, spectrograms (akin to 1-channel 2D images) autoencoder (Gong et al., 2022; He et al., 2022) with reconstruction objective as self-supervision have demonstrated the effectiveness of heterogeneous image-to-audio transfer, advancing the field of speech and audio processing on a variety of…",1,neutral
"(2022) study the Masked Autoencoders (MAE) (He et al., 2022) to self-supervised representation learning from audio spectrograms.",1,neutral
"Different from modeling fine-grain details of the signal, the usage of high-level self-supervised learning (SSL) (Baevski et al., 2020; Hsu et al., 2021; He et al., 2022) has been shown to effectively reduce the sampling space of generative algorithms.",1,neutral
"Among these approaches, Xu et al. (2022) study the Masked Autoencoders (MAE) (He et al., 2022) to self-supervised representation learning from audio spectrograms.",1,neutral
"Recently, spectrograms (akin to 1-channel 2D images) autoencoder (Gong et al., 2022; He et al., 2022) with reconstruction objective as self-supervision have demonstrated the effectiveness of heterogeneous image-to-audio transfer, advancing the field of speech and audio processing on
a variety of…",1,neutral
"Inspired by these, we inherit the recent success of spectrogram SSL in the frequency domain, which guarantees efficient compression and high-level semantic understanding.",2,positive
"Recently, spectrograms (akin to 1-channel 2D images) autoencoder (Gong et al., 2022; He et al., 2022) with reconstruction objective as self-supervision have demonstrated the effectiveness of heterogeneous image-to-audio transfer, advancing the field of speech and audio processing on a variety of downstream tasks.",1,neutral
"Following He et al. (2022), we split low-resolution radiograph into nonoverlapping image patches, where 75% patches are randomly masked.",2,positive
"In Figure 2, we provide relation maps of three samples from ImageNet, using 10 checkpoints of MAE-Large [13].",2,positive
Figure 2: Data samples and their labels from ImageNet and the corresponding relation maps by an MAE-Large model [13].,1,neutral
"When training the MAE-Large model [13] on ImageNet with 8% label noise, the validation top1-accuracy decreases by 1.",2,positive
2 Results and analysis ImageNet We measure the label error detection performance on ImageNet with the synthetic label noise by training an MAE-Large model [13].,2,positive
Figure 3: A relation graph with samples from ImageNet and the MAE-Large model [13].,1,neutral
"We measure the detection performance with MAE-Large [13], BEIT-Large [1], and ConvNeXt-Large [29] models.",2,positive
"1 Robustness to unseen augmentations The augmentation invariance guides the semantic features extraction in SSL [28, 29, 11], and demonstrated in the successes of non-contrastive models [3, 4, 5].",1,neutral
"It has been shown that the projection head can significantly improve the performance of SSL methods [1]; thus, the projection head design has been widely adopted in diverse SSL models [1, 2, 13, 14, 3, 15, 4, 5].",1,neutral
"With the rapid progress in the SSL models [1, 2, 3, 4, 5], a simple classifier learned from the pre-trained representations can achieve comparable performance to direct supervised learning.",1,neutral
"This projection head design is widely adopted by the later proposed methods, including MoCo-V2 [2, 13], BYOL [3], SwAV [14], Barlow Twins [15], SimSiam [4], MAE [5] where the architectures are displayed in Figure 3.",2,positive
"The algorithms include MoCo-v2 (He et al., 2020), MoCo-v3 (Chen et al., 2021a), InstDisc (Wu et al., 2018), BYOL (Grill et al., 2020), SwAV (Caron et al., 2020), OBoW (Gidaris et al., 2021), SimSiam (Chen & He, 2021), Barlow Twins (Zbontar et al., 2021), DINO (Caron et al., 2021), MAE (He et al., 2022), iBOT (Zhou et al., 2022) and EsViT (Li et al., 2022a).",2,positive
"This observation is in contrast to the finetuning procedure in transfer learning literature (Kornblith et al., 2019; He et al., 2022) where the learning rate is shared across the whole network; (2) the optimal hyperparameters are quite different for different ways, shots, and test datasets.",1,neutral
"…et al., 2021a), InstDisc (Wu et al., 2018), BYOL (Grill et al., 2020), SwAV (Caron et al., 2020), OBoW (Gidaris et al., 2021), SimSiam (Chen & He, 2021), Barlow Twins (Zbontar et al., 2021), DINO (Caron et al., 2021), MAE (He et al., 2022), iBOT (Zhou et al., 2022) and EsViT (Li et al., 2022a).",2,positive
"…for analyzing SSL models, contrastive learning methods MoCo v3 (Chen et al., 2021), DINO (Caron et al., 2021); masked image modeling (MIM) methods BEiT (Bao et al., 2021), MAE (He et al., 2021), and CAE (Chen et al., 2022a); and iBOT (Zhou et al., 2021) that combines contrastive learning and MIM.",2,positive
"For ADE20K, the input size is set to 512×512 following previous works (Bao et al., 2021; He et al., 2021; Chen et al., 2022a; Zhou et al., 2021).",2,positive
"For all the models involved in the experiments including DeiT (Touvron et al., 2020), MoCo v3 (Chen et al., 2021), DINO (Caron et al., 2021), BEiT (Bao et al., 2021), MAE (He et al., 2021), CAE (Chen et al., 2022a), and iBOT (Zhou et al., 2021), we use their official code to implement the encoders.",2,positive
", 2021) and MAE (Masked Autoencoder) (He et al., 2021) do not have an explicit process to predict the encoded representations of masked patches, instead, directly reconstruct the targets.",2,positive
"MAE (He et al., 2021) learns rich hidden information by directly performing masked image reconstruction in RGB color space using ViT while SimMIM (Xie et al., 2021b) uses Swin-transformer (Liu et al., 2021).",1,neutral
", 2021), or normalized RGB values used in MAE (He et al., 2021).",1,neutral
"…self-supervised representation pretraining methods, including MoCo v3 (Chen et al., 2021), DINO (Caron et al., 2021), CAE (Chen et al., 2022a), MAE (He et al., 2021), BEiT (Bao et al., 2021), and iBOT (Zhou et al., 2021), on object-level recognition (image classification and object segmentation)…",2,positive
"…reconstruct the targets.
methods:
`MIM(Decoder(Regressor(Encoder(Rv))),Target(Rm)), (3) where Target(Rm) is a function to map the masked patches to the targets, e.g., d-VAE (Ramesh et al., 2021) token used in CAE and BeiT (Bao et al., 2021), or normalized RGB values used in MAE (He et al., 2021).",2,positive
"…patches, which is commonly used in masked image modeling (MIM)
4Some MIM methods, such as BEiT (Bao et al., 2021) and MAE (Masked Autoencoder) (He et al., 2021) do not have an explicit process to predict the encoded representations of masked patches, instead, directly reconstruct the…",1,neutral
", 2021), MAE (He et al., 2021), and CAE (Chen et al.",1,neutral
"MAE (He et al., 2021) learns rich hidden information by directly performing masked image reconstruction in RGB color space using ViT while SimMIM (Xie et al.",1,neutral
"Recently, there has been a surge in reconstruction-based self-supervised pretraining methods with the introduction of MSN (Assran et al., 2022b), and MAE (He et al., 2021).",2,positive
"Similar to the conclusion obtained by the MAE(He et al. 2022), the optimal ratios are relatively high, and the accuracy increases steadily with the masking ratio growing until reaching 75%, which produces the best tracking results.",2,positive
"MAE (He et al. 2022) develops an asymmetric encoder-decoder architecture, the encoder operates on a small proportion of the visible patches, and the decoder reconstructs the original pixels.",2,positive
"The main tracker only consists of a ViT backbone and a box estimation head, we test both ViTBase and ViT-Large, and the ViT parameters are initialized with MAE (He et al. 2022) pre-trained model.",2,positive
"Our approach is inspired by the previous MIM method (Xie et al. 2022; He et al. 2022), but we have to deal with two fundamental problems in the tracking framework: (1) Visual tracking is a downstream vision task that generally does not have the pre-train process to apply the MIM strategy.",2,positive
"Recently, MIM approaches((Chen et al. 2020; He et al. 2022; Xie et al. 2022; Wei et al. 2022; Bao, Dong, and Wei 2021)) are extended to the modern vision transformers (Dosovitskiy et al. 2021; Liu et al. 2021). iGPT (Chen et al. 2020) first proposes a transformer to predict unknown pixels from a…",2,positive
"The correlative masked decoder, which is inspired by Masked Image Modeling (He et al. 2022; Xie et al. 2022), reconstructs the both original template and search pixels from the corresponding masked tokens, to guide the encoder to capture the invariant feature for tracking.",2,positive
"Recently, MIM approaches((Chen et al. 2020; He et al. 2022; Xie et al. 2022; Wei et al. 2022; Bao, Dong, and Wei 2021)) are extended to the modern vision transformers (Dosovitskiy et al.",2,positive
"Sparse training is critical and widely used in many fields of deep learning, such as MAE [36], pruning [41] and supernet training [18, 19].",1,neutral
"Dynamic sparsity is also widely used in the training strategies(sparse training), such as MAE [36] in vision, UniLMv2 [14] in NLP, CogView2 [27] and FLIP [43] in multi-modal, DropConnect [57] in robustness training, Oncefor-All [18] and Autoformer [19] in supernet training.",2,positive
Sparse training algorithms dynamically drop tokens/image pixels for better accuracy and efficiency [36].,1,neutral
"When a co-registered histology image is available, POLARIS first employs MAE [32] to extract features from the image tile of each spot and the image tile of its neighborhood.",2,positive
We only use the pretrained encoder part to extract the image features [32].,2,positive
"Several popular pre-trained models, such as convolutional neural networks, stacked sparse autoencoders, and masked autoencoders (MAE), have been employed as a first step to reduce image dimensions and demonstrate advantages in many applications [9, 10, 12, 13, 15].",1,neutral
"Also, it proposes a self-supervised pretraining strategy for point data inspired by the masked token modeling approaches in the RGB image [22] and text [12] domains.",1,neutral
"It means that our RangeViT approach, by being able to use off-the-shelf pre-trained ViT models, can directly benefit from current and future advances on the training ViT models with natural RGB images, a very active and rapidly growing research field [22, 39, 46, 47].",2,positive
"Note, we have not demonstrated it here, but Zorro can also be trained using unimodal self-supervised methods such as MAE [25] and DINO [12] separately on the audio and visual streams.",0,negative
"While approaches based on reconstruction [3, 20] have had a resurgence in the last years, the current state of the art in transfer linear probe is still held by joint-embedding approaches such as DINO [9] or iBOT [46].",1,neutral
"Recently, self-supervised learning using autoencoders for computer vision tasks has also achieved great success [8].",1,neutral
"As a result, researchers have divided images into patches and treated each patch as the smallest unit, as seen in works such as [7, 8].",1,neutral
"As a benchmark model often used in natural language, BERT (Devlin et al., 2019) uses a masking ratio of 15% while MAE uses a ratio of 75% for images (He et al., 2021) and 90% for videos (Feichtenhofer et al., 2022).",2,positive
"The optimal ratios are around 75%, which is in contrast to BERT (Devlin et al., 2019) and video-MAE (Feichtenhofer et al., 2022) but similar to MAE for images (He et al., 2021).",2,positive
"Following (He et al., 2021), the decoder is designed to be smaller than the encoder.",2,positive
"Motivated by the great success of other MAE-style approaches (He et al., 2021; Feichtenhofer et al., 2022; Hou et al., 2022), we also adopt an asymmetric design that the encoder only operates visible tokens after applying masking on input embedding, and a lighter decoder processes encoded tokens…",2,positive
"MAE (He et al., 2021) proposed to mask a high portion of patches and retain a small set of visible patches received by encoder in pre-training on image data.",2,positive
"It is hypothesized and summarized in (He et al., 2021; Feichtenhofer et al., 2022) that the masking ratio is related to the information density and redundancy of the data, which has an immense impact on the performance of the autoencoders.",2,positive
[22] proposed a pre-training method based on masked autoencoders [43] to perform SSL on the C-MAPSS datasets.,1,neutral
"For an image, even if more than 70% of the image is masked, the model can still produce a reliable restoration [18] , and this phenomenon can be explained from a high redundancy of the image information.",1,neutral
Our encoder/predictor architecture is reminiscent of the generative masked autoencoders (MAE) [35] method.,2,positive
"ods, such as MAE and data2vec.",2,positive
"I-JEPA outperforms pixel-reconstruction methods such as MAE [35] on ImageNet-1K linear probing, semi-supervised 1% ImageNet-1K, and semantic transfer tasks.",1,neutral
"I-JEPA outperforms pixel-reconstruction methods such as MAE [35] on ImageNet-1K linear probing, semi-supervised 1% ImageNet-1K, and se-
mantic transfer tasks.",1,neutral
"By reconstructing missing patches in pixels space, MAE achieves strong performance when fine-tuned end-to-end on large labeled datasets and exhibits good scaling properties.",2,positive
"Compared to reconstructionbased methods, such as MAE, which directly use pixels as targets, I-JEPA introduces extra overhead by computing targets in representation space (about 7% slower time per iteration).",1,neutral
"This idea is at the core of self-supervised generative methods, which remove or corrupt portions of the input and learn to predict the corrupted content [8, 35, 56, 65, 66, 69].",1,neutral
"The idea of image denoising has recently been revisited in the context of masked image modelling [8, 35, 69], where a Vision Transformer [28] is used to reconstruct missing input patches.",1,neutral
"Methods without view data augmentations data2vec [7] ViT-L/16 1600 73.3
MAE [35] ViT-L/16 1600 67.1 ViT-H/14 1600 71.5
trained with I-JEPA requires less computational effort than a ViT-Small/16 trained with iBOT.",0,negative
"Pretraining a ViT-H/14 on ImageNet requires less than 1200 GPU hours, which is over 2.5⇥ faster than a ViTS/16 pretrained with iBOT [75] and over 10⇥ more efficient than a ViT-H/14 pretrained with MAE.",0,negative
"In computer vision, there are two common families of approaches for self-supervised learning from images: invariance-based methods [9,16,17,23,34,36,71] and generative methods [7, 27, 35, 56].",1,neutral
I-JEPA outperforms MAE while requiring less pretraining epochs when using a similar encoder architecture.,2,positive
"I-JEPA significantly outperforms previous methods that do not use augmentations (MAE and data2vec), and decreases the gap with the best view-invariance-based methods, which leverage hand-crafted data augmentations during pretraining, even surpassing the popular DINO [17] on CIFAR100 and Place205 with a linear probe.",2,positive
"Methods without view data augmentations data2vec [7] ViT-L/16 1600 77.3
MAE [35] ViT-B/16 1600 68.0 ViT-L/16 1600 76.0 ViT-H/14 1600 77.2
CAE [21] ViT-B/16 1600 70.4 ViT-L/16 1600 78.1
Closest to our work is data2vec [7] and Context Autoencoders [24].",2,positive
The work on Masked Autoencoders (MAE) [35] proposed an efficient architecture that only requires the encoder to process visible image patches.,1,neutral
"Compared to popular methods such as Masked Autoencoders (MAE) [35], Context Autoencoders (CAE) [21], and data2vec [7], which also do not rely on extensive hand-crafted data-augmentations during pretraining, we see that I-JEPA significantly improves linear probing performance, while using less computational effort (see section 7).",2,positive
"Yet, pixel-level pre-training has been shown to outperform BEiT for fine-tuning [35].",1,neutral
"Our decoder is a lighter-weight 8-layer 512-dimensional transformer, following MAE [20].",2,positive
"Image-based recognition is entering a new era thanks to domain-agnostic architectures, like transformers [11, 65], and large-scale category-agnostic learning [20] .",1,neutral
"For images, masked autoencoders [20] paired with transformers and large-scale category-agnostic training learn general representations for 2D recognition.",1,neutral
This finding directly connects to recent advances in image-based self-supervised learning and masked autoencoders (MAE) [20] which learn powerful image representations by predicting masked (unseen) image patches.,1,neutral
We draw inspiration from MAE [19] for this design.,2,positive
We draw inspiration from MAE [20] for this design.,2,positive
"Our decoder is a lighter-weight 8-layer 512-dimensional transformer, following MAE [19].",2,positive
"LayerNorm [1] is used in all self-attention and MLP layers following standard practice [11, 20, 65].",1,neutral
This finding directly connects to recent advances in image-based self-supervised learning and masked autoencoders (MAE) [19] which learn powerful image representations by predicting masked (unseen) image patches.,1,neutral
"Self-supervised learning has advanced image [2, 20, 46] and language [3, 10] understanding.",1,neutral
Our decoder follows the decoder design from MAE [20].,2,positive
"For the image reconstruction loss, we normalize the ground-truth per-patch, following [38].",1,neutral
"MAE [38] drops the discretization, instead predicting raw pixel loss for a subset of the encoded patches in a manner strongly reminiscent of BART [51].",1,neutral
"During finetuning, we use a layer-wise learning rate decay, following [17, 38, 3].",1,neutral
"Positional encodings are once again added to all elements, following [38].",1,neutral
"We find that a combination of two state of the art approaches: masked auto-encoders, MAE [38] and contrastive language image pre-training, CLIP [69] provides a benefit over CLIP when trained on a corpus of 11.",2,positive
"For all models which rely on masking, we use a 75% masking ratio, consistent with [38, 31], as we did not find an alternative that improved downstream results.",2,positive
In MAE [38] the authors demonstrate a simple technique for self-supervised image-encoder pre-training that—to our knowledge—is still considered state-of-the-art.,2,positive
"[85, 23, 2, 98, 52, 62, 82, 16, 42, 48] apply the masked patch prediction problem from [21, 51, 38] to a joint image-text data space.",1,neutral
"As in [38] we experiment with predicting both normalized and un-normalized patch values, finding that predicting the normalized patch value slightly improves the performance of our MAE implementation.",2,positive
"For evaluation on CIFAR-10, CIFAR-100, Tiny Imagenet-100 and CBIS-DDSM datasets, we use the linear probing strategy (He et al., 2021).",2,positive
"As opposed to contrastive learning, the recent vision transformer autoencoder (ViT-AE ) approach (He et al., 2021) is different from the above methods in principle.",1,neutral
"Vision transformer-based autoencoder (ViT-AE ) (He et al., 2021) is a recent self-supervised learning technique that employs a patch-masking strategy to learn a meaningful latent space.",1,neutral
"For evaluation on CIFAR-10, CIFAR-100, Tiny Imagenet-100, and chest X-ray datasets, we use the linear probing strategy (He et al., 2021).",2,positive
"models pretrained solely on visual data, such as MAE [23], vision-language models are also supervised by the correar X iv :2 30 1.",1,neutral
"We explore several powerful encoders, which can be divided into two categories:
• Vision-based models such as ViT [19], MAE [23] and DiNO [15], which are pretrained solely on images and encompass the image visual content, including the class and location of its objects.",2,positive
"• Vision-based models such as ViT [19], MAE [23] and DiNO [15], which are pretrained solely on images and encompass the image visual content, including the class and location of its objects.",2,positive
"3 exhibits the performance of PARSeq with CLIPTER, when leveraging the vision-based image encoders of DiNO, ViT-MAE and OWL-ViT, and when using the visionlanguage models of CLIP, BLIP and GIT.",2,positive
"†Work done during an Amazon internship.
models pretrained solely on visual data, such as MAE [23], vision-language models are also supervised by the corre-
ar X
iv :2
30 1.",0,negative
"In the case of visual data, discriminative contrastive learning [6], [11], [12], [13], [14], [25], [48] and generative masked image modeling [4], [5], [24], [42], [58] have been demonstrated to learn transferable representations from images by attempting to solve pre-defined pretext objectives that aim to indirectly optimize I(X;Z), achieving state-of-the-art results on popular computer vision benchmarks.",1,neutral
"Inspired by the success of masked language modeling (MLM) using transformers [55] for natural language processing [8], [20], masked image modeling [5], [10], [24], [66] has been explored in the context of learning representations using vision transformers [21], [52].",1,neutral
"MIM has been shown to be more effective at learning transferable representations compared to contrastive learning [24], [66], indicating the effectiveness of generative pre-training.",1,neutral
", MAE [24]), and (iii) multi-modal discriminative (e.",1,neutral
", many existing methods are only evaluated on the ImageNet1K dataset [24], [25], [63].",1,neutral
", MAE [24]), (iii) multi-modal discriminative (e.",1,neutral
"When transferring the representation to ImageNet-1K [18], we follow the widely used fine-tuning recipe introduced by [5], [24].",1,neutral
single-modal pre-training MAE [24] gen.,1,neutral
"The training task was further simplified by MAE [24] and SimMIM [66], which only enforce the model to reconstruct the masked pixels of the input image using a `2 loss and do not require the use of discrete token encoders.",2,positive
"For single-modal SSL methods, we choose MoCoV3 [14] and SimCLR [11] as representative discriminative methods, and MAE [24] as a representative generative method.",1,neutral
MAE [28] and SimMIM [66] demonstrate directly reconstruct masked patches in raw pixel space can also lead to favorable transferability as well as scalability.,2,positive
"(MIM) [3, 28] open a new era of self-supervised visual representation learning, and show unprecedented transferability on various tasks, especially on fine-grained tasks such as object detection [22, 39].",1,neutral
"During pre-training, input images are resized to 224× 224 and we set random mask ratio to 75% following [28].",2,positive
6% better than the MAE [28] and CLIP [47] counterparts.,0,negative
"Among numerous architecture designing spaces, without loss of generalization, we adopt an asymmetric encoderdecoder architecture following MAE [28] and a dualencoder architecture following CLIP [47] for their flexibility.",2,positive
We follow most of setups in [28] for fine-tuning.,1,neutral
", raw pixel [28,66], low-level features [3,62] or high-level perceptions [12, 34, 63, 75]), we map patch features to a probabilistic distribution over a batch of text features as the reconstruction target, which is enabled by ITC that progressively aligns the image and text spaces.",2,positive
"Architecture comparisons between MAE [28], CLIP [47], MAE+CLIP and RILS.",2,positive
"Meanwhile, following MAE [28], we randomly mask a large portion of image patches and leave the remaining patches to be",2,positive
We choose MAE [28] and CLIP [47] as representative methods of masked image modeling and vision-language contrastive learning.,2,positive
"In computer vision, simple self-supervised methods have been employed in various models by randomly masking significant patches [70].",1,neutral
"Recently, due to its strong modeling capability, it has quickly provided leading methods for various vision tasks, including image classification [12, 16], object detection [37, 39], semantic segmentation [70,76], image generation [29,34], and self-supervised learning [2, 21]—see the surveys in [19, 30].",1,neutral
"Along the way, recent efforts have been applied to the popular image self-supervised learning [6, 16, 17] to reduce the demand for labeled data.",1,neutral
"In contrast, in the recognition field, the long-standing paradigm has been to build recognition models [29, 37, 76] by starting from a foundation model pretrained on large-scale image data [4,15,16] or image-text pairs [30, 44, 68].",1,neutral
"Recent Masked Image Modeling (MIM) methods [2, 8, 13], in the “mask-and-predict” style with Vision Transformer [4], are simple yet capable of achieving promising performance in various downstream tasks.",1,neutral
"Following [13], CMAE also introduces the pixel decoder Gp for mapping the latent features z s and MASK tokes z m s (shared in contrastive loss) to the feature space of the target encoder and the original images, i.",2,positive
3 [68] 4096 CL Based on Clustering: SwAV [66] 75.,1,neutral
"Beyond ViTs, a separate early investigation adopted context encoders [115], employing a concept akin to MAE, i.e. , image inpainting.",1,neutral
"Furthermore, MAE has been extended to other modalities beyond images [124]–[126].",1,neutral
"7)—namely, bidirectional encoder representation from image transformers (BEiT) [112], masked AE (MAE) [68], context AE (CAE) [113], and a simple framework for MIM (SimMIM) [114]—have gained significant popularity and pose a considerable challenge to the prevailing dominance of CL. MIM leverages co-occurrence relationships among image patches as supervision signals.",2,positive
"For a Low-Level Targets High-Level Targets Self-Distillation Contrastive / Multi-modal Teacher Algorithm ViT [5] MAE [68] SimMIM [114] Maskfeat [118] BEiT [112] CAE [113] PeCo [119] data2vec [120] SdAE [121] MimCo [122] BEiT v2 [ model of a certain size, when the dataset reaches a certain magnitude, further scaling of the data does not lead to significant performance gains in generative self-supervised methods.",1,neutral
"In contrast to the classic paradigm, during training, the main task head utilizes features acquired from the MAE encoder rather than the original examples.",1,neutral
"A notable distinction between the NLP and CV communities is their use of different that the actual differences their pipelines primary models, with transformers being prevalent in NLP and CNNs being widely adopted in CV. between On the other hand, MAE is a one-stage end-to-end approach, incorporating a decoder to decode the encoder-derived representation into the original pixels. limited to what is shown The landscape changed significantly with the introduction of the original ViT [5], which marked a pivotal moment.",2,positive
"7)—namely, bidirectional encoder representation from image transformers (BEiT) [112], masked AE (MAE) [68], context AE (CAE) [113], and a simple framework for MIM",1,neutral
Low-Level Targets High-Level Targets Self-Distillation Contrastive / Multi-modal Teacher Algorithm ViT [5] MAE [68] SimMIM [114] Maskfeat [118] BEiT [112] CAE [113] PeCo [119] data2vec [120] SdAE [121] MimCo [122] BEiT v2 [123] Target Raw Pixel HOG VQ-VAE VQ-GAN self MoCo v3 CLIP,2,positive
"further extended to various vision-related applications, as evidenced by [52], [68], [84], [112], [249], [250].",1,neutral
"Notably, it is pre-trained using the generative SSL method MAE [68] on the large-scale MillionAID dataset.",2,positive
"Recently, Gandelsman et al. [162] combined TTT with MAE for improved performance.",1,neutral
MAE’s simplicity and effectiveness have established it as a crucial baseline within the MIM domain.,2,positive
"They argued that by treating TTT as a one-sample learning problem, optimizing a model for each test input could be addressed using the MAE as Here, f and g refer to the encoder and decoder of MAE, and h denotes the main task head, respectively.",1,neutral
"In contrast to BEiT, MAE does not utilize special mask tokens and treats the task as a regression problem.",1,neutral
"Following the introduction of BEiT and MAE, several variants have been proposed. iBOT [111] is an “online tokenizer” adaptation of BEiT, aiming to address the limitation of dVAE in capturing only low-level semantics within local details.",2,positive
"Despite their structural alignment, MAE did not find significant application in vision research until the emergence of BEiT.",1,neutral
"While BEiT employs the token output from the pre-trained tokenizer as its target, MAE directly uses the original pixels as its target.",2,positive
The primary distinction between BEiT and MAE lies in their choice of T .,1,neutral
"However, to be consistent with [68], this paper considers BYOL and SimSiam to belong",1,neutral
"The two representative MIM approaches BEiT and MAE, showcase different architectural designs, with subsequent MIM methods often following one of these techniques.",2,positive
"Contrastive approaches are not always used in self-supervised methods [He et al., 2021; Ermolov et al., 2021; Chen et al., 2022].",1,neutral
"The learning targets vary from pixels (He et al., 2022) to image tokens (Bao et al.",1,neutral
"…processing (NLP), computer vision (CV), and other fields (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020; Raffel et al., 2020; Brown et al., 2020; Dosovitskiy et al., 2021; He et al., 2022; Bao et al., 2021; Lu
1ByteDance AI Lab 2The Hong Kong University of Science and Technology.",2,positive
"The learning targets vary from pixels (He et al., 2022) to image tokens (Bao et al., 2021; Peng et al., 2022).",2,positive
"Inspired by MAE [12], we propose a naive background reconstruction method (NBR).",2,positive
"MAE [12] developed an asymmetric encoder-decoder architecture, which masks random patches of the input image and reconstructs the missing pixels.",2,positive
"Therefore, we utilize a mature but large Vision Transformer (MAE-ViT-large [12]) for background reconstruction.",2,positive
MAE [12] removes random patches to reconstruct pixels under a high masking ratio (75%) and works well.,2,positive
"1: Pad the left and top regions of Xori with a width of 16 pixels to the right and bottom: Xori → X∗ ori ∈ R224×224; 2: Horizontally shift the padded image X∗ ori by ∆x pixels, and vertically shift by ∆y pixels; 3: Embed X∗ ori to the feature: X ∗ ori → Fori ∈ RB×P×C ; 4: Complementarily grid-mask Fori twice with a masking ratio of 50%: Fori → (F1, F2), Fi ∈ RB×P× C 2 , F1 ∪ F2 = Fori; 5: for Fi in (F1, F2) do 6: Reconstruct Fi by MAE [12]: Fi → F i ∈ RB×P× C 2 ;",1,neutral
"3) Comparison With Feature Extraction: We use pre-trained MAE10 [72] and SimCLR11 [73] models to extract features for the NUS-WIDE dataset and obtain 768- and 2,048dimensional feature representations respectively.",2,positive
"3) Comparison With Feature Extraction: We use pre-trained MAE10 [72] and SimCLR11 [73] models to extract features for the NUS-WIDE dataset and obtain 768- and 2,048- dimensional feature representations respectively.",2,positive
"advances, such as self-supervised learning for pre-training, as used by [54] and [55], or vision-and-language pre-training for",1,neutral
", 2019) are conceptually simple: they remove a portion of the data and learn to predict the removed parts (He et al., 2021).",2,positive
"We compute the loss only on masked parts, similar to BERT (Devlin et al., 2019) and MEA (He et al., 2021), so the model output on unmasked parts is qualitatively worse.",2,positive
"After being Inspired by NLP, researchers utilized masked autoencoder with the idea of target reconstruction (He et al., 2021).",1,neutral
"Our research is based on masked autoencoding, which is a form of more general denoising autoencoding (He et al., 2021).",2,positive
", 2019) and MEA (He et al., 2021), so the model output on unmasked parts is qualitatively worse.",2,positive
"%) is largely credited with this success (He et al., 2021; Xie et al., 2021).",1,neutral
"Both He et al. (2021) and Xie et al. (2021) regress raw RGBs to simplify the pre-training, while Wei et al. (2022) selects HOG (Dalal & Triggs, 2005) as targets due to their rich semantics.",0,negative
"We also observe calculating loss values only on masked patches gives higher accuracy (row 6), which is consistent with He et al. (2021).",1,neutral
"A bold move that increases the mask ratio to a staggering level (60~75%) is largely credited with this success (He et al., 2021; Xie et al., 2021).",1,neutral
"We try to conclude that in essence, the difficulty is rooted in the gap between language and vision in terms of data processing (Bateman, 2014; He et al., 2021).",1,neutral
"It is the emerging masked image modeling (Bao et al., 2021; He et al., 2021; Xie et al., 2021; Chen et al., 2022) initially extends the success of BERT from language transformers to vision transformers (ViTs).",2,positive
"Similar to MAE (He et al., 2021), SparK has the advantage of encoding efficiency, especially compared to contrastive learning that encodes two or more images in a forward pass.",1,neutral
He et al. (2021) ingeniously takes advantage of transformer’s ability to handle variable-length inputs and implements an efficient and scalable method.,2,positive
"The computer vision community has recently paid more attention to vision transformers, while convnets no longer appear in the spotlight (Liu et al., 2021; He et al., 2021).",2,positive
"One may therefore ask: what exactly is preventing the application of BERT to convnets? We try to conclude that in essence, the difficulty is rooted in the gap between language and vision in terms of data processing (Bateman, 2014; He et al., 2021).",1,neutral
"We then remove the hierarchical design (row 4), which results in a single-scale masked modeling that is commonly used
for transformers (Devlin et al., 2018; Bao et al., 2021; He et al., 2021; Xie et al., 2021).",2,positive
"(a) illustrates MAE (He et al., 2021) that has no such side effect thanks to the transformer’s ability to process variable-length input.",1,neutral
"These designs have been proven to facilitate models to learn more informative features in He et al. (2021), and are also verified by the ablation study later in section 4.5.",2,positive
"Many self-supervised methods [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12] are closing the performance gap with supervised pretraining in computer vision.",1,neutral
"As vision encoder, we consider (1) ViT-B/16 [7] (patch size of 16×16 pixels) with pre-trained weights from self-supervised MoCo-v3 [5], DINO [2] and MAE [10], all trained on IN-1K but without any labels.",2,positive
"%) pretraining are close, while MAE is worse (42.4%), presumably because the representations learned by instance discrimination (MoCo-v3 and DINO), which learns different embeddings for different images, is closer to zero-shot classification than MAE’s training objective.",0,negative
"For the downstream task of OAR segmentation, we employed the ViT backbone and UperNet [20] decoder as the encoder and decoder parts of the segmentation model, following the implementation described in a previous work [10].",2,positive
"[10], on the other hand, proposed a simpler masked autoencoder (MAE) strategy that employs an efficient encoder-decoder design to directly predict pixels within the masked patches.",1,neutral
"Specifically, we replace the multi-crop strategy with a random masked sampling strategy, consistent with pioneering SSL approaches that use masked image modeling with ViT in a patch-wise manner [9], [10].",1,neutral
"Furthermore, various self-supervised learning (SSL) methods have been proposed for ViT, including knowledge distillation-based semantic meaning learning [8] and masked image modeling [9], [10].",1,neutral
"Then for enhancing the learning of visual representations, we introduce a masked autoencoder (MAE)[8] branch, which is parallel to the SSL framework and they share the same encoder.",2,positive
"Therefore, we investigate other MIM methods besides MAE[8] and observe that LoMaR[18] can further boost the model performance by 0.",2,positive
"Other masked image modeling methods Several masked image modeling methods[18, 7, 8] have demonstrated their effectiveness to learn visual representations from images.",1,neutral
"Following the asymmetric design in [8], a small and independent decoder is used to reconstruct the corrupted image from the latent representation and mask tokens.",1,neutral
Therefore we introduce a masked autoencoder (MAE)[8] branch to enforce the visual representation learning and help ViT generate more accurate pseudo,2,positive
"As for the MAE branch, we follow the default settings of [8].",2,positive
"To date, Vision Transformers (ViT)[1] have achieved significant progress in supervised learning[1, 2, 3], self-supervised learning[4, 5, 6, 7, 8], and various other computer vision tasks[9, 10, 11, 12].",1,neutral
"Recently, masked autoencoder (MAE)[8] proposed an autoencoding approach, whose objective was simply to reconstruct missing original patches in the pixel space given a partial observation.",1,neutral
"has achieved the best results in the natural language processing (NLP) [10, 36] and computer vision (CV) [12, 15] fields, since its advanced self-attention mechanism.",1,neutral
"However, as shown in Table 2, MIM pre-training [18] mainly effects for relatively large models.",0,negative
"For data augmentation, we follow the settings in MAE [18].",2,positive
"Comparison among TinyMIM (ours), MAE [18] and training from scratch by using ViT-T, -S and -B on ImageNet-1K.",2,positive
"In this work, we adopt MAE [18] as the MIM model due to its popularity and simplicity.",2,positive
SimMIM [53] and MAE [18] find that reconstructing RGB pixels results in favorable representations.,1,neutral
"Masked image modeling (MIM), which masks a large portion of the image area and trains a network to recover the original signals for the masked area, has proven to be a very effective self-supervised method for pre-training vision Transformers [2, 12, 18, 53].",1,neutral
"Recently, masked autoencoders [4, 83, 33, 71, 28] have shown training efficiency [33], model scalability [33], data efficiency [63], and effectiveness on videos [71, 63, 28].",1,neutral
"Therefore, instead of following the common transferring assumption, we revisit the old good idea of training with indomain egocentric data only, but this time in light of the development of recent data-efficient training methods, such as masked autoencoders [33, 63, 28] as well as the scale growth of egocentric data collections (e.",2,positive
"MAE [33, 28] reconstruction results on Ego4D [31] MQ val set.",1,neutral
Our method applies the original MAE [33] and video MAE [28] algorithms.,2,positive
"4, we visualize the MAE [33, 28] reconstruction results on a few Ego4D [31] examples with a ViT-B [25] trained for 200 epochs without per-patch normalization.",2,positive
"…including ImageNet (Deng et al., 2009) classification, contrastive learning (He et al., 2020; Chen et al., 2020c), masked image modeling (MIM) (He et al., 2022; Xie et al., 2022; Xue et al., 2022), and language-vision pre-training (Radford et al., 2021), could guarantee superior…",2,positive
MIM.,2,positive
"SimMIM (Xie et al., 2022) is adopted as it is suitable for convolutional networks.",2,positive
", 2020c;b) and masked image modeling (Bao et al., 2021; He et al., 2022; Xie et al., 2022; Peng et al., 2022; Xue et al., 2022) have gained impressive improvement over ImageNet pre-training on various vision benchmarks.",2,positive
", 2020c), masked image modeling (MIM) (He et al., 2022; Xie et al., 2022; Xue et al., 2022), and language-vision pre-training (Radford et al.",2,positive
"C V
] 1
5 M
ar 2
02 3
et al., 2022; Xiao et al., 2022; Radosavovic et al., 2022; Shah et al., 2022) have demonstrated that applying popular visual pre-training approaches, including ImageNet (Deng et al., 2009) classification, contrastive learning (He et al., 2020; Chen et al., 2020c), masked image modeling (MIM) (He et al., 2022; Xie et al., 2022; Xue et al., 2022), and language-vision pre-training (Radford et al., 2021), could guarantee superior representation for robotic policy learning tasks, e.g., dexterous manipulation, motor control skills and visual navigation.",2,positive
Xiao et al. (2022); Radosavovic et al. (2022); Seo et al. (2022); Gupta et al. (2022) inherit the MIM spirit to realize visual pre-training for control tasks.,2,positive
"…self-supervised learning methods such as contrastive learning (He et al., 2020; Chen et al., 2020c;b) and masked image modeling (Bao et al., 2021; He et al., 2022; Xie et al., 2022; Peng et al., 2022; Xue et al., 2022) have gained impressive improvement over ImageNet pre-training on various…",2,positive
"In image representation, MAE [24] and SimMIM [25] use the random masking strategy to discard or replace the selected patches and in this paper, we adopt the former approach for selected frames.",1,neutral
"It is worth noting that, unlike MAE [24] or SimMIM [25], we do notmake predictions for themasked sequences at the pixel level, choosing to reconstruct the input at the representation level in an implicit way and ensuring that the pair of masked sequences can be as close as possible in the representation.",2,positive
"Inspired by the masked auto-encoders [24], we consider the masking mechanism as a way of data augmentation and combine it with contrastive learning framework; the frame-",2,positive
"On the other hand, in addition to contrastive learning methods, the development of masked auto-encoders [24,25], which aim to mask out patches from an input and attempt to reconstruct the masked parts by combining global and local information, shows the model’s understanding of data distribution.",1,neutral
"MAE [24] and SimMIM [25] model a simple reconstruction loss on the masked patches to achieve pixel-level restoration, while BEiT [33] is a token-level repair.",1,neutral
"Another prominent line of work in self-supervised learning is Masked Image Modeling, the core idea of which is to pre-train a encoder by masking part of the input patches and then reconstructing it [24,25,32,33].",1,neutral
"Moreover, recent work [63] also adopts ViTs for self-supervised learning via masked images, achieving stronger results than supervised learning.",1,neutral
"Masked image modeling, represented by masked autoencoders [31], is one of the latest self-supervised learning strategies.",1,neutral
This behavior is similar to that of the MAE pre-trained ViT model [31].,1,neutral
"Compared to the plain ViT pre-trained with MAE [31], our approach performs similarly up to the Large model regime, despite using much fewer parameters (198M vs 307M).",2,positive
"We compare our approach to previous masked auto-encoder methods [3, 31, 77], which were all designed for transformer-based models.",2,positive
"This is in contrast to the recent success of masked image modeling using transformer-based models [3, 31, 77], where the pre-trained models significantly outperform the supervised counterparts.",1,neutral
"To perform this analysis, we randomly select 1,000 images from different classes in the ImageNet-1K validation set and extract the high-dimensional features from each layer of different models, including the FCMAE models, the ConvNeXt supervised model [52] and the MAE pretrained ViT model [31].",2,positive
"Similar to MAE [31], the target is a patch-wise normalized image of the original input, and the loss is applied only on the masked patches.",1,neutral
"We also considered more complex decoders such as hierarchical decoders [48, 59] or transformers [21, 31], but the simpler single ConvNeXt block decoder performed well in terms of fine-tuning accuracy and reduced pre-training time considerably, demonstrated in Table 1.",1,neutral
"Among many different self-supervised algorithms, masked autoencoders (MAE) [31] have recently brought success in masked language modeling to the vision domain and quickly become a popular approach for visual representation learning.",1,neutral
"While these models were originally designed for supervised learning with ImageNet labels, they can also potentially benefit from self-supervised learning techniques such as masked autoencoders (MAE) [31].",1,neutral
We retrained the CSWin transformer without redesign and original transformer [9] separately and compared them with our redesigned,2,positive
", 2020) and vision tasks (He et al., 2022; Chang et al., 2022); new families of generative models such as diffusion (Ho et al.",1,neutral
"In this work, we tackle an active visual exploration problem using a vision transformer model [15] as the architectural backbone.",2,positive
"Table 3: MAE Attention as glimpse: We use CLS attention of pre-trained MAE[15] as the glimpse map and use locations corresponding to the max value, min value, median value, and random value as the next glimpse location.",2,positive
We study the use of CLS attention map as an alternative for the learned glimpse map through a set of heuristics to use the base MAE model for active visual exploration.,2,positive
"Since the pretext task for training MAE, i.e. random masking and pixel value prediction for masked regions, resembles the partial observability constraint that we are trying to solve, we find MAE encoder to be best suited for our context extractor module and the pre-trained weights to be transferable for our use case.",2,positive
"We build our context encoder on the mask-auto-encoder (MAE) [15] model, pre-trained as a masked-image-model for the image reconstruction pretext task.",2,positive
"In this work, we evaluate our method on the widely studied task of image reconstruction,
We first compare against a baseline where the base MAE model with random glimpse selection is finetuned on the SUN360 and ADE20K datasets, denoted by ‘Random glimpse’ in Table 1.",2,positive
"Next, in addition to finetuning the task module and the context extractor, initialized with MAE weights, we train the glimpse selection module to predict the loss of the task module (i.e reconstruction loss).",2,positive
"While the random selection outperforms other heuristics for base MAE, we see that random selection of glimpse performs inferior to a learned glimpse selection policy, Table 1.",1,neutral
Analysis for the importance of self-supervised feature: Our encoder and decoder are initialized with pretrained self-supervised weights [15].,1,neutral
"As we intend to evaluate the use of base MAE for active vision, we do not finetune it on SUN360 dataset.",2,positive
9% on Imagenet 1000 class classification [15].,0,negative
"We show that vision transformer mod-
els, and in particular MAE, trained on large unlabelled data can replace contemporary CNN-based counterparts.",2,positive
We therefore use MAE’s decoder as our task module.,2,positive
"As the MAE’s pretext task is image reconstruction, it provides a good initialization for the task module’s decoder.",2,positive
Our context extractor is a ViT [12] initialized with MAE’s encoder weights.,2,positive
"mask Transformer but are usually determined by experimental results [8], [10], [38].",1,neutral
"With the development of self-supervised or unsupervised learning methods in ViT [38], recent works [6], [8], [10], [11], [12] using self-supervised pre-training to boost the point cloud understanding and improve the performance of the fine-tuning in the downstream tasks.",1,neutral
"Following [8], [38], the width (feature dimension) of the Transformer encoder is set to 384 with six heads.",1,neutral
"Masking in Transformers is a widely used scheme in both languages [39], [40] and images [38], [41], [42], which usually",1,neutral
"With the development of self-supervised or unsupervised learning methods in ViT [38], recent works [6], [8], [10], [11], [12] using self-supervised pre-training to boost the point cloud understanding and improve the performance of the fine-tuning in",1,neutral
"2) Generating Input Tokens: According to ViT [52], a trainable and lightweight projection should be applied to embedding patches, denoted as F .",1,neutral
"1) Transformer Block: Following ViT [52], the Transformer encoder consists of serial Transformer blocks with the same or similar structure.",1,neutral
"Baselines We benchmark the performance of our two baselines - Average Pairwise Feature Distance (Distance) and Cluster Assignment Distribution (Cluster) using five different feature extractors for each: pixels (φ ), three intermediate ResNext-101 [32] convolutional feature layers (φ, φ, and φ), and features from the encoder output of a Masked Autoencoder [13] (φ ).",2,positive
"Finally, φ extracts the per-patch features produced by the encoder of a large Transformer model (we used Masked AutoEncoder [13]).",1,neutral
"Finally, ϕT extracts the per-patch features produced by the encoder of a large Transformer model (we used Masked AutoEncoder [13]).",2,positive
"Baselines We benchmark the performance of our two baselines - Average Pairwise Feature Distance (Distance) and Cluster Assignment Distribution (Cluster) using five different feature extractors for each: pixels (ϕP ), three intermediate ResNext-101 [32] convolutional feature layers (ϕR(1), ϕR(2), and ϕR(3)), and features from the encoder output of a Masked Autoencoder [13] (ϕT ).",2,positive
"In recent years, global representation-based MAE [20] and ViT [21] have achieved promising classification performance in popular data sets.",2,positive
"For example, in MAE [20], even though the input image is randomly masked, the well-designed encoder can still construct the invisible pixels for recognition tasks.",1,neutral
", transformer [166]–[168], unsupervised representation learning [158], [169]–[171]), and the lack of a concise and easily extensible code base for researchers.",1,neutral
MAE [14] and SimMIM [15] reveal that raw pixels as reconstruction targets are adequate for effective visual learning.,1,neutral
"It has been justified that discrete visual tokens [10]–[13], raw pixels [14], [15], and hand-crafted features [16] are suitable targets to learn versatile models for a broad spectrum of downstream tasks.",1,neutral
"Specifically, we pre-train models based on MAE [14] for 100 epochs on ImageNet-1k [17] with uniform masking and plot the corresponding convergence curve on the validation set during fine-tuning.",2,positive
"enough context to correctly recover the masked patches [14], making the model more intricate and unpredictable.",1,neutral
"LN, which enhances the patch-level local contrast for better performance [14]; Z i and Ti ·(1−M i ) are the encoder outputs of visible tokens and the corresponding targets, respectively; β is experimentally set to 2.",1,neutral
"We pre-train models on ImageNet1K [17] following the settings of MAE [14], where the decoder TABLE I ABLATION STUDY ON THE ADAPTIVE LEARNING RATE SCALE RULE.",2,positive
1) Disjoint Masking: We evaluate DM based on MAE [14] with normalized pixels as reconstruction targets and only pretrain 100 epochs for fast ablation study on ImageNet-1K.,2,positive
"Following prior arts [10], [14], [15], we use ViT [2] with different scales as backbones, i.",1,neutral
"Following a standard SSL evaluation recipe [14], [21], we conduct end-to-end fine-tuning or linear probing for classification on ImageNet-1K and transfer learning for object detection/instance segmentation on COCO [56] and semantic segmentation on ADE20k [57].",2,positive
"In Table III, we evaluate the training efficiency of DM based on several MIM methods with their original pre-training recipes, including MAE [14], BEiT [10], SimMIM [15], and MaskFeat [16].",2,positive
"Although the masked autoencoders have been successfully applied for SSL in images [14] and videos [12,46], it remains challenging and still in exploration due to the inherent irregularity and sparsity of the point cloud data.",2,positive
"Another line of work is completionbased [25, 31, 36, 55, 58, 60] methods, which get inspiration from Masked Autoencoders [14].",1,neutral
"(c) How does a pretraining procedure learn the desirable representation through the backward pass? Empirically, the pretraining procedure that minimizes empirical risks for predicting masked tokens (Devlin et al., 2018; Dosovitskiy et al., 2020; He et al., 2022) appears to succeed in the presence of long sequences.",1,neutral
"Particularly, in the self-supervised setting, e.g., as in MAE (He et al., 2022), we identify a condition number that is pivotal to solving downstream tasks.",1,neutral
"See, e.g., Devlin et al. (2018); Radford et al. (2018, 2019); Dai et al. (2019); Brown et al. (2020); Dosovitskiy et al. (2020); He et al. (2022) and the references therein.",2,positive
", as in MAE (He et al., 2022), we identify a condition number that is pivotal to solving downstream tasks.",1,neutral
"Empirically, the pretraining procedure that minimizes empirical risks for predicting masked tokens (Devlin et al., 2018; Dosovitskiy et al., 2020; He et al., 2022) appears to succeed in the presence of long sequences.",1,neutral
"The Masked Autoencoder (MAE) is a recent state-of-the-art self-supervised representation learning method in computer vision that pretrains a ViT encoder by masking an image, feeding the unmasked portion into a transformer-based encoder, and then tasking the decoder with reconstructing the input image [25].",1,neutral
Scale-MAE is a self-supervised pretraining framework based on the Masked Autoencoder (MAE) [25].,2,positive
Scale-MAE is a selfsupervised pretraining framework based on the Masked Autoencoder (MAE) [25].,2,positive
"As mentioned earlier, MAE-like pre-training frameworks [66, 34, 27, 68] that are specialized for transformer-style 3D point cloud backbones also attract much attention for their impressive performances, despite the inapplicability to many other widelyused (non-transformer) deep set architectures.",2,positive
"4.6 we also particularly included comparisons with the very recent MAE-like approaches [34, 27, 68], which are specialized for transformer-style backbones and thus not applicable to generic types of deep set architectures [36, 53].",2,positive
"More recently, inspired by the success of masked autoencoders (MAE) [17] in the 2D vision community, some researchers are devoted to adapting MAE-like pre-training pipelines [66, 34, 27, 68], which achieve impressive performances.",2,positive
"As a result, we develop an asymmetric encoder-decoder architecture similar to MAE [15].",2,positive
"The masked transformers turn out to be scalable vision learners via valid self-supervised learning [25], [35].",1,neutral
"In addition, an interesting discovery is that plain ViTs with masked pre-training [25], [35] achieve better segmentation results of small objects compared to [15], [18].",1,neutral
"Therefore, in this framework, we construct a semi-supervised cross-algorithm ensemble method for lesion identification in UW-OCTA DR images based on MAE, ConvNeXt, and SegFormer algorithms[10,24,13,5,6].",1,neutral
This stage mainly relies on the MAE algorithm [10] to pre-train on the diabetic retinopathy grade classification data set (Task 3) in the DRAC2022 challenge.,0,negative
"Finally, in order to form a good complement with MAE [10] and SegFormer algorithm [24] based on the self-attention mechanism.",1,neutral
"In this framework, first, we pre-train the MAE algorithm [10] on the DR grade assessment dataset in the diabetic retinopathy analysis challenge (DRAC) 2022.",2,positive
"MAE algorithm [10] as a common structure in self-supervised masked image modeling visual representation learning [1,10].",1,neutral
"After the pre-training, we would extract the backbone of the MAE encoder (also considered as pre-trained MAE-ViT) [9,10].",2,positive
"For example, the masked image modeling algorithm [1,10] in the domain of self-supervised learning.",1,neutral
"Then, the MAE encoder [10] performs feature extraction on the visible patches subset of the image.",1,neutral
"Although the MAE [10] algorithm designed based on the vision transformer (ViT) [9] architecture has good performance, the algorithm of this architecture outputs singlescale low-resolution features instead of multi-scale features.",2,positive
"Several approaches [6, 7, 14, 15] treat the augmented version of the original sample as a positive sample, and the rest of the samples as negative samples.",1,neutral
"We conduct experiments using MVTN with ResNet-50 [37] and ViT [21] as backbone networks, starting from scratch or using weights from ImageNet [73], Dino [9], and Masked Autoencoders (MAE) [36] as initial weights.",2,positive
"The pretraining methods include using ImageNet [73] weights and using SSL weights from Dino [9], and MAE [36].",2,positive
Trajectory analysis [Ayhan and Samet 2016; Hamed et al. 2013; Kanneganti et al. 2018] performs a key role in the design of aircraft by modeling performance for a mission.,2,positive
PRMAE is a simple and effective derivative of the popular MAE [8] with minimal but effective modifications on the masking strategy.,2,positive
"Figure 1: Architecture of the Masked Autoencoder (MAE) in [8], with Vision Transformer (ViT) backbone.",2,positive
"In [8], they compared different mask sampling strategies including the block-wise sampling, random sampling and grid-wise sampling.",1,neutral
"Masked Autoencoder (MAE) [8] is essentially a denoising autoencoder, which has a straight forward motivation that randomly masks patches of the input image and reconstruct the missing pixels.",1,neutral
"Inspired by the success of BERT, a series of works [5-12] has been proposed recently in the vision community for image understanding.",2,positive
"Among them, Masked AutoEncoder (MAE) [8] is the most representative method which significantly optimizes both the pre-training efficiency and fine-tuning accuracy and it is leading the new trend of SSL across computer vision tasks.",1,neutral
"linear probing) over a series of downstream task including image classification using ImageNet-1K(IN1K) [13], object detection and segmentation using COCO [26] and semantic segmentation using ADE20K [27] as suggested in MAE [8].",1,neutral
"As indicated in [8], MAE reconstructs pixels, which are not semantic entities and they observed that MAE infers complex, holistic reconstructions, suggesting it has learned numerous visual con-",1,neutral
Its great success is attributed to “a rich hidden representation inside the MAE” [8].,0,negative
We divide an image into regular non-overlapping patches as in [8] and then calculate the gradient sum of each patch and rank them by its gradient sum.,1,neutral
"Inspired by He et al[7], we propose a full convolutional neural network[16] based cross-modal text feature extractor(TFE).",2,positive
"via self-supervised pre-training(Oord et al., 2018; Devlin et al., 2018; He et al., 2022; Xie et al., 2022) or multi-task learning (Ruder, 2017; Caruana, 1997; Sener & Koltun, 2018).",2,positive
"Researchers have devoted great efforts to make the learned features to be more universally applicable, e.g. via self-supervised pre-training(Oord et al., 2018; Devlin et al., 2018; He et al., 2022; Xie et al., 2022) or multi-task learning (Ruder, 2017; Caruana, 1997; Sener & Koltun, 2018).",1,neutral
"One is a decoder (He et al., 2022) which reconstructs the input image, the other is a linear classifier.",1,neutral
"Secondly, we compared the classical MIM-based methods such as MAE (ViT-B) (He et al., 2022) and SimMIM (Swin-B) (Xie et al.",2,positive
", 2020), classical MIM-based methods (△) (He et al., 2022; Xie et al., 2022), self-distillation method ( ) (Caron et al.",1,neutral
", 2020), has achieved competitive results in many image interpretation tasks (Bao et al., 2021; He et al., 2022; Xie et al., 2022).",2,positive
"Notably, concurrent works on self-supervised learning with MAE [87] and BEiT [88] validates the potential of reconstruction objectives.",1,neutral
"Furthermore, some works like MAE (He et al., 2022), SiT (Atito et al., 2021) and BEiT (Bao et al., 2021) exhibit promising performance with the powerful random mask strategy exclusively.",2,positive
"MAE (He et al., 2022) and BEiT (Bao et al., 2021) achieve highly competitive results through inpainting the images occluded by random masks.",2,positive
MAE [13] and BEiT [14] achieve highly competitive results through inpainting the images occluded by random masks.,1,neutral
"Furthermore, some works like MAE [13], SiT [31] and BEiT [14] exhibit promising performance with the powerful random mask strategy exclusively.",1,neutral
"Deep learning models have witnessed pre-training on increasingly large-scale datasets as a general and effective pathway to succeed in both computer vision [2, 21, 40] and natural language processing [7, 13, 33].",1,neutral
"Borrowing the idea from MAE [16], Pang et al. [39] designs a masked auto-encoder to recover the masked parts of objects.",1,neutral
"We denote the methods with * when they are adapted to Point Transformer backbone, e.g., “PointMAE [39]* + PointTrans”.",1,neutral
"Compared to the conventional random masking method (i.e., PointMAE [39]) or contrastive-based method (i.e., PointContrast [56]), our MM-3DScene performs best on the linear probing task.",2,positive
"To solve this, self-supervised learning (SSL) becomes a favorable choice since it can extract rich representations without any annotation [10, 16, 17].",1,neutral
"With the same consistency loss, PointMAE gets 71.0%, which is 0.9% lower than ours.",0,negative
"Masked Modeling (MM) [16, 57], as one of the representative methods in SSL, recently draws significant attention in the vision community.",1,neutral
The table shows that the random masking strategy of PointMAE does not help the downstream segmentation task.,1,neutral
"Unsupervised Scene Pre-training Self-supervised learning (SSL) has recently achieved great success in 2D vision [2, 5, 6, 14, 16, 17, 57] and NLP tasks [3, 10, 11].",1,neutral
"Borrowing the idea from MAE [16], Pang et al.",1,neutral
"While BEIT and related methods [15, 18, 26, 61, 65, 71] essentially apply BERT-style pretraining onto images, several extensions to different data modalities have recently been proposed, e.",1,neutral
"Examples of pretext tasks in vision are image reconstruction from masked or transformed input patches [1, 26], re-ordering of image patches [43], or predicting parameters of image rotations [22].",1,neutral
"[30], whereas recently, a plethora of methods have been proposed for ViT architectures [3, 10, 26, 31].",1,neutral
"It has recently shown promising results in the NLP domain with BERT [14] as well as in the frame-based vision community [3,4,8,9,23,26,30].",2,positive
"As masked image modelingbased self-supervised learning methods using vision transformer [68] have recently become a new trend, we also use two SOTA methods RGMIM [69] and MAE [70] for comparison.",2,positive
"In our experiments, RGMIM and MAE used the ViT-Base model.",2,positive
"Compared with the vision transformer-based methods RGMIM [57] and MAE [58], although we use the traditional and straightfor-",2,positive
"As masked image modeling-based self-supervised learning methods using vision transformer [56] have recently become a new trend, we also use two SOTA methods RGMIM [57] and MAE [58] for comparison.",2,positive
"Compared with the vision transformer-based methods RGMIM [69] and MAE [70], although we use the traditional and straightforward ResNet model, our method outperformed them, especially when the amount of annotated data was significantly reduced.",2,positive
"…signals from unlabeled data itself and thus leverages underlying structure and common representation in data, which has achieved great success in natural language processing (Devlin et al., 2018; Brown et al., 2020) and computer vision (Chen et al., 2020a; Caron et al., 2021; He et al., 2022).",1,neutral
"During BERT pretraining, this percentage is typically 15%, while (He et al., 2022; Wettig et al., 2022) show that a large mask rate is beneficial for pre-training.",0,negative
"He et al., 2022; Wang, Cai, Gao, & Vasconcelos, 2019; X. Zhou, Koltun, & Krähenbühl, 2022).",2,positive
"He et al., 2022; K.",1,neutral
"The motivation of using DETR architecture [9] for background image encoding and understanding stems from its state-of-the-art performance for object detection using the state-of-the-art visual transformer (ViT) encoder architecture [22, 78].",2,positive
"Another important line of work generalizes masked modeling [23], which was initially proposed for language modeling, to other data modalities and domains [38, 67, 79].",1,neutral
"As a result, models learned from the masking supervision demonstrate strong capability for capturing correlations between sequential tokens [38].",1,neutral
This observation is similar to spatial masking in MAE [38] where an optimal masking ratio is found.,1,neutral
"Until now, the MAE-style reconstruction pre-training methods (Baade et al., 2022; Niizumi et al., 2022; Chong et al., 2022; Xu et al., 2022) show the best audio understanding performance on various audio classification tasks.",0,negative
"We employ the vanilla ViT model (Dosovitskiy et al., 2021) as the backbone of our audio SSL models without heavy
structure engineering, and apply the speed-up technique proposed in He et al. (2022).",2,positive
"Inspired by the success of the recent visual pre-training method MAE [He et al., 2022], MSM-MAE [Niizumi et al.",2,positive
"More importantly, it is a zero-cost backdoor removal solution, as conventional fine-tuning is a necessary step for users to adapt the pre-trained encoders to downstream tasks [8, 17, 26, 28].",2,positive
"In particular, for computer vision related tasks, self-supervised pre-training methods - including contrastive [46], reconstruction [47] and self-distillation-based ([48], [49]) methods - have achieved substantial success in transfer learning without the need for ground truth labels at the pretraining stage.",1,neutral
"A learnable corruption embedding e[M] is used to replace the masked position, with which the corrupted representation ZM = 1(M) e[M] + 1(1−M) T is input to encoder (Devlin et al., 2019) or decoder (He et al., 2022b)2.",1,neutral
"Since the rapid development of Transformer (Vaswani et al., 2017) in vision (Dosovitskiy et al., 2021; Liu et al., 2021b), various efforts have been made to spread this trend from NLP towards foundational 2D visual understanding (Bao et al., 2022; He et al., 2022b; Wang et al., 2022a).",2,positive
"We find that this leads to an inferior result, consistent with the observation in 2D that data of low semantics requires a non-trivial decoder for modeling purpose (He et al., 2022b).",1,neutral
"Pioneering architectures like PointNet (Qi et al., 2017a;b) can only encode 3D coordinates and it is not applicable for masked denoising autoencoding (DAE) (Vincent et al., 2008; 2010; Devlin et al., 2019) which is proved successful in NLP and 2D vision (He et al., 2022b).",2,positive
"…Liu et al., 2021b), abundant works have been proposed to generalize DAE to masked modeling of RGB pixel (Zhang et al., 2016; Chen et al., 2020a; He et al., 2022b), pretrained DALL-E token (Ramesh et al., 2021; Bao et al., 2022), online teacher token feature (Zhou et al., 2022), and HOG feature…",2,positive
"The objective is to train the a student encoder fS to predict/reconstruct the output from a teacher encoder fT , where the teacher could be a discrete variational autoencoder (dVAE) (Bao et al., 2022) or simply identity mapping (He et al., 2022b).",2,positive
"Masked signal modeling can be viewed as an extension of the classical denoising autoencoders (DAE) with masked corruption (He et al., 2022b), which has been recently explored for language models (Devlin et al., 2019) and vision (Bao et al., 2022).",1,neutral
"D (7)
With this parameter-efficient prompt tuning strategy, we are able to tune the pretrained foundational Transformer while preserving as much pretrained knowledge as possible (He et al., 2022a).",2,positive
Masked autoencoder [13] randomly masks 75% tokens which significantly speeds up the self-supervised visual representation pre-training based on masked image modeling.,1,neutral
"MaskCLIP [7] combines CLIP with the masked image modeling [2,13,24, 37].",1,neutral
The random masking method has been shown to be effective for masked image modeling [13].,1,neutral
"It has shown to perform also effectively on masked image modeling based pre-training approaches [13,15,26,31,36] when combined with the vision Transformer architectures [8].",2,positive
"Unlike ours, it uses a random masking strategy for CLIP training, like MAE [13].",2,positive
The mean absolute error (MAE) metric is also used in our evaluation to measure the foreground-background segmentation error.,1,neutral
"Specifically, compared with the currently second best result, Fωβ increased by 7.6% on average, Sm increased by 4.6%, Em increased by 2.6%, and MAE lowered by 1.4%.",0,negative
"Firstly, a vanilla Vision Transformer(ViT) is introduced in DQnet, which is pretrained in a self-supervised manner [12], to generate representations with long-range correlations.",1,neutral
"The design of image as patches facilitates the learning of object-centric representations, as evidenced by recent works, e.g. DINO, EsViT, MAE [2, 12, 15].",1,neutral
"Second, the design of image as patches facilitates the learning of object-centric representations, as evidenced by recent works, e.g. DINO, EsViT, MAE [2,12,15], that demonstrate ViTs trained with self-supervised learning (SSL) capture objects in the image without label annotations.",1,neutral
"To comprehensively compare our proposed model with other state-of-the-art methods, We use four widely used metrics to evaluate our method: structuremeasure (Sm) [6], E-measure (Em) [7], weighted F-measure (Fωβ ) [19], and mean absolute error (MAE).",2,positive
"DINO, EsViT, MAE [2,12,15], that demonstrate ViTs trained with self-supervised learning (SSL) capture objects in the image without label annotations.",1,neutral
"Recent works, such as [21, 2, 10, 1], have applied masking-based pretext tasks to computer vision tasks and achieved comparable performance to contrastive approaches.",1,neutral
"We note that this budget constraint is similar to MAE [10] and other random masking schemes, which choose to randomly mask out a fixed 75% of the image.",1,neutral
"This is supported by the boosted performance of MAE [10] when removing larger amounts of the image, forcing the network to use some notion of global reasoning.",1,neutral
"pre-trained by MAE [16], which inputs the template image and the search area image into the network together to realize the integration of feature extraction and matching.",2,positive
"The backbone network of OSTrack adopts the vanilla ViT-Base [14] model pre-trained by MAE [16], which inputs the template image and the search area image into the network together to realize the integration of feature extraction and matching.",2,positive
"Patchification has unlocked new capabilities, such as (random) dropping of image patch tokens [10, 20, 44, 53, 61], adding specialized tokens for new tasks [54, 56] or mixing image tokens with tokens from other modalities [1, 38, 64].",2,positive
"SuperViT [34] is most related to FlexiViT as it patchifies an image at multiple scales, passes all these patches to ViT, while dropping random tokens [20] to reduce the sequence length.",1,neutral
"6, 15 [20] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross B.",0,negative
"Some suggest removing tokens, either in randomized [20] or structured [10] fashion throughout training.",1,neutral
"[26] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross B.",0,negative
"Visual text and tokenization in NLP The most closely related method to CLIPPO from the NLP domain is PIXEL [54], which is an MAE [24] trained on rendered text.",1,neutral
"Visual text and tokenization in NLP The most closely related method to CLIPPO from the NLP domain is PIXEL [60], which is a masked autoencoder (MAE) [26] trained on rendered text.",1,neutral
"Following the design choices in MAE [2], MAViL employs 12-layer Transformers (ViT-B) with 12 attention heads as the encoders for each modality .",2,positive
"To learn robust audio-video representations, we go beyond raw input reconstruction in uni-model MAEs [2, 4, 55], multimodal MAE [56], and CAV-MAE [41].",1,neutral
"Masked autoencoders (MAEs) have recently emerged as powerful tools for learning uni-modal representations in various modalities such as image [2], video [3], and audio [4].",1,neutral
"Following MAE [2], we only keep the pre-trained encoders and use the average-pooled top-layer outputs for classification.",2,positive
We notice a concurrent and independent study CAV-MAE [41] uses inter-modal contrastive objective and MAE [2] to reconstruct raw inputs.,1,neutral
"It aims to reconstruct audio and video simultaneously as self-supervision, which sets it apart from uni-modal MAE approaches such as MAE [2], Audio-MAE [4], or Video-MAE [3].",2,positive
"For the visual branch, we either randomly initialize it or initialize it with the self-supervised MAE [2] pre-trained on ImageNet (compared in Table 6).",1,neutral
"NLIP contains a visual encoder-decoder inspired by MAE (He et al. 2021) for reducing the computation cost and maintaining the high quality of visual feature representation, a text encoder encoding the texts enriched by extra auxiliary visual concepts and a concept-conditioned cross-modal decoder…",2,positive
The text encoder and concept-conditioned cross-modal decoder are initialized from BARTbase (Lewis et al. 2020) and the MAE decoder only has 4 transformer blocks with 64-d head.,2,positive
"To enhance visual feature representation via self-supervised regularization, an MAE decoder is adopted to restore masked patches by Image Reconstruction (IR) loss LIR:
LIR = N∑ i=1 ( Ve(x′i) ∥Ve(x′i)∥ − xi ∥xi∥ )2.",1,neutral
"NLIP contains a visual encoder-decoder inspired by MAE (He et al. 2021) for reducing the computation cost and maintaining the high quality of visual feature representation, a text encoder encoding the texts enriched by extra auxiliary visual concepts and a concept-conditioned cross-modal decoder learning to synthesize semantic-consistent captions to complete noisy ones.",2,positive
"Furthermore, inspired by He et al. (2021), we explore enhancing the visual encoder via randomly masking the input image tokens and then reconstructing them, which can help reduce the computation cost during training and boost visual embedding by maintaining low-level visual information.",2,positive
"Patch-based masking methods for self-supervised learning (Bao et al., 2022; He et al., 2021; El-Nouby et al., 2021a) have recently demonstrated their potential for image generation (Chang et al.",1,neutral
MAE means MAE unsupervised pretraining [30] on the MillionAID [54].,0,negative
"Especially the reconstruction of a masked input gained huge traction across various domains of application, among others like natural language processing (NLP) [11] and 2D imagery [15] also on point clouds.",1,neutral
", image-image contrast [5, 9, 10, 21, 28], language-image contrast [12, 53], and masked image modeling [3, 4, 18, 19, 27, 31, 42, 70].",1,neutral
"Mirroring the idea of masked language modeling (MLM), MAE (He et al., 2022), BEiT (Bao et al.",1,neutral
"Mirroring the idea of masked language modeling (MLM), MAE (He et al., 2022), BEiT (Bao et al., 2021), and SimMIM (Xie et al., 2022) use masked image modeling (MIM) for self-supervised vision pretraining.",1,neutral
"Recent years have seen the rapid proliferation of vision transformers (ViTs) across a diverse range of tasks from image classification to semantic segmentation to object detection (Dosovitskiy et al., 2020; He et al., 2021; Dong et al., 2021; Liu et al., 2021; Zhai et al., 2021; Dai et al., 2021).",1,neutral
"1 INTRODUCTION Recent years have seen the rapid proliferation of vision transformers (ViTs) across a diverse range of tasks from image classification to semantic segmentation to object detection (Dosovitskiy et al., 2020; He et al., 2021; Dong et al., 2021; Liu et al., 2021; Zhai et al., 2021; Dai et al., 2021).",2,positive
"com/alinlab/OAMixer including out-of-distribution generalization [4, 37, 39], a natural extension to video domains [3, 5], integration with other domains like language or speech [2, 40], and easily combined with state-of-the-art visual self-supervised learning [22].",2,positive
"In works such as [21, 31], a masked autoencoder approach [18] is applied in the point cloud domain.",1,neutral
"Some works (He et al., 2021; Xie et al., 2022) use pixels as targets; others use pre-trained tokenisers (Bao et al., 2021) or modalityspecific handcrafted features (Wei et al., 2021; Hsu et al., 2021; Shi et al., 2022).",1,neutral
"…successful in various domains, such as natural language processing (Devlin et al., 2019; Radford et al., 2018; 2019; Brown et al., 2020), image recognition (He et al., 2021; Xie et al., 2022; Bao et al., 2021), and speech recognition (Baevski et al., 2020; Hsu et al., 2021; Shi et al., 2022).",1,neutral
"Note that mask tokens are applied to the predictors rather than the encoders (He et al., 2021).",1,neutral
"It is common to apply the loss only on masked inputs for within-modal losses (Hsu et al., 2021; He et al., 2021), since predicting targets corresponding to unmasked inputs may be trivial.",1,neutral
"Some works (He et al., 2021; Xie et al., 2022) use pixels as targets; others use pre-trained tokenisers (Bao et al.",1,neutral
", 2020), image recognition (He et al., 2021; Xie et al., 2022; Bao et al., 2021), and speech recognition (Baevski et al.",1,neutral
"Our masking is less aggressive than what was found to be optimal in related self-supervised image and action recognition literature (where 75% or even 90% of the input is masked) (He et al., 2021; Tong et al., 2022).",2,positive
"successful in 2D (images), even reaching the level of supervised pre-training [12, 16, 33, 37].",0,negative
"Another work, related to MAE [37] (images) or Point-MAE [67] (part segmentation): Voxel-MAE [59] reconstruct the complete voxel grid, given a partially masked input.",1,neutral
"The latter can operate the reconstruction in the feature space [12, 16, 29, 30, 33] trying to reconstruct the features issued from a teacher signal or in the images domain [4, 37], a partially masked input image being reconstructed.",1,neutral
"Recent advances in deep learning [6, 7, 13, 14, 53, 54] rely on massive amounts of training data that not only consume a lot of computational resources, but it is also timeconsuming to train these models on large data.",1,neutral
"We followed the data augmentation scheme of the MAE[21] fine-tuning phase, and the rest of the settings were kept consistent with the pre-training phase.",0,negative
Recently MAE[21] has effectively improved the performance of ViT downstream tasks by recovering the mask tokens pixel information.,2,positive
"proposed MAE[21], which greatly improves the model efficiency while enhancing the performance of ViT.",2,positive
"In Table 11, we compare our results with previous supervised pretraining methods[18, 29], self-supervised MIM methods [2, 10, 3, 7, 5] and CLIP-based MIM methods [25, 31, 13, 24] (i.",2,positive
"The prediction target is a vital component and has attracted a lot of research efforts exploiting different targets such as discrete dVAE code [2], pixels [10, 26], perceptual codebook [5], HOG features [23], online features [6, 1, 8] and so on.",2,positive
Recent advances of SSL methods (Caron et al. 2021; He et al. 2022; Zhou et al. 2021) can achieve comparable or even superior performance to their supervised pre-training version in solving the downstream task.,2,positive
"More recent works can be categorized in discriminative (Dosovitskiy et al. 2014; Bachman, Hjelm, and Buchwalter 2019; He et al. 2020; Chen et al. 2020a) or generative (Kingma and Welling 2013; Xie et al. 2021b; Bao et al. 2021; He et al. 2022) fashion.",2,positive
"Among the possible solutions, selfsupervised representation learning has shown its effectiveness in a wide range of fields including images [2, 12, 13], videos [6, 9, 15, 22, 28] and point clouds [16, 24, 31, 33, 34].",1,neutral
"Benefiting from the rapid development of 2D representational learning [12, 13, 20], 3D representational learning [23, 24, 32, 34] has also been widely explored.",1,neutral
", 2022) investigates the use of the masked autoencoder loss (He et al., 2021a) as a vision loss.",2,positive
"Finally, recent work (Singh et al., 2022) investigates the use of the masked autoencoder loss (He et al., 2021a) as a vision loss.",1,neutral
He et al. (2021b) discovered conducting self-supervised learning on images requires increasing the masking rate to 80%.,1,neutral
"This method was adopted in multiple recent papers and in particular for fine-tuning of self-supervised approaches [5, 22], yet the contribution of this fine-tuning ingredient was not quantitatively measured.",1,neutral
"For instance, recently there has been renewed interest in (masked) auto-encoders [5, 22, 16], which were popular in the early deep learning literature [7, 19, 27].",1,neutral
"More recently, similar methods have been adopted in the vision community as well [10, 41, 82].",1,neutral
We extend the masked-autoencoding framework [41] to learn audiovisual feature representations that can be leveraged for both multimodal and unimodal downstream tasks.,2,positive
"In the masked autoencoding framework [41], the input, x, is tokenised following previous supervised learning setups [6, 26, 33].",1,neutral
"Despite recent advances in selfsupervised image- [10, 41] and video-representation learning [29,76,82], these works still ignore the additional auditory information that is already present in their pretraining",1,neutral
"In the masked autoencoding framework [10, 29, 41], the decoder is another transformer that reconstructs the masked tokens given the encoded tokens as context.",1,neutral
"Additional standardisation may also be applied to x̃ [41, 76].",1,neutral
"Our approach is inspired by the masked autoencoding framework [10,41], which itself is based on similar masked data modelling approaches in NLP [24] and earlier works ar X iv :2 21 2.",2,positive
We begin with an overview of masked autoencoders [41] and transformers in vision in Sec.,2,positive
"Masked Autoencoders (MAE) [41] further showed that simply regressing to the original inputs in pixel-space was just as effective, and by only processing unmasked tokens in the encoder, training could be significantly accelerated.",1,neutral
"More recently, approaches such as masked autoencoders [41] have demonstrated how vision transformers can be pretrained with only self-supervision on smaller datasets.",1,neutral
"Following MAE [31] and BEiT [2], existing masked video modeling methods [21, 57, 63] pretrain video transformers through reconstructing low-level features, e.",1,neutral
We follow the training strategy in MAE [31] and VideoMAE [21] for image teachers and video teachers respectively.,2,positive
"In this paper, we follow the decoupled encoder-decoder transformer architecture in MAE [31] due to its effectiveness and simplicity.",2,positive
MAE [31] proposes an asymmetric encoderdecoder framework for the reconstruction of pixels.,2,positive
"For obtaining spatial targets, we adopt the vanilla image ViT pretrained by masked image modeling [31] on the image dataset (e.",1,neutral
"For pixel regression in MAE [31] and VideoMAE [57], the L2 distance is used as the distance metric.",1,neutral
"This is achieved by a two-stage framework, MVD, optimized to predict high-level features that derived from off-the-shelf MIM pretrained image models [31] and MVM pretrained video models [57] which are readily available.",2,positive
"For self-supervised visual representation learning, recent masked image modeling (MIM) methods like MAE [31] and BEiT [2] achieve promising results with vision transformers [17] on various vision downstream tasks.",1,neutral
MAE [21] adopts an asymmetric encoder-decoder architecture to produce informative latent representation by training image reconstruction tasks on 75% masked images.,2,positive
"To show the feasibility of the concept, we explore the adoption of masked autoencoder (MAE) structure [21] for image reconstruction, which achieves state-of-the-art performance in the self-supervised learning regime.",2,positive
"1, we compare the pooling strategies based on the scratch training and fine-tuning on pre-trained models by Self-Supervised Learning (SSL), including MAE [27], BeiT [3], SimMIM [77] and Data2Vec [2].",2,positive
"For ViT, recent works exploit the average pooling for achieving better performances than the class token [12, 52, 53], or preserving such per-token information [2, 3, 27, 77].",1,neutral
"Moreover, ViT has been actively used for Self-Supervised Learning (SSL) task [2, 3, 7, 27, 72, 77, 78].",1,neutral
"The average pooling [1] has been a standard pooling strategy for CNNs and also has been actively exploited in ViT [2, 3, 27, 77].",1,neutral
"Every SSL model are pre-trained with ImageNet-1K, except BeiT [3] with † used ImageNet-22K.",1,neutral
"Recent ViT based SSL approaches, such as MAE [27], SimMIM [77], BeiT [3] and Data2Vec [2], feed direct loss to patch tokens for each objective (i.",1,neutral
"As a result, Vision Transformer (ViT) [21] was introduced, and its variants have shown great success in image recognition [29, 47, 66, 71], self-supervised learning [2, 3, 7, 27, 77, 78], object detection [6, 24, 44, 61, 62], segmentation [9, 64], image compression [35], image retrieval [22,23], and multi-modal representation learning [36, 49, 56].",1,neutral
We ablate the type of visual representation and prior use by trying an initialization using the VGG16 network [68] (VideoDex-VGG) and the MVP network [7] [69] (VideoDex-MVP) based representation trained for robot learning.,2,positive
"Inspired by the representation bottleneck and the mask-based method MAE [45], we propose a training pipeline based on the mask to boost data augmentation.",2,positive
"Inspired by the representation bottleneck and the mask-based method MAE [45], we propose a training",2,positive
model image size FLOPs #param E2E-ViT [13] E2E [15] E2E-DeiT [38] DeiT + ours ∆,1,neutral
"efficiency-accuracy trade-off than all E2E baselines, including the recently proposed improved E2E baselines [15].",2,positive
"We also compare Deep Incubation with the recently proposed improved E2E baselines in [15], where a systematically hyper-parameter search is performed on training configurations.",2,positive
"A natural basis for image retrieval methods are self-supervised models that inherently learn strong feature descriptors, matching similar images to similar representations [11,13,15,29,31].",1,neutral
We follow the design choice of MAE [23] and VideoMAE [50] that skips the video mask token [M] in the encoder and then insert it in the decoder.,2,positive
15% in BERT [13]) since languages are highly semantic and information-dense [23].,1,neutral
"Following [23,50], we only apply the encoder on visible video tokens, a small subset (e.",1,neutral
", 75% in MAE [23] and 90% in VideoMAE [50]) to construct a challenging self-supervisory task for good feature learning.",1,neutral
"Although not included in our MEDIAR, we expect that self-supervised learning approaches [6, 7, 12, 26, 34, 39] can be a promising alternative direction for using the unlabeled images.",2,positive
"Although MAE-based methods have shown effectiveness in 2D image [18] and video [52], how to apply it in large-scale point clouds remains an open problem.",1,neutral
"Different from MAE [18], the multi-scale structure requires the backtracing [12, 73] to make the masked regions consistent across scales to avoid information leakage from previous stages.",2,positive
"Mask Autoencoder (MAE) [18], serving as one of the effective ways for pre-training, has demonstrated great potential in learning holistic representations.",1,neutral
"Previous MAE-style pre-training architectures of (a) single-scale [18, 19, 38] and (b) multi-scale [12, 73] take as inputs the visible tokens and learnable tokens for decoders.",1,neutral
"Inspired by the promising results achieved by MAE [18] in 2D vision, some works extend it into point clouds.",2,positive
"Inspired by the success of MAE [18] in 2D images, we develop the masked autoencoder for self-supervised learning on LiDAR point clouds, as shown in Figure 4.",2,positive
"To set a baseline with the transformer decoder, we follow MAE [18] and some existing works [12,18,19,38,73] to design the pipeline, as shown in Figure 5(a).",2,positive
"Directly applying the original masking strategy [12, 18, 38, 73] to the last stage of the multi-scale encoder would make the pretext task too difficult, especially for small objects.",1,neutral
"Then, we preserve the encoder part of MAE as our shared face encoder and fix it all the time during training.",2,positive
"Thus, we opt to employ a pre-trained masked autoencoder (MAE) (He et al. 2022) to extract facial features that better capture facial appearances and identity information.",2,positive
"In contrast to the ViT decoder in MAE, we find the convolutional decoder achieves more realistic results.",2,positive
"Compared to the compact latent code of StyleGAN2 (Karras et al. 2020) and the identity embedding, the latent space of MAE can better capture facial appearances and identity information, because masked training requires reconstructing masked image patches from visible neighboring patches, thus ensuring each patch embedding contains rich topology and semantic information.",2,positive
The encoder is designed following MAE (He et al. 2022) and pre-trained on a large-scale face dataset using the masked training strategy.,2,positive
"As for F swa, we first pre-trained the face encoder following the training strategy of MAE on our face dataset.",2,positive
"To verify the superiority of using the latent representation of MAE, we train a new model which adopts the identity embedding as the identity representation and employs AdaIN as the injection method.",2,positive
"We first train our face masked autoencoder, following the same settings in MAE4 (He et al. 2022).",2,positive
The improvement is more salient in the MAE backbone.,0,negative
"To demonstrate the robustness of VQT to different pretraining setups, we also evaluate VQT on self-supervised (MAE) [20] and image-language (CLIP) pre-trained [39]",2,positive
"As shown in Figure 2, Adapt-
2Table 3 shows the transfer learning results by each method alone, using the MAE backbone.",2,positive
"Moreover, in order to demonstrate the robustness of the compatibility, we evaluate performance on three different pre-trained backbones: self-supervised pre-trained (MAE with ImageNet1K) [20], image-language pre-trained (CLIP) [39] and supervised pre-trained (ImageNet-21K).",2,positive
"To demonstrate the robustness of VQT to different pretraining setups, we also evaluate VQT on self-supervised (MAE) [20] and image-language (CLIP) pre-trained [39]
backbones.",2,positive
"Since the MAE pre-training uses the reconstruction objective instead of the classification or contrastive one, we hypothesize that some useful intermediate features for classification may not be propagated to the final layer2.",0,negative
"Recent years have witnessed rapid development in self-supervised learning and it has achieved success in both computer vision [12, 13, 14, 15, 16] and speech processing domain [17, 18, 19, 20].",1,neutral
"BeiT [20] propose visual token prediction with the pretrained tokenizer [44], MaskFeat [6] predicts the hand-crafted image descriptor, and MAE [25] directly reconstructs the raw pixels.",2,positive
"To establish a feasible and effective spatiotemporal representation, we study both popular video masked modeling [23, 25] and multimodal contrastive learning [13, 26].",1,neutral
"Instead of directly performing mask image modeling on pixels [21, 56], the pioneer BEiT [2] reconstructs masked patches quantized by a discrete VAE [45].",1,neutral
"To overcome these limitations, non-autoregressive (NAR) transformers are introduced based on different theories, like mask image modeling [2, 21] (i.",1,neutral
"In previous studies, the random masked strategy has been proven the most effective in MAE [16] and SimMIM [17], rather than that of square and block-wise.",1,neutral
"studies, the random masked strategy has been proven the most effective in MAE [16] and SimMIM [17], rather than that of square and block-wise.",1,neutral
"Recently, adopting the MIM in the ViT-based model has been popular as investigated in a variety of frameworks, including iGPT [14], BEiT [15], MAE [16], and SimMIM [17], which have demonstrated strong representation learning ability in computer vision tasks.",1,neutral
We make the following modifications to the MAE decoding process to customize it for document image generation and our task unification framework: (4.a) Cross-Attention with Character Embeddings.,2,positive
"For UDOP-Dual, the text-layout encoder-decoder follows T5-large, and the vision encoderdecoder has the same configuration as MAE-large.",2,positive
MAE demonstrations with 75% masking.,0,negative
The vision decoder is MAE-large decoder [14].,2,positive
"Next, we describe the MAE decoding process.",2,positive
"Besides sequential generation, UDOP can also generate vision documents by leveraging masked autoencoders (MAE) [14] by reconstructing the document image from text and layout modalities.",2,positive
"Originally, MAE masks a percentage of the image patches and feed non-masked patches into a vision encoder.",2,positive
"For the vision decoder, we adopt the decoder of MAE [14] and directly generate the image pixels with text and layout information.",2,positive
We adopt the MAE objective [14] for vision selfsupervised learning.,2,positive
MAE uses mean squared error and apply loss only on masked patches.,1,neutral
"Thus, here we directly apply a standard masked image modeling (MIM) pipeline [5, 19, 48] for training, illustrated in Figure 2.",1,neutral
"community [32, 38] and previous works [5, 19, 48] to use a learnable token vector to replace each masked patch.",1,neutral
"We show that LOCA yields improved performance over state-of-the-art supervised [53,60,65] and unsupervised [13,17,34,84] representation learning methods for ViTs when transferred to 11 diverse and challenging semantic segmentation benchmarks.",2,positive
"Specifically, the task is to reconstruct masked [6] or dropped [34] patches from the input sequence tokens, either directly in pixel space [34] or in feature space [6,69,84].",1,neutral
"This is +4.8 points above the best self-supervised competitor, MAE, and +3.1 points above supervised pretraining with DeiT-3.",0,negative
"For example, dense contrastive approaches adapt the popular contrastive SSL paradigm to the patch level [52, 55, 68, 72, 73] while masked autoencoders propose to reconstruct masked patches [6, 34].",1,neutral
"Following the standard protocol of the Vision Transformers [26], query and reference views are divided into non overlapping patches of resolution P × P .",2,positive
"In terms of training efficiency, based on our implementation, one LOCA epoch takes 17.4 minutes while one MAE epoch takes 5.7 minutes.",0,negative
"Hence, LOCA achieves an improvement of +4.3 points over MAE while being only 1.1× longer to pretrain.",0,negative
"By contrast, models trained with a spatially-aware objective such as MAE or LOCA produce more spatially accurate predictions.",1,neutral
"Indeed, we have observed that freezing the backbone and training a linear classifier on top of MAE features perform very poorly [34].",1,neutral
"However, LOCA reaches 82.1% average relative improvement over random initialization in 600 epochs while MAE reaches 77.8% in 2.6× more epochs (1600).",0,negative
"In this section, we compare LOCA to popular state-ofthe-art SSL models for ViTs: DINO [13], MoCo-v3 [17], MAE [34] and iBOT [84].",2,positive
"Of particular interest, MAE representations achieve the second best SSL performance.",2,positive
"Interestingly, Caron et al. [13] have shown that segmentation masks emerge from the attention maps of Vision Transformers (ViT) [26] trained with these contrastive methods and several works have built on this observation to generate completely unsupervised segmentations [33, 58, 85].",1,neutral
"Recently, patchlevel SSL pretrainings have attracted more and more attention in the community [5, 6, 34, 70, 74, 80].",0,negative
"Recently, masked auto-encoders revisit this “inpainting” approach to pretraining Vision Transformers [6, 34, 69].",1,neutral
"In particular, compared to human-generated sources such as natural languages, which are highly semantic and information-dense [10], images typically require larger transmission bandwidth, and are generally natural signals with a lot of spatial redundant information, thus efficient image semantic communications deserve investigations.",1,neutral
"Under the longer training schedule (800 epochs), our model reaches 83.9% accuracy, 0.4% higher than MAE (He et al. 2021) and
0.9% higher than RandSAC (Hua et al. 2022) (a concurrent autoregressive work of ours).",2,positive
"For linear probing, we use the feature of the last block and adopt an extra BatchNorm (Ioffe and Szegedy 2015) layer before the linear classifier following MAE.",2,positive
"2020), recent works (He et al. 2021; Bao, Dong, and Wei 2021; Wei et al. 2021; Xie et al. 2021) introduce BERT-style pretraining by reconstructing the masked patches, which achieve an overall improvement in downstream tasks and greatly narrow the gap between vision and language.",2,positive
"We followed the pre-training parameters in MAE (He et al. 2021) and MaskFeat (Wei et al. 2021) without using color jittering, drop path, and gradient clip.",2,positive
"Our model pretrained with 300 epochs achieves the same accuracy as MAE (He et al. 2021) pretrained with 1600 epochs, which indicates that our method converges faster in the pretraining stage.",2,positive
"According to the MAE, reducing the number of image masks can improve reconstruction results, but it will decrease model representation due to image local dependency.",2,positive
"We observe that our method achieves 47.8 mIoU, which is slightly lower than MAE by 0.3, but higher than all others.",2,positive
"3, we discovered that MAE masked out 75% of the image tokens, resulting in blurred reconstruction results in large masked regions.",1,neutral
MAE (He et al. 2021) masks a high proportion of the input image and just predicts raw pixels.,1,neutral
"We note that BEIT and MAE are pretrained with 1600 epochs and use grid-search to find the best hyperparameters, while we only pretrain 800 epochs and don’t tune any parameters in the fine-tune stage due to limited access to computation.",2,positive
"3, shows that the baby in the image is ignored by MAE because it’s totally masked, but SAIM can efficiently generate the baby in the image.",1,neutral
"The patch-based iGPT achieves 82.70 Top1 accuracy, which is higher than DeiT-base (Touvron et al. 2021)(81.8) but still lags behind the MIM method (e.g. 83.6 for MAE).",2,positive
"While, our proposed stochastic autoregressive image modeling, SAIM, utilizes all the information of the image to generate clear images, and achieve better fine-tuning accuracy than MAE on ImageNet-1K.",2,positive
"Given a partly masked image, the network is trained to reconstruct properties of the masked areas such as VAE features [2, 17, 51], HOG features [93], or color information [29, 99].",1,neutral
"Method Plane Bcycl Bus Car Horse Knife Mcyle Persn Plant Sktb Train Truck Mean
CDAN [48]
R es
N et 85.2 66.9 83.0 50.8 84.2 74.9 88.1 74.5 83.4 76.0 81.9 38.0 73.9 MCC [36] 88.1 80.3 80.5 71.5 90.1 93.2 85.0 71.6 89.4 73.8 85.0 36.9 78.8 SDAT [57] 95.8 85.5 76.9 69.0 93.5 97.4 88.5 78.2 93.1 91.6 86.3 55.3 84.3 MIC (SDAT) 96.7 88.5 84.2 74.3 96.0 96.3 90.2 81.2 94.3 95.4 88.9 56.6 86.9 TVT [87]
V iT 92.9 85.6 77.5 60.5 93.6 98.2 89.3 76.4 93.6 92.0 91.7 55.7 83.9 CDTrans [85] 97.1 90.5 82.4 77.5 96.6 96.1 93.6 88.6 97.9 86.9 90.3 62.8 88.4 SDAT [57] 98.4 90.9 85.4 82.1 98.5 97.6 96.3 86.1 96.2 96.7 92.9 56.8 89.8 SDAT w/ MAE [25] 97.1 88.4 80.9 75.3 95.4 97.9 94.3 85.5 95.8 91.0 93.0 65.4 88.4 MIC (SDAT) 99.0 93.3 86.5 87.6 98.9 99.0 97.2 89.8 98.9 98.9 96.5 68.0 92.8",0,negative
"3 further provides a baseline of SDAT with MAE [25] pretraining, which includes masked image modeling (MIM) and ImageNet supervision.",2,positive
"3 further provides a baseline of SDAT with MAE [29] pretraining, which includes masked image modeling (MIM) and ImageNet supervision.",2,positive
"To sample the mask, block-wise masking [2], random patch masking [29,99], and attentionguided masking [43, 54] have been explored.",1,neutral
"Moreover, we compare the image imputation performance of our Edge-MAE with vanilla MAE [24].",2,positive
"Furthermore, the results of “With data augmentation” indicate that Edge-MAE works effectively without data augmentation, which is consistent with the findings of [24].",2,positive
"We adopt the partial fine-tuning strategy inspired by [24], i.",2,positive
"More recently, Masked Autoencoder (MAE) [24] is proposed, in which a transformerbased encoder learns the latent representation of a small subset of visible patches, while a lightweight decoder imputes the original input from mask tokens and latent representation.",1,neutral
", image segmentation), the network is initialized by performing a pretext task, such as solving jigsaw puzzles [36], masked pixel prediction [37], or image imputation for randomly masked image patches [24].",1,neutral
"Inspired by Masked Autoencoders [24], an Edge-preserving Masked Autoencoder (EdgeMAE) is presented, which is pre-trained using both paired and unpaired multimodal MR images in a self-supervised learning manner.",1,neutral
"To obtain α for each patch, a binary mask is generated based on the random masking strategy [24].",1,neutral
The original MAE [24] utilizes a transformer-based decoder to impute patches in the masked position.,2,positive
"Inspired by [16,23,46], we perform mask sampling on video and text to achieve end-to-end video-text alignment.",2,positive
"Different from [4, 16, 23, 46], we do not apply masked prediction on video and text but directly apply the cross-modal contrastive loss to pull video and text together.",1,neutral
"This is consistent with the conclusions of MAE [23] and BERT [4]: the information density of image is low, and the information density of text is high.",1,neutral
"Instead of blindly applying the mask-thenprediction paradigm from MAE, we propose a maskedthen-alignment paradigm, namely Masked Contrastive Pretraining, for efficient video-text alignment.",2,positive
"This is consistent with the conclusions of MAE [23] and BERT [4]: the information density of image is low, and the
information density of text is high.",1,neutral
"Benefiting from the success of the transformer architecture in the computer vision domain, recent methods [4,23] draw inspiration from MLM and propose a masked visual modeling (MVM) technique to randomly mask a high portion of spatial patches and encode with visible patches, which greatly reduces spatial redundancy.",1,neutral
This suggests that the masked-thenprediction paradigm in MAE [23] is inconsistent with the goal of retrieval tasks.,1,neutral
"The final masking ratio is 60% for video and 15% for text, which is consistent with [12] and [23, 46].",0,negative
"Different from [4, 16, 23, 46], we do not apply the masked prediction on video and text but directly apply the contrastive objective to pull the paired video and text together while pushing the unpaired video and text apart.",1,neutral
"From VideoMAE [46] and ST-MAE [16] perspectives, the high masking ratio also prevents the model from simply copying neighborhood pixels for low-level reconstruction and use an extreme masking ratio of 90%.",2,positive
"Without blindly applying mask-then-prediction paradigm from MAE, we explore the masking contrastive mechanism based on
the video language domain, and propose a mask-thenalignment paradigm to efficiently learn a multimodal alignment.",2,positive
"MAE [23] and follow-up works [16, 46, 55] utilize the autoencoder to directly reconstruct RGB pixels in an end-to-end manner.",2,positive
"Recent mask sampling techniques in the visual domain [16, 23, 46] propose to randomly mask a high-ratio of spatial regions and adopt the unmasked regions to pretrain the encoder, which brings a new idea to reduce spatial redundancy.",1,neutral
"Another common tactic is fulfilled with sinusoidal mapping Fsine [20,54], through which p is generated on-the-fly by a fixed function dependent on NH , NW and D. Due to space limitation, we provide explicit expression in Appendix B.1.",1,neutral
"All backbones are pre-trained on ImageNet-1k, among which MAE [21] uses unsupervised pre-training.",2,positive
We demonstrate the explicit mapping function Fsine for sine-cosine positional embedding p as follows.,1,neutral
"Another common tactic is fulfilled with sinusoidal mapping Fsine [21,53], through which p is generated on-the-fly by a fixed function dependent on NH , NW and D.",1,neutral
"Particularly, d-th dimension of pm,n can be mapped with Fsine(m,n, d) as below,
Fsin(m,n, d) = { fsin(m, d,NH , D) if d   D/2 fsin(n, d,NW , D) otherwise ,
fsin(pos, d,N,D) =
{ sin( posN+ /T
2d/D) if d%2 = 0 cos( posN+ /T 2(d−1)/D) otherwise ,
where the temperature T and is set to 10000 and 1e−6 respectively, and a normalization is also used to ensure better continuity among varying resolutions.",1,neutral
The Masked Autoencoder (MAE) method [29] further takes advantage of masking to reduce training time and memory.,1,neutral
"In computer vision, explorations along this direction include predicting large missing regions [50], sequence of pixels [10], patches [20, 29, 71], or pre-computed features [6, 66].",1,neutral
"Our fine-tuning implementation follows MAE [29], with the learning rate tuned for each entry.",2,positive
This is in line with the observation [29] that language data has higher information-density than images and thus the text masking ratio should be lower.,1,neutral
"The MAE design has been applied to videos [61, 22], point clouds [49], graphs [59, 9, 32], audio [4, 47, 13, 35], visual control [70, 57], vision-language [23, 41, 31, 19], and other modalities [5].",1,neutral
", 50% or 75%) of patches; the ViT encoder is only applied to the visible patches, following [29].",0,negative
MAE sparsely applies the ViT encoder [20] to visible content.,2,positive
"Our work is related to MAE and its vision-language extensions [23, 41, 31, 19].",2,positive
The reconstruction head follows the design in MAE [29]: it has a small decoder and reconstructs normalized image pixels.,2,positive
"We do not use a reconstruction loss, unlike MAE [29].",1,neutral
"While the encoder is pre-trained on masked images, it can be directly applied on intact images without changes, as is done in [29].",1,neutral
"Inspired by the sparse computation of Masked Autoencoders (MAE) [29], we randomly remove a large portion of image patches during training.",2,positive
"By default, we apply our models on intact images at inference-time, similar to [29].",2,positive
"Self-supervised learning (SSL) has recently provided a promising paradigm toward human-level intelligence and achieved great success in the domains of natural language processing and computer vision, such as BERT,(31) SimCLR,(32) and MAE.(33) SSL firstly pre-trains a model on a well-designed pretext task, then fine-tunes it on a specific downstream task of interest.",1,neutral
"Self-supervised learning (SSL) has recently provided a promising paradigm toward human-level intelligence and achieved great success in the domains of natural language processing and computer vision, such as BERT,31 SimCLR,32 and MAE.33 SSL firstly pre-trains a model on a well-designed pretext task, then fine-tunes it on a specific downstream task of interest.",1,neutral
We use MAE pre-training for most experiments by default unless otherwise specified.,2,positive
"MAE pretraining is to recover the masked patches in the image, which may exhibit a stronger localization capability helping object detection task.",1,neutral
"Thus, for the model initialized from MAE pre-training, we increase finetuning iterations to 180k and batch size to 64.",2,positive
"As shown in Table 3, the performance improves notably as the model size increases, and MAE pre-training shows better performance than GIT pre-training.",0,negative
"In experiments, we explore two pre-training schemes: 1) MAE pre-training: The ViT backbone is initialized from the self-supervised MAE [13] trained on ImageNet-1K [7], while the rest of the model parameters are randomly set; 2) GIT pre-training: The ViT backbone and text decoder are initialized from the pre-trained image VL model GIT [33] and the rest are randomly set.",2,positive
MAE [13] Image Reconstruction (ImageNet-1K) backbone ViT-B 53.,2,positive
We find MAE pre-training can alleviate overfitting and benefit from more training epochs as discussed in [20].,2,positive
Masked Traffic Reconstruction: The mask reconstruction model used by MAE [14] is applicable to What sets METCMVAE apart is its use of traffic tokens with ambiguous semantics to discover associations between traffic window lengths.,1,neutral
The architecture of METC-MVAE is made up of encoder and decoder blocks [14].,2,positive
"In recent years, self-supervised pre-training technique benefits from utilizing unlabeled data, have been widely used, for example, in NLP [12], [13], computer vision [14], [15], etc.",1,neutral
Masked Traffic Reconstruction: The mask reconstruction model used by MAE [14] is applicable to What sets METCMVAE apart is its use of traffic tokens with ambiguous,2,positive
"Number of Transformer laye [6,8,10,12,14,16] 12 Activation Functions [tanh,relu,elu,leakyrelu] tanh Batch Size [4,.",1,neutral
"Two other methods will be tested as well, the first being an adapted version of the Masked Language Modelling (MLM) of BERT [4], and the other being the one of Masked AutoEncoder (MAE) [7].",2,positive
These are heavily inspired by the choices of MAE [7].,1,neutral
"The MAE [7], like BEiT, is a variant of a denoising autoencoder [20] and is based on the pre-training of BERT’s MLM.",2,positive
"The self-supervised pre-training methods explored are mainly based on three methods, either Autoencoder based methods [4], [7], where the aim is to reconstruct the original data point, often after the noise has been added.",1,neutral
"Additionally, a special Classification (CLS) token is appended before adding the sinusoidal positional embedding as per convention [4], [5], [7] such that the final dimension of the input data after patchification is (S+1)×D.",1,neutral
"The target for BERT and MAE pre-training methods were normalized as proposed in MAE [7], and the outputs of the Transformer encoder/decoder are sent through a linear projection before the masked patches are compared with the target using L2loss.",2,positive
"In situations where attaining large labelled data sets on relevant tasks is not feasible, self-supervised learning has proven to be a powerful substitution, being able to leverage unlabelled data to increase performance on smaller data sets [3], [4], [7], [8], by learning a representation of the data for the task at hand.",1,neutral
"The pre-training methods evaluated are BERT style MDM self-supervised learning [4], MAE self-supervised learning [7], and the contrastive learning approach of BYOL.",2,positive
"MDM approaches, however, seem promising, as not only have MDM-based methods shown excellent results in two very different domains [4], [7], the methods used are conceptually simple and appear",1,neutral
"self-supervised learning on an unlabelled data set [4], [7].",1,neutral
"In the inference stage, the RMR module keeps the decoder for reconstruction, which is different from MAE.",2,positive
"The modified MAE is trained by adding the data augmentation methods, including random horizontal flips and color jitters.",2,positive
"Inspired by the masked auto-encoder (MAE) [21], we modified and applied it to the adversarial environment.",2,positive
"Another prominent line of works aims to use large unlabelled datasets to learn a strong visual representation, which can then be utilized for labelling downstream datasets with fewer supervised samples [4, 12, 13].",1,neutral
"In computer vision, linear probing performance is used as a fast on-the-fly metric for model evaluation [8, 13, 18], which is complementary to fine-tuning considering the computational cost.",1,neutral
"With these probing tasks, we compare advanced vision-only models including MAE [18] and MOCOv3 [8], with vision-andlanguage pretrained models including OFA [54], FLAVA [43] and CLIP [39].",2,positive
"Recently, thanks to the invention of vision transformer ViT [13] and the follow-up improvements [4, 18, 35, 49], end-to-end pretraining from raw images pixels is made possible.",2,positive
weights of a ViT that has been pre-trained on ImageNet-21k in a self-supervised manner using the Masked Autoencoders (MAE) technique [33].,1,neutral
"For instance, SCALE would achieve a performance improvement of 0.3% over the best VideoMAE [54] (1600 epochs checkpoint – finetuning) and of 0.6% in the case of the best ρBYOL [18] (800 epochs checkpoint – linear probe) on Kinetics400 with 64 V100 GPUs in about 5 minutes.",2,positive
", as the initialization parameters of the trained model) [25, 35, 54] on a downstream task, where only a small labeled dataset is available.",1,neutral
"Input Sparsity: Sparsity in the input to the model [1, 3, 19, 25, 54] is an effective way to drastically reduce the computational load and memory requirements, while taking advantage of the information redundancy in images and videos [16].",1,neutral
"In contrast, as a reference and with the same computational resources, VideoMAE requires about 27.7 hours to improve its performance of 0.5% through fine-tuning from its 800 to 1600 epochs checkpoint, and ρBYOL needs at least 48 hours to improve of 0.4% its linear probing performance from its 200 to 400 epochs checkpoint.",2,positive
"Similar to MAE [25], we found that applying a batch normalization layer [30] without affine transformations is beneficial for VideoMAE models.",1,neutral
"This is in contrast to MAE-based SSL methods for vision, where the proposed pseudo-tasks are based on the reconstruction of the whole input [16, 19, 25, 54], and even if the loss uses a subset of the tokens (the masked ones) for the loss calculation, the remaining tokens are still part of the decoder’s output and computation graph.",1,neutral
This task is similar to that of a masked autoencoder [25] and gives you an enhanced per-clip representation.,1,neutral
"Masked input reconstruction methods have recently become popular on images [25] and successfully translated to video [16, 54].",1,neutral
"Unless stated otherwise, we train our models for 500 epochs (for example, training with VMAEB on SSV2 takes 137 minutes with one 3090 GPU) with a batch size of 512 and use all 16 clips.",0,negative
"Our SCALE linear with SVT backbone beats the previous state of the art (71.8% vs. 71.5% [18]) and SCALE ft can even improve the accuracy of VMAEBft , which is a strong supervised model, from 81.5% to 81.84%.",2,positive
"We choose ρBYOL for their excellent linear performance, SVT for the usage of ViT [14], and VMAE for showing 1) the applicability of our proposed method to MAE models, 2) the scalability of our method to larger models, and 3) possibility of using supervisedly fine-tuned models as our backbone.",2,positive
We also used a backbone pretrained and fine-tuned on SSv2 (VMAEBSSv2) for the SSv2 experiment to show the universality of SCALE with respect to the pretraining dataset.,2,positive
"Since our clip representations are somewhat abstract representations of the video, we expect the optimal masking ratio to be close to NLP models rather than video MAEs.",2,positive
"Pretrained backbones: We use the pretrained checkpoints of ρBYOL [18], SVT [44], and three variants of VideoMAE [54] (base(B), large(L), and fine-tuned base(FT)).",2,positive
"With SCALE k-NN, we see a consistent improvement over the baseline and find that pre-trained MAE-based models greatly benefit from our training.",2,positive
"All the models are self-supervisedly pretrained on Kinetics-400, except the fine-tuned VMAE base that was also supervisedly finetuned on Kinetics-400.",2,positive
"Masking Ratio: Masking ratio is an important hyperparameter and depends on the data modality, for example, BERT [12] uses 15%, MSN [3] uses 30% (for ViT-Base), MAE [25] uses 75%, and VideoMAE [54] uses 90 to 95% masking.",1,neutral
We even improve the supervised model trained on SSv2 (VMAEBSSv2).,2,positive
"…(Chen et al.,
2020; He et al., 2020; Grill et al., 2020; Chen & He, 2021; Noroozi & Favaro, 2016; Zbontar et al., 2021), masked image models (He et al., 2022; Dosovitskiy et al., 2020; Doersch et al., 2015; Pathak et al., 2016), masked language models (Devlin et al., 2018; Raffel et al.,…",1,neutral
"…remarkable proficiency in addressing a myriad of complex downstream tasks that were very difficult or impossible for the previous methods (Ramesh et al., 2021; Rombach et al., 2022; Ramesh et al., 2022; Sohl-Dickstein et al., 2015; Brown et al., 2020; Radford et al., 2018; 2019; He et al., 2022).",2,positive
", 2021), masked image models (He et al., 2022; Dosovitskiy et al., 2020; Doersch et al., 2015; Pathak et al., 2016), masked language models (Devlin et al.",1,neutral
"Foundation models have recently exhibited remarkable proficiency in addressing a myriad of complex downstream tasks that were very difficult or impossible for the previous methods (Ramesh et al., 2021; Rombach et al., 2022; Ramesh et al., 2022; Sohl-Dickstein et al., 2015; Brown et al., 2020; Radford et al., 2018; 2019; He et al., 2022).",2,positive
"…representation learning have significantly improved downstream performance on virtually every modality, from images Aghajanyan et al. (2022); Bao et al. (2021); He et al. (2022) to text Liu et al. (2019); Lewis et al. (2020); Aghajanyan et al. (2021) to speech Conneau et al. (2020); Radford et al..",2,positive
The Transformer architecture [Vaswani et al. 2017] has received growing interest from various tasks in computer vision [Bao et al. 2021; Chang et al. 2022; Dosovitskiy et al. 2021; Esser et al. 2021a; He et al. 2021; Li et al. 2022; Liu et al. 2021].,2,positive
"Then, with this frozen visual encoder, we used the same feed forward architecture, Q-function parameterization, and training objective (CQL with C51) as scaled QL to
finetune the MAE network.",2,positive
"As before, we also evaluate fine-tuning performance using the representations obtained via masked auto-encoder pre-training (He et al., 2021; Xiao et al.).",2,positive
"In Figure 7, we present our results for offline fine-tuning on 5 games from Lee et al. (2022), ALIEN, MSPACMAN, SPACE INVADERS, STARGUNNER and PONG, alongside the prior approach based on decision transformers (“DT (pre-trained)”), and fine-tuning using pre-trained representations learned from state-of-the-art self-supervised representation learning methods such as contrastive predictive coding (CPC) (Oord et al., 2018) and masked autoencoders (MAE) (He et al., 2021).",2,positive
"• Representations learned by offline Q-learning give rise to more than 80% better performance when fine-tuning on new games compared to representations from state-of-the-art returnconditioned supervised (Lee et al., 2022) and self-supervised methods (He et al., 2021; Oord et al., 2018).",1,neutral
"For MAE, we first pretrained a vision transformer (ViT-Base) (Dosovitskiy et al., 2020) encoder with 80M parameters trained via a reconstruction loss on observations from multi-game Atari dataset and freeze the encoder weights as done in prior work (Xiao et al.).",2,positive
"…based on decision transformers (“DT (pre-trained)”), and fine-tuning using pre-trained representations learned from state-of-the-art self-supervised representation learning methods such as contrastive predictive coding (CPC) (Oord et al., 2018) and masked autoencoders (MAE) (He et al., 2021).",1,neutral
MAE is a more recent self-supervised approach that we find generally outperformed CPC in this comparison.,2,positive
"For the MAE implementation, we used the Scenic library (Dehghani et al., 2022) with the typical configuration used for ImageNet pretraining, except using 84× 84× 4 sized Atari observations, instead of images of size 224× 224× 3.",2,positive
"1 INTRODUCTION High-capacity neural networks trained on large, diverse datasets have led to remarkable models that can solve numerous tasks, rapidly adapt to new tasks, and produce general-purpose representations in NLP and vision (Brown et al., 2020; He et al., 2021).",1,neutral
"RA discussed the experiment design and project direction, helped set up and debug the training pipeline, took the lead on setting up and running the MAE baseline and the online fine-tuning experiments.",2,positive
"High-capacity neural networks trained on large, diverse datasets have led to remarkable models that can solve numerous tasks, rapidly adapt to new tasks, and produce general-purpose representations in NLP and vision (Brown et al., 2020; He et al., 2021).",1,neutral
"Fine-tuning with the frozen representations learned by MAE performs poorly, which we hypothesize is due to differences in game dynamics and subtle changes in observations, which must be accurately accounted for in order to learn optimal behavior (Dean et al., 2022).",1,neutral
", 2022) and self-supervised methods (He et al., 2021; Oord et al., 2018).",1,neutral
", 2018) and masked autoencoders (MAE) (He et al., 2021).",2,positive
We train the MAE for 2 epochs on the entire multi-task offline Atari dataset and we observe that the reconstruction loss plateaus to a low value.,2,positive
"Increasingly powerful architectures [3,13,24], learning methods [4, 12] and a large body of other techniques [15, 27] are constantly introduced.",1,neutral
"Recently, self-supervised learning [2, 3] has emerged as a possible solution to alleviate the dependency on large-scale ar X iv :2 21 1.",1,neutral
"robust visual features from discriminative [9–12, 27] and generative [13] self-supervised learning and vision-language alignment learning [28].",1,neutral
"[13] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"We train the completion network on the 11,426 complete skeletons using a masked auto-encoder strategy [32] where the missing keypoints are masked at the input and will be predicted using the unmasked keypoints.",2,positive
"More recently, generative-based methods, e.g., MAE [116], BEiT [117], and MST [118], has become the most successful self-supervised methods in the vision community and they have surpassed the promising performance achieved by contrastive learning methods.",1,neutral
", MAE [116], BEiT [117], and MST [118], has become the most successful self-supervised methods in the vision community and they have surpassed the promising performance achieved by contrastive learning methods.",2,positive
"Among popular methods to learn representation via self-supervise pre-training, masked autoencoder has been applied successfully to NLP (Devlin et al., 2018) and CV (He et al., 2021) domains.",1,neutral
", 2022) and masked autoencoders (He et al., 2021) are all using patches as input.",2,positive
"The following influential works such as BEiT (Bao et al., 2022) and masked autoencoders (He et al., 2021) are all using patches as input.",1,neutral
"Similar to [7], we utilized ADAMW [13] with learning of 1.",1,neutral
"1, our framework consists of three asymmetric vision transformer inspired by the work of [7].",2,positive
"Several pretext tasks such as jigsaw [8], inpainting [7], [9], contrastive learning [6], [10] are being widely studied.",1,neutral
"To address this problem, there has been increasing interest in self-supervision method [6], [7] that can learn the visual representation without additional data annotation.",1,neutral
"Inspired by these two papers, we add a mask-based random reconstruction module to the input images: x ← g(x), where g is a masked auto-encoder [16], defined by:",1,neutral
"Fortunately, [16] and [56] showcase that mask-based stochastic reconstruction models are semantically aware.",1,neutral
"Here, we examine whether an additional reconstruction module can regularize the partial derivatives to make them semantic-aware to produce human-meaningful images [16, 41, 56].",1,neutral
"The pre-training loss is computed on the masked tokens similar to MAE [29], and the final pre-training loss Lpre-train is defined as: Lpre-train = Lnode + Ledge, Lnode = ∑",1,neutral
"We investigated works in NLP [17] and computer vision (CV) [29] where the pre-trained models have been dominantly used, and we find that the key to most successful pre-training models is to design simple but effective tasks that can scale well.",2,positive
"Inspired by MAE [29], we propose a bi-branch graph masking and reconstruction task for molecular pre-training.",2,positive
"Based on [29], a narrower or shallower decoder would not impact the overall performance of the MAE.",0,negative
"Similar to MAE [29], we propose a transformer-style asymmetric encoder-decoder architecture for each branch.",2,positive
"[24] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"To enable effective expansion, we explore three prior models for guided imagination: DALL-E2 [56] and Stable Diffusion [59] are advanced image generative methods, while MAE [24] is skilled at reconstructing images.",2,positive
"Thanks to strong image reconstruction abilities, our GIF-MAE applies the MAE-trained model [24] as its prior model.",2,positive
"As DALL-E2 [56] and SD are powerful in generating images, and MAE [24] excels at reconstructing images, we explore their use as prior models of GIF for data imagination.",2,positive
"We choose ViT-Base (ViT-B) [24] as the backbone of our framework for both audio and video modalities, due to its stability in performance across different data streams [34, 30, 8].",2,positive
"This simple approach shows promise in different domains including image [13, 34, 11], video [65, 64, 25], and audio [50, 30, 20] among others.",1,neutral
"Inspired by the recent success of Transformers in different domains [23, 30, 34, 25], we use ViT [24] as the backbone of our framework for both audio and visual modalities.",2,positive
"It should be noted that, we use frame resolution of 112(2) and less, whereas, spatial resolutions of 224(2) and 384(2) are mostly used during pretraining with ViT amongst the earlier works [24, 8, 34, 64, 25, 3].",2,positive
"Further, we drop the masked tokens x before feeding the input to θae for computational efficiency [3, 34].",1,neutral
"Inspired by the recent success and scalability of pretraining with masked reconstruction in different domains [23, 13, 65, 30, 12, 34, 50, 64], we adopt masked data mod-",2,positive
"Self-supervised learning aims to learn meaningful representations from unlabelled data with no human supervision [16, 17, 46, 32, 34].",1,neutral
"Many self-supervised learning tasks have been explored [14, 21, 27, 34, 35, 45, 55], of which contrastive learning [23] currently most prevalent.",1,neutral
MAE [9] is trained on the self-supervised task of predicting an image from a partial observation.,1,neutral
"Our suggested method, MAEDAY, addresses FSAD by using Masked AutoEncoder (MAE) [9], a model trained for general image completion based on partial observations.",2,positive
[8] argued that the information density of NLP and CV are very different.,1,neutral
": To investigate the impact of neighbors on network performance, we set the number of neighbors [2, 8, 16, 32, 64] and compared them in the table II.",1,neutral
"Similar to the Masked Autoencoder (MAE) [16], an 8-layer transformer is used as an image decoder with the weights pre-trained on ImageNet dataset [6].",1,neutral
"To learn visual representations of images in a self-supervised manner, we apply Masked Autoencoder (MAE) which is
trained to reconstruct the randomly masked image patches.",2,positive
"Similar to the Masked Autoencoder (MAE)[17], a 8-layer transformer is used as an image decoder with the weights pre-trained on ImageNet dataset[7].",1,neutral
[11] use the self-supervised reconstruction task through masked autoencoders [15] for test-time adaptation.,1,neutral
"Considering the vigorous development of transformer [10–14] and computer vision technology in recent years, to reduce the computational cost and to ensure that the lane detection task can be efficiently completed, we propose a hybrid depth network composed of Swin Transformer and Predictive Recurrent Neural Network (PredRNN) [15] based on the MAE [16] network architecture for lane detection of continuous multi-frame image sequences.",2,positive
"Hence, the tokens can be arbitrarily masked (discarded from the input data) and the learning objective is to recover the masked contents at the pixel level [25, 62], the feature level [2, 55], or in the frequency space [39].",1,neutral
", BEiT [2] and MAE [25]) were built upon plain vision transformers.",1,neutral
", from the base level to the large or huge level) can boost the downstream performance [10, 25], which aligns with the observations in language modeling [3, 14].",1,neutral
"1,600-epoch MAE [25] by a significant margin of 3.",1,neutral
"1% accuracy with only 400 pre-training epochs, surpassing MAE [25] and HiViT [67] with 1,600 epochs.",0,negative
"Recent years have witnessed two major progresses in visual recognition, namely, the vision transformer architecture [18] as network backbone and masked image modeling (MIM) [2, 25, 62] for visual pre-training.",1,neutral
"The situation was changed when new pretext tasks were introduced, in particular, contrastive learning [5, 6, 9, 24, 24, 26, 61] and masked image modeling (MIM) [2, 25, 62], where the latter is yet another type of generation-based learning objective.",1,neutral
"A comparison between the attention maps generated by iTPN, the variant without integral pre-training (w/o iPT), and the MIM baseline (MAE [25]).",2,positive
"Following MAE [25], a random subset of 75% patches are masked from input, and the normalized pixels are preserved for reconstruction.",1,neutral
"Similar to prediction, VideoMAE cannot have feasible rewind results.",2,positive
"VideoMAE [73] is built upon MAE [30] and reconstructs the missing video cubes, which performs TVC by masking all video frames except the first or the last (or both).",2,positive
"VideoMAE attempts to produce all frames simultaneously, which is difficult to maintain video temporal consistency, resulting in a high 328.9 FVD on Kitchen.",2,positive
"The original work employed rotation prediction [15] as the auxiliary task, but subsequent works [12, 31] replaced it with the Masked Autoencoder reconstruction task [18] or contrastive learning [4].",1,neutral
"Accuracy, a common evaluation metric for image classification (Dosovitskiy et al., 2020; Xu et al., 2022b; He et al., 2022) was leveraged to assess different methods in a specific dataset.",2,positive
"To be more specific, training a ViT model 800 epochs in PlantCLEF2022 as MAE (He et al., 2022) requires more than five months with four RTX 3090 GPUs.",0,negative
"As shown in Figure 1, it is understood that three key factors essentially lead to a positive transfer learning performance, a desired source dataset, powerful model, and suitable loss function to pre-train the model (Wu et al., 2018; Kornblith et al., 2019; Kolesnikov et al., 2020; Tripuraneni et al., 2020; He et al., 2022).",2,positive
"To assess the generality, testing accuracy and mean testing accuracy was employed, instead of validation accuracy and mean validation accuracy as used in MAE (He et al., 2022).",2,positive
"Unfortunately, this setting may entail a long training epoch in PlantCLEF2022 to have a better performance, such as 800 epochs in MAE (He et al., 2022).",2,positive
"Specifically, MAE (He et al., 2022) uses reconstruction loss to learn better performance with a high occlusion.",1,neutral
"We emphasize here that we directly adopted the pre-trained model from masked autoencoder (MAE) (He et al., 2022), instead of training the model ourselves.",2,positive
"For a fair comparison, all models or transfer learning strategies were executed with the same settings with most of them following the fine-tuning schemes in MAE (He et al., 2022).",2,positive
"On the contrary, MAE (He et al., 2022) achieves a 85.",1,neutral
"Therefore, our dual transfer learning reduces training time via utilizing the public model from MAE (He et al., 2022).",2,positive
"These methods are simple to implement and scalable to large Internet-scale datasets and deep neural networks, leading to excellent flexibility and generalization for downstream tasks [9, 12, 1].",1,neutral
"Different from other masked prediction variants [12, 9], we found mask loss is not useful in our setting, as our goal is to obtain an scalable decision making model but not only for representation learning.",1,neutral
"• multi-goal reaching: For every trajectory in the validation set, we randomly sample a start state and 5 goal states at random future timesteps from [12, 60).",2,positive
Our first key observation is that masked token prediction with random masking similar to BERT [9] and MAE [12] provides a general and flexible way for learning from unsupervised data.,2,positive
"Unlike in MAE [12] and BERT [9] where the goal is learning representations, we want to directly apply MaskDP to various downstream tasks, and different mask ratios induce different pre-train and downstream gaps.",1,neutral
MAE [12] proposes to randomly mask patches of the input image and reconstruct the missing pixels.,1,neutral
"Architecture Our encoder is a Transformer [33] but applied only on visible, unmasked states and actions, similar to MAE [12].",2,positive
"Self-supervised pretraining has made tremendous successes for unsupervised representation learning in natural language processing (NLP) and vision [12, 9, 3, 4].",1,neutral
"methods have been proposed to model images [7, 10, 3, 12].",1,neutral
", Transformer [33], GPT [4], BERT [9], and MAE [12].",1,neutral
"The quality of the extracted features is then evaluated on the downstream application using a simple model, such as linear adaptation [1, 2, 10, 11], or a two hidden layer multilayer perceptron (MLP) [13, 32, 33] among others.",2,positive
"Hence, there has been growing interest in self-supervised methods [1, 2, 10, 11, 13, 32].",1,neutral
"The objective of unsupervised learning models [1, 2, 10, 11,13,32] is to pre-train neural networks or extract distilled information from input images without relying on labels.",1,neutral
"As in recent unsupervised papers [1, 10, 11, 13], we compare the quality of our unsupervised keypoints to the features obtained by a fullysupervised ImageNet model.",2,positive
"An exponential moving average (EMA)[20] with α = 0.998 is implemented, and the shadow weight is updated after each training step.",2,positive
An exponential moving average (EMA)[20] with α = 0.,1,neutral
"of reconstructing images from randomly masked image patches [12,27].",1,neutral
"In the self-supervised learning (SSL) literature, most works [2, 8, 9, 11, 13, 26, 27, 47, 59] focus on learning image-level representations by pre-training neural networks on natural images, such as ImageNet [16], where objects of interest are monotonously large, salient, and centered.",1,neutral
ST-MAE is proposed in [23] that is based on the MAE [19] utilizing the Siamese encoder structure and paradigm of deep feature transition for uniform VAD in multiple application scenarios.,2,positive
The structural configuration of SIVT follows the design of the MAE-base but we reduce the embedding dimension to 240 for efficient computation.,2,positive
ST-MAE is proposed in [23] that is based on the MAE [19] utilizing the Siamese encoder,2,positive
the vision Transformer has been explored and proven to be efficient for a wide range of visual tasks [19].,1,neutral
"Therefore, after adding fine training tricks and several key components, C-ResNet can also be a competitive model compared with SOTA method, such as Vision Transformer [4], Swin Transformer [5], ConvNeXt [18] and MAE[5].",2,positive
"It was competitive compared with Transformer [4, 5] in terms of accuracy and scalability and achieved 87.8% top-1 accuracy on ImageNet.",2,positive
"It was competitive compared with Transformer [4, 5] in terms of accuracy and scalability and achieved 87.",2,positive
"Co-DETR with Swin-L yields 56.9% and 62.3% AP on LVIS val and minival, surpassing ViTDet with MAE-pretrained [13] ViT-H and GLIPv2 [40] by
+3.5% and +2.5% AP, respectively.",0,negative
"3% AP on LVIS val and minival, surpassing ViTDet with MAE-pretrained [13] ViT-H and GLIPv2 [40] by Method Backbone enc.",0,negative
"Some researchers also turn their attention to masked self-supervised learning [66], [67], [68], [69], [70], [71], which predicts the masked patches from the visible ones.",1,neutral
", 2020), computer vision (He et al., 2022), and learning protein representations (Jumper et al.",1,neutral
"…transformers (Vaswani et al., 2017) has shown major success in several machine learning fields, including language (Devlin et al., 2018; Brown et al., 2020), computer vision (He et al., 2022), and learning protein representations (Jumper et al., 2021; Rao et al., 2021; Baek et al., 2021).",1,neutral
"Since images have heavy spatial redundancy, some degraded pixels can be recovered from contextual information of neighbor pixels [25].",1,neutral
"However, our network’s encoder and decoder are asymmetric, which indicates a significantly smaller decoder [25, 57].",2,positive
"zscdp is fed into a single decoder stage, which is asymmetrically smaller than the encoder [25, 57].",2,positive
"Similarly, while layer-wise lr decay improved the performance of high-level vision tasks when fine-tuning Transformer models [6, 25], our models could not learn the representations well with that regularization.",1,neutral
"Method Better Worse ×3, ×4 training warm-start [40] scratch std in normalization from data [25] 1.",1,neutral
Implementation Details We adopt the model architecture of Masked AutoEncoder (He et al. 2022) with BERT (Devlin et al. 2018) word embeddings and language modeling head.,2,positive
Implementation Details We adopt the model architecture of Masked AutoEncoder (He et al. 2022) with BERT (Devlin et al.,2,positive
"For instance, MAE [20] claims that having an image reconstruction pretraining stage using masked inputs can produce an effective image encoder for image classification tasks.",1,neutral
"Additionally, the most recent advancement [20,50] in the field of representation learning also indicates that autoencoding is a meaningful step for learning visual features.",1,neutral
"For test-time training we do not use any augmentation, instead we construct a batch from the single point cloud sample and for Masked Autoencoder reconstruction, we randomly mask 90% of the tokens.",2,positive
"Recently, He et al. [10] revisited autoencoders and proposed Masked Autoencoders (MAE) for self-supervised representation learning in the image domain.",1,neutral
[10] revisited autoencoders and proposed Masked Autoencoders (MAE) for self-supervised representation learning in the image domain.,1,neutral
"A concurrent work, TTT-MAE [6] substitutes the self-supervised objective with Masked Autoencoder [10] reconstruction task for TTT in the image domain.",1,neutral
"Source Tasks There is rich literature [8, 27, 28, 30] to use ImageNet1K to pretrain vision backbones for various downstream vision tasks (object detection [24], semantic segmentation [29]).",2,positive
"Recent transformer-based models can generate human-like texts (Brown et al. 2020), autocomplete codes (Chen et al. 2021), reconstruct images (He et al. 2022), or compose music (Dhariwal et al. 2020).",1,neutral
"2021), reconstruct images (He et al. 2022), or compose music (Dhariwal et al.",1,neutral
"Masked pretraining is a preeminent technique in image/text representation learning [3, 6, 10, 14, 61] and visual-language modeling [32,51,63].",1,neutral
"Recently, a lot of progress has been made towards representation learning with large-scale unsupervised data [Chen et al., 2020; Grill et al., 2020; He et al., 2022; Zbontar et al., 2021].",2,positive
"We test this approach on a diverse set of chromosome aberrations (an intra-chromosomal unbalanced abnormality: del(5q); intra-chromosomal balanced rearrangements: inv(3) and inv(16), and inter-chromosomal translocations: t(9;22), t(9;11), and t(11:19)) commonly seen in",2,positive
"In particular, del(5q) and t(9;22) returned perfect accuracy, while two of the other abnormalities (inv(3) and t(11;19)) showed 100% precision with >90% recall.",0,negative
"Moreover, when aggregating across multiple cells from the same specimen, mimicking the clinical practice in a diagnostic cytogenetics laboratory in that an abnormality is defined as clonal in nature when seen in at least 2 cells, the methods achieved perfect accuracy in multiple aberrations even in low-data regimes with only 39 abnormal examples of t(11;19) (Figure 4C).",0,negative
"(C) Precision-recall curves for t(9;11), t(11;19), del(5q), and t(9;22), at the individual chromosome image level (orange) or aggregated at the cell `(purple) or specimen levels.",1,neutral
"For example, the normal chromosome 9s from the held out pre-training folds were added to the t(9;11) aberration training set for normal vs aberrant chr9 identification and likewise for the remaining aberration datasets.",0,negative
"Similarly, (D) shows precision-recall for de novo aberration detection based on distance to N-nearest point (here 50th) for t(9;11), t(11;19), del(5q), and t(9;22), respectively.",1,neutral
"The great successes of transformer models [41] in other domains such as NLP [11, 33, 3] and computer vision [13, 19] motivates our work.",1,neutral
"Work in both NLP [11] and vision [5, 19] have explored how masked prediction is useful as a self-supervision task.",1,neutral
"MAE (He et al. 2021) eliminates the dVAE pre-training process by reconstructing pixels, in contrast to predicting tokens.",1,neutral
"In this section, we first briefly introduce Masked Image Modeling (MIM) for image representation learning and then extend it to our video representation learning scheme, Masked Video Modeling (MVM) (§3.1 and §3.2).",2,positive
"They find that spatiotemporal inductive bias in video clips helps a decoder predict input pixels in masked regions, allowing a higher masking ratio (∼ 90%) than MIM (∼ 60% [53] and ∼ 75% [24]) on image self-supervised learning.",1,neutral
"In vision tasks, Masked Image Modeling [24, 53] aims to learn representations of the input images by solving the regression problem in which a model predicts RGB pixel values in randomly masked patch regions of images.",1,neutral
"Masked video modeling Inspired by self-supervised learning with Masked Image Modeling [24, 53, 26], several recent works on video representation learning [57, 49] suggest spatiotemporal masking strategies given video streams.",1,neutral
"The initial condition follows a uniform distribution centered at [0, 0, 25] with width [36, 48, 41] respectively.",1,neutral
"To resolve this problem, we involve masked autoencoder [36], which applies 50% random masking to the input data.",1,neutral
"For example, one may consider generalization to BYOL (Grill et al., 2020) or SimSiam (Chen & He, 2021) that ensembles projection and/or prediction heads, or MAE (He et al., 2022) that ensembles the decoders (which introduces more training cost though).",2,positive
", 2020) or SimSiam (Chen & He, 2021) that ensembles projection and/or prediction heads, or MAE (He et al., 2022) that ensembles the decoders (which introduces more training cost though).",2,positive
"These successes have encouraged increasingly advanced SSL techniques
(e.g., Grill et al., 2020; Zbontar et al., 2021; He et al., 2022).",2,positive
"These successes have encouraged increasingly advanced SSL techniques (e.g., Grill et al., 2020; Zbontar et al., 2021; He et al., 2022).",2,positive
"Recent generative approaches that use masked image modeling as the pretraining task (Dosovitskiy et al., 2020; Bao et al., 2021; He et al., 2022; Zhou et al., 2022; Xie et al., 2022) have achieved competitive finetuning performance.",1,neutral
"Exploring spatiotemporal information by jointly using RNNs and convolutional neural networks (CNNs) [55, 8, 10] or using graph convolution networks [24, 40] also delivered decent performance.",1,neutral
"Nevertheless, to compare to other pre-training strategies, we consider MAE [29] pre-trained on ImageNet [63], thus with a cosine positional embedding, a ViT-Base encoder, and with a Small decoder that is randomly initialized.",2,positive
"In fact, pairs with high overlap make the task trivial, whereas pairs with negligible overlap reduce it to standard MIM [84].",1,neutral
"They have obtained some success on denser tasks such as object detection [29] or human pose estimation [91], and have been applied to robotic vision [59] when pre-trained on related datasets.",1,neutral
"We observe that CroCo pre-training obtains the lowest errors, significantly outperforming the MAE pre-training and the random initialization.",2,positive
"C V
] 1
8 A
ug 2
stance discrimination and MIM methods have achieved excellent performance on semantic tasks such as image classification, in particular with limited amounts of annotated data [2, 17, 71], but have not led to breakthroughs in more geometric tasks like stereo matching and optical flow.",1,neutral
"More recently, CroCo [84] introduces the pretext task of cross-view completion, where a second view of the same scene is added to MIM.",1,neutral
CroCo outperforms MIM pre-training on an array of geometric tasks.,2,positive
"Note that these methods, as MAE [29], regress pixel values that are normalized according to the mean and standard deviation inside each patch, we thus apply the inverse transform for
display: this means that the overall color of each patch will be correct, as it comes from the ground-truth values.",1,neutral
It extends MIM to pairs of images.,1,neutral
"Recently, [84] proposed the pretext task of cross-view completion (CroCo), a variant of MIM where a partially masked input image is reconstructed given visible patches and an additional view of the same scene.",1,neutral
"Overall, MIM models perform well on classification tasks.",2,positive
"Another recently successful pretext task is masked image modeling (MIM) [2, 22, 29, 83, 86, 102], where part of the input data is masked and an auto-encoder is trained to restore the full signal from the remaining visible parts.",1,neutral
"This is higher than the 75% masking ratio of MAE [29], as the unmasked reference view of the same scene adds redundancy.",0,negative
"Following MAE [29], CroCo [84] uses a small decoder of 8 blocks consisting of self-attention, cross-attention and an MLP, with 512 dimensions and 16 attention heads.",1,neutral
"MIM pre-training aims at reconstructing masked information from an input image either in the pixel space [3, 4, 15, 22, 29, 86], or in the feature space [2, 5, 83], and sometimes after quantization [7, 102].",1,neutral
The masking implementation follows [19]:,1,neutral
"Different from [19], we reshape xmask into a masked images as input xinput ∈ RH×W×C .",1,neutral
"Current vision Transformer based methods exploit the representation learning ability of MIM by predicting the clustering of colors [4], mean color [14], the color of raw pixels [19, 36] and patch tokens [1].",1,neutral
"MIM [19, 28] task is a recent promising self-supervised learning method that reconstructs the masked patches of the image.",1,neutral
"Specifically, apart from the original optimization target, we exploit the Masked Image Modeling (MIM) [1, 19] task to mask and recover random patches during the training.",2,positive
"Although we did not achieve the best performance on OSCC and temporal localization tasks in Ego4d Challenge 2022, we believe that, by paying much more
attention to downstream task formulation and optimization, models that are pretrained on egocentric datasets under the settings of VideoMAE will further improve state-of-the-art performance on various Ego4d tasks.",2,positive
"In this report, we demonstrate that merely pretraining the model on 3rd-person view datasets (e.g. Kinetics 400[7]) under the settings of VideoMAE can achieve state-of-the-art performance on egocentric video understanding tasks including Ego4d object state change classification and Ego4d PNR temporal localization.",2,positive
"We will show that even with weights obtained on 3rd-person view datasets, VideoMAE shows great generalization ability on egocentric downstream tasks and surpass most existing methods both on OSCC and temporal localization tasks.",2,positive
"MAE[5], proposed by Kaiming, etc, has now drawn the wide interest of researchers in self-supervised learning due to its pretraining efficiency and generalization ability in various downstream tasks.",2,positive
This demonstrates the great representation learning and generalization ability of VideoMAE in self-supervised video pretraining.,1,neutral
"As shown in Table 1 and Table 2, by simply pretraining on Kinetics 400 under the settings of VideoMAE, we ranked 2nd place in both tasks.",0,negative
"Recently, two parallel works[9][3] called VideoMAE are proposed to extend MAE from image to video domain.",2,positive
"In our experiments, we use ViT as our backbone of which weights are initialized from VideoMAE[9] pretrained on Kinetics-400.",2,positive
"Notably, MIM [5, 30] demonstrates great dense localization ability, while SiameseIM [67] can exhibit semantic alignment and spatial sensitivity at the same time.",1,neutral
MIM has also been proven to work well with large-scale networks [30].,1,neutral
"In recent years, large-scale pre-trained models [5,13,27, 30, 37, 55, 65, 89] have swept a variety of computer vision",1,neutral
"In order to make this mix strategy compatible with existing pretraining tasks like Masked Image Modeling (MIM) and Image Classification (IC), we split the mask m into patches with p× p size.",2,positive
"Some SSP methods have displayed great potential by surpassing SP on downstream tasks by a large margin [13,30,31].",1,neutral
"The comparison for intra-view pre-training is consistent with previous studies [30], implying that masking operation can greatly boost the model’s performance.",0,negative
Note that the widely used Mixup [94] and CutMix [90] are generally incompatible with MIM.,1,neutral
"Some SSP methods can already surpass SP on downstream tasks [5, 30, 31].",1,neutral
"Restrictions apply.
auto-encoder [34, 76], global/dense distillation [33, 81] and masked image modeling (MIM) [4, 5, 14, 30, 87].",1,neutral
", LAION-400M [56]), and self-supervised learning [5, 13, 27, 30, 89] on unlabeled images.",1,neutral
"Self-supervised Pre-training (intra-view) : Auto-Encoder view1 view1 dense feature dense pixels Gaussian (1)Dense Distillation FD [80],BEiT v2 tokenizer [54] view1 view1 dense feature dense feature stop gradient Gaussian Global Distillation view1 view1 dense feature global feature stop gradient Boltzmann Masked Image Modelingpixel MAE [30] masked view1 view1 dense feature dense pixels Gaussian",2,positive
"For example, p = 16 is by default used for MIM [5,30].",1,neutral
"create input and target from the same view, which includes auto-encoder [34, 76], global/dense distillation [33, 80] and masked image modeling (MIM) [4, 5, 14, 30, 85].",1,neutral
"Given that the supervision on masked patches is a regularization for the learning of CAE v2, we suppose that it may not be appropriate to adopt a high mask ratio (75% in MAE [27], 40%-50% in BEiT [3], CAE [10], and MVP [49]) for all scales of ViTs.",1,neutral
"Existing MIM methods explore different supervision targets on their frameworks, including RGB pixels [24, 27], HOG descriptors [48], discrete visual tokens [3,10,18,22,40], and feature representation from momentum models [13,44,51].",2,positive
"Unlike most MIM methods [3, 10, 27, 53] applying the reconstruction supervision on the masked patches, MVP supervises both masked and unmasked patches.",1,neutral
"Most previous MIM methods [3, 10, 27] apply the reconstruction supervision on the predictions of masked patches.",1,neutral
"The encoder F only receives the visible patches Xv following [10, 27].",1,neutral
"Our findings are different from the common sense in the current MIM methods [3,10,27] that only compute the loss on the masked patches, which is inherited from BERT [15] in the NLP areal and has been verified by most current works.",2,positive
"Specifically, in most MIM methods [3, 10, 27, 53], the supervision positions are only associated with the masked patches, i.",1,neutral
"It raises a question: how will the supervision position influence the CLIPtargeted MIM? On the other hand, the mask ratio performs differently for different supervision targets [3, 27].",1,neutral
"Following [10, 27], the encoder F maps the visible patches Xv to the latent representations Zv .",1,neutral
"For example, MAE [27] utilizes a mask ratio of 75%, BEiT [3], CAE [10], and MVP [49] empirically set the mask ratio as 40% and 50%.",1,neutral
Recall that MAE [27] points out a high mask ratio (75,1,neutral
"Following previous MIM methods [3, 10, 27, 48], CAE v2 first embeds x into a total length of N patches, which are then randomly masked by a specific proportion γ.",1,neutral
"With linear probing, CAE v2 shows significant improvements over previous methods with other targets, e.g., BEiT [3], MAE [27], CAE [10], and MaskFeat [48].",1,neutral
"The reconstruction loss of MIM can be applied in different domains or targets, such as RGB [27, 53], HOG [48], discrete visual tokens [3, 10, 18, 22, 40], momentum encoders [13,44,51], and pretrained models [48,49].",1,neutral
Recall that MAE [27] points out a high mask ratio (75%) is good for the balance of efficiency and effectiveness.,1,neutral
55× Table 10: Self-supervised learning results with MAE [22].,1,neutral
Pre-training Top-1 Accuracy (fine-tuning) Pre-training Speedup Epochs Baseline EfficientTrain Computation Wall-time MAE (ViT-B) [22] 86M 1600 83.,0,negative
Results with Masked Autoencoders (MAE).,0,negative
"Importantly, our method is also effective for self-supervised learning (e.g., MAE [22]).",1,neutral
"As a representative example, we deploy EfficientTrain on top of MAE [22] in Table 10.",2,positive
"[22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"Following [22], the models are pre-trained on ImageNet-1K w/ or w/o EfficientTrain, and evaluated by end-to-end fine-tuning.",2,positive
"Additionally, to test transferability, we use the adversarially generated samples using PRIME-ResNet50 and FANVIT as classifiers, and test them on other architectures: (1) ResNet50 [17], (2) MAE [16], a generalizable and scalable asymmetric encoder-decoder architecture, (3) RegNet-
Y [14], a ResNet-type model with a regulatory model to extract complementary features, and (4) data2vec [6], a selfsupervised transformer that predicts contextualized latent representations in a self-distillation setup for any modality.",2,positive
"Additionally, to test transferability, we use the adversarially generated samples using PRIME-ResNet50 and FANVIT as classifiers, and test them on other architectures: (1) ResNet50 [17], (2) MAE [16], a generalizable and scalable asymmetric encoder-decoder architecture, (3) RegNet-",2,positive
"…we notice that there are various open-sourced image ViTs (Wightman, 2019; Touvron et al., 2021), which have been well-pretrained on huge web datasets under rich supervision such as image-text contrastive learning (Radford et al., 2021) and mask image modeling (He et al., 2022; Bao et al., 2021).",2,positive
"…of our UniFormerV2 design, we apply it on the ViTs with different pertaining methods, including supervised learning (Dosovitskiy et al., 2021; Touvron et al., 2022), contrastive learning(Caron et al., 2021; Radford et al., 2021) and mask image modeling (He et al., 2022; Bao et al., 2021).",2,positive
"The availability of large weakly labeled web data combined with self-supervision methods [5, 10, 13, 18, 19] has made it easier to train such models.",1,neutral
MAE [4] has been proved to be a strong competitor of pre-training methods widely used in computer vision.,1,neutral
"1, existing MIM methods are mainly divided into two types: (a) inpaintingstyle [3, 43, 46] and (b) decoder-style [5, 11, 13].",1,neutral
"We only use standard random cropping and horizontal flipping for data augmentation, following MAE [13], CAE [5], etc.",2,positive
"For MAE [13] and MILAN [17], the encoder and decoder respectively process 25% and 100% patches.",2,positive
"Decoder-style models like MAE [13], CAE [5] and MCMAE [11] only take the partial image as the input.",1,neutral
"(MIM) has demonstrated a great ability of self-supervised learning [3, 13], while alleviating the data-hungry issue of Transformer architectures.",1,neutral
"masked modeling methods adopt the CLIP feature as the reconstruction target [17, 33, 44], outperforming counterparts using low-level features [6, 13].",1,neutral
"To restrain the feature magnitudes of teacher features, we generate the alignment target ỹ by normalizing each level of teacher features as MAE [13] does on pixel values:",1,neutral
"To generate a mask view V from an intact image I, one straightforward sampling strategy is random masking, which samples patches without replacement, following a uniform distribution [13].",1,neutral
"(b) Decoderstyle: MAE [13], CAE [5], MCMAE [11], etc.",1,neutral
MAE [13] and SimMIM [46] find that RGB values can act as a simple yet good enough reconstruction target for masked modeling.,1,neutral
"more than 75% of the image [88], thus stimulating the",1,neutral
Table 11: Validation accuracy of linear probing of MAE [21] on the ImageNet validation set.,1,neutral
"Self-supervised Learning
We pre-trained MAE following the official implementation9 on eight NVIDIA A-100 GPUs and fine-tuned the last layer of its encoder on 16 NVIDIA V-100 GPUs.",2,positive
"We pretrain a Vision Transformer model, specifically ViT-B [13],
as MAE’s encoder for 200 epochs with a mask ratio of 0.75.",2,positive
"To verify this idea, we adopt a self-supervised learning method called masked autoencoder (MAE) [21].",1,neutral
"Following MAE, we adopt the ViT as the backbone of the decoder g(·).",2,positive
3 improvement on AP bbox than MAE [20] (53.,0,negative
"In addition, α ≥ λ means that the pixel prediction task contributes more than reconstruction, i.e., prediction can provide more information, which is also demonstrated in MAE [20] and SimMIM [44].",1,neutral
"In the objective detection, MR SimCLR achieves the best results with 1.3 improvement on AP bbox than MAE [20] (53.7 vs. 52.4).",0,negative
"However, due to the low information density of image data [20, 44], there is still much noisy and redundant information after their proposed augmentation, which would affect the extraction of desired information and thus limit the performance of CL in practice.",2,positive
", prediction can provide more information, which is also demonstrated in MAE [20] and SimMIM [44].",1,neutral
"Especially this masking operation has been verified to be effective in Masked Image Modeling (MIM) and Masked Language Modeling (MLM) [6, 15, 20, 44].",1,neutral
"Recently, masked autoencoder (MAE) [20] and SimMIM [44] developed the MIM methods [4, 40, 50], using vision transformers (ViT) [16] backbone to narrow the data distinction between computer vision and natural language.",1,neutral
"In the objective segmentation, MR SimCLR significantly improves APmask over MAE by 0.4 points (46.9 vs. 46.5).",0,negative
MAE pre-trained the ViT model through mask and reconstruction tasks [17].,2,positive
"Latest works [322], [323] have demonstrated the capability of deep learning models to form learned priors and experiences from massive data.",2,positive
In this manuscript we use a ViT-B8 with the modifications proposed by He et al. (2022) where the classifier is applied after global average pooling over the vision tokens.,2,positive
"Among these methods, Masked Signal Modeling (MSM) has achieved promising results in both vision [18,62] and language understanding [8, 37] recently.",1,neutral
"Following [18], we divide the vectorized voxels into patches which will be subsequently transformed into embeddings using a 1D convolutional layer with a stride equal to the patch size.",2,positive
"We also adopt an asymmetric architecture as in [18]: the encoder is optimized to learn effective fMRI representations, while the decoder tries to predict the masked patches.",2,positive
"The portion of data to mask is different across data modalities, with an extremely high mask ratio (75%) usually used for visual signals [18].",1,neutral
"Different from [18], we use a much larger representation-to-data-space ratio to boost the information capacity of learned representations.",1,neutral
"Commonly, the encoder’s output is averaged, or a cls token is appended to produce a pooled 1D feature vector for downstream tasks [8,18].",1,neutral
"Masked Image Modeling (MIM) uses the embedding-topatch-size ratio around one [18], leading to a representation size similar to the original data size.",1,neutral
"Masked Signal Modeling The power of MSM in learning representations from a large-scale dataset was first exploited in [8], which was later adapted to computer vision [18,60,62].",1,neutral
"Recent work [4,21,30,32,38, 100, 110, 121] (re-)explore pixel / feature regression in MIM, but only in a relatively small model and data scales.",1,neutral
"Recently, masked image modeling (MIM) [5, 38, 110] has boomed as a viable approach for vision model pretraining and scaling.",1,neutral
"Following the evaluation procedure in [38], all these models are first fine-tuned on the original ImageNet1K training set and then evaluated on different validation sets using the same fine-tuned model without further hyperparameter selection and specialized fine-tuning.",0,negative
"The detailed model configurations are (arch-model size-img resolution-data): ConvNeXt-XL-384px21K [62], SwinV2-L-384px-21K [61], MAE-H-448px-1K [38], DeiT3-L384px-21K [93], EfficientNet-L2&NS-800px-JFT300M [107], BEiTv2-L224px-21K [66], BEiT-L-512px-21K [5], EVA-g-336px-merged30M&21k.",2,positive
"Self-supervised learning (SSL) has achieved remarkable success in the field of representation learning, applied in computer vision [1, 2], natural language processing [3, 4], as well as speech processing [5, 6].",1,neutral
"However, in general MAE is performing worse than other SSL methods on linear probing (see Table III), which has also been reported in recent works [41, 42].",1,neutral
"In this work, we benchmark four representative methods MoCo, DINO, MAE, and data2vec on the proposed dataset.",2,positive
"For MAE and data2vec, one random season is assigned for each patch in every epoch.",2,positive
"We find 70% to be the best masking ratio, which is similar to natural images as reported in MAE paper, where 75% is the best.",2,positive
"This way, we cover a reasonably diverse set of representative methods from each model category: MoCo utilizes contrastive representative, DINO represents a distillation method, MAE is based on masked reconstruction, and data2Vec combines the masking mechanism with a joint-embedding architecture.",2,positive
We pre-train the MAE models using its default settings following the publicly available repository (https: //github.com/facebookresearch/mae).,2,positive
"These methods reconstruct the masked parts of an input either at pixel-level [16], feature-level [17], or exploit visual tokens [22].",1,neutral
"Specifically, we evaluate four representative self-supervised learning algorithms—namely: MoCo [14], DINO [15], MAE [16], and data2vec [17]—on three different downstream tasks: scene classification, semantic segmentation and change detection.",2,positive
• MAE [16] is a masked autoencoding design by reconstructing missing patches in images.,1,neutral
3) Masking ratios of MAE: Table XIX shows the influence of masking ratios in MAE during pre-training.,0,negative
MAE.,0,negative
"For EO applications we demonstrate SSL4EOS12 to succeed in self-supervised pre-training for a set of methods: MoCo-v2, DINO, MAE, and data2vec.",2,positive
"According to previous studies [8, 10], the key elements hidden in patches are different semantic information they contain, i.",1,neutral
"first extend the masked autoencodeing approach to the pre-training of Vision Transformer (ViT) model, which has gained great success on both model pre-training and inference [10].",2,positive
"The existing model pre-training of MAE encoder is based on a random mask mechanism [10], which only randomly samples a patch subset to pre-train ViT model.",2,positive
"data themselves via carefully designed pretext tasks, such as jigsaw puzzles [29, 42], contrastive learning [5, 15, 41], and masked modeling [14, 62, 30].",1,neutral
"Inspired by this, masked image modeling [3, 14] and masked video modeling [46, 11] are actively investigated and present considerable successes.",1,neutral
"Among these pretext tasks, the masked modeling has validated its effectiveness on various data modalities [14, 30, 46], including the point cloud [50, 62, 30].",1,neutral
"Over the past decade, by leveraging AI techniques such as convolutional neural network [1], deep learning [2], [3], residual [4] and dense [5] block, knowledge distillation [6], network architecture searching [7], [8], attention mechanism [9], [10], selfsupervised learning [11], [12], etc.",1,neutral
"representations [48]- [49], confirming vanilla ViT’s promising potential and capacity in object-level recognition.",1,neutral
"The encoder is adapted from MAE [20], which is a vision transformer (ViT) [18] without prediction head (norm layer is included).",2,positive
Masked AutoEncoder (MAE) [20] is one of the most influential works in masked image modelling for its simple design and excellent efficiency.,1,neutral
"The weight decay is set to zero, following the setups in MAE [20].",1,neutral
(a) Example of Masked Image Modelling [20] (b) Example of Contrastive Learning [21],1,neutral
MAE [20] (see Figure 1a) is another very influential work in masked modelling.,1,neutral
"Discriminative and generative self-supervised methods received growing attentions in recent years (Chen et al. 2020a,b; Donahue and Simonyan 2019; Gidaris, Singh, and Komodakis 2018; Jaiswal et al. 2020; Zhang, Isola, and Efros 2016; He et al. 2022).",1,neutral
", masked autoencoders [115]) to learn general feature representations for matching.",1,neutral
"This inspired many researchers to study Transformers as direct competitors to CNNs in different settings [10, 39, 40] and across different vision tasks [12, 26].",1,neutral
"MAE splits an image into several patches, randomly masks a percentage of them, and learns to reconstruct the masked ones.",1,neutral
"In the first stage, we pretrain the model by borrowing the idea from masked autoencoders (MAE) [11].",2,positive
Our encoder uses the same settings as ViT-B in MAE [11].,2,positive
"In the pretraining stage, we apply RandomResizedCrop to augment data, which is similar to MAE.",2,positive
Masked autoencoders (MAE).,1,neutral
MAE [11] randomly divides N image patches into Nu unmasked ones and Nm masked ones.,1,neutral
The Mean Squared Error (MSE) loss is used to optimize MAE model.,1,neutral
"To better use the pretrained knowledge, different from MAE or ViTSTR [2], which only fine-tune on the pretrained encoder, we fine-tune on both the encoder and the decoder.",2,positive
"To this end, we adopt a two-stage training strategy to train the model as follows:
In the first stage, we pretrain the model by borrowing the idea from masked autoencoders (MAE) [11].",2,positive
"However, recognizing texts is beyond the scope of MAE, and we propose a novel language-aware model to deal with it, as shown in Figure 1.",2,positive
"Different from MAE, our MVLT recognizes scene text in addition to reconstructing the masked patches.",0,negative
"The encoder of MAE is a ViT, which only operates on xu to learn the visual feature embeddings:
vu = encoder(xu), (1)
where vu ∈ RNu×D1 and D1 is the feature dimension in the encoder.",2,positive
"Like MAE, the reconstruction of image patches helps our model to learn an effective visual representation.",2,positive
MAE [27] tries different masking methods to train the autoencoder which can be adopted to serve as the pre-training model.,2,positive
", locality and translation equivariance), the pure ViTs are more over-parameterized and rely on large-scale pre-training [6, 31, 40, 64] to a great extent.",1,neutral
"When finetuning the detector on COCO, we find that applying learning rate decay [1,4,5,9] for the components of the detector (encoder and decoder) gives a ∼0.",1,neutral
"With encoder pretraining on large-scale data [7,19], the models [1,4,9,18,22,24]are able",2,positive
This MAE-liked [13] strategy has two benefits.,1,neutral
"works have sought to obviate this problem through the use of MixUp [72], masking [6, 34], and k-NN [24, 42, 71], the latter of which is directly relevant to our work.",1,neutral
"To produce informative self-supervision signals, the design of handcrafted pretext tasks has flourished for a long time, including jigsaw puzzle completion [47], relative position prediction [15, 16], rotation perception [21], inpainting [48], colorization [41, 67], masked image modeling [27, 60], etc.",1,neutral
"It is simple and intuitive but sub-optimal, and tends to miss opportunities to learn task-specific features [12].",1,neutral
"Among them, contrastive learning [15, 20, 9, 5, 18] and masked image modeling [8, 1, 19, 56] have been particularly successful.",1,neutral
"Then, REGCLR is instantiated by integrating masked autoencoders [12] as a representative example of a contrastive method and enhanced Barlow Twins as a representative example of a regularized method with configurable input image augmentations in both branches.",1,neutral
MAE branch uses weak augmentation and masking to obtain X1 and then follows procedure introduced in MAE [12] to compute reconstructed loss for unmasked patches as LMAE to obtain X ′ 1.,1,neutral
", MAE [12]), and even Generative Adversarial Networks.",1,neutral
"Using the ViT architecture, MAE [12], in particular, generalizes masked language modeling (MLM) popular in natural language processing to MIM in computer vision.",1,neutral
"Our implementation of pretraining is built on the repo provided in MAE [12], and the projector is implemented as a 2 layer multilayer perceptron with 1024 dimensional output.",2,positive
"Many self-supervised learning (SSL) methods for learning visual representation without supervision (or labels) have been proposed in recent years [28, 19, 9, 6, 10, 27, 12].",1,neutral
"This simple yet highly-scalable strategy of masked-based unsupervised pre-training has yielded promising transfer learning results on visionbased downstream tasks such as object detection and segmentation, image classification, and action detection, even outperforming supervised pre-training [16, 24].",1,neutral
"The success of MLM-based techniques has similarly inspired recent work re-examining the classical formulation of Denoising Autoencoders (DAEs) [51], but for ViTs [3, 13, 28], introducing tasks such as Masked Image Encoding [16] and Masked Feature Prediction [24] for image and video modelling, respectively.",1,neutral
"The observed difference in convergence speed between RGMIM and MAE can be attributed to the fact that RGMIM incorporates the spatial information of lung X-ray im-
ages through the region-guided masking strategy, which helps it to focus on the most informative regions of the images during training.",2,positive
"In line with this objective, MAE [10] conducts experiments involving end-to-end training of a masked autoencoder.",2,positive
Table 3 reveals the lung disease detection accuracy of RGMIM and MAE when employing different masking ratios.,1,neutral
"The proposed method was contrasted with five self-supervised learning techniques (MAE, SKD, Cross, BYOL, and, SimSiam).",2,positive
"For RGMIM and MAE, we employed the same settings in all experiments, except for the masking strategy.",2,positive
"In their proposed method, MAE directly predicts masked patches from the unmasked ones, employing a simple mean squared error (MSE) loss.",1,neutral
The results indicate that RGMIM exhibits superior accuracy and faster convergence speed compared to MAE.,0,negative
"Specifically, after only ten epochs of learning, the detection accuracy of RGMIM has already begun to converge, while MAE is still in the process of convergence.",2,positive
We also studied the masking ratio for RGMIM and MAE using hyperparameters.,1,neutral
"Concurrently, a similar architecture called Simple Masked Image Modeling (SimMIM) is introduced in [11], which corroborates the findings of MAE.",1,neutral
"As comparison methods, we used five SOTA selfsupervised learning methods as comparative methods, including masked autoencoder (MAE) [10], self-knowledge distillation based self-supervised learning (SKD) [47], cross-view self-supervised learning (Cross) [48], bootstrap your own latent (BYOL) [20], and simple siamese self-supervised learning (SimSiam) [21].",2,positive
Traditional MIM techniques frequently employ a random masking strategy for ordinary images [10].,1,neutral
"Specifically, SimMIM demonstrates that directly predicting the pixels, as done in MAE, performs no worse than other methods with more complex designs involving tokenization, clustering, or discretization.",1,neutral
"As comparison methods, we used five SOTA selfsupervised learning methods as comparative methods, including masked autoencoder (MAE) [10], self-knowledge distillation based self-supervised learning (SKD) [47], cross-view",2,positive
"In addition, we observe that RGMIM outperforms MAE in terms of robustness, especially when the masking ratio is relatively low, demonstrating the superiority of our proposed method in handling incomplete lung X-ray images.",2,positive
"Method Architecture 1% 5% 10% 50% 100% RGMIM ViT-Base 0.771 0.893 0.919 0.957 0.962 MAE [10] ViT-Base 0.754 0.875 0.903 0.948 0.956 SKD [47] ResNet-50 0.742 0.812 0.896 0.947 0.957 Cross [48] ResNet-50 0.747 0.795 0.817 0.934 0.953 BYOL [20] ResNet-50 0.683 0.754 0.790 0.933 0.954 SimSiam [21] ResNet-50 0.623 0.700 0.781 0.929 0.949 Transfer ViT-Base 0.689 0.861 0.893 0.940 0.953 Transfer ResNet-50 0.539 0.619 0.665 0.913 0.936 From Scratch ViT-Base 0.413 0.580 0.645 0.810 0.848 From Scratch ResNet-50 0.284 0.496 0.532 0.619 0.774
has layer L = 8, latent vector size D = 512, and the number of heads is 16.",0,negative
"Although existing Transformer models like ViT [32] and MAE [10] are usually trained on the large-scale dataset, We trained the ST-MAE model on the limited samples from scratch by an AdamW optimizer with a learning rate of 1e-4 and batch size of 8 for 400 training epochs, while the weights",2,positive
"Note that our ST-MAE is a feature vision Transformer that operates on the deep features of DCNN, which is slightly different from the base ViT [10], its consecutive computational process is roughly demonstrated.",2,positive
"like the MAE [10] and Intr [36], we adopted the feature-level measurement, the relaxed version of the pixel-level constraints, for better robustness.",2,positive
"Like MAE [10], each encoder in our approach maps the partially observed signal to the latent representation, which has been approved to be effective to learn object semantics",2,positive
"The Masked Autoencoder(MAE) proposed in [10] shows that the ViT can learn meaningful visual representations from the small proportion of visible patches subset, which yields promising performance in the downstream tasks.",2,positive
"1, in analogy to MAE [10], our ST-MAE has an asymmetric encoder-decoder design that reconstructs the input in feature space, yet our encoder applies a Siamese architecture.",2,positive
"In our proposed method, MobileNetV3([16]) is adopted as the backbone network.",2,positive
"In our proposed method, MobileNetV3[16] is adopted as the backbone network.",2,positive
"[18] exploited such a property to propose a self-learning framework called Masked AutoEncoder (MAE), by which masking 15% of the patches would still maintain the stateof-the-art accuracy.",2,positive
"[17], [18], to derive a patch shuffling scheme to protect the training data privacy.",1,neutral
"1) Black-Box Attack: For the attack model, we adopt a similar structure to the edge model: an MAE decoder [18] is used, and is pretrained on ImageNet.",2,positive
"Self-supervised learning (SSL) has emerged as a promising pre-training method due to its remarkable progress on various computer vision tasks [24, 9, 21, 6, 23, 59].",1,neutral
"We also report results for our MAE implementation, which approximately matches the original numbers reported by He et al. (2022), validating our MAE results on JFT-300M.",0,negative
"First, masked image models such as the masked autoencoder (MAE) (He et al., 2022) are a nascent set of methods based on a mask-and-reconstruct training mechanism.",1,neutral
"Notably MAE (He et al., 2022) showed that classical masked autoencoding approaches could be used to pre-train ViTs without passing masked tokens through the encoder.",1,neutral
For linear probing we list the hyperparameters in Table 10 for which we followed the settings in He et al. (2022).,1,neutral
"For MAE pre-training, we use the same hyperparameters as listed in He et al. (2022), except for the use of Glorot uniform initialization instead of LeCun initialization as done in He et al. (2022).",2,positive
"Following He et al. (2022), only the T ′ unmasked patches are passed to the ViT encoder, which processes the two views in parallel.",2,positive
"Our approach is a minimal synthesis of contrastive learning, the masked autoencoder (He et al., 2022), and the denoising loss used in the training of diffusion models.",2,positive
"Corroborating the findings of He et al. (2022), we find it best to only compute the reconstruction loss on masked patches:
Lrec = 1
2n ∑ v=1,2 n∑ i=1 ‖Mvi ◦ (xvi − x̂vi )‖22
where ◦ multiplies all pixels in the tth patch of the residual image xvi − x̂vi by (Mvi )t ∈ {0, 1}.",1,neutral
Decoder architecture: Our decoder architecture is the same as He et al. (2022).,2,positive
"BERT (Kenton and Toutanova, 2019) and MAE (He et al., 2022), to classification tasks in NLP, the linear probing performs poorly because the extracted features are specialized for mask prediction and might not contain enough information for classification (Wallat et al.",2,positive
"However, when we apply generative pretrained backbones, e.g. BERT (Kenton and Toutanova, 2019) and MAE (He et al., 2022), to classification tasks in NLP, the linear probing performs poorly because the extracted features are specialized for mask prediction and might not contain enough information for classification (Wallat et al., 2020; Yosinski et al., 2014).",2,positive
"However, when we apply generative pretrained backbones, e.g. BERT (Kenton and Toutanova, 2019) and MAE (He et al., 2022), to classification tasks in NLP, the linear probing performs poorly because the extracted features are specialized for mask prediction and might not contain enough information…",2,positive
"Recently, works like MAE (He et al., 2021) and CAE (Chen et al.",1,neutral
"Recently, works like MAE (He et al., 2021) and CAE (Chen et al., 2022) have learned representations by solving masked reconstruction tasks using vision transformers.",1,neutral
"[18] proposed a masked autoencoder (MAE), which is an asymmetric encoder–decoder structure, to build unknown masked pixels from known pixels, which can also be seen as an extension of BERT [19].",1,neutral
It is worth noting that the decoder was only used in the pre-training phase and can be replaced with any architecture if transferred to downstream tasks [18].,0,negative
Our model-based image augmentation method was implemented based on the official codes of the MAE [18] and ViTPose [21].,2,positive
"To solve the problem of loss and deformation of data acquired by clients, we used a masked image-encoding method that learns image representations corrupted by masking [7, 8].",1,neutral
Random sampling prevented bias in the unmasked area [8].,1,neutral
"Self-supervised learning has shown great success in both NLP [10], [11] and computer vision [12], [13].",1,neutral
"Following [13], we apply an end-to-end fine-tuning mechanism instead of linear probing to obtain the highest performance.",1,neutral
"Instead of random initialization, we initialize the ViT encoder using the ImageNet-1k pre-trained encoder released by [13].",2,positive
"As an alternative, MAE [13] proposes to directly reconstruct pixels of the masked patches, which is a more straightforward method natively designed for image modeling.",2,positive
"Among the various self-supervised vision learning methods, masked autoencoding [12], [13], [59] is used as our pre-training paradigm.",1,neutral
"(ii) MAE-IN1k refers to fine-tuned from the ImageNet-1k [14] pre-trained MAE [13], where we use the same fine-tuning settings as that of MAE-Face.",2,positive
"Among the various techniques proposed for self-supervised visual representation learning, we opt to adopt masked autoencoder (MAE) [13] as our baseline.",2,positive
"Specifically, the pre-training follows a masking-then-reconstruct procedure [13].",1,neutral
"Instead of the L2 loss proposed by [13], the model pre-trained using L1 loss exhibits better performance both in reconstruction and downstream tasks (see Section 4.",1,neutral
"We also test the impact of patch-wise normalization (w/ norm), which has also been studied in [13].",1,neutral
"Similar to the self-supervised pre-training tasks of NLP, some self-supervised tasks based on image transformers have also been proposed recently, such as MAE[18], BEiT[19], etc.",1,neutral
"CONCLUSIONS In this paper, based on MAE[16] and combined with convolution, we propose a transformer image inpainting model to solve the image inpainting problem of irregular missing areas.",2,positive
"In this paper, based on MAE, we propose a transformerbased inpainting model for irregular missing images.",2,positive
"As transformers[15] have made great breakthroughs in the field of NLP, some models with attention[16], [17] have also proved their strong performance in the field of cv. Similar to the self-supervised pre-training tasks of NLP, some self-supervised tasks based on image transformers have also been proposed recently, such as MAE[18], BEiT[19], etc.",1,neutral
"In [12], masked autoencoder was proposed using vision transformer to recover the original images even if some patches are masked.",1,neutral
"In the existing vision transformer [8][12], uniformly partitioned patches are used for an image, as illustrated in Fig.",1,neutral
1) peak signal-tonoise ratio (PSNR); 2) mean absolute error (MAE) [19].,1,neutral
"By utilizing MAE pretraining, ProContEXT outperforms the recent SOTA method, OStrack [5], by 0.9%, 1.5%, and 2.1% for AO, SR0.",2,positive
We implement the proposed ProContEXT using PyTorch and utilize the ViT-base backbone that is pre-trained by MAE [28].,2,positive
"Table 1: Comparison of State-of-the-Art (SOTA) Methods on TrackingNet [20] and GOT-10k [21] Datasets: The evaluated methods include ""DT"" (Dynamic Template) and ""EB"" (Extra Branch to update Dynamic Templates), along with different initialization methods, such as ""RandomNone"" and pre-training with additional datasets, such as ""CLIP-WIT [29]"", ""CLS-ImageNet-1k [30]"", ""CLS-ImageNet-22k [31]"", or ""MAE-ImageNet1k [28]"".",2,positive
Implementation Details We implement the proposed ProContEXT using PyTorch and utilize the ViT-base backbone that is pre-trained by MAE [28].,2,positive
"As mentioned in Section 1, we follow a similar approach to the work put forth in [7], where masked autoencoders reconstructed images with large amounts of masking.",2,positive
"Unlike [7], we don’t remove the masked patches from the input to the encoder and provide positional information to the input of the decoder.",2,positive
"Recently, there has been some major breakthroughs in image reconstruction using masked autoencoders [7], where an image can be realistically reconstructed with 90% of it being masked in patches.",1,neutral
"We denote the former as “speech branch” and the latter as “text branch”, where the former can be viewed as a masked autoencoder [28] and the latter is TTS.",1,neutral
"Recently, BEiT[33] and MAE[34] utilized a BERT-style pre-training as part of the visual learner, and they discovered that models are effective at learning semantics with such a scheme.",1,neutral
"Recently, BEiT [33] and MAE [34] utilize a BERT-style pre-training as part of the visual learner, and they discover that models are effective at learning semantics with such a scheme.",1,neutral
"It is worth noting that both DistilHuBERT and FitHuBERT are investigated under the contrained track on the SUPERB benchmark, which might not be able to reflect the potential effect of the distilled model as it does not fully explore the powerful modeling capacity lied in the Transformer encoder and merely treats it as a frozen feature extractor during the whole fine-tuning stage, missing the opportunity to pursue strong but non-linear features [19].",2,positive
"Specifically, we propose an SSL model, MAEEG, inspired by both BENDR and the MAE model proposed in computer vision [He et al., 2022], to compare with BENDR and to broaden our understanding of reconstruction-based SSL for EEG.",2,positive
"While this result may not be intuitive, the same trend was observed in computer vision where masking more of the signal during SSL pretraining could result in better downstream performance [He et al., 2022].",1,neutral
This study was inspired by MAE [1] for an MIM and Bootstrap Your Own Latent [12] (BYOL) as a framework for directly learning la-,2,positive
"Among them, Masked Autoencoders [1] (MAE) have inspired numerous subsequent studies and influenced not only the image domain [2–5] but also the audio domain [6–9].",1,neutral
"While we use the same positional encoding as MAE [1], we tested various masking ratios as discussed in Section 4.",2,positive
"Similar with MAE [15], the decoder of CAAE is only used in pre-training CAAE to perform image reconstruction task.",1,neutral
"As MAE [15] indicates, the design of the decoder architecture can be flexible and independent of the encoder design.",2,positive
"Unlike in [2], the model is trained to reconstruct the full image as a mixture of individual component reconstructions.",2,positive
"In this work, we capitalize in particular on the masked autoencoding of [2] and investigate using it for object-centric learning.",1,neutral
"Then, exactly as in MAE [2], we add a learnable mask token at the positions of the masked tokens and the sine-cosine position embeddings.",1,neutral
"We therefore choose the random masking strategy, exactly as in MAE [2].",1,neutral
"Another example is the object-reasoning-like results seen in the predictions of masked image-modelling frameworks [2, 3].",1,neutral
"Just as in MAE [2], we add fixed sine-cosine postional encodings from [23] to the embeddings of the patches.",1,neutral
Our model has a narrower bottleneck in comparison to MAE [2].,2,positive
"We hypothesize that the object-reasoning-like behaviour demonstrated to appear in the self-supervision task of masked image modelling [2, 3] can be utilized also for explicit object-centric representation learning.",1,neutral
Our broadcasting module takes inspiration both from the MAE decoder [2] and from the spatial broadcast decoder [25] used in several object-learning models [9–14].,2,positive
Our model is inspired by the transformer-based masked autoencoder [2] with modifications to enable end-to-end unsupervised multi-object segmentation and representation learning.,2,positive
"More specifically, we adapt the Masked Autoencoder (MAE) [2] design and modify it to for explicit object-centric representation learning and segmentation.",2,positive
This is in line with recent findings on training transformers that show the effectiveness of supervising multiple output tokens instead of just a single [CLS] token [73].,1,neutral
"PowerBERT is adopted from BERT model [16, 19, 20] to extract the high-dimensional representations from the massive unlabeled data.",1,neutral
[17] used Vision Transformers [7] for masked image inpainting and reconstruction.,1,neutral
"Denoising autoencoders [15] are a recent deep-learning method demonstrated to reconstruct original images from their corrupted versions across a variety of perturbations, from salt-and-pepper noise [16] to image masking [17].",1,neutral
"Similarly, while ViT has been shown to outperform CNNs on large datasets [17], we observe that ViT outputs contain uneven pixelation when decoding the 16 × 16 patch embeddings, resulting in its low SSIM score.",2,positive
"While [55, 8, 23, 5, 20, 10] have adopted CNN as pretraining backbones, recent works [6, 50, 11, 22, 29, 28] have explored Transformers [46] for self-supervised visual learning, demonstrating their superiority over traditional CNN.",1,neutral
"Despite no labels, models trained with self-supervision have outperformed their supervised counterparts on several downstream tasks, such as image classification [19, 55, 8, 23, 9, 5, 20, 10, 6, 50, 11, 22] and object detection [53, 7].",0,negative
"For MAE He et al. (2022), a method based on a reconstruction objective, we select an attention-based ViT encoder.",2,positive
"0 0.5 1
−0.6
−0.4
−0.2
0
0 0.5 1 0 0.5 1
0 0.5 1
−0.6
−0.4
−0.2
0 0.5 1
CLIPPretrained MAEPretrained MLPMixerPretrained1k MLPMixerPretrained21k ResNet50Pretrained1k ResNet50Pretrained21k SimCLRPretrained ViTPretrained1k ViTPretrained21k iBotPretrained1k iBotPretrained21k
Generalization when Lighting varies in training
percent of diverse Lighting training samples percent of diverse Lighting training samples percent of diverse Lighting training samples
percent of diverse Lighting training samples percent of diverse Lighting training samples
t o p - 1
a c c u r a c y d
r o p
t o p - 1
a c c u r a c y d
r o p",0,negative
"While all other models use either supervised or InfoNCE based objectives, MAE uses a reconstruction objective.",1,neutral
"In addition to the finetning results, we include here linear evaluation results for class generalization gaps A11.
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Translation_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 92.24% 91.37% 92.25% 94.01% 96.33% 77.78% 75.93% 68.52% 68.52% 58.89% 39.82% 57.70% 64.51% 65.45% 66.98% MAEPretrained 52.54% 55.93% 60.44% 67.91% 83.79% 20.37% 27.78% 33.33% 38.89% 27.78% 9.67% 12.91% 14.26% 15.56% 15.18% MLPMixerPretrained1k 94.66% 93.98% 94.58% 95.75% 97.25% 77.78% 74.07% 72.22% 66.67% 48.15% 36.86% 53.70% 59.35% 63.05% 63.46% MLPMixerPretrained21k 94.95% 95.09% 95.19% 96.02% 97.13% 75.93% 75.93% 74.07% 77.78% 70.37% 43.89% 67.36% 71.25% 73.69% 76.18% ResNet50Pretrained1k 95.00% 94.58% 94.83% 95.79% 97.23% 88.89% 90.74% 87.04% 83.33% 70.37% 44.35% 63.26% 68.99% 70.04% 70.01% ResNet50Pretrained21k 95.51% 95.30% 95.90% 96.27% 97.39% 77.78% 72.22% 74.07% 74.07% 70.37% 46.61% 68.04% 73.02% 75.90% 77.13% SimCLRPretrained 96.13% 95.74% 96.22% 96.82% 97.50% 81.48% 79.63% 81.48% 72.22% 70.00% 43.73% 62.96% 69.10% 70.63% 72.02% ViTPretrained1k 95.79% 96.18% 96.37% 96.80% 97.75% 88.89% 83.33% 77.78% 77.41% 75.93% 49.34% 67.92% 72.74% 75.65% 77.22% ViTPretrained21k 95.43% 95.01% 95.62% 96.39% 97.50% 83.33% 83.33% 83.33% 77.78% 72.22% 46.59% 67.21% 71.71% 74.02% 75.17% iBotPretrained1k 96.67% 96.43% 96.49% 97.01% 97.66% 81.48% 81.48% 79.63% 79.63% 72.22% 40.27% 65.23% 73.10% 76.06% 77.33% iBotPretrained21k 96.84% 96.30% 96.44% 96.92% 97.61% 90.74% 85.19% 87.04% 81.48% 72.22% 47.69% 70.55% 76.57% 78.53% 79.04%
Table A1: Position varying linear eval top-1 accuracy across multiple percentages of varying training instances.
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Rotation_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 91.70% 91.43% 92.42% 93.98% 96.28% 81.48% 85.19% 85.19% 79.63% 55.56% 28.64% 41.16% 44.72% 46.17% 48.85% MAEPretrained 53.90% 57.24% 61.28% 68.44% 83.90% 25.93% 44.44% 38.89% 42.59% 29.63% 6.68% 7.93% 8.58% 8.63% 8.36% MLPMixerPretrained1k 94.24% 93.76% 94.74% 95.72% 97.28% 74.07% 74.07% 72.22% 70.37% 46.30% 26.84% 39.47% 43.04% 46.51% 48.73% MLPMixerPretrained21k 94.49% 94.81% 95.03% 95.88% 97.13% 79.63% 77.78% 77.78% 79.63% 72.22% 36.90% 55.46% 61.57% 63.67% 65.50% ResNet50Pretrained1k 94.72% 94.56% 94.89% 95.74% 97.18% 85.19% 87.04% 85.19% 81.48% 68.52% 34.44% 46.22% 49.90% 51.73% 53.24% ResNet50Pretrained21k 95.51% 94.96% 95.69% 96.12% 97.30% 77.78% 75.93% 75.93% 72.22% 66.67% 40.29% 57.68% 61.83% 63.91% 65.04% SimCLRPretrained 95.74% 95.63% 96.16% 96.80% 97.51% 81.48% 83.33% 83.33% 83.33% 62.96% 32.94% 47.35% 52.88% 55.97% 56.91% ViTPretrained1k 95.71% 95.76% 96.09% 96.79% 97.67% 87.04% 79.63% 81.48% 79.63% 59.26% 39.13% 55.09% 60.14% 64.14% 65.42% ViTPretrained21k 95.23% 94.89% 95.68% 96.42% 97.45% 85.19% 83.33% 83.33% 83.33% 66.67% 38.25% 54.50% 58.34% 60.94% 62.28% iBotPretrained1k 96.39% 96.22% 96.41% 96.96% 97.56% 83.33% 81.48% 81.48% 79.63% 72.22% 34.62% 51.88% 58.88% 62.19% 63.11% iBotPretrained21k 96.44% 96.09% 96.42% 96.92% 97.61% 90.74% 88.89% 90.74% 87.04% 72.22% 41.03% 57.48% 63.69% 65.49% 67.34%
Table A2: Pose linear eval top-1 accuracy across multiple percentages of varying training instances.
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Spot hue_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 92.49% 91.60% 92.46% 94.06% 96.32% 77.78% 75.93% 70.37% 70.37% 59.26% 39.61% 60.78% 66.72% 68.27% 68.64% MAEPretrained 52.26% 55.43% 60.36% 67.78% 83.62% 22.22% 31.48% 37.04% 37.04% 20.37% 11.51% 15.09% 16.10% 18.91% 15.58% MLPMixerPretrained1k 95.17% 94.20% 94.98% 95.86% 97.30% 79.63% 77.78% 70.37% 66.67% 57.04% 40.09% 56.90% 64.43% 67.35% 68.93% MLPMixerPretrained21k 94.89% 95.29% 95.44% 96.13% 97.20% 81.48% 79.63% 72.22% 74.07% 76.30% 44.68% 69.30% 73.32% 76.12% 77.75% ResNet50Pretrained1k 95.26% 94.81% 95.03% 95.80% 97.23% 87.04% 88.89% 87.04% 83.33% 77.78% 44.08% 62.96% 68.67% 71.81% 72.54% ResNet50Pretrained21k 95.91% 95.45% 96.00% 96.28% 97.41% 77.78% 75.93% 75.93% 72.22% 71.11% 44.22% 67.22% 70.95% 72.38% 74.39% SimCLRPretrained 96.30% 95.91% 96.27% 96.84% 97.59% 79.63% 75.93% 74.07% 74.07% 66.67% 43.36% 63.22% 71.32% 73.72% 72.26% ViTPretrained1k 95.99% 96.36% 96.54% 96.95% 97.80% 90.74% 87.04% 83.33% 81.48% 74.07% 52.53% 69.53% 71.81% 76.01% 77.28% ViTPretrained21k 95.57% 95.39% 95.84% 96.51% 97.59% 85.19% 81.48% 83.33% 77.78% 75.93% 46.01% 67.50% 71.97% 75.75% 77.06% iBotPretrained1k 96.50% 96.55% 96.74% 97.12% 97.70% 79.63% 81.48% 81.48% 77.78% 74.07% 42.32% 68.61% 72.75% 76.28% 78.04% iBotPretrained21k 97.01% 96.42% 96.65% 97.04% 97.64% 88.89% 87.04% 83.33% 83.33% 75.93% 48.83% 73.37% 78.09% 80.39% 81.55%
Table A3: Spot hue linear eval top-1 accuracy across multiple percentages of varying training instances.
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Scale_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 91.81% 91.51% 92.26% 93.90% 96.25% 79.63% 72.22% 74.07% 70.37% 61.11% 36.80% 51.29% 57.05% 56.99% 58.80% MAEPretrained 51.98% 56.15% 60.87% 68.07% 84.12% 25.93% 35.19% 33.33% 37.04% 35.19% 7.01% 10.74% 11.05% 11.29% 11.44% MLPMixerPretrained1k 94.27% 93.55% 94.47% 95.59% 97.17% 79.63% 77.78% 70.37% 68.52% 53.70% 32.50% 46.15% 51.66% 55.30% 56.62% MLPMixerPretrained21k 94.86% 94.98% 95.08% 95.93% 97.09% 79.63% 79.63% 75.93% 75.93% 74.07% 40.55% 60.87% 66.63% 67.52% 70.03% ResNet50Pretrained1k 94.89% 94.57% 94.84% 95.66% 97.16% 87.04% 90.74% 90.74% 83.33% 72.22% 39.22% 54.23% 59.10% 61.27% 60.89% ResNet50Pretrained21k 95.51% 95.28% 95.77% 96.08% 97.35% 81.48% 77.78% 77.78% 74.07% 74.07% 41.11% 60.06% 66.41% 69.69% 70.33% SimCLRPretrained 95.91% 95.53% 96.09% 96.66% 97.48% 83.33% 77.41% 77.78% 72.22% 62.96% 39.79% 54.93% 61.81% 62.93% 62.96% ViTPretrained1k 95.65% 96.01% 96.18% 96.69% 97.69% 87.04% 83.33% 79.63% 75.93% 72.22% 44.10% 58.13% 63.91% 67.85% 68.82% ViTPretrained21k 95.06% 94.87% 95.47% 96.30% 97.38% 83.33% 83.33% 83.33% 81.48% 72.22% 41.40% 58.53% 63.25% 65.43% 65.14% iBotPretrained1k 96.58% 96.37% 96.48% 96.98% 97.60% 84.07% 77.78% 79.63% 77.78% 66.67% 41.34% 58.93% 67.24% 68.66% 69.08% iBotPretrained21k 96.92% 96.18% 96.39% 96.86% 97.53% 85.19% 77.78% 81.48% 81.48% 75.93% 44.59% 63.11% 68.90% 71.45% 70.82%
Table A4: Scale linear eval top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Background path_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 90.37% 91.88% 92.73% 94.36% 96.70% 85.19% 77.78% 77.78% 77.78% 66.67% 41.30% 54.70% 59.96% 61.75% 63.41% MAEPretrained 51.51% 56.16% 62.31% 67.08% 84.32% 27.78% 31.48% 37.04% 37.04% 29.63% 9.47% 12.21% 11.84% 13.30% 12.67% MLPMixerPretrained1k 92.84% 93.82% 94.81% 95.41% 97.38% 75.93% 77.78% 72.22% 74.07% 61.11% 37.39% 49.19% 52.41% 55.75% 56.74% MLPMixerPretrained21k 95.42% 95.33% 95.62% 96.52% 97.55% 83.33% 81.48% 75.93% 75.93% 77.78% 48.30% 64.37% 66.79% 70.15% 70.84% ResNet50Pretrained1k 94.35% 95.12% 94.83% 95.93% 97.46% 90.74% 92.59% 88.89% 88.89% 83.33% 44.89% 59.87% 64.15% 66.70% 67.29% ResNet50Pretrained21k 95.20% 96.40% 95.89% 96.65% 97.67% 81.48% 79.63% 77.78% 77.78% 75.93% 51.21% 65.75% 69.47% 72.01% 73.90% SimCLRPretrained 95.50% 95.98% 96.14% 96.45% 97.52% 83.33% 83.33% 81.48% 77.78% 72.22% 44.33% 59.82% 65.39% 67.93% 68.93% ViTPretrained1k 95.72% 96.42% 96.27% 96.57% 97.95% 94.44% 88.89% 88.89% 87.04% 77.78% 50.62% 64.09% 67.14% 72.33% 73.19% ViTPretrained21k 94.91% 95.22% 96.09% 96.45% 97.71% 83.33% 83.33% 83.33% 83.33% 77.78% 49.36% 64.21% 67.42% 70.77% 72.13% iBotPretrained1k 96.42% 96.58% 96.60% 97.40% 97.28% 88.89% 83.33% 87.04% 81.48% 79.63% 44.58% 62.59% 68.10% 71.20% 73.36% iBotPretrained21k 96.16% 96.14% 96.38% 97.37% 97.27% 88.89% 90.74% 90.74% 83.33% 81.48% 51.20% 68.58% 71.57% 74.62% 75.94%
Table A5: Background path linear eval top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Translation_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 97.49% 97.00% 97.25% 97.55% 96.80% 87.04% 90.74% 77.78% 81.48% 75.93% 45.33% 69.74% 74.53% 77.72% 78.19% MAEPretrained 96.75% 96.63% 97.20% 97.46% 97.91% 83.33% 74.07% 66.67% 64.81% 72.22% 29.17% 51.35% 61.05% 65.16% 70.07% MLPMixerPretrained1k 96.95% 96.73% 97.17% 97.24% 97.88% 88.89% 77.78% 77.78% 74.07% 64.81% 44.65% 64.56% 70.67% 74.34% 75.95% MLPMixerPretrained21k 97.71% 97.69% 97.90% 97.98% 98.35% 85.19% 88.89% 85.19% 81.48% 77.78% 46.53% 70.54% 77.27% 79.78% 81.68% ResNet50Pretrained1k 97.97% 97.92% 97.91% 97.86% 98.27% 87.04% 87.04% 72.22% 81.48% 81.48% 45.05% 64.83% 71.87% 76.56% 79.63% ResNet50Pretrained21k 97.54% 97.62% 97.74% 97.69% 98.20% 88.89% 83.33% 83.33% 83.33% 77.78% 52.22% 70.96% 75.78% 81.10% 82.45% SimCLRPretrained 97.40% 97.57% 97.68% 97.81% 98.12% 90.74% 77.78% 87.04% 83.33% 77.78% 45.21% 68.73% 74.59% 76.55% 79.86% ViTPretrained1k 97.80% 97.88% 98.00% 97.92% 98.28% 90.74% 90.74% 85.19% 81.48% 81.48% 49.30% 71.82% 76.95% 80.39% 82.58% ViTPretrained21k 97.80% 97.59% 97.89% 97.91% 98.25% 87.04% 88.89% 81.48% 81.48% 87.04% 45.83% 71.80% 75.51% 79.96% 81.86% iBotPretrained1k 97.77% 97.55% 97.64% 97.77% 98.06% 88.89% 85.19% 79.63% 75.93% 79.63% 45.69% 68.94% 74.76% 76.63% 79.83% iBotPretrained21k 97.97% 97.83% 97.88% 97.92% 98.13% 88.89% 87.04% 88.89% 81.48% 79.63% 49.84% 70.97% 78.13% 82.53% 84.07%
Table A6: Position finetuning top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Rotation_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 97.60% 97.01% 97.30% 97.58% 97.97% 88.89% 88.89% 85.19% 83.33% 72.22% 34.29% 57.01% 62.67% 65.58% 68.45% MAEPretrained 96.87% 96.75% 97.25% 97.52% 97.95% 75.93% 68.52% 62.96% 68.52% 62.96% 22.64% 45.24% 50.87% 54.18% 53.82% MLPMixerPretrained1k 96.75% 96.58% 97.08% 97.25% 97.86% 83.33% 85.19% 83.33% 77.78% 64.81% 33.29% 51.43% 55.55% 58.78% 59.35% MLPMixerPretrained21k 97.68% 97.65% 97.90% 97.90% 98.35% 87.04% 87.04% 77.78% 77.78% 75.93% 38.93% 62.76% 67.97% 71.90% 73.47% ResNet50Pretrained1k 98.05% 97.81% 97.89% 97.93% 98.24% 84.81% 81.48% 81.48% 81.48% 75.93% 33.29% 53.51% 62.56% 64.45% 67.47% ResNet50Pretrained21k 97.52% 97.45% 97.65% 97.61% 98.18% 85.19% 79.63% 83.33% 87.04% 85.19% 37.45% 59.85% 65.39% 70.66% 73.13% SimCLRPretrained 97.37% 97.43% 97.53% 97.74% 98.07% 87.04% 87.04% 83.33% 87.04% 75.93% 35.50% 55.87% 64.34% 66.79% 69.34% ViTPretrained1k 97.94% 97.87% 97.98% 97.95% 98.31% 88.89% 87.04% 85.19% 77.78% 75.93% 40.13% 62.66% 68.53% 71.32% 73.36% ViTPretrained21k 97.68% 97.61% 97.90% 97.97% 98.27% 88.89% 79.63% 83.33% 74.07% 70.74% 39.75% 61.42% 68.39% 71.51% 73.76% iBotPretrained1k 97.49% 97.55% 97.56% 97.75% 98.02% 88.89% 77.78% 83.33% 75.93% 68.52% 36.30% 60.69% 65.98% 68.22% 70.00% iBotPretrained21k 98.00% 97.79% 97.96% 98.01% 98.19% 88.89% 87.04% 83.33% 85.19% 72.22% 38.86% 62.35% 69.18% 71.96% 73.84%
Table A7: Pose finetuning top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Spot hue_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 97.66% 97.13% 97.40% 97.62% 97.98% 90.74% 88.89% 87.04% 87.04% 81.48% 49.99% 70.06% 75.12% 82.36% 82.48% MAEPretrained 97.04% 96.67% 97.20% 97.41% 97.95% 79.63% 77.78% 72.22% 74.07% 68.52% 30.10% 54.63% 60.74% 66.78% 69.34% MLPMixerPretrained1k 97.32% 96.92% 97.20% 97.36% 97.89% 87.04% 83.33% 77.78% 79.63% 72.22% 48.80% 65.60% 70.82% 76.88% 78.59% MLPMixerPretrained21k 97.80% 97.83% 97.99% 97.99% 98.42% 88.89% 87.04% 81.48% 75.93% 79.63% 49.81% 73.15% 75.23% 79.01% 82.35% ResNet50Pretrained1k 98.02% 97.99% 98.03% 97.98% 98.28% 88.89% 87.04% 83.33% 81.48% 77.78% 48.48% 67.72% 74.65% 76.90% 75.83% ResNet50Pretrained21k 97.68% 97.60% 97.87% 97.79% 98.24% 90.74% 87.04% 83.33% 77.78% 75.93% 54.35% 71.69% 76.70% 81.03% 81.86% SimCLRPretrained 97.60% 97.61% 97.72% 97.87% 98.17% 90.00% 76.30% 85.19% 77.78% 74.07% 45.58% 70.36% 79.09% 78.06% 81.75% ViTPretrained1k 97.68% 97.93% NaN 98.00% 98.37% 94.44% 88.89% NaN 81.48% 81.48% 54.38% 70.94% NaN 82.53% 83.94% ViTPretrained21k 97.77% 97.68% 97.94% 98.00% 98.24% 92.59% 85.19% 77.78% 78.15% 81.48% 52.49% 72.55% 76.58% 79.75% 82.39% iBotPretrained1k 97.91% 97.58% 97.76% 97.88% 98.08% 88.89% 81.48% 79.63% 75.93% 74.07% 48.67% 69.24% 75.01% 79.44% 80.81% iBotPretrained21k 98.25% 97.87% 97.88% 97.96% 98.13% 90.74% 83.33% 92.59% 79.63% 83.33% 49.39% 74.42% 80.67% 83.05% 85.02%
Table A8: Spot hue finetuning top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Scale_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 97.68% 97.08% 97.39% 97.63% 97.95% 90.74% 90.74% 87.04% 83.33% 79.63% 45.47% 66.39% 72.46% 75.63% 78.05% MAEPretrained 96.81% 96.64% 97.19% 97.40% 97.91% 81.48% 70.37% 70.37% 70.37% 53.70% 28.70% 51.73% 60.66% 62.49% 64.47% MLPMixerPretrained1k 97.23% 96.60% 97.07% 97.24% 97.82% 87.04% 85.19% 83.33% 74.07% 70.37% 41.62% 60.01% 65.78% 70.24% 71.64% MLPMixerPretrained21k 97.80% 97.80% 97.96% 97.95% 98.37% 85.19% 87.04% 81.48% 72.22% 77.78% 46.02% 68.37% 73.08% 76.51% 77.38% ResNet50Pretrained1k 97.88% 97.94% 97.95% 97.89% 98.24% 87.04% 85.19% 81.48% 75.93% 79.63% 44.47% 62.86% 71.55% 73.81% 75.80% ResNet50Pretrained21k 97.40% 97.58% 97.84% 97.72% 98.18% 90.74% 79.63% 81.48% 79.63% 79.63% 49.37% 68.72% 70.89% 76.85% 79.18% SimCLRPretrained 97.57% 97.54% 97.65% 97.83% 98.10% 88.89% 83.33% 83.33% 83.33% 74.44% 42.25% 65.04% 71.88% 74.89% 76.25% ViTPretrained1k 97.80% 97.92% 98.05% 97.92% 98.34% 88.89% 83.33% 85.19% 79.63% 79.63% 44.17% 65.86% 71.66% 77.46% 78.46% ViTPretrained21k 97.77% 97.71% 97.85% 97.99% 98.24% 85.19% 85.19% 85.19% 79.63% 79.63% 42.63% 67.71% 70.61% 74.96% 76.14% iBotPretrained1k 97.85% 97.61% 97.74% 97.79% 98.04% 90.74% 87.04% 83.33% 83.33% 79.63% 45.14% 64.09% 71.34% 73.90% 76.99% iBotPretrained21k 97.88% 97.75% 97.86% 97.97% 98.12% 88.89% 87.04% 87.04% 81.48% 75.93% 48.70% 65.52% 72.39% 79.16% 80.04%
Table A9: Scale finetuning top-1 accuracy across multiple percentages of varying training instances
train_canonical_top_1_accuracy val_canonical_top_1_accuracy val_diverse_Background path_top_1_accuracy train_prop_to_vary 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 0.05 0.25 0.50 0.75 0.95 model CLIPPretrained 96.49% 97.05% 97.52% 97.81% 98.01% 94.44% 88.89% 92.59% 87.04% 81.48% 50.79% 67.24% 76.25% 79.79% 80.92% MAEPretrained 96.20% 96.61% 97.31% 97.63% 97.99% 81.85% 72.22% 77.78% 72.22% 62.96% 30.49% 52.04% 64.77% 66.67% 68.56% MLPMixerPretrained1k 96.16% 97.03% 97.13% 97.66% 97.76% 90.74% 87.04% 81.48% 75.93% 72.22% 47.41% 63.84% 71.85% 73.76% 75.96% MLPMixerPretrained21k 98.08% 98.15% 98.09% 98.33% 98.71% 88.89% 92.59% 90.74% 85.19% 79.63% 51.84% 72.00% 77.05% 80.46% 81.10% ResNet50Pretrained1k 97.71% 97.76% 97.95% 98.04% 98.38% 92.59% 92.59% 90.74% 92.59% 83.33% 46.50% 63.48% 70.90% 73.73% 77.72% ResNet50Pretrained21k 97.38% 98.09% 97.72% 98.33% 98.34% 88.89% 83.33% 87.04% 81.48% 85.19% 50.87% 71.59% 76.21% 79.13% 83.24% SimCLRPretrained 97.38% 97.31% 97.77% 97.55% 98.05% 77.78% 87.04% 88.89% 90.74% 83.33% 43.32% 61.47% 69.94% 76.99% 79.07% ViTPretrained1k 98.12% 97.76% 97.89% 97.73% 98.44% 88.89% 90.74% 87.04% 81.48% 83.33% 54.25% 71.56% 75.73% 80.58% 84.92% ViTPretrained21k 97.77% 97.49% 97.95% 98.04% 98.37% 91.67% 88.89% 90.74% 87.04% 81.48% 55.17% 73.53% 76.50% 82.28% 81.94% iBotPretrained1k 97.47% 97.44% 97.61% 98.15% 97.86% 88.89% 92.59% 90.74% 81.48% 79.63% 51.19% 71.17% 75.32% 78.37% 79.93% iBotPretrained21k 97.69% 97.91% 97.77% 98.17% 97.93% 93.52% 92.59% 90.74% 85.19% 83.33% 55.00% 73.11% 76.62% 83.56% 82.90%
Table A10: Background path finetuning top-1 accuracy across multiple percentages of varying training instances",0,negative
", 2019), Masked AutoEncoders (MAE, (He et al., 2022)), or ViTs Dosovitskiy et al.",2,positive
"0 0.5 1
−0.8
−0.6
−0.4
−0.2
0
0 0.5 1 0 0.5 1
0 0.5 1
−0.6
−0.4
−0.2
0 0.5 1
CLIPPretrained MAEPretrained MLPMixerPretrained1k MLPMixerPretrained21k ResNet50Pretrained1k ResNet50Pretrained21k SimCLRPretrained ViTPretrained1k ViTPretrained21k iBotPretrained1k iBotPretrained21k
Generalization when Lighting varies in training
percent of diverse Lighting training samples percent of diverse Lighting training samples percent of diverse Lighting training samples
percent of diverse Lighting training samples percent of diverse Lighting training samples
t o p - 1
a c c u r a c y d
r o p
t o p - 1
a c c u r a c y d
r o p",0,negative
"0 50 100 −40
−20
0 50 100 0 50 100 0 50 100 0 50 100
CLIP MAE MLPMixer1k MLPMixer21k ResNet50-1k ResNet50-21k SimCLR ViT-1k ViT-21k iBot-1k iBot-21k best fit
Percent of varying images across all instances
0 50 100 −40
−20
0
0 50 100 0 50 100 0 50 100 0 50 100",0,negative
"To test this,
0 10 20 30
Position gap
Pose gap
Lighting gap
Size gap Background gap CLIP MAE MLPMixer1k MLPMixer21k ResNet50-1k ResNet50-21k SimCLR ViT-1k ViT-21k iBot-1k iBot-21k
Relative top-1 accuracy drop when (all factors vary - no factors vary) (linear eval)
0 10 20 30 40
Position gap
Pose gap
Lighting gap
Size gap Background gap CLIP MAE MLPMixer1k MLPMixer21k ResNet50-1k ResNet50-21k SimCLR ViT-1k ViT-21k iBot-1k iBot-21k
Relative top-1 accuracy drop when (all factors vary - no factors vary) (finetuning)
0 10 20 30 40
Position gap
Pose gap
Lighting gap
Size gap Background gap CLIP MAE MLPMixer1k MLPMixer21k ResNet50-1k ResNet50 21k SimCLR ViT-1k ViT-21k iBot-1k iBot-21k
Relative top-1 accuracy drop when (all factors vary - no factors vary) (finetuning)
0 10 20 30 40
Position gap
Pose gap
Lighting color gap
Size gap Background gap CLIP MAE MLPMixer1k MLPMixer21k ResNet50-1k ResNet50-21k SimCLR ViT-1k ViT-21k iBot-1k iBot-21k
Relative top-1 accuracy drop when (all factors vary - no factors vary) (finetuning)
0 10 20 30
Position gap
Pose gap
Lighting color gap
Size gap Background gap CLIP MAE MLPMixer1k MLPMixer21k ResNet50-1k ResNet50-21k SimCLR ViT-1k ViT-21k iBot-1k iBot-21k
Relative top-1 accuracy drop when (all factors vary - no factors vary) (linear eval)
0 10 20 30
Position gap
Pose gap
Lighting gap
Size gap Background gap CLIP MAE MLPMixer1k MLPMixer21k ResNet50-1k ResNet50-21k SimCLR ViT-1k ViT-21k iBot-1k iBot-21k
Relative top-1 accuracy drop when (all factors vary - no factors vary) (linear eval)
Figure 6: Varying all factors during training improves robustness We show show relative generalization gaps when all factors vary during training relative to no instances seeing varying (no variability).
we trained models with increasing amounts of variability for a single factor and evaluated the robustness of other factors.",0,negative
"Using the typical evaluation procedures of self-supervised models (Caron et al., 2021; Dosovitskiy et al., 2021; Chen et al., 2020b) (finetuning and linear evaluation), we examine robustness across a catalog of state-of-the-art vision architectures, such as CLIP (Radford et al., 2021) that have significantly outperformed earlier models on robustness benchmarks such as ObjectNet (Barbu et al., 2019), Masked AutoEncoders (MAE, (He et al., 2022)), or ViTs Dosovitskiy et al. (2021)) among others.",2,positive
"As shown in Table 1 (b), the MAE model is especially susceptible to changes in background, with a-44.",1,neutral
"Learning objective is more impactful than architecture for robustness In general, we found that robustness was similar across models with the notable exception of MAE.",1,neutral
"…a catalog of state-of-the-art vision architectures, such as CLIP (Radford et al., 2021) that have significantly outperformed earlier models on robustness benchmarks such as ObjectNet (Barbu et al., 2019), Masked AutoEncoders (MAE, (He et al., 2022)), or ViTs Dosovitskiy et al. (2021)) among others.",2,positive
We use pre-trained weights from the official repo of He et al. (2022).,0,negative
MAE is also substantially more sensitive to position and lighting color.,1,neutral
"In our experiment we deploy our Lie operator on top of the Variance-Invariance-Covariance Regularization model (VICReg, (Bardes et al., 2021)), and of the very recent masked autoencoders model (MAE, (He et al., 2021)).",2,positive
"We incorporate our proposed Lie operator in two recent SSL approaches: masked autoencoders (MAE, (He et al., 2021)), and Variance-Invariance-Covariance",2,positive
", 2021)), and of the very recent masked autoencoders model (MAE, (He et al., 2021)).",2,positive
"We incorporate our proposed Lie operator in two recent SSL approaches: masked autoencoders (MAE, (He et al., 2021)), and Variance-Invariance-Covariance
Evaluate Robustness
Self-Supervised Lie Operator
Training Data
Regularization (VICReg (Bardes et al., 2021)) to directly model transformations in…",2,positive
", 2020), computer vision (Dosovitskiy et al., 2020; He et al., 2022) and related fields (Lu et al.",1,neutral
"…between vision and language using a pre-trained multimodal transformer (Geng et al., 2022), which is a large masked autoencoding transformer (Vaswani et al., 2017; He et al., 2022) jointly trained on image-text (Changpinyo et al., 2021; Thomee et al., 2016) and text-only data (Devlin et al., 2018).",2,positive
"It consists of two modules: a pre-trained multimodal masked autoencoder (He et al., 2022; Geng et al., 2022) to encode instructions and visual observations, and a transformer-based (Vaswani et al., 2017) policy that predicts actions.",2,positive
", 2022), which is a large masked autoencoding transformer (Vaswani et al., 2017; He et al., 2022) jointly trained on image-text (Changpinyo et al.",2,positive
"It consists of two modules: a pre-trained multimodal masked autoencoder (He et al., 2022; Geng et al., 2022) to encode instructions and visual observations, and a transformer-based (Vaswani et al.",2,positive
"Transformers (Vaswani et al., 2017) have led to significant gains in natural language processing (Devlin et al., 2018; Brown et al., 2020), computer vision (Dosovitskiy et al., 2020; He et al., 2022) and related fields (Lu et al., 2019; Radford et al., 2021; Geng et al., 2022).",1,neutral
"In addition to the differentiation of the network structure pre-trained on ImageNet, we include models with a variety of pre-trained strategies, including SimCLR [6], MoCov2 [8] and
1https://pytorch.org/vision/stable/index.html 2https://github.com/rwightman/pytorch-image-models 3https://github.com/open-mmlab
BYOL [21] for ResNet50, MoCov3 [9] and MAE [26] for ViT-B.",2,positive
"BYOL [21] for ResNet50, MoCov3 [9] and MAE [26] for ViT-B.",1,neutral
The ViT pre-training2 is analogous to the image reconstruction task proposed in MAE [45]: to reconstruct the masked image patches from visible ones.,2,positive
The most popular pretraining scheme for ViTs is called Masked Autoencoders (MAE) [45].,1,neutral
5× compared with the original MAE [45]3 and enables us to scale up ViTs with greater model capability (Figure 1a).,0,negative
The optimal masking ratio is related to the information redundancy in the data: BERT [55] uses a masking ratio of 15% for language and MAE [45] uses a ratio of 75% for images.,1,neutral
"Moreover, studies in both CNNs [87, 113] and ViTs [45, 101] indicate that alternative loss functions (e.",1,neutral
"Following this spirit, we take masked autoencoders (MAE) [45] as pre-training task for its simplicity, efficiency, scalability, and compelling performance.",2,positive
This design reduces time and memory complexity [45]: a masking ratio of 90% (used in our paper) can reduce the encoder complexity to <1/10.,2,positive
"Recently, self-supervised learning [24, 13, 16, 4, 15] has shown remarkable results in representation learning.",1,neutral
"Particularly, among popular pretext tasks are Contrastive Learning (CL) [14, 16, 38, 56, 75] and Masked Image Modeling (MIM) [7, 35, 76] for Convolutional Nets (ConvNet) [37, 47] and vision transformers [25, 69, 72], respectively.",1,neutral
"where LMIM is the MIM loss such as the prediction error used in the MAE [35] when reconstructing missing matches, p[u,v ] is the relative positional embedding with a 2D index of [u, v], g(xi − x j + δx ) and g(yi − y j + δy) are the continuous indexing function for x− and y−coordinate respectively, andM is the index of masked patches.",1,neutral
"A straight idea is to find the worst-case perturbations by jointly maximizing the contrastive loss and the MAE loss of a MIM task, and to use the obtained adversarial perturbations to train a robust representation by minimizing the joint
, Vol. 1, No. 1, Article .",1,neutral
"Publication date: October 2022.
where L𝑀𝐼𝑀 is the MIM loss such as the prediction error used in the MAE [35] when reconstructing missing matches, p[𝑢,𝑣 ] is the relative positional embedding with a 2D index of [𝑢, 𝑣], 𝑔(𝑥𝑖 − 𝑥 𝑗 + 𝛿𝑥 ) and 𝑔(𝑦𝑖 − 𝑦 𝑗 + 𝛿𝑦) are the continuous indexing function for 𝑥− and 𝑦−coordinate respectively, andM is the index of masked patches.",2,positive
"Defined on the image level, the distance can be the pixel-wise mean squared error as in MAE [35].",1,neutral
"However, due to its natural connection with the pretraining of language transformers [8, 23], the Masked Image Modeling (MIM) [7, 35, 76] has attracted extensive attentions recently to pre-train the vision transformers.",1,neutral
"Moreover, Masked Image Modeling (MIM) [7, 35, 76] has attracted increasing attentions for pretraining vision transformers.",1,neutral
"Then one adopts a Masked Auto-Encoder (MAE) [35] reconstruction error
L𝑚𝑎𝑒 (xm, x;𝜃 ) = D(I𝜃 (xm), x) with a distance measure D between reconstructed and original images to pre-train the network 𝜃 .",1,neutral
Then one adopts a Masked Auto-Encoder (MAE) [35] reconstruction error,1,neutral
"They have pushed forward the state of the art in multiple domains (Chowdhery et al., 2022; He et al., 2022).",2,positive
", 2021), and the patches themselves (Dosovitskiy et al., 2020; He et al., 2022).",2,positive
"For example, MAE (He et al., 2022) relies on further fine-tuning to purify image features.",1,neutral
"Motivated by such success, computer vision methods begin to apply Transformer (Vaswani et al., 2017) architectures, and operate on the sequences of raw pixels (Chen et al., 2020a), discrete patch tokens (Bao et al., 2021), and the patches themselves (Dosovitskiy et al., 2020; He et al., 2022).",2,positive
"Transformer framework was first proposed in [15], achieved huge success in natural language process tasks [16], [17] and was recently adopt in image processing tasks [18], [19].",1,neutral
"As the most effective scheme for few-shot learning, the ”pretraining & fine-tuning” paradigm has achieved great success in NLP and CV [11, 12, 13, 14, 15, 16, 17], but no researchers have tried it in spectral detection.",1,neutral
"On the other hand, many researchers [12, 13, 14, 26, 28] have also conducted in-depth research on the application of pre-trained models in the CV field.",1,neutral
"Even though transformers have shown better localization properties than convolutional networks, especially in the self-supervised setting [4, 22, 33], the few studies so far on vision transformers for image retrieval are limited to using the [CLS] token from the last layer of ViT as a global representation [12, 4, 14].",1,neutral
"For student networks, the learning objective is to recover the masked token in feature domain, as the successful progress made by MAE, random mask is implied to learn more generalized body feature.",1,neutral
"As a milestone of visual MIM, MAE [4] could almost recover the general content of the image under the mask rate of more than 75%.",2,positive
"Then inspired by DINO [3] and MAE [4], this paper formulate mask image modeling as knowledge distillation, as most of ViT-based backbone network did, the image pairs are cropped into patches with 16× 16 pixel.",2,positive
"He et al. (2021a), also motivated by the label-deficiency issue in federated learning, developed a series of self-supervised FL algorithms that incorporated the advances of supervised FL, especially those algorithms with personalization, to handle the heterogeneity in data.",1,neutral
"…of unlabeled data and achieved tremendous successes for a wide range of downstream tasks in computer vision (He et al., 2020; Chen et al., 2020; He et al., 2021b), natural language processing (Devlin et al., 2018; Sarzynska-Wawer et al., 2021), and embodied intelligence (Sermanet et al., 2018;…",2,positive
"Very recently, there have been a few contemporaneous/concurrent attempts (He et al., 2021a; Zhuang et al., 2021; 2022; Lu et al., 2022; Makhija et al., 2022) that bridged unsupervised/self-supervised learning and decentralized learning, with focuses on designing better algorithms that mitigate the…",2,positive
"…al. (2022) requires access to an unlabeled public dataset for all models to measure the distance, and requires the communication of some datasets; He et al. (2021a) experiments with multiple methods to do personalization in decentralized learning, and our clustering-based approach can be viewed…",2,positive
"For masked autoencoder (He et al., 2021b) experiment on CIFAR-10, we use Vit-Tiny (Dosovitskiy et al., 2020) with AdamW optimizer for 1000 epochs with batch size 256.",2,positive
"In particular, the works He et al. (2021a); Zhuang et al. (2021; 2022); Lu et al. (2022); Makhija et al. (2022) are closest to ours.",2,positive
"To the best of our knowledge, there have only been a few contemporaneous/concurrent attempts (Zhang et al., 2020a; He et al., 2021a; Zhuang et al., 2021; 2022; Lu et al., 2022; Makhija et al., 2022) that bridged unsupervised/self-supervised learning with unlabeled data and decentralized learning,…",2,positive
"Other significant examples of SSL include masked auto-encoding in language (Devlin et al., 2018) and vision (He et al., 2021b).",1,neutral
"Recently, the self-supervised learning methods, SimMIM [34], UM-MAE [19], BEiT [2], MAE [13], SplitMask [12], MoCo v3 [9], and DINO [5], are effective in pretraining Transformers [11, 22] for learning visual represenar X iv :2 21 0.",2,positive
"Hence, a high masking ratio (75% (He et al., 2022)) may not be necessary to suppress the amount of information the encoder sees, and we consequently attempt lower ratios of 50%, 60% to introduce more information about the subordinate target.",0,negative
"…as pre-text based methods (Doersch et al., 2015; Zhang et al., 2016; Gidaris et al., 2018), contrastive learning with Siamese networks (Oord et al., 2018; He et al., 2020; Chen et al., 2020; Henaff, 2020), masked image modeling (MIM) (He et al., 2022; Bao et al., 2022; Xie et al., 2022), and etc.",1,neutral
"In this work, we focus on MAE (He et al., 2022), which proposes to use a high masking ratio and a non-arbitrary ViT decoder.",2,positive
"We notice that recent work in the literature (He et al., 2022; Bao et al., 2022) performs many experiments in masking strategies, but to the best of our knowledge, we are the first to introduce image mixtures in the pre-training of MIM.",2,positive
"For better classification performance, we use normalized pixels (He et al., 2022) and a high masking ratio (0.75); for better visual reconstructions, we use a lower masking ratio (0.5) without normalizing target pixels.",2,positive
"Vanilla MAE exhibits strong signs of semantics understanding (He et al., 2022).",1,neutral
"Among them, MIM has shown a preponderant advantage in performance, and the representative method Masked Autoencoders (MAE) (He et al., 2022) has attracted much attention in the field.",2,positive
", 2020; Henaff, 2020), masked image modeling (MIM) (He et al., 2022; Bao et al., 2022; Xie et al., 2022), and etc.",2,positive
"Linear Probing: For linear evaluation, we follow MAE (He et al., 2022) to train with no extra augmentations and use zero weight decay.",1,neutral
", in MAE [19]), and is used to generate latent representation of a full image.",1,neutral
"Though MAE and iBOT are both strong MIM-based methods, TEC can still further improve them with the proposed target-enhanced conditional MIM scheme, verifying its effectiveness.",2,positive
"In SSL, a pretext task is first built, e.g., instance discrimination task [20, 8] or masked image modeling (MIM) [2, 19], and then pseudo labels are generated via the pretext task to train a network model without requiring manual labels.",1,neutral
"Instead, our patch-dim normalization stresses the relations among patches, which is compatible to the patch-prediction in the MIM scheme.",1,neutral
", iBOT models with more category semantics while MAE models with more image details [19].",1,neutral
", instance discrimination task [20, 8] or masked image modeling (MIM) [2, 19], and then pseudo labels are generated via the pretext task to train a network model without requiring manual labels.",1,neutral
"We therefore choose a mask-reconstruction [19] SSL scheme for the pretraining of the new model, in which the base model generates reconstruction targets from the full input images and the new model tries to predict base model targets from random masked image input.",2,positive
"After normalization, following [19], new model uses an fully-connected layer followed by the decoder to generate Zf for predicting the base model target Yf on masked patches: Lfea = ∥M ◦ (Yf − Zf )∥(2)2, (4) where M is the mask matrix and ◦ denotes the element-wise product.",1,neutral
"Under the mask-reconstruction [19] framework, TEC consists of a randomly initialized new ViT encoder to be pretrained, conditional adapters for conditional pretraining, and a multi-target decoder for reconstruction targets prediction, an SSL pretrained ViT encoder as the base model and an target-enhancing module to generate patch-relation enhanced reconstruction base model targets.",2,positive
"1 Introduction Self-supervised learning (SSL) has achieved overwhelming success in unsupervised representation learning, with astonishing high performance in many downstream tasks like classification [50, 51], object detection and segmentation [2, 19].",1,neutral
"TEC follows [19, 2], and uses Vision Transformer (ViT) [14] for implementation.",2,positive
"For MIM, this patch-dim normalization can better enhance the spatial relations among tokens than the widely used feature normalization [41, 39, 1] along channel dimension.",1,neutral
"Therefore, it is the relation among patches that helps the MIM training.",1,neutral
We then propose to utilize the self-attention maps as a type of reconstruction targets for MIM to further enhance the semantic relation modeling capability of the new model.,2,positive
"[19] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
", MoCo [20] trained with 200 epochs while MAE [19] with 16,00 epochs to release its potential.",0,negative
"With the plain backbone pretrained as a MAE [21], our method achieves 4.",2,positive
"Recently, the original ViT has been reintroduced as a competitive backbone for semantic segmentation [8] and object detection [25], with the aid of MAE [21] pretraining and window attention.",1,neutral
"Instead, we simply use the readily available pretrained MAE weights from [21].",0,negative
The three backbones were pretrained on ImageNet-1k as MAEs [21].,2,positive
Backbone PretrainingOur backbonemodels are pretrained as MAEs [21] on ImageNet-1K [11].,2,positive
This simple self-supervised approach turns out to be an efficient and scalable way to pretrain ViT models [21].,1,neutral
"Different from Masked Image Modeling (MIM), which is to reconstruct the masked contents for learning good representation, the Masked Siamese Networks will not predict the information in removed areas, so erasing will only lose information and is not desired in the learning procedure.",1,neutral
"This is different from vision transformer based MIM methods [20, 3] that will recover the masked regions by operating on the pixel space using rear X iv :2 21 0.",1,neutral
[20] proposed a simple transformer-based Masked Autoencoder (MAE) architecture that employs Mean Squared Error (MSE) loss to reconstruct the original image.,1,neutral
"• We show that block-wise masking provides superior performance on Masked Siamese ConvNets to the discrete random masking, commonly used in selfsupervised representation learning frameworks [20, 24, 1].",1,neutral
"Our observation on Masked Siamese ConvNets is opposite to that in MIM methods, which found discrete/random masking is better [20, 39].",1,neutral
He et al. [20] proposed a simple transformer-based Masked Autoencoder (MAE) architecture that employs Mean Squared Error (MSE) loss to reconstruct the original image.,1,neutral
"However, this approach simply employs the masking scheme from MIM [20, 3] without adapting it to the peculiarities of Siamese ConvNet.",1,neutral
This is quite different from the mechanism of Masked Autoencoders (MAE) [20] that predict masked areas to learn good representations.,1,neutral
Such independent patch-wise processing of an image allows to simply drop masked patches thus decreasing the computational cost [20].,1,neutral
"Recently, Masked Image Modeling (MIM) [20, 3, 1] has emerged and proven to be an effective approach to learning useful representation.",1,neutral
"Furthermore, instead of using discrete random masking that is applied in [20], we found using block-wise masking will give better performance since it can better preserve global information important for contrastive loss.",1,neutral
"Spatiotemporal representation learning using MAE was proposed in [15] by Feichtenhofer et al. Several works [18, 2, 16] have explored ways to adapt MAEs to handle multimodal data.",1,neutral
"This observation is different from MIM-based approaches [20, 39], where a discrete mask achieves better performance and highlights the importance of maintaining global features when generating an image mixture as opposed to the case when erasing parts of the image by performing a vanilla masking operation.",1,neutral
"This is different from vision transformer based MIM methods [20, 3] that will recover the masked regions by operating on the pixel space using rear X iv :2
21 0.",1,neutral
"The comparisons are made in Table 5 with previous methods based on Siamese networks [12, 29, 10, 11, 31], and also recently proposed masked auto-encoding methods [30, 90] tailored for Transformers.",1,neutral
"In this sense, SSL [12, 31, 10, 29, 15, 14, 86, 30] has seen great progress in computer vision, and can achieve competitive or even better performance on various downstream tasks compared to its supervised counterparts.",1,neutral
"In computer vision, however, there emerge a much wider variety of pretext tasks [87, 88, 52, 83, 20, 80, 55, 19, 30, 4, 42], since images contain much more information than languages so that there are much more intrinsic structures to be mined as free learning signals Solving pretext tasks requires the model to understand the visual concepts present in images and thus useful representations can be learned.",1,neutral
model MEC iBOT [90] MAE [30] DINO [11] MoCo v3 [15] SimCLR [12] BYOL [29] SwAV [10],0,negative
"MAE [31] presents a masked autoencoder for representation learning, which masks random patches from the input image and trains an encoder to reconstruct the masked patches.",1,neutral
"Meanwhile, MAE [31] is a representative restorative method for ViTs.",1,neutral
"However, such methods are generally less competitive in direct discriminative representation learning tasks, as assessed by linear evaluation and k-NN classification [31], [33].",1,neutral
"Recently, masked image modeling [31], [32] has gained great research attention as an SSL paradigm for ViTs.",1,neutral
"1) Evalutation protocols: Following previous works [14], [28], [31], we adopt three common evaluation protocols, namely fine-tuning evaluation, linear evaluation and k-NN classification [43], to assess the performance of each pretrained model.",2,positive
"Model pre-training and fine-tuning have been shown effective in many vision tasks [17, 46, 67].",1,neutral
"This finding is in line with those in recent model pre-training studies [17, 46, 67].",0,negative
"We also collect 7 ViT-S models with different training regimes, including the original ViT training setup [26]6, a stronger data augmentation setup in the Deit paper [89]-36, the training setup with distillation [89]-36, an improved DeiT training setup [90]-36 , and selfsupervised training fashions by MoCo v3 [12]7, MAE [38]8 and BYOL [33]9.",0,negative
", MOCO [12], MAE [38] and BYOL [33]) or semi-weakly supervised learning [103].",1,neutral
"Similarly, we investigate the effect of different types of supervision, such as self-supervision (e.g., MOCO [12], MAE [38] and BYOL [33]) or semi-weakly supervised learning [103].",1,neutral
"For the low-level target, ViT (Dosovitskiy et al., 2020), MAE (He et al., 2022), SimMIM (Liu et al., 2022b), ConvMAE (Gao et al., 2022), HiViT (Zhang et al., 2022) and GreenMIM (Huang et al., 2022) utilize the original or normalized pixels as the MIM target.",2,positive
"Following MAE (He et al., 2022) and BEiT v2 (Peng et al.",1,neutral
", 2020) Pixel ViT FC / N/A MAE (He et al., 2022) Pixel ViT Decoder LayerNorm `2 SimMIM (Liu et al.",2,positive
"Following MAE (He et al., 2022) and BEiT v2 (Peng et al., 2022), we test the robustness of MaskDistill on three ImageNet validation sets, i.e., ImageNet-Adversarial (Hendrycks et al., 2021b), ImageNet-Rendition (Hendrycks et al., 2021a) and ImageNet-Sketch (Wang et al., 2019).",2,positive
"After that, the main difference between previous work lies in the architecture design (He et al., 2022; Chen et al., 2022a) and reconstruction targets (He et al.",2,positive
"MAE (He et al., 2022) also introduces per-patch normalized pixels (i.e., layer normalization without affine transformation) as the target to boost local pixels contrast, resulting in better performance.",2,positive
"After that, various target supervision has been explored under the masked image modeling framework, such as original or normalized pixels (He et al., 2022; Dong et al., 2021; Liu et al., 2022b; Gao et al., 2022; Liu et al., 2022a; Zhang et al., 2022; Huang et al., 2022), high-level features (Wei et al.",2,positive
", 2022a) and reconstruction targets (He et al., 2022; Liu et al., 2022b; Wei et al., 2021; 2022a; Baevski et al., 2022).",0,negative
"After that, the main difference between previous work lies in the architecture design (He et al., 2022; Chen et al., 2022a) and reconstruction targets (He et al., 2022; Liu et al., 2022b; Wei et al., 2021; 2022a; Baevski et al., 2022).",0,negative
"MAE (He et al., 2022) introduces a decoder to decouple the masked prediction task from the encoder.",2,positive
"After that, various target supervision has been explored under the masked image modeling framework, such as original or normalized pixels (He et al., 2022; Dong et al., 2021; Liu et al., 2022b; Gao et al., 2022; Liu et al., 2022a; Zhang et al., 2022; Huang et al., 2022), high-level features (Wei et…",2,positive
"MAE (He et al., 2022) also introduces per-patch normalized pixels (i.",1,neutral
The key difference between anomaly detection and ordinary benchmarks where MAE excel is that anomaly detection is an unsupervised task.,2,positive
We compare between DINO [5] and MAE [11] as a representation for a kNN based anomaly detection algorithm.,2,positive
It intended to convey that even for easy tasks the MAE doesn’t achieve as good results as DINO.,2,positive
"Recently, masked-autoencoder (MAE) based methods achieved significant improvements on several self-supervised representation learning benchmarks [11].",1,neutral
MAE’s optimization objective may explain why its strong representation does not translate into better anomaly detection capabilities.,2,positive
A comparison between MAE to contrastive self-supervised method (DINO) is presented in Tab.,1,neutral
"Yet, the representations learnt by MAE underperform contrastive self-supervised methods on unsupervised tasks such as anomaly detection.",1,neutral
"For MAE, we experimented both with kNN and reconstruction error for anomaly scoring and found that the latter works badly, therefore we report just the kNN results.",2,positive
"This is also suggested by MAE’s worse performance with linear probing (as reported by the original paper), where the supervised labels cannot be used to improve the backbone representations.",1,neutral
"As MAE’s objective is to reconstruct patches, it may learn a representation that encodes local information needed for reconstructing the image, overlooking semantic object properties.",2,positive
"A.1 Anomaly detection comparison of MAE and DINO
We compare between DINO [5] and MAE [11] as a representation for a kNN based anomaly detection algorithm.",2,positive
Regressing RGB values normalized by the mean and standard deviation within each patch has proven to be effective for MAE [38].,1,neutral
This optimal ratio is higher than what was found for instance in MAE [38] for the auto-completion task.,1,neutral
"In particular, the masked autoencoder (MAE) [38] accelerates pre-training by using an asymmetric architecture that consists of a large encoder that operates only on unmasked patches followed by a lightweight decoder that reconstructs the masked patches from the latent representation and mask tokens.",2,positive
"Reminiscent of denoising autoencoders [78] or context encoders [60], and aiming to reconstruct masked pixels [3, 16, 26, 30, 38, 85], discrete tokens [7, 95] or deep features [5, 82], these methods have demonstrated the ability to scale to large datasets and models and achieve state-of-the-art results on various downstream tasks.",1,neutral
"This is due to the MSE loss, but as noted in MAE [38], beyond a certain point, sharper reconstructions do not necessarily lead to better pre-training.",0,negative
"In practice these models are typically trained on object-centric datasets such as ImageNet [67] and thus tend to learn high-level semantic information; that makes them well suited for tasks such as image classification or object detection [4, 38, 47].",1,neutral
"In particular, we compare to DINO [14], a state-of-the-art self-supervised method based on instance discrimination, and to MIM methods with MAE [38] and MultiMAE [4].",2,positive
"We follow the exact same protocol as MAE [38] for that, with global average pooling for CroCo as we did not include a [CLS] token in our model.",2,positive
"More recently, Masked Image Modeling (MIM) [7, 16, 19, 32, 38, 60, 93] has emerged as a powerful alternative for self-supervision.",1,neutral
"Our model is trained using a simple pixel reconstruction loss over all masked patches, similar to MAE [38].",2,positive
"” In the past decade, representation learning has made significant progress in representing high-dimensional sensory signals, such as images and audio, to reveal the geometric and semantic structures hidden in raw signals (Bengio et al., 2013; Chen et al., 2018; Kornblith et al., 2019; Chen et al., 2020; Baevski et al., 2020; Radford et al., 2021; Bardes et al., 2021; Bommasani et al., 2021; He et al., 2022; Chen et al., 2022).",1,neutral
"…images and audio, to reveal the geometric and semantic structures hidden in raw signals (Bengio et al., 2013; Chen et al., 2018; Kornblith et al., 2019; Chen et al., 2020; Baevski et al., 2020; Radford et al., 2021; Bardes et al., 2021; Bommasani et al., 2021; He et al., 2022; Chen et al., 2022).",1,neutral
"Note that the random masking as a regularization technique has also been successfully used in reconstruction-based self-supervised learning [24, 74].",1,neutral
The head architecture is based on MAE and is composed of two layers: BatchNormalization(affine=False) and Linear.,2,positive
"Therefore, we used ArcFace [9] to train a projection head composed of two layers, BatchNormalization and Linear. this architecture is based on MAE [13].",1,neutral
this architecture is based on MAE [13].,1,neutral
"…tries to maximize the agreement between positive pairs (Chen et al., 2020; He et al., 2020; Grill et al., 2020), or clustering-based methods to generate pseudo labels for data (Caron et al., 2020; 2018), or mask autoencoder to predict the masked patches by the remaining patches (He et al., 2022).",1,neutral
", 2020; 2018), or mask autoencoder to predict the masked patches by the remaining patches (He et al., 2022).",2,positive
"In order to increase robustness to such varying resolution, we utilize up to 2⇥ higher resolution images during training but randomly drop 80% of visual tokens to minimize additional compute overhead (similar to [38, 52]).",2,positive
"Self-supervised models pretrained on ImageNet-1k (He et al., 2022).",2,positive
"Table 1: Token Merging ablation experiments using ViT-L/16 from MAE (He et al., 2022) on ImageNet-1k evaluated off-the-shelf without training, using r = 8.",2,positive
"MAE (He et al., 2022) is a self-supervised pretraining method for ViT with models pretrained and fine-tuned on ImageNet-1k.",2,positive
"…making them faster than their raw flop count would suggest; they support powerful self-supervised pre-training techniques such as MAE (He et al., 2022) that can put up state-of-the art results while being fast to train; given their lack of assumptions about the data, they can be…",2,positive
"We perform several experiments on ImageNet-1k (Deng et al., 2009) using ViT models trained in four different ways: AugReg (Steiner et al., 2022), MAE (He et al., 2022), SWAG (Singh et al., 2022), and DeiT (Touvron et al., 2021).",2,positive
"Yet, vanilla ViTs still have many desirable qualities: they consist of simple matrix multiplications, making them faster than their raw flop count would suggest; they support powerful self-supervised pre-training techniques such as MAE (He et al., 2022) that can put up state-of-the art results while being fast to train; given their lack of assumptions about the data, they can be applied with little or no changes across many modalities (Feichtenhofer et al.",1,neutral
"…in methods that leverage large amount of unlabeled data in domains such as speech, vision and language to produce state-of-the-art results, e.g. Baevski et al. (2020; 2022); Chen et al. (2020a); Caron et al. (2021); He et al. (2022); Cai et al. (2022); Brown et al. (2020); Ramesh et al. (2021).",2,positive
"Bert [8] solves this problem by a small proportion of random token in the mask, and MAE [16] embed latent representation without mask token in the encoder to avoid this problem.",1,neutral
"Different from MAE [16], our encoder operates on the full set.",2,positive
"It has recently achieved incredible success in pre-training vision Transformers [16], motivating researchers to extend its application to molecular graphs.",2,positive
High mask ratio was originally applied in images to encourage learning effective semantic information [16].,1,neutral
"A high mask ratio offers a challenging reconstruction task, which requires the learned model to capture numerous correlations among the tokens and thus encourages learning effective structure information [16].",1,neutral
"Various self-supervised pretrain tasks have been proposed for ViT and applied in downstream tasks [2, 6, 10].",1,neutral
"Transformer structure has been reported to work better than the customized convolutional neural networks or recurrent networks for both vision and language tasks [11, 17], thereby implying the potential as a unified cross-modal encoder.",1,neutral
"For image representation, the supervised model ViT [12] and the self-supervised BEiT [1] and MAE [17] also prove the effectiveness of the transformer in learning visual semantics.",1,neutral
"Although the transformer is good at learning semantic representations, researchers have found that it requires larger scale supervision [11, 12, 17] than the customized networks.",1,neutral
MAE [17] adopts a similar pre-training scheme as BERT and predicts the masked regions of an image with unmasked ones.,1,neutral
"Both in the domains of NLP and computer vision, these large models [13, 56, 68, 25, 94, 95] achieve enormous performance improvements compared to the small-scale models and provide pre-trained weights for downstream tasks.",1,neutral
"The experiments reveal that MAE and U-Net are the best shape denoising methods we evaluated for all six
types of noise.",1,neutral
"The EBM, U-Net, Deeplabv3+ and MAE were trained with clean shapes as well as shapes perturbed by all types of noise except the detection noise.",2,positive
"The paper evaluated seven methods from different areas that could be used for shape denoising: Active Shape Model (ASM) as a classical segmentation method, two generative models based on Boltzmann Machines (Deep Boltzmann Machine (DBM) and Convolutional DBM), another generative model named Energy Based Model (EBM), and three deeplearning based models used for object segmentation: U-Net, DeepLabv3+ and Masked Autoencoder (MAE).",2,positive
"U-Net [11], DeepLabv3+ [12], masked autoencoder (MAE) [13], etc.",2,positive
"We evaluate seven shape denoising methods (ASM [6], DBM [8], CDBM [9], EBM [10], U-Net [11], Deeplabv3+ [12], MAE [13]) for denoising shapes corrupted by the six types of noise introduced in Section 2.",2,positive
"We evaluate seven shape denoising methods (ASM [6], DBM [8], CDBM [9], EBM [10], U-Net [11], Deeplabv3+ [12], MAE [13]) for denoising shapes corrupted by the six types of noise introduced in Section 2.2.",2,positive
"Moreover, deep learning based methods for semantic segmentation can also be used for shape modeling, e.g. U-Net [11], DeepLabv3+ [12], masked autoencoder (MAE) [13], etc. 1.1.",1,neutral
"Recently, view-based self-supervised vision transformers (ViTs) have revealed emerging properties that have not been shown in either the supervised ViTs or previous unsupervised CNNs, attracting a lot of research interest in the community [6], [7], [8], [9], [10].",1,neutral
"Concurrent BERT [35] like approachs (BEiT [36], MAE [10], CAE [37]) propose to reconstruct raw pixels via mask image modeling.",1,neutral
[15] show that the mask ratio has a decisive influence on the downstream performance of MAE.,1,neutral
"Beside the popular contrastive learning methods [4, 14, 12, 27], recently there has been a renaissance of reconstruction-based autoencoders for SSL, for example, MAE [15], BEiT [1], iBOT [32], and SimMIM [29], which demonstrate state-of-theart performance on various downstream tasks.",1,neutral
"For the decoder, we use a flexible one following [15].",1,neutral
"We mainly follow the basic setup of MAE [15]: for the encoder, we adopt different variants of ViT [10], i.",2,positive
"Specifically, MAE adopts a simple mean square error (MSE) loss: LMAE(h) = Ex̄Ex1,x2|x̄ ‖g(f(x1))− x2‖ 2 , (2) where the decoder output x̂2 = h(x1) = g(f(x1)) is assumed to be l2-normalized following the original paper of MAE saying that normalized target x2 yields better performance [15].",1,neutral
", BERT [9]), also shows promising results in visual representation learning, to name a few, BEiT [1], iBOT [32], SimMIM [29], and MAE [15].",1,neutral
We begin by introducing a mathematical formulation of MAE [15].,1,neutral
"As an implication of this theorem, a small U-MAE loss would provably imply a small downstream classification error, which helps explain the good downstream generalization ability of MAE [15].",1,neutral
"[15], the patchwise masking strategy is a key component that distinguishes MAE from standard autoencoders, and different mask ratios ρ have a large impact on the downstream performance of pretrained features of MAE.",2,positive
"Secondly, MAE discards the masked image blocks at the input layer, only extracts the features of the non-masked image blocks, and then adds the mask information to form the image features [31].",2,positive
As inpainter we use a publicly available Masked AutoEncoder (MAE) [21] trained with an adversarial loss.,2,positive
The MAE [21] we use is based on a ViT architecture and has been pre-trained in an adversarial fashion (as opposed to the standard training with an MSE loss) to output more realistic-looking details,2,positive
(MAE) [21] with an adversarial loss to increase the details of the reconstructed images.,0,negative
"Following the recent trend of methods for unsupervised object segmentation [7–12, 22], we build our method on top of SSL features, and, in particular, DINO [4] or MAE [21] features.",2,positive
"Our approach is compared not only with MAE [9] and CPC [7], but also with SimCLR [7], BYOL [7], and MoCov2 [16].",2,positive
"This may be somewhat related to the network (transformer) used in MAE, so although we use a masked representation learning approach similar to MAE, we improve the network structure to make it more adaptable to the learning of ECG representations.",2,positive
An asymmetric encoder-decoder structure is used in MAE [9].,1,neutral
"From Table I, we can know that although MAE has excellent performance in computer vision, it is not able to learn good ECG representations.",2,positive
"This recent trend began with natural language processing when BERT [10] and successive large pretrained language models [31, 7] were released and quickly gained popularity in other domains such as computer vision [19, 9, 11] and graph learning [54, 22, 33].",1,neutral
"A mask-noise is usually used to perturb the images and those approaches predict the masked inputs either at the pixel level (Dosovitskiy et al., 2020; He et al., 2021; Xie et al., 2019) or at a token-level, using a pixel (often patch-level) tokenizer (Bao et al., 2021; Wei et al., 2021).",1,neutral
"We also compare the performance of those models to instance-based methods such as MAE (He et al., 2021) and data2vec (Baevski et al., 2022), which do not employ volume maximization regularizers.",2,positive
"The MAE method employs a simple pixel-reconstruction loss for representation learning, and thus does not explicitly compute mini-batch statistics during pretraining.",1,neutral
"We also compare the performance of those models to instance-based methods such as MAE (He et al., 2021) and data2vec (Baevski et al.",2,positive
"Auto-regressive models, and denoising auto-encoders, in particular, predict clean visual inputs from noisy views (Chen et al., 2020a; Vincent et al., 2010; He et al., 2021; Bao et al., 2021; Baevski et al., 2022).",1,neutral
Our pretraining follow the standard hyperparameters defined in He et al. (2021).,2,positive
"Tables 5, 6,7, 8 and 9, and show the results of SimCLR, MSN, VICReg, data2vec and MAE on the CIFAR100, CIFAR100 1%, Place205, Clevr/Count, Clevr/Dist and KITTI downstream tasks.",0,negative
"We pretrain a ViT-B/16 with data2vec (He et al., 2021) using the AdamW optimizer with a batch size of 2048 for 800 epochs using the official code base, which is publicly available: http: //github.com/facebookresearch/data2vec_vision/tree/main/beit.",2,positive
"By contrast, evaluations with instanced-based methods data2vec and MAE in Table 1 show different trends.",1,neutral
"We pretrain a ViT-L/16 with MAE (He et al., 2021) using the AdamW optimizer with a batch size of 1024 for 800 epochs using the official code base, which is publicly available: https:// github.com/facebookresearch/mae.",2,positive
MAE.,0,negative
"Masking patches individually works well for the original MAE [19]; but for a significantly longer sequence, it can bring about delicacies and degenerate the task even if the same percentage of tokens are masked [48].",1,neutral
"Our long-sequence MAE is a simple and minimallychanged extension of MAE [19], which we summarize first.",1,neutral
"Fortunately, pre-training, especially variants of Masked Autoencoding (MAE) [12, 19], have risen as a domainagnostic approach to reduce overfitting and scale models.",1,neutral
"Pioneered by earlier efforts [6,14,35,43], many recent methods [1, 9, 13, 19, 33, 46, 48, 52] have revisited this idea as a highly effective solution for visual representation learning.",1,neutral
"Notably, MAE [19] employs an explicit encoder-decoder architecture, and drops (instead of ‘replaces’ [1, 12]) tokens for the heavier encoder.",1,neutral
"Extending its success to computer vision, MAE [19] again demonstrates strong model scalability by directly pre-training on raw pixels.",2,positive
"conducted in MAE [19], the same model size is used for both stages), whereas for input dimensions, we can easily change them due to the extensive weight-sharing used in modern model architectures.",2,positive
"This not only helps a clean, scientific understanding in contrast to prior studies that scale both [4,10,28], but also offers a more efficient, practical solution compared to scaling supervised transfers alone [13, 19, 48].",1,neutral
"Compared to [19], the sequence length is increased to 4× (compute is also ∼4×), resulting from an enlarged image.",1,neutral
"Finally, the output sequence of the decoder is used to predict the normalized pixel values [19] in the masked patches.",1,neutral
"Unsupervised representation learning has shown great success in computer vision [27] and natural language processing [10]; however, one challenge is that RL includes both behavior learning and representation learning [32, 53].",1,neutral
"In [22, 59], the input patches are masked and the network is tasked to predict the masked pixels.",1,neutral
"Self-supervised learning: In recent years, several self-supervised techniques have been proposed to pre-train ViTs [1, 3, 22, 36, 59, 66].",1,neutral
"And the patches encoded from input images containing high-level understanding of parts, objects, and scenes [8].",1,neutral
"redundancy, which is unsuitable for representation learning [15].",1,neutral
"Linear evaluation misses the opportunity of pursuing strong but non-linear features, which is indeed a strength of deep learning [15].",1,neutral
"Prior works [5, 37, 84] have successfully introduced the mask-and-predict scheme in NLP [9,23] to pre-train an image transformer.",1,neutral
"To help the model better model the trajectory, we adopt the patch norm method to normalize the trajectory values in each patch into a standard normal distribution, as has also been done in MAE [37].",1,neutral
"First, as the appearance information can be well reconstructed in a single image with an extremely high masking ratio (85% in MAE [37]), it is also feasible to be reconstructed in the tube-masked video frame-by-frame and neglect to learn important temporal clues.",1,neutral
"Since an image with a high masking ratio (85% in MAE [37]) can be well reconstructed [37,75], we conjecture that the masked appearance information of a video can also be reconstructed frame by frame independently.",1,neutral
"These methods vary in different reconstruction objectives, including raw RGB pixels [37], handcrafted local patterns [75], and VQ-VAE embedding [5], all above are static appearance information in images.",1,neutral
"For computation efficiency, we follow MAE [37] to only feed the unmasked patches (and their positions) to the encoder.",2,positive
"2) Pixel: predicting all the pixels of each 3D patch [28, 37, 64].",1,neutral
"Recently, BEiT and MAE [5, 37] show two excellent mask image modeling paradigms.",1,neutral
"We consider two supervised feature backbones: ResNet50 [25] and ViT-B/16 [18], and four selfsupervised backbones: SimCLR [13], MAE [23], MSN [4] and DINO [11].",2,positive
"We consider two supervised feature backbones: ResNet50 [16] and ViT-B/16 [13], and four self-supervised backbones: SimCLR [9], MAE [17], MSN [2] and DINO [7].",2,positive
"[17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"long-range interactions on sequential image patches [63], [64], [65], [66].",1,neutral
"According to a recent study [60], self-supervised learning can be as effective as or even superior to supervised learning in various computer vision tasks.",1,neutral
The random mixing and block-wise mixing strategies are inspired by MAE [18] and BEiT [3] and we replace the masking operation with image mixing on patch-level and block-level (both of size 16×16) respectively.,2,positive
"Inspired by MAE [18] and BEiT [3], we implement a random mixing strategy and block-wise mixing strategy.",2,positive
"With the success of Transformers and Bidirectional Embedding Representation for Transformer (BERT) on natural language processing (NLP), the idea of attention mechanism and pre-training and fine-tuning has also been shown effective in the convolutional network, graph network, and RS [18, 21, 22, 31, 32].",1,neutral
"Unsupervised pre-training on big datasets succussed in most tasks in NLP and CV [18, 31, 32] but is studied slightly in RS.",1,neutral
"In particular, we compare to two approaches: R3M [41], which utilizes the Ego4D dataset of human videos to obtain a representation, and MVP [46, 56], which trains a masked auto-encoder [16] on the Bridge Dataset and utilizes the learned latent space as the representation of the new image.",2,positive
"Some other prior approaches that attempt to leverage large, diverse datasets via representation learning [36, 58, 59, 41, 16, 56, 35], as well as other methods for learning from human demonstrations, such as behavioral cloning methods with expressive policy architectures [47].",1,neutral
", 2022), and recent works even explore to use LMs as a general-purpose interface for diverse modalities (Hao et al., 2022; Xie et al., 2022; He et al., 2022).",2,positive
"…(LMs) have demonstrated great success on a wide range of natural language tasks (Devlin et al., 2018; Brown et al., 2020b; Chowdhery et al., 2022), and recent works even explore to use LMs as a general-purpose interface for diverse modalities (Hao et al., 2022; Xie et al., 2022; He et al., 2022).",1,neutral
"For a fair comparison, we compare our DMAE ViT-B model with the MAE ViT-B checkpoint released by He et al. (2022) in the linear probing setting on ImageNet.",2,positive
"The other class is the generative learning approach, which randomly masks patches in an image and learns to generate the original one (Bao et al., 2021; He et al., 2022).",1,neutral
"Following He et al. (2022); Xie et al. (2022), we use ImageNet-1k as the pre-training corpus which contains 1.28 million images.",2,positive
"Like MAE (He et al., 2022), we adopt the asymmetric encoder-decoder design for DMAE.",2,positive
"To enforce the encoder (but not the decoder) to learn robust semantic features, we control the capacity of the decoder by setting a smaller value of the hidden dimension and depth following He et al. (2022).",2,positive
"For the pre-training of the two DMAE models, we set the masking ratio to 0.75 following He et al. (2022).",2,positive
"…auto-encoder (Vincent et al., 2008; 2010) or denoising diffusion model (Ho et al., 2020; Nichol & Dhariwal, 2021) to pre-train θdenoiser, and leverage contrastive learning (Chen et al., 2020; He et al., 2020) or masked image modelling (He et al., 2022; Xie et al., 2022) to pre-train θencoder.",2,positive
", 2020) or masked image modelling (He et al., 2022; Xie et al., 2022) to pre-train θencoder.",2,positive
"Motivated by the Masked AutoEncoder (MAE) (He et al., 2022), which learns latent representations by reconstructing missing pixels from masked images, we design a new self-supervised task called Denoising Masked AutoEncoder (DMAE).",2,positive
"The compared MAE[9] method implemented in this paper follows a similar design as our MV-SSTMA to fit the EEG data, calculating the spatial attention between EEG channels but removing the multi-view and temporal aspects of MV-SSTMA.",2,positive
• MAE[9]: Masked autoencoders as scalable self-supervised learners by reconstructing the missing patches in images for computer vision.,1,neutral
"Nevertheless, MAE cannot be directly applied well with EEG data
considering the complication of EEG signals, and it doesn’t take temporal properties into account.",1,neutral
"Furthermore, the MAE method also exceeds baselinemethods on SEED-IV, whereas performsworse than some supervised models on SEED.",2,positive
He et al. [9] presented masked autoencoders (MAE) as scalable self-supervised learners for computer vision to reconstruct the missing patches in images.,1,neutral
"In the case of only few labeled data for calibration, the proposed MV-SSTMA is evaluated with the self-supervised method MAE and the supervised methodMD-AGCN.",2,positive
[9] presented masked autoencoders (MAE) as scalable self-supervised learners for computer vision to reconstruct the missing patches in images.,1,neutral
"The Paired t-test is also conducted on the performance of MVSSTMA and MAE for all subjects in all situations above, as well as performance of MV-SSTMA and MD-AGCN.",0,negative
"In addition, our model outperforms MAE and MD-ADCN in every scenario.",2,positive
[9] proposed a masked autoencoder for self-supervised learning in the computer vision area achieving excellent performance.,1,neutral
"Moreover, as the temporal information is also considered in the NoHybrid model and SingleScale model, their performance are still better than MAE.",1,neutral
"[18]) are stochastic and can significantly alter the distribution of the data [19, 16, 20].",1,neutral
"data masking [16], cutout [17], mixup Both senior authors contributed equally.",2,positive
"2 show that random mask [16], cutout [17] and our new random rotation augmentation yield comparable generalization error for a wide range of hyperparameters (masking probability, cutout width and rotation angle respectively); the random rotation is a new augmentation proposed in this work and frequently beats ridge regularization as well as interpolation.",2,positive
"This type of augmentation has been widely used in practice [16, 67]8, and is a simplified version of the popular cutout augmentation [17].",1,neutral
"An example is as follows: while the classical noise injection augmentation [21] causes only a constant shift in the spectrum, data masking [16, 24], cutout [17] and distribution-preserving augmentations [15] tend to isotropize the equivalent data spectrum.",1,neutral
", random-masking [16], cutout [17], noise injection [21], and group-invariant augmentations [15].",1,neutral
"2 Comparisons of different types of augmentations In this section, we compare the generalization of three canonical augmentations that we analyzed in this work: 1) Gaussian noise injection [21], 2) random mask [16], and 3) random rotation (which we introduced in Section 3.",1,neutral
"In our framework, we allow for cases in which augmentation leads to significant changes in distribution and provide a path to analysis for such OOD augmentations that encompass empirically popular DA [16, 17].",2,positive
"3, we simulate the biased and unbiased random mask augmentation [16] and test their performance in regression and classification tasks.",2,positive
"Dropout also induces implicit regularization by randomly dropping out intermediate neurons (rather than covariates, as does the random mask [16] augmentation) during the learning process, and has been shown to have a close connection with sparsity regularization [64].",1,neutral
"Note that a natural constraint r ≤ m holds, and the original MAE setting [14] can be regarded as the special case when r = m.",1,neutral
"Examples include MAE [14], SimMiM [61], and BEiT [2] for images, and [10, 11, 52] for videos.",1,neutral
"As observed in the recent self-supervised pre-training work, such as MAE [14] for images and [10, 52] for videos, it is sufficient to use only a small fraction of the input visual tokens to reconstruct the visual signal.",1,neutral
"Inspired by MAE [14], we also use a shallow decoder with much fewer Transformer blocks than the encoder (Φdec(.",2,positive
"such as NLP [28] and vision [29], [30].",1,neutral
"We only calculate the loss on the mask tokens, which is consistent with MAE and SimMIM.",2,positive
"Different from the MAE [8] which utilizes a multilayer converter structure as decoder, we only employ a single-layer MLP is used as decoder.",2,positive
"More recently, Masked Auto-Encoder (MAE) [8] predicts random masked patches of the input image and reconstructs the missing pixels by an autoencoder, and this method has obtained superior performance for the image classification task.",1,neutral
"Given an image-text pair (𝐼 ,𝑇 ), we random sample 25% of the image patches, the other 75% image patches will not be fed into the models following MAE[11].",2,positive
"Given an image-text pair (I ,T ), we random sample 25% of the image patches, the other 75% image patches will not be fed into the models following MAE[11].",2,positive
"Recent research explores intriguing properties of vision transformers [59, 65], jointly modeling multiple modalities [60, 90], and inpainting via masked autoencoders [29, 64].",1,neutral
"The image reconstruction and patch masking follow [12], the difference is that the masking probability is set to 1/3 for ease of implementation.",1,neutral
"Moreover, some works [2, 13, 40] have proven the masking mechanism is efficient to facilitate model to focus on the masked contents, and significantly enhancing the reasoning ability of model.",1,neutral
"The encoder and decoder of PA-former adopt the same structure following [11], in which the encoder transforms the input sub-sampled signal into latent representation and the encoder maps the latent representation back to the full-sampled data.",1,neutral
"Although preliminary studies have shown the promising application of image-based Transformer in classification [26, 15, 14], segmentation [41, 32, 49, 23] or detection [7, 50], transformers do not have inductive bias for images which means that they can not perform well on a small-scale dataset [8].",1,neutral
", raw pixel [16, 43], HOG features [42], visual tokens [2] from a pre-trained dVAE [33], visual tokens from a momentum model [48], etc.",1,neutral
Some masked image modeling methods such as MAE [16] adopt an asymmetric encoder-decoder architecture.,1,neutral
"Most recently, inspired by MAE [14], several concurrent works, e.g., M3AE [10] and VLC [13], transfer the pixel reconstruction task into VLP by simply adding pixel reconstruction tasks on top of VLP models.",1,neutral
"For example, BERT [17] formulates masked language modeling (MLM) to predict masked linguistic tokens, while MAE [14] and BEiT [2] formulate masked image modeling (MIM) to reconstruct raw pixels and dVAE visual tokens [29] of image patches respectively.",1,neutral
"Particularly, for the masked image tokens, instead of reconstructing low-level raw pixels [16] or predicting mid-level pre-defined visual tokens [2] (encapsulating mostly patch details, e.",1,neutral
Some masked image modeling methods such as MAE [14] adopt an asymmetric encoderdecoder architecture.,1,neutral
"For example, BERT [20] formulates masked languagemodeling (MLM) to predict masked linguistic tokens, while MAE [16] and BEiT [2] formulate masked image modeling (MIM) to reconstruct raw pixels and dVAE visual tokens [33] of image patches respectively.",1,neutral
"In self-supervised learning of images, popular MIM targets include raw pixels [16, 43], visual tokens [2] from a pre-trained dVAE [33], etc, as are displayed in the left side of Fig.",1,neutral
"Most recently, inspired by MAE [16], several concurrent works, e.",1,neutral
"Generally, pretraining a vision encoder by generative objectives (Bao et al., 2021; He et al., 2022) or discriminative objectives (He et al.",1,neutral
"However, without the supervision from other modalities, such pretrained models require task-specific finetuning (Bao et al., 2021; He et al., 2022; O Pinheiro et al., 2020; Wang et al., 2022) or linear probing He et al. (2020); Chen et al. (2020) for reasonably domainadapted predictions.",2,positive
"Generally, pretraining a vision encoder by generative objectives (Bao et al., 2021; He et al., 2022) or discriminative objectives (He et al., 2020; Chen et al., 2020; Grill et al., 2020; Caron et al., 2021) at the scale of one to ten million images (Deng et al., 2009) is sufficient to yield good…",2,positive
"However, without the supervision from other modalities, such pretrained models require task-specific finetuning (Bao et al., 2021; He et al., 2022; O Pinheiro et al., 2020; Wang et al., 2022) or linear probing He et al.",2,positive
"The impressive recent progresses in unsupervised pretraining (Devlin et al., 2019; He et al., 2022) and contrastive learning (Wu et al.",1,neutral
"The impressive recent progresses in unsupervised pretraining (Devlin et al., 2019; He et al., 2022) and contrastive learning (Wu et al., 2018; He et al., 2020; Chen et al., 2020) are all closely related to this direction.",1,neutral
"For computational efficiency, we adopt an asymmetric MAE architecture (He et al., 2022) based on vanilla Vision Transformers (ViTs) (Dosovitskiy et al.",2,positive
"For computational efficiency, we adopt an asymmetric MAE architecture (He et al., 2022) based on vanilla Vision Transformers (ViTs) (Dosovitskiy et al., 2020): The encoder operates only on visible patches and the decoder on all the patches.",2,positive
"Masked (denoising) autoencoding (MAE) (Vincent et al., 2010; 2008) has resurged as the state-ofthe-art self-supervised image representation learning approach (He et al., 2022; Atito et al., 2021; Bao et al., 2021; Wei et al., 2022; Dosovitskiy et al., 2020; Li et al., 2021).",2,positive
"For each decoder head, specific positional embeddings are applied following MAE (He et al., 2022).",1,neutral
"Interestingly, MAE (He et al., 2022) demonstrates the strength of the straightforward idea of image patch reconstruction, in addition to improving the pretraining efficiency by adopting high masking ratios and encoding only unmasked patches.",1,neutral
"Our decoder is another vanilla ViT deployed on the union of the encoded patch set and a set of mask tokens (He et al., 2022).",2,positive
"Existing image MAE models focus on visual tokenization Bao et al. (2021); Dong et al. (2021), token masking strategy (Li et al., 2021; Atito et al., 2021), reconstruction target (Wei et al., 2022), and architectural efficiency (He et al., 2022).",2,positive
"Following (He et al., 2022; Feichtenhofer et al., 2022), the encoder is applied only on unmasked patches.",2,positive
"Our proposed MotionMAE adopts the asymmetric Transformer based masked autoencoder architecture (He et al., 2022), as depicted in Fig.",2,positive
", 2017) in computer vision, masked autoencoders (MAEs) have recently emerged as a general SSL framework (He et al., 2022; Bao et al., 2021; Radford et al., 2018).",2,positive
"Masked autoencoders With the increasingly wide adoption of Transformers (Vaswani et al., 2017) in computer vision, masked autoencoders (MAEs) have recently emerged as a general SSL framework (He et al., 2022; Bao et al., 2021; Radford et al., 2018).",2,positive
", 2022), and architectural efficiency (He et al., 2022).",2,positive
", 2010; 2008) has resurged as the state-ofthe-art self-supervised image representation learning approach (He et al., 2022; Atito et al., 2021; Bao et al., 2021; Wei et al., 2022; Dosovitskiy et al., 2020; Li et al., 2021).",2,positive
", 2021) and MAE (He et al., 2022) have also achieved impressive performances with much more scalable potential.",2,positive
"Recently, generative models such as BEiT (Bao et al., 2021) and MAE (He et al., 2022) have also achieved impressive performances with much more scalable potential.",2,positive
"This idea is inspired by recent studies of computer vision [7], [23] that engage semantic context learning by masking most parts of the input and predicting missing components.",1,neutral
"Amongst the standard models, MAE outperforms the ResNet50-based models on most measures.",2,positive
"Hence, the robustness of MAE cannot be solely attributed to the transformer backbone.",1,neutral
"MAE uses masked autoencoders (He et al., 2022) and we use its ViT-Large variant.",2,positive
"B we evaluate the transformer-based models MAE (He et al., 2022) and MOCOv3 (Chen et al.",2,positive
"B we evaluate the transformer-based models MAE (He et al., 2022) and MOCOv3 (Chen et al., 2021) as well as adversarially fine-tuned versions of MOCOv3 using the same three attacks as for MOCOv2.",2,positive
MAE also has some of the best robustness against targeted U-PGD attacks and is competitive to PixPro for the untargeted case.,2,positive
"…language processing (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020; Smith et al., 2022; Rae et al., 2021; Hoffmann et al., 2022a; Chowdhery et al., 2022; Kim et al., 2021a), automatic speech recognition (Baevski et al., 2020), and computer vision (He et al., 2022; Xie et al., 2022).",1,neutral
"As for segmentation, we use the ViT-base model provided by MMSegmentatation, which is pre-trained by MAE on ImageNet and then finetuned on the ADE20k dataset.",2,positive
"Based on transformers and masked image modeling, MAE [21] becomes a good alternative for pre-training.",1,neutral
", natural language processing [25, 34, 2], computer vision [4, 20, 21].",1,neutral
"Note that SimR101, SimR101 and MAEViT stand for Resnet101 pretrained by SimCLRv2, Resnet50 pretrained by SimCLRv2 and ViT-base-16 pretrained by MAE, respectively.",0,negative
"We craft pre-trained adversarial perturbations (PAPs) for three pre-trained models (i.e., Resnet50 by SimCLRv2, Resnet101 by SimCLRv2, ViT16 by MAE) and evaluate the attack success rates on ten downstream tasks.",2,positive
We report the results of SimCLR and MAE in Section 4.2.,0,negative
"Note that Resnet50 and Resnet101 [18] are pre-trained by SimCLRv2 [4], and ViT16 [56] is pre-trained by MAE [21].",0,negative
"We conduct extensive experiments on typical pre-trained vision models [4, 21] and ten downstream tasks.",2,positive
"Recent progress in self-supervised learning [9, 21, 24, 25] has resulted in methods that can extract informative visual representations without requiring any supervised labels.",1,neutral
"There are also further SSL approaches that are not limited to instance discrimination, but instead use information from nearest neighbors [17], prototype clustering [8], and image patch reconstruction [25].",1,neutral
"We fine-tune a stateof-the-art vision transformer model pretrained with masked autoencoding (He et al., 2021) for 100 epochs on rotated MNIST (Weiler and Cesa, 2019) (details in Appendix C.5).",2,positive
"We fine-tune a stateof-the-art vision transformer model pretrained with masked autoencoding (He et al., 2021) for 100 epochs on rotated MNIST (Weiler and Cesa, 2019) (details in Appendix C.",2,positive
"We use the base model and default finetuning procedure from (He et al., 2021), finetuning for 100 epochs, halving the learning rate on loss plateaus.",2,positive
"MAE (He et al., 2022) simplifies the pre-training pipeline by only encoding a small set of visible patches.",2,positive
"We opted to use a more flexible transformerbased visual architecture, which has recently achieved stateof-the-art results in computer vision tasks [11, 14], and language tasks [10, 31].",2,positive
"Recently, an in part due the successful usage of transformers in language [31] and vision tasks [11], such a pre-training strategy has been successfully applied to text [10] and vision tasks [14].",1,neutral
"Visual transformers are typically trained either with a supervised loss [11] or a maskingbased objective, followed by fine-tuning [14].",1,neutral
"97 Despite ViT models trained via the MAE framework yield improving performance in vision tasks as 98 model sizes grow [8, 15, 48], previous work [9] does not show improvement from switching a ViT99 Small model to the ViT-Base counterpart of 4x as many parameters.",2,positive
"Masked autoencoding [42, 15] 77 overcomes this issue and has shown superior performance on visual recognition tasks.",1,neutral
125 We pre-train the models via the MAE framework [15].,2,positive
MAE masks out random patches in an image and reconstructs the missing content with a vision transformer (ViT) [9].,2,positive
We leverage the masked 29 autoencoders (MAE) [15] that learn representations by masked prediction.,2,positive
"An important property of our visual pre-training approach is that uses a self-supervised objec194 tive [15] that makes minimal assumptions about the data distribution and does not rely on human195 designed proxy tasks, like data augmentations.",2,positive
At the core of our self-supervised representation learning approach is masked image modeling via the masked autoencoders (MAE) [16].,2,positive
"The training recipe closely follows [15], 126 with dataset specific settings from [9].",0,negative
"While the MAE-trained ViT models yield improving performance in vision tasks as model sizes grow [9, 16, 46], previous work [10] does not show improvement from switching a ViT-Small model to the ViT-Base counterpart of 4x as many parameters.",2,positive
"Simple and free from dataset or task-specific augmentations [41], MAE is the state-of-the-art self-supervised framework in computer vision [42, 43, 44, 45], and has been demonstrated to work well for motor control tasks in simulation as well [10].",2,positive
We pre-train the models via the MAE framework [16].,2,positive
2 Masked Visual Pre-training 90 At the core of our self-supervised visual representation learning approach is masked image modeling 91 via the masked autoencoders (MAE) [15].,2,positive
We train the MAE models for 400 epochs for the combined Ego dataset; 1600 epochs for the HOI dataset; and 1600 epochs for ImageNet dataset.,2,positive
"propose Masked AutoEncoder (MAE) for a largescale self-supervised pre-train [11], which can obviously enhance the performance of the purely attention-based model in vision.",1,neutral
"It is further MAE fine-tuned (He et al., 2021), using the same in-domain data as for the Mask R-CNN object detector.",2,positive
"To prevent catastrophic forgetting, VIMA only fine-tunes the last two layers of the language encoder with layer-wise learning rate decay (He et al., 2021) but freezes all other layers.",2,positive
"Our experiments follow the same architecture, settings, and pre-training recipe as MAE (He et al., 2022), and we find that the simple addition of a teacher (RC-MAE) consistently outperforms MAE in all model sizes (e.",2,positive
"(MAE) (He et al., 2022) is a self-supervised approach with a ViT encoder f and decoder h, which randomly masks a portion of input patches, and then reconstructs the masked patches given the visible patches.",2,positive
"Method Pre-Train Data APbox APmask Superivsed (He et al., 2022) IN1K w/ labels 47.",0,negative
"To validate the effectiveness of the EMA Teacher in RC-MAE, we compare RC-MAE to an otherwise identical MAE (He et al., 2022) baseline.",2,positive
"Inspired by the success of masked language modeling (MLM) pre-training in NLP, recent SSL approaches (Bao et al., 2022; Zhou et al., 2022; Xie et al., 2022; He et al., 2022; Assran et al., 2022) in the vision community have proposed forms of masked image modeling (MIM) pretext tasks, using ViT-based backbones.",2,positive
", 2022) based masked image modeling (MIM) approaches (Zhou et al., 2022; Bao et al., 2022; He et al., 2022; Xie et al., 2022; Assran et al., 2022) for computer vision tasks have been proposed.",2,positive
"For the reconstruction target, we use standard normalized patch pixels as done with MAE (He et al., 2022).",2,positive
"Pixel-level prediction (Chen et al., 2020b; Xie et al., 2022; He et al., 2022) learns visual representations by reconstructing masked input patches at the RGB pixel-level.",1,neutral
", 2022) and MAE (He et al., 2022) provide pixel-level reconstructions of masked patches, and lead to superior performance on dense prediction tasks such as object detection and segmentation.",2,positive
", 2021) and implementation details as MAE (He et al., 2022), we fine-tune Mask R-CNN (He et al.",2,positive
Which masking strategy would work better for depth estimation? ‚ MIM pre-training uses a relatively high mask ratio and mask size [9]–[11] due to more information redundancy ar X iv :2 21 0.,0,negative
"Instead, SiMMIM [10] and MAE [11] show that even random masking with a higher mask ratio or mask size can similarly perform well for selfsupervised pretraining from image data.",1,neutral
"MIM pre-training involves masking a portion of image patches and then using the unmasked portion to predict the masked input [10], [11] or its features [9], [12].",1,neutral
"Self-supervised learning (SSL) has shown great progress to learn informative data representations in recent years (Chen et al., 2020a; He et al., 2020; Chen et al., 2020b; Grill et al., 2020; Lee et al., 2021; Caron et al., 2020; Zbontar et al., 2021; Bardes et al., 2021; Tomasev et al., 2022; Caron et al., 2021; Chen et al., 2021; Li et al., 2022a; Zhou et al., 2022a;b; HaoChen et al., 2021; He et al., 2022), catching up to supervised baselines and even surpassing them in few-shot learning, i.",2,positive
"…Bardes et al., 2021; Tomasev et al., 2022; Caron et al., 2021; Chen et al., 2021; Li et al., 2022a; Zhou et al., 2022a;b; HaoChen et al., 2021; He et al., 2022), catching up to supervised baselines and even surpassing them in few-shot learning, i.e., when evaluating the SSL model from only a…",2,positive
"The predicted property can be the original pixels [15], latent representation [29], or visual tokens [6, 8].",1,neutral
"By randomly masking a portion of patches and optimizing the loss between reconstructed masked patches and real patches, MAE achieves state-of-the-art performance.",2,positive
We observe the ASR ranged from 66.34% to 99.18% on both MAE and CAE.,0,negative
"As MIM methods randomly mask a large portion of the input images, i.e., 75% in MAE, the trigger can be masked in the pre-training phase.",1,neutral
We then take MAE as the target model’s architecture and conduct comprehensive ablation studies to understand the impacts of important backdoor attack components in each supply chain’s phase.,2,positive
"Conventionally, after obtaining the released MAE model, Type II attacker would directly apply backdoor attacks on the encoder.",2,positive
"Concretely, for Type I and Type II attacks, as the adversary does not involve in the pre-training phase, we utilize the public MAE 3 and CAE 4 as our target model.",2,positive
"We compare the MAE performance of using AdamW, SGD, and LARS as the optimizer and find AdamW reaches the best clean accuracy (see Table 9).",2,positive
"We consider two MIM architectures as the target models, i.e., Masked Autoencoder (MAE) [15] for end-to-end MIM and Contextual Autoencoder (CAE) [8] for tokenizer-based MIM.",2,positive
VideoMAE: Masked Autoencoders are DataEfficient Learners for Self-Supervised Video PreTraining.,1,neutral
"Recently, with the advent of the Transformer architecture, masked image modeling (MIM), a generative method, has successfully surpassed contrastive learning and reached state-of-the-art performance on self-supervised pre-training tasks [6, 8, 15, 32].",1,neutral
"To promise the results are comparable, we adopt the same linear probing configurations in all three scenarios for both MAE and CAE.",2,positive
MAE-AST: Masked Autoencoding Audio Spectrogram Transformer.,2,positive
Mask is a key component of MAE.,2,positive
"For MAE, the batch size is 32, epoch is 200, mask ratio is 75%, and norm pix loss is False.",0,negative
MultiMAE: Multi-modal Multi-task Masked Autoencoders.,1,neutral
"Also, previous work [8, 15] leverages the pre-training dataset as the downstream dataset as well.",2,positive
", Masked Autoencoder (MAE) [15] for end-to-end MIM and Contextual Autoencoder (CAE) [8] for tokenizer-based MIM.",1,neutral
"The existing techniques used various forms of contrastive learning and different pretext tasks such as predicting the masked portion of the image [7], predicting",1,neutral
We find that threshold [3-7] produces the best results which shows that it is important to keep a balance between similar and dissimilar view-pairs.,1,neutral
"In the past, in single image setting, these included masked image modeling [7], object mask prediction [8], instance discrimination [17] and others.",1,neutral
"2) Specialized ViT backbone (BodyPressureSD distribution
via MAE).",2,positive
Our decoder must be initialized randomly because the MAE authors do not share their decoder model parameters.,2,positive
"For our third specialized model, we first pretrain on in-domain simulated data for 150 epochs, initializing our ViT encoder from MAE’s pretrained model parameters and our ViT decoder randomly.",2,positive
"We introduce steps 2 and 3 that leverage self-supervised MAE training and finetuning, respectively, using additional adult pose datasets.",2,positive
1) ViT backbone (ImageNet-1k distribution via MAE).,2,positive
MAE is an SSL algorithm that pretrains ViTs by encoding a small portion of patches (referred to as visible patches) and then employs a ViT decoder to reconstruct the hidden patches.,2,positive
"When pretraining with MAE, a smaller decoder size is typically chosen to reduce the computational costs of pretraining; specifically, they chose a decoder model width of 512 and two transformer blocks.",1,neutral
"3) Establish, for the first time, the benefit of a hierarchical pretraining strategy for pose estimation using MAE for ViT-based models.",1,neutral
[34] for 1600 epochs on the ImageNet-1k dataset.,1,neutral
"We leverage hierarchical pretraining by continuing to pretrain our ViT encoder on in-domain (i.e., fused depth and pressure) data using the MAE SSL algorithm.",2,positive
"fied by MAE [34], which we leverage in Section V.",0,negative
"This may be due to the relatively small dataset size of SLP, resulting in overfitting on the MAE task.",1,neutral
"For our first two specialized models, we pretrain on in-domain—either simulated or real—data for 150 epochs, initializing our ViT encoder from MAE’s pretrained model parameters and our ViT decoder randomly.",2,positive
We also demonstrate a masked autoencoding (MAE) hierarchical pretraining strategy for ViT that significantly improves accuracy on the SMaL dataset.,2,positive
"This is a popular class of SSL algorithms often referred to as reconstructive learning and is recently exemplified by MAE [34], which we leverage in Section V.",1,neutral
"art performance in image classification [34], [37], semantic",1,neutral
"Given that the ViT backbone is amenable to hierarchical MAE pretraining strategy, we can further improve the performance of the best-performing architecture, ViTPose.",2,positive
This asymmetric encoder–decoder design reduces pretraining FLOPs by approximately 3× [34].,1,neutral
"In more detail, MAE first encodes a randomly sampled 25% of image patches using a ViT encoder and holds the remaining 75% of patches aside.",2,positive
"In ViTPose, they initialize with MAE’s encoder; we use this as a baseline to compare with ViTPose models initialized with our three specialized encoders.",2,positive
"5(left), our method is significantly superior to MAEL, which has the best accuracy of all single deep networks.",2,positive
"Following the procedure described in previous section, we first constructed object embeddings based on ViT (MAE), CNN (DenseNet) and used kNN classifier (our first approach).",2,positive
"A. Classifier comparison
In the first round of experiments, we encode RGB and depth modalities of the object separately using ViT (MAE) and CNN (DenseNet) and assessed the performance of seven classifiers, including k-Nearest Neighbors (kNN) [50], Multi-layer Perceptron (MLP) [51], Support Vector Machine (SVM) [52], Decision Tree (DT) [53], Gaussian Process (GP) [54], Random Forest (RF) [55], and Gaussian Naive Bayes (GNB) [56], on the restaurant fine-grained object dataset.",2,positive
"In particular, the accuracy of the multimodal with ViT (MAE) and DenseNet reached 93.13%.",2,positive
", and ViTs (MAE+MAE-L).",1,neutral
We used MAE (RGB) + DenseNet (RGB-D) to represent each of the objects.,1,neutral
"Subsequently, Swin [34], DeiT [35], and MAE [36] were introduced for various computer vision tasks.",1,neutral
"The accuracy of our multimodal appraoch-II with DeseNet (RGB-D) and MAE (RGB) was 93.51%, which outperformed all single models, CNNs (Dense.+Mnas.)",2,positive
It can be observed that the computation time of the MAEL approach is generally higher compared to the CNN-based approach.,1,neutral
We find that existing theory designed to transfer knowledge of ConvNets trained in a self-supervised manner results in a significant performance gap between teacher and student.,2,positive
"Compared with ConvNet, the attention-based ViTs suffer less from an image-specific inductive bias and have a larger potential when training on large scale datasets.",1,neutral
"Moreover, existing selfsupervised knowledge distillation (SSKD) methods focus on ConvNet architectures and are suboptimal for ViT knowledge distillation.",2,positive
"For future work, we are interested to explore AttnDistill for knowledge distillation between ConvNets and ViT.",2,positive
5-(c)) : A recent work for distillation of selfsupervised representations of ConvNet is CompRess [1].,1,neutral
"Observing that the previous SSKD methods focussed on ConvNet do not work well on ViT, we proposed AttnDistill to distill the knowledge from a pretrained teacher model to its student model.",2,positive
"From the aspect of backbone architectures, the previous methods are all based on ConvNet [5, 8, 26].",1,neutral
A drawback of the attention mechanism is that it is tailored for transformer usage and requires additional computation when applied to ConvNets (namely the computation of the attention maps).,1,neutral
"We draw the following conclusions:
• Based on ViT-T/16 distilled from Mugs(ViT-S/16), our method AttnDistill gets state-of-theart k-NN and Linear probing performance compared with previous knowledge distillation methods based on ConvNet.",2,positive
"The potential of attention distillation has been explored for ConvNet [24, 71], however, since for these networks attention is not explicitly computed, additional computation and attention definition are needed.",1,neutral
"He et al., 2022), we avoid such learnable prefix design with random initialization and propose a parallel attention (PATT) to the original attention module (see Figure 3).",1,neutral
"He et al. (2022) analyzed the unified view among PETL techniques such as prefix-tuning, low-rank (LoRA) adaptation, and Adapter, pointing out the similarity between prefix-tuning and Adapter in terms of calculating the attention.",1,neutral
"He et al., 2022) and VideoMAE (Tong et al., 2022) to conduct further comparison on video and image datasets, which follows the self-supervised pre-training setting2 in S. Chen et al. (2022) except that the batch size is set to 256 instead of 1, 024.",0,negative
"He et al., 2022), etc. ST-Adapter adapts image-text models pre-trained on large scale datasets such as 400M image-text pair proposed by CLIP (Radford et al., 2021) and the IG-3.",2,positive
"He et al., 2022) in the NLP domain, we aim to propose a unified model for the vision domain, especially for video-based downstream tasks.",2,positive
He et al. (2022).,2,positive
"He et al. (2022); Houlsby et al. (2019) for PETL in NLP tasks, Adapter (S. Chen et al., 2022) has been directly used for vision tasks, showing promising performance using far less tunable parameters.",2,positive
"He et al., 2022; Tong et al., 2022).",1,neutral
"He et al. (2022); X.L. Li and Liang (2021) can be regarded as prepending contextual information for downstream tasks, which is similar to the pre-training process aiming to predict masked words in the process of an inner loop (Brown et al., 2020).",1,neutral
"He et al., 2022), fine-tuning VLP models do not lead to results as good as fine-tuning supervised pre-trained vision models.",1,neutral
"He et al., 2022; Tong et al., 2022), adding a prefix for a sentence input in NLP can be structurally different from the visual domain.",1,neutral
"In particular, contrastive learning [5, 18] and masked auto-encoding [12, 17, 48] are two popular self-supervised schemes.",1,neutral
"By removing the local inductive bias [19] from convolutional neural networks [28, 64, 60], vision transformers armed with global self-attention show superiority in scalability for large-scale models and billion-scale dataset [18, 84, 61], self-supervised learning [27, 76, 1], connecting vision and language [53, 34], etc.",1,neutral
Existing masked data modeling approaches include Context Encoders [21] and Masked Autoencoders (MAE) [12].,1,neutral
"conST [31] first concatenates gene expressions and the pre-extracted morphology features that are extracted from images via the Masked Auto-encoder (MAE) into a feature vector [32], which is then fed into a graph convolutional network to learn latent representations.",1,neutral
"Specifically, we use the weight from the original vision MAE model (He et al., 2022) (Weights from https://github.com/ facebookresearch/mae) with only self-supervised learning (SSL) pretraining for all audio, visual, and joint encoder and the decoder.",2,positive
", 2022) Given the success of MAE in the vision domain (He et al., 2022; Bachmann et al., 2022; Girdhar et al., 2022; Tong et al., 2022; Feichtenhofer et al., 2022), several efforts adapt MAE for audio with relatively minor changes to the overall pipeline (Baade et al.",2,positive
"…based on the Audio Spectrogram Transformer (Gong et al., 2021a) and Vision Transformer (Dosovitskiy et al., 2020) backbones, the single-modal Masked Auto-Encoder (MAE) (He et al., 2022) achieved state-of-the-art (SOTA) performance on images and audio tasks (Huang et al., 2022a) individually.",2,positive
"Specifically, we use the weights of the original vision MAE He et al. (2022).",2,positive
"Due to the space limitation, please refer to He et al. (2022); Huang et al. (2022a) for single-modal MAEs.",1,neutral
"…applied on visual and audio domains (Baevski et al., 2020; Hsu et al., 2021; Srivastava et al., 2022) Given the success of MAE in the vision domain (He et al., 2022; Bachmann et al., 2022; Girdhar et al., 2022; Tong et al., 2022; Feichtenhofer et al., 2022), several efforts adapt MAE for audio…",2,positive
This is mainly due to many previous MAE works reporting a masking ratio ∼75% is appropriate for both audio and visual input He et al. (2022); Baade et al. (2022); Huang et al. (2022a); Niizumi et al. (2022).,0,negative
These settings are identical to the original vision MAE He et al. (2022).,0,negative
", 2020) backbones, the single-modal Masked Auto-Encoder (MAE) (He et al., 2022) achieved state-of-the-art (SOTA) performance on images and audio tasks (Huang et al.",2,positive
"Based masked modeling as all autoencoder [18], MAE [6] uses the encoder to map the observed signal to the potential representation, and the decoder to reconstruct the original signal from the latent representation.",1,neutral
"These pre-trained masked modeling has been proved to be well applied to various downstream tasks, among which a simpler and more effective way is masked autoencoders (MAE) [6].",1,neutral
"The demand for large scale data processing has been solved by self-supervised pretraining in natural language processing (NLP) and computer vision (CV) fields [5], [6].",1,neutral
"Masked modeling encourages the model to infer the deleted parts according to the context information, so that the model can learn the deep semantics, which has become the benchmark of self-supervised pre-training in NLP and CV fields [5], [6].",1,neutral
"Implementation Detail For the image classification problem, we use Vision Transformer pretrained on unlabeled ImageNet dataset with masked auto-encoding (He et al., 2022) and fine-tune it on the downstream task with AdamW optimizer (Loshchilov & Hutter, 2019) for 10,000 steps with batch size 32.",2,positive
"Firstly, we remove masked auto-encoding objective LMAE and train the model with only distillation loss LDistill before fine-tuning.",2,positive
"Then we take the copy of the pre-trained model (fθinit , gϕinit) as a student, and match the representations of the student encoder and those of the teacher encoder while optimizing the student with the MAE on the target unlabeled data.",2,positive
"1 INTRODUCTION Pre-trained transformer models (Devlin et al., 2019; Brown et al., 2020; Liu et al., 2019; He et al., 2022) have been effective on various vision and natural language processing tasks.",2,positive
"Masked Auto-Encoding We briefly describe the masked auto-encoding objective (Liu et al., 2019; He et al., 2022) for a language model such as RoBERTA (Liu et al.",1,neutral
"In this work, we focus on the masked auto-encoding (Devlin et al., 2019; He et al., 2022) as a pre-training objective function since its generality compared to other self-supervised methods (Chen et al.",2,positive
"Masked Auto-Encoding We briefly describe the masked auto-encoding objective (Liu et al., 2019; He et al., 2022) for a language model such as RoBERTA (Liu et al., 2019) and Vision Transformer (ViT) (Dosovitskiy et al., 2021).",2,positive
"Specifically, we take the pre-trained model with an encoder fθinit and a decoder gϕinit which are pretrained on a massive amount of unlabeled data from general domain, and continue to pre-train it with masked auto-encoding (MAE) (Devlin et al., 2019; He et al., 2022) objective on the target unlabeled data to obtain fθ0 and gϕ0 .",2,positive
"Then we take the copy of the pre-trained initial network gϕinit ◦fθinit as a student and further pre-train the student with masked auto-encoding objective but enforce hidden representation of the encoder of the student fθinit to be close to that of the teacher fθ0 as follows:
(θ1, ϕ1) ∈ argmin θ,ϕ (LMAE(θ, ϕ;Du) + LDistill(θ; θ0,Du))
LDistill (θ; θ0,Du) = 1
n n∑ i=1 ∥∥∥fθ(x(i))− StopGrad(fθ0(x(i)))∥∥∥2 2
(3)
where θ and ϕ are initialized with the pre-trained parameters θinit and ϕinit, respectively and StopGrad denotes the stop-gradient operation which does not back-propagate through the input.",1,neutral
"Then the final objective for masked auto-encoding is defined as follows:
LMAE(θ, ϕ;Du) = 1
n n∑ i=1 Ez(i)∼pγ,T (z)
[ −
K∑ k=1 z (i) k Z(i) · log pθ,ϕ(x(i)k |x̂(i))
] , Z(i) =
K∑ k=1 z (i) k , (2)
where pγ,K(z) denotes a Binomial distribution with its parameters γ for probability that zk = 1 and K for the number of trials.",1,neutral
"In this experiment, we continue to pretrain the encoder of the pre-trained transformer (He et al., 2022) with supervised contrastive loss (Khosla et al.",2,positive
"Specifically, we take the pre-trained model with an encoder fθinit and a decoder gφinit which are pretrained on a massive amount of unlabeled data from general domain, and continue to pre-train it with masked auto-encoding (MAE) (Devlin et al., 2019; He et al., 2022) objective on the target unlabeled data to obtain fθ0 and gφ0 .",2,positive
"…model with an encoder fθinit and a decoder gϕinit which are pretrained on a massive amount of unlabeled data from general domain, and continue to pre-train it with masked auto-encoding (MAE) (Devlin et al., 2019; He et al., 2022) objective on the target unlabeled data to obtain fθ0 and gϕ0 .",2,positive
"In this work, we focus on the masked auto-encoding (Devlin et al., 2019; He et al., 2022) as a pre-training objective function since its generality compared to other self-supervised methods (Chen et al., 2020b; He et al., 2020; Grill et al., 2020; He et al., 2020; Chen & He, 2021; Caron et al.,…",2,positive
"In this experiment, we continue to pretrain the encoder of the pre-trained transformer (He et al., 2022) with supervised contrastive loss (Khosla et al.,
2020) while fine-tuning a randomly initialized linear classifier with cross-entropy loss.",2,positive
"Pre-trained transformer models (Devlin et al., 2019; Brown et al., 2020; Liu et al., 2019; He et al., 2022) have been effective on various vision and natural language processing tasks.",1,neutral
"For example, MAE-based [24] methods [17, 51] use pixel values of video frames as supervision by masking raw videos with an extremely high ratio and reconstructing them.",1,neutral
"For example, MAE-based [24] methods [17, 49] use pixel values of video frames as supervision by masking raw videos with an extremely high ratio and reconstructing them.",1,neutral
"Xiao et al. (2022) demonstrates that masked-autoencoder (He et al., 2022) trained on diverse video data (Goyal et al., 2017; Shan et al., 2020) can be an effective visual embedding for online RL.",2,positive
"(2022) demonstrates that masked-autoencoder (He et al., 2022) trained on diverse video data (Goyal et al.",2,positive
"Inspired by pre-training successes in computer vision (CV) (He et al., 2020; 2022) and natural language processing (NLP) (Devlin et al., 2018; Radford et al., 2019; 2021), pre-training visual representations on out-of-domain natural and human data (Deng et al., 2009; Grauman et al., 2022) has…",2,positive
"Most methods train for a constant factor more (sometimes an order of magnitude more) [7, 12] than their supervised counterparts, necessitate multiple forward passes per gradient step [6, 7, 12, 16, 32], or use expensive Transformer-based decoders that scale quadratically with image resolution [13].",1,neutral
"We address this gap by profiling four popular self-supervised methods (BYOL [12], SimCLR [7], DINO [6], and MAE [13]) and two supervised methods (CLIP [27] and standard classification).",2,positive
"[13] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.",0,negative
"We use the MAE ViT [13] as the backbone in SimCLR, BYOL, and CLIP (using e.",2,positive
"1 Pre-training Methods and Models We run four common self-supervised methods (BYOL [12], SimCLR [7], DINO [6], and MAE [13]) and two supervised methods (CLIP [27] and standard softmax classification).",2,positive
"Similar to [13], we ran an exploratory learning rate (LR) search across {1e-5, 5e-5, 1e-4, 5e-4} and weight decay (WD) values {0.",2,positive
"BYOL [12], data2vec [2], DINO [6], and ODIN [16]), and masked autoencoding (MAE) [13, 36] are three of the best-known flavours of modern SSL for visual pre-training.",1,neutral
MIM pre-training forces the ViT to learn the local facial action units and global facial structures in various expressions [18].,0,negative
"Recently, several works (He et al., 2022; Fang et al., 2022; Wei et al., 2022; Chen et al., 2022; Xie et al., 2022; El-Nouby et al., 2021; Bao et al., 2022) also explored masked content prediction tasks for self-supervised representation learning.",1,neutral
"Luckily, such signals can nowadays be easily obtained with self-supervised learning algorithms, which have been successful in learning powerful representations for vision tasks such as classification and object detection purely from images (Chen et al., 2020b; Grill et al., 2020; He et al., 2022).",1,neutral
"4.3), we investigate several recent ones (DINO (Caron et al., 2021), MoCo-v3 (Chen et al., 2021), MSN (Assran et al., 2022), MAE (He et al., 2022)).",2,positive
"For the pre-trained weights, we use the timm library for DINO and third-party releases for MoCo-v35, MSN6, and MAE7.",2,positive
"This is interesting because MAE, the worst among the self-supervised methods with the Transformer decoder, yields the overall best results in terms of ARI on COCO object discovery: 42.3.",1,neutral
"To this end, we train DINOSAUR with a ResNet34 encoder (from scratch) on COCO, but reconstruct targets obtained from ViTs pre-trained with different methods: DINO, MoCo-v3, MSN, and MAE.",2,positive
"In the following, we use block 9 for supervised training, DINO and MSN, and block 12 for MoCo-v3 and MAE.",2,positive
Both characteristics contribute to the significant improvement compared to ConvNets on medical image segmentation Tang et al. (2022); Bao et al. (2021); He et al. (2022); Atito et al. (2021).,2,positive
"Except for the MAE method used ViT-Large [5], all other methods used ResNet-50 [7] as the backbone network.",2,positive
"Method Ours SKD BYOL SimSiam MAE Transfer From Scratch Accuracy 82.7% 74.2% 68.3% 66.8% 62.3% 53.9% 28.4%
Figure 3: Examples of real and distilled images.",0,negative
"For comparative methods, we used several SOTA self-supervised learning methods, including SKD [12], BYOL [6], SimSiam [2] and MAE [7].",1,neutral
"While most SSL methods require architectural or training strategies to avoid collapse (Grill et al., 2020; He et al., 2021), Bardes et al. (2022) and Zbontar et al. (2021) only require to modify the loss.",0,negative
"While most SSL methods require architectural or training strategies to avoid collapse (Grill et al., 2020; He et al., 2021), Bardes et al.",1,neutral
"While research in applying plain ViTs to dense vision tasks continues [15, 21], research into hierarchical vision transformers quickly became dominant [26,41] and continues to grow [13, 25].",1,neutral
"Recently, methods based on masked image modeling, such as SimMIM [41] and MAE [20], are proposed to improve the self-supervised ViTs training by masking a large ratio of patches.",1,neutral
"This is also inspired by MAE [20], i.e., the image can be reconstructed with only a few patches thanks to the powerful global attention ability of ViTs, which, if unconstrained, also makes the model more sensitive to some local patches during training.",2,positive
"First, the prosecution network and the defendant network pre-trained by MAE [13] are adopted to initialize CourtNet.",2,positive
"First, the prosecution and defendant networks pretrained by MAE [13] are adopted to initialize CourtNet.",2,positive
"In a contemporary work, He et al. (2022) showed that pretraining with a Masked AutoEncoder (MAE) objective (analogue of MLM objective for images) boosts the performance of ViT models on the Imagenet-1K dataset.",2,positive
"Self-pretraining in Computer Vision Most relevant to our work, recent/concurrent works in computer vision explore self-pretraining (He et al., 2022; El-Nouby et al., 2021).",2,positive
"For each training batch, we compute each objective through a separate forward pass and use the weighted sum of them for the final loss, where λVAM = 1.0 and λMAE = 0.3.
loss = λVAMlossVAM + λMAElossMAE (1)",2,positive
"(1), we use λVAM = 1.0 and λMAE = 0.3.",1,neutral
[26] that is pretrained on ImageNet [14].,0,negative
"Following MAE-AST [7], we randomly mask 75% of the spectrogram patches.",2,positive
"The challenge lies in the difference between text and acoustic signals; text is discrete and dense in information, while acoustic signals are continuous and sparse in information [26; 7].",1,neutral
"We calculate the mean squared error between the reconstructed and original video frames and spectrograms:
lossMAE = 1
NVM ∑ i∈masked ||xVi − x̂Vi ||22 + 1 NAM ∑ j∈masked ||xAj − x̂Aj ||22 (3)
whereNVM andN A M are the number of masked patches for vision and audio, respectively.",2,positive
"4.2, we use separate decoders (with shared weights) for the vision and audio MAE pretraining objectives.",2,positive
[26] and use a shallow decoder that only serves for masked autoencoding objective (Sec.,1,neutral
"Following MAE [26], we randomly mask 75% of the visual patches, and the masking is applied for each video frame independently.",2,positive
"In addition to the VAM objective to learn cross-modal representation, we also use the masked autoencoding (MAE) objective to improve unimodal representations in the vision-and-language settings, by masking random patches of visual frames and the audio spectrogram, and reconstruct missing inputs as shown in Fig.",2,positive
Table 11 shows that each of the pretraining objectives (MAE and VAM) improves finetuning performance over random weight initialization.,0,negative
"In Figure 3 and Figure 4, we show the reconstruction results with the MAE head, described in the main paper Sec.",0,negative
"The combination of VAM and MAE further improves the finetuning performance, and we use this configuration as default for TVLT pretraining.",2,positive
"(a) Using the state-ofthe art reconstruction-based SSL strategy, MAE [8] architecture for pre-training an representation extractor (encoder).",2,positive
"As indicated in [1, 7, 8] that SSL training requires a large amount of data, we begin with training a MAE on SSL Pre-Training Set.",0,negative
"1), we utilize a state-of-the-art reconstructed-based strategy, Masked Auto-Encoder (MAE) [8] to learn lowerdimensional semantic feature embedding.",2,positive
"These pretext tasks are created solely using the input features, such as predicting a missing image patch [8], recovering the color channels of an image from context [19], predicting missing words in texts [12], forcing the similarity of the different views of images [1, 7], etc.",1,neutral
"Motivated by theoretical implication of SSL embedding space, we leverage a Masked Autoencoder [8] for feature extraction.",1,neutral
"The structure of residual MLP is inspired by masked autoencoders (MAE)[22], which is used to reconstruct the random missing pixels.",1,neutral
MAE uses vision transformer (ViT)[23] as encoder to reconstruct random masked patches in a image.,1,neutral
"Researchers first borrow transformer as the base model [17, 53, 37, 56, 12], and then design novel self-supervised algorithms to pretrain them with the help of huge amounts of unlabeled data and modern hardware [10, 5, 30, 22, 8, 32].",1,neutral
[55] further verified the effectiveness of this class of methods by applying self-supervised masks to the computer vision domain.,1,neutral
", 2021), this has been primarily explored in the context of pretraining or designing auxiliary loss (Rasmus et al., 2015; Sabour et al., 2017; Chen et al., 2020; He et al., 2021).",1,neutral
"While the object-centric approach is known to improve the model’s generalization performance (Dittadi et al., 2021), this has been primarily explored in the context of pretraining or designing auxiliary loss (Rasmus et al., 2015; Sabour et al., 2017; Chen et al., 2020; He et al., 2021).",1,neutral
"In this section, we verify that GPaCo can enhance model generalization ability and robustness on full ImageNet and its variants when compared with MAE [14] models.",2,positive
(c) Comparison with MAE [14] ViT models on full ImageNet and out-of-distribution robustness.,2,positive
[14] justifies that masked autoencoders are scalable vision learners.,1,neutral
"To demonstrate the advantages of GPaCo on out-ofdistribution data, we load MAE [14] pre-trained weights and then fine-tune on full ImageNet with same training strategy as in [14].",2,positive
"These augmentations are suitable for usage with deep learning models Kaiming et al. (2022), Mathieu et al. (2015) and have been considered for oil drilling logging data Xingye et al. (2021).",2,positive
"…et al., 2017] that can exacerbate posterior collapse when used for decoding and self-supervised learning, where input (words in NLP, image patches in computer vision) are often masked randomly [Devlin et al., 2019, He et al., 2022], but a learnt policy might improve performance or convergence.",1,neutral
VideoMAE is a video transformer pretrained in a selfsupervised strategy inspired by ImageMAE [27] and proposed customized video tube masking and reconstruction.,2,positive
"We also experiment with the recently proposed masked auto-encoder framework [60], which is based on the ViT",2,positive
"Among the existing models, we select seven models [42], [44], [55], [58]–[60], [62] to show results before and after including SSPCAB and SSMCTB, respectively.",0,negative
"Inspired by the success of masked auto-encoders [60], Jiang et al.",1,neutral
"Another exception is the masked auto-enconder [60] based on the ViT backbone, where we place SSPCAB and SSMCTB before the first transformer block.",2,positive
"The reconstruction of masked information has recently become an attractive area of interest [60], [82]–[85].",1,neutral
"[60] proposed to reconstruct masked (erased) patches as a self-supervised pretext task for pre-training auto-encoders, subsequently using them for mainstream tasks, including object detection and object recognition.",1,neutral
"Although CoMo has unified three types of pre-training, our future direction will focus on integrating more existing pre-trained knowledge, such as the maskgenerated knowledge of MAE (He et al. 2022) and the 3Dcontrastive knowledge of CrossPoint (Afham et al. 2022).",2,positive
"Self-supervised learning is initialized as pretext tasks, such as image restoration from corruption (Vincent et al. 2008; Pathak et al. 2016; He et al. 2022), pseudo labels (Doersch, Gupta, and Efros 2015; Noroozi and Favaro 2016) and clustering (Caron et al.",1,neutral
"Self-supervised learning is initialized as pretext tasks, such as image restoration from corruption (Vincent et al. 2008; Pathak et al. 2016; He et al. 2022), pseudo labels (Doersch, Gupta, and Efros 2015; Noroozi and Favaro 2016) and clustering (Caron et al. 2018).",1,neutral
"Although CoMo has unified three types of pre-training, our future direction will focus on integrating more existing pre-trained knowledge, such as the maskgenerated knowledge of MAE (He et al. 2022) and the 3Dcontrastive knowledge of CrossPoint (Afham et al.",2,positive
"Mixed Feature Prediction Task Motivated by MAE [12] and Point-BERT [11], we wanted to extend this self-supervised method to point cloud segmentation.",2,positive
MAE [12] encoded incomplete patches with an autoencoder and reconstructed the original image through a lightweight decoder.,2,positive
"Mixed Feature Prediction Task
Motivated by MAE [12] and Point-BERT [11], we wanted to extend this self-supervised method to point cloud segmentation.",2,positive
"We pretrained five encoders, one using our proposed method TOV-VICReg and four using state-of-the-art self-supervised methods: MoCo v3 (Chen et al., 2021), DINO (Caron et al., 2021), VICReg (Bardes et al., 2022), and MAE (He et al., 2021).",2,positive
"For this study we selected DINO (Caron et al., 2021), MoCo (Chen et al., 2021), MAE (He et al., 2021), and VICReg (Bardes et al., 2022) since they are currently considered state-of-the-art, their official implementations are available in
PyTorch, and each represents a different type of approach.",2,positive
"Lastly, we also consider MAE (He et al., 2021), a masked reconstruction method, which consists in training an auto-encoder based on ViT to reconstruct an image with a set of patches masked.",2,positive
", 2021), MAE (He et al., 2021), and VICReg (Bardes et al.",1,neutral
"The procedures here can be summarized as follow:
y = ClassHead(FF (CLSL))
ViT and other models based on it have found much success in producing state-of-the-art results in image processing domains [23, 24] and have been the building block for many pre-trained models such as, to mention a few, DINO [25], MAE [26], and MOCOv3 [27].",2,positive
"ViT and other models based on it have found much success in producing state-of-the-art results in image processing domains [23, 24] and have been the building block for many pre-trained models such as, to mention a few, DINO [25], MAE [26], and MOCOv3 [27].",2,positive
"Recently, the vision Transformer has demonstrated excellent performances in supervised learning [11, 27, 37, 41], weakly supervised learning [10, 28], and self-supervised learning tasks [16, 38].",1,neutral
"On the other hand, with the release of ViT [7], more and more work apply vision Transformer to achieve better results in different vision areas [1, 10, 15, 16, 18, 24, 38, 39].",2,positive
"In particular, these outstanding studies in the field of weakly supervised localization [10, 28] and self-supervised learning [1, 16, 38] demonstrate the powerful local representation capabilities of tokens in vision Transformer models.",1,neutral
"Based on ViT, MAE [185] first masks random patches and tries to reconstruct them during training, which is a typical selfsupervised learning method.",2,positive
"We take Vit [38], MAE [185], and MoCo [186] in our experiments.",2,positive
"Besides, as Transformer-based methods have gained increasing attention for MedISeg in recent years, we also present experimental results on ViT [38], MAE [185], and MoCo [186] for comparison in this section.",2,positive
"To further reduce the encoder training time and improve robustness, a pre-trained encoder [27], [28] could be used and fine-tuned on our data.",2,positive
MAE [14] is an effective selfsupervised learning model processing images.,1,neutral
"Thus, we treat STMM as images and borrow the training approach of MAE [14].",2,positive
"To this end, we treat STMM as images and borrow the key principle of MAE [14] to process",2,positive
"3 Masked language/image models and the pretraining technique The design of NIERT is also inspired by the recent advances in masked language/image models [14], [15], [26], [27] and pre-trained models for symbolic regression [28], [29].",1,neutral
"More recently, masked image modeling (MIM) methods represented by MAE [11] have been proved to learn rich visual representations and significantly improve the performance on downstream tasks [18].",1,neutral
‘w/ SSL’ denotes training the segmentation network with the model weights pre-trained by a 3D Masked Autoencoder (MAE) SSL method [8].,1,neutral
"Recently, reconstruction-based SSL methods [8,28], which pre-train transformers for patch-level recovering with natural images.",1,neutral
"Implementation Details To pre-train the ASA model, we use center-cropping augmentation, Xavier uniform initializer [5] for SW-ViT blocks and set the hyper-parameters following [8] (see Table 1(a)).",2,positive
"To address this problem, the Masked Autoencoder (MAE) self-supervised training method [41] is used to pre-train the proposed model on unlabeled data.",1,neutral
"Further, the Masked Autoencoder pre-training method [41] is introduced.",1,neutral
"Moreover, whether to perform pixel normalization in the reconstruction loss [41] is compared.",1,neutral
"[41] used the idea of mask encoding from BERT [42], which is a self-supervised pre-training method for NLP.",1,neutral
"The best result is from ViT-Huge pretrained by MAE [19] and finetuned by DAT, which suggests DAT is also effective in downstream fine-tuning tasks.",1,neutral
"By combining MAE [19] pre-training and DAT fine-tuning, our ViT-Huge [20] achieves 31.40 mCE on ImageNet-C [2] and 32.77% top-1 accuracy on Stylized-ImageNet [3].",2,positive
"By combining MAE [19] pre-training and DAT fine-tuning, our ViT-Huge [20] achieves 31.",2,positive
"Besides, we use DAT to conduct supervised finetuning on downstream ImageNet classification task based on a self-supervised ViTHuge pretrained by MAE [19].",2,positive
"Furthermore, in 2021, the Facebook AI Research team led by Kaiming proposed the Masked Autoencoder(He et al. 2021) pre-train model in CV.",2,positive
"Particularly, the task embedding vectors are employed to perform convolution or attention operation together with the encoded image features [25].",1,neutral
"It is commonly acknowledged that masked image modeling (MIM) methods [55, 56, 57] achieve impressive performance in self-supervised learning.",1,neutral
"For the transformer-based CVmodels such as ViT [10], BEiT [2], masked autoencoders(MAE) [14], and swin-transformer [25], the local trigger information is more easily masked by the overall image due to the increased attention span, while the neurons that learn the backdoor features are easily lost in fine-tuning, so they have a certain ability to resist backdoor attacks.",1,neutral
"This paper adopts a purely Transformerbased backbone architecture using the dual-stream fusion with ViT-based grid features and BERT-based dynamic text features and three common pretext tasks (i.e., MLM, MIM, and ITM).",2,positive
"We perform the pre-training on three largescale medical image-text datasets, i.e., ROCO [40], MedICaT [44], and MIMIC-CXR [21].",2,positive
"For ROCO and MedICaT, we filter nonradiology samples, and for MIMIC-CXR, we only keep images in the frontal view.",2,positive
"As for the dataset split, we adopt the official splits of ROCO and MIMIC-CXR.",2,positive
"For pretext tasks, inspired by uni-modal pre-training schemes such as MLM [10, 33] and causal language modeling [6], existing studies explore a variety of pre-training tasks, including MLM [27, 35, 47], MIM [8, 35], ITM [27, 58], image-text contrastive [26] and prefix language modeling [51].",1,neutral
"In general, a VLP system consists of three elements: (i) uni-modal encoders (i.e., a vision encoder and a language encoder) that encode images and texts into image and text features, respectively; (ii) a multi-modal fusion module that performs the fusion of the encoded image and text features; (iii) pretext tasks (e.g., masked image modeling (MIM), masked language modeling (MLM), and image-text matching (ITM)) that assist the learning of VLP models.",1,neutral
"For the pretext tasks, we adopt (knowledge-enhanced) MLM, MIM [17], and ITM, where the masking ratios of MLM and MIM are set to 15% and 75%, respectively.",2,positive
"Pretext Tasks Given the aforementioned structure (denoted as M𝜃 ) with its parameters 𝜃 , the Med-VLP framework develops various pretext tasks (e.g., masked language modeling (MLM), masked image modeling (MIM), and image-text matching (ITM)) to guide the learning of 𝜃 .",1,neutral
"We first fine-tune a ViT-Large model [9, 14] on Places365 [54], which is dubbed PlacesViT.",2,positive
", with masked pretraining [14,12,30] or token pruning [26,18,36]).",0,negative
"Moreover, Hydra attention is a general technique that doesn’t make any assumptions about the relationships between tokens, so it can be applied to further improve the speed of token-sparse applications such as masked pretraining [14,12,30] or token pruning [26,18,36].",1,neutral
[13] claimed that languages are highly semantic and information-dense.,1,neutral
"It recently attracted attention as proxy task for self-supervised pre-training, especially for the ViTs [36, 37].",1,neutral
Salman et al. [15] propose to apply DRS to Vision Transfomers (ViTs).,1,neutral
"1 Introduction Advances in computer algorithms and hardware have made machine learning a great success in the fields of computer vision [1,2], data mining [3], medical diagnosis [4], cyber security [5], etc.",1,neutral
"As for the algorithm readiness, recent years have witnessed a great blossom for general vision, where Transformer [14], ViT [15, 16], Masked Auto-encoders (MAE) [17] and CLIP [18], etc.",2,positive
"As for the algorithm readiness, recent years have witnessed a great blossom for general vision, where Transformer [14], ViT [15, 16], Masked Auto-encoders (MAE) [17] and CLIP [18], etc., achieve impressive gain over conventional methods.",1,neutral
"Thus, for example, state-of-the-art Self Supervised Learning approaches [3, 5, 8, 9] (pre) train the latent representation on so-called pretext tasks that are related to but not identical to the downstream task for which the system is actually intended.",1,neutral
"In MaskFeat [38], authors use HOG [11], MoCo [20] and DINO [7] features to perform MIM; MVP [39] employs a multi-modality model, CLIP [31], which is pre-trained by rich image-text pairs.",2,positive
The architectural settings strictly follow [19].,0,negative
method mIoU mAcc ViT-B ViT-L ViT-B ViT-L supervised [19] 47.,1,neutral
"In addition to using the token obtained from offline or online model as reconstruct target, MAE [19], SimMIM [42],
and MaskFeat [38] achieve good performance in maskedimage reconstruction using low-level pixels or HOG [11] features.",2,positive
"As a special case of MIM, we formulate MKD upon which an empirical investigation is conducted about the influence of different target representations on self-supervised masked autoencoders.",1,neutral
"Masked Image Modeling (MIM) [2, 19, 38, 46] has recently become an active research topic in the field of visual representation learning and establishes strong performance for vision recognition tasks, e.g., image classification, object detection, and semantic segmentation, which also surpasses traditional supervised learning [35] mechanism.",1,neutral
"To answer these questions, we employ the standard masked autoencoder framework [19] to give a system-level study, introduced next.",2,positive
"MAE [19] uses image pixels as the target, which functions likewise to a randomly initialized teacher network, as demonstrated in Appendix B.",1,neutral
", DeiT [35] for supervised learning, DINO [7] for contrastive learning, DALL-E [32] for autoregressive generation, and MAE [19] for autoencoding.",1,neutral
"A crucial problem of MIM is how to choose the reconstructed target, i.e., T (·) in Eq.",1,neutral
"In addition to using the token obtained from offline or online model as reconstruct target, MAE [19], SimMIM [42], and MaskFeat [38] achieve good performance in maskedimage reconstruction using low-level pixels or HOG [11] features.",2,positive
"Nevertheless, it shows negligible gains on MIM-trained models such as MAE.",1,neutral
"To be specific, MIM randomly masks a portion of the input and then reconstructs the masked portion according to the transformed target, formulated as
min θ E x∼D M(T (x (1−M)), fθ(x M)), (1)
where “ ” means element-wise product; M is the patch mask; “x M” represents “unmasked patches” and vice ∗Equal contribution.",1,neutral
method ViT-B ViT-L ViT-H ViT-H448 supervised [19] 82.,1,neutral
"In this work, we paraphrase a term Masked Knowledge Distillation (MKD) to focus our discussion on a special case of MIM where the target is generated by a parameterized network (teacher network), i.e., T (·) = hφ(·).",1,neutral
method data2vec [2] BEiT [3] MAE [19] dBOT,2,positive
"Masked Image Modeling (MIM) [2, 19, 38, 46] has recently become an active research topic in the field of visual representation learning and establishes strong performance for vision recognition tasks, e.",1,neutral
And the painstaking selection of the target representations in the field of MIM.,1,neutral
", DINO [7] for contrastive learning, MAE [19] for masked autoencoding, DeiT [35] for supervised learning, and DALL-E [32] for autoregressive generation.",1,neutral
Concurrent work manages to adopt conventional feature distillation [40] to match contrastive models with MIM-trained ones.,1,neutral
"Similar to other MIM-based SSL work [19, 45], we reconstruct knowledge for those masked patches of input sample x .",2,positive
"For all patterns, the MIM methods SimMIM [45] and MAE [19] tend to group the patches with similar colors regardless of their semantic meaning.",1,neutral
"Following BEiT [3], MAE [19] develops an asymmetric encoder-decoder architecture to reconstruct the normalized masked patches.",2,positive
"MIM [45], MAE [19], and our MimCo on ImageNet-1K dataset.",2,positive
"Some methods [19, 45] propose to directly regress the raw pixels of the masked patches in a simple and effective way.",1,neutral
"What Semantic PatternsDoesMimCoLearn? To further help reveal what patterns does MIM learn, we follow the visualization of iBOT [47] to explore the learned patterns of the pre-trained models of SimMIM [45], MAE [19], and our MimCo via visualization, respectively.",2,positive
"Previous work [19, 45] have shown that the accuracy of linear probing is not always consistent with that of finetuning, especially for MIM-based pretraining methods.",1,neutral
"We randomly choose 10 classes of ImageNet-1K dataset to visualize for simplicity, the visualization of learned representation shows that our MimCo significantly improves the linear separability of representations compared to SimMIM [45] and MAE [19].",2,positive
Recent works in unsupervised learning have largely focused on removing inductive biases from the training process: transformer-based methods have successfully removed the scale-and-shift invariance from CNNs [18] and autoencoders have successfully removed the hardcoded augmentation-based invariances from contrastive learning methods [23].,1,neutral
"BEiT [52, 53, 54], Masked Autoencoder (MAE) [55], Contextual Autoencoder (CAE) [56], and Masked Image Modeling (MIM) [57] are other examples for visual representation learning by masking random patches of an input image and reconstructing the missing pixels.",1,neutral
", 2022), self-attention block (He et al., 2022), and convolutional projector (Yang et al.",1,neutral
"Furthermore, we choose different generative blocks, including cross-attention block (Chen et al., 2022), self-attention block (He et al., 2022), and convolutional projector (Yang et al., 2022e).",1,neutral
"The second way is a ‘self-attention’ block from MAE (He et al., 2022), which also includes 6 transformer layers.",2,positive
", [63, 64]) to get the pre-trained feature network.",1,neutral
"Different from existing works that focus on MVM for pure vision problems [4, 22, 86], we study MVM as a VidL pre-training task.",2,positive
"Meanwhile, self-supervised vision pre-training has been proven highly effective by reconstructing the masked image patches through raw pixel values [22, 78], discrete visual tokens [4, 86], or visual-semantic features [75, 76].",1,neutral
"Among the literature of vision pre-training itself, MAE [22, 67] and SimMIM [78] reconstruct the pixels of the masked image patches to enhance visual representation.",1,neutral
"While MVM has been explored in pure vision tasks [4, 22, 75], it remains an open question whether MVM can facilitate the interactions between video and language modalities.",1,neutral
"In addition, most of the existing Transformers [51], [52], [53] divide images or videos into nonoverlapping patches to generate tokens, resulting in loss of local details and cannot be well applied to video reconstruction tasks in this work.",1,neutral
"Recently, we have seen great success in natural language processing (NLP), as transformer models like BERT, GPT-3, RoBERTa, and other variants have achieved top performance on a wide array of language tasks.",2,positive
", ImageNet) processing, such as Image GPT, Swin transformer [13], and MAE [5].",1,neutral
This appetite for data has been successfully addressed in NLP by self-supervised pre-trained models such as BERT and GPT-3 [5].,1,neutral
"Similar models has been successful in producing strong features for natural images (e.g., ImageNet) processing, such as Image GPT, Swin transformer [13], and MAE [5].",1,neutral
"Conventionally, one would train an auto-encoder neural network comprising an encoder and a decoder, to obtain embeddings that contain summarized information of the encoder’s input [10].",1,neutral
", in 2021, showed that randomly masking pixels of an input image helps an autoencoder learn more robust embeddings, which would be useful for subsequent fine tuning tasks [10].",1,neutral
"As mentioned earlier, recent deep learning research has shown that incorporating self-supervised learning as part of the neural network’s training process can improve model performance [10][11][12].",1,neutral
"Recently, a new self-supervised learning approach named masked autoencoders (MAE) [15] demonstrates a strong generalization capability with remarkable performance in computer vision tasks.",1,neutral
"Inspired by the MAE [15], we propose a spatial-temporal masked autoencoder framework for self-supervised 3D skeleton-based action recognition (SkeletonMAE).",2,positive
"However, MAE [15] can not be directly utilized for selfsupervised skeleton action recognition due to the following reasons:",1,neutral
• The Vision Transformer (ViT) [10] architecture is used in MAE [15] to process the image input.,1,neutral
"The Transformer architecture [18] is now ubiquitous in Natural Language Processing [19, 20, 21, 22], and is also gaining traction in Computer Vision [23, 24], as well as in Offline Reinforcement Learning [25, 26].",1,neutral
"%) of reconstruction achieves the peak of classification performance, which is as high as the masking ratio of Image MAE [9].",1,neutral
"The high masking ratio (75%) of reconstruction achieves the peak of classification performance, which is as high as the masking ratio of Image MAE [9].",1,neutral
"In particular, some recent research on the field of SSL [4, 9, 10] has shown excellent results, yet self-supervision alone is still insufficient due to its limited practical applicability.",1,neutral
"In the field of computational vision, especially in the area of image classification and image generation, PixelCNN [34], VQ-VAE-2 [28], and MAE [9] successfully used the whole input image as the self-supervised target.",1,neutral
This paper links the inductive semi-supervised algorithm (e.g. Pseudo Label [13]) with the generative self-supervision (e.g.MAE [9]).,1,neutral
"Compared to generative SSL, the motivation of contrastive SSL is to measure the similarity of different inputs (e.g., mutual informationmaximization and instance discrimination).",1,neutral
The generative SSL trains a generator consisting of an encoder and decoder to reconstruct the input data.,2,positive
"Like what ImageMAE does in [9], we directly discard a subset (e.g., 50",1,neutral
"As for the generative-contrastive SSL, most works focus on learning knowledge from unlabeled data with generative adversarial networks.",1,neutral
"Like what ImageMAE does in [9], we directly discard a subset (e.",1,neutral
"Besides, Self-Supervised Learning (SSL) is also a powerful learning framework that exploits the generalizable representations from unlabeled data.",1,neutral
"Through Liu’s research [21] on SSL, its main methods can be divided into three categories: generative SSL, contrastive SSL, and generative-contrastive SSL.",1,neutral
"We propose the MAE-VQGAN model, which combines ideas from MAE [20] and VQGAN [15].",2,positive
"Based on the recent success of Vision Transformers (ViTs) [13], multiple works have proposed to hole-filling a self-supervised pretext task for ViTs [1, 20, 54].",1,neutral
"For example, in MAE [20], the goals is to reconstruct the image given a small subset of input patches.",1,neutral
"During training, an input image is patchified, masked and fed into an MAE [20].",1,neutral
"While N3F can work on top of any 2D dense image features, including recent ones based on Vision Transformers (ViT) [11] and variants [2, 5, 14, 19, 31, 58, 59, 68, 76, 81], of particular interest are self-supervised versions such as [4, 8, 17, 28] as they are more generically applicable and can benefit more from the consistency induced by N3F.",1,neutral
as self-supervised learning [53] and foundation models [54].,1,neutral
"Considering that the continuous data flow in each vehicle brings massive unlabelled data, self-supervised learning [53] and",1,neutral
"Recently, to encourage learning useful features from images with spatial redundancy, MAE [8] presents a simple strategy to reduce redundancy by masking a very high portion of random patches.",1,neutral
"The intuitive strategy of combating spatial redundancy is to apply redundancy removal approaches to pre-process images before being input into the networks, including PCA [6], DCT [7], etc. Recently, to encourage learning useful features from images with spatial redundancy, MAE [8] presents a simple strategy to reduce redundancy by masking a very high portion of random patches.",1,neutral
"Among vision learners, vision transformers (ViTs) [9] enjoy the ability to capture intra-image long-range dependencies and exhibit impressive performances, as MAE [8] uses ViTs as encoders.",1,neutral
"…our lexiconbottlenecked masked autoencoder (LexMAE) contains one encoder and one decoder with masked inputs in line with the masked autoencoder (MAE) family (He et al., 2021a; Liu & Shao, 2022), while is equipped with a lexicon-bottlenecked module for document-specific lexicon-importance learning.",2,positive
"Recently, pre-trained language models (PLM), e.g., BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), DeBERTa (He et al., 2021b), have been proven generic and effective when transferred to a broad spectrum of downstream tasks via fine-tuning.",2,positive
"We also compared our model with the few-shot counting sota method Fam-
Net [24], and our model outperforms it with a large advance (15.16 MAE and 24.29 RMSE on Val-COCO and 11.87 MAE and 14.81 RMSE on Test-COCO), which demonstrates the superiority of our model.",2,positive
"We use two standard metrics to measure the performance of our model, namely, Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE).",2,positive
"MAE = 1 NI
NI ∑ i=1 |Ci−CGTi |, RMSE = √√√√ 1 NI NI ∑ i=1 (Ci−CGTi )2 (6)
Here, NI is the total number of testing images, and Ci and CGTi are the predicted number and ground truth of the ith image.",1,neutral
"In this section, we aim to give the detail of our proposed two-stage training procedure, that is, first pre-train the ViT encoder with MAE [11], and then fine-tune the whole model on supervised object counting.",2,positive
"We propose a two-stage training scheme, with the transformer-based image encoder being firstly pre-trained with self-supervision via masked image modeling [11], followed by supervised fine-tuning for the task at hand.",2,positive
"# Shots Val Test
MAE RMSE MAE RMSE
A0 % % % % 0 24.84 86.",0,negative
"As input for pre-training ViT with MAE, we randomly drop 50% of the visual tokens, and task the model to reconstruct the masked patches with pixel-wise mean square error.",2,positive
MAE Pre-training.,0,negative
Self-supervised Pre-training with MAE.,0,negative
"1 Training Details In this section, we aim to give the detail of our proposed two-stage training procedure, that is, first pre-train the ViT encoder with MAE [11], and then fine-tune the whole model on supervised object counting.",2,positive
"Specifically, we adopt the recent idea from Masked Autoencoders (MAE), to train the model by image reconstruction with only partial observations.",2,positive
"In Section 3.2, we further introduce a two-stage training regime, in which the model is firstly pre-trained with self-supervision via masked image reconstruction (MAE), followed by fine-tuning on the downstream counting task.",2,positive
"Many work in self supervised learning (SSL) have been proposed [10, 30], in particular, the methods based on masked image reconstruction have achieved SOTA results [6, 29].",1,neutral
"The two methods are based on the recent Masked Auto Encoder (MAE) approach [26], where we randomly mask the image ( e.g. , 75",1,neutral
"The two methods are based on the recent Masked Auto Encoder (MAE) approach [29], where we randomly mask the image (e.",1,neutral
"We also exploit more efficiently the input data with Masked Image Modeling, based on the recently proposed Masked Auto Encoder (MAE) [26].",1,neutral
"We also exploit more efficiently the input data with Masked Image Modeling, based on the recently proposed Masked Auto Encoder (MAE) [29].",1,neutral
"We propose a new efficient VLP approach centered on 3 main components; stronger Vision-Language pre-alignment through hierarchical contrastive objective, self supervision via masked image modeling based on MAE, and a new Visual Concepts injection and extraction technique.",2,positive
"We favor the MAE based, unimodal MIM loss which improves the results by 2.4% RSUM.",0,negative
"Unsupervised Learning Techniques Among the most popular unsupervised learning techniques for images, there are exist autoencoders [29], sparse coding networks [30], and Generative Adversarial Networks (GANs) [31].",1,neutral
"Based on the above insight, inspired by the Vision Transformer, which is entirely based on a self-attentive mechanism in machine vision tasks [5],and its good performance in the Masked Auto-Encoder task [6],a network structure named Padded Auto-Encoder for reactor monitoring parameter feature extraction is proposed, taking into account the actual needs in reactor accident diagnosis.",2,positive
"Generative methods [2,16,36] try to reconstruct the original input to learn meaningful latent representation.",1,neutral
"Predicting the enhancing methods utilized for images, predicting the relative placements of image blocks, recoloring sample grayscale maps [21], and restoring the missing parts of images to complete them [22] are some of the gen-",1,neutral
SimMIM [27] and MAE [11] both propose to directly reconstruct the pixel values of the masked image patches.,2,positive
"With the most recent representative work in this direction, masked autoencoder (MAE) [11], we are now able to efficiently and effectively pre-train high-capacity Vision Transformers (ViTs) with strong feature representations, leading to state-of-the-art solutions for a wide range of downstream visual tasks.",2,positive
"Following MAE [11], we first perform self-supervised pre-training on ImageNet-1k [7].",2,positive
We choose a decoder depth of 8 as the default setting as in [11].,2,positive
"Similar to MAE [11], the final performance gets (slightly) increased with a deeper decoder.",2,positive
"Following the setting in MAE [7], we add a batch-norm layer without learnable affine parameters before the classifier to avoid adjusting the learning rate for each model.",2,positive
"CLIP
+
SimCLR
CLIP
+ MAE
Sandwich bread
MaskCLIP
(Ours)
CLIP
Bird
MaskCLIP
(Ours)
CLIP
CLIP
+
SimCLR
CLIP
+ MAE
SnowMountain goats Santa hatBearded Man
BandanaDog",0,negative
We start from CLIP+MAE and add three components of the distillation loss one by one.,2,positive
"Progress in self-supervised pretraining in computer vision has led to improvements on a wide range of transfer learning tasks such as image classification, object detection, and semantic segmentation [6, 8, 25, 26, 28].",1,neutral
We adopted the MAE structure proposed in [14].,2,positive
"Recently, masked autoencoder (MAE) [14] has been proposed, which combines autoencoding and pixel masking to learn discriminative image features.",1,neutral
"8: In pixel-level pretext tasks (§3.2.1), the aim is to reconstruct the original image x̂ from a corrupted input x.
corrupted input images, as represented by three standard low-level image processing tasks: (1) image inpainting [126], [127] learns by inpainting the masked-out missing regions in the input images, which is also known as masked autoencoders (MAE) [127]; (2) denoising [128] learns to denoise the partial destructed input; and (3) colorization [129], [130], [131] aims to predict the colour values of the grayscale images.",1,neutral
"Pretext tasks Pixel-level reconstruction [124], [125], inpainting [126], MAE [127], denoising [128], colorization [129], [130], [131]",1,neutral
"corrupted input images, as represented by three standard low-level image processing tasks: (1) image inpainting [126], [127] learns by inpainting the masked-out missing regions in the input images, which is also known as masked autoencoders (MAE) [127]; (2) denoising [128] learns to denoise the partial destructed input; and (3) colorization [129], [130], [131] aims to predict the colour values of the grayscale images.",1,neutral
"Pixel-level pretext task is generally designed as a dense prediction task that aims to predict the expected pixel values of an output image as a self-supervision signal [124], [125], [126], [127], [128], [129], [130], [131].",1,neutral
"Families of Models Model Rationale Representative Strategies and Methods
Pretext tasks Pixel-level reconstruction [124], [125], inpainting [126], MAE [127], denoising [128], colorization [129], [130], [131] Instance-level predict image rotations [123], scaling and tiling [122], patch ordering [11], patch re-ordering [121]
Discriminative models Instance discrimination negative sampling large batch size (SimLR [12]), memory bank (InstDis [132]), queue (MoCo [16]) input transformation data augmentation (PIRL [133]), multi-view augmentation (CMC [134]) negative-sample-free simple siamese (SimSiam [135]), Bootstrap (BYOL [136]), DirectPred [137]
Deep clustering offline clustering DeepCluster [138], JULE [139], SeLa [140] online clustering IIC [141], PICA [142], AssociativeCluster [143], SwAV [144]
Deep generative models Discriminator-level DCGAN [145], Self-supervised GAN [146], Transformation GAN [147] Generator-level BiGAN [148], BigBiGAN [149]
✓",1,neutral
"Following the previous work [9], we take ViTB [57] as the backbone network, which consists of 12 transformer layers and was pre-trained on ImageNet-21K with the self-supervised method MAE [21].",2,positive
"Note that the patch-level strategy is essentially similar to [8], which was proposed to analyse 2D images.",1,neutral
"Different from the 2D approach [8], a dual-level masking strategy is adopted to explicitly extract features in both the spatial and temporal dimensions.",1,neutral
"Inspired by [8], we design a decoder using a lightweight transformer structure [8] and is only used during the pre-training phase.",2,positive
"For image encoder, we employ a standard ViT-B/16 (Dosovitskiy et al. 2020) with random masking strategy (He et al. 2022).",2,positive
"Following MAE (He et al. 2022), we randomly divide the patches",1,neutral
"Following (He et al. 2022), we take a 4-layer transformer with 512 dim as our image decoder.",2,positive
"MAE (He et al. 2022) and SimMIM (Xie et al. 2022) predict pixel RGB values directly to promote image pre-training, achieving even better performance than complicatedly designed token classification methods.",2,positive
"Unlike MAE (He et al. 2022), we aim to reconstruct the invisible patches with both image and text features to facilitate multi-modal information fusion.",2,positive
"Motivated by recent generative methods in visual self-supervised learning (He et al. 2022; Chen et al. 2022), we propose to introduce pixel-level reconstruction task to VLP.",2,positive
"Following MAE (He et al. 2022), we adopt a lightweight vision transformer as our image decoder.",2,positive
"Following MAE (He et al. 2022), we randomly divide the patches into visible patches { xvisi }N−M i=1
and invisible patches{ xmski }M i=1
according to mask ratio α, where M = αN .",1,neutral
"Recently, with asymmetric encoder-decoder architecture, masked autoencoders (MAE) (He et al. 2022) exhibits high generalizability and remarkable performance in vision tasks.",1,neutral
"According to the finding in MAE (He et al. 2022), unlike contrastive learning models, generative learning models are not prone to saturation in training, and a longer training schedule can further boost model performance.",1,neutral
"The autoencoder architectures [74] based on FCNs achieved encouraging performance on image segmentation tasks and have been widely used in the field of medical image segmentation [5][11], target detection [8] and video object segmentation [55].",1,neutral
", through learning invariant mapping (DrLIM), It is used to learn nonlinear functions of global coherence[7] , paving the way for Kaiming He to propose an expanded self-supervised learning scheme Masked AutoEncoders(MAE)[8].",1,neutral
"[51] proposed an asymmetric encoder-decoder architecture, which the encoder operates only on a subset of visible patches (tokens without masks).",1,neutral
BMM shares a similar idea with MAE [13] and BEIT [2] in that both learn better representation through random masking.,1,neutral
MAE and BEIT aim at image reconstruction using an autoencoding style.,2,positive
"The above method shares a similar idea to Random Erasing [50], MAE [13], and BEIT [2].",1,neutral
"On the other hand, some self-supervised learning-based methods, such as MoCo [48], BYOL [49], and MAE [50], can also alleviate the requirement of large-scale training data.",1,neutral
"The other two better performing transformer architectures are MixMIML and MAE (ViT-H), which have ∼300M and ∼600M parameters and perform at 83.9% and 88.3% respectively.",2,positive
"We compared the performance with other baselines involving Vision Transformers such as ConViT [16], Masked Auto Encoders (MAE) [53], Convolution-enhanced image Transformer (CeiT) [33], LeViT [34].",2,positive
"Additionally, examining different training strategies, such as the masked-autoencoder discussed in [9], might also be beneficial.",1,neutral
", masked autoencoding) has achieved great success for representation learning in the fields of computer vision [56], [57], [58], natural language processing [52], [59], and speech signal processing [60], [61].",1,neutral
", only utilize local neighbor information to accomplish this task instead of inferring global semantics) [17], [58], [62].",1,neutral
"Inspired by (He et al. 2022; Wettig et al. 2022) that a large mask rate may be beneficial for pre-training, we employ a mask rate greater than the BERT’s 15% setting.",2,positive
"Inspired by (He et al. 2022; Wettig et al. 2022) that a large mask rate may benefit pre-training, we adopt up to 30% and 45% mask rate in CoT-MAE’s encoder and decoder, respectively.",2,positive
"Thus, borrowing insights from masked autoencoders [33], we believe most of the points can be redundant and we randomly masked the points with a probability of 75% per visit.",2,positive
", 2020) and has also helped excel at various computer vision tasks (Goyal et al., 2021; Ramesh et al., 2021; He et al., 2022).",2,positive
"This new paradigm has had great success in advancing NLP (Devlin et al., 2019; Conneau et al., 2020; Brown et al., 2020) and has also helped excel at various computer vision tasks (Goyal et al., 2021; Ramesh et al., 2021; He et al., 2022).",2,positive
"The longsequence prediction data set adopts the evaluation indicators of the Mean Squared Error (MSE) and Mean Absolute Error (MAE), and the short-sequence data set adopts the evaluation indicators of the Root Relative Squared Error (RSE) and Empirical Correlation Coefficient (CORR).",2,positive
2020) in Natural Language Processing (NLP) or MAE (He et al. 2022) in computer vision.,1,neutral
"In the long series prediction, we employ Mean Squared Error (MSE) and Mean Absolute Error (MAE) as indicators and follow the setting (Zhou et al. 2021).",1,neutral
"…deep learning models add more functional modules or modify the training process to improve accuracy, which is representative of the trend of self-supervised or unsupervised learning, such as GPT-3 (Brown et al. 2020) in Natural Language Processing (NLP) or MAE (He et al. 2022) in computer vision.",1,neutral
"The above deep learning models add more functional modules or modify the training process to improve accuracy, which is representative of the trend of self-supervised or unsupervised learning, such as GPT-3 (Brown et al. 2020) in Natural Language Processing (NLP) or MAE (He et al. 2022) in computer vision.",2,positive
"The formula is as follows: • Mean Squared Error (MSE):
MSE = 1
n n∑ i=1 (Yi − Ŷi)2
• Mean Absolute Error (MAE):
MAE = 1
n n∑ i=1 |Yi − Ŷi|
In the short series prediction, we employ Root Relative Squared Error (RSE) and Empirical Correlation Coefficient (CORR) as indicators and follow the setting (Lai et al. 2018).",2,positive
"In the absence of prior work applying masked autoencoding to abstract/synthetic imagery, we explore a large masked pixel percentage (75% of the image), thus forcing the model to attempt to recover the image based only on the unmasked 25% of the image, such high percentages have been shown to work well for natural imagery [30].",2,positive
Recent work [30] has shown that masking a large portion of pixels in natural images (such as ImageNet) leads to a challenging self-supervisory task capable of generating useful representations for downstream tasks.,1,neutral
[13] constructed a scalable self-supervised learner which masked random patches of the input image and reconstructed them.,1,neutral
"In CV, both masked autoencoders [15] and contrastive learning [31], with different architectures, obtain promising results.",1,neutral
"ResNet [7], ConvNext [8], ViT [9], 16 Swin [10], MAE [11], Transformer-XL [12] and BERT [13].",2,positive
"All these results show that in most cases, Adan can use half of the training cost (epochs) of SoTA optimizers to achieve higher or comparable performance on ViT, ResNet, MAE, etc.
NLP Results.",2,positive
%) of ViT-B and ViT-L trained by selfsupervised MAE on ImageNet.,0,negative
"MAE-ViT-B MAE-ViT-L Epoch 300 800 1600 800 1600
AdamW 82.9 — 83.6 85.4 85.9 Adan 83.4 83.8 — 85.9 —",0,negative
"More surprisingly, Adan can use half of the training cost (epochs) of SoTA optimizers to achieve higher or comparable performance on ViT, ResNet, MAE, etc, and also shows great tolerance to a large range of minibatch size, e.g. from 1k to 32k. Code is released at https://github.com/sail-sg/Adan.",2,positive
"Extensive experimental results show that Adan surpasses the corresponding SoTA optimizers for vision, language, and RL tasks and sets new SoTAs for many popular networks and frameworks, e.g. ResNet [7], ConvNext [8], ViT [9], Swin [10], MAE [11], Transformer-XL [12] and BERT [13].",2,positive
"2) self-supervised settings: we follow the MAE training framework to pretrain and fine-tune ViT-B and ViT-L, and report results in Table 3.",0,negative
"As shown in Table 3, compared with MAE (He et al., 2022), BEIT V2 achieves dramatic gains across datasets, demonstrating the superiority of the proposed method in terms of model generalization.",2,positive
"Existing MIM approaches can be coarsely categorized to three according to the reconstruction targets: low-level image elements (e.g., raw pixels; He et al. 2022; Fang et al. 2022; Liu et al. 2022), handcrafted features (e.",2,positive
"Masked image modeling (MIM), which greatly relieves the annotation-hungry issue of vision Transformers, has demonstrated great potential in learning visual representations (Bao et al., 2022; He et al., 2022).",1,neutral
"MAE (He et al., 2022) treated MIM as a denoising pixel-level reconstruction task.",1,neutral
"Existing MIM approaches can be coarsely categorized to three according to the reconstruction targets: low-level image elements (e.g., raw pixels; He et al. 2022; Fang et al. 2022; Liu et al. 2022), handcrafted features (e.g., HOG features; Wei et al. 2021), and visual tokens; Bao et al. 2022; Wang…",2,positive
All the compared methods are based on ViT-B/16 and pretrained for 300 epochs except MAE for 1600 epochs.,0,negative
"Table 2 presents the top-1 accuracy for linear probing and compares BEIT V2 with recent methods including BEIT, CAE, MAE, MVP and MoCo v3.",2,positive
VideoMAE [13] further extends MAE to video and shows data-efficient learners for self-supervised video pre-training.,2,positive
"Specifically, motivated by the autoencoding paradigm in BERT [6] in NLP, MAE adopts an asymmetric encoder-decoder architecture with visible patches encoding in the encoder and masked token reconstruction in the decoder.",2,positive
"To obtain the benefits of self-supervised learning in video classification, we utilize VideoMAE pre-trained in Something-Something [8] to extract video features inherent in make-up scenes.",2,positive
2.1.2 MAE-based Features.,2,positive
"As the existing pre-trained models, e.g., VideoSwin [11] and VideoMAE [13], are pre-trained on benchmarks like action recognition, the extracted video features are inevitably more suitable to general-purpose action scenes and not optimal for the specialized-purpose make-up scenes.",1,neutral
"Among them, Masked Autoencoders (MAE) [9] demonstrate superior learning ability and scalability.",1,neutral
"Recently, the pre-training and fine-tuning paradigm [30, 31, 32, 33] achieves promising results.",2,positive
"The reason for lower performance on STL-10 might result from the usage of the self-supervised pre-trained model [33], rather than the supervised pre-trained model is used in other settings.",1,neutral
The models are self-pretrained by MAE[24].,0,negative
"Effect of Self-pretraining The self-pretraining of MAE [24] has a substantial boost in performances, as seen in Table 3.",1,neutral
"When self-pretrained, MAE [24] is mainly used, and we directly use their pretrained models.",2,positive
"When the model is self-pretrained by MAE [24], we first evaluate the fine-tuning performances of MAE on the labeled data only, as the common practice in self/un-supervised learning literature [25, 14, 22], with results shown Table 1.",1,neutral
"In the past few years, Vision Transformers (ViT) [18], which adapt the transformer architectures [60] to the visual domain, have achieved remarkable progresses in supervised learning [59, 41, 69], un/self-supervised learning [16, 12, 24], and many other computer vision tasks [11, 19, 1, 54] (with architecture modifications).",1,neutral
"To ensure the representation quality of the pretrained model, previous methods [2, 25, 56] usually require very long pretraining epochs.",1,neutral
"Full fledged transformer blocks are used in the decoder of MAE [25] to reconstruct masked input patches pixel by pixel, whereas lightweight linear layer is adopted in the decoder of MaskFeat [56] to reconstruct local features of the image.",1,neutral
"Following the training recipe provided by MAE [25], the ViT models pretrained on ImageNet1K dataset serve as the backbone of UperNet [59], and are finetuned together with the segmentation layers.",2,positive
We use a masked autoencoder architecture similar to MAE [25].,2,positive
", grid, block, random) of the input image patches affect the final performance of masked image pretraining [25, 61].",1,neutral
"Following MAE [25], the pretrained ViT backbones are adapted to FPN [36] in the Mask R-CNN framework [27], which is finetuned end-to-end on COCO training set to produce the bounding boxes (evaluated by box AP) and the instance masks (evaluated by mask AP) simultaneously.",2,positive
MAE [25] uses a deep decoder that not only updates the mask tokens but also enhances the features of the unmasked patches.,2,positive
"Majority of prior arts [2, 3, 9, 18, 25, 56, 61] sample the masked patches uniformly at random since it is unbiased and can guarantee coverage.",0,negative
"Using the default 75% masking ratio, our prompting decoder reduces the decoding computation cost by 20% compared to MAE [25].",2,positive
"Works in [3, 25, 56, 61] adopt similar techniques in CV to address the data-hungry issue of ViT models.",1,neutral
"MAE [6], which is designed for more efficient self-supervised pre-training, proposes dropping a high proportion of patches and subsequently inferring the missing patches through an autoencoder setup.",2,positive
"is that visual data, often, contains considerable redundancy or correlation in appearance [6] (see Figure 2).",1,neutral
"Our work takes some inspiration from MAE, however, PatchDropout can be applied to target tasks directly using standard ViTs (unlike MAE).",2,positive
Later studies [28] show higher mark ratio (e.,0,negative
"While scaling up training sets has indeed been integral to the recent progress in large models, advances in weak and self-supervision [14, 29, 30, 12, 20] have led to an abundance of potential training data.",1,neutral
"To ensure the effectiveness of training, we compute the loss on the common parts of the masked patches of T1 and unmasked patches of T2 (following [29]).",2,positive
"For simplicity, we take MAE [29] as an representative example of MIM, in which the similarity measurement is simply l2−distance on the masked patches.",1,neutral
"iBOT [57], MSN [2] and data2vec [4], whose frameworks involve various distance measurements between the siamese branches instead of reconstructing the unmasked parts, however, achieve comparable or even better results than the original reconstructionbased MIMs like [5, 29].",2,positive
"Several works try to interpret MIM from different views, for example, [29] suggests MIM model learns ""rich hidden representation"" via reconstruction from masked images; afterwards, [8] gives a mathematical understanding for MAE [29].",1,neutral
"3In original MAE [29], the encoder network only generates tokens of unmasked patches and the decoder only predict the masked patches during training.",2,positive
9 – MAE [29] patch masking reconstructive 1600 83.,0,negative
"l2-distance [29], cross-entropy [5] or perceptual loss [20] in codebook space.",1,neutral
"Furthermore, taking MAE [29] as a representative example of MIM, we empirically find the success of MIM models relates a little to the choice of similarity functions, but the learned occlusion invariant feature introduced by masked image – it turns out to be a favored initialization for vision transformers, even though the learned feature could be less semantic.",1,neutral
"Compared with conventional contrastive methods, MIM requires fewer effort on tuning the augmentations, furthermore, achieves outstanding performances especially in combination with vision transformers [21], which is also demonstrated to be scalable into large vision models [29, 37].",1,neutral
"Inspired by the above pilot studies, we employ the same MIM pretraining and finetuning routine to investigate the influences of plain ViTs on RS tasks.",2,positive
"Recently, reconstruction-based SSL methods such as MAE [12] have been proposed and shown effective for pretraining plain ViTs and adapting them for downstream tasks [13], [14].",1,neutral
"For more details about MAE, please refer to [12].",0,negative
"Some works resort to self-supervised learning [12], [32]–[34] with different RS characteristics taken into the design, e.",1,neutral
2) MAE: MAE [12] aims to recover the masked image parts given the visible ones with an encoder-decoder structure.,2,positive
"The models are trained for 1,600 epochs if not specified, following the default setting in MAE [12].",0,negative
"Different from these works, we use the representative MAE method [12] for MIM pretraining and specifically focus on advancing plain ViTs for remote sensing tasks.",2,positive
"image modeling (MIM) [12], recent work reveals that such a pretraining process can give a good initialization for plain ViTs to achieve surprising results on various downstream tasks including object detection and pose estimation [13], [14].",1,neutral
"Thanks to the development of the unsupervised learning in masked image modeling (MIM) [12], recent work reveals that such a pretraining process can give a good initialization for plain ViTs to achieve surprising results on various downstream tasks including object detection and pose estimation [13], [14].",1,neutral
"Different from these works, we use the representative MAE method [12] for",1,neutral
"To explore the effectiveness of MIM pretraining in remote sensing tasks, we pretrain our model using MAE [12] with the large-scale RSI dataset, i.e., MillionAID [11].",2,positive
"To explore the effectiveness of MIM pretraining in remote sensing tasks, we pretrain our model using MAE [12] with the large-scale RSI dataset, i.",2,positive
