text,target_M6_predict,target_predict_M6_label
"Many prior studies have found hate-speech classification to be biased against Black people [16, 27, 28, 46] and other marginalized groups [54].",1,neutral
"Anti-Black biases in hate speech/offensive speech classification [27, 28, 46] and sexist biases in nudity classification [20] have both been identified in prior literature on content moderation.",1,neutral
"Prior research Black social media users are more likely to have their speech be classified as hate speech or otherwise offensive speech by content moderation systems [27, 28, 46], more likely to be classified as negative sentiment in sentiment analysis systems [31], have higher word error rates in speech-to-text captioning [33, 52], and broadly to have poor performance on natural language processing systems in general [6, 60].",0,negative
"These issues are part of a larger phenomenon of unequal treatment of minoritized groups by technological systems; from algorithm biases making unfair and inaccurate predictions and classification for marginalized groups [5, 27, 28], to racism and bigotry on social media causing these groups to experience heightened harassment, abuse, and mistreatment [40, 47], Black content creators must work around these challenges to succeed in the creator economy.",1,neutral
"1, these ’shadowbanning’ experiences imply the existence of biases against Black folks and other marginalized groups, consistent with content moderation biases identified on other platforms [6, 27, 28, 46].",1,neutral
"The black-box nature of these classifiers and datasets contain the risk of predicting text features of some identities as more offensive than others without sufficient understanding of contexts surrounding the identity, such as African-American English (Sap et al., 2019; Harris et al., 2022).",1,neutral
"This phenomenon of perpetuating biases from annotated data has been discovered and investigated in tasks such as hate speech detection (Sap et al., 2021; Xia et al., 2020; Harris et al., 2022; Davidson et al., 2019) and sentiment analysis (Kir-",1,neutral
"…biases from annotated data has been discovered and investigated in tasks such as hate speech detection (Sap et al., 2021; Xia et al., 2020; Harris et al., 2022; Davidson et al., 2019) and sentiment analysis (Kiritchenko and Mohammad, 2018; Garg et al., 2022) where individual perspectives…",1,neutral
", 2022) and data bias (Park et al., 2018; Dixon et al., 2018; Dodge et al., 2021; Harris et al., 2022) are the cause of this impact, and some studies have investigated the connection between training data and downstream task model behavior (Gonen and Webster, 2020; Li et al.",1,neutral
"…et al., 2019; Sap et al., 2019; Davani et al., 2022; Sap et al., 2022) and data bias (Park et al., 2018; Dixon et al., 2018; Dodge et al., 2021; Harris et al., 2022) are the cause of this impact, and some studies have investigated the connection between training data and downstream task model…",2,positive
Another major documented issue is that the Perspective model displays false positive bias for AAE (African American English) dialect examples [64] and for a host of demographic identity terms that were over-represented among the toxic comments in the training dataset [20].,0,negative
and that profanity in particular is a significant factor in driving misclassification of AAE [31].,1,neutral
"Highlighted examples of unique insights raised by study participants
and that profanity in particular is a significant factor in driving misclassification of AAE [31].",1,neutral
"…et al., 2018; Zhao et al., 2018) and other categories of social biases (Nangia et al., 2020; Nadeem et al., 2021; Parrish et al., 2022), perform poorly against minority demographic groups (Koh et al., 2021; Harris et al., 2022) or dialectical variations (Ziems et al., 2022; Tan et al., 2020).",0,negative
