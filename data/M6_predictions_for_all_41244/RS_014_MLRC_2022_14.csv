text,target_M6_predict,target_predict_M6_label
"Dialogue summarization, a recently popular subfield of text summarization, has more challenging factual issues involved (Wang et al., 2022; Gao and Wan, 2022).",1,neutral
"The outputs of each system on the SAMSum test set are obtained from DialSummEval (Gao and Wan, 2022).",2,positive
DialSummEval: Revisiting summarization evaluation for dialogues.,0,negative
"The DialSummEval (Gao and Wan, 2022) benchmark is a summarization evaluation benchmark created following the format of SummEval (Fabbri et al., 2021) for the domain of dialogue summarization.",2,positive
"The DialSummEval (Gao and Wan, 2022) benchmark is a summarization evaluation benchmark created following the format of SummEval (Fabbri et al.",2,positive
", 2022) and DialSummEval (Gao and Wan, 2022) and uncover limitations that guide the design principles of the SUMMEDITS benchmark we build.",2,positive
"Prior work (Kryściński et al., 2020; Fabbri et al., 2021; Gao and Wan, 2022) has annotated corpora Human Perform.",2,positive
"In this section we analyze two popular benchmarks for factual consistency detection in summarization: AggreFact (Tang et al., 2022) and DialSummEval (Gao and Wan, 2022) and uncover limitations that guide the design principles of the SUMMEDITS benchmark we build.",2,positive
"In DialSummEval, each (dialogue, summary) tuple is evaluated by three annotators, each assigning a Likert score (1-5) assessing the consistency of the summary.",2,positive
"Although most annotation effort has focused on the summarization of news, some prior work also looked at dialogue summarization (Gao and Wan, 2022), or the medical domain (Tang et al.",0,negative
"Following previous research (Gao and Wan, 2022; Kryscinski et al., 2019), we demand human annotators evaluate samples on the summary level from the following three aspect: Relevance measures how well the question summary captures the main concerns of the patient’s questions.",2,positive
"∗ Equal contribution for manual evaluation (Bhandari et al., 2020; Fabbri et al., 2022a; Gao and Wan, 2022).",0,negative
"Thus, recent efforts have focused on aggregating model outputs and annotating quality dimensions to better assess summarization model and metric progress (Huang et al., 2020; Bhandari et al., 2020; Stiennon et al., 2020; Zhang and Bansal, 2021; Fabbri et al., 2022a; Gao and Wan, 2022).",2,positive
", 2021b) and DialSummEval (Gao and Wan, 2022) collections of system summaries, respectively, rather than generating summaries from scratch.",2,positive
"With each dataset we collect system summaries for a set of 100 randomly selected samples from the test set, following recent work on measuring correlations between metrics (Bhandari et al., 2020; Fabbri et al., 2021b; Gao and Wan, 2022).",2,positive
"12https://github.com/PKULCWM/PKUSUMSUM is used for Lead, LexPageRank, and ClusterCMRW
13https://github.com/RaRe-Technologies/ gensim
14https://pypi.org/project/ bert-extractive-summarizer/
15https://github.com/Yale-LILY/SummEval 16https://github.com/kite99520/
DialSummEval
Models: LEAD-3, LONGEST-3, Pointergenerator (See et al., 2017), Transformer (Vaswani et al., 2017), BART (Lewis et al., 2019), Pegasus (Zhang et al., 2020), UniLM (Dong et al., 2019), CODS (Wu et al., 2021), ConvoSumm (Fabbri et al., 2021a), MV-BART (Chen and Yang, 2020), PLM-BART (Feng et al., 2021), Ctrl-DiaSumm (Chen et al., 2021), S-BART (Chen and Yang, 2021).",2,positive
"Evaluation metrics have also been reevaluated in the context of scientific articles (Cohan and Goharian, 2016), and more recently, dialogues (Gao and Wan, 2022), both using single documents as input.",2,positive
"For comparable results, for the CNN/DM (Hermann et al., 2015) and SAMSum (Gliwa et al., 2019) datasets, we use the model outputs from the SummEval (Fabbri et al., 2021b) and DialSummEval (Gao and Wan, 2022) collections of system summaries, respectively, rather than generating summaries from scratch.",2,positive
