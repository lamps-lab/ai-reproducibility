text,target_M6_predict,target_predict_M6_label
"Additionally, we release the checkpoints of the latter two monolingual BERT models (BERT-small and BERT-base) mentioned above (Garcia, 2021).",2,positive
"• BERT-small (6 hidden layers) and BERT-base (12 layers) released by Garcia (2021), both trained on a corpus of about 550M tokens.",2,positive
", 2018) and language models such as SemantiGal (Garcia, 2021), Bertinho (Vilares et al.",2,positive
"Also available are language analysis and information extraction tools like Linguakit (Gamallo et al., 2018) and language models such as SemantiGal (Garcia, 2021), Bertinho (Vilares et al., 2021), as well as other resources.",2,positive
"…on the CCNet tools and data (Ortega et al., 2022b), the development and testing of two BERT language models (with 12 and 6 layers, respectively) (Garcia, 2021), as well as the development and testing of a Spanish-Galician neural machine translation (NMT) system prototype (Ortega et al., 2022a).",2,positive
", 2022b), the development and testing of two BERT language models (with 12 and 6 layers, respectively) (Garcia, 2021), as well as the development and testing of a Spanish-Galician neural machine translation (NMT) system prototype (Ortega et al.",2,positive
", 2018) or between instances of homonyms and synonyms (Garcia, 2021).",1,neutral
"Although Portuguese and Galician have strong historical ties, they are categorized as two different languages (Ramallo and Rei-Doval, 2015; Garcia, 2021).",0,negative
"…with 12 hidden layers) provided by Devlin et al. (2019), we evaluate the following monolingual models: Bertinho-base, with 12 layers (Vilares, Garcia, and GómezRodŕıguez, 2021), and two models of BertGalician (‘base’ and ‘small’) released by Garcia (2021), with 12 and 6 layers, respectively.",2,positive
"In addition, we have created a new dataset in GalicianPortuguese, English and Spanish that includes examples of homonymy and synonymy in context, also used to compare various contextualization models and strategies [21].",2,positive
