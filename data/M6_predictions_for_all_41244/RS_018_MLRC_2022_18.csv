text,target_M6_predict,target_predict_M6_label
"” The sources of unfairness are many, including data sampling bias or under-representation [16, 70, 15, 7], data labeling bias [60, 65, 26], model architecture (or feature representation) [2, 47, 68, 56, 66, 39, 55, 41], distribution shift [23, 17, 50, 27] etc.",1,neutral
"Within the domain of algorithmic fairness, our work is related to recent investigations into the effects of distribution shift, or data mismeasurement, on fair learning [17, 33, 37, 38].",2,positive
"For software properties such as fairness [3], [7], [8], [14], [22] and safety [52], complementary approaches provide high-confidence, probabilistic guarantees based on statistical tests and confidence bounds [2], [16], [21], [31], [52].",1,neutral
"empirically compared Seldonian algorithms to other fairnessaware ML algorithms [19], [31], [45].",1,neutral
"contextual bandits [31], the setting where the training data and deployment data come from different distributions [19], and to enforce measures of long-term fairness [48], suggesting future extensions of the Seldonian Toolkit.",2,positive
"Probabilistic verification has successfully provided guarantees for such properties for machine learning systems [27, 57, 89, 96].",1,neutral
"There are some works [Rezaei et al., 2021, Schrouff et al., 2022, An et al., 2022, Singh et al., 2021, Giguere et al., 2022] that aim to solve fairness under various distribution shifts.",2,positive
"The ability to measure the stability of chosen fairness metrics across dynamically specified bias factors can be a great first step towards safe deployment of fair classifiers, similar to recent works like Shifty [30].",1,neutral
"Recent works have studied fair classification subject to these distribution shifts and proposed solutions under reasonable assumptions on the data distribution [10, 12, 18, 19, 21, 30, 43, 54, 57, 59].",1,neutral
"The above assumptions are used in the theoretical analyses of the previous works (Maity et al., 2021; Giguere et al., 2022).",2,positive
"The reason is that Shifty is selecting the final model among the candidates that were already trained on the original training data, whereas ours trains a new model on the improved (pre-processed) data.",0,negative
", 2021), Shifty (Giguere et al., 2022), and our framework.",2,positive
"6, we add a new baseline called Shifty (Giguere et al., 2022), which focuses on the distribution shift most relevant to ours.",2,positive
"Our framework thus works best when the x distribution does not change, but does not strictly require this condition unlike other previous works on fairness under different types of shifts (Maity et al., 2021; Giguere et al., 2022).",2,positive
"• Similarly, the demographic shift (Giguere et al., 2022) assumes that the joint probabilities of x and y on the training and deployment distributions are identical (i.",2,positive
Shifty first trains candidate models on the training data and selects only the models showing high fairness in the shifted deployment data.,2,positive
"To give a favorable condition to Shifty, we assume that Shifty knows the exact test distribution.",1,neutral
"Fairness-specific shifts (Maity et al., 2021; An et al., 2022; Giguere et al., 2022; Schrouff et al., 2022) handle group (z) changes, as z is especially correlated with fair training.",1,neutral
"Another study (Giguere et al., 2022) designs a new test method to serve a fair model under another distribution change called demographic shifts, where the subgroup distribution may change – see an empirical comparison with our work in Sec.",2,positive
"• Similarly, the demographic shift (Giguere et al., 2022) assumes that the joint probabilities of x and y on the training and deployment distributions are identical (i.e., Prtrain(x = x, y = y|z = z) = Prtest(x = x, y = y|z = z)).",2,positive
"As a result, both ours and Shifty improve the fairness of the in-processing-only baseline, but ours shows better fairness than Shifty while achieving similar or higher accuracy.",2,positive
"Table 9 shows the accuracy and fairness performances of the in-processing-only baseline FairBatch, Shifty, and our framework w.r.t. a single metric (DP) and multiple metrics (DP & EO) in the synthetic and COMPAS datasets used in Tables 1 and 2.",2,positive
"Robust fairness is also well studied (Mehrotra & Vishnoi, 2022; Ma et al., 2022; Chai & Wang, 2022; An et al., 2022; Giguere et al., 2022; Jiang et al., 2023), such as under distribution shift and with limited sensitive attributes.",1,neutral
", 2021), demographic shift (Giguere et al., 2022), prior probability shift (Biswas & Mukherjee, 2021)) that may be violated in practice.",2,positive
"…and many also imposed rather strong assumptions on distributional shifts (e.g., covariate shifts (Singh et al., 2021; Coston et al., 2019; Rezaei et al., 2021), demographic shift (Giguere et al., 2022), prior probability shift (Biswas & Mukherjee, 2021)) that may be violated in practice.",1,neutral
"It is widely recognized in the literature that learning a fair classifier without any auxiliary information about the target distribution or the data collection process is practically impossible [18, 22, 30, 71, 81].",1,neutral
"[21] Stephen Giguere, Blossom Metevier, Bruno Castro da Silva, Yuriy Brun, Philip Thomas, and Scott Niekum.",0,negative
"We would like to note that the procedure that first chooses a candidate set of tuning parameters and then selects the best one has been commonly used in machine learning, such as in Seldonian algorithm framework to control safety and fairness [43, 21, 48], the Learn then Test framework for risk control [3], and in high-dimensional statistics [47].",1,neutral
"Indeed, those works define an upper-bound of the generalisation loss of either the error [28, 119], the fairness guarantees [98, 41] or both [72] characterised by the training loss and/or unfairness, model complexity and confidence.",1,neutral
"[41] considered that the distribution shift is caused by demographic shift, and assumed that the demographic proportions (i.",1,neutral
"The dissimilarity between training and deployment environments can significantly deteriorate and potentially cause harm in fairness-critical applications [17, 27, 41].",1,neutral
about the in-field distribution is known) [18].,1,neutral
"While recent work explores methods to transfer fairness [54, 48, 23], most considered settings fall into subpopulation shifts.",1,neutral
"[23] Stephen Giguere, Blossom Metevier, Yuriy Brun, Philip S.",0,negative
[16] uses reweighting to deal with fairness problem under covariate shift and [23] uses reweighting together with a fairness test to guarantee the fairness under demographic shift.,1,neutral
These guarantees can even extend to settings when the distribution of the training data is different from that of the data to which the model is applied [13].,1,neutral
", that the component will not exhibit racist or sexist behavior when applied to future inputs [5, 46, 64, 96].",0,negative
